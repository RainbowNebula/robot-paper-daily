{
    "2025-09-24": [
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies",
            "authors": "Lars Ankile,Zhenyu Jiang,Rocky Duan,Guanya Shi,Pieter Abbeel,Anusha Nagabandi",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19301",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19301",
            "arxiv_html_link": "https://arxiv.org/html/2509.19301v1",
            "abstract": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor control policies. However, these approaches are limited by the quality of human demonstrations, the manual effort required for data collection, and the diminishing returns from increasing offline data.\nIn comparison, reinforcement learning (RL) trains an agent through autonomous interaction with the environment and has shown remarkable success in various domains. Still, training RL policies directly on real-world robots remains challenging due to sample inefficiency, safety concerns, and the difficulty of learning from sparse rewards for long-horizon tasks, especially for high-degree-of-freedom (DoF) systems.\nWe present a recipe that combines the benefits of BC and RL through a residual learning framework. Our approach leverages BC policies as black-box bases and learns lightweight per-step residual corrections via sample-efficient off-policy RL.\nWe demonstrate that our method requires only sparse binary reward signals and can effectively improve manipulation policies on high-degree-of-freedom (DoF) systems in both simulation and the real world. In particular, we demonstrate, to the best of our knowledge, the first successful real-world RL training on a humanoid robot with dexterous hands.\nOur results demonstrate state-of-the-art performance in various vision-based tasks, pointing towards a practical pathway for deploying RL in the real world.",
            "introduction": "Enabling robots to learn and improve directly in their deployment environments remains a fundamental challenge in robotics. Recently, significant progress has been made in training visuomotor control policies in the real world with behavior cloning (BC) from human demonstrations [1, 2, 3, 4, 5, 6, 7, 8, 9]. However, this success requires significant infrastructure, as well as numerous hours of manual and cumbersome data collection.\nEven if unlimited data could be collected for every task, not only is human teleoperator performance generally suboptimal, but there is also emerging evidence that policy performance saturates with increasing demonstrations [10, 6, 11, 12, 13].\n\nReinforcement learning (RL) offers a complementary paradigm: agents learn autonomously through trial and error in the environment. Deep RL has shown great success in various domains [14, 15, 16, 17, 18, 19, 20, 21], including in-hand manipulation [22, 23] and locomotion [24, 25, 26, 27]. However, strong RL performance generally requires large amounts of data from online interactions, so its application has been mainly in simulation [28, 29] since real-world data are expensive and potentially unsafe to gather in large amounts.\n\nA natural direction to improve BC policies is to leverage online RL [12, 30, 31, 32], combining the strengths of each: BC policies provide a strong prior that can regularize exploration in the RL process, while online RL enhances policy performance by learning from interactions with the environment. However, modern BC architectures are typically deep models with tens of millions to billions of parameters that utilize action chunking or diffusion-based approaches, which can make it challenging to apply RL methods directly to optimize the policy. A simple yet powerful recipe that avoids several of the above issues is residual RL [33, 34, 35, 36, 12, 32, 31], where RL is applied not to learn a full policy, but only to learn corrective terms on top of a fixed base controller. Previous work has demonstrated that residual RL can indeed enhance the reliability of a pre-trained policy. Still, it has so far been limited to learning in simulation [12, 32, 31] or demonstrating results in simple or constrained settings [33, 34, 35, 36]; Applications to high-DoF systems learning directly in the real world are still lacking.\n\nIn this work, we present an off-policy residual fine-tuning (ResFiT) approach that utilizes online RL to enhance BC policies. By treating the base policy as a black box and learning a per-step residual correction that is independent of chunk size and policy parameterization, we sidestep the challenges of directly optimizing huge base policies. By carefully designing our off-policy recipe, we make the RL process sample efficient enough to scale to high-DoF bimanual systems, require only sparse binary reward signals, and be safe enough to deploy in the real world. We demonstrate robust performance on sparse-reward, long-horizon, vision-based tasks, showing that our approach achieves state-of-the-art performance for a range of tasks in simulation. We also investigate each design decision in our recipe. To the best of our knowledge, we provide the first demonstration of RL on a humanoid robot with five-fingered hands, trained entirely in the real world.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在保留行为克隆（BC）基线的同时通过离线/在线强化学习实现对复杂高自由度系统的高效微调与改进\n2. 如何实现对高DoF系统的安全、样本高效的Residual RL，以最少数据和稀疏奖励实现现实世界鲁棒性\n3. 如何在不直接优化大型基准策略的前提下，对其进行逐步残差修正以提升 visuomotor 控制性能\n4. 如何将离线BC与在线RL结合，解决数据质量、数据收集成本、以及现实世界训练的安全性挑战\n5. 如何在真实 humanoid/五指手等高复杂度机械臂上实现端到端的实证RL训练与评估\n\n【用了什么创新的方案】\n核心解决方案：提出离线行为克隆基线上的残差强化学习（ResFiT），将基线策略视为黑盒，通过学习逐步的每步残差信号来实现修正；使用面向离线/在线混合的高效RL配方，确保在高DoF、稀疏二值奖励下也能实现样本友好、真实世界的训练，并在仿真与现实中获得前所未有的鲁棒性与性能提升；通过残差学习避免直接优化庞大基线策略，降低参数化、数据需求和安全风险；首次在真实 humanoid/五指手上实现全现实世界的RL微调。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration",
            "authors": "Yang Jin,Jun Lv,Han Xue,Wendi Chen,Chuan Wen,Cewu Lu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19292",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19292",
            "arxiv_html_link": "https://arxiv.org/html/2509.19292v1",
            "abstract": "Intelligent agents progress by continually refining their capabilities through actively exploring environments. Yet robot policies often lack sufficient exploration capability due to action mode collapse. Existing methods that encourage exploration typically rely on random perturbations, which are unsafe and induce unstable, erratic behaviors, thereby limiting their effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a framework that enhances policy exploration and improvement in robotic manipulation. SOE learns a compact latent representation of task-relevant factors and constrains exploration to the manifold of valid actions, ensuring safety, diversity, and effectiveness. It can be seamlessly integrated with arbitrary policy models as a plug-in module, augmenting exploration without degrading the base policy performance. Moreover, the structured latent space enables human-guided exploration, further improving efficiency and controllability. Extensive experiments in both simulation and real-world tasks demonstrate that SOE consistently outperforms prior methods, achieving higher task success rates, smoother and safer exploration, and superior sample efficiency. These results establish on-manifold exploration as a principled approach to sample-efficient policy self-improvement.",
            "introduction": "“We want AI agents that can discover like we can, not which contain what we have discovered.”\n            — Richard Sutton, The Bitter Lesson\n\nIn recent years, data-driven robot learning [10, 53, 5, 7, 25] has attracted considerable attention, particularly for its potential to enhance robotic manipulation capabilities through large-scale data collection and training. By modeling visuomotor behaviors with neural networks, these approaches allow robot policies to learn from expert demonstrations and achieve near-human performance across a variety of tasks.\n\nDespite these advances, most existing methods still rely heavily on human teleoperation for data acquisition [53, 13] and policy refinement [30, 31], which presents several challenges. A primary concern is the high cost of teleoperation, as it typically requires skilled operators and specialized equipment, thereby limiting the scalability of data collection. More critically, teleoperated demonstrations often fail to cover the diverse scenarios a robot could encounter in the real world, resulting in distributional bias [52] and compounding error [39]. The problem is further exacerbated by the fact that human operators may act based on contextual cues inaccessible to robot sensors. Robots, on the other hand, may internalize human habits rather than task-relevant behaviors. As a result, simply scaling up teleoperated data is not the optimal path toward improving policy performance.\n\nInstead of passively imitating human-provided behavior, a line of research addresses this challenge by enabling robot policy self-improvement [6, 23, 35, 32]—actively exploring the environment to collect diverse experience and leveraging that experience to refine policies. Under this paradigm, robots can autonomously discover novel behaviors that go beyond the coverage of human demonstrations. By iteratively practicing the learned behaviors, they also develop a deeper understanding of the natural variability in their actions, ultimately leading to a more robust and resilient policy.\n\nThe key to sample-efficient robot policy self-improvement lies in effective exploration. Prior work [3, 23] has shown that imitation-learned policies often overfit demonstrations, collapse into single-modal motions, and fail to produce diverse behaviors. Without proper exploration, these policies tend to repeat failed behaviors, limiting their ability to discover improved solutions. While random exploration strategies can occasionally yield novel behaviors [29], they are generally ineffective in high-dimensional action spaces [28] and can pose safety risks in real-world deployment [16], causing potential hardware damage. This necessitates a more structured approach to exploration—one that ensures safety and effectiveness without sacrificing the diversity of experiences.\n\nTo this end, we propose SOE, a novel framework for Sample-Efficient Robot Policy Self-improvement via On-Manifold Exploration. The core idea of our method is to ensure that exploration remains constrained to the manifold of valid actions—critical for both safety and effectiveness. Prior works often perturb the action space directly [29] or inject random noise [23], leading to temporally inconsistent and unsafe behaviors, particularly under “action chunking” representations [53]. In contrast, we perform exploration in a compact latent space learned through a variational information bottleneck (VIB). The latent representation in this space preserves only task-essential information in observation while discarding irrelevant details, ensuring exploration remains structured and efficient. As illustrated in Fig. 1, by operating on this latent representation, our framework enables effective on-manifold exploration and more robust policy improvement. Furthermore, we demonstrate that in the latent space, action chunks are naturally disentangled into distinct modes. Leveraging this property, we achieve controllable exploration, which allows users to guide exploration toward preferred directions, thereby enhancing interpretability and further boosting sample efficiency. Implemented as a plug-in module, our approach can be seamlessly integrated with existing imitation learning algorithms and jointly optimized, without any degradation in their performance.\n\nTo evaluate the effectiveness of our method, we conduct extensive experiments across a variety of robot manipulation tasks in both simulation and real world. The results show that SOE consistently outperforms prior exploration methods in effectiveness, motion smoothness, and sample efficiency. With just one round of policy self-improvement, our method achieves substantial gains over the base policy, including an average relative improvement of 50.8% on real-world tasks. Additional experiments in simulation and ablation studies further confirm multi-round performance improvements and the contribution of each component in our framework. Collectively, these findings demonstrate that on-manifold exploration provides a structured, safe, and effective approach to sample-efficient robot policy self-improvement.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在高维机器人动作空间中实现安全、有效且样本高效的策略自我改进中的探索？\n2. 如何通过潜在表征约束探索到行动流形以避免无效或危险的随机扰动？\n3. 如何实现对探索的可控性与可解释性，同时保持对既有模仿学习的无干扰性？\n4. 如何在仿真与真实世界任务中提升任务成功率、平滑性和样本效率？\n\n【用了什么创新的方案】\nSOE在任务相关因素的紧凑潜在表示上进行探索，利用变分信息瓶颈学习一个仅保留任务本质信息的潜在空间，并在该潜在流形上进行探索以保持动作的有效性和安全性。它将探索与现有策略模型无缝对接，可作为插件模块嵌入到任意模仿学习框架中；潜在空间还使动作块在不同模式上解耦，从而实现可控探索，并支持人工引导以提高效率。实验结果表明在仿真和真实任务中，SOE实现更高的成功率、更平滑且更安全的探索以及更强的样本效率。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces",
            "authors": "Kuanqi Cai,Chunfeng Wang,Zeqi Li,Haowen Yao,Weinan Chen,Luis Figueredo,Aude Billard,Arash Ajoudani",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19261",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19261",
            "arxiv_html_link": "https://arxiv.org/html/2509.19261v1",
            "abstract": "Robotic manipulation in dynamic environments often requires seamless transitions between different grasp types to maintain stability and efficiency. However, achieving smooth and adaptive grasp transitions remains a challenge, particularly when dealing with external forces and complex motion constraints. Existing grasp transition strategies often fail to account for varying external forces and do not optimize motion performance effectively. In this work, we propose an Imitation-Guided Bimanual Planning Framework that integrates efficient grasp transition strategies and motion performance optimization to enhance stability and dexterity in robotic manipulation. Our approach introduces Strategies for Sampling Stable Intersections in Grasp Manifolds for seamless transitions between uni-manual and bi-manual grasps, reducing computational costs and regrasping inefficiencies. Additionally, a Hierarchical Dual-Stage Motion Architecture combines an Imitation Learning-based Global Path Generator with a Quadratic Programming-driven Local Planner to ensure real-time motion feasibility, obstacle avoidance, and superior manipulability. The proposed method is evaluated through a series of force-intensive tasks, demonstrating significant improvements in grasp transition efficiency and motion performance.\nA video demonstrating our simulation results can be viewed at https://youtu.be/3DhbUsv4eDo.",
            "introduction": "Robotic manipulation in dynamic forceful operations—such as collaborative cutting or drilling—demands real-time adaptation to varying external forces that critically affect grasp stability. Consider a human-robot woodworking scenario (Fig. 1) where the robot must continuously adjust between uni-manual and bi-manual grasps to counteract changing cutting and drilling forces. This fundamental requirement exposes two unresolved challenges in existing methods: efficient grasp transitions by minimizing execution time and arm movement and motion performance awareness, as crucial metrics like manipulability and joint limits essential for control safety are often overlooked. To bridge this gap, we propose an imitation-guided planning framework that integrates efficient grasp transitions with motion performance constraints, ensuring both stability and dexterity in forceful tasks.\n\nMulti-step manipulation planners have long tackled regrasping and grasping transitions [1]. Traditional grasping involves transporting an object by repeatedly releasing and regrasping it as needed [2]. Conventional regrasp planners rely on a supporting surface for single-arm manipulation [3, 4], while recent research extends these strategies to dual-arm scenarios [5, 6, 7].\nHowever, existing methods do not explicitly account for dynamic external forces, which vary over time, nor do they optimize regrasp transitions during forceful interactions. Studies in forceful human-robot collaboration [8, 9, 10] focus on regulating contact forces but assume a fixed or pre-determined grasp. The key challenge remains: determining where and how a robot should grasp for stability and when to transition seamlessly under complex external forces. Recent works [11, 12, 13] have made progress, but achieving stable grasps that withstand varying forces while ensuring efficient planning, manipulability, and dexterity remains difficult.\nThis paper addresses two key challenges in forceful robotic manipulation: efficient grasp transitions and motion performance optimization, proposing a novel framework to overcome them.\n\nEfficient Grasp Transitions. Previous methods [11, 13] mostly rely on random sampling-based planners for grasp transitions, often resulting in high computational costs and unstable changes. To reduce task execution time and minimize the movement distance of the robot arm, we introduce Strategies for Sampling Stable Intersections in Grasp Manifolds for seamless uni-manual and bi-manual transitions. Our Directional Gradient-Based Resampling locally adjusts the unimanual manipulator along the negative gradient, ensuring stability while maintaining a secure unimanual grasp and minimizing movement. For tasks with multiple grasp changes, Multi-Grasp Transition Check (MTC) identifies a shared intermediate configuration, reducing redundant regrasping. To further boost efficiency, we propose a Hierarchical Dual-Stage Motion Architecture, combining an Imitation Learning-based Global Path Generator with a QP-driven local planner for real-time motion optimization and obstacle avoidance, enabling faster collision-free path generation than sampling-based methods.\n\nMotion Performance Optimization.\nIn forceful operations, robotic stability is determined by three motion performance factors: manipulability, dexterity, and joint limits, which ensure kinematic feasibility under varying forces. However, many recent methods [14, 15] tend to overlook these critical aspects.\nOur framework optimizes motion performance at both the grasp configuration level and during execution. To enhance manipulability and avoid kinematic limitations, we introduce a Motion Performance Map that encodes feasibility, manipulability, and joint limit proximity. This map guides grasp sampling toward optimal workspace regions, improving selection efficiency. During execution, we enforce manipulability constraints within the QP framework, ensuring control authority over the end-effector while avoiding singularities and joint limits. This real-time optimization enables stable, collision-free motion.\n\nBy integrating these advancements, our framework enhances grasp transition efficiency while ensuring superior motion performance in both grasp selection and execution, effectively addressing key limitations in existing forceful robotic planning approaches.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在动态外部力下实现稳定而高效的双手抓持与无缝转换（uni-manual到bi-manual）？\n2. 如何在抓取转移中兼顾运动性能（可控性、可控度、关节极限）并降低计算成本？\n3. 如何在强力操作场景中实时规划可行路径并避免碰撞，同时优化抓取点选择？\n\n【用了什么创新的方案】\n策略性在抓取流形中采样稳定交点以实现单臂到双臂的平滑转变，并用方向梯度重采样局部调整单臂抓持以保持稳定性。提出多抓取转变检查以寻找共用中间配置，降低冗余重新抓取。建立分层双阶段运动架构：基于模仿学习的全局路径生成器+QP驱动的局部规划器实现实时、避障且高可 manipulability 的路径规划。引入运动性能地图（可行性、可操作性、关节极限接近度）以引导抓取选择并在执行中通过QP约束实现对端执行器的控制权和避免奇异点。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Proactive-reactive detection and mitigation of intermittent faults in robot swarms",
            "authors": "Sinan Oğuz,Emanuele Garone,Marco Dorigo,Mary Katherine Heinrich",
            "subjects": "Robotics (cs.RO); Multiagent Systems (cs.MA); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19246",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19246",
            "arxiv_html_link": "https://arxiv.org/html/2509.19246v1",
            "abstract": "Intermittent faults are transient errors that sporadically appear and disappear. Although intermittent faults pose substantial challenges to reliability and coordination, existing studies of fault tolerance in robot swarms focus instead on permanent faults. One reason for this is that intermittent faults are prohibitively difficult to detect in the fully self-organized ad-hoc networks typical of robot swarms, as their network topologies are transient and often unpredictable. However, in the recently introduced self-organizing nervous systems (SoNS) approach, robot swarms are able to self-organize persistent network structures for the first time, easing the problem of detecting intermittent faults. To address intermittent faults in robot swarms that have persistent networks, we propose a novel proactive–reactive strategy to detection and mitigation, based on self-organized backup layers and distributed consensus in a multiplex network. Proactively, the robots self-organize dynamic backup paths before faults occur, adapting to changes in the primary network topology and the robots’ relative positions. Reactively, robots use one-shot likelihood ratio tests to compare information received along different paths in the multiplex network, enabling early fault detection. Upon detection, communication is temporarily rerouted in a self-organized way, until the detected fault resolves.\nWe validate the approach in representative scenarios of faulty positional data occurring during formation control, demonstrating that intermittent faults are prevented from disrupting convergence to desired formations, with high fault detection accuracy and low rates of false positives.",
            "introduction": "Reliability in networked systems requires consistently accurate information exchange among components, often under dynamic and uncertain conditions [1, 2]. If communication links fail or become unreliable during multi-hop communication, system convergence and performance guarantees can be compromised [3, 4, 5, 6]. In self-organized robot swarms, this challenge is exacerbated by asynchronous ad-hoc communication and decentralized coordination of actuation and decision making. Robots in a self-organized swarm rely solely on local information and communication with nearby robots, without any estimation of the global state of the swarm or its environment, often leading to prolonged convergence times and vulnerability to the spread of incorrect information [7]. Frequent communication between robots can cause faulty information to spread quickly and potentially degrade overall swarm performance or lead to permanent failures.\n\nSelf-organized robot swarms exhibit some inherent fault tolerance, through redundancy and a lack of single points of failure [8, 9].\nHowever, many fault types are not mitigated by this passive tolerance and instead require dedicated mechanisms for detection and mitigation [10, 11, 12, 13]. Somewhat counter-intuitively, self-organized robot swarms are inherently much more tolerant to complete robot failures than to partial ones [14]. For example, a single robot producing faulty or malicious information has been shown to be capable of severe disruption to overall swarm behavior [14, 15]. Faulty robots can also physically obstruct the rest of the swarm, and this interference can paradoxically be worsened by the redundancy that provides swarms with some types of inherent fault tolerance [16].\n\nOther faults to which self-organized robot swarms are vulnerable and which require dedicated mechanisms for detection and mitigation are intermittent faults (IFs). IFs are temporary faults that can appear, disappear, and reappear [17], potentially caused by communication interference, sensor malfunctions, or software bugs [18]. IFs are difficult to detect and diagnose due to their transience [19] and can cause significant disruptions without leaving an easily detectable trace [18].\nA representative example involves intermittent GPS signal degradation in cluttered environments, which can induce sporadic localization errors. These errors propagate through decentralized state estimation protocols, gradually undermining coordination mechanisms without generating explicit failure indicators.\nIn real applications, e.g., in robot swarms deployed in inaccessible or dangerous environments [20, 12], the consequences of IFs to mission performance and to safety can be severe and in some cases could be irreversible. Detecting and resolving IFs before they escalate is key to minimizing disruption: early detection can prevent cascading failures leading to erroneous execution of tasks and can prevent culmination in permanent failures, either of individual robots or the swarm as a whole [21].\n\nIFs are difficult to detect in robot swarms with fully self-organized ad-hoc networks, because the network topology is transient and often unpredictable. IFs are much more straightforward to detect in fully centralized systems and in networks with static structures, for example in sensor networks [22, 23, 24]. However, for multi-robot systems, full centralization and fully static networks also present downsides, such as single points of failure and limited scalability.\n\nOur recently introduced self-organizing nervous systems (SoNS) [25] approach combines aspects of centralization and decentralization through self-organized hierarchy. Using the SoNS approach, robot swarms are coordinated via temporary logical networks that are hierarchical and culminate in a dynamic “brain” robot (i.e., leader), but which are not imposed from the outside, being instead established and maintained in a self-organized manner. This provides robot swarms with persistent and predictable network structures that are more amenable to detecting IFs, without introducing any single points of failure.\nIn short, the SoNS approach allows, for the first time, to apply centralized fault detection and mitigation strategies to robot swarms without sacrificing their oft-cited benefits of scalability, flexibility, and a lack of single points of failure.\n\nSwarm robotics usually studies passive tolerance to permanent faults [26]—that is, faults such as electromechanical failures that will remain unless they are actively repaired.\nWhen relying on passive fault tolerance, studies have usually demonstrated that a swarm continues its mission after some or many robots have failed, either by continuing with fewer robots  [10, 27, 28] or by replacing/repairing the failed robots without pausing the mission [29, 30, 31, 25].\n\nSwarm robotics studies that focus specifically on fault tolerance do not typically rely on passive tolerance, instead developing dedicated mechanisms to handle permanent faults.\nThe majority of these methods detect and react to permanent electromechanical failures after they have occurred [21, 32], often relying on time-out mechanisms in which a robot is considered non-operational if it does not respond to a message within a certain time. Existing methods for detecting permanent faults include LED synchronization [29], simulation comparison [33], shared sensor data analysis [28], and behavioral feature vectors (BFVs) [11].\nThese methods often focus on detection, assuming that once a fault is detected, a repair or other intervention is possible during normal operation (e.g., [34, 29, 21]). Although such repairs might be unrealistic in inaccessible, hazardous, or congested environments [35, 21, 12], future methods for autonomous repair could be developed to complement detection. In short, the existing reactive methods can be considered effective for many types of permanent faults [10].\nHowever, the above-mentioned detection approaches are unlikely to be applicable to the transience of IFs and their long response times [36] would likely be too slow for the early detection and recovery that IFs require. Methods to detect and repair IFs in robot swarms still need to be developed.\n\nTo the best of our knowledge, there are no existing swarm robotics methods focused on IF detection and recovery. Strategies developed for IFs in other types of systems, such as model-based analysis (e.g., discrete-event-system models [37], causal models [38]) and quantitative analysis (e.g., parameter estimation [39], geometric approaches [40], Kalman-like filtering [41]), provide valuable insights but primarily target single-unit systems with static and known system models [24, 42], which is incompatible with self-organized systems such as robot swarms. Likewise, IF strategies developed for sensor networks [22, 23] typically use fully centralized architectures to correct information transmission and reception [24], and are therefore incompatible with self-organized systems.\n\nFurthermore, although fully centralized monitoring is highly effective for detecting and correcting IFs, it can present problems of inflexibility, limited scalability, and single points of failure (e.g., at the point where monitoring is centralized). Fully self-organized approaches, by contrast, would be highly flexible and offer greater scalability and a lack of single points of failure, but would present problems of limited accuracy and potentially slow reaction times.\nIn this paper, we aim to combine elements of each system type to get the benefits of both. Using our proposed proactive–reactive approach, robots can monitor each other using self-organizing hierarchy, detecting IFs accurately and remedying them proactively.\n\nTo demonstrate our proposed proactive–reactive approach, we use the SoNS concept of self-organizing hierarchy in a robot swarm, which has been shown to incorporate temporarily centralized structures into an otherwise self-organized robot swarm without introducing single points of failure or inherently limiting scalability [25, 30, 43, 44, 45, 46]. We build on our recent theoretical foundations for self-organizing hierarchical frameworks: hierarchical Henneberg construction (HHC) [47]. In our previous work [47], we demonstrated HHC for key self-reconfiguration problems (framework merging, robot departure, and framework splitting), derived the mathematical conditions of these problems, and developed algorithms that preserve rigidity and hierarchy using only local information.\n\nIn the remainder of this paper, we assume all graphs are constructed using these already demonstrated HHC algorithms, and refer to such graphs as HHC-constructed graphs. See Appendix A for details on how HHC and SoNS are related.\n\nIn fault tolerance for multi-robot systems, both proactive and reactive mechanisms are important [48].\nIn this paper, we propose a novel proactive–reactive method to detect and mitigate IFs in robot swarms.\nIn the proposed proactive–reactive method, the robots first use distributed consensus to preemptively self-organize dynamic backup communication paths before IFs are detected. Then, the robots compare information received via primary and backup paths to detect IFs, using a one-shot likelihood ratio test. When IFs are detected, the robots react by rerouting communication through the dynamic backup paths. In this paper, we apply the proposed proactive–reactive method to a scenario of intermittently faulty relative positional information within multi-robot formations that have a hierarchical structure towards a fault-free leader, and demonstrate that the method mitigates IFs and robots are able to continue with the desired formations.\n\nThe main technical contributions of this paper can be summarized as follows:\n\nWe address a current gap in robot swarm networking, specifically how to establish back-up communication paths for leader–follower formation control in a self-organized robot swarm. We address this gap by extending the biased minimum consensus (BMC) [49] protocol for shortest path planning in static graphs. We introduce the adaptive biased minimum consensus (ABMC) protocol for dynamic graphs—addressing time-varying topologies, node neighborhoods, and costs. We demonstrate that our ABMC protocol addresses the minimum-cost path problem, with two objectives integrated into a single cost function: to minimize the number of hops to the destination (the leader robot) and to minimize the degree of network congestion (by minimizing the occurrence of parallel edges).We provide the mathematical properties and stability analysis of the ABMC protocol as a distributed consensus mechanism in dynamic graphs with piecewise constancy, including providing the necessary and sufficient conditions to uniquely determine an equilibrium point representing a minimum-cost backup path.\n\nWe address a current gap in robot swarm fault tolerance, specifically tolerance against intermittent faults (IFs). We address this gap by proposing a novel proactive–reactive fault-tolerance strategy for detection and mitigation of IFs in robot swarms. Our proposed strategy uses the ABMC protocol to construct backup network layers and combines it with a distributed likelihood ratio (LR) protocol to dynamically reroute traffic in the constructed multiplex network. We propose the mathematical conditions and design the distributed algorithms for backup layer construction and for execution of the proactive–reactive strategy for IF detection and mitigation. We also provide the time and space complexity and efficiency properties of both distributed algorithms. Finally, we demonstrate the proactive–reactive fault-tolerance strategy in formations of 20 robots with moving leaders.\n\nThe rest of the paper is organized as follows. In Sec. II, the foundational concepts regarding hierarchical frameworks are presented, along with the existing BMC protocol. In Sec. III, we formulate three key problems addressed in this paper: construction of dynamic minimum-cost backup paths, detection of IFs using the constructed backup paths, and mitigation of the detected IFs using the constructed backup paths. The first problem is addressed in Secs. IV and V, and the second and third problem are addressed in Sec. VI. Finally, in Sec. VII we validate our contributions in experiments of representative scenarios. The conclusions are summarized in Sec. VIII.",
            "llm_summary": "【关注的是什么问题】\n1. Intermittent faults (IFs) in self-organized robot swarms and their impact on formation convergence and reliability\n2. 如何在自组织机器人群体中检测与缓解IFs，兼顾可扩展性与无单点故障\n3. 在动态拓扑中实现备份路径的自组织构建与快速故障检测\n4. 将集中式检测策略与去中心化群体控制结合的主动-被动（proactive–reactive）两层防护\n\n【用了什么创新的方案】\n核心解决方案：提出主动-被动（proactive–reactive）IF检测与缓解框架，利用自组织层次化的备份通信路径实现多路径冗余；通过自组织的ABMC协议在动态图构建最小代价备份路径，并使用分布式一次性似然比检验比较主路径与备份路径的信息以实现早期IF检测；IF检测后，故事通路通过自组织方式重新路由通信直至故障消失；在20机器人队形实验中验证方法提升鲁棒性与收敛性。并将ABMC与LR检验结合，给出时空复杂度分析与稳定性条件。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap",
            "authors": "Tianyu Wu,Xudong Han,Haoran Sun,Zishang Zhang,Bangchao Huang,Chaoyang Song,Fang Wan",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 4 figures, accepted to Data@CoRL2025 Workshop",
            "pdf_link": "https://arxiv.org/pdf/2509.19169",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19169",
            "arxiv_html_link": "https://arxiv.org/html/2509.19169v1",
            "abstract": "The transfer of manipulation skills from human demonstration to robotic execution is often hindered by a “domain gap” in sensing and morphology. This paper introduces MagiClaw, a versatile two-finger end-effector designed to bridge this gap. MagiClaw functions interchangeably as both a handheld tool for intuitive data collection and a robotic end-effector for policy deployment, ensuring hardware consistency and reliability. Each finger incorporates a Soft Polyhedral Network (SPN) with an embedded camera, enabling vision-based estimation of 6-DoF forces and contact deformation. This proprioceptive data is fused with exteroceptive environmental sensing from an integrated iPhone, which provides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS application, MagiClaw streams synchronized, multi-modal data for real-time teleoperation, offline policy learning, and immersive control via mixed-reality interfaces. We demonstrate how this unified system architecture lowers the barrier to collecting high-fidelity, contact-rich datasets and accelerates the development of generalizable manipulation policies. Please refer to the iOS app at https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.",
            "introduction": "The success of modern robot learning paradigms, from Learning from Demonstration (LfD) [1, 2] to offline reinforcement learning, is fundamentally dependent on the quality and richness of the underlying data [3]. For contact-rich manipulation tasks, robust policies require more than just kinematic trajectories; they demand a holistic understanding of interaction forces, tactile feedback, and environmental context [4, 5]. Consider a human deftly handling a delicate object: the action is a symphony of precise motion, modulated forces, and continuous tactile adjustments [6]. Replicating such skills requires capturing this multi-modal information stream in its entirety.\n\nHowever, existing data collection methodologies present significant challenges. First, they often rely on a patchwork of disparate, expensive sensors—such as external motion capture systems, wrist-mounted force/torque sensors, and complex tactile skins [7, 8]—resulting in cumbersome and costly setups. This high barrier to entry limits the scale and diversity of data collection efforts [9]. Second, and more critically, a persistent domain gap exists between the human demonstrator and the robotic learner [10]. Data is often collected using one set of hardware (e.g., an instrumented glove) and deployed on a robot with entirely different sensor suites and end-effector morphology. This mismatch necessitates complex domain adaptation techniques and is a primary reason why policies trained on demonstration data often fail to generalize to physical hardware [11].\n\nTo address these challenges, we present MagiClaw, a unified hardware platform designed to seamlessly bridge the gap from human demonstration to robotic deployment. MagiClaw is a dual-purpose, two-fingered gripper that merges three key innovations:\n\nUnified Hardware Form Factor: The exact same MagiClaw device can be used as a hand-held tool for human demonstration or mounted on a robot arm for autonomous execution. This hardware consistency minimizes the sensor and morphological domain gap, facilitating direct policy transfer.\n\nVision-Based Proprioceptive Fingertips: Each finger integrates a Soft Polyhedral Network (SPN) [12] with an embedded miniature camera. This novel design enables visuotactile perception, inferring 6-DoF forces, torque, and high-resolution contact deformation from the distortion of the internal lattice structure, thereby obviating the need for costly external force sensors.\n\nIntegrated Multi-Modal Exteroception: An attached iPhone leverages its powerful sensor suite (LiDAR, RGB cameras, IMU) and ARKit framework [13] to provide synchronized, rich environmental context, including gripper pose, depth maps, and high-resolution video.\n\nOur primary contribution is an integrated system that fundamentally streamlines the collection of holistic, contact-centric data for robot learning. By fusing proprioceptive force/tactile data from the fingertips with exteroceptive visual and spatial data from a commodity smartphone, MagiClaw offers a low-cost, powerful, and user-friendly solution for both teleoperation and autonomous policy development. We posit that by democratizing access to such high-fidelity, multi-modal data, MagiClaw can serve as a catalyst for developing more robust and generalizable manipulation skills, advancing the pursuit of universal action embodiment in robotics.",
            "llm_summary": "【关注的是什么问题】\n1. 领域差距导致的人类示范到机器人执行的迁移困难（≤40词）\n2. 需要高质量多模态数据但现有传感器系统昂贵、零散、耦合度高的问题（≤40词）\n3. 如何在同一硬件上实现手持示范与机器人执行之间的无缝切换（≤40词）\n4. 如何通过 visuotactile 与环境感知实现对力、接触变形的高保真推断（≤40词）\n5. 如何降低数据采集成本并提高数据多样性以提升策略泛化能力（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：MagiClaw 将两指软聚合网格内嵌相机实现视觉-本体感知的六自由度力与接触变形估计；每指作為 SPN 传感器；通过 iPhone 的 LiDAR、RGB、深度数据进行同步的外感知；手持与机器人端可互换的统一硬件形式；自带 iOS 应用实现实时远控、离线学习数据流；降低数据采集成本、提升多模态数据质量与一致性。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination",
            "authors": "Mark Gonzales,Ethan Oh,Joseph Moore",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 Pages, 7 Figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19168",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19168",
            "arxiv_html_link": "https://arxiv.org/html/2509.19168v1",
            "abstract": "In this paper, we present a receding-horizon, sampling-based planner capable of reasoning over multimodal policy distributions. By using the cross-entropy method to optimize a multimodal policy under a common cost function, our approach increases robustness against local minima and promotes effective exploration of the solution space. We show that our approach naturally extends to multi-robot collision-free planning, enables agents to share diverse candidate policies to avoid deadlocks, and allows teams to minimize a global objective without incurring the computational complexity of centralized optimization. Numerical simulations demonstrate that employing multiple modes significantly improves success rates in trap environments and in multi-robot collision avoidance. Hardware experiments further validate the approach’s real-time feasibility and practical performance.",
            "introduction": "Local minima pose a fundamental challenge for finite-horizon, gradient-based planning approaches. In multi-robot scenarios, local minima can arise not only from the environment but also from dynamic factors, such as the changing trajectories of teammates, which may inadvertently block or cut off routes that would otherwise be viable. These pitfalls often cause robots to become stuck, find suboptimal solutions, or fail to coordinate effectively in complex environments.\n\nSampling-based planners, such as Model Predictive Path Integral (MPPI) [1] and Cross-Entropy Method (CEM) [2, 3], attempt to improve the trajectory cost by stochastically sampling and evaluating trajectories in the cost landscape. In practice, these methods utilize hyperparameters, such as sampling variance, number of samples, and the horizon length, to adapt the exploration to the environment. However, both MPPI and CEM typically sample trajectories around the prior best policy, leading to a concentration of samples in a narrow region of the solution space. This localized search impedes the planner’s ability to effectively navigate around traps or escape from local minima once they occur, especially in environments with challenging topology. As a result, the planner can become stuck in suboptimal regions, regardless of the variance or adaptation strategy.\n\nIn multi-robot systems, the difficulty is exacerbated by the need for robots to coordinate planned trajectories. Centralized control approaches [4, 5, 6, 7] can, in principle, achieve globally optimal coordination; however, they suffer from scalability issues and high computational costs as the team size increases. Distributed methods, while scalable, often require robots to individually select their optimal trajectory, subsequently negotiating with teammates to reach a feasible consensus. When each robot contributes only a single candidate trajectory, the team risks deadlock or persistent local minima, as a lack of trajectory diversity reduces the likelihood of discovering collision-free, cooperative maneuvers, especially when teammates dynamically update their plans or block each other’s routes in real-time.\n\nTo overcome these limitations, we introduce a multimodal sampling and clustering framework that maintains multiple policy candidates for each robot, thereby increasing diversity and robustness against local minima in both environmental and collaborative planning contexts.\n\nOur contributions are:\n\nA cross-entropy planning approach capable of preserving multiple policy modes for increased planning robustness.\n\nA multi-robot coordination framework that enables reasoning about sets of candidate policies to avoid local minima and deadlocks more reliably.\n\n1. A cross-entropy planning approach capable of preserving multiple policy modes for increased planning robustness.\n\n2. A multi-robot coordination framework that enables reasoning about sets of candidate policies to avoid local minima and deadlocks more reliably.",
            "llm_summary": "【关注的是什么问题】\n1. 本地最优陷阱与多模态政策在单机器人导航中的鲁棒性（≤40词）\n2. 多机器人协同中的死锁、协调与全局目标优化的可扩展性（≤40词）\n3. 受限搜索空间中多模态策略对避障与探索的提升（≤40词）\n\n【用了什么创新的方案】\n- 引入跨熵法的多模态策略规划，保留多条候选策略以提升对局部极小值的鲁棒性。\n- 基于采样与聚类的框架，在多机器人场景中共享多策略集合，避免死锁并实现分布式协同。\n- 通过 receding-horizon 与多模态分布采样实现对全局成本的高效优化，降低集中化计算开销。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer",
            "authors": "Kangmin Kim,Seunghyeok Back,Geonhyup Lee,Sangbeom Lee,Sangjun Noh,Kyoobin Lee",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 5 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19142",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19142",
            "arxiv_html_link": "https://arxiv.org/html/2509.19142v1",
            "abstract": "Bimanual grasping is essential for robots to handle large and complex objects. However, existing methods either focus solely on single-arm grasping or employ separate grasp generation and bimanual evaluation stages, leading to coordination problems including collision risks and unbalanced force distribution. To address these limitations, we propose BiGraspFormer, a unified end-to-end transformer framework that directly generates coordinated bimanual grasps from object point clouds. Our key idea is the Single-Guided Bimanual (SGB) strategy, which first generates diverse single grasp candidates using a transformer decoder, then leverages their learned features through specialized attention mechanisms to jointly predict bimanual poses and quality scores. This conditioning strategy reduces the complexity of the 12-DoF search space while ensuring coordinated bimanual manipulation. Comprehensive simulation experiments and real-world validation demonstrate that BiGraspFormer consistently outperforms existing methods while maintaining efficient inference speed (<<0.05s), confirming the effectiveness of our framework. Code and supplementary materials are available at https://sites.google.com/bigraspformer",
            "introduction": "Bimanual grasping enables robots to manipulate large, heavy, or unwieldy objects beyond single-arm capabilities, making it essential for tasks such as lifting furniture, carrying long boards, or moving large boxes [1, 2]. However, most robotic grasping research has focused on single-arm systems, primarily on learning to detect 6-DoF grasp poses from point clouds [3, 4, 5, 6, 7, 8]. While effective for single-arm tasks, these approaches cannot be directly extended to bimanual scenarios. First, bimanual grasping expands the action space to 12-DoF, doubling the computational complexity. Second, it introduces new challenges, including collision avoidance, balanced force/torque distribution, and dual-arm coordination for post-grasp manipulation.\n\nFor bimanual grasping, only a few methods have been proposed so far. The DA2 dataset [9] introduced the first benchmark by extending single-arm datasets [10, 11, 3] with dual-arm-specific metrics such as force closure, dexterity, and torque balance [9, 12]. However, most existing approaches adopt modular architectures that separate grasp generation and evaluation. For example, Dual-PointNetGPD [9] evaluates the quality of grasp pairs from given candidates, requiring external single-arm grasp generators. Similarly, CGDF [13] directly generates bimanual grasps but lacks integrated quality prediction, instead relying on additional scoring modules or heuristic pairing strategies [14, 15]. As a result, current methods yield limited diversity, poor coordination, and high computation due to modular pipelines.\n\nIn this paper, we propose BiGraspFormer, the first unified end-to-end framework that directly generates coordinated bimanual grasps from object point clouds (Fig. 1). The key insight is that single-grasp features can effectively guide bimanual grasp generation, rather than treating dual-arm coordination as two independent problems. BiGraspFormer introduces a novel Single-Guided Bimanual (SGB) strategy: it first generates diverse single-arm grasp candidates, then leverages their learned features through specialized attention mechanisms to jointly predict bimanual poses and quality scores. This unified approach eliminates separate modules and explicitly models coordination between grasps, enabling stable and efficient dual-arm manipulation. Comprehensive experiments in both simulation and real-world environments demonstrate that BiGraspFormer achieves superior success, diversity, and speed compared to existing methods.\n\nOur contributions are summarized as follows:\n\nWe propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\nWe propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\nWe introduce the Single-Guided Bimanual (SGB) strategy, which leverages single-arm grasp features to guide bimanual generation, reducing computational complexity and enhancing dual-arm coordination.\n\nWe achieve state-of-the-art bimanual grasping performance while maintaining fast inference suitable for real-world deployment.\n\n1. We propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\n2. We introduce the Single-Guided Bimanual (SGB) strategy, which leverages single-arm grasp features to guide bimanual generation, reducing computational complexity and enhancing dual-arm coordination.\n\n3. We achieve state-of-the-art bimanual grasping performance while maintaining fast inference suitable for real-world deployment.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在点云上直接生成协调的双臂抓取姿态（12-DoF）以实现稳定的双臂物体操作\n2. 现有方法的模块化导致双臂协作差、碰撞风险与力分配不均等问题\n3. 如何在端到端框架中有效建模双臂协调并提高推理速度\n\n【用了什么创新的方案】\nSingle-Guided Bimanual (SGB) 策略：先通过变换器解码器生成多样的单臂抓取候选，再利用这些单臂抓取的特征，通过专门的注意力机制共同预测双臂抓取的姿态与质量分数，从而端到端地产生协调的双臂抓取。该统一框架 eliminates 分离的抓取生成与评估模块，直接从对象点云进行协调抓取的生成与评分。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation",
            "authors": "Sarvesh Prajapati,Ananya Trivedi,Nathaniel Hanson,Bruce Maxwell,Taskin Padir",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 10 figures, submitted to Robotic Computing & Communication",
            "pdf_link": "https://arxiv.org/pdf/2509.19105",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19105",
            "arxiv_html_link": "https://arxiv.org/html/2509.19105v1",
            "abstract": "Successful navigation in outdoor environments requires accurate prediction of the physical interactions between the robot and the terrain. To this end, several methods rely on geometric or semantic labels to classify traversable surfaces. However, such labels cannot distinguish visually similar surfaces that differ in material properties. Spectral sensors enable inference of material composition from surface reflectance measured across multiple wavelength bands. Although spectral sensing is gaining traction in robotics, widespread deployment remains constrained by the need for custom hardware integration, high sensor costs, and compute-intensive processing pipelines. In this paper, we present RGB Image to Spectral Signature Neural Network (RS-Net), a deep neural network designed to bridge the gap between the accessibility of RGB sensing and the rich material information provided by spectral data. RS-Net predicts spectral signatures from RGB patches, which we map to terrain labels and friction coefficients. The resulting terrain classifications are integrated into a sampling-based motion planner for a wheeled robot operating in outdoor environments. Likewise, the friction estimates are incorporated into a contact-force–based MPC for a quadruped robot navigating slippery surfaces. Thus, we introduce a framework that learns the task-relevant physical property once during training and thereafter relies solely on RGB sensing at test time. The code is available at https://github.com/prajapatisarvesh/RS-Net.",
            "introduction": "Autonomous robots are increasingly deployed in everyday settings, ranging from self-driving taxis [1] and search-and-rescue missions [2, 3] to wildfire prevention [4, 5]. In such unstructured environments, reliable autonomy demands more than obstacle avoidance. It requires precise reasoning about how terrain properties influence motion. For example, vehicles must modulate braking on icy roads, and off-road platforms should bypass dense swamps to avoid entrapment. These scenarios show that perception must move beyond geometry and semantics toward reliable estimates of robot–terrain interactions.\n\nSeveral off-road motion planning pipelines use RGB cameras to identify terrain from images [6, 7, 8]. In some cases, visually similar surfaces with different physical properties, such as ice on asphalt, may be mislabeled, leading to invalid traversability cost maps. Depth cameras and LiDAR are often used to estimate the ease of motion over a surface [9, 10]. However, the robot must first drive the terrain to create a dataset, which risks hardware damage and necessitates tuning specific to the operating site.\n\nIn contrast, spectral sensors offer a non-invasive way to estimate material properties. This is accomplished by leveraging distinct patterns of light absorption and reflection, known as spectral signatures, to characterize underlying material composition. These capabilities are finding use in robotics applications such as wildfire risk monitoring [3], manipulation [11, 12], and exploration [13]. By mapping spectral signatures to physical quantities such as moisture content, rigidity, or surface type, the same sensing stack can be repurposed across robots and environments with minimal changes to the processing pipeline. However, challenges such as custom mounts, calibration requirements, large datasets, and high sensor costs currently limit deployment at scale.\n\nRGB cameras are inexpensive, widely available, and already standard in robotic perception pipelines such as object detection [14] and tracking [15]. Compared to hyperspectral systems, they are cheaper, lighter, and more power efficient, which simplifies integration on mobile platforms. Advances in deep learning [16] now allow RGB imagery to approximate measurements traditionally obtained from more information-dense sensors.\n\nMotivated by this, we seek to retain the deployment advantages of RGB cameras while recovering spectral sensor features. We introduce RGB Image to Spectral Signature Neural Network or RS-Net, a deep neural network architecture trained on spectral data collected from diverse materials. It maps RGB image patches to their corresponding spectral signatures. These estimates are passed to a lightweight feedforward neural network whose weights are fine-tuned for the target physical property. We retrain this network once per task, enabling the same neural network architecture to perform terrain classification and friction estimation. Our entire inference pipeline runs at approximately 5 Hz, making it suitable for real-time robotic applications. Fig. LABEL:fig:paper_intro outlines the proposed architecture.\n\nWe validate our method in both simulation and hardware experiments. The terrain classification is used in a sampling-based motion planner for outdoor navigation of a skid-steer robot. Similarly, the friction estimates are integrated into a model predictive control (MPC) scheme for a quadrupedal robot operating on slippery surfaces. Finally, we also discuss how the proposed approach generalizes to other robots and additional physical properties relevant to off-road planning.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在不增加硬件成本的情况下，通过RGB图像推断材料光谱特征以更准确地评估地形与摩擦\n2. how to bridge RGB sensing with spectral properties to improve terrain traversability prediction and motion planning\n3. 将光谱特征转化为可用于采样式路径规划和MPC的物理属性\n4. 实时性与通用性：在多种环境与机器人上实现低成本、快速推断的框架\n\n【用了什么创新的方案】\nRS-Net 将 RGB patch 映射到光谱签名，在此基础上训练一个轻量前馈网络输出地形标签和摩擦系数；整个管线在测试时仅需 RGB，推理约 5 Hz；通过在训练阶段明确学习任务相关的物理属性，使同一架构可一次性对不同任务进行微调并发布到实际机器人上。代码开放，支持仿真与硬件实验中的地形导航和滑移表面的 MPC/规划集成。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation",
            "authors": "Hongli Xu,Lei Zhang,Xiaoyue Hu,Boyang Zhong,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "project website:this https URL, 11 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.19102",
            "code": "https://sites.google.com/view/funcanon",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19102",
            "arxiv_html_link": "https://arxiv.org/html/2509.19102v1",
            "abstract": "General-purpose robotic skills from end-to-end demonstrations often leads to task-specific policies that fail to generalize beyond the training distribution.\nTherefore, we introduce FunCanon, a framework that converts long-horizon manipulation tasks into sequences of action chunks, each defined by an actor, verb, and object.\nThese chunks focus policy learning on the actions themselves, rather than isolated tasks, enabling compositionality and reuse. To make policies pose-aware and category-general, we perform functional object canonicalization for functional alignment and automatic manipulation trajectory transfer, mapping objects into shared functional frames using affordance cues from large vision–language models.\nAn object-centric and action-centric diffusion policy FuncDiffuser trained on this aligned data naturally respects object affordances and poses, simplifying learning and improving generalization ability.\nExperiments on simulated and real-world benchmarks demonstrate category-level generalization, cross-task behavior reuse, and robust sim-to-real deployment, showing that functional canonicalization provides a strong inductive bias for scalable imitation learning in complex manipulation domains.\nDetails of the demo and supplemental material are available on our project website\n https://sites.google.com/view/funcanon.",
            "introduction": "As robots transition from controlled laboratory settings to unstructured real-world environments, developing robust and generalizable manipulation policies becomes increasingly critical. A fundamental challenge is enabling agents to generalize across unseen objects, diverse poses, and varying tasks — a capability that remains elusive for current imitation learning approaches.\n\nImitation learning methods based on RGB images [1] or point clouds [2] often suffer from limited precision and generalization due to viewpoint sensitivity, noisy observations, and redundant scene encodings. In contrast, 3D scene representations have shown promise in improving generalization [3]. To further address these challenges, object-centric representations, which focus on structured, object-level information such as 6D poses [4, 5] and scene flow [6], have gained significant attention. SPOT [4] demonstrates that SE(3) pose diffusion policy can improve cross-embodiment generalization, even when trained solely on passive human videos.\nExisting object-centric approaches [4] often depend on instance-specific, goal-conditioned trajectories, limiting generalization across categories and tasks. We attribute this to viewing manipulation as monolithic programs rather than modular, reusable behaviors.\nIn the field of computer vision, prior work, such as UAD [7] and Object Canonicalization [8], mainly targets improving visual representations and semantic understanding. Related efforts have also investigated category-level affordance pose estimation [9]. However, these methods have not explored how such representations can be leveraged for improving category-level alignment of robotic manipulation data, or augmenting manipulation trajectories.\n\nKey open questions remain: how can generalized representations be leveraged to synthesize diverse manipulation data, model long-horizon tasks, and train robust, generalizable manipulation policies?\n\nTo address these challenges, we propose FunCanon, a framework that models manipulation as compositions of reusable action primitives—such as pouring, grasping, or inserting—defined over functionally aligned bi-object interactions. By leveraging affordance cues from large vision-language models (VLMs), FunCanon canonicalizes semantically related objects (e.g., kettles and pitchers) into shared functional frames. This functional alignment enables the automatic manipulation trajectory transfer and the training of pose-aware, object-centric diffusion policies that focus solely on interaction dynamics, decoupled from specific object identities, camera viewpoints, or task semantics. To achieve robust and generalizable manipulation, we first decompose long-horizon tasks into meaningful action chunks, each specifying an actor, an action, and an object. This segmentation is performed by a large multimodal language model (MLLM), such as GPT-4o, in combination with large vision models (LVM) that extract affordance cues to guide chunking based on functional relevance. Next, we integrate these affordance cues with precise object pose estimates to perform functional alignment, canonicalizing objects into shared functional frames. This process identifies action-related affordance regions and aligns bi-object poses, producing a semantically grounded representation of manipulation interactions. Leveraging this functional alignment, automatic trajectory transfer method is proposed to augment training data on RLBench base tasks to increase data diversity and functional coverage. During policy training and inference, our object-centric diffusion policy receives inputs encoding both affordances, poses of bi-object pairs, point clouds and action verb and estimate actions.\nOur main contributions are:\n\nIntroducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\nIntroducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\nLeveraging large vision-language models for affordance-driven functional canonicalization for functional alignment and automatic trajectory transfer, enabling pose-aware and category-generalizable policy learning.\n\nDeveloping an object-centric diffusion policy trained on functionally aligned data, achieving both instance-level and category-level generalization and robust sim-to-real transfer.\n\n1. Introducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\n2. Leveraging large vision-language models for affordance-driven functional canonicalization for functional alignment and automatic trajectory transfer, enabling pose-aware and category-generalizable policy learning.\n\n3. Developing an object-centric diffusion policy trained on functionally aligned data, achieving both instance-level and category-level generalization and robust sim-to-real transfer.",
            "llm_summary": "【关注的是什么问题】\n1. 如何将长时程操纵任务分解为可重复使用的动作原语以提高泛化性\n2. 如何利用功能对齐和对象-功能框架实现从示例到通用策略的迁移\n3. 如何在无关对象身份、视角和任务语义的前提下实现姿态感知的策略学习\n4. 如何通过对齐数据训练对象中心的扩散策略以增强实例级和类别级泛化能力\n\n【用了什么创新的方案】\n- 将长时程操作分解为由行为者-动作-对象组成的可重用动作原语，并在功能对齐的双对象框架内进行分段\n- 通过大视觉语言模型的 affordance 提供和功能 canonicalization 将相关对象映射到共享功能框架，实现自动轨迹迁移\n- 基于对齐数据训练对象中心的扩散策略 FuncDiffuser，使策略在对象姿态、可供性和交互动力学上具备姿态感知能力并实现良好跨域泛化\n- 结合多模态分段（MLLM/ LVM）与功能对齐，进行自动化的轨迹迁移和增强数据多样性\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation",
            "authors": "Zhennan Jiang,Kai Liu,Yuxin Qin,Shuai Tian,Yupeng Zheng,Mingcai Zhou,Chao Yu,Haoran Li,Dongbin Zhao",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19080",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19080",
            "arxiv_html_link": "https://arxiv.org/html/2509.19080v1",
            "abstract": "Robotic manipulation policies are commonly initialized through imitation learning, but their performance is limited by the scarcity and narrow coverage of expert data. Reinforcement learning can refine polices to alleviate this limitation, yet real-robot training is costly and unsafe, while training in simulators suffers from the sim-to-real gap. Recent advances in generative models have demonstrated remarkable capabilities in real-world simulation, with diffusion models in particular excelling at generation. This raises the question of how diffusion model-based world models can be combined to enhance pre-trained policies in robotic manipulation. In this work, we propose World4RL, a framework that employs diffusion-based world models as high-fidelity simulators to refine pre-trained policies entirely in imagined environments for robotic manipulation. Unlike prior works that primarily employ world models for planning, our framework enables direct end-to-end policy optimization. World4RL is designed around two principles: pre-training a diffusion world model that captures diverse dynamics on multi-task datasets and refining policies entirely within a frozen world model to avoid online real-world interactions. We further design a two-hot action encoding scheme tailored for robotic manipulation and adopt diffusion backbones to improve modeling fidelity. Extensive simulation and real-world experiments demonstrate that World4RL provides high-fidelity environment modeling and enables consistent policy refinement, yielding significantly higher success rates compared to imitation learning and other baselines. More visualization results are available at https://world4rl.github.io/.",
            "introduction": "Despite recent progress in robotic manipulation, the field still faces critical challenges for practical deployment. Imitation learning is widely used to bootstrap policies from demonstrations, but its effectiveness is constrained by the inconsistency[1] and limited diversity[2, 3, 4] of available datasets. Although offline reinforcement learning (RL) can extract better policies from imperfect data, its susceptibility to overestimation[5] still makes it difficult to work effectively with limited datasets. Online RL offers a natural way to refine such pre-trained policies through interaction. However, real-robot RL, while capable of overcoming dataset limitations, suffers from high interaction costs and significant safety risks that hinder large-scale training. Training in simulation avoids these risks but inevitably introduces discrepancies from real-world physics, leading to a persistent sim-to-real gap[6].\n\nIn recent years, generative models have achieved remarkable progress in the visual domain[7], with diffusion models[8] demonstrating particularly strong performance in image[9] and video generation[10, 11]. Such generative capacity opens new opportunities for modeling complex and dynamic environments, offering a promising path toward learnable world simulators that provide realistic yet flexible environments for RL training in robotic manipulation.\n\nBuilding on this idea, we introduce World4RL, a framework that systematically integrates diffusion world models into RL for robotic manipulation. World4RL follows a two-stage paradigm: we first pre-train a diffusion world model on multi-task datasets to capture diverse dynamics, and then refine policies entirely within the frozen model to avoid costly and unsafe online interactions. Serving as a high-fidelity simulator, the world model is composed of a diffusion transition model that predicts future observations conditioned on current observations and actions, and a reward classifier that provides sparse success signals, enabling policy optimization without real-world rollouts.\n\nThis design of framework contrasts with prior approaches such as IRASim[12] and NWM[13], which primarily use generative video models for planning at test time rather than for direct policy training. A closer line of work, DiWA[14], also employs world models for policy learning. However, it relies on recurrent state-space models (RSSM[15]), which lead to blurry generations and compounding rollout errors. In contrast, World4RL leverages diffusion backbones that generate sharper and temporally coherent rollouts, thereby supporting effective end-to-end reinforcement learning.\n\nTo further adapt world models to robotic manipulation, which involves high-dimensional action spaces and complex environment interactions compared to navigation[13] and games[16], we investigate two critical design choices: a two-hot action encoding[17] scheme that provides an efficient representation of continuous actions while enabling lossless reconstruction, thereby serving as a robust bridge between the RL agent and the world model, and diffusion backbone architectures that determine the fidelity and consistency of predictions. These considerations are essential for enabling diffusion world models to serve not only as visual predictors but also as reliable simulators for policy training. To this end, our work makes the following key contributions.\n\nWe propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\nWe propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\nTo improve modeling fidelity and enable more effective policy refinement, we design a two-hot action encoding tailored for robotic manipulation and adopt a diffusion backbone as the world model.\n\nWe validate the effectiveness of World4RL through extensive experiments, showing that it consistently outperforms competitive baselines and significantly enhances policy refinement, improving success rates by 16% and 25% in simulation and real-robot experiments, respectively.\n\n1. We propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\n2. To improve modeling fidelity and enable more effective policy refinement, we design a two-hot action encoding tailored for robotic manipulation and adopt a diffusion backbone as the world model.\n\n3. We validate the effectiveness of World4RL through extensive experiments, showing that it consistently outperforms competitive baselines and significantly enhances policy refinement, improving success rates by 16% and 25% in simulation and real-robot experiments, respectively.",
            "llm_summary": "【关注的是什么问题】\n1. 如何利用扩散模型构建高保真世界模型以提升机器人操控策略的学习效率与安全性（≤40词）\n2. 如何在不在线真实交互的前提下，通过在冻结的扩散世界模型中对策略进行端到端优化来实现改进（≤40词）\n3. 如何设计适合高维持续动作空间的两热编码以及扩散骨干网络以提升生成的时序一致性与预测保真度（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出 World4RL，将扩散式世界模型作为高保真仿真器用于策略的离线强化学习训练；先在多任务数据集上预训练扩散世界模型，再在冻结的模型中进行端到端策略优化；使用两热动作编码和扩散骨干来提升建模保真度与鲁棒性；通过扩散转移模型预测未来观测并结合奖励分类器提供稀疏成功信号，实现无真实滚动的策略 refinement。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions",
            "authors": "Laura Connolly,Aravind S. Kumar,Kapi Ketan Mehta,Lidia Al-Zogbi,Peter Kazanzides,Parvin Mousavi,Gabor Fichtinger,Axel Krieger,Junichi Tokuda,Russell H. Taylor,Simon Leonard,Anton Deguet",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19076",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19076",
            "arxiv_html_link": "https://arxiv.org/html/2509.19076v1",
            "abstract": "Image-guided robotic interventions involve the use of medical imaging in tandem with robotics. SlicerROS2 is a software module that combines 3D Slicer and robot operating system (ROS) in pursuit of a standard integration approach for medical robotics research. The first release of SlicerROS2 demonstrated the feasibility of using the C++ API from 3D Slicer and ROS to load and visualize robots in real time. Since this initial release, we’ve rewritten and redesigned the module to offer greater modularity, access to low-level features, access to 3D Slicer’s Python API, and better data transfer protocols. In this paper, we introduce this new design as well as four applications that leverage the core functionalities of SlicerROS2 in realistic image-guided robotics scenarios.",
            "introduction": "Medical robotics is an evolving and rapidly growing research field with the potential to transform standard clinical practice. It is possible that robots will one day transcend human capabilities while offering higher efficiency, lower costs, improved training outcomes and better safety [1]. The advancement of image-guided robotics in particular, which are systems that rely on both medical imaging and robotics, is critical for achieving this potential. This is because image-guided robots can be used to fuse preoperative and intraoperative realities [2].\n\nThere are several combinations of imaging modalities and robotic systems have been explored in this capacity. For example, the SpineBot uses computed tomography (CT) imaging to help define the trajectory of pedicle screws, and robotics to guide the surgeon through those trajectories [3], [4]. Another example is the MrBot, which was designed to help perform percutaneous needle interventions within the confines of a magnetic resonance imaging (MRI) scanner [5]. Similarly, the Artemis robot was designed to facilitate transrectal prostate biopsy under ultrasound guidance with MRI fusion [6]. These are just a few examples of the numerous procedures and therapies where the use of image-guidance in tandem with robotics has been investigated. More recently, advancements in image-guided robotics have enabled: navigation of catheters into blood vessels with magnetic continuum devices [7], autonomous needle steering for lung biopsy [8] and teleoperated neurovascular interventions [9].\n\nDespite this extensive investigation, there are only a few areas of intervention where image-guided robotic systems that have achieved widespread adoption and financial commercial success such as robotic bronchoscopy, radiation oncology and neurosurgery [2] [10]. One contributor to this slow growth and adoption is the lack of a common integration approach. For any image-guided robotic system, integration of the imaging modality and the robot is the most important factor for usability. However, several companies and research systems take their own unique approach to integration. This results in device-specific software, expensive research licenses, incompatible communication protocols, and overall, a high barrier to entry to develop such systems. Considering these challenges and their potential threat to continued development, it is imperative to provide a common integration scheme for image-guided robotics. We hypothesize that this will prevent re-engineering and promote reproducibility across different clinical applications.\n\nFrom a development perspective, image-guided therapy (IGT) platforms and medical robotics platforms are often separated. In the realm of IGT, the open-source medical imaging platform, 3D Slicer is the most commonly used research platform [11] [12]. With over one million downloads and an active research and support community, 3D Slicer is used for segmentation, virtual reality, image analysis, artificial intelligence, and several other applications [13]. Several research platforms have been enabled by or added to 3D Slicer such as SlicerIGT [12], SlicerVR [14], MONAI label [15], and Total Segmentator [16]. There are also open-source platforms like OpenIGTLink and the PLUS toolkit that allow users to interface commercial hardware with 3D Slicer to build complete IGT systems [17] [18]. As a result of these efforts, 3D Slicer is considered the de-facto open source software for developing navigated, image-guided interventions.\n\nIn the field of medical robotics, robot operating system (ROS), an open-source middleware designed to support robotics development, is the predominant framework. ROS is a modular development framework that provides tools for autonomous navigation, simulation, visualization, and control [19]. Like 3D Slicer, ROS has a very active research community that is constantly contributing to the platform. For medical robotics specifically, several research tools like the Computer Integrated Surgical Systems Surgical Assistant Workstation (CISST-SAW) libraries [20] and the Asynchronous Multi-Body Framework (AMBF) [21] support ROS. The da Vinci Research Kit (dVRK), a popular open-source medical robot [22], also supports ROS for software development. Furthermore, many commercially available robots provide a ROS interface off the shelf.\n\nIn an effort to pursue a common integration scheme for image-guided robotics research, we decided to bridge these two ecosystems. As evidenced by the numerous published papers that employ both 3D Slicer and ROS (71 papers available on Google Scholar using keywords “3D Slicer” AND “robot operating system”), there is also demand from the community for this integration. Previous attempts to bridge 3D Slicer and ROS such as the ROS-IGTL bridge [23], custom applications for specific robots [24] [25], and our initial offering of SlicerROS2 [26] have fallen short of meeting all of the needs of a common integration tool. These needs include greater usability by providing access to low-level features, robust data transfer protocols that support commonly used message types, thorough documentation, and maintainability [27]. We have since redesigned SlicerROS2 to further support image-guided robotics research considering these requirements. The details of this new design are described in the following sections. The contributions of this paper are: 1) A newly designed research module for efficient data transfer between 3D Slicer and ROS 2 and 2) four relevant applications that demonstrate how it can be used for rapid research prototyping.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在图像引导的机器人干预中实现 3D Slicer 与 ROS2 的高效、通用集成\n2. 提高跨平台数据传输的鲁棒性与对低级功能的可访问性\n3. 快速原型化研究所需的模块化、可维护的工具体系\n4. 在医学机器人研究中促进可重复性与协同开发的标准化方案\n【用了什么创新的方案】\n核心解决方案：重新设计的 SlicerROS2 模块实现更高模块化和低级功能访问，整合 3D Slicer 的 Python 与 C++ API，改进数据传输协议，支撑多种消息类型，并提供四个实际应用场景以展示核心功能在图像引导机器人中的应用与原型化能力\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation",
            "authors": "Geonhyup Lee,Yeongjin Lee,Kangmin Kim,Seongju Lee,Sangjun Noh,Seunghyeok Back,Kyoobin Lee",
            "subjects": "Robotics (cs.RO)",
            "comment": "9 pages, 9 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19047",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19047",
            "arxiv_html_link": "https://arxiv.org/html/2509.19047v1",
            "abstract": "Contact-rich manipulation tasks such as precision assembly require precise control of interaction forces, yet existing imitation learning methods rely mainly on vision-only demonstrations. We propose ManipForce, a handheld system designed to capture high-frequency force–torque (F/T) and RGB data during natural human demonstrations for contact-rich manipulation. Building on these demonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT). FMT encodes asynchronous RGB and F/T signals using frequency- and modality-aware embeddings and fuses them via bi-directional cross-attention within a transformer diffusion policy. Through extensive experiments on six real-world contact-rich manipulation tasks—such as gear assembly, box flipping, and battery insertion—FMT trained on ManipForce demonstrations achieves robust performance with an average success rate of 83% across all tasks, substantially outperforming RGB-only baselines. Ablation and sampling-frequency analyses further confirm that incorporating high-frequency F/T data and cross-modal integration improves policy performance, especially in tasks demanding high precision and stable contact.\nHardware, software, and video demos are available at: https://sites.google.com/view/manipforce/홈.",
            "introduction": "Contact-rich manipulation tasks such as precise assembly [1, 2, 3, 4], battery disassembly [5], and non-prehensile handling [6] require high precision and force-aware manipulation. Humans naturally perceive contact forces and their subtle changes when assembling parts, adjusting their strategies accordingly. Yet most robotic approaches rely solely on visual demonstrations, missing the rich F/T information humans provide.\n\nRecent advances in imitation learning [7, 8, 9] have demonstrated strong potential for dexterous and contact-rich manipulation by learning directly from human demonstrations.\nHowever, these methods still rely on high-quality demonstration data, which is costly and difficult to collect for fine-grained interactions.\nHand-held data collection systems such as UMI [10] have been proposed to address this challenge by enabling natural human demonstrations without the expertise requirements and remote-control limitations of teleoperation.\nWhile effective for simplifying demonstration collection, UMI does not capture force–torque (F/T) information, which is essential for accurately modeling contact behaviors.\nMore recent work [11] combines visual and F/T data but relies on point clouds to represent the scene, which introduces complex setup requirements and fundamentally limits the ability to perceive small objects and fine clearances essential for contact-rich manipulation.\nFurthermore, from a learning perspective, this approach down-samples high-frequency F/T signals to match the image frame rate, losing rich temporal information necessary for modeling contact dynamics.\n\nTo address these limitations, we introduce ManipForce a handheld system for simultaneous RGB–F/T data collection during natural human demonstrations, and the Frequency-Aware Multimodal Transformer (FMT), which learns robust policies from the collected data for diverse, precise, and contact-rich manipulation tasks.\n\nManipForce consists of a dual handheld camera setup with a wrist-mounted F/T sensor to capture both visual and high-frequency force signals during human-guided demonstrations.\nThis configuration enables robust perception of small objects, tight clearances, and fine-grained contacts, allowing collected demonstrations to transfer directly to robotic execution.\nWe replace SLAM-based wrist tracking with 3D ArUco marker pose estimation to maintain accuracy during close-contact interactions without environmental dependencies, and apply tool gravity compensation to ensure precise and interaction-focused F/T measurements.\nWe propose the FMT, which learns from asynchronous RGB (30 Hz) and F/T (>200 Hz) signals using a Transformer-based Diffusion Policy [7] architecture.\nTo exploit the higher-frequency force signals relative to images, the model tokenizes both RGB and F/T inputs using learnable frequency and modality embeddings.\nThis design enables the model to effectively handle heterogeneous modalities with asynchronous sampling rates.\nIn addition, bi-directional cross-attention modules fuse complementary information across modalities.\nWe evaluate our approach on six contact-rich manipulation tasks spanning precision assembly, non-prehensile manipulation, and complex disassembly, and observe significant performance gains over RGB-only baselines.\nAblation studies further confirm that high-frequency F/T sensing, unified positional embeddings, and bi-directional cross-attention each make complementary contributions to robust multimodal policy learning.\n\nOur main contributions are:\n\nWe introduce ManipForce, a handheld RGB–F/T data collection system enabling diverse and fine-grained contact-rich manipulation demonstrations.\n\nWe propose FMT, which handles inputs with asynchronous sampling rates through frequency-aware multimodal representation learning and cross-attention within a Transformer architecture, enabling robust policy learning for contact-rich manipulation.\n\nWe demonstrate robust performance on diverse contact-rich manipulation tasks—including gear assembly, plug insertion, battery disassembly, and lid operations—consistently outperforming RGB-only baselines.\n\n1. We introduce ManipForce, a handheld RGB–F/T data collection system enabling diverse and fine-grained contact-rich manipulation demonstrations.\n\n2. We propose FMT, which handles inputs with asynchronous sampling rates through frequency-aware multimodal representation learning and cross-attention within a Transformer architecture, enabling robust policy learning for contact-rich manipulation.\n\n3. We demonstrate robust performance on diverse contact-rich manipulation tasks—including gear assembly, plug insertion, battery disassembly, and lid operations—consistently outperforming RGB-only baselines.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在接触丰富的操纵任务中同时利用高频力矩信息与RGB视觉实现鲁棒仿生策略\n2. 如何在异步采样率下融合多模态数据以提升策略学习的准确性与稳定性\n3. 如何减少对复杂场景感知的依赖并实现自然人类演示的高效数据收集\n\n【用了什么创新的方案】\nManipForce 提供可手持的 RGB–F/T 数据采集系统，结合高频力矩与 RGB 数据；FMT 使用频率感知的多模态表示对异步 RGB(30 Hz) 与 F/T(>200 Hz) 信号进行编码，采用双向交叉注意力在 Transformer diffusion 策略中进行跨模态融合，显著提升对接触力与微小几何的建模能力；并通过 3D ArUco 标定与工具重力补偿实现近距离接触的高精度演示对齐与采样。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors",
            "authors": "Qingzheng Cong,Steven Oh,Wen Fan,Shan Luo,Kaspar Althoefer,Dandan Zhang",
            "subjects": "Robotics (cs.RO)",
            "comment": "14 pages, 8 figures. Equal contribution: Qingzheng Cong, Steven Oh, Wen Fan. Corresponding author: Dandan Zhang (d.zhang17@imperial.this http URL). Additional resources atthis http URL",
            "pdf_link": "https://arxiv.org/pdf/2509.19037",
            "code": "http://stevenoh2003.github.io/TacEva/, http://ac.uk",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19037",
            "arxiv_html_link": "https://arxiv.org/html/2509.19037v1",
            "abstract": "Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because of the high spatial resolution they offer and their relatively low manufacturing costs. However, variations in their sensing mechanisms, structural dimension, and other parameters lead to significant performance disparities between existing VBTSs. This makes it challenging to optimize them for specific tasks, as both the initial choice and subsequent fine-tuning are hindered by the lack of standardized metrics. To address this issue, TacEva is introduced as a comprehensive evaluation framework for the quantitative analysis of VBTS performance. The framework defines a set of performance metrics that capture key characteristics in typical application scenarios. For each metric, a structured experimental pipeline is designed to ensure consistent and repeatable quantification. The framework is applied to multiple VBTSs with distinct sensing mechanisms, and the results demonstrate its ability to provide a thorough evaluation of each design and quantitative indicators for each performance dimension. This enables researchers to pre-select the most appropriate VBTS on a task by task basis, while also offering performance-guided insights into the optimization of VBTS design. A list of existing VBTS evaluation methods and additional evaluations can be found on our website:\nhttps://stevenoh2003.github.io/TacEva/.",
            "introduction": "Robots have yet to attain the level of manipulative dexterity exhibited by humans, a challenge rooted in the difficulty of accurately acquiring detailed contact information in physical environments [1]. Tactile sensing has therefore become indispensable for delicate and precise robotic manipulation in embodied intelligence systems [2]. A notable development in this domain has been the rise of vision-based tactile sensors (VBTSs) [3]. These sensors employ high-resolution cameras to capture detailed contact surface information, thereby integrating seamlessly with computer vision and image-based deep learning methods. Nevertheless, among VBTSs, we see a wide variety of architectures, structural dimensions, and fabrication techniques, depending on the exact nature of the application requirements [4].\n\nThe rapid development of VBTSs has created an urgent need for standardized performance evaluation. Selecting an appropriate VBTS for a specific task scenario remains challenging, as distinct sensor designs offer substantial variation in performance. A universal evaluation protocol would therefore facilitate fair comparison across sensor designs, support informed selection, and guide design optimization. However, a significant gap persists in the field: no standardized framework currently exists for VBTS evaluation, and the inconsistency of current metrics limits objective, reproducible, and comprehensive cross-sensor assessment. This challenge is further compounded by the inherently multi-modal nature of tactile sensing [5], which necessitates coordinated evaluation across multiple performance dimensions.\n\nThe application scenarios for VBTSs are inherently diverse. This makes it difficult to establish universal, broadly applicable evaluation metrics that remain meaningful across the field. Further complexity arises from the fact that VBTSs are commonly fabricated using silicone elastomers, introducing additional considerations related to soft-material mechanics, optics, and imaging. Owing to differences in their underlying sensing mechanisms, each VBTS design inevitably exhibits its own profile of strengths and weaknesses. As a result, certain positive performance characteristics will typically tend to be emphasized, while weaker ones may be under-reported. This is likely to complicate informed selection, making it more challenging for prospective users.\n\nTo address these challenges, we propose TacEva, a systematic evaluation framework that integrates performance quantification with a structured and reproducible assessment pipeline. TacEva is designed to provide consistency and comparability across VBTS designs, enabling practitioners to make evidence-based decisions while obtaining a holistic understanding of sensor performance (see Fig. 1). It also offers sensor developers clear guidance for targeted optimization during the design process. By defining and standardizing a comprehensive set of performance metrics and evaluation protocols, TacEva aims to provide a unified and transparent characterization of VBTS designs, thereby facilitating objective comparison, reliable selection, and more informed innovation for future sensor development.",
            "llm_summary": "【关注的是什么问题】\n1. 缺乏标准化评估框架导致VBTSs在不同设计间难以公平比较与任务级优化（≤40词）\n2. 多模态触觉传感的评估指标缺乏统一、可重复的量化流程（≤40词）\n3. 不同VBTS sensing机制、结构与材料差异造成性能特征分布不一致，难以选型与设计优化（≤40词）\n4. 缺少可跨传感器的全面综合性能指标与任务驱动的性能指示（≤40词）\n\n【用了什么创新的方案】\nTacEva 提供一个综合的性能评估框架，定义一套标准化、可重复的评测指标和实验流水线，覆盖典型应用场景的关键性能维度，并可对多种VBTS 进行横向比较与任务驱动的选型与优化指导。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion",
            "authors": "Shuai Liu,Meng Cheng Lau",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "11 pages, 5 figures, 1 table, Computational Science Graduate Project",
            "pdf_link": "https://arxiv.org/pdf/2509.19023",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19023",
            "arxiv_html_link": "https://arxiv.org/html/2509.19023v1",
            "abstract": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a two-stage reinforcement learning framework for humanoid walking that requires no motion capture data or elaborate reward shaping. In the first stage, a compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via Proximal Policy Optimization. This generates energy-efficient gait templates. In the second stage, those dynamically consistent trajectories guide a full-body policy trained with Soft Actor–Critic augmented by an adversarial discriminator, ensuring the student’s five-dimensional gait feature distribution matches the ROM’s demonstrations. Experiments at 1 m/s and 4 m/s show that ROM-GRL produces stable, symmetric gaits with substantially lower tracking error than a pure-reward baseline. By distilling lightweight ROM guidance into high-dimensional policies, ROM-GRL bridges the gap between reward-only and imitation-based locomotion methods, enabling versatile, naturalistic humanoid behaviors without any human demonstrations.",
            "introduction": "Achieving natural humanoid locomotion is a longstanding goal in both robotics and computer animation. From bipedal robots that can walk and run with human-like grace, to virtual characters that move realistically in games, the ability to synthesize lifelike walking gaits remains a persistent challenge. Traditional model-based control has produced impressive feats, but often requires painstaking design and does not always capture the natural motion nuances of human walking Kuindersma et al. (2016). In recent years, reinforcement learning (RL) has emerged as a promising data-driven paradigm for developing locomotion controllers Radosavovic et al. (2024). RL allows simulated humanoids to learn complex gait behaviors through trial-and-error, offering the potential to discover agile and robust walking strategies that would be difficult to manually design.\n\nReinforcement learning (RL) methods for natural locomotion generally fall into two main paradigms aimed at producing lifelike gait behaviors.\n\nA purely objective‑driven RL policy can discover stable walking gaits by optimizing energy and stability rewards. These reference-free methods train locomotion policies from scratch by optimizing carefully crafted reward functions, without any motion capture examples. The reward terms are designed to encourage physically plausible and human-like traits, such as forward speed with energy efficiency, maintaining center-of-mass stability and upright posture, periodic foot contact patterns, and symmetric gait cycles between left and right legs. By rewarding such objectives, controllers can spontaneously develop stable walking gaits that emerge naturally from learning. Notably, researchers have demonstrated that symmetric bipedal walking can arise solely from reward design and curriculum training, without any reference motions Yu et al. (2018). Indeed, recent work showed that an RL policy trained with biomechanically inspired rewards could produce natural walking behaviors purely through self-exploration Peng et al. (2025). The appeal of this approach is that it does not require any pre-recorded motions – the agent invents its own walking cycle. However, designing the reward function is notoriously difficult: the policy’s behavior is highly sensitive to the choice and weighting of reward terms, often requiring elaborate hand-tuning and expertise. Even with careful tuning, purely objective-driven policies may develop subtle artifacts or unnatural quirks since there is no direct template of “human” motion to imitate. In summary, while motion-free RL can yield impressively natural gaits under the right conditions, it faces challenges in reward engineering and consistency of motion style.\n\nAlternatively, imitation‑driven RL uses human mocap examples to ensure motion realism but at the cost of dataset dependency. To directly ensure natural movement quality, a dominant approach is to imitate example locomotion trajectories from motion capture data. In this paradigm, the RL agent is guided by reference motions of humans (or animals) and receives rewards for matching the reference pose and velocity at each time step, or uses adversarial critics to judge realism against a motion dataset Peng et al. (2018, 2021). DeepMimic pioneered this line of work by showing that standard RL algorithms can learn robust control policies capable of imitating a broad range of example motion clips from a motion capture library Peng et al. (2018). By combining motion imitation objectives with task goals, DeepMimic enabled physically simulated characters to reproduce dynamic skills (flips, spins, walks) with high fidelity to the mocap examples. Following this, numerous studies have pushed the state of the art in imitation-based locomotion. For instance, ASE (Adversarial Skill Embeddings) uses large unstructured motion datasets to train latent skill models via adversarial imitation, yielding a repertoire of reusable behaviors that look remarkably life-like Peng et al. (2022). Another example is CALM (Conditional Adversarial Latent Models), which learns a rich latent representation of human movement through imitation learning, capturing the complexity and diversity of human motion while allowing direct user control over the character’s style and direction Tessler et al. (2023). Most recently, the concept of Behavioral Foundation Models have emerged – here a policy is pre-trained on massive collections of motion data (in an unsupervised manner) to serve as a generalist locomotion model. These foundation models, once trained on unlabeled motion trajectories, can be prompted to perform new tasks in a zero-shot fashion, while retaining human-like gait qualities Tirinzoni et al. (2025a). Motion imitation approaches thus achieve state-of-the-art realism in simulated walking; policies closely mimic human kinematics and can produce motions nearly indistinguishable from motion capture. The downside, however, is their heavy reliance on curated motion data – one must have access to large datasets of reference gaits, and the learned skills are inevitably tied to the distribution of motions in those datasets.\n\nDespite these advances, achieving natural, human‑like locomotion without any motion capture data remains an open challenge. Purely objective‑driven methods often fail to reproduce the fluidity and subtle timing of human gait, while imitation‑based approaches are fundamentally constrained by the availability and diversity of mocap archives.\n\nTo bridge this gap, we introduce a reduced‑order model–guided reinforcement learning(ROM-GRL) framework. It leverages a simplified locomotion model as a stand‑in for motion capture, providing high‑level gait guidance to the RL agent without any human demonstrations. Our framework unfolds in two stages. In the first stage, we train a lightweight teacher model to generate efficient, dynamically consistent gait templates that capture the essence of natural walking. In the second stage, we distill these templates into a full-body controller by rewarding adherence to the teacher’s motion distribution, ensuring smooth, human-like locomotion. By separating high-level gait planning from detailed control, we achieve a single walking policy that is both robust and naturally fluid, all without relying on motion capture data or intricate reward design.\n\nIn summary, our approach marries the insights from model-based gait generation with the flexibility of reinforcement learning. By using a ROM to guide RL instead of direct motion capture, we maintain a purely physics-driven training regime while still inducing realistic movement patterns. The proposed framework demonstrates that natural humanoid locomotion can emerge without demonstrations, closing the gap between reward engineering and motion imitation. We validate that our ROM-guided RL method produces walking controllers that are stable across different speeds and exhibit natural gait symmetry and fluidity comparable to reward-based policies, suggesting a novel solution to produce life-like humanoid locomotion in the absence of motion data.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在没有运动捕捉数据的情况下实现自然、稳定的人形机器人行走\n2. 如何在强化学习中兼顾高层步态规划与全身控制的协同学习\n3. 如何避免繁琐的奖励设计而仍获得自然的步态与对称性\n4. 如何将简化的ROM释放为对高维策略的有效引导\n\n【用了什么创新的方案】\n两阶段ROM-GRL：先用4-DOF简化模型通过PPO生成高效、动态一致的步态模板；再用这些模板对全身策略进行蒸馏，使策略通过对ROM分布的遵循实现自然、对称的行走，并结合带对抗判别的Soft Actor–Critic以匹配ROM示范分布，且无任何人类示范数据。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
            "authors": "Dapeng Zhang,Jin Sun,Chenghui Hu,Xiaoyan Wu,Zhenlong Yuan,Rui Zhou,Fei Shen,Qingguo Zhou",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19012",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19012",
            "arxiv_html_link": "https://arxiv.org/html/2509.19012v1",
            "abstract": "The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.",
            "introduction": "Robotics has long been a prominent area of scientific research. Historically, robots primarily relied on pre-programmed instructions and engineered control policies to perform task decomposition and execution. These methods were commonly applied to simple, repetitive tasks, such as factory assembly lines and logistics sorting. In recent years, the rapid advancement of artificial intelligence has enabled researchers to exploit the feature extraction and trajectory prediction capabilities of deep learning across diverse modalities, including images, text, and point clouds. By integrating techniques such as perception, detection, tracking, and localization, researchers have decomposed robotic tasks into multiple stages to meet execution requirements, thereby advancing the development of embodied intelligence and autonomous driving. However, most of these robots still operate as isolated agents, designed for specific tasks and lacking effective interaction with humans and external environment.\n\nTo address these limitations, researchers have begun exploring the incorporation of large language models (LLMs) and vision language models (VLMs) to enable more accurate and flexible robotic manipulation. Modern robotic manipulation methods [1, 2] typically leverage vision language generative paradigms (e.g., autoregressive models [3, 4, 5, 6] or diffusion models [7]), combined with large-scale datasets [8] and advanced fine-tuning strategies. We refer to these as VLA foundation models, which have substantially improved the quality of robotic manipulations. Fine-grained action control over generated content provides users with greater flexibility, unlocking the practical potential of VLA for task execution.\n\nDespite their promise, reviews of pure VLA methods remain scarce. Existing surveys either focus on taxonomy over VLM foundational models or provide broad overviews of robotic manipulation as a whole. Firstly, VLA methods represent a nascent field in robotics, with no established methodological landscape or consensus taxonomy, making it challenging to systematically summarize these approaches. Secondly, current reviews either classify VLA approaches based on differences in foundational models or present a comprehensive analysis of robotic applications across the entire history of the field, often emphasizing traditional methods at the expense of emerging techniques. While these reviews offer valuable insights, they provide only cursory examinations of robotic models or concentrate primarily on foundational models, leaving a significant gap in the literature regarding pure VLA methods.\n\nIn this paper, we investigate VLA methods and associated resources, providing a focused and comprehensive review of existing approaches. Our goal is to present a clear taxonomy, systematically summarize VLA research, and elucidate the development trajectory of this rapidly evolving field. After a brief overview of LLMs and VLMs, we focus on the policy strategies of VLA models, highlighting the unique contributions and distinctive features of previous studies. We classify VLA approaches into 4 categories: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods, and provide a detailed analysis of their motivations, core strategies, and mechanisms. As shown in Fig. 2, we present a VLA skeleton of these methods.\nWe examine application domains, including robotic arms, quadruped robots, humanoids, and wheeled robots (autonomous vehicles), offering a comprehensive assessment of VLA deployment across diverse scenarios. Given the strong dependence of VLA models on datasets and simulation platforms, we provide a concise overview of these resources. Finally, based on the current VLA landscape, we identify key challenges and outline future research directions—including data limitations, inference speed, and safety—to accelerate the advancement of VLA models and generalizable robotics.\n\nThe overall structure of this survey is illustrated in Fig. 1. First, Section 2 provides an overview of the background for VLA research. Section 3 presents the existing VLA approaches in the robotics field. Section 4 introduces the datasets and benchmarks employed by VLA approaches. Sections 5 and 6 discuss simulation platforms and robotic hardware. Section 7 further discusses the challenges and future directions for VLA-based robotic methods. Finally, we summarize the paper and provide our perspective on future developments.\n\nIn summary, our contributions are as follows:\n\nWe present the well-structured taxonomy of pure VLA methods, classifying approaches based on their action-generation strategies. This facilitates understanding of existing methods and highlights core challenges in the field.\n\nThe survey emphasizes the defining characteristics and methodological innovations of each category and technique, providing a clear perspective on current approaches.\n\nWe provide a comprehensive overview of associated resources (datasets, benchmarks and simulation platforms) for training and evaluating VLA models.\n\nWe investigate the practical impact of VLA in robotics, identify key limitations of existing techniques, and propose potential avenues for further exploration.\n\n1. We present the well-structured taxonomy of pure VLA methods, classifying approaches based on their action-generation strategies. This facilitates understanding of existing methods and highlights core challenges in the field.\n\n2. The survey emphasizes the defining characteristics and methodological innovations of each category and technique, providing a clear perspective on current approaches.\n\n3. We provide a comprehensive overview of associated resources (datasets, benchmarks and simulation platforms) for training and evaluating VLA models.\n\n4. We investigate the practical impact of VLA in robotics, identify key limitations of existing techniques, and propose potential avenues for further exploration.",
            "llm_summary": "【关注的是什么问题】\n1. 如何对纯VLA方法进行清晰分类与系统综述，以揭示其核心动机与实现机制？（≤40词）\n2. VLA模型在机器人执行中的生成策略、数据/基准资源、仿真平台与应用域的整合与挑战是什么？（≤40词）\n3. 现有VLA方法的局限性、安全性与可扩展性如何影响通用机器人能力的提升？（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出基于动作生成策略的纯VLA方法分型（自回归、扩散、强化学习、混合、专用），并对各类方法的动机、核心策略与实现机制进行系统比较；梳理数据集、基准与仿真平台；并给出未来方向与挑战。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Category-Level Object Shape and Pose Estimation in Less Than a Millisecond",
            "authors": "Lorenzo Shaikewitz,Tim Nguyen,Luca Carlone",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18979",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18979",
            "arxiv_html_link": "https://arxiv.org/html/2509.18979v1",
            "abstract": "Object shape and pose estimation is a foundational robotics problem, supporting tasks from manipulation to scene understanding and navigation. We present a fast local solver for shape and pose estimation which requires only category-level object priors and admits an efficient certificate of global optimality. Given an RGB-D image of an object, we use a learned front-end to detect sparse, category-level semantic keypoints on the target object. We represent the target object’s unknown shape using a linear active shape model and pose a maximum a posteriori optimization problem to solve for position, orientation, and shape simultaneously. Expressed in unit quaternions, this problem admits first-order optimality conditions in the form of an eigenvalue problem with eigenvector nonlinearities. Our primary contribution is to solve this problem efficiently with self-consistent field iteration, which only requires computing a 4×44\\times 4 matrix and finding its minimum eigenvalue-vector pair at each iterate. Solving a linear system for the corresponding Lagrange multipliers gives a simple global optimality certificate. One iteration of our solver runs in about 100 microseconds, enabling fast outlier rejection. We test our method on synthetic data and a variety of real-world settings, including two public datasets and a drone tracking scenario. Code is released at https://github.com/MIT-SPARK/Fast-ShapeAndPose.",
            "introduction": "A diverse set of robotics applications benefits from object shape and pose estimation. Autonomous cars, for example, need to locate obstacles and other cars [1], while household manipulators need to locate objects to interact with [2].\nIn many of these applications the object shape is not known exactly but its category is available (e.g., from a semantic segmentation method). We consider this setting and derive a shape and pose estimator using category-level priors.\n\nThe work of Shi et al. [3] established a certifiably optimal approach for category-level shape and pose estimation using a semidefinite relaxation. We consider a similar problem setup but emphasize both speed and certifiability. A fast estimator allows fast reaction to new inputs, performance with limited compute, and comprehensive outlier rejection [4, 5]. Certifiability provides an a posteriori guarantee that the estimate returned is statistically optimal. When the certificate fails, the user can decide to trust the output, try a different initialization, or acquire a new batch of measurements.\n\nOur algorithm relies on the eigenvalue structure of the first-order optimality conditions written in the quaternion representation of rotations. It returns local solutions which are often globally optimal. To verify this, we introduce a fast global optimality certifier based on Lagrangian duality. Specifically, our contributions are:\n\nA fast local solver for category-level shape and pose estimation\nusing self-consistent field iteration [6].\n\nA fast a posteriori certificate of global optimality for our local solutions.\n\nExperimental evaluation of runtime and accuracy on synthetic data, a drone tracking scenario, and two large-scale datasets.\n\nThe remainder of the paper is organized as follows. We begin with a literature review (Section II) and quaternion preliminaries (Section III). Then, we give the problem formulation in Section IV and reformulate it with quaternions in Section V.\nTo solve the nonlinear eigenproblem, we use self-consistent field iteration for local solutions and SDP optimality conditions to certify global optimality in Section VI. In Section VII, we show our method is significantly faster than other local solvers and learned baselines.\n\n1. A fast local solver for category-level shape and pose estimation\nusing self-consistent field iteration [6].\n\n2. A fast a posteriori certificate of global optimality for our local solutions.\n\n3. Experimental evaluation of runtime and accuracy on synthetic data, a drone tracking scenario, and two large-scale datasets.",
            "llm_summary": "【关注的是什么问题】\n1. 在低计算资源下，基于类别级先验进行对象形状与姿态的快速估计\n2. 给定RGB-D，如何通过稀疏语义关键点实现高效、可证伪的全局最优性口径\n3. 如何在四元数表示下将非线性优化转化为可迭代求解并给出全局最优性证据\n\n【用了什么创新的方案】\n核心解决方案：提出基于自洽场迭代的快速局部求解器，用线性主动形状模型表示未知形状，构造最大后验优化以同时估计位置、姿态和形状；在四元数表示下，将一阶最优性条件转化为特征值问题，通过迭代求解一个4×44×4的矩阵并获取最小特征值-向量对；通过拉格朗日乘子求解线性系统，提供快速的全局最优性证书（可后验判断是否可信）。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation",
            "authors": "Minoo Dolatabadi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18954",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18954",
            "arxiv_html_link": "https://arxiv.org/html/2509.18954v1",
            "abstract": "LiDAR-based localization and SLAM often rely on iterative matching algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align sensor data with pre-existing maps or previous scans. However, ICP is prone to errors in featureless environments and dynamic scenes, leading to inaccurate pose estimation. Accurately predicting the uncertainty associated with ICP is crucial for robust state estimation but remains challenging, as existing approaches often rely on handcrafted models or simplified assumptions. Moreover, a few deep learning-based methods for localizability estimation either depend on a pre-built map—which may not always be available—or provide a binary classification of localizable versus non-localizable, which fails to properly model uncertainty.\nIn this work, we propose a data-driven framework that leverages deep learning to estimate the registration error covariance of ICP before matching, even in the absence of a reference map. By associating each LiDAR scan with a reliable 6-DoF error covariance estimate, our method enables seamless integration of ICP within Kalman filtering, enhancing localization accuracy and robustness. Extensive experiments on the KITTI dataset demonstrate the effectiveness of our approach, showing that it accurately predicts covariance and, when applied to localization using a pre-built map or SLAM, reduces localization errors and improves robustness.",
            "introduction": "In recent years, autonomous vehicles have become an integral component of intelligent transportation systems, driving continuous research to push the boundaries of their capabilities. A key requirement for autonomous driving is achieving precise self-localization at the centimeter level [1]. The Global Navigation Satellite System (GNSS) is a cost-effective and widely used method for vehicle localization. Although GNSS delivers reliable positioning in open-sky environments, its accuracy is significantly compromised in urban settings due to factors such as signal blockage, non-line-of-sight (NLOS) conditions, and multipath effects [2]. To address these challenges, vision-based approaches—particularly those leveraging LiDAR—have been proposed as alternative or complementary solutions [3]. In these systems, map matching techniques like the Iterative Closest Point (ICP) algorithm are frequently employed to align sensor data with pre-existing maps, thereby enhancing localization precision [4].\n\nHowever, vision-based localization methods—like other relative localization techniques—are susceptible to error accumulation, where minor errors can progressively lead to significant drift over time. This phenomenon is observed in both Simultaneous Localization and Mapping (SLAM) and pre-built map-based localization approaches, although it tends to be more pronounced in SLAM [5]. In the context of ICP, previous work has demonstrated that factors such as featureless environments (e.g., tunnels) and the presence of dynamic objects can adversely affect the matching accuracy [6].\n\nState estimation methods, such as Kalman filtering, are commonly employed to mitigate these problems. Yet, these techniques depend on an accurate error model (often represented in the simplest form by an error covariance matrix), which is challenging to determine for matching algorithms\n[7].\n\nTo address this limitation, several studies have proposed data-driven approaches to predict ICP error, either using classification-based methods [8] or by directly estimating the covariance [7].\n\nSpecifically, we introduce a data-driven framework that predicts the full six-degree-of-freedom (6-DoF) ICP registration covariance from a single LiDAR scan prior to correspondence search and ICP refinement. The output is a symmetric positive-definite (SPD) 6×66\\times 6 covariance on S​E​(3)SE(3) suitable for probabilistic fusion. An overview of the pipeline is shown in Fig. 1.\n\nContributions.\n\nPre-ICP, per-scan S​E​(3)SE(3) covariance. We predict the full 6×66\\times 6 registration covariance directly from a single LiDAR scan, capturing translation–rotation coupling before ICP is run. Per-scan training targets are obtained by estimating empirical covariances from Monte Carlo ICP under randomized initializations.\n\nKalman fusion and evaluation. Each scan is paired with a reliable 6-DoF covariance, used as the measurement noise in a standard Kalman filter. This improves pose accuracy over fixed or heuristic covariances on the evaluated sequences.\n\nPracticality. The model does not require a pre-built map at inference, supporting both SLAM and map-based operation.\n\n1. Pre-ICP, per-scan S​E​(3)SE(3) covariance. We predict the full 6×66\\times 6 registration covariance directly from a single LiDAR scan, capturing translation–rotation coupling before ICP is run. Per-scan training targets are obtained by estimating empirical covariances from Monte Carlo ICP under randomized initializations.\n\n2. Kalman fusion and evaluation. Each scan is paired with a reliable 6-DoF covariance, used as the measurement noise in a standard Kalman filter. This improves pose accuracy over fixed or heuristic covariances on the evaluated sequences.\n\n3. Practicality. The model does not require a pre-built map at inference, supporting both SLAM and map-based operation.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在 ICP 匹配前直接估计六自由度（6-DoF）注册协方差以提升 LiDAR 基于定位的鲁棒性\n2. 缺少地图时如何进行无地图依赖的协方差预测，并实现与卡尔曼滤波的无缝融合\n3. 面对 featureless 或动态环境，如何更准确地量化 ICP 的不确定性并减少定位误差\n4. 将深度学习预测的协方差用于先验状态估计，从而提升 SLAM/基于地图的定位的鲁棒性\n\n【用了什么创新的方案】\nPre-ICP 直接从单帧 LiDAR 预测对称正定的 6×6 协方差矩阵，捕捉平移-旋转耦合；以蒙特卡洛 ICP 在随机初始值下得到的经验协方差作为训练目标。将该协方差作为卡尔曼滤波的测量噪声，实现 ICP 前后的一体化鲁棒定位；方法在推断阶段不依赖预构建地图，支持 SLAM 与地图匹配场景。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations",
            "authors": "Hanqing Liu,Jiahuan Long,Junqi Wu,Jiacheng Hou,Huili Tang,Tingsong Jiang,Weien Zhou,Wen Yao",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18953",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18953",
            "arxiv_html_link": "https://arxiv.org/html/2509.18953v1",
            "abstract": "Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60%, with object transformations causing up to 97.8% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.",
            "introduction": "Vision-Language-Action (VLA) models represent a paradigm shift in robotic manipulation, integrating visual perception, language understanding, and action generation into unified end-to-end systems [1]. Recent deployments across manufacturing [2], healthcare [3], and service robotics [4, 5] demonstrate their transformative potential. However, in real-world deployments, VLA models inevitably face challenging physical variations, such as spatial transformations, illumination variations, and visual disruptions, which can dramatically alter robot behavior without being immediately detectable, posing significant safety risks. Therefore, it is crucial to investigate VLA robustness across various physical conditions systematically.\n\nExisting research has explored the robustness of VLA-based robotic systems through approaches like adversarial patches [6], which generate localized perturbations via gradient-based white-box attacks to achieve visual interference. However, these methods suffer from critical limitations: they violate physical plausibility constraints and fail to capture the rich spectrum of real-world physical variations. Moreover, their reliance on gradient access restricts applicability to black-box deployment scenarios. Based on these limitations, we aim to generate more diverse and realistic physical variations for comprehensively evaluating VLA robustness, while two key challenges must be addressed: (1) How to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility? (2) How to discover worst-case scenarios without prohibitive real-world data collection costs efficiently?\n\nTo address these challenges, we propose Eva-VLA, a unified framework for evaluating vision-language-action models’ robustness. Our key innovation lies in transforming discrete physical variations into continuous optimization problems. First, as shown in Fig. 1, we decompose real-world variations into three distinct domains: object 3D transformations parameterized with rotation angles(α\\alpha, β\\beta, γ\\gamma), illumination variations defined by point light parameters including position(xx, yy), radius(σ\\sigma), intensity(II), and adversarial patch placement specified by (Δ​x\\Delta x, Δ​y\\Delta y). This parameterization enables systematic exploration of the variation space while maintaining physical plausibility through explicit constraints. Second, to overcome the black-box nature of VLA models and non-differentiable simulation environments, we employ Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [7], a gradient-free optimization algorithm, to efficiently discover worst-case scenarios by iteratively optimizing physical variations parameters. This approach enables comprehensive vulnerability assessment without requiring model gradients or expensive real-world data collection.\n\nOur main contributions are as follows: ❶ To the best of our knowledge, we are the first to decompose real-world physical variations into three key domains—object 3D transformation, illumination variations, and adversarial patches—enabling a comprehensive evaluation of VLA robustness under these physical variations. ❷ We propose Eva-VLA, a novel framework that transforms discrete physical variations into continuous parameter optimization. By leveraging a simulator environment that allows us to reset to the same conditions, we ensure the repeatability and reliability of the evaluation process, which enables efficient exploration of worst-case scenarios without the need for expensive real-world data collection. ❸ Through extensive experiments on state-of-the-art model OpenVLA across multiple benchmarks, we expose significant fragility in current VLA systems, with failure rates exceeding 60% across all variation categories, with object transformations causing up to 97.8% failure in long-horizon tasks. These findings provide crucial insights for developing more robust VLA architectures and underscore the urgent need for improved robustness training methodologies.",
            "llm_summary": "【关注的是什么问题】\n1. 如何系统评估 Vision-Language-Action (VLA) 模型在真实世界物理变异下的鲁棒性（robustness）  \n2. 如何在不依赖大量真实数据的情况下发现最坏场景（worst-case scenarios）并保证评估可重复性  \n3. 如何将离散物理变异转化为连续优化问题以便可控探索  \n4. 如何覆盖三大变异域（object 3D transformation、illumination、adversarial patches）的全面评估  \n【用了什么创新的方案】\n核心方案：提出 Eva-VLA 框架，将离散物理变异转化为连续参数优化问题；将变异分解为 object 3D transformations、illumination variations、adversarial patches 三大域，采用 CMA-ES（协方差矩阵适应进化策略）进行黑盒优化以高效发现 worst-case 场景，并在可复现的仿真环境中重置到相同条件进行评估，从而在无需梯度信息或大量真实数据的情况下系统评估 VLA 的鲁棒性。通过对 OpenVLA 在多基准上的大规模实验，揭示了超过 60% 的失败率，且对象变换在长时任务中可达 97.8% 的失败率，强调了现实部署中的鲁棒性缺口与改进必要性。  \n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands",
            "authors": "Yanyuan Qiao,Kieran Gilday,Yutong Xie,Josie Hughes",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18937",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18937",
            "arxiv_html_link": "https://arxiv.org/html/2509.18937v1",
            "abstract": "Designing robotic hand morphologies for diverse manipulation tasks requires balancing dexterity, manufacturability, and task-specific functionality. While open-source frameworks and parametric tools support reproducible design, they still rely on expert heuristics and manual tuning. Automated methods using optimization are often compute-intensive, simulation-dependent, and rarely target dexterous hands. Large language models (LLMs), with their broad knowledge of human-object interactions and strong generative capabilities, offer a promising alternative for zero-shot design reasoning. In this paper, we present Lang2Morph, a language-driven pipeline for robotic hand design. It uses LLMs to translate natural-language task descriptions into symbolic structures and OPH-compatible parameters, enabling 3D-printable task-specific morphologies. The pipeline consists of: (i) Morphology Design, which maps tasks into semantic tags, structural grammars, and OPH-compatible parameters; and (ii) Selection and Refinement, which evaluates design candidates based on semantic alignment and size compatibility, and optionally applies LLM-guided refinement when needed. We evaluate Lang2Morph across varied tasks, and results show that our approach can generate diverse, task-relevant morphologies. To our knowledge, this is the first attempt to develop an LLM-based framework for task-conditioned robotic hand design.",
            "introduction": "Designing the morphology and structure of a robotic hand for a specific application is a fundamental yet long-standing challenge in robotics.\nUnlike the design of general-purpose manipulators, for example anthropomorphic robotic hands, task-specific robotic hands must balance dexterity and generality with optimization for a specific task, whilst remaining manufacturable [1, 2].\nThe design of dexterous, yet tasks-specific hands requires a full understanding of the task, and also fabrication constraints. For example, whilst increasing finger count or joint complexity improves capability but complicates fabrication and control.\nAs a result, tasks specific hand design has typically relied on expert-driven heuristics and iterative prototyping, which are time-consuming and hard to scale.\n\nThe development of parametric hand designs can provide a design space for task-specific hand morphology design.\nOpen-source hardware projects such as the Yale OpenHand Project [3, 4] have released reproducible underactuated hand designs, lowering the barrier for building functional prototypes. Parametric frameworks like the Open Parametric Hand (OPH) [5] further expose a structured design space with tunable parameters, enabling systematic exploration of hand morphologies.\nHowever, both approaches still depend on human designers to interpret tasks, identify likely grasp types, and manually adjust parameters. Without automated mapping from task requirements to design instantiations, scaling to diverse, task-specific morphologies remains difficult.\n\nIn parallel, the robotics community has explored automated robot morphology optimization and generation through evolutionary optimization, grammar-based synthesis, and differentiable pipelines [6, 7, 8, 9].\nThese works demonstrate that morphology can indeed be generated algorithmically, but they are often computationally expensive, require carefully crafted objective functions, and are typically coupled to physics-based simulators and rely on the curation of algorithmic representation of a fitness function or objective.\nMoreover, they have rarely targeted dexterous hand design, where functional requirements such as fingertip precision, lateral pinching, or stabilizing support are used to determine the resulting morphology.\nThus, these approaches lacks generality and relies on expert identification of key functional metrics or objective functions.\n\nRecently, large language models (LLMs) have been explored in robotics, with applications in navigation and manipulation where language is grounded into action policies [10, 11].\nWhilst these efforts focus mainly on control and planning,\nLLMs are not only capable of semantic reasoning but also encode broad background knowledge across biology, engineering, and everyday practice, which traditional optimization-based methods typically lack.\nThis combination makes them promising candidates for reasoning about form and functionality in design.\nSome recent works have adapted LLMs for computer-aided design (CAD) [12, 13], translating text into parametric part models.\nYet these methods remain restricted to geometry generation and do not address morphology design, where task semantics such as lateral pinching or stabilizing support directly dictate structural choices.\nTo the best of our knowledge, the use of LLMs for robotic hand morphology design remains unexplored, motivating our study.\n\nIn this paper, we propose Lang2Morph, a language-driven pipeline for robotic hand morphology generation. Our key insight is that LLMs are well suited to reasoning about task semantics and mapping them into symbolic and geometric design representations. Unlike prior methods that rely on expert-driven heuristics or costly physics-based simulation, Lang2Morph leverages LLM reasoning and semantic feedback to achieve scalable, task-conditioned morphology generation, as illustrated in Fig. 1, where a user-provided instruction is mapped into a design rationale and a CAD-ready morphology.\n\nLang2Morph builds upon the Open Parametric Hand (OPH) framework [5], which defines a structured and fabricable design space for robotic hands. OPH supports single-piece 3D printing, allowing generated designs to be directly manufactured without additional assembly or simulation. This enables our method to output physically realizable morphologies from natural language instructions.\n\nSpecifically, the pipeline comprises two major modules: (i) LLM-powered Morphology Design, which performs dual-level task analysis (semantic and structural), followed by geometry parameterization and constraint-aware validation to produce OPH-compatible parameters; and (ii) LLM-Guided Selection and Refinement, which ranks rendered variants based on semantic alignment and size compatibility, and optionally provides design refinements through feedback.\nTogether, these components form an end-to-end pipeline that generates fabricable, task-adaptive morphologies directly from natural language task description.\n\nOur main contributions are as follows:\n\nWe present Lang2Morph, a novel framework that generates robotic hand morphologies from natural-language instructions using large language models.\n\nWe design a two-stage pipeline that combines symbolic grammar generation, geometric parameterization, and semantic feedback for task-adaptive hand design.\n\nWe explore the use of LLMs for early-stage robot morphology generation, offering a flexible alternative to expert tuning.\n\nWe evaluate our method on a range of manipulation tasks, demonstrating improved design validity, diversity, and semantic alignment.\n\n1. We present Lang2Morph, a novel framework that generates robotic hand morphologies from natural-language instructions using large language models.\n\n2. We design a two-stage pipeline that combines symbolic grammar generation, geometric parameterization, and semantic feedback for task-adaptive hand design.\n\n3. We explore the use of LLMs for early-stage robot morphology generation, offering a flexible alternative to expert tuning.\n\n4. We evaluate our method on a range of manipulation tasks, demonstrating improved design validity, diversity, and semantic alignment.",
            "llm_summary": "【关注的是什么问题】\n1. 如何从自然语言任务描述自动生成可直接制造的任务适应性机器人手形态\n2. 在确保可制造性与任务相关性的同时，减少对人工经验启发式设计的依赖\n3. 面向 dexterous 手掌的任务特定形态设计的高效自动化路径\n4. 将语言模型用于高层语义分析到几何参数化、结构约束的端到端设计\n\n【用了什么创新的方案】\nLang2Morph 使用大语言模型进行双层任务分析（语义与结构），将自然语言指令映射到 OPH 兼容的参数化几何；结合符号语法生成与几何参数化，通过语义对齐和尺寸兼容性评估进行候选选择与 refinement；实现从文本到可直接 3D 打印的任务特定形态的端到端流程，且无需仿真驱动。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation",
            "authors": "Masato Kobayashi,Thanpimon Buamanee",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18865",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18865",
            "arxiv_html_link": "https://arxiv.org/html/2509.18865v1",
            "abstract": "We propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral control-based imitation learning to handle more than one task within a single model. Conventional bilateral control methods exploit joint angle, velocity, torque, and vision for precise manipulation but require task-specific models, limiting their generality.\nBi-VLA overcomes this limitation by utilizing robot joint angle, velocity, and torque data from leader-follower bilateral control with visual features and natural language instructions through SigLIP and FiLM-based fusion.\nWe validated Bi-VLA on two task types: one requiring supplementary language cues and another distinguishable solely by vision. Real-robot experiments showed that Bi-VLA successfully interprets vision-language combinations and improves task success rates compared to conventional bilateral control-based imitation learning.\nOur Bi-VLA addresses the single-task limitation of prior bilateral approaches and provides empirical evidence that combining vision and language significantly enhances versatility.\nExperimental results validate the effectiveness of Bi-VLA in real-world tasks.\nFor additional material, please visit the website: https://mertcookimg.github.io/bi-vla/",
            "introduction": "Robotic manipulation is increasingly important in human-centered applications such as cooking, eldercare, and interactive service robots [1, 2, 3, 4].\nUnlike traditional industrial robots that excel at repetitive and pre-programmed routines, service and collaborative robots must adapt to dynamic environments and interact with objects of diverse shapes, sizes, and material properties [5, 6]. Achieving such adaptability requires learning frameworks capable of acquiring human-like manipulation strategies [7].\n\nImitation learning (IL) has emerged as a promising approach for transferring human manipulation skills directly to robots [8].\nLeader-follower teleoperation has become a common pipeline for collecting demonstrations. For example, ALOHA and Mobile ALOHA use position-based unilateral control to gather diverse datasets that enable a wide range of manipulation [9, 10].\nAlthough effective for kinematics-driven tasks, such unilateral control omits force feedback, which limits robustness in contact-rich interactions.\n\nBilateral control-based imitation learning addresses these limitations by exchanging both position and force information between the demonstrator and robot [11, 12].\nBilateral control allows demonstrators to feel contact forces directly, yielding richer demonstrations and improving generalization across objects with different hardness and weights.\n\nBuilding on this foundation, recent methods have explored combining bilateral control with modern architectures.\nBilateral Control-Based Imitation Learning via Action Chunking with Transformers (Bi-ACT) [13] integrates bilateral control with visual observations via Transformers, yielding improved manipulation accuracy.\nHowever, it remains restricted to single-task settings, limiting its practicality in dynamic environments where multiple tasks must be handled seamlessly.\nIn parallel, Bilateral Control-Based Imitation Learning via Natural Language and Action Chunking with Transformers (Bi-LAT) [14] introduced natural language instructions into bilateral control-based imitation learning, demonstrating effective force modulation in manipulation.\nWhile this work highlights the promise of language integration, it focuses on regulating applied force and does not address the challenge of enabling a single model to adapt to multiple task contexts.\n\nIn this paper, we propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that unifies robot joint angle, velocity, and torque data from bilateral control with visual and language features, as shown in Fig. 1.\nBy leveraging SigLIP-based text embeddings [15] and FiLM-based EfficientNet feature fusion [16], Bi-VLA learns a shared representation of vision and language aligned with robot state information. Unlike prior bilateral frameworks, Bi-VLA is designed to handle multiple tasks within a single model, enabling flexible task switching without retraining or manual model selection.\n\nThe main contributions of this paper are summarized as follows:\n\nWe propose Bi-VLA, the bilateral control-based imitation learning framework that fuses vision and language features into a unified representation.\n\nWe demonstrate that Bi-VLA enables a single model to perform multiple tasks, overcoming the single-task limitation inherent in prior bilateral control-based imitation learning approaches.\n\nWe validate Bi-VLA through real-robot experiments on two distinct tasks, showing improved performance and adaptability compared to conventional methods.\n\n1. We propose Bi-VLA, the bilateral control-based imitation learning framework that fuses vision and language features into a unified representation.\n\n2. We demonstrate that Bi-VLA enables a single model to perform multiple tasks, overcoming the single-task limitation inherent in prior bilateral control-based imitation learning approaches.\n\n3. We validate Bi-VLA through real-robot experiments on two distinct tasks, showing improved performance and adaptability compared to conventional methods.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在单一模型中实现多任务的双边控制模仿学习与任务切换（≤40词）\n2. 如何将视觉与语言信息与机器人关节角度、速度、力矩数据融合以提升操控鲁棒性（≤40词）\n3. 如何在现实机器人环境中验证视觉-语言融合对多任务泛化的提升（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：在双边控制基础上，结合 SigLIP 文本嵌入和 FiLM 机制对 EfficientNet 提取的视觉特征进行融合，形成一个统一的视觉-语言-状态表示；通过该表示实现单模型处理多任务、并通过任务切换无需重新训练；在两类任务与真实机器人上验证性能提升。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation",
            "authors": "Suzannah Wistreich,Baiyu Shi,Stephen Tian,Samuel Clarke,Michael Nath,Chengyi Xu,Zhenan Bao,Jiajun Wu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "Accepted to CoRL 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18830",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18830",
            "arxiv_html_link": "https://arxiv.org/html/2509.18830v1",
            "abstract": "Human skin provides a rich tactile sensing stream, localizing intentional and unintentional contact events over a large and contoured region.\nReplicating these tactile sensing capabilities for dexterous robotic manipulation systems remains a longstanding challenge.\nIn this work, we take a step towards this goal by introducing DexSkin. DexSkin is a soft, conformable capacitive electronic skin that enables sensitive, localized, and calibratable tactile sensing, and can be tailored to varying geometries.\nWe demonstrate its efficacy for learning downstream robotic manipulation by sensorizing a pair of parallel jaw gripper fingers, providing tactile coverage across almost the entire finger surfaces.\nWe empirically evaluate DexSkin’s capabilities in learning challenging manipulation tasks that require sensing coverage across the entire surface of the fingers, such as reorienting objects in hand and wrapping elastic bands around boxes, in a learning-from-demonstration framework. We then show that, critically for data-driven approaches, DexSkin can be calibrated to enable model transfer across sensor instances, and demonstrate its applicability to online reinforcement learning on real robots.\nOur results highlight DexSkin’s suitability and practicality for learning real-world, contact-rich manipulation.\nPlease see our project webpage for videos and visualizations: https://dex-skin.github.io/.",
            "introduction": "Tactile feedback is essential for robust and dexterous manipulation in natural and artificial systems. In humans, mechanoreceptors within the skin provide a rich sensory stream that guides tasks ranging from handling delicate objects to using tools with force [1]. This tactile feedback enables more precise reactive control than can be achieved with human and proprioceptive feedback alone [2].\n\nEmulating this tactile sensitivity in robotic systems has long been a challenge, starting from sensing hardware. Research efforts typically focus on rigid or partially flexible sensors that offer low spatial coverage, limited adaptability, and poor conformability to complex surfaces. In contrast, everyday tasks such as rotating a key in-hand or picking up delicate berries require sensing coverage in multiple contact regions.\nSimilarly, wrapping an elastic band around an object requires a sensor to detect, distinguish, and localize dynamic contact events on all surfaces of a human hand or end effector accurately, applying the appropriate force without allowing the band to slip.\n\nLearning-based approaches offer a general way to harness tactile sensing information. However, data-driven systems pose requirements beyond sensing coverage. Seemingly small distribution shifts in sensor readings from wear or replacement can render previously trained models unusable, thus, output signals must be replicable across sensor hardware instances. Additionally, for real-world online learning, sensors must remain consistent, durable, and precise under repeated stresses during trial-and-error interaction, and output interpretable signals amenable to reward or cost specification.\n\nTo address these challenges, we introduce a novel soft tactile skin named DexSkin that is particularly suitable for robotic learning applications. DexSkin can be conformably integrated onto robotic end-effectors with unparalleled spatial coverage.\nIt is based on a capacitive mechanism, and features high sensitivity and robustness under repeated interactions.\nBecause each of the dozens of taxels on the skin are individually addressable, DexSkin can localize and characterize simultaneous contacts from distinct regions.\nIt can also be calibrated to provide consistent readings across distinct sensor units, enabling re-use of learned networks. At the same time, it can withstand deformations encountered during typical dexterous tasks such as pinching, twisting, and bending.\n\nIn this work, we introduce the DexSkin framework and its fabrication, as well as a representative integration with a soft cylindrical robotic fingertip that sensorizes the distal dome of the finger and 294∘ of the circumference. Then, we evaluate DexSkin’s applicability to robot learning. First, we test whether DexSkin’s coverage and tailorability expands the range of learnable manipulation tasks. Then, we evaluate its calibration performance, which are critical for working with and transferring learned tactile models. Finally, we demonstrate its suitability for online robotic learning settings by performing real-world reinforcement learning for a delicate object picking task. The results highlight DexSkin’s robust applicability to a wide range of robotic tasks and morphologies, and its particular practicality for robot learning researchers and practitioners.",
            "llm_summary": "【关注的是什么问题】\n1. 需要大面积、可 conform 的触觉传感覆盖以实现高鲁棒的接触丰富操控\n2. 如何在多传感单元上获得高灵敏、可标定、跨传感器的一致性读取\n3. 在 learns-from-demonstration 与 在线强化学习 场景中实现对不同几何和材料的可转移性\n4. 传感硬件对姿态、变形的耐久性及对数据分布偏移的鲁棒性\n\n【用了什么创新的方案】\nDexSkin 是一种软性、可 conform 的电容式电子皮肤，具有大量独立可寻址的触觉单元（taxels），可覆盖手指表面且对变形鲁棒；通过逐个触觉单元标定实现跨传感器的一致性，使已学习的模型可在不同传感器实例间转移；支持对多点同时接触的定位与表面广覆盖的感知，适用于从演示学习到在线强化学习的任务。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations",
            "authors": "Lukas Zanger,Bastian Lampe,Lennart Reiher,Lutz Eckstein",
            "subjects": "Robotics (cs.RO); Multiagent Systems (cs.MA); Software Engineering (cs.SE)",
            "comment": "7 pages, 2 figures, 2 tables; Accepted to be published as part of the 2025 IEEE International Conference on Intelligent Transportation Systems (ITSC 2025), Gold Coast, Australia, November 18-21, 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18793",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18793",
            "arxiv_html_link": "https://arxiv.org/html/2509.18793v1",
            "abstract": "Vehicles are becoming increasingly automated and interconnected, enabling the formation of cooperative intelligent transport systems (C-ITS) and the use of offboard services. As a result, cloud-native techniques, such as microservices and container orchestration, play an increasingly important role in their operation.\nHowever, orchestrating applications in a large-scale C-ITS poses unique challenges due to the dynamic nature of the environment and the need for efficient resource utilization.\nIn this paper, we present a demand-driven application management approach that leverages cloud-native techniques – specifically Kubernetes – to address these challenges. Taking into account the demands originating from different entities within the C-ITS, the approach enables the automation of processes, such as deployment, reconfiguration, update, upgrade, and scaling of microservices.\nExecuting these processes on demand can, for example, reduce computing resource consumption and network traffic.\nA demand may include a request for provisioning an external supporting service, such as a collective environment model.\nThe approach handles changing and new demands by dynamically reconciling them through our proposed application management framework built on Kubernetes and the Robot Operating System (ROS 2).\nWe demonstrate the operation of our framework in the C-ITS use case of collective environment perception and make the source code of the prototypical framework publicly available at https://github.com/ika-rwth-aachen/application_manager.",
            "introduction": "In future cooperative intelligent transport systems (C-ITS), various entities, such as vehicles equipped with driving automation systems, sensor-equipped roadside infrastructure units, edge/cloud servers, and control centers, will be connected, exchange data, and may offer computational resources.\nThese advancements enable new applications – such as collective environment perception, cooperative decision-making, computation offloading, and intelligent traffic management – that can contribute to improved comfort and safety for road users [1], [2], [3].\nNot only cloud and edge servers but also vehicles and roadside units can be part of a distributed computing system.\nHowever, these applications may also introduce complexity that is difficult to manage. The dynamic nature of C-ITS, the presence of resource-constrained entities, and the strict requirements for safety and security pose unique challenges.\n\nCloud-native techniques provide a promising foundation for the development and operation of scalable applications in dynamic environments. Such techniques involve paradigms like containerization, microservice architectures, and container orchestration. They enable loosely coupled systems which are manageable and resilient [4]. Said techniques and paradigms have the potential to contribute to the advancement of C-ITS.\n\nKubernetes has evolved as the de facto standard for orchestrating containerized applications in distributed systems. It is open-source and widely adopted by software companies worldwide [5].\nNevertheless, Kubernetes lacks methods that are domain-specific, e.g., to C-ITS, considering that specific tasks like the deployment of required applications are only needed at certain times or may depend on the specific content of data exchanged in the C-ITS.\nWe have developed the approach RobotKube [6] to extend the regular capabilities of Kubernetes. RobotKube comprises software components designed to automate the identification of requirements and the formulation of specific Kubernetes workloads.\nThese components include the event detector and the application manager.\n\nIn this paper, we propose a demand-driven application management approach and present the methodology behind the application manager as part of an application management framework.\nThis methodology integrates seamlessly into the RobotKube architecture and complements parts of RobotKube which were not detailed yet.\nThe application management framework – comprising the application manager and a set of custom operators – addresses the orchestration challenges in C-ITS through a demand-driven approach.\nIn this context, applications are deployed, reconfigured, scaled, and updated based on the current demands of C-ITS entities.\n\nWith our work, we make the following main contributions:\n\nPresentation of the methodology for demand-driven application management allowing to deploy, reconfigure, update, upgrade, and scale applications based on demands of entities in a C-ITS.\n\nPrototypical implementation of the application manager and the custom operators in an application management framework based on Kubernetes and ROS 2.\n\nDemonstration and evaluation of the capabilities of the application management framework in the complex C-ITS use case of collective environment perception involving various C-ITS entities.\n\nOpen source code publication of the application management framework and the demonstration use case allowing for reproducibility and extensibility.\n\nComplementation of RobotKube [6] by providing the concrete methodology of the application manager.\n\n1. Presentation of the methodology for demand-driven application management allowing to deploy, reconfigure, update, upgrade, and scale applications based on demands of entities in a C-ITS.\n\n2. Prototypical implementation of the application manager and the custom operators in an application management framework based on Kubernetes and ROS 2.\n\n3. Demonstration and evaluation of the capabilities of the application management framework in the complex C-ITS use case of collective environment perception involving various C-ITS entities.\n\n4. Open source code publication of the application management framework and the demonstration use case allowing for reproducibility and extensibility.\n\n5. Complementation of RobotKube [6] by providing the concrete methodology of the application manager.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在动态且资源受限的C-ITS环境中对应用进行按需驱动的管理与部署（≤40词）\n2. 如何将云原生技术（Kubernetes、ROS 2、微服务）整合用于C-ITS中的应用编排与再配置（≤40词）\n3. 如何通过需求感知实现应用的部署、重配置、更新、升级与扩缩以优化资源与网络开销（≤40词）\n【用了什么创新的方案】\n核心解决方案：提出基于需求驱动的应用管理框架，结合 Kubernetes 与 ROS 2，开发应用管理器与自定义 Operator，用事件检测器识别需求并动态编排、部署、重配置、升级和扩缩，支持对外部服务等需求的 provisioning；框架作为 RobotKube 的扩展，公开源码以便复现与扩展。\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Human-Interpretable Uncertainty Explanations for Point Cloud Registration",
            "authors": "Johannes A. Gaus,Loris Schneider,Yitian Shi,Jongseok Lee,Rania Rayyes,Rudolph Triebel",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18786",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18786",
            "arxiv_html_link": "https://arxiv.org/html/2509.18786v1",
            "abstract": "In this paper, we address the point cloud registration problem, where well-known methods like ICP fail under uncertainty arising from sensor noise, pose‐estimation errors, and partial overlap due to occlusion. We develop a novel approach, Gaussian Process Concept Attribution (GP-CA), which not only quantifies registration uncertainty but also explains it by attributing uncertainty to well-known sources of errors in registration problems. Our approach leverages active learning to discover new uncertainty sources in the wild by querying informative instances. We validate GP-CA on three publicly available datasets and in our real-world robot experiment. Extensive ablations substantiate our design choices. Our approach outperforms other state-of-the-art methods in terms of runtime, high sample-efficiency with active learning, and high accuracy. Our real-world experiment clearly demonstrates its applicability. Our video also demonstrates that GP-CA enables effective failure-recovery behaviors, yielding more robust robotic perception.",
            "introduction": "Point cloud registration refers to the problem of estimating a relative transformation between two sets of 3D points [1]. This problem is essential in many robotic perception tasks, such as simultaneous localization and mapping (SLAM) [2], 3D reconstruction [3], and 6-DoF object pose estimation [4], to name a few. A widely used approach for point cloud registration is Iterative Closest Point (ICP) [5].\nHowever, in ICP, well-known causes\nof uncertainty are\nsensor noise, poor initialization of the optimization process of the relative transformation between the two point clouds, and insufficient overlap between the two point clouds (e.g., due to occlusions), which can make ICP inaccurate or unreliable in practice  [6]. Hence, several researchers attempt to improve the robustness of the registration process [7].\n\nAmong others, several probabilistic approaches were proposed to quantify uncertainty in point cloud registration [6, 8, 9, 10, 11]. The underlying idea is that uncertainty estimates provide insight into the reliability of the obtained results, enabling the identification and rejection of unreliable registrations. Moreover, information about uncertainty is often used in many downstream tasks, including sensor fusion, state estimation, and 3D reconstruction [6]. To this end, a variety of tools have been developed to quantify uncertainty in point cloud registration, ranging from closed-form Gaussian solvers [11] to particle-based methods [6].\n\nWhile quantifying uncertainty helps to improve the registration, in practice, the magnitude of uncertainty is rarely sufficient in practical robotic perception tasks, as it does not reveal why registration failed or how to recover from it. Distinct failure causes — such as sensor noise, poor initialization, or occlusion [6] — require different actions to recover from failure.\nIn this work, we propose a novel approach, Gaussian Process Concept Attribution (GP-CA). A major difference to existing work is that, our approach not only quantifies uncertainty, but also explains uncertainty in a human-interpretable manner. Our GP-CA integrates active learning, learned point-cloud representations, and a Gaussian Process classifier to provide concept-level uncertainty attribution. The active learning allows fast adaptation to new uncertainty sources.\nIn preliminary work [12], the explainability method SHAP was used to analyze the influence of different uncertainty sources on ICP-based registration. However, it cannot identify which uncertainty source is present in a registration and depends on manual control over uncertainty sources and heavy computation, limiting applicability and hindering its feasibility for robotic decision-making.\n\nGP-CA attributes uncertainty to semantically meaningful concepts that can be used later to enable robots to select targeted recovery actions. An example is depicted in Fig. 1, where the robot is tasked to perform a 6D pose estimation of objects using the ICP algorithm. Initially, an object of interest (the bottle) is occluded by another object, which causes the ICP algorithm to fail. Using our method, the occlusion is identified as the cause of failure, thereby enabling targeted recovery actions (e.g., changing the viewpoint).\nHence, our main idea of explaining uncertainty in point cloud registration can improve the robustness of the perception tasks.\n\nWhen registration fails or reports high uncertainty, GP-CA embeds the ICP-aligned source point cloud using a learned representation. Then, a multi-class Gaussian Process classifier (GPC) maps these representations to confidence scores over different concepts. In this way, we attribute the uncertainty to specific concepts, thereby identifying its source. The concepts are predefined by sets of examples. Still, a robot may encounter unknown sources of uncertainty during its operations. Therefore, GP-CA is equipped with an active learning mechanism [13, 14], which can learn a new concept by querying the user for the label and choosing the most informative data to learn from. Across experiments on the LINEMOD [15], the YCB [16], and the Coffee Cup [17] dataset and a custom real-world RGB-D dataset from our laboratory, we validate GP-CA and provide extensive ablation studies. We further propose how GP-CA can enable a more robust robotic perception by executing a set of failure recovery actions associated with uncertainty explanations.\n\nIn summary, our main contributions are as follows.\n\nWe propose a novel approach, GP-CA, which enables human-interpretable uncertainty explanations in point cloud registration. To the best of our knowledge, this is the first work to show how explainability can advance point cloud registration for real-world applications\n\nWe extend our approach with active learning to adapt and integrate new concepts.\n\nWe validate the GP-CA design through an extensive ablation study\n\nWe demonstrate clearly the runtime-efficiency, the high accuracy, and the sample-efficiency of our method, with SOTA baseline comparison across four datasets.\n\nWe propose how GP-CA can be employed for recovery actions in a real-world robot setting.\n\n1. We propose a novel approach, GP-CA, which enables human-interpretable uncertainty explanations in point cloud registration. To the best of our knowledge, this is the first work to show how explainability can advance point cloud registration for real-world applications\n\n2. We extend our approach with active learning to adapt and integrate new concepts.\n\n3. We validate the GP-CA design through an extensive ablation study\n\n4. We demonstrate clearly the runtime-efficiency, the high accuracy, and the sample-efficiency of our method, with SOTA baseline comparison across four datasets.\n\n5. We propose how GP-CA can be employed for recovery actions in a real-world robot setting.",
            "llm_summary": "【关注的是什么问题】\n1. 点云配准中的不确定性量化与原因解释（≤40词）\n2. 如何将不确定性解释转化为人类可理解的概念级因果归因（≤40词）\n3. 在实际机器人场景中实现快速、鲁棒的失败恢复策略（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出高斯过程概念归因GP-CA，结合点云嵌入、GPC分类器实现对不确定性的概念级归因，并通过主动学习发现新不确定源；可用于在ICP失败时给出针对性的恢复操作，提升运行时鲁棒性与效率。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models",
            "authors": "Shijia Ge,Yinxin Zhang,Shuzhao Xie,Weixiang Zhang,Mingcai Zhou,Zhi Wang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "submitted to AAAI 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.18778",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18778",
            "arxiv_html_link": "https://arxiv.org/html/2509.18778v1",
            "abstract": "Visual imitation learning frameworks allow robots to learn manipulation skills from expert demonstrations. While existing approaches mainly focus on policy design, they often neglect the structure and capacity of visual encoders—limiting spatial understanding and generalization. Inspired by biological vision systems, which rely on both visual and proprioceptive cues for robust control, we propose VGGT-DP, a visuomotor policy framework that integrates geometric priors from a pretrained 3D perception model with proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer (VGGT) as the visual encoder and introduce a proprioception-guided visual learning strategy to align perception with internal robot states, improving spatial grounding and closed-loop control. To reduce inference latency, we design a frame-wise token reuse mechanism that compacts multi-view tokens into an efficient spatial representation. We further apply random token pruning to enhance policy robustness and reduce overfitting. Experiments on challenging MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines such as DP and DP3, particularly in precision-critical and long-horizon scenarios.",
            "introduction": "未获取到引言",
            "llm_summary": "【关注的是什么问题】\n1. 如何将视觉编码器的结构与能力用于提升机器人视觉-动作嵌入的泛化和精确控制\n2. 如何融合预训练3D感知模型的几何先验与机器人本体的 proprioceptive 信息以改善空间定位与闭环控制\n3. 如何降低多视角视觉输入的推理时延并提升鲁棒性\n4. 如何通过 token 重用和随机剪枝提升效率与抗过拟合能力\n【用了什么创新的方案】\n- 以 Visual Geometry Grounded Transformer (VGGT) 作为视觉编码器，将几何先验整合到 visuomotor 策略中\n- 引入 proprioception-guided visual learning，使感知对齐内部状态并增强空间着陆与闭环控制\n- 设计 frame-wise token reuse 机制，将多视图 token 压缩为高效的空间表示以降低推理延迟\n- 采用随机 token pruning 增强策略鲁棒性并抑制过拟合\n- 在 MetaWorld 任务上对比 DP、DP3，显示显著提升，尤其在精度关键与长时程场景\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning",
            "authors": "Omar Rayyan,John Abanes,Mahmoud Hafez,Anthony Tzes,Fares Abu-Dakka",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "For project website and videos, see httpsthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18757",
            "code": "https://mv-umi.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18757",
            "arxiv_html_link": "https://arxiv.org/html/2509.18757v1",
            "abstract": "Recent advances in imitation learning have shown great promise for developing robust robot manipulation policies from demonstrations. However, this promise is contingent on the availability of diverse, high-quality datasets, which are not only challenging and costly to collect but are often constrained to a specific robot embodiment. Portable handheld grippers have recently emerged as intuitive and scalable alternatives to traditional robotic teleoperation methods for data collection. However, their reliance solely on first-person view wrist-mounted cameras often creates limitations in capturing sufficient scene contexts. In this paper, we present MV-UMI (Multi-View Universal Manipulation Interface), a framework that integrates a third-person perspective with the egocentric camera to overcome this limitation. This integration mitigates domain shifts between human demonstration and robot deployment, preserving the cross-embodiment advantages of handheld data-collection devices. Our experimental results, including an ablation study, demonstrate that our MV-UMI framework improves performance in sub-tasks requiring broad scene understanding by approximately 47% across 3 tasks, confirming the effectiveness of our approach in expanding the range of feasible manipulation tasks that can be learned using handheld gripper systems, without compromising the cross-embodiment advantages inherent to such systems.\nVideos can be found here: https://mv-umi.github.io",
            "introduction": "Imitation Learning (IL) provides a compelling pathway toward acquiring general robot policies capable of performing long-horizon tasks across diverse environments. This approach, particularly through supervised methods like Behavioral Cloning (BC), enables robots to acquire complex behaviors by learning to imitate human-directed actions in response to observations. Recent advances in architectures that better model this mapping [1, 2, 3, 4], coupled with enhancements in embodiments and hardware integrations [5, 6], have made this route increasingly convincing.\n\nRecent studies on data scaling laws in imitation learning [7] show that robot policy performance follows training scenario diversity, emphasizing the need for extensive and diverse data for robust policies. Data collection typically lies between two extremes. On one end, robot teleoperation enables the acquisition of high-quality, precise data with minimal embodiment discrepancies. However, this method is time-consuming and costly, as it requires an actively operated robot. On the other end, the internet is replete with videos of humans performing various tasks. However, substantial effort is required to establish structured explicit mappings between observed states and actions from these videos.\n\nAs a middle-ground, portable handheld grippers [8, 9, 10, 11, 12] have emerged as inexpensive and intuitive to use data collection devices. By relying exclusively on a wrist-mounted camera, they enable non-experts to record demonstrations without the need for a robotic manipulator. While this egocentric viewpoint minimizes visual discrepancies between training and deployment, resulting in cross-embodiment policies, it demands that the robot maintain a longer memory context to recall scene elements that move out of the constraining wrist view.\n\nIn this work, we propose a novel framework that augments the conventional wrist-mounted camera in handheld gripper systems with a third-person camera viewpoint, without incurring distributional shifts. We achieve this by performing real-time masking of the human demonstrator in the third-person video stream, effectively removing the operator’s presence from the training data. As a result, the model benefits from a broader view of the environment, while relying less on memory for scene remembrance. A side benefit we find of this masking is its removal of correlations between the demonstrator’s motions and the gripper’s actions, encouraging the policy to focus on task-relevant cues such as the manipulated objects rather than overfitting to human-specific signals. We also utilize a custom-made three-jaw gripper for some of the tasks that require greater dexterity in this work. This design allows for greater payload weight in comparison to other hand-held devices, at the cost of its volume. Schematics and instructions to reproduce the hardware are open-sourced separately. The hardware aspect is not a key focus in this paper.\n\nSummary of Contributions:\n\nMulti-View Cross-Embodiment Framework: MV-UMI fuses wrist-mounted and third-person views using SAM-2 segmentation and inpainting to eliminate domain shift, boosting performance in context-dependent tasks by 47%.\n\nEnd-to-End Open-Source System: Complete pipeline, including hardware design, data collection, training code, and deployment tools, is publicly released to advance cross-embodiment manipulation research, https://mv-umi.github.io.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在跨 embodiment 的示范学习中利用多视角来缓解域间差异（跨人体/设备）的影响\n2. 如何在手持抓取设备的数据集上融合第一视角和第三视角以提升上下文理解能力\n3. 如何通过去除示范者在第三视角中的影像来减少示范者动作与抓手动作的相关性\n4. 如何在不增加分布漂移的前提下扩展数据视角以提升对场景理解的鲁棒性\n5. 如何在跨视角学习中保持对任务相关 cues 的聚焦，避免过拟合人类信号\n\n【用了什么创新的方案】\n核心解决方案：在手持夹具的第一视角与第三视角之间进行实时多视图融合，利用 SAM-2 对人类示范者进行分割并在第三视角视频中进行去人化处理，从而获得更广的环境观察并降低域偏移；结合自定义三撬式抓手以提高耐受载荷，输出端到端的开放源码数据收集、训练与部署管线；通过掩码化第三视角实现对示范者的去域化，减少示范者动作与抓取动作的相关性，提升跨嵌入学习的任务自适应性。实验结果显示在需要广域场景理解的子任务上约提升47%。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation",
            "authors": "Nishant Doshi,Amey Sutvani,Sanket Gujar",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18734",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18734",
            "arxiv_html_link": "https://arxiv.org/html/2509.18734v1",
            "abstract": "One of the challenges faced by Autonomous Aerial\nVehicles is reliable navigation through urban environments.\nFactors like reduction in precision of Global Positioning System\n(GPS), narrow spaces and dynamically moving obstacles make the\npath planning of an aerial robot a complicated task. One of the\nskills required for the agent to effectively navigate through such an\nenvironment is to develop an ability to avoid collisions using\ninformation from onboard depth sensors. In this paper, we propose\nReinforcement Learning of a virtual quadcopter robot agent\nequipped with a Depth Camera to navigate through a simulated\nurban environment.",
            "introduction": "In recent years, Quadcopters have been extensively used\nfor civilian task like object tracking, disaster rescue, wildlife\nprotection and asset localization. It presents interesting\napplication avenues especially in tasks such as automated mail\ndelivery system, fire protection and disaster management.\nHowever, quadcopter navigation through urban environments\nis a complex task because of frequent dynamic obstacles\n(Humans, Posters, etc.). Also, the GPS navigation system can\nperform poorly when surrounded by tall buildings in urban\nenvironment, dilating the precision of the 3D position fix. It\nbecomes more dangerous when the quadcopter is flying\nthrough tight spaces and is uncertain of its position, increasing\nthe chances of collision. The quadcopter also needs to take\nsmart action after detecting dynamic obstacles (Humans,\nVehicles, animals, traffic signals etc.) during navigation in\nruntime in urban environment. Traditionally, obstacle\navoidance techniques have been designed as end point solution\nin an aerial robot navigation. One of the promising approach\nfor this problem is deep reinforcement learning. In this paper a\nsimple model is developed for the task of detecting and\navoiding common civilian obstacles encountered by a\nquadcopter while navigating a path in an urban environment.\n\nFrom the reinforcement learning view, the main challenge\nhere is that, the policy should update itself during runtime for\nstochastic obstacles detected in the environment and take the\noptimal action accordingly. Also, the navigation problem has\nsparse distributed reward in state space which is a challenge for\nlearning the shortest distance.\n\nThe objective of this project is to train a quadcopter to\nnavigate without hitting obstacles and taking a shortest path\naround through a high-rise urban environment where stochastic\nand dynamic obstacles are frequent.\n\nThe organization of the paper is as follows: Section I\nprovides a general introduction to the challenges for\nquadcopter urban navigation. Section II provides a\nprerequisites required to understand the experiments. Section\nIII defines the problem outlining the agent used and the\nenvironment. Section IV gives a brief description about the\nAirSim simulator, while section V defines the solution\napproaches for the problem defined. Section VI describes the\nexperiments and training and testing arena used. Section VII\ndiscusses the results for the experiments, while section VIII\ndescribes the future attempt that can be made and section IX\ndescribes the challenges faced during the experiments.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在城市环境中实现四旋翼无人机的可靠导航。  \n2. 如何处理动态障碍物对无人机路径规划的影响。  \n3. 如何在稀疏奖励环境中优化深度强化学习策略。  \n\n【用了什么创新的方案】  \n本研究提出了一种基于深度强化学习的虚拟四旋翼机器人代理，利用深度摄像头在模拟城市环境中进行导航。该方法通过实时更新策略来应对环境中的随机障碍物，并优化路径选择以避免碰撞，同时解决了稀疏奖励的问题。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Query-Centric Diffusion Policy for Generalizable Robotic Assembly",
            "authors": "Ziyi Xu,Haohong Lin,Shiqi Liu,Ding Zhao",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "8 pages, 7 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.18686",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18686",
            "arxiv_html_link": "https://arxiv.org/html/2509.18686v1",
            "abstract": "The robotic assembly task poses a key challenge in building generalist robots due to the intrinsic complexity of part interactions and the sensitivity to noise perturbations in contact-rich settings. The assembly agent is typically designed in a hierarchical manner: high-level multi-part reasoning and low-level precise control. However, implementing such a hierarchical policy is challenging in practice due to the mismatch between high-level skill queries and low-level execution. To address this, we propose the Query-centric Diffusion Policy (QDP), a hierarchical framework that bridges high-level planning and low-level control by utilizing queries comprising objects, contact points, and skill information. QDP introduces a query-centric mechanism that identifies task-relevant components and uses them to guide low-level policies, leveraging point cloud observations to improve the policy’s robustness. We conduct comprehensive experiments on the FurnitureBench in both simulation and real-world settings, demonstrating improved performance in skill precision and long-horizon success rate. In the challenging insertion and screwing tasks, QDP improves the skill-wise success rate by over 50% compared to baselines without structured queries.",
            "introduction": "Contact-rich manipulation has been widely recognized as a critical task when building generalist intelligent robots [1, 2, 3]. In this field, robot assembly [4, 5] stands out because it requires policies that are both precise and versatile to control the robot arm and interact with multiple objects.\nDespite the access to some offline demonstration data from human priors, robotic assembly poses two key challenges: inter-part relational reasoning and intra-part precise control in the online deployment. As is visualized in Figure 1, the first challenge arises from the long-horizon, multi-part nature of the task, which demands accurate prediction of the next skill based on current observations, as well as identifying the correct objects for interaction. The second challenge becomes especially difficult in contact-rich scenarios where successful assembly hinges on precise object alignment. Even minor noise or occlusions in raw sensory observations can completely fail the sim-to-real transfer of low-level non-prehensile control policies.\n\nVarious methods have been explored to address these challenges. One line of work focuses on high-level reasoning by leveraging cross-modality affordance-based approaches [6], graph-based key point reasoning [7], or skill-based retrieval [8, 9]. However, these methods rely heavily on heuristic-based low-level controllers and predefined skill libraries, thus limiting their adaptability and precision.\nMeanwhile, recent advances in robot learning have significantly improved the precision and adaptability of low-level policies, thanks to (i) powerful imitation learning backbones such as diffusion models [10, 11, 12] and transformer-based architectures [13, 14], (ii) enhanced learning regimes like residual policies [15] and safe failure prevention methods [16], and (iii) the integration of sensor modalities beyond vision and proprioception, such as tactile sensing [17, 9] or point clouds [18] for improved contact modeling.\nStill, transferring these policies from simulation to the real world remains challenging due to the inherent difficulty of accurately simulating contact dynamics.\n\nIn addition to the individual challenges of high-level and low-level policy design, integrating these two levels in a hierarchical framework introduces further complications. For instance, high-level policies may mis-specify objects or skills, causing the low-level policy to rely on the false queries that include irrelevant factors such as background pixels or non-impactful objects, ultimately leading to failure in real-world environments. Hence, it is crucial to establish a parsimonious proxy between high-level and low-level modules.\n\nTo address these challenges and establish a robust interface between high-level and low-level modules, we propose Query-centric Diffusion Policy (QDP). Our framework leverages powerful pre-trained foundation models to extract high-level information by specifying both the requisite skills and target objects as a query. This query then serves as a powerful precondition to guide point cloud-based low-level control, forcing the robot agent to focus on the current task-relevant components when generating the action chunks under different contexts.\nOur contributions are threefold:\n\nWe introduce QDP, a query-centric diffusion policy framework that selects task-relevant queries for guiding low-level policies, enabling accurate skill selection and precise object interaction.\n\nWe introduce QDP, a query-centric diffusion policy framework that selects task-relevant queries for guiding low-level policies, enabling accurate skill selection and precise object interaction.\n\nWe propose a query-conditioned policy learning scheme to model the complex geometry captured by point cloud observations, improving precision and facilitating sim-to-real transfer.\n\nWe demonstrate the effectiveness of our approach on FurnitureBench in both simulation and real-world settings, showcasing robust performance under object misalignment and human intervention.\n\n1. We introduce QDP, a query-centric diffusion policy framework that selects task-relevant queries for guiding low-level policies, enabling accurate skill selection and precise object interaction.\n\n2. We propose a query-conditioned policy learning scheme to model the complex geometry captured by point cloud observations, improving precision and facilitating sim-to-real transfer.\n\n3. We demonstrate the effectiveness of our approach on FurnitureBench in both simulation and real-world settings, showcasing robust performance under object misalignment and human intervention.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂的机器人组装任务中实现高层次的多部件推理与低层次的精确控制。  \n2. 如何解决在接触丰富的环境中，低层次控制策略的噪声敏感性和仿真到现实的转移问题。  \n3. 如何有效整合高层和低层策略，以避免错误的对象或技能指定导致的失败。  \n\n【用了什么创新的方案】  \n提出了Query-centric Diffusion Policy (QDP)框架，通过查询机制选择任务相关的组件，指导低层策略的执行。QDP利用点云观察来增强政策的鲁棒性，改善技能选择的准确性和物体交互的精确性。该框架在FurnitureBench上进行了全面实验，展示了在技能精度和长时间成功率上的显著提升。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space",
            "authors": "Sangjun Noh,Dongwoo Nam,Kangmin Kim,Geonhyup Lee,Yeonguk Yu,Raeyoung Kang,Kyoobin Lee",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "7 main scripts + 2 reference pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18676",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18676",
            "arxiv_html_link": "https://arxiv.org/html/2509.18676v1",
            "abstract": "Learning robust visuomotor policies that generalize across diverse objects and interaction dynamics remains a central challenge in robotic manipulation. Most existing approaches rely on direct observation-to-action mappings or compress perceptual inputs into global or object-centric features, which often overlook localized motion cues critical for precise and contact-rich manipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework that leverages scene-level 3D flow as a structured intermediate representation to capture fine-grained local motion cues. Our approach predicts the temporal trajectories of sampled query points and conditions action generation on these interaction-aware flows, implemented jointly within a unified diffusion architecture. This design grounds manipulation in localized dynamics while enabling the policy to reason about broader scene-level consequences of actions. Extensive experiments on the MetaWorld benchmark show that 3D FDP achieves state-of-the-art performance across 50 tasks, particularly excelling on medium and hard settings. Beyond simulation, we validate our method on eight real-robot tasks, where it consistently outperforms prior baselines in contact-rich and non-prehensile scenarios. These results highlight 3D flow as a powerful structural prior for learning generalizable visuomotor policies, supporting the development of more robust and versatile robotic manipulation. Robot demonstrations, additional results, and code can be found at https://sites.google.com/view/3d-fdp.",
            "introduction": "Learning robust manipulation skills in unstructured environments is a fundamental challenge in robotics. Policies deployed in such settings must generalize across diverse object geometries, appearances, dynamics, and contextual variations. Visual imitation learning offers a scalable alternative by enabling robots to acquire visuomotor skills from expert demonstrations without requiring task-specific reward functions [1, 2]. Recent approaches [3, 4, 5, 6, 7] often employ end-to-end architectures that map perception directly to control, leveraging transformers whose attention mechanisms excel at capturing global context and integrating multiple modalities.\n\nMore recently, diffusion models have shown significant promise in this domain [8, 4]. By formulating action generation as an iterative denoising process, they provide stable training and support complex, multimodal action distributions. When combined with structured inputs such as point clouds, these models achieve strong performance across diverse manipulation tasks [9, 10, 11]. Despite these advances, most existing methods still follow a direct observation-to-action mapping, often compressing perceptual inputs into global or object-centric features. Such representations can overlook localized motion cues that are essential for precise and contact-rich manipulation. While some works introduce intermediate representations, approaches based on object poses [12, 13] can struggle to capture scene-level dynamics, and those predicting future frames [14, 15, 16, 17, 18, 19] are often computationally intensive.\n\nTo address these limitations, we introduce the 3D Flow Diffusion Policy (3D FDP), an architecture that explicitly models local interaction dynamics through an intermediate representation of 3D scene flow. Our method predicts the temporal trajectories of sampled 3D query points and uses this predicted flow to infer actions. Unlike prior approaches [12, 20] that rely on object-level representations, our model learns both flow prediction and action generation jointly within a unified diffusion framework (Fig. 1). This integrated design offers two key advantages. First, 3D flow captures fine-grained correlations between the robot gripper and manipulated objects, providing spatial grounding that supports contact-aware behavior. Second, by modeling how local motion propagates across the scene, the policy can reason about the downstream effects of manipulation, such as how inserting a book might disturb surrounding objects on a cluttered shelf. These capabilities improve generalization across both geometric and dynamic variations.\n\nWe evaluate 3D FDP on the MetaWorld benchmark [21] and a set of real-world manipulation tasks. On MetaWorld, our approach achieves state-of-the-art performance across all difficulty levels, with particularly strong improvements on medium and hard tasks. In real-robot experiments involving diverse physical interactions, 3D FDP demonstrates capabilities beyond baseline policies, successfully handling contact-rich and non-prehensile tasks where previous methods fail. These results demonstrate that 3D scene flow provides an effective inductive bias for learning generalizable visuomotor policies. Our main contributions are as follows:\n\nWe introduce a scene-level 3D flow representation that captures interaction-aware motion throughout the scene.\n\nWe develop a unified diffusion-based architecture that jointly learns 3D flow prediction and action generation in an end-to-end manner.\n\nWe demonstrate that 3D FDP achieves state-of-the-art results on 50 MetaWorld tasks and outperforms prior baselines in real-world manipulation scenarios.\n\nWe provide a comprehensive analysis that demonstrates the effectiveness of our 3D flow representation, investigating key aspects such as conditioning strategies and query point density.\n\n1. We introduce a scene-level 3D flow representation that captures interaction-aware motion throughout the scene.\n\n2. We develop a unified diffusion-based architecture that jointly learns 3D flow prediction and action generation in an end-to-end manner.\n\n3. We demonstrate that 3D FDP achieves state-of-the-art results on 50 MetaWorld tasks and outperforms prior baselines in real-world manipulation scenarios.\n\n4. We provide a comprehensive analysis that demonstrates the effectiveness of our 3D flow representation, investigating key aspects such as conditioning strategies and query point density.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何学习能够在多样化对象和交互动态中泛化的稳健视觉运动策略。  \n2. 现有方法如何忽视局部运动线索，影响接触丰富的操控精度。  \n3. 如何有效地结合3D场景流与动作生成以改善操控性能。  \n\n【用了什么创新的方案】  \n我们提出了3D Flow Diffusion Policy (3D FDP)，一种新颖的框架，通过场景级3D流作为结构化中间表示，捕捉细粒度的局部运动线索。该方法预测采样查询点的时间轨迹，并基于这些交互感知流条件生成动作，所有这些都在统一的扩散架构中共同实现。此设计使操控 grounded 在局部动态上，同时使策略能够推理动作的更广泛场景级后果。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout",
            "authors": "Kaixin Chai,Hyunjun Lee,Joseph J. Lim",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18671",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18671",
            "arxiv_html_link": "https://arxiv.org/html/2509.18671v1",
            "abstract": "In mobile manipulation, the manipulation policy has strong preferences for initial poses where it is executed. However, the navigation module focuses solely on reaching the task area, without considering which initial pose is preferable for downstream manipulation.\nTo address this misalignment, we introduce N2M, a transition module that guides the robot to a preferable initial pose after reaching the task area, thereby substantially improving task success rates. N2M features five key advantages: (1) reliance solely on ego-centric observation without requiring global or historical information; (2) real-time adaptation to environmental changes; (3) reliable prediction with high viewpoint robustness; (4) broad applicability across diverse tasks, manipulation policies, and robot hardware; and (5) remarkable data efficiency and generalizability.\nWe demonstrate the effectiveness of N2M through extensive simulation and real-world experiments.\nIn the PnPCounterToCab task, N2M improves the averaged success rate from 3% with the reachability-based baseline to 54%.\nFurthermore, in the Toybox Handover task, N2M provides reliable predictions even in unseen environments with only 15 data samples, showing remarkable data efficiency and generalizability.\nProject website: https://clvrai.github.io/N2M/",
            "introduction": "Mobile manipulators, which integrate mobility and environmental interaction capabilities, hold significant promise for a wide range of real-world applications.\nBy leveraging scene understanding Rana et al. (2023); Hughes et al. (2022); Rosinol et al. (2020) and navigation modules Zheng et al. (2025); Chai et al. (2024); Chang et al. (2023), these robots can reach the task area based on the task descriptions, and subsequently accomplish the task by executing pre-trained manipulation policies Fu et al. (2024); Chi et al. (2024); Black et al. (2024).\n\nHowever, existing works mainly focus on enhancing navigation and manipulation independently, while not giving sufficient attention to the interplay between them.\nIn this paper, we identify an inherent misalignment between navigation and manipulation, which significantly reduces the task success rate.\nSpecifically, due to factors such as joint limitation and training data distribution, the performance of the manipulation policy is sensitive to the initial pose from which execution begins.\nMeanwhile, navigation merely focuses on guiding the robot to task areas without considering which initial pose is preferable for executing the manipulation policy.\n\nThe most direct solution would be to develop an end-to-end model handling both navigation and manipulation Yang et al. (2024), thereby avoiding challenges in inter-module coordination. However, due to the inherent complexity of both navigation and manipulation, the design, training, and data collection for such end-to-end models remain an open problem.\nAn alternative approach within modular frameworks is to enhance the robustness of the manipulation policy. However, visuomotor policies are sensitive to viewpoint changes Heo et al. (2023), necessitating data collection from various initial poses throughout the task area Gu et al. (2022), which is costly.\n\nIn this paper, we propose a simple but effective transition module, named N2M (Navigation-to-Manipulation), serving as a bridge between navigation and manipulation. As depicted in Fig. 1, after reaching the task area, the robot is transferred from the end pose of navigation to an initial pose that is preferable for executing the manipulation policy, thereby improving the task success rate.\n\nWe identify five fundamental challenges for bridging navigation and manipulation, and propose our corresponding solutions.\n\nAdaptability to non-static environments.\nThe environments are typically non-static, requiring predictions to adapt to environmental changes. To support this, N2M predicts the preferable initial pose from the ego-centric RGB point cloud with a single forward pass. This efficient design enables N2M to generate real-time predictions in dynamic environments, as demonstrated in Section 5.2.\n\nMulti-modality of preferable initial poses.\nMultiple preferable initial poses may exist within the task area. Consequently, predicting a single pose is insufficient, as it can cause the model to learn an interpolation between viable poses Bishop (1994), which may not be preferable to execute manipulation policies. To address this multi-modality, N2M predicts the distribution of preferable initial poses, which is represented with a Gaussian Mixture Model (GMM) Bahl et al. (2023).\n\nCriterion of preferable initial poses.\nManipulation performance depends on multiple factors: policy architecture, training data distribution, robot configuration, task, and environment. Rather than attempting to model these complex relationships, we directly evaluate the pose through policy rollouts.\nDuring data collection, we position the robot at various poses and execute the manipulation policy, and successful execution indicates a preferable initial pose.\nLearning initial pose preferences directly from policy rollouts ensures that N2M’s predictions align with the policy’s actual performance while simultaneously enabling broad applicability across diverse policies, tasks, and robot hardware, as shown in Sections 4.2, 5.1, and 5.2.\n\nViewpoint Robustness.\nSince the robot navigation end poses can be anywhere within the task area, N2M needs to provide reliable predictions at various viewpoints. To achieve this, we augment N2M’s training data from multiple viewpoints. Experiments in Sections 4 and 5 demonstrate that N2M reliably predicts preferable initial poses across the whole task area. Interestingly, we note that our proposed data augmentation approach also significantly improves data efficiency and generalizability. We will further analyze the reason behind these benefits in Section 6.\n\nData Efficiency.\nCollecting rollouts requires substantial time and human effort, as each rollout must be monitored and manually labeled with success or failure.\nWe incorporate two main strategies to make N2M data-efficient:\nFirst, we design the module to directly predict the initial pose distribution, rather than low-level action Lee et al. (2019);\nSecond, we augment the dataset through viewpoint rendering to increase its coverage and diversity. In Sections 4.3, 4.4, and 5, we demonstrate that N2M has remarkable data efficiency and generalizability.\n\nOur contributions are as follows:\n\nFirst, we identify a critical misalignment between navigation and manipulation modules and introduce N2M, which predicts preferable initial poses on the fly from ego-centric observations.\n\nSecond, we propose learning policy initial pose preferences from rollouts, without making additional assumptions about tasks, policies, and hardware, thereby endowing N2M with broad applicability.\n\nThird, we achieve remarkable data efficiency and generalizability through a novel data augmentation approach combined with our carefully designed input-output architecture.\n\nFinally, we conduct extensive experiments validating the effectiveness of our proposed N2M module across various settings and release our code to facilitate community exploration.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有导航和操作模块之间的协调不足，导致任务成功率低。  \n2. 操作策略对初始姿态的敏感性未被导航模块考虑。  \n3. 数据收集和训练过程中的高成本和低效率问题。  \n\n【用了什么创新的方案】  \nN2M（Navigation-to-Manipulation）模块作为导航与操作之间的桥梁，实时预测可执行操作的初始姿态。它依赖于自我中心的RGB点云进行高效预测，采用高斯混合模型表示多种可行姿态，直接从策略回放中学习初始姿态偏好，显著提高数据效率和泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Distributionally Robust Safe Motion Planning with Contextual Information",
            "authors": "Kaizer Rahaman,Simran Kumari,Ashish R. Hota",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18666",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18666",
            "arxiv_html_link": "https://arxiv.org/html/2509.18666v1",
            "abstract": "We present a distributionally robust approach for collision avoidance by incorporating contextual information. Specifically, we embed the conditional distribution of future trajectory of the obstacle conditioned on the motion of the ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional kernel mean embedding operator. Then, we define an ambiguity set containing all distributions whose embedding in the RKHS is within a certain distance from the empirical estimate of conditional mean embedding learnt from past data. Consequently, a distributionally robust collision avoidance constraint is formulated, and included in the receding horizon based motion planning formulation of the ego agent. Simulation results show that the proposed approach is more successful in avoiding collision compared to approaches that do not include contextual information and/or distributional robustness in their formulation in several challenging scenarios.",
            "introduction": "Safe motion planning in uncertain and dynamic environments is a fundamental challenge in the field of robotics and autonomous systems [1, 2]. Past works have proposed a plethora of approaches, including sampling-based methods [3], collision avoidance using velocity obstacles [4], and optimization-based techniques [5, 6]. In recent years, optimal control with a receding horizon implementation, popularly known as model predictive control (MPC), have gained popularity as it enables the designer to systematically include the effects of robot dynamics, constraints on robot states and inputs as well as collision avoidance constraints while planning the motion of the robot.\n\nWhile early works on MPC based motion planning focused on avoiding static obstacles [7, 8, 6], more recent works have considered dynamic obstacles in the robust and stochastic MPC frameworks. While robust MPC tends to result in conservative solutions, stochastic MPC techniques formulate collision avoidance conditions in terms of probabilistic constraints or via suitable risk measures [9]. However, the safety guarantees provided by the above techniques may not hold when the probability distribution of the future obstacle position is unknown and/or time-varying.\n\nConsequently, several recent works have proposed distributionally robust motion planning techniques where the stochastic collision avoidance constraints hold for an entire family of distributions (or ambiguity set) that are close (in terms of the Wasserstein distance) to the empirical distribution constructed from the observed data [10, 11, 12, 13, 14]. However, not all available past data are equally relevant at a given context or scenario. In particular, when we consider multiple agents that share the same environment, and are in the vicinity of each other, the future position of the obstacle is a function of the future position of the ego agent. Therefore, in order to obtain more accurate solutions, it is essential to consider ambiguity sets that depend on the actions of the ego agent and other available contextual information. Despite its significance, there is no prior work which considers decision-dependent ambiguity sets or any other contextual information while formulating the distributionally robust safe motion planning problem.\n\nA few recent works have explicitly included the behavior of the ego vehicle for trajectory prediction. Specifically, [15] investigated the impact of ego vehicle planning on nearby vehicles’ trajectories using an LSTM-based encoder combined with a convolutional social pooling module. The authors in [16] modeled each vehicle as a wave characterized by amplitude and phase, proposing that wave-pooling better captures dynamic states and high-order interactions through wave superposition. More recently, [17] provided a detailed comparison of several contextual trajectory prediction techniques. However, the robustness of neural network based trajectory prediction to distribution shifts is not adequately explored in the literature. Neural network based techniques also do not provide any rigorous (e.g., finite-sample) guarantees on the probability of collision among vehicles.\n\nIn this paper, we aim to fill this research gap. First, we compute the empirical estimate of the conditional kernel mean embedding (CKME) [18] of the (conditional) distribution of the future obstacle position as a function of the (i.e., conditioned on the) current states and the predicted trajectory of the ego agent. Then, we formulate an optimal control problem that embeds the ego-conditioned predicted trajectory in the constraints. In particular, the CKME operator provides a closed-form expression of the future trajectory of other vehicles as a function of the states and inputs of the ego vehicle, i.e., the decision variables of the optimal control problem. In order to robustify against distribution shifts, the conditional value at risk of the collision avoidance constraint is required to hold for all distributions whose mean embeddings are within a specified maximum-mean discrepancy (MMD) distance from the empirical estimate of the CKME; a similar approach was recently examined in [19] in the context of optimal control. Another recent paper [20] explored RKHS to select probable trajectories and adopted a sampling-based optimization approach, which is distinct from our approach. We present a tractable approximation of the above constraints following the reformulations developed in [21, 22] such that off-the-shelf nonlinear optimization solvers, such as IPOPT, can be deployed to solve the optimal control problem.111Neural network based predictors do not provide a simple closed form expression of the predicted trajectories as a function of the input applied to the ego vehicle. Therefore, computing optimal control inputs with predicted trajectories used in the constraints is not straightforward.  We provide detailed simulation results involving an autonomous ground vehicle moving on a road with obstacle agent(s), and show that the proposed approach successfully avoids collision in several challenging scenarios.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在不确定和动态环境中实现安全的运动规划。  \n2. 如何有效利用上下文信息来增强碰撞避免的鲁棒性。  \n3. 如何处理未来障碍物位置的概率分布未知或时变的情况。  \n\n【用了什么创新的方案】  \n本文提出了一种基于条件核均值嵌入（CKME）的分布鲁棒运动规划方法，通过将障碍物未来轨迹的条件分布嵌入到再生核希尔伯特空间（RKHS）中，构建了一个依赖于自我代理动作的模糊集合。通过优化控制问题，确保在所有与CKME的经验估计保持在特定距离内的分布下，碰撞避免约束的条件值风险得以满足。该方法在多个挑战场景中展现出优越的碰撞避免能力。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer",
            "authors": "Yarden As,Chengrui Qu,Benjamin Unger,Dongho Kang,Max van der Hart,Laixi Shi,Stelian Coros,Adam Wierman,Andreas Krause",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18648",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18648",
            "arxiv_html_link": "https://arxiv.org/html/2509.18648v1",
            "abstract": "Safety remains a major concern for deploying reinforcement learning (RL) in real-world applications. Simulators provide safe, scalable training environments, but the inevitable sim-to-real gap introduces additional safety concerns, as policies must satisfy constraints in real-world conditions that differ from simulation. To address this challenge, robust safe RL techniques offer principled methods, but are often incompatible with standard scalable training pipelines. In contrast, domain randomization, a simple and popular sim-to-real technique, stands out as a promising alternative, although it often results in unsafe behaviors in practice. We present SPiDR, short for Sim-to-real via Pessimistic Domain Randomization—a scalable algorithm with provable guarantees for safe sim-to-real transfer. SPiDR uses domain randomization to incorporate the uncertainty about the sim-to-real gap into the safety constraints, making it versatile and highly compatible with existing training pipelines. Through extensive experiments on sim-to-sim benchmarks and two distinct real-world robotic platforms, we demonstrate that SPiDR effectively ensures safety despite the sim-to-real gap while maintaining strong performance.",
            "introduction": "Reinforcement learning (RL) has made significant strides in recent years, demonstrating remarkable progress across a range of domains. These include achieving superhuman capabilities in games (Mnih et al., 2015; Silver et al., 2016), fine-tuning large language models (Ouyang et al., 2022), advancing applications in healthcare (Fox et al., 2020; Zhu et al., 2020), robotics (Lee et al., 2020; Degrave et al., 2022; Lin et al., 2025) and autonomous driving (Cusumano-Towner et al., 2025; Cornelisse et al., 2025).\nYet despite these achievements, ensuring safety and preventing harmful behaviors remains a critical challenge and a prerequisite for unlocking the full potential of RL as a ubiquitous element in everyday life (Amodei et al., 2016; Gu et al., 2022).\n\nThe use of simulators has been a key component behind the success of many of the mentioned applications  (Visentin et al., 2014; Makoviychuk et al., 2021; Degrave et al., 2022; Kazemkhani et al., 2024). Training in simulation allows agents to learn from unsafe interactions, which in reality would lead to catastrophic outcomes. In addition, learning complex behaviors fully online can be prohibitively time-consuming. Modern simulators accelerate training, reducing hours of real-world experience to minutes on consumer-grade GPUs (Rudin et al., 2022). However, while being a major driver in the development of the above examples, even state-of-the-art simulators often fall short in precisely mirroring the real-world. Indeed, “all models are wrong” (Box, 1976)—the so-called sim-to-real gap can make simulation-trained policies violate real-world constraints, which can be particularly dangerous in high-stakes settings where safety must be guaranteed on first contact.\n\nExisting literature to address this challenge often relies on tools from robust optimization (Queeney and Benosman, 2024; Kitamura et al., 2024; Zhang et al., 2024). While being theoretically grounded, such methods typically require practitioners to significantly alter their existing training pipelines, rendering them less prevalent in practice. In contrast, due to its simplicity, domain randomization has become the de facto tool for sim-to-real transfer (Tobin et al., 2017; Peng et al., 2018; Lee et al., 2020; Degrave et al., 2022). Despite its success, in problems that require adherence to safety constraints, domain randomization lacks safety guarantees and often fails to satisfy the constraints in practice (cf. Queeney and Benosman, 2024, and Figure˜3). Therefore, a method that provably guarantees safe sim-to-real transfer, while being highly compatible with standard training practices, is still missing.\n\nIn this work, we address this gap by presenting a simple method that builds on domain randomization while ensuring safety under sim-to-real transfer. We theoretically show that unsafe transfer can be associated with large uncertainty about the sim-to-real gap, quantified as the disagreement among next-state predictions from domain-randomized dynamics models. This key idea is illustrated in Figure˜1, where spikes in uncertainty (e.g. at t=4.6t=4.6 and t=5.3t=5.3) coincide with unstable or unsafe behaviors, such as stumbling or flipping. Motivated by this insight, we propose to penalize the cost with the uncertainty to achieve safe sim-to-real transfer, leading to the design of SPiDR. Notably, SPiDR seamlessly integrates with state-of-the-art RL algorithms (Schulman et al., 2017; Haarnoja et al., 2019), delivering strong empirical performance on both in simulated and real-world safe RL tasks while ensuring constraint satisfaction, even under severe model mismatch.\n\nWe address an important challenge to real-world adoption of RL: zero-shot safe sim-to-real transfer, where an agent must learn a safe and effective policy using only simulated interactions. We propose SPiDR, a practical algorithm with formal safety guarantees that integrates easily into popular sim-to-real pipelines.\n\nWe address an important challenge to real-world adoption of RL: zero-shot safe sim-to-real transfer, where an agent must learn a safe and effective policy using only simulated interactions. We propose SPiDR, a practical algorithm with formal safety guarantees that integrates easily into popular sim-to-real pipelines.\n\nWe validate SPiDR on two real-world robotic platforms, where it achieves zero-shot constraint satisfaction, substantially outperforming other baselines in terms of safety and performance. These results provide empirical evidence that our theoretical guarantees translate to the real-world, suggesting that SPiDR can be safely used in real-world deployment.\n\nFinally, we extensively evaluate SPiDR on well established simulated continuous control benchmarks, including the RWRL benchmark (Dulac-Arnold et al., 2020), Safety Gym (Ray et al., 2019) and RaceCar environments (Kabzan et al., 2020), where SPiDR consistently satisfies safety constraints while achieving strong task performance.\n\n1. We address an important challenge to real-world adoption of RL: zero-shot safe sim-to-real transfer, where an agent must learn a safe and effective policy using only simulated interactions. We propose SPiDR, a practical algorithm with formal safety guarantees that integrates easily into popular sim-to-real pipelines.\n\n2. We validate SPiDR on two real-world robotic platforms, where it achieves zero-shot constraint satisfaction, substantially outperforming other baselines in terms of safety and performance. These results provide empirical evidence that our theoretical guarantees translate to the real-world, suggesting that SPiDR can be safely used in real-world deployment.\n\n3. Finally, we extensively evaluate SPiDR on well established simulated continuous control benchmarks, including the RWRL benchmark (Dulac-Arnold et al., 2020), Safety Gym (Ray et al., 2019) and RaceCar environments (Kabzan et al., 2020), where SPiDR consistently satisfies safety constraints while achieving strong task performance.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在sim-to-real转移中确保安全性，尤其是零-shot情况下。  \n2. 现有的安全强化学习方法在实际应用中往往不兼容标准训练流程。  \n3. 域随机化在满足安全约束方面的不足。  \n\n【用了什么创新的方案】  \nSPiDR（Sim-to-real via Pessimistic Domain Randomization）是一种新颖的算法，通过将对sim-to-real差距的不确定性纳入安全约束，确保在模拟到现实的转移中实现安全性。该方法与现有的强化学习算法无缝集成，能够在模拟和现实世界中有效地满足安全约束，同时保持强大的任务性能。SPiDR的设计使其在保持高兼容性的同时，提供了形式化的安全保证。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Do You Need Proprioceptive States in Visuomotor Policies?",
            "authors": "Juntu Zhao,Wenbo Lu,Di Zhang,Yufeng Liu,Yushen Liang,Tianluo Zhang,Yifeng Cao,Junyuan Xie,Yingdong Hu,Shengjie Wang,Junliang Guo,Dequan Wang,Yang Gao",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18644",
            "code": "https://statefreepolicy.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18644",
            "arxiv_html_link": "https://arxiv.org/html/2509.18644v1",
            "abstract": "Imitation-learning–based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0% to 85% in height generalization and from 6% to 64% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment.",
            "introduction": "Imitation-learning-based visuomotor policies [1, 2, 3, 4, 5] have been widely used in robotic manipulation.\nLeveraging large-scale demonstration datasets [6, 7, 8, 9] and fine-tuning powerful pre-trained policies have enabled robots to achieve remarkable performance across diverse real-world tasks.\n\nFor precise and reliable control, these visuomotor policies typically incorporate not only visual observations of the task environment but also proprioceptive state (hereafter referred to as state) inputs [10, 3, 11], such as end-effector poses and joint angles. The state inputs provide compact and accurate information about the robot’s configuration, but they also make the policy prone to overfitting by simply memorizing the training trajectories.\nTherefore it severely limits spatial generalization [12, 13, 14] if the training data lacks diversity [15].\nIn today’s context, where collecting demonstration data with wide state coverage (i.e., diverse spatial locations of task-relevant objects) is prohibitively expensive, this has become a critical bottleneck for the development of visuomotor policies.\n\nIn this study, we propose to completely remove the state input in visuomotor policies to enhance their spatial generalization ability, hereafter referred to as “State-free Policies.”\nThis design is built upon two conditions:\n\nRelative end-effector (EEF) action space [16]: The visuomotor policies predict relative displacements of the end-effector based on the current observation. Among different action spaces, the relative EEF action space most naturally supports the generalization of policies.\n\nFull task observation: Another key condition for effective State-free Policies is to ensure sufficient task-relevant visual information, which we term “full task observation”. This enables visuomotor policies to fully “see” the task-relevant objects in the task.\n\nThis mechanism of State-free Policies forces the policy to develop a deeper understanding of the task environment rather than simply memorizing the trajectories, thereby enabling State-free Policies to achieve advantages that state-based policies cannot provide:\n\nSpatial Generalization: Since State-free Policies do not rely on state inputs, they avoid overfitting to the training trajectories.\nTherefore, they exhibit strong height and horizontal generalization abilities, where height refers to variations of the task-relevant object’s location in the vertical direction, and horizontal refers to variations of the object’s location in the 2D plane.\n\nData efficiency: Even in in-domain settings, state-based policies require diverse demonstrations to avoid overfitting to specific trajectories. In contrast, removing the state input eliminates this dependence on trajectory diversity, allowing State-free Policies to be fine-tuned with less demonstration data. This reduces the cost of data collection, which is often a major bottleneck in deploying real-world robots.\n\nCross-embodiment adaptation: Since State-free Policies rely only on visual inputs and predict actions in the relative EEF space, they exhibit stronger cross-embodiment adaptation ability than state-based policies. They do not require additional adaptation to different state spaces, so the same task can be easily adapted to new embodiments with fewer fine-tuning steps.\n\nCross-embodiment adaptation: Since State-free Policies rely only on visual inputs and predict actions in the relative EEF space, they exhibit stronger cross-embodiment adaptation ability than state-based policies. They do not require additional adaptation to different state spaces, so the same task can be easily adapted to new embodiments with fewer fine-tuning steps.\n\nWe have conducted extensive experiments across a diverse range of tasks, robot embodiments, and policy architectures.\nIn both real-world and simulation environments, State-free Policies achieve comparably great in-domain performance to state-based policies.\nMost importantly, when trained on strictly collected real-world demonstration data\n(i.e., the task-relevant object location has a constrained initial distribution range),\nState-free Policies exhibit significantly stronger spatial generalization ability than state-based policies.\nFor further benefits, e.g., data efficiency and cross-embodiment adaptation ability, they also demonstrate obvious advantages over state-based policies, highlighting their potential for scalable and practical deployment in real-world robotic systems.\n\n1. Relative end-effector (EEF) action space [16]: The visuomotor policies predict relative displacements of the end-effector based on the current observation. Among different action spaces, the relative EEF action space most naturally supports the generalization of policies.\n\n2. Full task observation: Another key condition for effective State-free Policies is to ensure sufficient task-relevant visual information, which we term “full task observation”. This enables visuomotor policies to fully “see” the task-relevant objects in the task.\n\n1. Spatial Generalization: Since State-free Policies do not rely on state inputs, they avoid overfitting to the training trajectories.\nTherefore, they exhibit strong height and horizontal generalization abilities, where height refers to variations of the task-relevant object’s location in the vertical direction, and horizontal refers to variations of the object’s location in the 2D plane.\n\n2. Data efficiency: Even in in-domain settings, state-based policies require diverse demonstrations to avoid overfitting to specific trajectories. In contrast, removing the state input eliminates this dependence on trajectory diversity, allowing State-free Policies to be fine-tuned with less demonstration data. This reduces the cost of data collection, which is often a major bottleneck in deploying real-world robots.\n\n1. Cross-embodiment adaptation: Since State-free Policies rely only on visual inputs and predict actions in the relative EEF space, they exhibit stronger cross-embodiment adaptation ability than state-based policies. They do not require additional adaptation to different state spaces, so the same task can be easily adapted to new embodiments with fewer fine-tuning steps.",
            "llm_summary": "【关注的是什么问题】  \n1. 过度依赖proprioceptive状态输入导致策略对训练轨迹的过拟合。  \n2. 现有的visuomotor策略在空间泛化能力上存在严重限制。  \n3. 数据收集的多样性不足成为开发visuomotor策略的瓶颈。  \n\n【用了什么创新的方案】  \n提出了一种“无状态策略”（State-free Policy），完全去除proprioceptive状态输入，仅基于视觉观察预测动作。这种策略在相对末端执行器动作空间中构建，确保充分的任务相关视觉信息，从而增强空间泛化能力。实验结果表明，该策略在多种真实世界任务中表现出显著的空间泛化能力、数据效率和跨躯体适应性，提升了实际部署的可行性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments",
            "authors": "Yuan Zhou,Jialiang Hou,Guangtong Xu,Fei Gao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18636",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18636",
            "arxiv_html_link": "https://arxiv.org/html/2509.18636v1",
            "abstract": "Formation maintenance with varying number of drones in narrow environments hinders the convergence of planning to the desired configurations. To address this challenge, this paper proposes a formation planning method guided by Deformable Virtual Structures (DVS) with continuous spatiotemporal transformation. Firstly, to satisfy swarm safety distance and preserve formation shape filling integrity for irregular formation geometries, we employ Lloyd algorithm for uniform P​A¯\\underline{PA}rtitioning and Hungarian algorithm for A​S¯\\underline{AS}signment (PAAS) in DVS. Subsequently, a spatiotemporal trajectory involving DVS is planned using primitive-based path search and nonlinear trajectory optimization. The DVS trajectory achieves adaptive transitions with respect to a varying number of drones while ensuring adaptability to narrow environments through affine transformation. Finally, each agent conducts distributed trajectory planning guided by desired spatiotemporal positions within the DVS, while incorporating collision avoidance and dynamic feasibility requirements. Our method enables up to 15% of swarm numbers to join or leave in cluttered environments while rapidly restoring the desired formation shape in simulation. Compared to cutting-edge formation planning method, we demonstrate rapid formation recovery capacity and environmental adaptability. Real-world experiments validate the effectiveness and resilience of our formation planning method.",
            "introduction": "In recent years, formation flight becomes the foundation requirement for aerial swarms in practical applications, such as collaborative exploration [1], light show [2], search and rescue [3]. For large-scale swarms [4], [5], formation inevitably encounters agent loss in narrow environments [6, 7, 8]. Naturally, the integration of new members can restore the impaired formation and enhance the efficiency of mission execution [9], [10]. Therefore, dynamic adaptation of formation reconfiguration for changes in the number of drones is essential to ensure operational resilience under unforeseen disturbance. Besides, formation needs to experience adaptively overall shape transformations to navigate through narrow environments and maintain its configuration as much as possible to keep resilience [11, 12, 13, 14].\n\nWhen the number of drones in formations varies, the abrupt variation of formation cooperative constraints tends to degrade the feasibility of trajectory optimization. In narrow environments, the inherent conflict between formation maintenance and collision avoidance also deteriorates the feasibility, resulting in no solution for the classical planning method without global formation adjustment mechanism.\n\nNumerous works have demonstrated formation systems with swarm number variations.\nArtificial Potential Fields (APF) [4], [8], [15] are commonly used to guide the formation generation with variable swarm numbers. This approach eliminates the need for explicit assignment of desired positions, enabling adaptive number variation of formation. However, the slow convergence rate and inadequate swarm collision avoidance hinder its applicability in real-time formation planning. Although explicit position and assignment during number variations enable rapid formation recovery [16], [17], [18], they often fail to guarantee robust performance in narrow environments, where collision avoidance remains challenging during formation recovery.\nAdaptive deformation of the formation shape is widely adopted for safe and rapid navigation through obstacle environments [11], [13]. Nevertheless, distributed cooperative planning is insufficient without effective guidance for formation in narrow environments. Consequently, adjusting the overall deformation of the swarm provides an effective resolution [11]. However, this category of approaches is limited by either locally reactive mechanisms [13] and simply adjusts the local target that hinders formation guidance and environmental adaptability [11], or an excessive pursuit of collision-free passage that severely compromises formation maintenance integrity while neglecting post-maneuver recovery capability. These methods often struggle to rapidly restore desired formations in cluttered environments. Furthermore, most studies of formation planning do not address adaptive adjustment with variations in swarm number [11], [19], [20].\n\nTo achieve real-time formation flight with a variable number of drones, we propose a number adaptive formation planning system. According to the desired formation shape, we employ explicit partitioning and assignment to designate formation positions for the swarm. We focus on designing a Deformable Virtual Structures (DVS) trajectory with spatiotemporal motion and deformation that can effectively guide formation recovery and maintenance. Specifically, we introduce virtual rigid structures with two additional degrees of freedom, including scaling and affine transformation in DVS. The concept of virtual structures has been applied in multiple fields, but predominantly uses rigid virtual structures which are unable to adaptively deform [21, 22, 23]. In this work, we design a spatiotemporal trajectory optimization method for DVS, which utilizes compact piecewise polynomial representations in Cartesian and deformable spaces to formulate nonlinear optimization problems. This enables adaptive adjustment of the scale of formations according to swarm number variations, while permitting appropriate affine deformation of the virtual structures to facilitate the formations to navigate in narrow environments. Integrating PAAS and DVS with spatiotemporal transformation, we designate this framework as Deformation Guidance (DG), and agents perform distributed trajectory planning under the guidance of DG. Extensive simulation and real-world experiments demonstrate that our method supports variable number formation planning while achieving a high success rate in navigating through narrow environments.\n\nThe contributions of this paper are as follows:\n\nTo satisfy swarm safety distance and preserve shape filling integrity for irregular formation geometries, we implement the Lloyd algorithm for uniform partitioning in formation planning with variable number, with assignment via the Hungarian algorithm. We experimentally verified that the computational efficiency of PAAS is sufficient to achieve real-time planning.\n\nTo satisfy swarm safety distance and preserve shape filling integrity for irregular formation geometries, we implement the Lloyd algorithm for uniform partitioning in formation planning with variable number, with assignment via the Hungarian algorithm. We experimentally verified that the computational efficiency of PAAS is sufficient to achieve real-time planning.\n\nWe propose a spatiotemporal trajectory optimization method for DVS with scaling and affine transformation to guide formation planning in narrow environments. We employ a primitive path search to efficiently obtain high-dimensional DVS paths as initial values for DVS trajectory optimization, which can be transformed into unconstrained optimization problems for real-time solution.\n\nWe incorporate DVS-based spatiotemporal formation constraint in distributed trajectory optimization to achieve rapid formation recovery and enhanced environmental adaptability. Extensive experiments and benchmarks validate the efficiency and advantages of our system.",
            "llm_summary": "【关注的是什么问题】  \n1. 变动数量的无人机在狭窄环境中的编队维护问题。  \n2. 传统规划方法在动态调整编队形状和避免碰撞方面的局限性。  \n3. 如何实现实时的编队飞行以适应无人机数量的变化。  \n\n【用了什么创新的方案】  \n本文提出了一种基于可变形虚拟结构（DVS）的编队规划方法，通过连续的时空变换来指导编队恢复和维护。采用Lloyd算法进行均匀划分，并结合匈牙利算法进行分配，以满足安全距离和形状完整性要求。设计了一种具有缩放和仿射变换能力的DVS轨迹优化方法，能够在狭窄环境中有效导航。通过将DVS与时空变换相结合，支持动态的编队调整，确保在复杂环境中快速恢复所需的编队形状。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training",
            "authors": "Shuo Cheng,Liqian Ma,Zhenyang Chen,Ajay Mandlekar,Caelan Garrett,Danfei Xu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18631",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18631",
            "arxiv_html_link": "https://arxiv.org/html/2509.18631v1",
            "abstract": "Behavior cloning has shown promise for robot manipulation, but real-world demonstrations are costly to acquire at scale. While simulated data offers a scalable alternative, particularly with advances in automated demonstration generation, transferring policies to the real world is hampered by various simulation and real domain gaps. In this work, we propose a unified sim-and-real co-training framework for learning generalizable manipulation policies that primarily leverages simulation and only requires a few real-world demonstrations. Central to our approach is learning a domain-invariant, task-relevant feature space. Our key insight is that aligning the joint distributions of observations and their corresponding actions across domains provides a richer signal than aligning observations (marginals) alone. We achieve this by embedding an Optimal Transport (OT)-inspired loss within the co-training framework, and extend this to an Unbalanced OT framework to handle the imbalance between abundant simulation data and limited real-world examples. We validate our method on challenging manipulation tasks, showing it can leverage abundant simulation data to\nachieve up to a 30% improvement in the real-world success rate\nand even generalize to scenarios seen only in simulation.",
            "introduction": "Behavior cloning pomerleau1988alvinn  is a promising approach for acquiring robot manipulation skills directly in the real world, due to its simplicity and effectiveness in mimicking expert demonstrations robomimic2021 ; florence2022implicit . However, achieving robust and generalizable performance requires collecting large-scale datasets khazatsky2024droid ; open_x_embodiment_rt_x_2023  across diverse environments, object configurations, and tasks. This data collection process is labor-intensive, time-consuming, and costly, posing significant challenges to scalability in real-world applications.\n\nRecently, with rapid advancements in physics simulators Genesis ; Xiang_2020_SAPIEN , procedural scene generation raistrick2024infinigen ; deitke2022️ , and motion synthesis techniques mandlekar2023mimicgen ; cheng2023nod , there has been growing interest in leveraging simulation as an alternative source of training data. These simulation-based approaches enable scalable and controllable data generation, allowing for diverse and abundant supervision at a fraction of the real-world cost. However, transferring policies trained in simulation to the physical world remains a non-trivial challenge due to sim-to-real gap—the discrepancies between the simulated and real-world environments that a policy encounters during execution. These differences can manifest in various forms, such as variations in visual appearance, sensor noise, and action dynamics andrychowicz2020learning ; tobin2017domain . In particular, learning visuomotor control policies that remain robust under changing perceptual conditions during real-world deployment continues to be an open area of research.\n\nCommon strategies to bridge this domain gap include domain randomization andrychowicz2020learning ; tobin2017domain  and data augmentation hansen2020self ; yarats2021mastering , though these often require careful tuning. Domain adaptation (DA) techniques aim to explicitly align distributions, either at pixel bousmalis2017unsupervised ; james2019sim  or feature levels tzeng2014deep ; long2015learning ; zhao2019learning . However, many feature-level methods align only marginal observation distributions (e.g., MMD tzeng2014deep ; long2015learning ), which can be insufficient for fine-grained manipulation alignment as it may not preserve action-relevant relationships across domains. More recently, sim-and-real co-training—simply training a single policy on mixed data from both domains wei2025empirical ; maddukuri2025sim —has shown surprising effectiveness. We argue that while beneficial for data diversity, such co-training approaches typically lack explicit constraints for feature space alignment across domains, potentially hindering optimal transfer and generalization because they don’t enforce a consistent mapping of task-relevant structures.\n\nWe present a unified sim-and-real co-training framework that explicitly learns a shared latent space where observations from simulation and the real world are aligned and preserve action-relevant information. Our key insight is that aligning the joint distributions of observations and their corresponding actions or task-relevant states across domains provides a direct signal for learning transferable features.\nConcretely, we leverage Optimal Transport (OT) courty2016optimal  as an alignment objective to learn representations where the geometric relationships crucial for action prediction are consistent, irrespective of whether the input comes from simulation or the real world. Further more, to robustly handle the data imbalance in co-training with abundant simulation data and limited real-world data, we further extend to an Unbalanced OT (UOT) formulation fatras2021unbalanced ; chizat2018scaling  and develop a temporally-aware sampling strategy to improve domain alignment learning in a mini-batch OT setting.\n\nOur contributions are: (1) a sim-and-real co-training framework that learns a domain-invariant yet task-salient latent space to improve real-world performance with abundant simulation data, (2) an Unbalanced Optimal Transport framework and temporally-aware sampling strategy to mitigate data imbalance and improve alignment quality in mini-batch OT training, (3) comprehensive experiments using both image and point-cloud modalities, evaluating sim-to-sim and sim-to-real transfer across diverse manipulation tasks, demonstrating up to a 30% average success rate improvement and achieving generalization to real-world scenarios for which the training data only appears in simulation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效地将模拟数据中的策略转移到真实世界中。  \n2. 如何处理模拟与真实世界之间的域间差距。  \n3. 如何在有限的真实世界示例下利用丰富的模拟数据进行训练。  \n\n【用了什么创新的方案】  \n提出了一种统一的sim-and-real共训练框架，通过学习一个域不变且任务相关的特征空间，来提高机器人操作策略的可迁移性。核心思想是对观察和对应动作的联合分布进行对齐，利用Optimal Transport (OT)作为对齐目标，确保无论输入来自模拟还是现实，几何关系在动作预测中保持一致。此外，扩展到Unbalanced OT框架以处理数据不平衡，并开发了时间感知采样策略，以改善小批量OT训练中的域对齐学习。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving",
            "authors": "Jay Patrikar,Apoorva Sharma,Sushant Veer,Boyi Li,Sebastian Scherer,Marco Pavone",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "8 pages, 5 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.18626",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18626",
            "arxiv_html_link": "https://arxiv.org/html/2509.18626v1",
            "abstract": "Learning-based autonomous driving systems are trained mostly on incident-free data, offering little guidance near safety–performance boundaries. Real crash reports contain precisely the contrastive evidence needed, but they are hard to use: narratives are unstructured, third-person, and poorly grounded to sensor views. We address these challenges by normalizing crash narratives to ego-centric language and converting both logs and crashes into a unified scene–action representation suitable for retrieval. At decision time, our system adjudicates proposed actions by retrieving relevant precedents from this unified index; an agentic counterfactual extension proposes plausible alternatives, retrieves for each, and reasons across outcomes before deciding. On a nuScenes benchmark, precedent retrieval substantially improves calibration, with recall on contextually preferred actions rising from 24% to 53%. The counterfactual variant preserves these gains while sharpening decisions near risk.",
            "introduction": "End-to-end learning-based autonomous vehicle (AV) systems are trained primarily through imitation learning on positive, incident-free driving data [1, 2].\nThis data is typically collected by expert human drivers driving sensor-instrumented vehicles in a variety of driving scenarios, resulting in a dataset pairing the sensor observations that the AV will encounter with the action that the human driver chose in that moment.\nWhile this data helps define “good” driving that an AV should imitate, it does not provide direct supervision of what behaviors are to be avoided.\nSome have aimed to address this gap through auxiliary reward functions defining a rules-based definition of risky driving [3], but such rules can be challenging to specify: Risk is difficult to quantify due to uncertainty over other road user’s behaviors. Moreover, competent driving requires appropriately managing the risk that is taken on to make progress; remaining stopped is the safest policy, but not competent driving behavior.\n\nInstead, in this work, we consider an alternative data-driven approach to provide negative supervision for AV decision making. Specifically, we explore the use of crash reports as a complementary source of driving knowledge.\nAgencies such as the National Highway Traffic Safety Administration (NHTSA) collect structured narrative accounts of real-world accidents, including the actions taken and the conditions under which failures occurred. While these reports lack the rich multimodal data of first-person human-driven AV logs, they contain valuable causal and contextual information that can support counterfactual reasoning. While these reports can’t directly be used in policy training, recent advances in vision-language models (VLMs) capable of reasoning across sensor and text domains offer a compelling avenue for bringing such valuable sources of negative data into AV decision making.\n\nIn this paper, we study how negative data influences VLM reasoning capabilities in AV decision making tasks by developing a retrieval-augmented-generation (RAG) pipeline for AV safety adjudication. Specifically our contributions are as follows:\n(i) a GraphRAG [4] style retrieval pipeline for both positive and negative driving precedent, using a unified structured language representation for both sensor-domain positive data and language-domain negative data; (ii) an agentic extension which uses additional test-time compute to reason about counterfactuals prior to making a safety judgment; (iii) evaluation of both approaches in terms of alignment with human judgement on the safety of possible actions in driving scenarios, showing the impact of negative crash report data on VLM decision making capability.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用负数据（如事故报告）来改善自动驾驶系统的决策能力。  \n2. 如何将非结构化的事故叙述转换为可用于自动驾驶的结构化信息。  \n3. 如何在决策时使用检索机制来提高安全性判断的准确性。  \n\n【用了什么创新的方案】  \n本研究提出了一种基于检索增强生成（RAG）管道的方法，利用事故报告作为负监督数据来提升自动驾驶决策的安全性。通过将正负驾驶先例统一为结构化语言表示，构建了一个GraphRAG风格的检索管道。同时，增加了一个代理扩展，在决策前进行反事实推理，以提高与人类判断的一致性，并评估了负数据对视觉语言模型（VLM）决策能力的影响。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones",
            "authors": "Maximilian Adang,JunEn Low,Ola Shorinwa,Mac Schwager",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18610",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18610",
            "arxiv_html_link": "https://arxiv.org/html/2509.18610v1",
            "abstract": "Large vision-language models have driven remarkable progress in open-vocabulary robot policies, e.g., generalist robot manipulation policies, that enable robots to complete complex tasks specified in natural language. Despite these successes, open-vocabulary autonomous drone navigation remains an unsolved challenge due to the scarcity of large-scale demonstrations, real-time control demands of drones for stabilization, and lack of reliable external pose estimation modules.\nIn this work,\nwe present SINGER for language-guided autonomous drone navigation in the open world using only onboard sensing and compute. To train robust, open-vocabulary navigation policies, SINGER leverages three central components: (i) a photorealistic language-embedded flight simulator with minimal sim-to-real gap using Gaussian Splatting for efficient data generation, (ii) an RRT-inspired multi-trajectory generation expert for collision-free navigation demonstrations, and these are used to train (iii) a lightweight end-to-end visuomotor policy for real-time closed-loop control.\nThrough extensive hardware flight experiments, we demonstrate superior zero-shot sim-to-real transfer of our policy to unseen environments and unseen language-conditioned goal objects. When trained on ∼\\sim700k-1M observation action pairs of language conditioned visuomotor data and deployed on hardware, SINGER outperforms a velocity-controlled semantic guidance baseline by reaching the query 23.33%\\mathbf{23.33\\%} more on average, and maintains the query in the field of view 16.67%\\mathbf{16.67\\%} more on average, with 𝟏𝟎%\\mathbf{10\\%} fewer collisions.",
            "introduction": "Everyday, humans demonstrate notable semantic and physical understanding of their environments. For example, given a task to go to a specified location, a person relatively easily transforms the language instruction into a physical goal location using semantic cues and navigates to the desired location, safely avoiding collisions.\nAlthough autonomous drones excel at agile flight, they are often limited to controlled environments with pre-specified goal locations. In this work, we ask the question: “Can we train a vision-language drone navigation policy to reach previously unseen goal objects in a previously unseen environment using only on board sensing and compute?”\n\nAdvances in diffusion policies [1] and vision-language-action (VLA) models [2, 3] have led to significant research breakthroughs in robot policy learning from expert demonstration via imitation, particularly in robot manipulation.\nSpecifically, leveraging imitation learning on large-scale robot manipulation datasets [4, 5], state-of-the-art policies endow robots with the requisite task understanding and planning capabilities necessary to perform complex tasks entirely from task descriptions provided in natural language, e.g., to “pick up the apple and place it on a plate.”\nHowever, this paradigm has been largely unsuccessful in drone navigation, due to scarcity of large-scale drone navigation datasets, and effective semantic distillation methods for open-world drone navigation. This is exacerbated by inherent challenges in collecting large quantities of high quality visuomotor data on highly dynamic and naturally unstable drones.\n\nTo address the data scarcity challenge, prior work [6, 7] trains visuomotor policies for drone navigation in simulation, but the effectiveness of the resulting policies are often limited by the non-negligible sim-to-real gap. SOUS-VIDE [8] introduces FiGS, a high-fidelity Gaussian-Splatting-based drone simulator to narrow the sim-to-real gap for stronger real-world transfer; however, FiGS lacks the semantic knowledge required for open-world drone navigation, limiting its deployment to only environments and trajectories seen during training.\n\nIn this paper, we introduce SINGER (Semantic In-situ Navigation and Guidance for Embodied Robots), a pipeline for training language-conditioned drone navigation policies addressing the aforementioned limitations. SINGER consists of three central components: (i) a semantics-rich photorealistic flight simulator based on 3D Gaussian Splatting for efficient data generation with expert demonstrations,\n(ii) a high-level rapidly exploring random trees (RRT*) based planner that efficiently computes spatially spanning collision-free paths to a language-specified goal by time-inverting an expanded tree, and\n(iii) a robust low-level visuomotor policy that tracks the resulting high-level plans with real-time feedback.\nWith these components, SINGER trains a lightweight viusal policy that runs onboard a drone in real-time for online navigation given a natural-language goal object.\n\nTo build an effective flight simulator, we blend the high-fidelity scene-reconstruction capabilities of Gaussian Splatting [9] with the generalizable open-world vision-language semantic features computed by CLIP [10], achieving minimal sim-to-real gap.\nThis core design choice underpins SINGER’s strong zero-shot generalization capabilities to unseen tasks and environments at inference time. In particular, by abstracting goal specification to a semantic (vision-language) space, SINGER effectively aligns a small dataset of synthetic expert trajectories with a broad set of tasks, yielding a data-efficient training scheme for robust visuomotor policies. We augment this training approach with domain randomization for added robustness.\n\nAt deployment, we inference CLIPSeg [11] to produce open-vocabulary semantic images of the environment as conditioning inputs, processed by an end-to-end visuomotor drone policy for low-level drone commands.\n\nThrough our experiments, we show that SINGER outperforms baseline methods in achieving sub-meter proximity to goal by 23.33%23.33\\% with 10%10\\% less collisions and keeping the query in the field of view 16.67%16.67\\% more often without relying on external pose estimation or map-based navigation methods.\n\nWe summarize our contributions as follows:\n\nWe introduce a high-fidelity drone simulator for efficient imitation learning in language-specified drone navigation problems built on language embedded Gaussian Splatting.\n\nWe design a RRT* trajectory planner that efficiently finds thousands of collision-free feasible trajectories across multiple Gaussian Splatting scenes and multiple semantic classes, used to produce large quantities of data for training a generalist policy.\n\nWe present a real-time, lightweight, low-level visual policy architecture for language guided drone navigation using onboard sensing and compute.\n\nUsing these components, we train robust visuomotor policies for drone guidance given a natural language goal specification that generalizes to never before seen environments and semantic queries.\n\n1. We introduce a high-fidelity drone simulator for efficient imitation learning in language-specified drone navigation problems built on language embedded Gaussian Splatting.\n\n2. We design a RRT* trajectory planner that efficiently finds thousands of collision-free feasible trajectories across multiple Gaussian Splatting scenes and multiple semantic classes, used to produce large quantities of data for training a generalist policy.\n\n3. We present a real-time, lightweight, low-level visual policy architecture for language guided drone navigation using onboard sensing and compute.\n\n4. Using these components, we train robust visuomotor policies for drone guidance given a natural language goal specification that generalizes to never before seen environments and semantic queries.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现无人机在开放环境中的语言引导导航？  \n2. 如何解决无人机导航中的数据稀缺和sim-to-real差距问题？  \n3. 如何在无人机上实现实时的低级视觉控制策略？  \n\n【用了什么创新的方案】  \nSINGER提出了一种新的无人机导航策略，通过三个核心组件实现语言引导的自主导航：首先，利用基于3D Gaussian Splatting的高保真飞行模拟器生成高效的专家演示数据；其次，设计了一种基于RRT*的多轨迹生成规划器，以确保碰撞自由的导航；最后，开发了一种轻量级的端到端视觉运动策略，实现实时闭环控制。该方法通过在未见环境和目标对象上的实验，展示了优越的零-shot sim-to-real迁移能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving",
            "authors": "Chengran Yuan,Zijian Lu,Zhanqi Zhang,Yimin Zhao,Zefan Huang,Shuo Sun,Jiawei Sun,Jiahui Li,Christina Dao Wen Lee,Dongen Li,Marcelo H. Ang Jr",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18609",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18609",
            "arxiv_html_link": "https://arxiv.org/html/2509.18609v1",
            "abstract": "End-to-end motion planning is promising for simplifying complex autonomous driving pipelines. However, challenges such as scene understanding and effective prediction for decision-making continue to present substantial obstacles to its large-scale deployment. In this paper, we present PIE, a pioneering framework that integrates advanced perception, reasoning, and intention modeling to dynamically capture interactions between the ego vehicle and surrounding agents. It incorporates a bidirectional Mamba fusion that addresses data compression losses in multimodal fusion of camera and LiDAR inputs, alongside a novel reasoning-enhanced decoder integrating Mamba and Mixture-of-Experts to facilitate scene-compliant anchor selection and optimize adaptive trajectory inference. PIE adopts an action-motion interaction module to effectively utilize state predictions of surrounding agents to refine ego planning. The proposed framework is thoroughly validated on the NAVSIM benchmark. PIE, without using any ensemble and data augmentation techniques, achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of prior state-of-the-art methods. Comprehensive quantitative and qualitative analyses demonstrate that PIE is capable of reliably generating feasible and high-quality ego trajectories.",
            "introduction": "End-to-end motion planning has emerged as a promising paradigm for general robotic systems, including autonomous vehicles (AVs). This data-driven approach has the potential to enable AVs to handle complex and previously unseen scenarios, a capability that becomes increasingly critical as urban environments grow denser and more intricate. By leveraging sensor data directly, end-to-end methods [1, 2, 3] aim to consolidate the traditionally segmented autonomy pipeline—encompassing perception, prediction, and planning—into a single, cohesive framework.\n\nDespite the promising performance of end-to-end methods, several key challenges remain. First, fusing multimodal data (e.g., image and LiDAR inputs) often leads to compression-induced losses when reducing historical information or fusing features from different sources. Second, while data-driven approaches offer the potential for enhancing environmental understanding, the complexity of real-world driving requires more sophisticated models that are capable of both reasoning and dynamically adjusting their strategies. Third, incorporating the predictions of other traffic participants into the end-to-end planning pipeline often introduces substantial computational overhead. Developing efficient methods to seamlessly integrate these predictions into the planning process remains an open challenge, presenting considerable opportunities for further advancements.\n\nTo address these issues, we present PIE, an encoder-decoder framework designed to model the interaction between the action of ego vehicle and the motion of nearby agents and to enable more nuanced reasoning about the driving environment. Our approach mitigates data loss and integrates prediction and planning effectively. The contributions of this work are threefold:\n\nBidirectional Mamba Fusion We introduce a bidirectional Mamba fusion that effectively improves the multimodal data fusion between camera and LiDAR. A notable improvement of 1.9 PDM score can be achieved by merely employing this fusion approach based on the Transfuser backbone.\n\nBidirectional Mamba Fusion We introduce a bidirectional Mamba fusion that effectively improves the multimodal data fusion between camera and LiDAR. A notable improvement of 1.9 PDM score can be achieved by merely employing this fusion approach based on the Transfuser backbone.\n\nReasoning-Enhanced Decoder To improve scene reasoning in complex driving scenarios, we design an efficient decoder integrating the MoE, harnessing Mamba to enhance trajectory generation.\n\nAction-Motion Interaction We propose an action-motion interaction module via a shared cross-attention that directly integrates the velocity predictions of surrounding agents into ego action to model the dynamic interactions between traffic users.\n\nOur approach surpasses the previous state-of-the-art DiffusionDrive [4] by achieving an 88.9 PDM score and 85.6 EPDM score on the NAVSIM navtest split, demonstrating the superiority and effectiveness of the proposed modules.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效融合多模态数据（如相机和LiDAR）以减少压缩损失？  \n2. 如何在复杂的驾驶场景中实现更高效的推理和动态策略调整？  \n3. 如何将其他交通参与者的预测无缝整合进端到端规划流程？  \n\n【用了什么创新的方案】  \n核心解决方案：本文提出了PIE框架，通过引入双向Mamba融合技术来改善相机和LiDAR数据的多模态融合，减少数据压缩损失。设计了一个集成Mixture-of-Experts的推理增强解码器，以提高复杂驾驶场景的推理能力。同时，提出了一个动作-运动交互模块，通过共享的交叉注意力机制，将周围代理的速度预测直接整合到自我动作中，从而更好地建模交通用户之间的动态交互。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning",
            "authors": "Ana Luiza Mineiro,Francisco Affonso,Marcelo Becker",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Accepted to the 22nd International Conference on Advanced Robotics (ICAR 2025). 7 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18608",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18608",
            "arxiv_html_link": "https://arxiv.org/html/2509.18608v1",
            "abstract": "Reliable navigation in under-canopy agricultural environments remains a challenge due to GNSS unreliability, cluttered rows, and variable lighting. To address these limitations, we present an end-to-end learning-based navigation system that maps raw 3D LiDAR data directly to control commands using a deep reinforcement learning policy trained entirely in simulation. Our method includes a voxel-based downsampling strategy that reduces LiDAR input size by 95.83%, enabling efficient policy learning without relying on labeled datasets or manually designed control interfaces. The policy was validated in simulation, achieving a 100% success rate in straight-row plantations and showing a gradual decline in performance as row curvature increased, tested across varying sinusoidal frequencies and amplitudes.",
            "introduction": "Autonomous robots have seen significant growth in modern agriculture, particularly for under-canopy tasks such as plant phenotyping, crop row harvesting, and disease scouting. These applications require platforms that are not only compact and agile but also capable of accurately navigating between dense crop rows (Fig. 1) [1]. However, reliable navigation in such environments remains an active area of research due to several challenges, including clutter and occlusions caused by narrow row spacing and the high visual variability introduced by different plant growth stages [2].\n\nTo enable decision-making systems capable of navigating through plantations in under-canopy environments, robots typically rely on exteroceptive sensors [3]. This choice is driven by the unreliability of GNSS-based localization in such environments, where signal degradation is common due to foliage occlusion [4]. As a result, there is a need for local perception strategies that allow the robot to infer navigation information from its immediate surroundings.\n\nRecent methods have adopted learning-based approaches that aim to extract row-following features directly from curated and labeled datasets, using cameras or LiDAR sensors as the primary source of perception data. On one hand, cameras provide rich semantic and textural information; however, their performance often degrades in low-light conditions, which are common in dense canopy environments [5, 6]. On the other hand, LiDAR offers accurate depth measurements and is unaffected by lighting variations, making it more robust in shaded or poorly lit areas [7, 8]. Additionally, while a sim-to-real gap exists for LiDAR data, the structural consistency of point clouds tends to generalize well across training scenarios. This characteristic allows learning approaches based on point cloud data to effectively leverage large-scale simulated datasets [9].\n\nIn addition, most of these methods typically divide the navigation task into separate modules (e.g., perception, locomotion controller), which introduces challenges in designing effective interfaces between them [10]. As a result, important environmental features may be lost or overly simplified when translated into reference values for the controller. Combined with the high cost and impracticality of collecting labeled data across diverse agricultural conditions, these limitations hinder generalization and reduce overall system robustness.\n\nFor instance, to address the limitations of curated datasets and the challenge of designing interfaces between navigation modules, deep reinforcement learning (RL) can be explored. This approach enables agents to learn end-to-end policies by mapping perceptual observations directly to navigation actions through interaction with the environment. The control policy is optimized via trial-and-error, guided by reward functions that encode the desired task [11, 12].\n\nIn this paper, we present an end-to-end row-following navigation system for under-canopy agricultural environments that maps raw 3D LiDAR data directly to control commands using a RL policy trained in simulation. Our approach avoids the need for manually designed interfaces between navigation modules and does not rely on expert-labeled datasets. Additionally, we propose a LiDAR downsampling technique that transforms raw point clouds into compact row maps, significantly reducing the observation space while preserving essential structural features for learning.\n\nThe key contributions of this work are:\n\nAn end-to-end learning system for crop row-following using only raw 3D LiDAR data.\n\nA voxel-based downsampling technique to convert LiDAR point clouds into compact 2D row maps.\n\n1. An end-to-end learning system for crop row-following using only raw 3D LiDAR data.\n\n2. A voxel-based downsampling technique to convert LiDAR point clouds into compact 2D row maps.",
            "llm_summary": "【关注的是什么问题】  \n1. 在农业环境中，如何实现可靠的导航以应对GNSS不可靠性和环境复杂性？  \n2. 如何有效利用3D LiDAR数据进行自主导航而不依赖于标注数据或手动设计的控制接口？  \n3. 如何通过深度强化学习优化导航策略以提高系统的鲁棒性和通用性？  \n\n【用了什么创新的方案】  \n本研究提出了一种端到端的学习系统，利用原始3D LiDAR数据直接映射到控制命令，采用深度强化学习策略在模拟环境中训练。通过引入体素下采样技术，将LiDAR点云转换为紧凑的2D行地图，显著减少了观察空间，同时保留了学习所需的结构特征，从而避免了手动设计模块接口的需求。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
            "authors": "Neel P. Bhatt,Yunhao Yang,Rohan Siva,Pranay Samineni,Daniel Milan,Zhangyang Wang,Ufuk Topcu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Systems and Control (eess.SY)",
            "comment": "Codebase, datasets, and videos for VLN-Zero are available at:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18592",
            "code": "https://vln-zero.github.io/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18592",
            "arxiv_html_link": "https://arxiv.org/html/2509.18592v1",
            "abstract": "Rapid adaptation in unseen environments is essential for scalable real-world autonomy, yet existing approaches rely on exhaustive exploration or rigid navigation policies that fail to generalize. We present VLN-Zero, a two-phase vision-language navigation framework that leverages vision-language models to efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic navigation. In the exploration phase, structured prompts guide VLM-based search toward informative and diverse trajectories, yielding compact scene graph representations. In the deployment phase, a neurosymbolic planner reasons over the scene graph and environmental observations to generate executable plans, while a cache-enabled execution module accelerates adaptation by reusing previously computed task–location trajectories. By combining rapid exploration, symbolic reasoning, and cache-enabled execution, the proposed framework overcomes the computational inefficiency and poor generalization of prior vision-language navigation methods, enabling robust and scalable decision-making in unseen environments. VLN-Zero achieves 2x higher success rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned baselines, and reaches goal locations in half the time with 55% fewer VLM calls on average compared to state-of-the-art models across diverse environments.††The full codebase, datasets, and videos for VLN-Zero are available at https://vln-zero.github.io/.",
            "introduction": "Deploying autonomous agents in new environments remains a fundamental challenge: policies trained in one setting often fail in another due to novel layouts, obstacles, or constraints, and consequently require fine-tuning or multi-shot inference.\nFor example, a robot trained to navigate one office building may struggle in a different building unless retrained, a process that is both slow and impractical for rapid deployment.\n\nThe central problem is twofold: (i) How can an agent efficiently construct a representation of an unseen environment, such as a scene graph, without exhaustive search?\n(ii) Given this representation, how can the agent efficiently generate constraint-satisfying plans in real time without fine-tuning or multi-shot inference?\n\nExisting approaches, ranging from frontier-based exploration to reinforcement learning with fixed policies, struggle with either computational inefficiency, lack of generalization, or both. Vision-language navigation models (VLNs) offer promise; however, current approaches suffer from slow, exhaustive exploration, weak task decomposition, and high training and query cost.\n\nWe argue that addressing these limitations requires rethinking the interaction between perception, symbolic reasoning, and policy adaptation. Specifically, agents must be able to (i) rapidly acquire symbolic representations of their environment to minimize exploration cost, and (ii) leverage these representations for efficient navigation in new environments without retraining or extensive fine-tuning.\n\nTo address these challenges, we introduce VLN-Zero, a two-phase zero-shot framework that combines vision-language model (VLM) guided exploration with neurosymbolic navigation. In the exploration phase, the agent interacts with the environment using structured and compositional task prompts, guiding exploration toward informative and diverse trajectories to construct a compact scene graph with semantic area labels. In the deployment phase, a neurosymbolic planner reasons over this scene graph, environmental observations to generate executable plans, eliminating reliance on fixed policies. To further improve scalability, we propose a cache-enabled execution procedure that stores previously computed task–location trajectories for reuse, accelerating both exploration and deployment.\n\nIn summary, VLN-Zero offers three key contributions:\n\nVLM-guided rapid exploration: We design structured, compositional prompts that steer a VLN agent to propose exploration actions while incrementally constructing compact symbolic scene graphs. This enables coverage of novel environments within a time- and compute-constrained exploration budget while avoiding unsafe behaviors.\n\nZero-shot neurosymbolic navigation: We introduce a planner that reasons jointly over\nscene graphs, task prompts, and real-time observations, transforming free-form natural language\ninstructions into constraint-satisfying action sequences without fine-tuning or multi-shot inference.\n\nCache-enabled execution for fast adaptation: We develop a trajectory-level caching mechanism\nthat stores validated task–location pairs, allowing the system to reuse previously computed plans.\nThis reduces redundant VLM queries to minimize execution time, cost, and compute demands which accelerates real-world deployment.\n\n1. VLM-guided rapid exploration: We design structured, compositional prompts that steer a VLN agent to propose exploration actions while incrementally constructing compact symbolic scene graphs. This enables coverage of novel environments within a time- and compute-constrained exploration budget while avoiding unsafe behaviors.\n\n2. Zero-shot neurosymbolic navigation: We introduce a planner that reasons jointly over\nscene graphs, task prompts, and real-time observations, transforming free-form natural language\ninstructions into constraint-satisfying action sequences without fine-tuning or multi-shot inference.\n\n3. Cache-enabled execution for fast adaptation: We develop a trajectory-level caching mechanism\nthat stores validated task–location pairs, allowing the system to reuse previously computed plans.\nThis reduces redundant VLM queries to minimize execution time, cost, and compute demands which accelerates real-world deployment.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在未见环境中高效构建场景图表示，而不依赖于耗时的探索？  \n2. 如何在实时生成约束满足的导航计划，而无需进行微调或多次推理？  \n\n【用了什么创新的方案】  \n核心解决方案：VLN-Zero是一个两阶段的零-shot框架，结合了基于视觉语言模型（VLM）的快速探索和神经符号导航。在探索阶段，使用结构化提示引导VLM进行有效的探索，构建紧凑的场景图。在部署阶段，神经符号规划器基于场景图和环境观察生成可执行计划，同时通过缓存机制加速适应，重用先前计算的任务-位置轨迹，从而提高决策效率和可扩展性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA",
            "authors": "Zeyi Kang(1),Liang He(2),Yanxin Zhang(3),Zuheng Ming(4),Kaixing Zhao(5) ((1) Northwestern Polytechnical University, (2) University Sorbonne Paris Nord)",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18576",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18576",
            "arxiv_html_link": "https://arxiv.org/html/2509.18576v1",
            "abstract": "Multimodal semantic learning plays a critical role in embodied intelligence, especially when robots perceive their surroundings, understand human instructions, and make intelligent decisions. However, the field faces technical challenges such as effective fusion of heterogeneous data and computational efficiency in resource-constrained environments. To address these challenges, this study proposes the lightweight LCMF cascaded attention framework, introducing a multi-level cross-modal parameter sharing mechanism into the Mamba module. By integrating the advantages of Cross-Attention and Selective parameter-sharing State Space Models (SSMs), the framework achieves efficient fusion of heterogeneous modalities and semantic complementary alignment. Experimental results show that LCMF surpasses existing multimodal baselines with an accuracy of 74.29% in VQA tasks and achieves competitive mid-tier performance within the distribution cluster of Large Language Model Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a 4.35-fold reduction in FLOPs relative to the average of comparable baselines while using only 166.51M parameters (image-text) and 219M parameters (video-text), providing an efficient solution for Human-Robot Interaction (HRI) applications in resource-constrained scenarios with strong multimodal decision generalization capabilities.",
            "introduction": "In contemporary research, powerful multimodal understanding capabilities have emerged as the foundational element for enabling robotic perception, cognition, and interaction within complex dynamic environments[1]. In the domain of embodied intelligence, Vision-Language Pre-training (VLP) [2] has advanced into a critical technological paradigm for the development of sophisticated intelligent robotic systems, offering substantial support for the realization of more intelligent HRI [3]. Concurrently, to address the challenges inherent in multimodal learning, such as the scarcity of labeled data and the prohibitive costs of annotation, self-supervised learning [4] has garnered considerable attention and research focus. More precisely, by setting multi-task optimization objectives (multimodal masked modeling, contrastive learning, etc.) [5], these methods provide possibilities for common robotic tasks, such as environmental understanding [6], decision-making (Visual Question Answering (VQA) [7], Embodied Question Answering (EQA) [8]) or even more advanced cross-modal general understanding [9].\n\nHowever, embodied intelligence [10] still faces numerous challenges that limit learning capabilities in visual-language decision tasks. At the semantic understanding level, current models [11, 12, 13] struggle to reconstruct fine-grained mask labels, resulting in an information gap between local features, mask features, and global scene understanding. In addition, the efficiency problem of long sequence modeling cannot be ignored, as the Transformer architecture’s computational complexity grows quadratically when processing large-scale sequence data [14, 15], making it difficult to achieve optimal trade-offs between cross-modal understanding performance and hardware efficiency.\n\nIn response to the above challenges, this paper proposes the lightweight LCMF architecture, which achieves high-quality multimodal understanding and inference acceleration on low-computation robotic platforms. LCMF uses a semantic diffusion mechanism [11] to address the information gap in multi-scale visual semantics and enhance the ability to model fine-grained masked information. For cross-modal interaction, Cross-Modality Mamba (CMM) extends the Mamba state-space model to the multimodal domain, achieving comprehensive optimization in hardware awareness, time efficiency, and lightweight design. At the level of multimodal semantic fusion, Enhanced Mamba Fusion (EMF) introduces efficient semantic bridging mechanisms and fine-grained feature modulation techniques, enabling the effective integration of heterogeneous modality semantics.\n\nIn summary, our contributions include:\n\n1) CMM implements multi-level sharing of state space parameters and parallel modeling of multimodal long sequence semantics, achieving linear computational complexity and inference acceleration.\n\n2) LCMF has implemented a lightweight Mamba-Transformer (Selective SSMs-Attention) architecture in the fields of unimodal feature extraction, multimodal (image, text, video) interaction, and multimodal fusion.\n\n3) Under a significantly reduced parameter scale compared to existing multimodal baselines and LLM Agents, LCMF achieves improved computational efficiency while maintaining strong performance on downstream tasks such as VQA and EQA, demonstrating its effectiveness in efficient multimodal modeling.\n\nThe rest of this article is organized as follows.\n\nSection II reviews research on Mamba variant architecture, VQA, EQA.\nSection III details the LCMF model architecture, pretraining, fine-tuning, and evaluation methods.\nSection IV describes the experimental setup, performance evaluation, and ablation experiments on specific downstream tasks.\nFinally, Section V concludes this article.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效融合异构数据以提升机器人在复杂环境中的理解能力？  \n2. 如何在资源受限的环境中实现高效的多模态学习和决策？  \n3. 当前模型在视觉语言决策任务中面临的学习能力限制是什么？  \n\n【用了什么创新的方案】  \n本研究提出了轻量级的LCMF框架，通过引入多级跨模态参数共享机制，结合Cross-Attention和选择性状态空间模型（SSMs），实现了异构模态的高效融合和语义互补对齐。该框架在保持较低计算复杂度的同时，显著提升了多模态理解能力，并在视觉问答（VQA）和视频问答（EQA）任务中表现出色，展示了其在资源受限场景下的有效性和强大的多模态决策泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Spatial Envelope MPC: High Performance Driving without a Reference",
            "authors": "Siyuan Yu,Congkai Shen,Yufei Xi,James Dallas,Michael Thompson,John Subosits,Hiroshi Yasuda,Tulga Ersal",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18506",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18506",
            "arxiv_html_link": "https://arxiv.org/html/2509.18506v1",
            "abstract": "This paper presents a novel envelope-based model predictive control (MPC) framework designed to enable autonomous vehicles to handle high-performance driving across a wide range of scenarios without a predefined reference.\nIn high-performance autonomous driving, safe operation at the vehicle’s dynamic limits requires a real-time planning and control framework capable of accounting for key vehicle dynamics and environmental constraints when following a predefined reference trajectory is suboptimal or even infeasible. State-of-the-art planning and control frameworks, however, are predominantly reference-based, which limits their performance in such situations.\nTo address this gap, this work first introduces a computationally efficient vehicle dynamics model tailored for optimization-based control and a continuously differentiable mathematical formulation that accurately captures the entire drivable envelope.\nThis novel model and formulation allow for the direct integration of dynamic feasibility and safety constraints into a unified planning and control framework, thereby removing the necessity for pre-defined references.\nThe challenge of envelope planning, which refers to maximally approximating the safe drivable area, is tackled by combining reinforcement learning with optimization techniques.\nThe framework is validated through both simulations and real-world experiments, demonstrating its high performance across a variety of tasks, including racing, emergency collision avoidance and off-road navigation.\nThese results highlight the framework’s scalability and broad applicability across a diverse set of scenarios.",
            "introduction": "High-performance autonomous driving technology has advanced rapidly and significantly in the past decade [1].\nThe grand opening of the first autonomous racing competition has thrust autonomous driving technology into the spotlight, showcasing performance that rivals expert human drivers in tasks demanding high levels of precision and speed [2]. Recent developments in autonomous driving have enabled reliable performance in advanced collision avoidance capabilities [3, 4, 5], complex drifting maneuvers [6, 7, 8], and challenging off-road navigation [9, 10, 11].\nThese advancements have greatly expanded the potential applications of autonomous vehicles across various fields. To execute these extreme maneuvers safely, the vehicle must operate at the limits of its capabilities.\nHowever, this is a challenge, because even a small deviation from the desired trajectory can result in catastrophic outcomes.\n\nIn this regard, the autonomous system should be designed to effectively utilize all feasible operational regions including the limits, ensuring performance is not unnecessarily sacrificed for safety via overly conservative constraints.\nFor instance, every inch of the race track is critical for drivers to adjust their strategies and enhance their performance. In addition, the algorithm’s design should ensure scalability when applied to a diverse set of scenarios with varying levels of complexity.\nIn this context, the term ‘scalability’ refers to the algorithm’s ability to handle an increasing amount of tasks or scenarios without a significant compromise in performance or without significant redesign.\nIt is also desirable that the system be capable of generating optimal trajectories online in real-time without having to depend on a predefined reference, because deviations from the original plan may render the predefined reference suboptimal or even infeasible.\n\nHowever, as the literature review in Sec. II reveals, existing methods typically rely on predefined references and therefore either limit vehicle performance to the quality of that reference or do not provide optimal and scalable solutions in performance-demanding scenarios.\n\nTo address this gap, this paper presents a novel, spatial envelope model predictive control (MPC) framework for reference-free high performance driving.\nThe proposed framework builds upon a new, computationally efficient vehicle dynamics model tailored for closed-loop optimization based planning and control, capturing the essential dynamics required for aggressive maneuvers.\nA twice continuously differentiable mathematical formulation of the entire driving envelope is introduced to conservatively estimate the drivable region to be used in MPC.\nThis enables the MPC to break from the restrictive constant-speed assumption previously used in spatial envelope MPC [12, 13, 14, 5], and instead optimize speed, as well, to maximize performance while maintaining safety.\n\nTo the authors’ knowledge, this is the first published MPC algorithm that is experimentally validated for safe and effective high-performance driving at the handling limits in a fully reference-free setting.\nThe algorithm is validated across a wide range of scenarios, including racing, off-road navigation and emergency collision avoidance, demonstrating both generality and real-world applicability.\n\nFinally, a new spatial envelope planning technique is introduced to further enhance applicability. A hybrid approach that combines optimization-based formulation with reinforcement learning is developed to segment the drivable area into blocks, enabling scalable planning in complex environments.\n\nThe original contributions are summarized as follows:\n\nA validated 3-DoF single-track dynamic model that accounts for longitudinal load transfer and the friction circle limit, while remaining computationally efficient in a fully reference-free setting.\n\nA validated 3-DoF single-track dynamic model that accounts for longitudinal load transfer and the friction circle limit, while remaining computationally efficient in a fully reference-free setting.\n\nA hard constraint formulation to mathematically express the spatial envelope with guaranteed conservativeness.\n\nA real-time Model Predictive Control (MPC) formulation that leverages the first two contributions to optimize vehicle trajectories online without any predefined path.\n\nA reinforcement learning approach to design a set of blocks to approximate arbitrary shapes of spatial envelopes in real time.\n\nValidation of the proposed MPC formulation in racing, emergency collision avoidance and off-road environments.\n\nThe rest of the paper is organized as follows. Sec. II reviews the relevant literature. Sec. III-A describes the 3 DoF single-track vehicle dynamics. Sec. III-B describes the MPC formulation including the conservative spatial envelope constraints. Sec. III-C describes the real-time spatial envelope planner. Sec. IV describes the model fidelity test. The results and discussion of the proposed spatial envelope MPC are presented in Sec. IV.\nFrom Sec. V-A to E, the simulation and experimental results of spatial envelope MPC are conducted and analyzed in multiple scenarios.\nIn Sec. III-F, the proposed spatial envelope planning technique is presented and discussed.\nFinally, Sec. VI concludes the study.",
            "llm_summary": "【关注的是什么问题】  \n1. 高性能自动驾驶中缺乏有效的参考轨迹规划与控制方法。  \n2. 现有方法过于依赖预定义参考，限制了车辆性能。  \n3. 如何在复杂环境中实现实时、无参考的轨迹优化。  \n\n【用了什么创新的方案】  \n本研究提出了一种新颖的空间包络模型预测控制（MPC）框架，旨在实现高性能的无参考自动驾驶。该框架结合了高效的车辆动力学模型和连续可微的数学公式，能够准确捕捉可行驶区域。通过将动态可行性和安全约束直接整合到规划和控制框架中，消除了对预定义参考的依赖。此外，采用强化学习与优化技术相结合的方法，能够在复杂环境中进行可扩展的规划。该框架在多种场景下进行了验证，包括赛车、紧急避障和越野导航，展示了其广泛的适用性和高性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain",
            "authors": "Junnosuke Kamohara,Feiyang Wu,Chinmayee Wamorkar,Seth Hutchinson,Ye Zhao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18466",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18466",
            "arxiv_html_link": "https://arxiv.org/html/2509.18466v1",
            "abstract": "Model predictive control (MPC) has demonstrated effectiveness for humanoid bipedal locomotion; however, its applicability in challenging environments, such as rough and slippery terrain, is limited by the difficulty of modeling terrain interactions.\nIn contrast, reinforcement learning (RL) has achieved notable success in training robust locomotion policies over diverse terrain, yet it lacks guarantees of constraint satisfaction and often requires substantial reward shaping.\nRecent efforts in combining MPC and RL have shown promise of taking the best of both worlds, but they are primarily restricted to flat terrain or quadrupedal robots.",
            "introduction": "Legged locomotion conventionally employs model-based controllers (MBCs), particularly Model Predictive Control (MPC), due to their optimization-based constraint satisfaction [1, 2].\nWhile whole-body dynamics models [3] are more accurate,\nresearchers use simplified models [4, 5, 6, 7] for computational efficiency and consequently suffer from model mismatch due to simplification of the dynamics.\nAs a result, simplified models exhibit poorer tracking accuracy and instability, particularly during contact [8].\nAdditionally, MPC with simplified dynamics usually requires predefined contact sequence and swing leg trajectory, which limits its adaptivity to diverse terrains.\nOverall, the deterministic but inaccurate dynamic model and manual constraint design of MPC restrict its robustness and versatility, limiting its applicability to diverse terrains in the real world.\n\nIn contrast, learning-based controls (LBC), exemplified by Reinforcement Learning (RL) methods, have gained wide attention for their robustness and agility [9, 10, 11, 12, 13].\nBy training policies parameterized by neural networks, RL policies can achieve zero-shot transfer from simulation to reality.\nHowever, training robust policies requires substantial environmental interactions and extensive reward shaping.\nFurthermore, RL policies lack explicit constraint satisfaction because of the absence of explicit constraints.\n\nMotivated by the unique advantages of both sides, recent years have witnessed a surge of methods combining model-based and learning-based approaches, leveraging the safety offered by MPC’s explicit constraints as well as powerful reactive behaviors offered by RL [14, 15].\nIn legged robotics, there are two main threads of combination.\nThe first thread uses MPC within a policy.\nRecent works either adopt a hierarchical architecture, where RL parametrizes MPC’s components, including system dynamics, center of mass reference trajectory, and gait frequency [16, 8, 17, 18]; or follows a parallel architecture, where RL policies refine MPC outputs by adding corrective actions such as footholds and joint commands [19, 20, 21].\nAnother thread uses MPC as an expert policy, training the policy through behavior cloning or RL with imitation loss to increase sample efficiency and motion accuracy [22, 13, 23, 24].\nEach of these designs carries trade-offs:\nMPC as an expert improves training efficiency by imitating MPC motions, yet it incurs significant computational overhead during training due to repeated optimization solves, making training in parallelized RL environments particularly challenging [24].\nWhile parallel architectures offer flexibility by directly augmenting MPC outputs, they raise safety concerns since the RL policy bypasses feasibility constraints from optimization.\nHierarchical architectures, in contrast, preserve the optimization structure and computational complexity, as the policy is evaluated before solving the optimization problem.\nThis ensures the feasibility and constraint satisfaction within the optimization framework.\n\nDespite these advances, most combined approaches for bipedal locomotion remain limited to flat terrain, as prior works primarily emphasize improving tracking accuracy rather than adaptability [19, 21], leaving integration of MPC and RL for rough-terrain-adaptive locomotion unexplored.\nIn this work, we aim to enhance the adaptability of humanoid locomotion, enabling responsive and robust behaviors in the face of terrain disturbances.\nWe leverage a hierarchical method that augments MPC via RL by incorporating rich whole-body information into the simplified system model, adjusting the gait frequency to modulate step length, and modifying the swing foot trajectory to improve robustness against challenging terrain.\nWe focus on addressing the limitations of MPC with simplified dynamics: model mismatch, predefined swing leg curve, and static gait frequency.\nThe RL policy learns residual dynamics through whole-body dynamics simulation, as well as swing leg curve parameters, including apex height and control points, and dynamic gait frequency within one locomotion cycle.\n\nThese learned adaptations enable reactive behaviors, including recovery from foot entrapment and severe slippage.\nWe implement our method on bipedal locomotion tasks with the HECTOR robot [25] in NVIDIA IsaacLab, a state-of-the-art GPU-accelerated simulator [26].\nOur framework significantly improves robustness against disturbances on diverse terrains, including slippery surfaces, stairs, and stepping stones.\nAdditionally, we conduct ablation studies on the three residual modules to analyze the contribution of each component.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高双足机器人在复杂地形上的适应性和鲁棒性。  \n2. 现有的模型预测控制（MPC）在动态建模和适应性方面的局限性。  \n3. 强化学习（RL）在约束满足和训练效率方面的不足。  \n\n【用了什么创新的方案】  \n本研究提出了一种层次化方法，通过将强化学习（RL）与模型预测控制（MPC）相结合，增强了双足机器人在复杂地形上的适应性。该方法利用丰富的全身信息来调整简化系统模型，学习残余动态和摆腿曲线参数，并动态调整步频，从而提高在滑面、楼梯和踏石等多样地形上的鲁棒性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task",
            "authors": "Jannick van Buuren,Roberto Giglio,Loris Roveda,Luka Peternel",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18463",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18463",
            "arxiv_html_link": "https://arxiv.org/html/2509.18463v1",
            "abstract": "This paper explores how deliberate mutations of reward function in reinforcement learning can produce diversified skill variations in robotic manipulation tasks, examined with a liquid pouring use case. To this end, we developed a new reward function mutation framework that is based on applying Gaussian noise to the weights of the different terms in the reward function. Inspired by the cost-benefit tradeoff model from human motor control, we designed the reward function with the following key terms: accuracy, time, and effort. The study was performed in a simulation environment created in NVIDIA Isaac Sim, and the setup included Franka Emika Panda robotic arm holding a glass with a liquid that needed to be poured into a container. The reinforcement learning algorithm was based on Proximal Policy Optimization. We systematically explored how different configurations of mutated weights in the rewards function would affect the learned policy. The resulting policies exhibit a wide range of behaviours: from variations in execution of the originally intended pouring task to novel skills useful for unexpected tasks, such as container rim cleaning, liquid mixing, and watering. This approach offers promising directions for robotic systems to perform diversified learning of specific tasks, while also potentially deriving meaningful skills for future tasks.",
            "introduction": "For robots to successfully operate in unstructured and unpredictable real-world environments, they need the ability to constantly adapt and learn many tasks. One way to do this is to learn from human demonstration [1]. However, human involvement can be costly, and humans are not always available to correct or teach robots new skills. Indeed, an alternative is reinforcement learning (RL) that allows the robots to autonomously acquire new skills through trial‑and‑error interaction with their environment [2]. The robot is given an objective function (typically from a human), which then guides its autonomous exploration to obtain a policy of how to perform a given task. At each timestep, the robot observes the current state, executes an action according to its policy, and receives a scalar reward. Over many episodes, it refines its policy to maximise the expected sum of discounted rewards, thereby acquiring skills optimised for long‑term success.\n\nRL has been successfully applied to robots to solve a diverse range of tasks, ranging from pick-and-place actions [3, 4], object lifting [5], to assembly [6], as well as play ball-in-the-cup game [7], table tennis [8], and air hockey [9]. Within this context, the liquid pouring task [10, 11, 12, 13] stands out as a particularly compelling benchmark for investigating the role of reward function design in shaping learned behaviours. Unlike binary success criteria seen in stacking or placement tasks, pouring involves balancing multiple continuous objectives, such as avoiding spillage, reducing effort, and maximising efficiency, making it highly sensitive to how learning is incentivised.\n\nRL has achieved impressive results in specific robotic tasks with well-crafted and tailored reward functions. This typically results in good skills specialised for the given task, but lacks generalisation capabilities when new tasks arise. Learning new tasks is typically relatively long and sample-inefficient, especially in complex tasks without prior knowledge and where rewards are sparse or delayed. Rather than relying solely on environmental feedback, an agent can benefit from understanding the reward logic itself, such as temporal dependencies, conditional sequences, or subgoals—thereby improving learning efficiency and policy quality [14, 15, 16]. By leveraging structured reward representations, agents can more effectively sequence and reuse behaviours, enabling them to adjust previously learned skills to new or modified tasks. This structured approach facilitates faster adaptation, as agents can generalise from prior experience rather than starting from scratch each time [17].\n\nAnother approach to reduce learning time and improve generalisability is to utilise direct prior task knowledge from models or human demonstrations [18, 2]. In that way, an agent already has a rough policy, which then only needs to be refined and optimised for the given specifics of the robot and the environment. However, resulting policies that are not considered optimal for the given specific task are often discarded. We argue that such “failed” or “suboptimal” policies should not be discarded, since such skill variations resulting from various mutations might be useful starting points for learning new tasks.\n\nMutation of policy in robot learning when subject to physical interaction with humans and unpredictable environments has been observed in [19]. A follow-up study [20] investigated how the policy mutations occur during the learning and what kind of variations of skill emerge in a sawing task. The study concluded that certain policy variations may not be optimal or suitable for the original task, but can be useful for optimising some other parameters/tasks. This highlighted the potential for diversification of skills and the importance of not discarding the policies that appear to be suboptimal for the current task, as they might be a good starting point for unforeseen new tasks. However, the work so far relied on mutations from a passive environment, while intentional mutations with a systematic mechanism are still missing.\n\nTo address this challenge, we introduce a reward mutation framework that treats the reward function as a tunable mechanism for active skill diversification (Fig. 1). While online reward-shaping has been investigated for improvement of sample efficiency and optimisation of a specific task in [21], differently, the proposed approach mutates the reward function to discover new skills. As a starting point, we study a liquid pouring task performed by a Franka Emika Panda robotic manipulator in the NVIDIA Isaac Sim simulation environment, where the reward is composed of three weighted terms: pouring accuracy, time spent, and effort spent. The goal of the study is to systematically explore how the mutation of the weights of these reward terms induces the emergence of diverse policies. We perform training of agents with Proximal Policy Optimization (PPO) [22] under 25 distinct reward configurations. Some of the emerging policies are useful for executing the original task in different ways (fast and slow), some are identified to be useful for unforeseen other tasks (container rim cleaning, liquid mixing, watering), and some are not useful for any identifiable tasks.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何通过变异奖励函数来实现机器人技能的多样化。  \n2. 现有的强化学习方法在新任务学习中的效率和泛化能力不足。  \n3. 如何有效利用“次优”策略作为新任务学习的起点。  \n\n【用了什么创新的方案】  \n本研究提出了一种奖励变异框架，通过对奖励函数中不同权重施加高斯噪声，系统性地探索了不同奖励配置对学习策略的影响。该方法允许机器人在液体倒入任务中产生多样化的技能，从而不仅优化原有任务的执行方式，还为意外任务（如容器边缘清洁、液体混合和浇水）提供了有用的技能。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems",
            "authors": "Haeyoon Han,Mahdi Taheri,Soon-Jo Chung,Fred Y. Hadaegh",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18460",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18460",
            "arxiv_html_link": "https://arxiv.org/html/2509.18460v1",
            "abstract": "Perception systems provide a rich understanding of the environment for autonomous systems, shaping decisions in all downstream modules. Hence, accurate detection and isolation of faults in perception systems is important. Faults in perception systems pose particular challenges: faults are often tied to the perceptual context of the environment, and errors in their multi-stage pipelines can propagate across modules. To address this, we adopt a counterfactual reasoning approach to propose a framework for fault detection and isolation (FDI) in perception systems. As opposed to relying on physical redundancy (i.e., having extra sensors), our approach utilizes analytical redundancy with counterfactual reasoning to construct perception reliability tests as causal outcomes influenced by system states and fault scenarios. Counterfactual reasoning generates reliability test results under hypothesized faults to update the belief over fault hypotheses. We derive both passive and active FDI methods. While the passive FDI can be achieved by belief updates, the active FDI approach is defined as a causal bandit problem, where we utilize Monte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find control inputs that maximize a detection and isolation metric, designated as Effective Information (EI). The mentioned metric quantifies the informativeness of control inputs for FDI. We demonstrate the approach in a robot exploration scenario, where a space robot performing vision-based navigation actively adjusts its attitude to increase EI and correctly isolate faults caused by sensor damage, dynamic scenes, and perceptual degradation.",
            "introduction": "Autonomous systems such as self-driving cars, unmanned aerial vehicles (UAV), and autonomous robots rely on perception systems to convert heterogeneous sensor measurements into a coherent representation of their surrounding environment [1]. The role of the perception system is to provide accurate and timely information on objects, terrain, and the surrounding environment so that higher-level modules in an autonomous system (e.g., localization, motion planning, and control) can guarantee safety and achieve mission objectives [2]. The combination of utilizing heterogeneous sensors (e.g., LiDAR, radar, cameras) and deep learning-based algorithms has led to recent advances in perception-based control. However, this has also resulted in an increased level of complexity in perception systems, which makes detecting their faults and algorithmic errors challenging [3, 4]. Considering the importance of a perception system in the guidance and control of an autonomous system, perception faults can result in the complete loss of a mission. For instance, on 6 June 2025, the Japanese lunar lander Resilience (Hakuto-R Mission 2) had a hard landing during its final descent on the Moon when its laser range finder began outputting erroneous altitude values in the last few kilometers before touchdown [5]. This highlights the need for accurate monitoring systems that can address the problem of fault detection and isolation (FDI) in perception systems.\n\nThe method presented in this paper can handle a broad range of fault and failure types, including both physical malfunctions and algorithmic errors in perception systems that cause deviations from their intended functionality. On the physical side, sensors can suffer calibration shifts, temporary occlusions, and environmental interference [7]. At the algorithmic level, deep neural networks (DNN) can misclassify objects due to distribution shifts (i.e., out-of-distribution inputs), and multi-sensor fusion can become erroneous due to calibration issues [8, 9, 10]. Moreover, faults that occur at an early stage of a perception system’s pipeline propagate through it and do not remain isolated [11]. Hence, FDI methodologies that rely on physical redundancy may not be sufficient [12]. Thus, one needs to study and investigate FDI methodologies based on the available analytical redundancy in perception systems. Once a certain fault is detected and isolated, a fault recovery control can be implemented.\n\nThe faults that occur in Simultaneous Localization and Mapping (SLAM) and Visual Inertial Odometry (VIO) systems are sensor faults [3, 7], tracking failures [15, 16], data association failures [17, 18], and filtering inconsistency problems [19]. Sensor faults are caused by hardware damage or software malfunction. Faults in front-end modules, such as tracking and data association failures, are often caused by visually deprived conditions (i.e., textureless surfaces and repetitive patterns), dynamic scenes (i.e., aggressive camera motion), and undesirable lighting conditions (i.e., high-contrast images). Lastly, the filtering inconsistency problems, a type of fault in back-end modules, result from large inter-frame transformations that trigger the accumulation of linearization errors.\n\nThe work in [8] compares perception outputs with a predefined fault threshold for runtime monitoring. Additionally, [3] developed fault diagnostic graphs to associate errors with individual perception module outputs, as evaluated by diagnostic tests. Although these works enable FDI, they rely on having redundant sensors, which can be costly. To enhance the robustness of SLAM [20] developed image quality metrics to select confident features or scenes. Similarly, feature quality metrics that assess keypoint co-visibility between frames [15, 21, 3] and the dynamic scene metrics that leverage vehicle velocity [15], optical flow [22], and image sharpness [23, 24] have been proposed.\n\nWe define perception reliability tests for various fault modes to capture differences between fault-free and fault-induced behaviors. We utilize the structural causal model (SCM) formalism of Pearl [25] and its operational rules for interventions and counterfactual queries, where we treat each hypothesized fault mode as an intervention on the perception pipeline. We then introduce and define an information-theoretic metric based on the Kullback–Leibler (KL) divergence between the reliability test results and those from a baseline fault-free case to measure the detectability and isolability of the hypothesized faults. This metric, designated as Effective Information (EI), captures how control inputs influence the reliability test results by affecting the autonomous system’s state. To the best of our knowledge, this is the first work that studies the FDI as a counterfactual reasoning problem for a closed-loop autonomous system and also connects the informativeness of control inputs to the detection and isolation of the hypothesized faults. Finally, we show that finding the control input that helps maximizing the EI leads to having a causal bandit problem [26], where each action arm corresponds to an intervention on the control input that improves our FDI accuracy. A Monte-Carlo Tree Search (MCTS) approach with Upper Confidence Bound (UCB) [27, 28] that penalizes large deviations from primary mission objectives (e.g., tracking a trajectory) is employed to solve the mentioned causal bandit problem.\n\nThe main contributions of this paper are as follows.\n\nWe exploit analytical redundancy of the perception system and actively use control inputs for FDI by applying the do-operator from causal inference. This is achieved via a counterfactual reasoning approach, where it is analyzed how control inputs affect reliability test outcomes under various fault hypotheses. A quantitative detection and isolation metric measuring the informativeness of each control input for FDI is introduced.\n\nWe formulate the problem of selecting control inputs for FDI as a causal bandit problem. Using a MCTS strategy with UCB, we maximize a weighted reward function that prioritizes inputs informative about the most likely fault modes. In addition, our reward function penalizes large deviations from the desired trajectory of the system.\n\nOur FDI method uses the distribution of reliability test results under various fault modes and accounts for the uncertainty inherent in the perception system’s outputs. Thus, our method encodes more information than mean value and threshold-based FDI methods, which only reflect the central tendency of a distribution.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在感知系统中准确检测和隔离故障。  \n2. 传统的物理冗余方法在故障检测中的局限性。  \n3. 如何利用反事实推理提高故障检测和隔离的有效性。  \n\n【用了什么创新的方案】  \n本研究提出了一种基于反事实推理的故障检测和隔离（FDI）框架，利用分析冗余而非物理冗余来构建感知可靠性测试。通过生成假设故障下的可靠性测试结果，更新对故障假设的信念。我们将FDI问题建模为因果赌博问题，使用蒙特卡洛树搜索（MCTS）和上置信界（UCB）策略来最大化有效信息（EI），从而优化控制输入以提高故障检测和隔离的准确性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands",
            "authors": "Yunshuang Li,Yiyang Ling,Gaurav S. Sukhatme,Daniel Seita",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18455",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18455",
            "arxiv_html_link": "https://arxiv.org/html/2509.18455v1",
            "abstract": "Nonprehensile manipulation, such as pushing and pulling, enables robots to move, align, or reposition objects that may be difficult to grasp due to their geometry, size, or relationship to the robot or the environment. Much of the existing work in nonprehensile manipulation relies on parallel-jaw grippers or tools such as rods and spatulas. In contrast, multi-fingered dexterous hands offer richer contact modes and versatility for handling diverse objects to provide stable support over the objects, which compensates for the difficulty of modeling the dynamics of nonprehensile manipulation.\nTherefore, we propose Geometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile manipulation with dexterous robotic hands. We study pushing and pulling by framing the problem as synthesizing and learning pre-contact dexterous hand poses that lead to effective manipulation. We generate diverse hand poses via contact-guided sampling, filter them using physics simulation, and train a diffusion model conditioned on object geometry to predict viable poses.\nAt test time, we sample hand poses and use standard motion planners to select and execute pushing and pulling actions.\nWe perform 840 real-world experiments with an Allegro Hand, comparing our method to baselines. The results indicate that GD2P offers a scalable route for training dexterous nonprehensile manipulation policies. We further demonstrate GD2P on a LEAP Hand, highlighting its applicability to different hand morphologies. Our pre-trained models and dataset, including 1.3 million hand poses across 2.3k objects, will be open-source to facilitate further research.\nOur project website is available at: geodex2p.github.io.",
            "introduction": "Nonprehensile actions are fundamental to how humans and robots interact with the physical world [4, 5, 6, 7].\nThese actions permit the manipulation of objects that may be too large, heavy, or geometrically complex to grasp directly.\nWhile there has been tremendous progress in nonprehensile robot manipulation [8, 9, 10, 11, 12], most work uses simple end-effectors such as parallel-jaw grippers, rods [13, 14], or spatulas [15]. In contrast, multi-fingered hands with high degrees-of-freedom (DOF) such as the Allegro Hand or LEAP Hand [16] enable contact patterns that can be especially useful for stabilizing complex, awkward, or top-heavy objects, or for coordinating contact across multiple objects, compensating for the challenges of modeling nonprehensile manipulation dynamics.\nHowever, despite their promise and recent progress [17] [18], leveraging high-DOF hands for nonprehensile manipulation remains relatively underexplored due to the challenges of modeling hand-object relationships and planning feasible contact-rich motions.\n\nIn this paper, we study pushing and pulling objects using the 4-finger, 16-DOF Allegro and LEAP Hands. We select pushing and pulling as representative tasks of nonprehensile manipulation because they are more commonly used for manipulating general daily objects and are\nmore amenable to scaling.\nOur insight is to recast this problem into one of synthesizing effective pre-contact hand poses, an approach inspired by recent success in generating large-scale datasets for dexterous manipulation [19, 20, 21, 22, 23, 24].\nWe propose a scalable pipeline for generating hand poses for pushing and pulling objects. This involves contact-guided optimization and validation via GPU-accelerated physics simulation with IsaacGym [25].\nThese filtered hand poses are then used to train a generative diffusion policy conditioned on object geometry, represented using basis point sets [1].\n\nAt test time, we use visual data to reconstruct an object mesh in physics simulation. The trained diffusion policy uses this mesh to generate diverse hand poses for pushing or pulling. We then validate the resulting hand poses in simulation, and execute the best-performing action in the real world.\nWe call this pipeline Geometry-aware Dexterous Pushing and Pulling (GD2P) with multi-fingered hands.\nFigure LABEL:fig:pull shows several real-world examples where the hand pose differs depending on object geometry. Overall, our experimental results across diverse daily objects demonstrate that GD2P is a promising approach for generalizable object pushing and pulling. It outperforms alternative methods such as querying the nearest hand pose in our data or using a fixed spatula-like hand pose, highlighting the need for a diffusion model to generate diverse hand poses.\n\nTo summarize, the contributions of this paper include:\n\nA scalable pipeline for generating and filtering dexterous hand poses for nonprehensile pushing and pulling.\n\nA diffusion model for geometry-conditioned hand pose prediction for nonprehensile pushing and pulling.\n\nA motion planning framework to execute these poses in the real world, with results across 840 trials showing that GD2P outperforms alternative methods.\n\nA dataset of 1.3 million hand poses for nonprehensile pushing and pulling across 2.3k objects with corresponding canonical point cloud observations.\n\n1. A scalable pipeline for generating and filtering dexterous hand poses for nonprehensile pushing and pulling.\n\n2. A diffusion model for geometry-conditioned hand pose prediction for nonprehensile pushing and pulling.\n\n3. A motion planning framework to execute these poses in the real world, with results across 840 trials showing that GD2P outperforms alternative methods.\n\n4. A dataset of 1.3 million hand poses for nonprehensile pushing and pulling across 2.3k objects with corresponding canonical point cloud observations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用多指灵巧手进行非抓取操作（如推拉）以处理复杂物体。  \n2. 如何生成有效的手部预接触姿势以提高非抓取操作的成功率。  \n3. 如何在真实环境中执行基于物体几何的手部姿势以实现有效的推拉操作。  \n\n【用了什么创新的方案】  \n提出了一种几何感知的灵巧推拉（GD2P）方法，通过接触引导采样生成多样的手部姿势，并利用物理仿真进行过滤。训练一个条件于物体几何的扩散模型来预测可行的手部姿势。测试时，使用标准运动规划器选择并执行推拉动作。该方法在840次真实实验中表现优于基线，展示了其在不同手形态上的适用性，并提供了130万个手部姿势的数据集以促进后续研究。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction",
            "authors": "Rishabh Madan,Jiawei Lin,Mahika Goel,Angchen Xie,Xiaoyu Liang,Marcus Lee,Justin Guo,Pranav N. Thakkar,Rohan Banerjee,Jose Barreiros,Kate Tsui,Tom Silver,Tapomayukh Bhattacharjee",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Conference on Robot Learning (CoRL)",
            "pdf_link": "https://arxiv.org/pdf/2509.18447",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18447",
            "arxiv_html_link": "https://arxiv.org/html/2509.18447v1",
            "abstract": "Physical human–robot interaction (pHRI) requires robots to adapt to individual contact preferences, such as where and how much force is applied. Identifying preferences is difficult for a single contact; with whole-arm interaction involving multiple simultaneous contacts between the robot and human, the challenge is greater because different body parts can impose incompatible force requirements. In caregiving tasks, where contact is frequent and varied, such conflicts are unavoidable. With multiple preferences across multiple contacts, no single solution can satisfy all objectives–trade-offs are inherent, making prioritization essential. We present PrioriTouch, a framework for ranking and executing control objectives across multiple contacts. PrioriTouch can prioritize from a general collection of controllers, making it applicable not only to caregiving scenarios such as bed bathing and dressing but also to broader multi-contact settings. Our method combines a novel learning-to-rank approach with hierarchical operational space control, leveraging simulation-in-the-loop rollouts for data-efficient and safe exploration. We conduct a user study on physical assistance preferences, derive personalized comfort thresholds, and incorporate them into PrioriTouch. We evaluate PrioriTouch through extensive simulation and real-world experiments, demonstrating its ability to adapt to user contact preferences, maintain task performance, and enhance safety and comfort. Website: https://emprise.cs.cornell.edu/prioritouch.",
            "introduction": "Physical human–robot interaction (pHRI) requires physical contact. Contact is not uniform: individuals have distinct preferences for acceptable forces and contact locations [1, 2, 3, 4, 5]. For pHRI to be safe and effective, robots must personalize their behavior, and a critical aspect of personalization is contact preferences. Even for a single contact, identifying and respecting these preferences while ensuring task success is challenging. Many physical robot caregiving tasks, such as bathing [6, 7], dressing [8, 9], and transferring [10], require whole‑arm pHRI [11], where multiple segments of the robot arm simultaneously touch the human body. For example, during bed bathing (Fig. 1), the robot may need to reach over a user to wipe the upper arm while maintaining comfortable forces on the torso and shoulder. Although whole‑arm manipulation expands workspace and improves maneuverability, it also exacerbates conflicts: different body parts can impose incompatible force requirements, and no single policy can satisfy all objectives.\n\nTo bootstrap personalization, we elicit population‑level contact preferences offline and use them to seed a conservative base policy. However, a one‑size‑fits‑all policy is insufficient: (i) stated preferences can diverge from realized comfort under true contact (pressure/shear, approach, speed, duration); and (ii) preferences are context‑dependent and time‑varying (posture, clothing, fatigue). Therefore, online interaction is necessary to accommodate individual preferences. Experimenting directly with the user is risky and inefficient because each update can involve repeated physical contact and multiple feedback exchanges. This increases the user’s cognitive workload, prolongs the interaction, and may cause discomfort when forces are suboptimal or excessive.\n\nWe introduce PrioriTouch, a framework that casts contact preference learning as a learning‑to‑rank problem over control objectives. Given a reference trajectory produced by a high‑level policy (e.g., a contact‑aware planner generating end‑effector or joint‑space paths), PrioriTouch instantiates pose‑tracking and force‑regulation objectives from the current contact state. We develop LinUCB‑Rank, a contextual bandit that learns a priority policy; H‑OSC [12] then executes this ordering as a null space hierarchy, translating high‑level preferences into low‑level control. We initialize the policy with conservative priors derived from population-level user-study statistics. During interaction, LinUCB-Rank adapts the ordering online using sparse user feedback while safely refining the policy via simulation-in-the-loop learning before deploying it in real-world interactions. The framework is controller-agnostic: it can rank heterogeneous objectives, enabling principled trade-offs across simultaneous objectives.\n\nWe evaluate PrioriTouch across simulated and real-world environments, progressively increasing in complexity and realism. First, we design a simplified simulation scenario with predefined contacts and a static end-effector pose to isolate and specifically assess LinUCB-Rank’s ability to learn user contact preferences. Second, we demonstrate PrioriTouch in a simulated caregiving scenario involving robot-assisted bed bathing, requiring whole-arm contact to safely wipe a user’s limbs. Third, we showcase our approach’s capability in intricate multi-contact scenarios through a real-world 3D goal-reaching maze with multiple vertical cylinders representing distinct body-part contact preferences. Finally, we validate PrioriTouch’s practical feasibility by performing a realistic caregiving task in a user study with human subjects.\n\nOur contributions are summarized as follows:\n\nWe propose PrioriTouch, a framework that formulates contact preference learning as a ranking problem over control objectives and executes the learned priority ordering as a null space hierarchy via H-OSC for whole-arm pHRI.\n\nWe introduce LinUCB‑Rank, a contextual bandit that learns priority orderings from sparse user feedback while accounting for inter‑objective coupling in hierarchical control.\n\nWe enable safe and data-efficient learning through simulation-in-the-loop validation, where candidate priority updates are tested in a digital twin before real-world deployment.\n\nWe conduct a user study to inform realistic models of contact preferences for robot-initiated touch, which we leverage to simulate authentic user feedback in our evaluation.\n\nWe evaluate PrioriTouch through extensive simulation, real-world experiments, and a realistic caregiving user study, demonstrating effective adaptation to individual contact preferences without compromising task performance or comfort.\n\nOur framework integrates user contact preference learning with low-level control by parameterizing operational space control using the outputs of a learned ranking policy. This structured integration ensures that high-level feedback is directly translated into low-level force regulation and pose tracking, effectively bridging the gap between user preferences and robot control.\n\n1. We propose PrioriTouch, a framework that formulates contact preference learning as a ranking problem over control objectives and executes the learned priority ordering as a null space hierarchy via H-OSC for whole-arm pHRI.\n\n2. We introduce LinUCB‑Rank, a contextual bandit that learns priority orderings from sparse user feedback while accounting for inter‑objective coupling in hierarchical control.\n\n3. We enable safe and data-efficient learning through simulation-in-the-loop validation, where candidate priority updates are tested in a digital twin before real-world deployment.\n\n4. We conduct a user study to inform realistic models of contact preferences for robot-initiated touch, which we leverage to simulate authentic user feedback in our evaluation.\n\n5. We evaluate PrioriTouch through extensive simulation, real-world experiments, and a realistic caregiving user study, demonstrating effective adaptation to individual contact preferences without compromising task performance or comfort.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在物理人机交互中适应个体的接触偏好。  \n2. 在多接触场景中，如何处理不同身体部位的力要求冲突。  \n3. 如何高效地学习和调整接触偏好以确保安全和舒适。  \n\n【用了什么创新的方案】  \nPrioriTouch框架将接触偏好学习视为控制目标的排名问题，通过LinUCB-Rank上下文赌博机从稀疏用户反馈中学习优先级排序，并利用H-OSC以空心空间层次结构执行该排序。该方法结合了模拟环中的验证，确保在真实世界中的安全部署，同时通过用户研究获得个性化的接触阈值，以增强适应性和任务性能。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Latent Action Pretraining Through World Modeling",
            "authors": "Bahey Tharwat,Yara Nasser,Ali Abouzeid,Ian Reid",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18428",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18428",
            "arxiv_html_link": "https://arxiv.org/html/2509.18428v1",
            "abstract": "Vision-Language-Action (VLA) models have gained popularity for learning robotic manipulation tasks that follow language instructions. State-of-the-art VLAs, such as OpenVLA and π0\\pi_{0}, were trained on large-scale, manually labeled action datasets collected through teleoperation. More recent approaches, including LAPA and villa-X, introduce latent action representations that enable unsupervised pretraining on unlabeled datasets by modeling abstract visual changes between frames. Although these methods have shown strong results, their large model sizes make deployment in real-world settings challenging.\nIn this work, we propose LAWM, a model-agnostic framework to pretrain imitation learning models in a self-supervised way, by learning latent action representations from unlabeled video data through world modeling. These videos can be sourced from robot recordings or videos of humans performing actions with everyday objects. Our framework is designed to be effective for transferring across tasks, environments, and embodiments. It outperforms models trained with ground-truth robotics actions and similar pretraining methods on the LIBERO benchmark and real-world setup, while being significantly more efficient and practical for real-world settings.",
            "introduction": "Self-supervised learning has been a key enabler of recent breakthroughs in Large Language Models (LLMs) such as ChatGPT [1] and Gemini [2], where models learn from large amounts of text on the Internet. Inspired by this success, the robotics community is now ready for its own transformative moment, where we can build systems that learn action representations directly from raw, unstructured video data, rather than relying on curated action labels.\n\nMost current approaches to robot learning are heavily based on supervised learning frameworks. Methods like imitation learning and VLA models, including OpenVLA [3] and π0\\pi_{0} [4], require paired image action datasets often obtained through teleoperation. These action annotations are expensive to collect, difficult to scale, and prone to bias, limiting the generalizability of these systems across tasks, environments, and embodiments.\n\nIn this work, we introduce LAWM, a Latent Action pretraining framework through World Modeling that aims to overcome these limitations by combining an Imitation Learning Model with a World Model. Our objective, as shown in Fig. LABEL:fig:first-page, is to learn action representations from both robot-collected and human demonstration videos in a fully self-supervised way. These learned representations serve as action priors that can be effectively leveraged during finetuning on downstream tasks. The proposed framework, illustrated in Figure 2, is designed to be model-agnostic, meaning that it does not depend on any specific architecture for the imitation learning model or the world model. This flexibility allows for the integration of a variety of different models.\nOur pipeline follows a two-stage steps. The first stage is an end-to-end pretraining in a self-supervised way, with the learning signal derived from predicting the next image in a video sequence. The inputs to the system consist of: (i) an image frame from a human or robot performing a manipulation task, and (ii) a natural language instruction describing the goal of the task. The imitation learning model takes these inputs to produce action chunks representations. These representations of nn actions are paired with the current image frame and the next n−1n-1 frames, then fed into the world model to generate the next image frames that would result from executing the action in the current environment. The second stage is a finetuning stage, where labeled data are used to finetune only the imitation learning model to downstream tasks. During this phase, the world model is no longer used. The learned imitation learning model is now equipped with a robust prior from large-scale unlabeled videos and can be finetuned efficiently.\n\nWe summarize our main contributions and findings below:\n\nWe propose LAWM, a model-agnostic framework, to learn action chunk representations for imitation learning models from both robot and human videos without action labels.\n\nOur experiments show that our framework can learn superior action priors from human demonstrations and robotic manipulation videos without using ground-truth action labels, compared to supervised pretraining.\n\nWe demonstrate that our framework with small models such as BAKU [5] and Dreamerv3 [6] outperforms similar methods with large models such as villa-X [7] on the LIBERO benchmark  [8].\n\n1. We propose LAWM, a model-agnostic framework, to learn action chunk representations for imitation learning models from both robot and human videos without action labels.\n\n2. Our experiments show that our framework can learn superior action priors from human demonstrations and robotic manipulation videos without using ground-truth action labels, compared to supervised pretraining.\n\n3. We demonstrate that our framework with small models such as BAKU [5] and Dreamerv3 [6] outperforms similar methods with large models such as villa-X [7] on the LIBERO benchmark  [8].",
            "llm_summary": "【关注的是什么问题】  \n1. 当前机器人学习方法依赖于昂贵的监督学习框架，限制了系统的可扩展性。  \n2. 大规模标注数据集的收集困难且容易产生偏差，影响模型的泛化能力。  \n3. 现有方法在真实世界应用中面临模型规模过大和部署效率低的问题。  \n\n【用了什么创新的方案】  \n提出LAWM框架，通过世界建模在自监督方式下学习潜在动作表示，利用机器人和人类视频数据，无需动作标签。该框架采用两阶段流程：第一阶段进行自监督预训练，第二阶段在有标签数据上微调模仿学习模型。LAWM在LIBERO基准测试中表现优于依赖真实动作标签的模型，且在真实环境中更高效、实用。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections",
            "authors": "Navya Tiwari,Joseph Vazhaeparampil,Victoria Preston",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
            "comment": "6 pages, 5 figures. Accepted as a poster at Northeast Robotics Colloquium (NERC 2025). Extended abstract",
            "pdf_link": "https://arxiv.org/pdf/2509.18407",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18407",
            "arxiv_html_link": "https://arxiv.org/html/2509.18407v1",
            "abstract": "未获取到摘要",
            "introduction": "Intersections account for nearly 40% of U.S. crashes [1], with many occurring at uncontrolled or partially controlled locations [2]. Ambiguous right of way, compounded by occlusions, non-compliant drivers, and limited sensing leave drivers uncertain how to act [3]. Addressing these challenges requires assistive technology that reduces driver uncertainty and improves awareness. We propose an Advanced Driver Assistance System (ADAS) that fuses sensor data, interprets intersection context, and applies uncertainty-aware frameworks to recommend safe actions at uncontrolled intersections. We pose three research questions (RQs):\n\nHow can the accuracy of ego- and external-vehicle state estimation be improved while constraining uncertainty through frustum-based fusion of camera and lidar data, given real-time, computationally limited resources?\n\nHow can the accuracy of ego- and external-vehicle state estimation be improved while constraining uncertainty through frustum-based fusion of camera and lidar data, given real-time, computationally limited resources?\n\nTo what extent can a driver-assist system enable safe navigation of uncontrolled intersections by efficiently interpreting intersection context (lane markings, stop signs, traffic flow patterns, and pedestrian presence) under partial observability?\n\nHow effectively can different decision-making frameworks handle uncertainty at uncontrolled intersections, and what trade-offs emerge between safety, efficiency, and computational feasibility in real-time deployment?\n\nHere, we present an initial analysis of RQ3 in synthetic uncontrolled intersections. We show that probabilistic planners, particularly POMCP (Partially Observable Monte Carlo Planning) [7] and DESPOT (Determinized Sparse Partially Observable Tree) [15], outperform deterministic approaches in predicting the intent of other drivers and selecting collision-free actions, while maintaining safety under complex, partially observable scenarios. Continued work will integrate Sensor Fusion (RQ1) and Environment Perception (RQ2) modules for end-to-end, real-time navigation under realistic traffic and environmental conditions.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高自我和外部车辆状态估计的准确性，同时在实时计算资源有限的情况下约束不确定性？  \n2. 如何在部分可观测的情况下高效解读交叉口环境，以实现安全导航？  \n3. 不同决策框架在处理不确定性时的有效性如何，以及在实时部署中安全性、效率和计算可行性之间的权衡是什么？  \n\n【用了什么创新的方案】  \n提出了一种先进的驾驶辅助系统（ADAS），通过融合传感器数据、解释交叉口上下文，并应用不确定性感知框架，推荐在无人控制交叉口的安全行动。初步分析表明，概率规划方法（如POMCP和DESPOT）在预测其他驾驶员意图和选择无碰撞行动方面优于确定性方法，同时在复杂的部分可观测场景中保持安全。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation",
            "authors": "Rajitha de Silva,Jonathan Cox,James R. Heselden,Marija Popovic,Cesar Cadena,Riccardo Polvara",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "Sumbitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.18342",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18342",
            "arxiv_html_link": "https://arxiv.org/html/2509.18342v1",
            "abstract": "Accurate localisation is critical for mobile robots in structured outdoor environments, yet LiDAR-based methods often fail in vineyards due to repetitive row geometry and perceptual aliasing. We propose a semantic particle filter that incorporates stable object-level detections, specifically vine trunks and support poles into the likelihood estimation process. Detected landmarks are projected into a bird’s eye view and fused with LiDAR scans to generate semantic observations. A key innovation is the use of semantic walls, which connect adjacent landmarks into pseudo-structural constraints that mitigate row aliasing. To maintain global consistency in headland regions where semantics are sparse, we introduce a noisy GPS prior that adaptively supports the filter. Experiments in a real vineyard demonstrate that our approach maintains localisation within the correct row, recovers from deviations where AMCL fails, and outperforms vision-based SLAM methods such as RTAB-Map.",
            "introduction": "Accurate localisation is a critical component of mobile robot navigation in outdoor environments [1]. Among the various approaches, LiDAR-based localisation remains widely adopted due to its reliable and precise perception of geometric structure. However, these methods rely solely on scene geometry, which can be problematic in outdoor agricultural settings like vineyards, where repetitive and ambiguous structures are common [2]. In such environments, incorporating semantic information complements the geometric structure offering a promising alternative to enhance localisation performance [3].\n\nIn this paper, we tackle the challenge of semantic ambiguity in geometry-based localisation within vineyard environments. The repetitive structure of vineyard rows often induces perceptual aliasing in LiDAR range data, resulting in localisation drift and errors. To overcome this limitation, we exploit semantically meaningful landmarks, specifically vine trunks and support poles whose distinctive spatial distributions provide stronger discriminative cues. Our approach detects these semantic objects and estimates their relative positions from RGB-D imagery, which are then projected onto the LiDAR frame. This enables a semantic-LiDAR particle filter that offers a robust alternative to conventional localisation methods.\n\nTraditional particle filters, such as Adaptive Monte Carlo Localisation (AMCL) [4], estimate a robot’s pose by evaluating the geometric consistency between sensor observations and a known map, an approach that has proven highly effective in structured indoor and urban settings where distinctive geometric features are abundant. Vineyards, however, present a markedly different challenge: their long, repetitive rows induce strong perceptual aliasing, while unstable elements such as foliage and grape clusters provide little reliability for long-term localisation [5]. We contend that robust localisation in such environments requires moving beyond raw geometry and explicitly exploiting semantics. Our key insight is that vine trunks and support poles serve as stable, distinctive landmarks whose consistent spatial distribution across rows can disambiguate pose estimates. Moreover, we introduce the concept of semantic walls, where the space between consecutive landmarks is modelled as a pseudo-structural boundary. This transforms sparse semantic detections into continuous row-level constraints, creating a representation that is far more robust to vineyard aliasing and seasonal variation. Together, these ideas lay the foundation for a semantic-LiDAR particle filter that redefines localisation in repetitive agricultural environments, as illustrated in Fig. 1.\n\nThe main contributions of this paper are threefold:\n(i) the design of a semantic particle filter that integrates object-level detections of stable vineyard landmarks (trunks and support poles) with a 2D semantic map, enabling robust localisation in highly repetitive environments;\n(ii) the introduction of the semantic walls concept, which augments sparse landmark detections by modelling pseudo-structural boundaries between adjacent landmarks, thereby strengthening row-level constraints and mitigating perceptual aliasing; and\n(iii) a systematic evaluation against established baselines, demonstrating the trade-offs in accuracy, robustness, and sensor requirements.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在重复结构的环境中实现准确的移动机器人定位。  \n2. 如何克服LiDAR方法在葡萄园中因感知别名而导致的定位漂移和错误。  \n3. 如何利用语义信息增强几何结构以提高定位性能。  \n\n【用了什么创新的方案】  \n提出了一种语义粒子滤波器，将稳定的对象级检测（如葡萄藤干和支撑杆）融入到定位过程中，通过将检测到的地标投影到鸟瞰图并与LiDAR扫描融合生成语义观测。此外，引入了语义墙的概念，将相邻地标连接成伪结构约束，以减轻行别名问题，并在语义稀疏的区域引入噪声GPS先验以维持全局一致性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020",
            "authors": "Marsette Vona",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18330",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18330",
            "arxiv_html_link": "https://arxiv.org/html/2509.18330v1",
            "abstract": "The Landform contextual mesh fuses 2D and 3D data from up to thousands of Mars 2020 rover images, along with orbital elevation and color maps from Mars Reconnaissance Orbiter, into an interactive 3D terrain visualization. Contextual meshes are built automatically for each rover location during mission ground data system processing, and are made available to mission scientists for tactical and strategic planning in the Advanced Science Targeting Tool for Robotic Operations (ASTTRO). A subset of them are also deployed to the ”Explore with Perseverance” public access website.",
            "introduction": "Like its predecessors including Spirit, Opportunity, and Curiosity, the Mars 2020 Perseverance rover carries a suite of stereo cameras to image the surrounding terrain [1, 2, 3]. Data from those instruments is used for multiple purposes including both on-board and ground-based navigation, engineering operations, and science analysis. An important use case is tactical and strategic science planning, where teams of mission scientists use the imagery to select areas of interest and plan subsequent observations.\n\nStereo vision produces a “tactical wedge” 3D terrain mesh for each stereo image pair, so-called because often a radial panorama of such wedges is acquired from a single rover location using the pan/tilt mast. Of course, only a fraction of nearby terrain is included since the cameras have limited fields of view, effective resolution decreases with distance, the rover occludes areas underneath itself, nearby rocks and hills create self-occlusions in the terrain, and stereo reconstruction fails in areas with insufficient texture. Nevertheless, viewing such panoramas of tactical wedges in 2D and 3D (latter also called the tactical mesh) has been a standard approach for science planning on Mars 2020 and its predecessors. Figure 1(a) shows such a view for site 40, drive 132 of the Perseverance rover, acquired on sols 821–832111One sol is the equivalent of a day on Mars..\n\nAn alternative to the tactical mesh is to use a portion of a digital elevation map (DEM) derived from orbital observations, as shown in Figure 1(b). The Mars 2020 mission typically uses colored DEM data from the HiRISE instrument on the Mars Reconnaissance Orbiter [4]. This orbital mesh can cover a much larger extent—up to 10s of km—and typically has no gaps in its coverage. However, at a typical resolution of 1 elevation sample (and 16 color samples) per square meter, it’s much coarser than the tactical mesh, which can have sub-millimeter resolution near the rover.\n\nIn this paper we introduce the contextual mesh, which we have developed to fuse up to thousands of images from in-situ stereo cameras together with orbital DEM data into a single 3D scene, shown in Figure 1(c) and Figure 2. The contextual mesh is produced by Landform, a subsystem within the Mars 2020 ground data system (M20 GDS), and typically viewed in the Advanced Science Targeting Tool for Robotic Operations (ASTTRO) collaborative web application [5, 6], also part of the M20 GDS [7].\n\nWhereas the tactical mesh offers the highest fidelity local terrain reconstruction, and the orbital mesh the longest range reconstruction, the intention of the contextual mesh is to provide spatial awareness. It is typically visualized from a first-person navigable 3D point of view in ASTTRO, showing not only local terrain features such as sand, pebbles, rocks, ridges, and hills, but also distant landmarks on the horizon. ASTTRO also displays a 3D model of the Perseverance rover on the terrain as it was posed at the corresponding time in the mission.\n\nEach contextual mesh is comprised of two tilesets in the open-standard 3DTiles format [8]. One tileset contains the terrain itself, typically extending to a 1km square with a 100m square central detail area. The other tileset is a hemispherical representation of the surrounding horizon and sky, enabling visualization of distant features potentially many kilometers away. ASTTRO displays both of these simultaneously so that users can see the context of local terrain features relative to both nearby hills and to the horizon.\n\nThe 3DTiles format enables data to be progressively streamed to distributed users in the web-based ASTTRO client. Only the subset of tile data required depending on the user’s current viewpoint is transferred and rendered, enabling dynamic level-of-detail, fast load times, and deployment to resource limited clients.\n\nLike the products of many data fusion and reconstruction algorithms, e.g. computed tomography, and considering that the input data contains noise and outliers, the contextual mesh may contain some artifacts. For example\n\nboundaries between areas reconstructed primarily from surface vs orbital data may have some discontinuities\n\noutlier images with extreme brightness variations may not be completely blended\n\nreconstructed geometry may have “island” topological artifacts due to noise and residual misalignment in the input data.\n\nThe Landform contextual mesh differs from many other photogrammetry and terrain fusion systems not only because it combines both surface and orbital data and has a sky sphere, but also in that it is entirely automated, whereas most other systems require some human intervention. It heavily leverages properties of Mars mission datasets, including pose priors from rover navigation, calibrated stereo camera data, and co-registered orbital data. These enable automated processing while maintaining reasonable quality relative to manual and semi-manual approaches.\n\nThe Landform codebase will soon be released as open source. And, throughout the mission, a selection of contextual meshes have been made publicly available for interactive viewing at the “Explore with Perseverance” website [9].\n\nIn this paper we summarize the research context of related terrain fusion approaches, describe the novel algorithms we developed to implement the contextual mesh system in Landform, and present examples of contextual mesh data products.\n\n1. boundaries between areas reconstructed primarily from surface vs orbital data may have some discontinuities\n\n2. outlier images with extreme brightness variations may not be completely blended\n\n3. reconstructed geometry may have “island” topological artifacts due to noise and residual misalignment in the input data.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效融合火星探测器的2D和3D数据以生成高质量的地形可视化。  \n2. 在自动化处理过程中如何保持数据融合的质量，减少人为干预。  \n3. 如何在动态环境中实现3D场景的实时可视化和交互。  \n\n【用了什么创新的方案】  \n本文提出了一种名为“Landform contextual mesh”的新方法，通过自动融合来自火星2020探测器的数千张立体图像与轨道数字高程图（DEM）数据，生成一个交互式的3D地形可视化。该方法利用了火星任务数据集的特性，自动处理并生成高保真度的地形重建，提供了局部和远程地形特征的空间意识。此外，使用3DTiles格式实现数据的渐进式流式传输，支持动态细节级别和快速加载时间。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Haptic Communication in Human-Human and Human-Robot Co-Manipulation",
            "authors": "Katherine H. Allen,Chris Rogers,Elaine S. Short",
            "subjects": "Robotics (cs.RO)",
            "comment": "9 pages, 18 figures, ROMAN 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18327",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18327",
            "arxiv_html_link": "https://arxiv.org/html/2509.18327v1",
            "abstract": "When a human dyad jointly manipulates an object, they must communicate about their intended motion plans. Some of that collaboration is achieved through the motion of the manipulated object itself, which we call “haptic communication.” In this work, we captured the motion of human-human dyads moving an object together with one participant leading a motion plan about which the follower is uninformed. We then captured the same human participants manipulating the same object with a robot collaborator. By tracking the motion of the shared object using a low-cost IMU, we can directly compare human-human shared manipulation to the motion of those same participants interacting with the robot. Intra-study and post-study questionnaires provided participant feedback on the collaborations, indicating that the human-human collaborations are significantly more fluent, and analysis of the IMU data indicates that it captures objective differences in the motion profiles of the conditions. The differences in objective and subjective measures of accuracy and fluency between the human-human and human-robot trials motivate future research into improving robot assistants for physical tasks by enabling them to send and receive anthropomorphic haptic signals.",
            "introduction": "In physical collaboration tasks like carrying a couch or moving a table, haptic signals are an important channel of communication between participants to coordinate the group action. In human-human interactions, the communication and interpretation of these signals is primarily subconscious, but prior research suggests that they may enable more efficient human-robot collaboration [1]. In order for robots to participate in this haptic conversation, we need to develop a more robust understanding of how haptic communication occurs in both human-human and human-robot interaction. This knowledge can then be used to develop models for interpreting haptic intent, provide robots with comprehensible and predictable behavior, and avoid unwanted oscillations in collaborative manipulation.\n\nIn this paper, we present a study of haptic interaction, without use of visual or auditory signaling, during the collaborative manipulation of a shared object. We compare human-human and human-robot dyads to test whether there are observable differences in the subjective fluency of human-human and human-robot dyads, and whether these correlate with changes in the character of acceleration profiles of the co-manipulated objects. We additionally collect data on human perceptions of robot collaborators to identify potential co-variables in subjective fluency.\n\nWe conducted a user study with 34 participants. In the study, two agents collaborated to move a shared object, with one participant designated as the motion leader and one as a follower. Each participant acted in one these roles, and interacted with both another human participant and a mobile manipulator robot. We collected measures of subjective and objective measures of task fluency, as well as video and IMU recordings. We found that the acceleration data from an IMU mounted on the shared object changes more smoothly in human-human dyads than in human-robot dyads, with more fluent collaborations having smaller accelerations overall and smaller changes in acceleration during the task. We further find that common objective measures of collaboration fluency (e.g. task duration) do not correlate linearly with subjective fluency measures, and propose alternate measures based on our data. This work contributes to our understanding of the differences and similarities in current human-human and human-robot haptic communication during collaborative manipulation, and provides insights that can inform future methods for autonomous haptic signaling by robots.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何理解人类之间的触觉通信在共同操控中的作用？  \n2. 人类与机器人之间的触觉通信是否存在显著差异？  \n3. 如何提高机器人在物理任务中的协作能力？  \n\n【用了什么创新的方案】  \n本研究通过用户研究比较了人类与人类和人类与机器人在共同操控共享物体时的触觉交互。使用低成本IMU捕捉物体运动，分析了不同条件下的加速度数据，发现人类之间的协作流畅性显著高于人机协作。研究还探讨了主观流畅性与客观加速度特征之间的关系，为未来机器人自主触觉信号的研究提供了见解。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Fine-Tuning Robot Policies While Maintaining User Privacy",
            "authors": "Benjamin A. Christie,Sagar Parekh,Dylan P. Losey",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18311",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18311",
            "arxiv_html_link": "https://arxiv.org/html/2509.18311v1",
            "abstract": "Recent works introduce general-purpose robot policies.\nThese policies provide a strong prior over how robots should behave — e.g., how a robot arm should manipulate food items.\nBut in order for robots to match an individual person’s needs, users typically fine-tune these generalized policies — e.g., showing the robot arm how to make their own preferred dinners.\nImportantly, during the process of personalizing robots, end-users leak data about their preferences, habits, and styles (e.g., the foods they prefer to eat).\nOther agents can simply roll-out the fine-tuned policy and see these personally-trained behaviors.\nThis leads to a fundamental challenge: how can we develop robots that personalize actions while keeping learning private from external agents?\nWe here explore this emerging topic in human-robot interaction and develop PRoP, a model-agnostic framework for personalized and private robot policies.\nOur core idea is to equip each user with a unique key; this key is then used to mathematically transform the weights of the robot’s network.\nWith the correct key, the robot’s policy switches to match that user’s preferences — but with incorrect keys, the robot reverts to its baseline behaviors.\nWe show the general applicability of our method across multiple model types in imitation learning, reinforcement learning, and classification tasks.\nPRoP is practically advantageous because it retains the architecture and behaviors of the original policy, and experimentally outperforms existing encoder-based approaches.\nSee videos and code here: https://prop-icra26.github.io",
            "introduction": "Generalist policies enable robots to learn multiple tasks [1, 2].\nSo far these methods have traditionally been used in research labs and factories.\nBut we envision a future where robots enter domestic settings for assisting humans [3].\nFor example, consider a robot that is developed to help in a kitchen.\nThis robot will have some initial policy π0\\pi_{0} that users may want to finetune to match their own preferences and requirements.\nFor instance, perhaps the robot knows how to make a hamburger, but individual users prefer different ingredients, condiments, or even specific ways of stacking the burger.\nThis finetuning raises privacy concerns: the manufacturers can share the users’ data collected during finetuning with third-parties.\nConsequently, there is increasing demand for exploring new avenues to maintain the privacy and transparency of robotic agents [4].\nFollowing this, we come to a fundamental scientific question: how do we make systems that can learn and adapt to individual end-users, while still maintaining those user’s privacy?\n\nPrivacy in machine learning has traditionally been examined from two perspectives.\nFirst is data privacy, which concerns safeguarding the sensitive information of individuals represented in the dataset [5, 6, 7, 8].\nSecond is model privacy, which focuses on protecting the learned parameters of a neural network through techniques such as encryption or differentially-private learning [9, 10, 11, 12].\nIn this work, we adopt a third perspective with respect to robot learning: ensuring that a trained, personalized robot does not leak user preference information to other users.\nReturning to our example, privacy in this context means that the robot can be finetuned to learn your preferred way of making a burger while preventing unauthorized users from accessing those preferences even if they have access to the trained model.\nIn practice, this can be difficult to achieve because — if someone has access to the finetuned model — they can roll-out this model and infer the previous user’s preference by watching the robot actions.\nSo how do we safeguard privacy of user preferences?\nOur insight is that:\n\nConcretely, we leverage keys (Figure 1).\nA key is any feature that is unique to the user such as facial structures, vocal patterns, or a textual password.\nWhen finetuning the robot under out approach, a user combines their unique key with the intermediate features of the network and trains it to output their personalized actions.\nThis unique mechanism for personalizing robots safeguards user privacy since preference information remains inaccessible to anyone who does not have the user’s key.\nWithout careful design, keys may unintentionally cause the robot to forget its general-purpose policy.\nBut our technical approach avoids this pitfall — and preserves the initial model architecture — by leveraging the key to perform mathematical operations on the intermediate weights.\nOur proposed mechanism is not tied to a specific network architecture or application as we later demonstrate in our experiments with visual data, imitation learning, MLP classifiers, and reinforcement learning.\nIndeed, as shown in our experiments on robot arms, users can finetune the robot to make their desired hamburger without losing the robot’s previously learned behaviors, and without exposing their preference to other agents.\nWe see this work as a step towards safe and personalized human-robot interaction.\n\nOverall, we make the following contributions:\n\nKey-based Personalization of Robot Policies.\nWe present a formulation for key-based personalization of robot control policies.\nUnder this formalism, the robot learns to personalize to new users’ specifications while retaining its original, general behavior.\nThis formalism is nontrivial to implement in a learning algorithm, since the original and conditional policies operate in different domains, i.e., adding a key as the input requires changing the size of the pre-trained architecture.\nInstead, we use keys to transform the intermediate features of the pre-trained policy, circumventing the need for changing the architecture size.\n\nPersonalized and Private Robot Policies.\nWe present our implementation of the aforementioned key-based personalization with privacy guarantees.\nOur method, PRoP (Personalized and Private Robot Policies) retains the original network architecture, exhibits behavior of the original robot policy for unprivileged users, and personalizes to specific users through a privacy-oriented mechanism.\nImportantly, PRoP extends to arbitrary learning rules and architectures that enables simple, end-to-end training of the model.\n\nReal-world Validation and Empirically Verified Robustness.\nWe empirically test the performance of PRoP in a collection of controlled simulations and real-world studies, including Imitation Learning, Reinforcement Learning, Image Classification, and Task Allocation.\nWe further extend PRoP to more complex settings, such as language prose personalization and key-based obfuscation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在个性化机器人行为的同时保护用户隐私？  \n2. 现有的机器人政策如何防止用户偏好信息泄露？  \n3. 如何在保持原有政策架构的情况下实现个性化？  \n\n【用了什么创新的方案】  \n本研究提出了一种名为PRoP的模型无关框架，通过为每个用户分配一个唯一的密钥来实现个性化和隐私保护。该密钥用于对机器人的网络权重进行数学变换，使得机器人可以根据用户的偏好进行调整，而未授权用户则无法访问这些偏好信息。此外，该方法保留了原始政策的架构和行为，并在多种学习任务中表现优于现有方法。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies",
            "authors": "Jesse Zhang,Marius Memmel,Kevin Kim,Dieter Fox,Jesse Thomason,Fabio Ramos,Erdem Bıyık,Abhishek Gupta,Anqi Li",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "11 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18282",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18282",
            "arxiv_html_link": "https://arxiv.org/html/2509.18282v1",
            "abstract": "Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: (1) end-effector paths specifying what actions to take, and (2) task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4×\\times real-world improvement for a 3D policy trained only in simulation, and 2–3.5×\\times gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need—where, what, and how.\nWebsite at https://peek-robot.github.io.",
            "introduction": "Imagine walking through a crowded store when your child suddenly cries out, “I want the Labubu!”\nThough you’ve never heard the word before, context clues guide your eyes to the fuzzy toy on the shelf, and you effortlessly weave through the crowd to grab it.\nWhat makes this possible is not raw perception ability, but the ability to interpret ambiguous instructions and distill them into just the right cues—where to focus, what actions to take, and how to perform these actions at the low level. Similarly, if given where to focus and what motions to take, a robot manipulation policy should be able to achieve the visual robustness and semantic generalization necessary for open-world deployment by focusing only on how to perform actions.\n\nA common tactic for training manipulation policies is through imitation learning of human-collected robotics data [1, 2, 3, 4], which attempts to learn the where, what, and how all at the same time.\nYet their performance degrades on novel objects, clutter, or semantic variations [5, 6], since the policy alone bears the burden of handling task, semantic, and visual complexity.\nSuch failures often entangle the axes of where, what, and how—for example, grasping a distractor simultaneously reflects misplaced attention, an incorrect object choice, and a wrong motion.\n\nOur key idea is to offload high-level reasoning to vision-language models (VLMs), which can excel at semantic and visual generalization [7, 8], leaving the policy to determine how low-level behavior should be executed. Instead of forcing the policy to directly parse raw images and instructions, a high-level VLM modulates the input representation to the low-level policy by providing: (1) a path that encodes what the policy should do, and (2) masks showing where to attend. By “absorbing” semantic and visual variation, the VLM provides the policy a simplified, annotated “peek” of the scene that gives the what and the where, while the policy only needs to learn how to perform the low-level actions. This intermediate representation helps policy execution inherit many of the VLM’s semantic and visual generalization capabilities. Our VLM-modulated representation is naturally policy-agnostic, allowing it to be applied to arbitrary image-input robot manipulation policies, including state-of-the-art RGB and 3D manipulation policies [9, 1, 3].\n\nTo concretely instantiate this insight into a practical algorithm, we introduce PEEK (Policy-agnostic Extraction of Essential Keypoints), which proposes a unified, point-based intermediate representation that trains VLMs to predict what policies should do and where to focus on. Specifically, we propose to finetune pretrained VLMs [10] to predict a sequence of points corresponding to (1) a path that guides the robot end-effector in what actions to take and (2) a set of task-relevant masking points that show the policy where to focus on (see Figure 1). During low-level visuomotor policy training and inference, we modulate the policy’s image observations by directly drawing these VLM-predicted paths and masks onto the image, allowing the policy to simply focus on how to act, rather than learning all three simultaneously. Doing so significantly bolsters policy generalization, combining the generality of high-level VLM predictions with the precision of low-level policy learning. In this paper, we instantiate a full-stack implementation of PEEK, from devising a scalable data annotation scheme that enables large-scale VLM finetuning on robotic datasets to representation-modulated training of low-level robot policies from simulation and real world data.\n\nIn 535 real-world evaluations across 17 task variations, we demonstrate that PEEK consistently boosts zero-shot policy generalization: a 3D policy (3DDA [9]) trained only in simulation achieves 41.4×\\times higher success in the real world when guided by PEEK, and both large-scale vision-language-action models (π0\\pi_{0} [3]) and small transformer-based policies [1] see 2–3.5×\\times success rate improvements. These results demonstrate the power of using high-level VLMs to absorb task complexity, providing low-level policies with exactly the minimal cues they need for generalizable manipulation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高机器人操作策略的零-shot泛化能力。  \n2. 如何有效地将高层次推理任务分配给视觉-语言模型（VLMs）。  \n3. 如何简化机器人策略的输入表示以增强其性能。  \n\n【用了什么创新的方案】  \nPEEK（Policy-agnostic Extraction of Essential Keypoints）提出了一种统一的基于点的中间表示，旨在通过微调视觉-语言模型（VLMs）来预测机器人操作所需的路径和关注区域。这种表示直接叠加在机器人观察上，使得策略可以专注于如何执行操作，而不是同时学习“在哪里”、“做什么”和“如何做”。通过引入自动标注管道，PEEK能够在多个机器人数据集上生成标注数据，从而提升零-shot泛化能力，显著提高了在真实世界中的成功率。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "A Fast Initialization Method for Neural Network Controllers: A Case Study of Image-based Visual Servoing Control for the multicopter Interception",
            "authors": "Chenxu Ke,Congling Tian,Kaichen Xu,Ye Li,Lingcong Bao",
            "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19110",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19110",
            "arxiv_html_link": "https://arxiv.org/html/2509.19110v1",
            "abstract": "Reinforcement learning-based controller design methods often require substantial data in the initial training phase. Moreover, the training process tends to exhibit strong randomness and slow convergence. It often requires considerable time or high computational resources.\nAnother class of learning-based method incorporates Lyapunov stability theory to obtain a control policy with stability guarantees. However, these methods generally require an initially stable neural network control policy at the beginning of training.\nEvidently, a stable neural network controller can not only serve as an initial policy for reinforcement learning, allowing the training to focus on improving controller performance, but also act as an initial state for learning-based Lyapunov control methods.\nAlthough stable controllers can be designed using traditional control theory, designers still need to have a great deal of control design knowledge to address increasingly complicated control problems.\nThe proposed neural network rapid initialization method in this paper achieves the initial training of the neural network control policy by constructing datasets that conform to the stability conditions based on the system model.\nFurthermore, using the image-based visual servoing control for multicopter interception as a case study, simulations and experiments were conducted to validate the effectiveness and practical performance of the proposed method.\nIn the experiment, the trained control policy attains a final interception velocity of 15 m/s.",
            "introduction": "In recent years, an increasing number of studies have utilized learning-based methods to address control issues[1]. There are two types of learning-based methods for control problem.\nOne is the reinforcement learning (RL) method, and the other is the learning-based Lyapunov control (LLC) method.\nIn contrast to conventional control methods, RL techniques engage with the environment via trial and error to identify optimal strategies and may complete intricate tasks without dependence on exact models of the controlled entities.\nRL, akin to control systems, functions through feedback mechanisms.\nWhile RL largely uses input to refine its decision-making processes, control systems focus on achieving predetermined targets mainly by using static controller techniques during operation.\nThe training process of RL can be unstable and unsafety[2], especially in safety-critical situations like the unmanned aerial vehicle (UAV) visual servoing control.\nIn order to ensure that the trained policy can be applied in practice, the datasets used in the training process should encompass the Region of Interest (RoI)[3; 4], which is exceedingly challenging before obtaining a available control policy.\n\nThe Lyapunov stability method provides a definitive analytical and design framework in control theory, especially for nonlinear systems [5].\nNumerous research studies have recently integrated Lyapunov stability approaches into learning-based control, referred to as Lyapunov function learning, thereby providing formal stability guarantees for deep neural network policies.\nIn the studies [6; 7], the Lyapunov function is utilized as a critic function to assess policies performance.\nIn [8] and [9], Lyapunov functions are integrated into optimization frameworks to guarantee system stability.\nThe Lyapunov stability condition is incorporated into the reward design in [10] and [11].\nReferences [10] and [11] develop the target control policy by incorporating a Lyapunov function into the reward design.\nThe research in [12] proposes learning the Lyapunov function and its derivative (referred to as the D-function) from expert demonstration data while adhering to stability constraints, thus facilitating the development of a control policy that inherently ensures Lyapunov stability.\nNote that uniformly sampled data is necessary for this approach.\nOtherwise, the D-function employed may not accurately represent the actual system model.\nWhile these methods offer formal stability guarantees for the target policy and yield favorable outcomes, they depend on the posteriori expert controllers or trajectories and are not suitable for the original design of the control policy.\n\nThe posteriori expert controllers are also used as the initial policy of RL to circumvent the drawback of slow convergence at the beginning of the training.\nAlthough the conventional control theory can be applied for the controller designing, the rich experience of that is also important to solve a complex control problem.\nTherefore, this paper proposed an initial policy training method that involves constructing datasets that meet stability requirements and then training a neural network control policy based on the datasets.\nMoreover, acquiring a group of datasets without a stable control policy is exceedingly challenging.\nConversely, without a controller, acquiring the model of controlled objects may be more attainable than gathering data.\nUtilizing the datasets produced by mathematical models that adhere to Lyapunov stability, an untrained neural network may be directly developed into a control policy, circumventing the conventional control design and debugging procedures.\nThe trained neural network control policy can be enhanced further by RL methods or the LLC method that needs an initial stable control policy [2].\nIn this paper, the case of image-based visual servoing control for the multicopter interception is adopted to demonstrate the effectiveness of the proposed method.\nIn the experiment, a final flight speed of up to 15 m/s was achieved.\nNoted that the purpose of this method is to obtain a usable control policy with mediocre performance at least, and the optimization of the control policy still needs to be accomplished through RL and LLC methods.\n\nThe paper is organized as follows: Section II outlines the coordinate systems and mathematical models employed. Section III presents the rapid methodology for training neural network policies, applies it to the design of multicopter interception control, and validates the stability of the trained policies by the almost Lyapunov condition[13]. Section IV presents the experimental result of the trained policies implemented on quadrotor platforms.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何快速初始化神经网络控制器以提高训练效率？  \n2. 如何确保在强化学习和Lyapunov控制中使用的控制策略具备稳定性？  \n\n【用了什么创新的方案】  \n提出了一种快速初始化方法，通过构建符合稳定性条件的数据集来实现神经网络控制策略的初始训练。该方法避免了传统控制设计的复杂性，使得未训练的神经网络可以直接发展为控制策略，从而提高训练效率并确保稳定性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Guaranteed Robust Nonlinear MPC via Disturbance Feedback",
            "authors": "Antoine P. Leeman,Johannes Köhler,Melanie N. Zeilinger",
            "subjects": "Optimization and Control (math.OC); Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "Code:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18760",
            "code": "https://github.com/antoineleeman/robust-nonlinear-mpc",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18760",
            "arxiv_html_link": "https://arxiv.org/html/2509.18760v1",
            "abstract": "Robots must satisfy safety-critical state and input constraints despite disturbances and model mismatch.\nWe introduce a robust model predictive control formulation that is fast, scalable, and compatible with real-time implementation.\nOur formulation guarantees robust constraint satisfaction, input-to-state stability and recursive feasibility.\nThe key idea is to decompose the uncertain nonlinear system into (i) a nominal nonlinear dynamic model, (ii) disturbance-feedback controllers, and (iii) bounds on the model error. These components are optimized jointly using sequential convex programming.\nThe resulting convex subproblems are solved efficiently using a recent disturbance-feedback MPC solver.\nThe approach is validated across multiple dynamics, including a rocket-landing problem with steerable thrust. An open-source implementation is available at https://github.com/antoineleeman/robust-nonlinear-mpc.",
            "introduction": "Autonomous robots, whether agile drones, wheeled machines, or (autonomous) spacecrafts, must operate in dynamic and uncertain environments while satisfying strict safety and performance requirements [1].\nIn addition, model mismatch arises naturally due to many factors, such as wind gusts, actuators misalignments, or unmodelled frictions.\nIn robotics applications, disturbances such as wind gusts, actuator misalignments, or unmodeled friction are typically handled by introducing ad hoc safety margins in the control design, resulting in slower motions, reduced maneuverability, and under-utilization of the system’s capabilities.\n\nReinforcement learning, often with domain randomization, has recently shown success in achieving robust sim2real performance[2], particularly in contact-rich tasks. While learned policies can be executed in real time, training requires extensive offline computation, careful reward design, and heuristics to ensure convergence.\n\nIn contrast, trajectory-optimization methods enforce constraint satisfaction for nonlinear dynamics and are widely used in practice as a model-based control technique based on sequential convex programming [3].\nHowever, these methods typically do not ensure safety or stability in the presence of disturbances, which is critical for real-world deployment.\n\nIn this paper, we introduce a scalable robust model predictive control formulation for nonlinear systems that is safe-by-design.\nRobust model predictive control commonly accounts for disturbances by predicting a set containing all possible future states [4].\nTo reduce conservatism, these robust predictions are based on closed-loop predictions and a corresponding feedback law is typically optimized offline, e.g., using contraction metrics [5, 6].\nHowever, fixing the feedback a priori can limit closed-loop performance and the offline computations also limit scalability.\nRobust model predictive control approaches that optimize feedback laws to reduce conservatism have been proposed in [7, 8], which rely on (conservative) sequential over-approximations of the robust predictions.\nIn contrast, the disturbance feedback MPC [9] framework framework (also known as system level synthesis [10]) provides an exact characterization of the robust prediction for linear time-varying systems, thereby avoiding this compounding effect. Recent extensions [11, 12] further enable its application to nonlinear systems.\nWhile these formulations improve performance compared to fixed policy approaches, they generally do not provide guarantees of recursive feasibility or stability. However, such guarantees are crucial, since loss of feasibility at any step can cause safety constraint violations.\nContribution: Building on the nonlinear SLS formulation in [11],\nwe propose a fast and scalable robust model predictive control formulation for nonlinear systems with robust closed-loop guarantees.\nOur approach jointly optimizes the nominal nonlinear trajectory, a disturbance-feedback controller, and an upper bound on the prediction error.\n\nFormal guarantees are provided, i.e., robust constraint satisfaction, recursive feasibility (Thm. 1), and input-to-state stability (Thm. 2). Recursive feasibility is ensured by a novel treatment of the mismatch with respect to the nonlinear nominal prediction.\n\nFormal guarantees are provided, i.e., robust constraint satisfaction, recursive feasibility (Thm. 1), and input-to-state stability (Thm. 2). Recursive feasibility is ensured by a novel treatment of the mismatch with respect to the nonlinear nominal prediction.\n\nAn efficient sequential convex programming algorithm tailored to the robust MPC formulation is provided to enable real-time deployment. Each iteration consists of solving a nominal trajectory optimization with a quadratic program, updating a disturbance-feedback controller via Riccati recursions, and evaluating Jacobians of the nonlinear dynamics. The design is general and the provided code can be directly applied to systems with large state and input dimensions and long prediction horizons.\n\nReal-time feasibility (computation times) is demonstrated across different dynamics, including a quadcopter and a rocket landing. Robust performance is validated on the rocket-landing problem with steerable thrust including actuator dynamics, illustrated in Fig. 1, demonstrating robust constraint satisfaction with an average total latency of 19.7 [ms] per iteration.\nA comparison to a soft-constrained MPC baseline highlights increased safety and stability of the proposed approach.\n\nNotation:\nFor vectors or matrices aa and bb with the same number of rows, we denote their horizontal concatenation by [a,b][a,~b].\nWe denote stacked vectors or matrices by (a,b)=[a⊤,b⊤]⊤\\left(a,b\\right)=[a^{\\top},~b^{\\top}]^{\\top}. For a vector r∈ℝnr\\in\\mathbb{R}^{n}, we denote its ithi^{\\text{th}} component by rir_{i}.\nFor a sequence of matrices Mk,j∈ℝp×qM_{k,j}\\in\\mathbb{R}^{p\\times q}, indexed by k>j≥0k>j\\geq 0,\nwe define the shorthand horizontal concatenation M(k):=[Mk,k−1,Mk,k−2,…,Mk,0]∈ℝp×k​q.M_{(k)}\\vcentcolon=[M_{k,k-1},~M_{k,k-2},~\\dots,~M_{k,0}]\\in\\mathbb{R}^{p\\times kq}.\nFor a vector v∈ℝnv\\in\\mathbb{R}^{n}, we write its 1-norm as ‖v‖1=|v1|+…+|vn|\\|v\\|_{1}=|v_{1}|+\\ldots+|v_{n}| and its infinity norm as ‖v‖∞=maxi=1,…,n⁡|vi|\\|v\\|_{\\infty}=\\max_{i=1,\\ldots,n}|v_{i}|.\nFor a matrix M∈ℝm×nM\\in\\mathbb{R}^{m\\times n}, the matrix infinity norm is ‖M‖∞=maxi​∑j|Mi​j|\\|M\\|_{\\infty}=\\max_{i}\\sum_{j}|M_{ij}|.\nWe denote sets with calligraphic letters, e.g., 𝒲⊆ℝn\\mathcal{W}\\subseteq\\mathbb{R}^{n}.\nLet ℬm\\mathcal{B}^{m} be the unit ball defined by ℬm:={d∈ℝm|‖d‖∞≤1}\\mathcal{B}^{m}\\vcentcolon=\\{d\\in\\mathbb{R}^{m}|~\\|d\\|_{\\infty}\\leq 1\\}. The Minkowski sum of two sets 𝒜,𝒟⊆ℝn\\mathcal{A},\\mathcal{D}\\subseteq\\mathbb{R}^{n} is defined as 𝒜⊕𝒟:={a+d|a∈𝒜,d∈𝒟}\\mathcal{A}\\oplus\\mathcal{D}\\vcentcolon=\\{a+d|a\\in\\mathcal{A},d\\in\\mathcal{D}\\}.\nFor a vector-valued function f:ℝn→ℝqf:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{q}, we denote the Jacobian by ∂ϕ∂x|x∈ℝq×n\\frac{\\partial\\phi}{\\partial x}|_{x}\\in\\mathbb{R}^{q\\times n}. For the ithi^{\\text{th}} component (f)i(f)_{i}, we denote its Hessian by\n∂2(f)i∂x2|x∈ℝn×n\\frac{\\partial^{2}(f)_{i}}{\\partial x^{2}}|_{x}\\;\\in\\;\\mathbb{R}^{n\\times n}.\n\n1. Formal guarantees are provided, i.e., robust constraint satisfaction, recursive feasibility (Thm. 1), and input-to-state stability (Thm. 2). Recursive feasibility is ensured by a novel treatment of the mismatch with respect to the nonlinear nominal prediction.\n\n2. An efficient sequential convex programming algorithm tailored to the robust MPC formulation is provided to enable real-time deployment. Each iteration consists of solving a nominal trajectory optimization with a quadratic program, updating a disturbance-feedback controller via Riccati recursions, and evaluating Jacobians of the nonlinear dynamics. The design is general and the provided code can be directly applied to systems with large state and input dimensions and long prediction horizons.\n\n3. Real-time feasibility (computation times) is demonstrated across different dynamics, including a quadcopter and a rocket landing. Robust performance is validated on the rocket-landing problem with steerable thrust including actuator dynamics, illustrated in Fig. 1, demonstrating robust constraint satisfaction with an average total latency of 19.7 [ms] per iteration.\nA comparison to a soft-constrained MPC baseline highlights increased safety and stability of the proposed approach.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在存在扰动和模型不匹配的情况下确保机器人控制的安全性和稳定性。  \n2. 如何实现快速、可扩展的非线性模型预测控制，以满足实时实施的需求。  \n\n【用了什么创新的方案】  \n提出了一种鲁棒模型预测控制（MPC）方法，通过将不确定的非线性系统分解为名义非线性动态模型、扰动反馈控制器和模型误差界限，联合优化这些组件以确保鲁棒约束满足、输入到状态的稳定性和递归可行性。采用高效的顺序凸编程算法实现实时部署，验证了在不同动态下的实时可行性，特别是在火箭着陆问题中展示了鲁棒性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "An Extended Kalman Filter for Systems with Infinite-Dimensional Measurements",
            "authors": "Maxwell M. Varley,Timothy L. Molloy,Girish N. Nair",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO)",
            "comment": "8 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18749",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18749",
            "arxiv_html_link": "https://arxiv.org/html/2509.18749v1",
            "abstract": "This article examines state estimation in discrete-time nonlinear stochastic systems with finite-dimensional states and infinite-dimensional measurements, motivated by real-world applications such as vision-based localization and tracking.\nWe develop an extended Kalman filter (EKF) for real-time state estimation, with the measurement noise\nmodeled as an infinite-dimensional random field.\nWhen applied to vision-based state estimation, the measurement Jacobians required to implement the EKF are shown to correspond to image gradients.\nThis result provides a novel system-theoretic justification for the use of image gradients as features for vision-based state estimation, contrasting with their (often heuristic) introduction in many computer-vision pipelines.\nWe demonstrate the practical utility of the EKF on a public real-world dataset involving the localization of an aerial drone using video from a downward-facing monocular camera.\nThe EKF is shown to outperform VINS-MONO, an established visual-inertial odometry algorithm, in some cases achieving mean squared error reductions of up to an order of magnitude.",
            "introduction": "In this paper we focus on state estimation for systems with finite-dimensional states and infinite-dimensional measurements.\nThis focus is motivated by vision-based state estimation, control, and localization problems that arise across robotics [1, 2, 3] and control [4, 5, 6].\nIn such problems, the measurements take the form of images with dimensions determined by the camera’s resolution (i.e., number of pixels), while the underlying state of interest (e.g., position and orientation) is typically relatively low-dimensional.\nWith modern cameras offering increasingly high-resolution images, the emergent challenge in many of these problems is how best to estimate a low-dimensional state with arbitrarily high-dimensional measurements.\nTraditional approaches from computer vision and robotics for processing high-dimensional measurements rely on (spatial) feature extraction [3].\nHowever, such approaches may fail to exploit the dynamics and uncertainty of the state estimates in determining which features to extract or how to weight them in computing a state estimate.\nIn this paper, we therefore take a different approach by formulating an extended Kalman filter (EKF) capable of processing an entire (infinite-dimensional) image domain, and assigning dynamic weights (via gains) to every pixel based on its contribution to state estimates.\n\nAlthough Kalman filters and their nonlinear variants, such as the EKF, have long been used for estimation in robotics and control, the vast majority of vision-based filters are feature-based [7, 8, 6, 1, 3].\nThe use of these filters thus typically involves first reducing images to a sparse set of extracted keypoints before applying standard Kalman filter techniques.\nIn contrast, our novel EKF is capable of operating directly on dense image data (i.e., pixel intensities) directly in real-time, avoiding the need to extract features, and preserving the image structure.\nA key insight that allows the formulation of our filter, is the modeling of the measurement noise as an infinite-dimensional random field. This allows us to construct a continuous image-domain measurement model that naturally integrates with the structure of an infinite-dimensional EKF. In doing so, we are able to derive a system-theoretic justification for using image gradients in the filter update step. These image gradients are usually introduced heuristically in computer-vision pipelines (cf. [3]), but here emerge from the principles of the filter design itself.\n\nA number of different approaches have been employed in early works to derive the Kalman filter for distributed parameter systems [9, 10, 11, 12], although none of these works examined systems with finite-dimensional states and infinite-dimensional measurements as presented here. A survey contextualizing the methods and results of these early derivations is given in [13] and a modern, comprehensive examination of control and estimation of distributed parameter systems is given in [14]. In the case of nonlinear distributed parameter systems, the EKF is generally utilized either by reducing the dimensionality of the underlying system before designing the estimator (the early lumping approach) [15], or designing an infinite-dimensional distributed parameter EKF and using some discretization scheme for real-world implementation (the late lumping approach) [16].\n\nThis article makes the following key contributions, extending the optimal linear filter work in [17, 18].\nFirstly, the optimal linear filter originally derived in that prior work is generalized to construct an EKF for systems with finite-dimensional states and infinite-dimensional measurements, and with both nonlinear state dynamics and nonlinear measurement equations.\nWe provide a derivation of this EKF and establish and interpret the measurement Jacobians that arise within it, with the latter relating to image gradients in the case of image measurements.\nWe verify the efficacy of the EKF for vision-based state estimation on a real-world dataset.\nSpecifically, the filter estimates the position, velocity, acceleration, and yaw of an aerial drone equipped with an Inertial Measurement Unit (IMU) as well as optical cameras providing measurements in the form of grayscale downward-facing images. The estimates are evaluated against the ground truth included in the dataset, and the results are compared with the performance of the well-established monocular visual-inertial odometry algorithm VINS-MONO [2], showing that our filter generally achieves superior or comparable state estimation performance.\n\nThis article is structured as follows. Section II presents the notational definitions used throughout this work. Section III will define the system model that we will analyze, as well as the assumptions used throughout. Section IV will describe a linearization of the previously described system model, in preparation for an application of the optimal linear filter derived in [18] to this linearized system. Section V will present the filter procedure and two results, Proposition V.1 and Proposition V.2, which help to simplify the implementation and reduce the computational complexity of our algorithm. Section VI will give details pertaining to the dataset, as well as how the filter is implemented on the data within, and our chosen measures of filter performance. Using these performance metrics, Section VII demonstrates an empirical verification of the proposed filter and compares the results with those of VINS-MONO.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在有限维状态和无限维测量的系统中进行状态估计。  \n2. 传统特征提取方法在处理高维测量时的局限性。  \n3. 如何实时处理整个图像域以提高状态估计的准确性。  \n\n【用了什么创新的方案】  \n本文提出了一种扩展卡尔曼滤波器（EKF），能够直接在密集图像数据上进行实时状态估计，而无需提取特征。通过将测量噪声建模为无限维随机场，EKF能够动态地为每个像素分配权重，从而更好地利用图像梯度进行状态更新。这种方法提供了对图像梯度作为特征使用的系统理论依据，并在实际应用中表现出优于现有视觉惯性里程计算法的性能。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Dual Iterative Learning Control for Multiple-Input Multiple-Output Dynamics with Validation in Robotic Systems",
            "authors": "Jan-Hendrik Ewering,Alessandro Papa,Simon F.G. Ehlers,Thomas Seel,Michael Meindl",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO)",
            "comment": "11 pages, 4 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.18723",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18723",
            "arxiv_html_link": "https://arxiv.org/html/2509.18723v1",
            "abstract": "Solving motion tasks autonomously and accurately is a core ability for intelligent real-world systems.\nTo achieve genuine autonomy across multiple systems and tasks, key challenges include coping with unknown dynamics and overcoming the need for manual parameter tuning, which is especially crucial in complex  Multiple-Input Multiple-Output (MIMO) systems.",
            "introduction": "Accurate reference tracking is a critical control capability for a wide range of real-world applications, from industrial manufacturing to service robotics and biomedical systems [1, 2, 3], which often involve complex MIMO system dynamics.\nFor these systems to be effective and user-friendly, they must be capable of self-reliantly adapting to new tasks and environments. In other words, it is required to learn to perform reference tracking autonomously.\nThis capability is crucial to eliminate the need for expert-provided model information or time-consuming manual tuning.\n\nIn repetitive settings,  Iterative Learning Control (ILC) is an established method that enables highly accurate reference tracking, given a reference trajectory [4, 5, 6].\nHowever, satisfactory learning performance is almost always dependent on human expert knowledge, such as access to model information or the manual tuning of algorithmic (hyper) parameters.\nThe necessary manual tuning effort is typically even more aggravated in MIMO systems due to cross-coupling effects and scale variations between different inputs and outputs [7].\nMoreover, learning performance is often system- or reference-specific, which hinders genuine autonomous deployment across numerous systems and tasks.\n\nHence, an ILC method must possess the following three characteristics in order to enable real-world systems to autonomously learn to solve reference tracking tasks.\nFirst, the ILC method must neither require prior model information nor the manual tuning of parameters to enable autonomous application.\nSecond, the ILC method has to be applicable to MIMO dynamics, as these are often present in realistic settings.\nAnd third, the ILC method should be validated – ideally on multiple – real-world systems.\nBased on these criteria, we continue to review the state of research in ILC.\n\nFirst, there exists an extensive class of so-called  Model-Based Iterative Learning Control (MB-ILC) methods that can yield remarkable reference tracking performance in various real-world applications.\nFor example,  Norm-Optimal Iterative Learning Control (NO-ILC) schemes have been applied to gantry robots [8] and stroke rehabilitation [9], and NO-ILC can readily be applied to MIMO dynamics [10, 11].\nSimilarly,  Frequency-Domain Iterative Learning Control (FD-ILC) has been successfully applied to real-world systems with MIMO dynamics such as a marine vibrator [12] or nano-positioning systems [13].\nDespite these achievements, the aforementioned and other MB-ILC methods are limited because they require prior model information and typically involve manual tuning of learning parameters, which hinders their autonomous application.\n\nTo overcome the need for prior model information, so-called  Data-Driven Iterative Learning Control (DD-ILC) methods have been developed.\nA common approach is to use the input/output trajectory pairs from previous trials to estimate the gradient of the tracking errors with respect to the input trajectory to update the latter [14, 15, 16].\nOn the other hand, several DD-ILC schemes use experimental data to estimate a model of the plant dynamics and combine this plant approximation with well-known MB-ILC methods [11, 17].\nFor instance, some approaches use  Recursive Least Squares (RLS) to estimate a model of the system dynamics and combine it with NO-ILC or adaptive ILC [18, 19, 20].\nOther approaches combine  Iterative Learning Identification (ILI) and ILC [21, 22] to iteratively learn a model and input trajectory without prior model information.\nFD-ILC has been combined with an iterative learning approach for MIMO dynamics using a pseudo-inversion approach [23].\nNotably, there are approaches that iteratively learn a dynamic linearization of the plant dynamics, which can be utilized in a NO-ILC update law [24, 25, 26, 27, 28].\nWhat is common among all of these methods is that they overcome the need for prior model information, and many of them are applicable to MIMO dynamics.\nHowever, most of these methods have not been validated in real-world experiments, and all of these methods require the manual tuning of learning parameters, which precludes the autonomous application of the learning methods.\n\nTo overcome the need for prior model information and manual parameter tuning, MB-ILC has been combined with repeated model learning using Gaussian processes and self-parametrization schemes [29, 30, 31], and some of the methods have been validated on different real-world systems [29].\nHowever, these approaches are limited in terms of their applicability and validation in MIMO dynamics.\nWe, hence, conclude that there is no DD-ILC method that is autonomous in the sense that it neither requires prior model information nor manual parameter tuning, is applicable to MIMO dynamics, and has been validated on multiple—possibly real-world—systems.\n\nTo address these three issues, we propose a novel MIMO DILC framework that builds on previous results [32] and enables autonomous learning of reference tracking tasks in real-world systems with MIMO dynamics.\nSpecifically, the contributions of this paper are threefold:\n\nFirst, a novel DILC scheme for simultaneous model and control learning in MIMO systems, while requiring neither prior model information nor manual parameter tuning. It exploits a novel iterative learning paradigm that generalizes ILC approaches for iterative model learning, thus enabling the learning of system models using established ILC methods. The algorithmic architecture is illustrated in Figure 1.\n\nSecond, a theoretical analysis providing convergence conditions of the proposed algorithm under mild assumptions. We emphasize that iterative model learning in complex MIMO systems poses significant challenges, such as an overparametrized model, for which we present novel analysis to prove convergence.\n\nThird, an extensive empirical validation with two real-world MIMO systems and a six-degree-of-freedom industrial robot simulation. We demonstrate, in contrast to the vast majority of existing works, the truly autonomous learning capabilities of DILC without any model information or human tuning effort. To the best of our knowledge, this is the first time that a DD-ILC method has solved different reference tracking tasks in multiple real-world systems with MIMO dynamics, without requiring prior model information or manual parameter tuning. We highlight that DILC solves many reference tracking tasks within 1010-2020 trials and learns even complex motions in less than 100100 iterations.\n\nThis paper is structured as follows.\nWe formally define the considered problem in Section II and introduce preliminaries on ILC in Section III.\nThe proposed method and its theoretical properties are detailed in Section IV.\nThe simulative and experimental results are presented in Section V.\nFinally, we conclude the paper in Section VI.\n\nNotation: We denote the set of real numbers by ℝ\\mathbb{R}, the set of natural numbers by ℕ\\mathbb{N}, the set of all natural numbers greater than or equal to a∈ℕa\\in\\mathbb{N} by ℕ≥a\\mathbb{N}_{\\geq a}, and the set of natural numbers in the interval [a,b]⊂ℕ[a,b]\\subset\\mathbb{N} by ℕ[a,b]\\mathbb{N}_{[a,b]}.\nWe denote vectors (matrices) by lower-case (upper-case) letters in bold, e. g., 𝐯∈ℝN\\mathbf{v}\\in\\mathbb{R}^{N} (𝐀∈ℝN×N\\mathbf{A}\\in\\mathbb{R}^{N\\times N}).\nIf not explicitly stated, all vectors are column vectors, and by writing [𝐯]i[\\mathbf{v}]_{i}, we refer to the ii-th entry of 𝐯\\mathbf{v}.\nBy writing [𝐀]i,j[\\mathbf{A}]_{i,j}, we refer to the ii-th entry of the jj-th column of 𝐀\\mathbf{A}.\nTo vectorize 𝐀\\mathbf{A}, we write vec​(𝐀)\\mathrm{vec}(\\mathbf{A}).\nThe Euclidean norm of a vector 𝐯\\mathbf{v} is denoted by ‖𝐯‖\\left\\lVert\\mathbf{v}\\right\\rVert, and the induced Euclidean norm of a matrix 𝐀\\mathbf{A} is denoted by ‖𝐀‖\\left\\lVert\\mathbf{A}\\right\\rVert.\nThe weighted norm with respect to a positive definite matrix 𝐖≻0\\mathbf{W}\\succ 0 with 𝐖=𝐖⊤\\mathbf{W}=\\mathbf{W}^{\\top} is denoted by ‖𝐯‖𝐖=𝐯⊤​𝐖𝐯\\left\\lVert\\mathbf{v}\\right\\rVert_{\\mathbf{W}}=\\sqrt{\\mathbf{v}^{\\top}\\mathbf{W}\\mathbf{v}}.\nWe denote the identity matrix of size N×NN\\times N by 𝐈N\\mathbf{I}_{N}, and the zero matrix of suitable dimension by 𝟎\\mathbf{0}.\nThe Kronecker product of two matrices 𝐀\\mathbf{A} and 𝐁\\mathbf{B} is 𝐀⊗𝐁\\mathbf{A}\\otimes\\mathbf{B}.\nWe recall that a function α:ℝ≥0→ℝ≥0\\alpha:\\mathbb{R}_{\\geq 0}\\rightarrow\\mathbb{R}_{\\geq 0} is of class 𝒦\\mathscr{K} if it is continuous, strictly increasing, and satisfies α​(0)=0\\alpha(0)=0.\nBy ℒ\\mathscr{L}, we refer to the class of functions θ:ℝ≥0→ℝ≥0\\theta:\\mathbb{R}_{\\geq 0}\\rightarrow\\mathbb{R}_{\\geq 0} that are continuous, non-increasing, and satisfy lims→∞θ​(s)=0\\lim_{s\\rightarrow\\infty}\\theta(s)=0, and by 𝒦​ℒ\\mathscr{K}\\negthinspace\\negthinspace\\mathscr{L} to the class of functions β:ℝ≥0×ℝ≥0→ℝ≥0\\beta:\\mathbb{R}_{\\geq 0}\\times\\mathbb{R}_{\\geq 0}\\rightarrow\\mathbb{R}_{\\geq 0} with β​(⋅,s)∈𝒦\\beta(\\cdot,s)\\in\\mathscr{K} and β​(r,⋅)∈ℒ\\beta(r,\\cdot)\\in\\mathscr{L} for any fixed s∈ℝ≥0s\\in\\mathbb{R}_{\\geq 0} and r∈ℝ≥0r\\in\\mathbb{R}_{\\geq 0}, respectively.\nLast, we denote the space of all block-lower-triangular Toeplitz matrices of dimension NN with sub-matrices, ∀n∈ℕ[1,N]\\forall n\\in\\mathbb{N}_{[1,N]}, 𝐓¯n∈ℝL×M\\bar{\\mathbf{T}}_{n}\\in\\mathbb{R}^{L\\times M}, by 𝒯NL,M\\mathcal{T}^{L,M}_{N}, that is,\n\n1. First, a novel DILC scheme for simultaneous model and control learning in MIMO systems, while requiring neither prior model information nor manual parameter tuning. It exploits a novel iterative learning paradigm that generalizes ILC approaches for iterative model learning, thus enabling the learning of system models using established ILC methods. The algorithmic architecture is illustrated in Figure 1.\n\n2. Second, a theoretical analysis providing convergence conditions of the proposed algorithm under mild assumptions. We emphasize that iterative model learning in complex MIMO systems poses significant challenges, such as an overparametrized model, for which we present novel analysis to prove convergence.\n\n3. Third, an extensive empirical validation with two real-world MIMO systems and a six-degree-of-freedom industrial robot simulation. We demonstrate, in contrast to the vast majority of existing works, the truly autonomous learning capabilities of DILC without any model information or human tuning effort. To the best of our knowledge, this is the first time that a DD-ILC method has solved different reference tracking tasks in multiple real-world systems with MIMO dynamics, without requiring prior model information or manual parameter tuning. We highlight that DILC solves many reference tracking tasks within 1010-2020 trials and learns even complex motions in less than 100100 iterations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂的MIMO系统中实现自主的参考跟踪任务？  \n2. 如何消除对先前模型信息和手动参数调整的需求？  \n3. 如何在多个真实世界系统中验证学习方法的有效性？  \n\n【用了什么创新的方案】  \n提出了一种新的MIMO DILC框架，能够在不需要先前模型信息或手动参数调整的情况下，实现对参考跟踪任务的自主学习。该框架结合了迭代学习控制（ILC）方法，提供了理论分析以证明算法的收敛性，并通过在两个真实世界的MIMO系统和一个六自由度工业机器人仿真中的广泛验证，展示了其真正的自主学习能力。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction",
            "authors": "Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO); Image and Video Processing (eess.IV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18566",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18566",
            "arxiv_html_link": "https://arxiv.org/html/2509.18566v1",
            "abstract": "Reconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond temporal resolution, making them a superior sensing choice for dynamic human reconstruction. Accordingly, we present a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. Specifically, a unified set of 3D Gaussians carries a learnable semantic attribute; only Gaussians classified as human undergo deformation for animation, while scene Gaussians stay static. To combat blur, we propose an event-guided loss that matches simulated brightness changes between consecutive renderings with the event stream, improving local fidelity in fast-moving regions. Our approach removes the need for external human masks and simplifies managing separate Gaussian sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.",
            "introduction": "Human reconstruction from monocular videos is a critical task in computer vision and graphics, with applications spanning virtual reality [1], augmented reality [2], and film production [3]. Recent neural rendering advancements, including Neural Radiance Fields (NeRFs)[4] and 3D Gaussian Splatting (3DGS)[5], enable highly-fidelity, photorealistic 3D reconstruction. Building on this, various 3D human reconstruction methods have emerged. Examples include 3DGS-Avatar [6] and ASH [7], which focus on animatable avatars, and HUGS [8], which reconstructs human and scene simultaneously using separate Gaussian sets.\n\nDespite these promising results, existing methods still face significant challenges.\nFirst, most approaches require an external human mask, necessitating a prior segmentation step that can introduce artifacts.\nSecond, rapid human motion in frame-based camera captures often leads to motion blur, deteriorating image quality.\nWhile some methods attempt to deblur RGB images or integrate event data for reconstruction, their generalizability is limited.\nExFMan [9] is a notable exception that leverages event data for dynamic human reconstruction but lacks static scene modeling.\n\nTo address these challenges, we introduce a unified framework for reconstructing animatable humans and static scenes from a monocular event camera (Fig. 1).\nUnlike HUGS [8], which uses separate Gaussian sets, our method encodes both human and scene in a single set of 3D Gaussians with semantic attributes, refined during training via rendering feedback.\nFurthermore, synthetic events generated from rendered images are aligned with real event streams, providing supervision that alleviates motion blur.\n\nWe evaluate our method on two newly created datasets, ZJU-MoCap-Blur and MMHPSD-Blur, generated by simulating motion blur to test performance under challenging conditions.\nExperiments show that our unified human-scene reconstruction framework surpasses the state-of-the-art HUGS [8], with notable gains on ZJU-MoCap-Blur: +19.5%19.5\\% PSNR, +3.95%3.95\\% SSIM, and –32.5%32.5\\% LPIPS.\nIn summary, our main contributions are:\n\nA novel framework for unified human and scene reconstruction using a single semantically attributed set of 3D Gaussians.\n\nThe integration of event data to mitigate motion blur and enhance the reconstruction quality of fast-moving subjects.\n\nAn extensive evaluation on self-generated motion-blurred datasets that demonstrates state-of-the-art performance in challenging high-speed scenarios.\n\n1. A novel framework for unified human and scene reconstruction using a single semantically attributed set of 3D Gaussians.\n\n2. The integration of event data to mitigate motion blur and enhance the reconstruction quality of fast-moving subjects.\n\n3. An extensive evaluation on self-generated motion-blurred datasets that demonstrates state-of-the-art performance in challenging high-speed scenarios.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何从单目视频中重建动态人类与静态场景，尤其是在快速运动下。  \n2. 现有方法依赖外部人类掩码，增加了处理复杂性与潜在伪影。  \n3. RGB帧在快速运动中容易产生运动模糊，影响重建质量。  \n\n【用了什么创新的方案】  \n提出了一种新颖的事件引导的人类-场景重建框架，通过单一的3D高斯点集联合建模动态人类和静态场景。该方法利用事件数据来减轻运动模糊，并通过事件引导损失提高快速运动区域的重建质量。此外，框架简化了高斯点集的管理，消除了对外部人类掩码的需求。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Policy Gradient with Self-Attention for Model-Free Distributed Nonlinear Multi-Agent Games",
            "authors": "Eduardo Sebastián,Maitrayee Keskar,Eeman Iqbal,Eduardo Montijano,Carlos Sagüés,Nikolay Atanasov",
            "subjects": "Systems and Control (eess.SY); Multiagent Systems (cs.MA); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18371",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18371",
            "arxiv_html_link": "https://arxiv.org/html/2509.18371v1",
            "abstract": "Multi-agent games in dynamic nonlinear settings are challenging due to the time-varying interactions among the agents and the non-stationarity of the (potential) Nash equilibria. In this paper we consider model-free games, where agent transitions and costs are observed without knowledge of the transition and cost functions that generate them. We propose a policy gradient approach to learn distributed policies that follow the communication structure in multi-team games, with multiple agents per team. Our formulation is inspired by the structure of distributed policies in linear quadratic games, which take the form of time-varying linear feedback gains. In the nonlinear case, we model the policies as nonlinear feedback gains, parameterized by self-attention layers to account for the time-varying multi-agent communication topology. We demonstrate that our distributed policy gradient approach achieves strong performance in several settings, including distributed linear and nonlinear regulation, and simulated and real multi-robot pursuit-and-evasion games.",
            "introduction": "Multi-robot problems encompass a variety of expected behaviors [1, 2, 3, 4], including cooperative, conflicting or competitive actions. For instance, in a perimeter-defense setting [5, 6], multiple teams must coordinate to effectively defend a region from potential attackers (Fig. 1). These kinds of problems can be formulated as multi-team dynamic games [7, 8, 9, 10, 11], where each multi-agent team is viewed as a player with specific goals and constraints, and where agents interact with teammates (intra-team interactions) and agents on other teams (inter-team interactions). These settings are typically nonlinear and dynamic, requiring complex interactions that evolve with time as a function of how the agents play the game. These challenging features are specially relevant when we seek distributed policies subject to the communication constraints imposed by the topology of the teams; and in the absence of a mathematical description of the game dynamics and costs, demanding model-free approaches that only rely on transition and cost samples to assess the performance of the teams. Inspired by distributed policies in linear quadratic games, we present a novel policy gradient approach to learn distributed policies for nonlinear dynamic games that are both effective and scalable.\n\nModel-based methods for nonlinear multi-agent dynamic games rely on iterative linearization of the system dynamics and quadratic approximation of the game cost [12, 13, 14]. This allows for fast computation with guarantees of convergence to a saddle configuration [15] but imposes a centralized calculation that limits the applicability in distributed settings, where the agent communication is restricted according to a graph topology. To overcome such limitations, it is possible to restrict the class of nonlinear multi-agent dynamic games to potential games [16, 17, 18], where it is assumed that a potential function exists such that the relative incentives in modifying one agent’s policy is equal to the difference in value of the potential function. Under this constraint, it is possible to derive algorithms that compute open-loop optimal trajectories for the agents under centralized [19] or distributed topological constraints [20, 17]. However, open-loop policies lack robustness and require knowledge on how the multi-agent topology will evolve with time. In contrast, we propose a novel policy parameterization that is distributed by construction and does not require network topology prediction. In all previous cases, a model of the system dynamics and the structure of the cost function is needed to compute the actions. Instead, to address general nonlinear multi-agent dynamic games, we propose a model-free policy gradient approach that relies only on transition and cost samples.\n\nModel-free solutions for games are limited due to the non-stationarity of the Nash equilibria (if one exists) [21, 22]. Traditional approaches either focus on providing theoretical guarantees of convergence or addressing practical settings assuming the existence of such Nash equilibria. An instance of the former is [23], where distributed linear quadratic regulators are learned assuming that the sequence of graphs representing the communication structure of the game is known. From a different perspective, when the linear cost function is known and the strategies of all players are available, the problem can be posed as a multi-team distributed optimization program [24, 25]. In practical settings, existing solutions rely on multi-agent reinforcement learning algorithms [26] that consider independent heterogeneous agents to apply policy gradient methods [27, 28, 29, 30]. In this work, we bring together the benefits of both alternatives by proposing a self-attention-based policy parameterization built from first principles and which enforces distributed execution constraints. The distributed policy is trained using a policy gradient learning method that considers, simultaneously, the policies of all the agents from all different teams, addressing the non-stationarity of the game in practice.\n\nOur main contribution is a policy gradient method for learning distributed policies in model-free nonlinear multi-agent dynamic games (Sec. II). Our approach uses a nonlinear feedback gain formulation of the agent policies, parameterized using self-attention layers (Sec. III). The use of self-attention enables to enforce intra- and inter-team graph constraints, handling time-varying communication and achieving invariance with respect to the total number of agents. Furthermore, a neural network parameterization of the policies motivates the use of a policy gradient method to learn effective and scalable policies in the model-free setting.\nThe method also allows to learn heterogeneous policies per team, such that the teams adjust to specific goals. We demonstrate that our method applies broadly, from linear quadratic settings under topology constraints to multi-agent reinforcement learning in competitive games with simulated and real robots (Sec. IV).",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在动态非线性多智能体游戏中学习分布式策略。  \n2. 如何处理多智能体之间的时间变化通信拓扑。  \n3. 如何在缺乏系统动态和成本函数模型的情况下实现模型无关的学习。  \n\n【用了什么创新的方案】  \n提出了一种基于自注意力机制的策略梯度方法，用于学习动态非线性多智能体游戏中的分布式策略。该方法通过非线性反馈增益的参数化，能够处理时间变化的多智能体通信结构，并且不需要对网络拓扑进行预测。通过使用神经网络参数化，方法能够有效地学习异质策略，适应不同团队的具体目标，并在多种设置中展示了良好的性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata",
            "authors": "Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "Accepted at NeurIPS 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18350",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18350",
            "arxiv_html_link": "https://arxiv.org/html/2509.18350v1",
            "abstract": "Accurate visual localization from aerial views is a fundamental problem with applications in mapping, large-area inspection, and search-and-rescue operations. In many scenarios, these systems require high-precision localization while operating with limited resources (e.g., no internet connection or GNSS/GPS support), making large image databases or heavy 3D models impractical. Surprisingly, little attention has been given to leveraging orthographic geodata as an alternative paradigm, which is lightweight and increasingly available through free releases by governmental authorities (e.g., the European Union). To fill this gap, we propose OrthoLoC, the first large-scale dataset comprising 16,425 UAV images from Germany and the United States with multiple modalities. The dataset addresses domain shifts between UAV imagery and geospatial data. Its paired structure enables fair benchmarking of existing solutions by decoupling image retrieval from feature matching, allowing isolated evaluation of localization and calibration performance. Through comprehensive evaluation, we examine the impact of domain shifts, data resolutions, and covisibility on localization accuracy. Finally, we introduce a refinement technique called AdHoP, which can be integrated with any feature matcher, improving matching by up to 95% and reducing translation error by up to 63%. The dataset and code are available at: https://deepscenario.github.io/OrthoLoC.",
            "introduction": "Visual localization for Unmanned Aerial Vehicles is essential for digital-twin modeling [60, 74], surveillance [29], search-and-rescue [51], and infrastructure inspection [34], yet faces unique challenges not addressed by ground-level localization systems. While ground-level approaches [56, 71, 70] benefit from similar viewpoints between images [59, 49, 57], aerial applications encounter dramatic perspective differences and require scalability over large areas [69, 72].\n\nCurrent UAV localization algorithms rely on retrieving the closest match from a database of posed images [72, 77], which is inaccurate, or on 3D models of the scene [69, 66], which are memory and computationally expensive. In limited resources settings, as it is often the case for connectivity-limited environments, this can result in accuracy degradation. Recent approaches like LoDLoc [78] improve storage efficiency by using  Level-of-Detail (LoD) but still assume unchanged environments, perform poorly in building-sparse areas such as highways, and its initialization depends on positioning sensors.\n\nIn contrast, a compelling solution involves geodata, such as orthographic aerial views (Digital Orthophotos) and elevation maps (Digital Surface Models). These provide a reliable, lightweight source for localizing UAV images, as shown in Figure˜1. Such data is increasingly accessible through free releases from European government geoportals [46, 17], and where public access is limited, can be synthesized using photogrammetric tools [20]. Geodata are scalable and better suited for low-resource settings. For example, covering an area of approximately 0.265 km² would require a 3D model of around 8 GB [69], whereas geodata requires about 30 times less memory. Surprisingly, no existing UAV localization approach seems to fully leverage these data sources. We believe this is mainly due to the absence of aligned cross-domain datasets and the lack of full-pose paired large-scale benchmarks specifically designed for localization using these types of geodata.\n\nTo fill this gap, we capture and release the  Orthographic Aerial Localization and Calibration Dataset (OrthoLoC). It comprises 5 main modalities such as UAV imagery, DOPs, DSMs, 3D point maps, and 3D meshes with a total of 16.4K images captured in 47 regions in 19 cities across 2 countries. Our dataset is the first to offer three key advantages: (1) paired UAV-geodata structure that decouples pose estimation from image retrieval, eliminating confounding error sources in the evaluations; (2) precise 6-DoF poses obtained through multi-view georeferenced photogrammetric reconstruction; and (3) additional reference data sources to increase the domain gaps in the dataset.\n\nWe have evaluated state-of-the-art methods on this novel localization and calibration task in a comprehensive benchmark. Additionally, we introduce a method-agnostic refinement technique called  Adaptive Homography Preconditioning (AdHoP) that further improves localization and calibration accuracy. The technique exploits the uniform structure of DOPs to perform homography-based warping by assuming quasi-planar surfaces common in built environments.\n\nOur evaluation reveals several insights. First, state-of-the-art matching algorithms can generalize to aerial perspectives but struggle with the substantial domain gap between perspective UAV imagery and orthographic reference data. Second, our AdHoP technique significantly reduces the perspective disparity, improving all metrics across the tested methods, particularly achieving up to 95% and 63% enhancements in matching and translation accuracy, respectively. Third, camera calibration in aerial settings presents unique challenges due to fundamental geometric ambiguities that affect parameters estimation. Finally, reference data characteristics including domain shifts, data resolutions, and covisibility. significantly impact localization performance, with higher resolution geodata providing improvement in accuracy.\n\nThe main contributions of this paper are: (1) OrthoLoC, the first UAV dataset providing alignment with geodata across multiple modalities and locations; (2) a unified benchmarking framework for UAV localization and calibration that integrates with state-of-the-art matching algorithms and includes our AdHoP technique for addressing perspective disparities; and (3) benchmarking results for camera localization and calibration and an analysis of performance factors including cross-domain challenges, data resolution effects, and covisibility.",
            "llm_summary": "【关注的是什么问题】  \n1. UAV视觉定位的准确性和资源限制下的挑战  \n2. 现有UAV定位算法对地理数据的利用不足  \n3. 跨域数据集缺乏导致的评估困难  \n\n【用了什么创新的方案】  \n本研究提出了OrthoLoC数据集，包含16,425幅UAV图像及多种地理数据，旨在解决UAV图像与地理数据之间的领域转移问题。通过配对结构，该数据集能够独立评估定位和校准性能。此外，提出的AdHoP技术可与任何特征匹配器集成，显著提高匹配精度和降低平移误差。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought",
            "authors": "Yu Ti Huang",
            "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18200",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18200",
            "arxiv_html_link": "https://arxiv.org/html/2509.18200v1",
            "abstract": "Conversational agents must translate egocentric utterances (e.g., “on my right”) into allocentric orientations (N/E/S/W). This challenge is particularly critical in indoor or complex facilities where GPS signals are weak and detailed maps are unavailable. While chain-of-thought (CoT) prompting has advanced reasoning in language and vision tasks, its application to multimodal spatial orientation remains underexplored. We introduce Conversational Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese conversational navigation projected from real-world environments, addressing egocentric-to-allocentric reasoning in non-English and ASR-transcribed scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: (1) extracting spatial relations, (2) mapping coordinates to absolute directions, and (3) inferring user orientation. A curriculum learning strategy progressively builds these capabilities on Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of resource-constrained settings. Experiments show that MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. Moreover, MCoT demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching. The model also maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity. These findings highlight the potential of structured MCoT spatial reasoning as a path toward interpretable and resource-efficient embodied navigation. Code and data are available at https://github.com/yu-ti-huang/Conversational-Orientation-Reasoning.",
            "introduction": "Humans naturally describe spatial environments in egocentric (agent-centric) terms (e.g., “The exit is on my right”), whereas navigation systems typically operate on allocentric (world-centric) orientations such as north, south, east, and west. Conversational navigation has emerged as a promising paradigm that enables users to specify goals through dialogue, offering a natural and human-centered means of guidance in unfamiliar environments Sundar et al. (2024); Sheshadri & Hara (2024); Kaniwa et al. (2024); Liu et al. (2024); Levi & Kadar (2025). However, the crucial problem of grounding egocentric language into allocentric orientation remains underexplored. Current approaches typically assume access to GPS, detailed maps, or fixed global frames de Vries et al. (2018); Chen et al. (2020), and have concentrated primarily on English-based scenarios. Recent progress has also relied heavily on large-scale models Ghosh et al. (2024); Tang et al. (2023), which show strong reasoning abilities but demand substantial computational resources, hindering deployment in resource-constrained settings such as mobile navigation and edge devices.\n\nResearch in embodied AI and MCoT has advanced vision-language navigation and action planning Mu et al. (2023); Sun et al. (2024); Liu et al. (2025); Shen et al. (2025); Pareek et al. (2024), but orientation reasoning from natural language has been largely overlooked. These approaches typically assume that the agent’s orientation is already known or operate on high-level action spaces rather than inferring fundamental spatial relationships de Vries et al. (2018); Chen et al. (2020). Meanwhile, large audio-language models (LALMs) Zhang et al. (2023); Xie & Wu (2024); Fu et al. (2025); Défossez et al. (2024) have advanced speech understanding and dialogue Tang et al. (2024); Gong et al. (2023); Ghosh et al. (2024); Kong et al. (2024), yet their reasoning abilities remain limited to perception-level tasks such as transcription or summarization yu Huang et al. (2024); Yang et al. (2024); Wang et al. (2025); Shi et al. (2025). While recent efforts like Audio-CoT Ma et al. (2025) show promise for enhanced speech-based reasoning, the challenge of transforming egocentric spatial descriptions into allocentric orientation inference remains unaddressed.\n\nTo address this gap, we introduce Conversational Orientation Reasoning (COR), a new benchmark for egocentric-to-allocentric orientation reasoning in Traditional Chinese conversational navigation. COR is derived from real-world urban transportation environments in Taiwan, projected into structured grid representations. Unlike prior studies that rely on vision or raw audio, COR combines ASR-transcribed egocentric language with structured landmark coordinates, evaluating both clean text and ASR transcripts to simulate realistic recognition errors. COR addresses the lack of non-English benchmarks in multimodal spatial reasoning, particularly under noisy ASR conditions.\n\nOur study is guided by three research questions:\n\nRQ1 (Effectiveness): How effective is multimodal CoT prompting for orientation reasoning compared to unimodal and unstructured baselines?\n\nRQ2 (Component analysis): What are the contributions of ASR preprocessing, multimodal fusion, and structured CoT steps?\n\nRQ3 (Robustness and generalization): How robust is the approach to linguistic variation, and how well does it generalize across different spatial domains?\n\nOur contributions are as follows:\n\nTask and benchmark. We introduce the COR benchmark for egocentric-to-allocentric orientation reasoning, combining ASR-transcribed speech with landmark coordinates.\n\nFramework. We develop a multimodal CoT framework with a structured three-step reasoning process that integrates noisy transcripts with spatial signals for orientation inference.\n\nEvaluation. We provide extensive experiments in Traditional Chinese, demonstrating effectiveness, component contributions, and robustness validation across linguistic variation, cross-domain generalization, and referential ambiguity beyond English-centric research.\n\n1. RQ1 (Effectiveness): How effective is multimodal CoT prompting for orientation reasoning compared to unimodal and unstructured baselines?\n\n2. RQ2 (Component analysis): What are the contributions of ASR preprocessing, multimodal fusion, and structured CoT steps?\n\n3. RQ3 (Robustness and generalization): How robust is the approach to linguistic variation, and how well does it generalize across different spatial domains?",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将自我中心的语言描述转换为世界中心的方向推理？  \n2. 在缺乏GPS和详细地图的情况下，如何实现室内导航的有效性？  \n3. 如何在多模态环境中处理ASR识别错误和语言变化？  \n\n【用了什么创新的方案】  \n我们提出了对话方向推理（COR），这是一个新的基准，旨在解决自我中心到世界中心的方向推理问题。我们开发了一个多模态链式思维（MCoT）框架，通过结构化的三步推理过程，将ASR转录的语音与地标坐标相结合，提取空间关系、映射坐标到绝对方向，并推断用户的方向。该框架在资源受限的环境中表现出色，能够有效应对噪声条件下的对话和跨领域评估。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation",
            "authors": "Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin",
            "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18198",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18198",
            "arxiv_html_link": "https://arxiv.org/html/2509.18198v1",
            "abstract": "Autonomous systems have advanced significantly, but challenges persist in accident-prone environments where robust decision-making is crucial. A single vehicle’s limited sensor range and obstructed views increase the likelihood of accidents. Multi-vehicle connected systems and multi-modal approaches, leveraging RGB images and LiDAR point clouds, have emerged as promising solutions. However, existing methods often assume the availability of all data modalities and connected vehicles during both training and testing, which is impractical due to potential sensor failures or missing connected vehicles. To address these challenges, we introduce a novel framework MMCD (Multi-Modal Collaborative Decision-making) for connected autonomy. Our framework fuses multi-modal observations from ego and collaborative vehicles to enhance decision-making under challenging conditions. To ensure robust performance when certain data modalities are unavailable during testing, we propose an approach based on cross-modal knowledge distillation with a teacher-student model structure. The teacher model is trained with multiple data modalities, while the student model is designed to operate effectively with reduced modalities. In experiments on connected autonomous driving with ground vehicles and aerial-ground vehicles collaboration, our method improves driving safety by up to 20.7%{\\it 20.7}\\%, surpassing the best-existing baseline in detecting potential accidents and making safe driving decisions. More information can be found on our website https://ruiiu.github.io/mmcd.",
            "introduction": "Autonomous technology has rapidly evolved over the past few decades, with advancements in perception [1, 2, 3, 4, 5], decision-making [6, 7, 8], and control systems [9, 10]. However, the deployment of autonomous vehicles still face challenges, particularly in accident-prone scenarios. These scenarios demand high robustness and reliability, as any failure in decision-making could have severe consequences. A single vehicle navigating these scenarios is prone to have accidents due to occlusions and limited sensor range. One promising solution to mitigate these risks is to have multi-vehicle connected systems [11, 12, 13]. By sharing information, vehicles can expand their field of view and reduce the chances of accidents. Another promising direction is the use of multi-modal data [14, 15, 16, 17], such as RGB images and LiDAR point clouds, to enhance the perception and decision-making capabilities of autonomous systems. Recent works have combined these two paradigms to develop multi-vehicle, multi-modal systems [18, 19, 20, 21], leveraging both connectivity and diverse sensor data to further improve autonomous driving performance.\n\nHowever, existing works often assume that the ego vehicle has consistent access to all sensors and connected vehicles during both training and testing. For example, methods utilizing both RGB and LiDAR data for training [14, 15, 22] assume the availability of both modalities during testing. This is not always realistic; for instance, LiDAR sensors may malfunction or become unavailable during testing, leaving only RGB data accessible, or some connected vehicles may not be able to share data, as shown in Fig. 1. Additionally, cost efficiency is a crucial consideration, as LiDAR sensors are more expensive than RGB cameras. Reducing the reliance on LiDAR sensors while still achieving high performance with RGB-only models during testing presents a more cost-effective solution.\n\nTo address these challenges, we introduce a novel multi-modal collaborative decision-making framework for connected autonomy, enabling the ego vehicle to make informed decisions by leveraging shared multi-modal data from collaborative vehicles. To handle scenarios where certain data modalities are missing during testing, we propose an approach based on knowledge distillation (KD) with a teacher-student model structure. Our multi-modal framework serves as the teacher model, trained with multiple data modalities (e.g., RGB and LiDAR), while the student model operates with reduced modalities (e.g., RGB). The knowledge distillation process ensures the student model maintains robust performance even with missing modalities during test time.\n\nIn summary, the main contributions of this paper are:\n\nWe introduce MMCD, a novel multi-modal collaborative decision-making framework for connected autonomy. Our approach fuses single or multi-modal observations provided by ego or connected vehicles in a principled way to make decisions for the ego vehicle in accident-prone scenarios. Our method improves the driving safety by up to 20.7%\\bf 20.7\\% in experiments on connected autonomous driving with ground vehicles and aerial-ground vehicles collaboration, outperforming the best-existing baseline.\n\nWe propose a cross-modal knowledge distillation-based approach for MMCD. Our model is trained with multi-modal cues (e.g., LiDAR and RGB) from connected vehicles but executes using single-modality observations (e.g., RGB). This design ensures robust performance in the presence of missing modalities during testing.\n\n1. We introduce MMCD, a novel multi-modal collaborative decision-making framework for connected autonomy. Our approach fuses single or multi-modal observations provided by ego or connected vehicles in a principled way to make decisions for the ego vehicle in accident-prone scenarios. Our method improves the driving safety by up to 20.7%\\bf 20.7\\% in experiments on connected autonomous driving with ground vehicles and aerial-ground vehicles collaboration, outperforming the best-existing baseline.\n\n2. We propose a cross-modal knowledge distillation-based approach for MMCD. Our model is trained with multi-modal cues (e.g., LiDAR and RGB) from connected vehicles but executes using single-modality observations (e.g., RGB). This design ensures robust performance in the presence of missing modalities during testing.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在事故多发环境中实现鲁棒的决策制定？  \n2. 如何处理测试时缺失数据模态的情况？  \n\n【用了什么创新的方案】  \n提出了MMCD（Multi-Modal Collaborative Decision-making）框架，通过融合来自自我和协作车辆的多模态观察，增强决策能力。采用基于知识蒸馏的教师-学生模型结构，教师模型使用多模态数据进行训练，而学生模型则在测试时能够有效地使用减少的模态，从而确保在某些数据模态缺失时仍能保持鲁棒性能。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        }
    ],
    "2025-09-25": [
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation",
            "authors": "Shaofeng Yin,Yanjie Ze,Hong-Xing Yu,C. Karen Liu,Jiajun Wu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "Website:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.20322",
            "code": "https://visualmimic.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20322",
            "arxiv_html_link": "https://arxiv.org/html/2509.20322v1",
            "abstract": "Humanoid loco-manipulation in unstructured environments demands tight integration of egocentric perception and whole-body control. However, existing approaches either depend on external motion capture systems or fail to generalize across diverse tasks. We introduce VisualMimic, a visual sim-to-real framework that unifies egocentric vision with hierarchical whole-body control for humanoid robots. VisualMimic combines a task-agnostic low-level keypoint tracker—trained from human motion data via a teacher-student scheme—with a task-specific high-level policy that generates keypoint commands from visual and proprioceptive input. To ensure stable training, we inject noise into the low-level policy and clip high-level actions using human motion statistics. VisualMimic enables zero-shot transfer of visuomotor policies trained in simulation to real humanoid robots, accomplishing a wide range of loco-manipulation tasks such as box lifting, pushing, football dribbling, and kicking. Beyond controlled laboratory settings, our policies also generalize robustly to outdoor environments.\nVideos are available at: visualmimic.github.io",
            "introduction": "How do humans manage to push a box that is too heavy to move with only their arms? We start with vision perception to localize the box and rely on visual feedback to guide our interaction with the box. To generate sufficient force, we might bend down and push with our hands, lean in with the strength of our arms and shoulders, or even nudge the box forward with our feet. In such cases, every part of the body can be brought into play to accomplish the task. These strategies underscore two fundamental aspects of human loco-manipulation: egocentric visual perception and whole-body dexterity.\n\nEquipping humanoid robots with such human-like object interaction abilities has been a long-standing challenge. Current approaches can be categorized into three main paradigms based on tasks: First, locomotion-focused methods [1, 2] that excel at terrain traversal but do not address object interaction. Second, approaches that rely on external motion capture systems [3, 4] for object state estimation, restricting their deployment to controlled laboratory environments. Third, vision-based methods for object interaction, which follow two distinct paths: 1) imitation learning approaches [5, 6, 7] that train visuomotor policies via human demonstrations, which are constrained by the scarcity of large-scale demonstration data and result in limited generalization capabilities; and 2) sim-to-real reinforcement learning (RL) methods [8, 9] that offer greater robustness and generalizability; however, vision-based RL is currently limited to simple environmental interactions such as sitting [8] and stair climbing [8, 9], falling significantly short of human-level object interaction abilities, due to the large exploration and action space of humanoid robots.\n\nWe aim to take one step forward on the pathway of sim-to-real RL for visual humanoid-object interaction. To make sim-to-real RL generalize better, we adopt a hierarchical design comprising low-level and high-level policies. In such a hierarchical framework, the task-agnostic low-level policy takes care of balanced control and tracks the command sent by the high-level policy, and the task-specific high-level policy generates simplified tracking commands conditioning on egocentric vision input. This design enables more effective task-specific training. We formulate the command interface as body keypoints (root, hands, feet, head) to ensure both compactness and expressiveness.\n\nTo obtain a low-level keypoint tracker that performs human-like behaviors while tracking commands, we curate human motion data and supervise the tracker via motion imitation rewards. However, because keypoint commands alone do not capture the entirety of human motion, we observe that the keypoint tracker can track target keypoints while not perfectly producing human-like behaviors. To address this problem, we adopt a teacher–student training scheme: 1) We first train a motion tracker with full access to current and future whole-body motions, thereby capable of precisely following human reference motions; 2) We then distill this motion tracker into a keypoint tracker that operates on simplified keypoint commands. By doing so, our keypoint tracker captures human motion behaviors while still maintaining a compact command space. Notably, our keypoint tracker is task-agnostic and shared across tasks once trained.\n\nBuilt upon this general keypoint tracker, we train a high-level keypoint generator via sim-to-real RL. Directly training polices via visual RL significantly slows down the training and leads to non-optimal solutions. Therefore, we also apply a teacher–student scheme: 1) We first train a state-based policy with privileged access to object states, enabling them to solve tasks effectively; 2) We then distill the state-based policy into the visuomotor policy that rely solely on egocentric vision and robot proprioception, making it ready for real-world deployment without external object state estimation. To address the large visual sim-to-real gap (Fig. 8), we apply heavy masking to depth images in simulation, approximating real-world sensor noise.\n\nDue to the exploration nature of RL, we find that the high-level policy training is not stable when the high-level policies explore the action space that is beyond the human motion space (HMS) present in training motion datasets. We adopt two strategies to alleviate this problem: 1) injecting noise during training the low-level policy to help it adapt to potentially noisy commands from the high-level policy, and 2) clipping actions from the high-level policy to keep them within the feasible HMS.\n\nThe resulting framework, VisualMimic, enables us to obtain robust and generalizable visuomotor policies that can zero-shot transfer to the real robot, across a broad range of humanoid loco-manipulation tasks, with relatively simple task-specific reward design and without requiring paired human-object motion data. For real-world experiments (Fig. 4 and Fig. 3), we show that our humanoid robot can 1) lift a 0.5-kilogram box to a height of 1 meter, 2) push a very large box (similar height as the robot and weight 3.8 kilograms) straight and steady with its whole body, 3) dribble a football with the fluency of an experienced player, and 4) kick a box forward with alternating feet. Notably, we also show that our visuomotor policies achieve stable performance in outdoor scenarios, showing strong robustness to real-world variability such as lighting changes and uneven ground.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现人形机器人在非结构化环境中的物体交互与运动控制。  \n2. 现有方法依赖于外部运动捕捉系统，限制了其在多样化任务中的泛化能力。  \n3. 如何通过视觉反馈和全身控制来提升人形机器人的物体操控能力。  \n\n【用了什么创新方法】  \nVisualMimic提出了一种视觉sim-to-real框架，结合了任务无关的低级关键点跟踪器和任务特定的高级策略。低级策略通过教师-学生方案从人类运动数据中训练，确保稳定训练并适应高层策略的指令。通过注入噪声和剪辑高层动作，VisualMimic实现了在多种人形运动操控任务中的零-shot转移，表现出强大的鲁棒性和泛化能力，能够在真实环境中稳定执行如箱子搬运、足球运球等任务。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies",
            "authors": "Remo Steiner,Alexander Millane,David Tingdahl,Clemens Volk,Vikram Ramasamy,Xinjie Yao,Peter Du,Soha Pouya,Shiwei Sheng",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted to CoRL 2025 Workshop RemembeRL",
            "pdf_link": "https://arxiv.org/pdf/2509.20297",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20297",
            "arxiv_html_link": "https://arxiv.org/html/2509.20297v1",
            "abstract": "End-to-end learning of robot control policies, structured as neural networks, has emerged as a promising approach to robotic manipulation. To complete many common tasks, relevant objects are required to pass in and out of a robot’s field of view.\nIn these settings, spatial memory - the ability to remember the spatial composition of the scene - is an important competency.\nHowever, building such mechanisms into robot learning systems remains an open research problem.\nWe introduce mindmap (Spatial Memory in Deep Feature Maps for 3D Action Policies), a 3D diffusion policy that generates robot trajectories based on a semantic 3D reconstruction of the environment.\nWe show in simulation experiments that our approach is effective at solving tasks where state-of-the-art approaches without memory mechanisms struggle.\nWe release our reconstruction system111github.com/nvidia-isaac/nvblox, training code222github.com/NVlabs/nvblox_mindmap, and evaluation tasks22footnotemark: 2 to spur research in this direction.",
            "introduction": "Designing generalist robot manipulation policies remains a holy grail of robotics.\nSuch policies would perform manipulation tasks with a high level of competence and be instructed to do so in natural language.\nRecent advances in deep learning, vision, and natural language processing have, for the first time, brought this goal within reach; however, significant challenges remain.\n\nExisting approaches to developing learned manipulation policies generally aim to learn a mapping from sensor observations to robot control signals [2, 3, 4, 5].\nThese models typically employ transformer-based architectures to process image and proprioceptive inputs to generate control signals.\nSuch methods have shown an impressive ability to complete language-guided manipulation tasks.\nOne limitation of several leading approaches, however, is that the generation of output signals is conditioned on current visual observations only.\nSuch approaches lack spatial memory - the ability to remember the spatial and semantic composition of the scene (see [6] for a taxonomy of robot memory).\nThis leads to surprising limitations to their capabilities.\nAlthough some methods incorporate temporal information by maintaining a temporal window of past images, these approaches have drawbacks of their own (see Section 2).\n\nIn this work, we introduce mindmap, an approach that combines a diffusion policy with a metric-semantic 3D reconstruction of the scene.\nmindmap generates trajectories of 3D end-effector poses in the reconstructed space.\nThis approach allows the policy to generate actions that depend on parts of the scene that are outside of the camera’s current  Field of View (FOV).\nOur experiments show that, on tasks requiring spatial memory, mindmap is effective in completing tasks on which several current approaches struggle.\n\nContributions: \nIn this paper, we contribute tools for extending 3D manipulation policies with spatial memory.\nIn particular, we release metric-semantic mapping333nvidia-isaac.github.io/nvblox/pages/torch_examples_deep_features in nvblox [7], our GPU-accelerated reconstruction library11footnotemark: 1, in addition to our training code22footnotemark: 2, and simulation environments22footnotemark: 2 for testing spatial memory.\nWe demonstrate the efficacy of these tools by extending a state-of-the-art 3D diffusion policy [1].\nWe show that by making changes to the architecture and training, the policy’s performance, on challenging tasks that require spatial memory, is significantly improved.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在机器人控制策略中整合空间记忆以提高任务完成能力。  \n2. 现有方法在处理需要空间记忆的任务时表现不佳。  \n3. 如何利用3D语义重建来生成有效的机器人轨迹。  \n\n【用了什么创新方法】  \n本研究提出了mindmap，一个结合扩散策略与度量-语义3D重建的框架。该方法生成的3D末端执行器姿态轨迹能够依赖于当前视野外的场景部分，从而有效解决了需要空间记忆的任务。实验结果表明，mindmap在这些任务上的表现显著优于现有方法，展示了其在机器人操控中的潜力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video",
            "authors": "Georgios Tziafas,Jiayun Zhang,Hamidreza Kasaei",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20286",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20286",
            "arxiv_html_link": "https://arxiv.org/html/2509.20286v1",
            "abstract": "Learning visuomotor policies from expert demonstrations is an important frontier in modern robotics research, however, most popular methods require copious efforts for collecting teleoperation data and struggle to generalize out-of-distribution. Scaling data collection has been explored through leveraging human videos, as well as demonstration augmentation techniques. The latter approach typically requires expensive simulation rollouts and trains policies with synthetic image data, therefore introducing a sim-to-real gap. In parallel, alternative state representations such as keypoints have shown great promise for category-level generalization. In this work, we bring these avenues together in a unified framework: PAD (Parse-Augment-Distill), for learning generalizable bimanual policies from a single human video. Our method relies on three steps: (a) parsing a human video demo into a robot-executable keypoint-action trajectory, (b) employing bimanual task-and-motion-planning to augment the demonstration at scale without simulators, and (c) distilling the augmented trajectories into a keypoint-conditioned policy. Empirically, we showcase that PAD outperforms state-of-the-art bimanual demonstration augmentation works relying on image policies with simulation rollouts, both in terms of success rate and sample/cost efficiency.\nWe deploy our framework in six diverse real-world bimanual tasks such as pouring drinks, cleaning trash and opening containers, producing one-shot policies that generalize in unseen spatial arrangements, object instances and background distractors.\nSupplementary material can be found in the project webpage https://gtziafas.github.io/PAD_project/.",
            "introduction": "Visuomotor policy learning for robot manipulation has seen great success in recent years [1, 2, 3, 4, 5], yet it typically demands costly and time-consuming data collection from expert demonstrators.\nThis data-hungriness stems from the different required axes of generalization: a competent policy must generalize in unseen object arrangements (spatial) and object instances (object), as well as be robust to environmental conditions such as scene background, camera placement etc. (background).\nAs a result, most common policies struggle to generalize in out-of-distribution scenarios where corresponding data has not been collected.\nA recent methodology to tackle this data scarcity is to tap into the vast repository of videos available in the web, showcasing humans interacting with objects in diverse scenarios [6, 7, 8, 9, 10].\nHere the main challenge is bridging the embodiment gap between humans and robot morphologies [11, 12, 13].\nAlternatively, a recent line of works aims at dealing with spatial generalization by augmenting a small number of source demos with structured, object-centric task-and-motion planning (TAMP) procedures [14, 15, 16, 17].\nHowever, most works train image policies that require calibrated digital twins and expensive on-robot rollouts to generate the augmentations, therefore introducing a visual sim-to-real gap, while still struggling with object and background generalization.\nWhen it comes to bimanual manipulation, additional considerations related to arm collaboration strategies for different task scenarios further complicate data collection / generation.\n\nIn this work we wish to tackle these challenges by proposing PAD (Parse-Augment-Distill), a unified framework for learning bimanual visuomotor policies from a single human video demonstration.\nOur framework works in three steps (see Fig. 1): (a) parsing the video into robot-executable data, (b) augmenting the data in a simulation-free fashion and, (c) distilling the augmented data into a closed-loop policy.\n\nIn our work, we explicitly seek spatial, object and background generalization.\nTo accommodate this, we utilize 3D keypoint coordinates as state representations for our trained policy, which offers three important advantages:\nFirst, keypoints abstract the visual scene into a low-dimensional geometric representation, which is task-specific and decoupled from object semantics, and therefore has empirically shown to aid in sample-efficiency and robustness to background noise [18, 13, 19].\nSecond, keypoints facilitate category-level object generalization, inherited by the open-world capabilities of pretrained vision models for identifying semantic correspondences [20, 21, 22].\nFinally, 3D point states enable efficient spatial augmentations, as keypoint coordinates can be computed on-the-fly through 3D rigid geometry assumptions [23].\nThis alleviates the need for a digital twin and expensive simulation rollouts, which would be required by a typical image policy to obtain image observations [14, 15, 16, 17].\nIn turn, this significantly improves data collection time and bridges the sim-to-real gap introduced by simulators.\n\nConcretely, in PAD we introduce a general TAMP framework for spatial demo augmentations, specialized for bimanual manipulation.\nTo that end, we introduce bimanual task templates, symbolic representations that declare information about each arm’s object assignments, involved contacts and requirements for arm synchronization, while abstracting away the specific semantics of the task.\nWe particularly focus on handling issues related to bimanual manipulation, such as out-of-range arm-object assignments and re-synchronization between the arms during motion planning, which are missing from previous works in bimanual demo augmentation [17].\nOur augmentation framework is general, cost-efficient and embodiment-agnostic, as it uses human video as the source demo that can be mapped to any given morphology.\nFinally, we use prescribed 3D keypoints as our state representation instead of RGB images or point-clouds, and accompany them with augmentations that aid the policy in object generalization.\nTo distill the augmented data, we introduce Kp-RDT, an adapted version of RDT [4] for learning bimanual diffusion policies with keypoint conditioning.\n\nEmpirically, we show that our framework outperforms state-of-the-art bimanual demo augmentation methods [17] in four simulation tasks from the DexMimicGen robosuite benchmark [24], both in terms of success rates, as well as sample-efficiency and data generation time.\nWe further apply our framework in six diverse real-world tasks and show that PAD obtains policies that generalize to unseen spatial arrangements, object instances and background scene noise, while doing so from a single human demonstration.\n\nIn summary, our contributions with this work are threefold:\n\nWe introduce PAD, a unified framework for generalizable bimanual policy learning from a human video.\n\nWe propose a general bimanual TAMP framework for spatial demo augmentations, applicable to a wide variety of manipulation skills and arm-coordination strategies, as well as open-ended object categories.\n\nWe perform extensive robot experiments in 10 tasks, 4 in simulation and 6 with hardware, demonstrating significant gains compared to previous works in terms of success rates and sample/cost efficiency, as well as strong generalization in real-world tasks.\n\n1. We introduce PAD, a unified framework for generalizable bimanual policy learning from a human video.\n\n2. We propose a general bimanual TAMP framework for spatial demo augmentations, applicable to a wide variety of manipulation skills and arm-coordination strategies, as well as open-ended object categories.\n\n3. We perform extensive robot experiments in 10 tasks, 4 in simulation and 6 with hardware, demonstrating significant gains compared to previous works in terms of success rates and sample/cost efficiency, as well as strong generalization in real-world tasks.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何从单个视频中学习可泛化的双手视觉运动策略。  \n2. 如何解决数据收集的高成本和时间消耗问题。  \n3. 如何实现空间、对象和背景的泛化能力。  \n\n【用了什么创新方法】  \n本研究提出了PAD（Parse-Augment-Distill）框架，通过三个步骤实现从单个视频学习双手策略：解析视频为机器人可执行的关键点-动作轨迹，利用无模拟器的任务与运动规划进行演示数据增强，最后将增强的数据蒸馏为关键点条件政策。实验结果表明，PAD在成功率和样本/成本效率上优于现有的双手演示增强方法，能够在六个真实世界任务中生成可泛化的策略，适应未见的空间排列和背景干扰。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "HL-IK: A Lightweight Implementation of Human-Like Inverse Kinematics in Humanoid Arms",
            "authors": "Bingjie Chen,Zihan Wang,Zhe Han,Guoping Pan,Yi Cheng,Houde Liu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20263",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20263",
            "arxiv_html_link": "https://arxiv.org/html/2509.20263v1",
            "abstract": "Traditional IK methods for redundant humanoid manipulators emphasize end-effector (EE) tracking, frequently producing configurations that are valid mechanically but not human-like. We present Human-Like Inverse Kinematics (HL-IK), a lightweight IK framework that preserves EE tracking while shaping whole-arm configurations to appear human-like—without full-body sensing at runtime. The key idea is a learned elbow prior: using large-scale human motion data retargeted to the robot, we train a FiLM-modulated spatio-temporal attention network (FiSTA) to predict the next-step elbow pose from the EE target and a short history of EE–elbow states. This prediction is incorporated as a small residual alongside EE and smoothness terms in a standard Levenberg–Marquardt optimizer, making HL-IK a drop-in addition to numerical IK stacks. Over 183k simulation steps, HL-IK reduces arm-similarity position and direction error by 30.6% and 35.4% on average, and by 42.2% and 47.4% on the most challenging trajectories. Hardware teleoperation on a robot distinct from simulation further confirms the gains in anthropomorphism. HL-IK is simple to integrate, adaptable across platforms via our pipeline, and adds minimal computation, enabling human-like motions for humanoid robots. Project page: https://hl-ik.github.io/",
            "introduction": "A robotic arm can be defined as a series of links connected together by joints [1]. Inverse kinematics (IK) is a fundamental problem in such robotics, traditionally formulated to compute joint configurations that achieve a specified end-effector (EE) pose. For industrial manipulators, this formulation is often sufficient, since the primary objective is to place the tool center point at the desired location with high precision. Classical IK solvers—whether based on closed-form derivations [2, 3, 4, 5], numerical iterations [6, 7], or optimization frameworks [8, 9, 10] — focus almost exclusively on EE tracking.\n\nFor redundant robotic arms, the inverse solution to a given EE pose is often not unique, with infinitely many possible configurations [11, 12]. When only the EE pose is constrained, the intermediate joints remain underdetermined [13], which can lead to solutions that are mechanically valid but visually unnatural and non-human-like. In scenarios such as humanoid robot teleoperation [14, 15, 16], beyond accurate EE tracking, we also aim for the robot’s overall arm configuration to closely resemble that of the human arm, thereby achieving a higher level of anthropomorphism. Existing methods [17, 18] often rely on external cameras to capture human body keypoints and align them with robot joints to improve configuration similarity. Yet, such approaches not only require additional perception inputs but also typically do not treat EE tracking as the primary constraint, and thus cannot be regarded as strict IK solutions. Therefore, our goal is to develop a system that, given only the desired EE pose as input (as in traditional IK), not only ensures precise EE tracking but also achieves close similarity between the human and robot arm configurations.\n\nTo realize this goal, we first model the human arm as a four-point, three-segment kinematic chain comprising the shoulder, elbow, wrist, and fingertips (the human EE) [19]. For a fixed EE pose, the dominant redundancy manifests as the elbow “swivel” about the shoulder–wrist axis. Aligning the elbow pose effectively sets the arm plane and the forearm pointing direction, thereby resolving the main ambiguity and yielding anthropomorphic configurations without sacrificing EE accuracy. In other words, once the elbow is aligned, the overall arm configuration becomes perceptually natural and significantly more similar to that of a human. Furthermore, given a desired EE pose, determining a prior elbow pose that best reflects the natural human form becomes a central aspect of our approach. In summary, the primary contributions of this paper are:\n\n1) Human-like data acquisition framework: We propose an automatic EE–elbow data collection scheme based on large-scale human motion trajectory datasets, which can be readily adapted to different robots.\n\n2) Elbow prediction network: We design a FiLM-modulated Spatio-Temporal Attention Network (FiSTA) that uses only a partial history of EE and elbow frames to predict the desired human-like elbow pose for a given EE target.\n\n3) Comprehensive validation: The effectiveness of our approach is validated both in simulation and on real hardware, as well as across different robotic configurations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在保证末端执行器（EE）跟踪精度的同时，实现类人臂的运动配置？  \n2. 现有的逆向运动学（IK）方法如何导致机械有效但视觉上不自然的解决方案？  \n3. 如何在没有全身传感器的情况下，实现类人运动的机器人臂？  \n\n【用了什么创新方法】  \n本研究提出了一种轻量级的类人逆向运动学（HL-IK）框架，通过学习的肘部先验来改善机械臂的运动配置。使用大规模人类运动数据训练的FiLM调制时空注意力网络（FiSTA）预测肘部姿态，并将其作为小残差与EE和光滑性项结合，应用于标准的Levenberg-Marquardt优化器中。实验结果显示，HL-IK在183k次仿真步骤中，平均减少了30.6%和35.4%的臂部相似性位置和方向误差，并在最具挑战性的轨迹上分别减少了42.2%和47.4%。此外，硬件遥操作验证了其在类人运动方面的提升，表明HL-IK易于集成并适应不同平台。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving",
            "authors": "Jinhao Chai,Anqing Jiang,Hao Jiang,Shiyi Mu,Zichong Gu,Shugong Xu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "IWACIII 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.20253",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20253",
            "arxiv_html_link": "https://arxiv.org/html/2509.20253v1",
            "abstract": "End-to-end multi-modal planning has become a transformative paradigm in autonomous driving, effectively addressing behavioral multi-modality and the generalization challenge in long-tail scenarios. We propose AnchDrive, a framework for end-to-end driving that effectively bootstraps a diffusion policy to mitigate the high computational cost of traditional generative models. Rather than denoising from pure noise, AnchDrive initializes its planner with a rich set of hybrid trajectory anchors. These anchors are derived from two complementary sources: a static vocabulary of general driving priors and a set of dynamic, context-aware trajectories. The dynamic trajectories are decoded in real-time by a Transformer that processes dense and sparse perceptual features. The diffusion model then learns to refine these anchors by predicting a distribution of trajectory offsets, enabling fine-grained refinement. This anchor-based bootstrapping design allows for efficient generation of diverse, high-quality trajectories. Experiments on the NAVSIM benchmark confirm that AnchDrive sets a new state-of-the-art and shows strong generalizability.",
            "introduction": "End-to-end autonomous driving algorithms have gained substantial attention in recent years owing to their superior scalability and adaptability over traditional rule-based motion planning approaches. By learning control signals directly from raw sensor data—such as camera images or LiDAR point clouds—these methods bypass the complexity of modular design pipelines, mitigate the accumulation of perception errors, and enhance overall system consistency and robustness. Earlier end-to-end planners, including UniAD[1], VAD[2], and Transfuser[3], relied on ego queries to regress single-modal trajectories, while more recent approaches such as SparseDrive[4] explored sparse perception modules in combination with parallel motion planners. Nevertheless, in complex traffic conditions—such as intersections or high-speed lane changes—potential vehicle behaviors can be highly ambiguous and diverse. Ignoring the inherent uncertainty in driving behavior and the multi-modal decision-making requirements imposed by environmental perception often leads to overconfident or outright failed predictions when relying on a single predicted trajectory.\n\nRecent research has therefore begun to incorporate multi-modal modeling strategies, producing multiple trajectory proposals consistent with current scene constraints to improve decision coverage. Methods such as VADv2[5] and Hydra-MDP[6] achieve this by using predefined discrete trajectory sets. While this increases coverage to some extent, the reliance on fixed trajectory sets inherently discretizes what is fundamentally a continuous control process, thus constraining expressiveness and flexibility.\n\nDiffusion models have emerged as a promising alternative, offering generative and adaptive capabilities well suited for multi-modal trajectory planning. They enable direct sampling from the high-dimensional joint distribution of the ego vehicle and surrounding agents’ trajectories, and have demonstrated strong modeling capacity in high-dimensional continuous control spaces—evidenced by successes in domains such as image synthesis and robotic motion planning. Their ability to naturally model conditional distributions makes it straightforward to integrate key contextual inputs, including trajectory history, map semantics, and ego objectives, thereby improving both consistency and contextual relevance in policy generation. Moreover, their controllable test-time sampling allows for incorporating additional constraints without retraining, unlike many Transformer-based architectures.\n\nDespite improvements such as DDIM[7] for accelerating sampling, conventional diffusion models require numerous iterative denoising steps, resulting in high computational and latency costs at inference. To address this, prior work has shown that initializing the generation process from non-standard noise distributions can shorten the sampling path by leveraging prior information. Building on this idea, DiffusionDrive[8] proposed a truncated diffusion strategy that anchors the process to a fixed set of trajectory anchors, enabling sampling to begin from intermediate states and thus reducing the number of required iterations. However, such fixed anchor sets lack the flexibility to adapt to scenarios demanding dynamically generated anchors.\n\nWe address this limitation with AnchDrive, a novel end-to-end multi-modal autonomous driving framework. AnchDrive employs a multi-head trajectory decoder to dynamically generate a set of dynamic trajectory anchors informed by scene perception, capturing behavioral diversity under local environmental conditions. Simultaneously, we construct a broad-coverage static anchor set from large-scale human driving data, providing cross-domain behavioral priors. These dynamic anchors provide context-aware guidance tailored to the immediate scene, while the static anchor set mitigates overfitting to training distributions and improves generalization to unseen environments. By leveraging this hybrid anchor set, our diffusion-based planner can produce high-quality and diverse predictions within a reduced number of denoising steps.\n\nWe evaluate AnchDrive in closed-loop settings on the Navsim-v2[9] simulation platform, which features reactive background traffic agents and high-fidelity synthetic multi-view imagery. Experiments on a navtest set show that AnchDrive achieves 85.5 EPDMS, indicating robust and contextually appropriate behavior generation in complex driving scenarios.\n\nOur key contributions are as follows:\n\nWe propose AnchDrive, an end-to-end autonomous driving framework that employs a truncated diffusion process initialized from a hybrid set of trajectory anchors. This approach, which integrates both dynamic and static anchors, significantly improves initial trajectory quality and enables robust planning. We validate its effectiveness on the challenging Navsim-v2[9] benchmark.\n\nWe propose AnchDrive, an end-to-end autonomous driving framework that employs a truncated diffusion process initialized from a hybrid set of trajectory anchors. This approach, which integrates both dynamic and static anchors, significantly improves initial trajectory quality and enables robust planning. We validate its effectiveness on the challenging Navsim-v2[9] benchmark.\n\nWe design a hybrid perception model with dense and sparse branches. The dense branch builds a bird’s-eye-view (BEV) representation for the planner’s primary input, while the sparse branch extracts instance-level cues—such as detected obstacles, lane boundaries, centerlines, and stop lines—to enhance the planner’s understanding of obstacles and road geometry.\n\n1. We propose AnchDrive, an end-to-end autonomous driving framework that employs a truncated diffusion process initialized from a hybrid set of trajectory anchors. This approach, which integrates both dynamic and static anchors, significantly improves initial trajectory quality and enables robust planning. We validate its effectiveness on the challenging Navsim-v2[9] benchmark.\n\n2. We design a hybrid perception model with dense and sparse branches. The dense branch builds a bird’s-eye-view (BEV) representation for the planner’s primary input, while the sparse branch extracts instance-level cues—such as detected obstacles, lane boundaries, centerlines, and stop lines—to enhance the planner’s understanding of obstacles and road geometry.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂交通条件下生成多模态的驾驶轨迹以应对行为的不确定性。  \n2. 如何降低传统生成模型在推理时的计算成本和延迟。  \n3. 如何结合动态和静态轨迹锚点以提高轨迹生成的质量和多样性。  \n\n【用了什么创新方法】  \nAnchDrive框架通过引入混合轨迹锚点，采用截断扩散过程来初始化生成模型。动态轨迹锚点由Transformer实时解码，结合静态锚点提供广泛的行为先验。该方法显著提高了初始轨迹的质量，并在Navsim-v2基准测试中表现出强大的泛化能力，达到了85.5 EPDMS，展示了在复杂驾驶场景中的稳健和上下文适应行为生成能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Techno-Economic analysis for Smart Hangar inspection operations through Sensing and Localisation at scale",
            "authors": "Angelos Plastropoulos,Nicolas P. Avdelidis,Argyrios Zolotas",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20229",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20229",
            "arxiv_html_link": "https://arxiv.org/html/2509.20229v1",
            "abstract": "The accuracy, resilience, and affordability of localisation are fundamental to autonomous robotic inspection within aircraft maintenance and overhaul (MRO) hangars. Hangars typically feature tall ceilings and are often made of materials such as metal. Due to its nature, it is considered a GPS-denied environment, with extensive multipath effects and stringent operational constraints that collectively create a uniquely challenging environment. This persistent gap highlights the need for domain-specific comparative studies, including rigorous cost, accuracy, and integration assessments, to inform a reliable and scalable deployment of a localisation system in the Smart Hangar. This paper presents the first techno-economic roadmap that benchmarks motion capture (MoCap), ultra-wideband (UWB), and a ceiling-mounted camera network across three operational scenarios: robot localisation, asset tracking, and surface defect detection within a 40 × 50 m hangar bay. A dual-layer optimisation for camera selection and positioning framework is introduced, which couples market-based camera-lens selection with an optimisation solver, producing camera layouts that minimise hardware while meeting accuracy targets. The roadmap equips MRO planners with an actionable method to balance accuracy, coverage, and budget, demonstrating that an optimised vision architecture has the potential to unlock robust and cost-effective sensing for next-generation Smart Hangars.",
            "introduction": "Accurate localisation is fundamental to enabling autonomous robotic inspection in MRO hangars, where metallic structures, extensive multipath effects, and strict operational constraints define a unique and challenging environment [1]. Existing localisation technologies, including infrared MoCap, UWB real-time location systems, and camera-based or Simultaneous Localisation and Mapping (SLAM) approaches, offer different trade-offs in terms of achievable accuracy, infrastructure complexity, cost, and robustness to occlusion and interference [2]. Sensor fusion frameworks that combine vision, inertial, and UWB data can improve robustness and deliver cost–accuracy trade-offs, as demonstrated in large-scale warehouse deployments. However, their performance and economic viability remain highly dependent on environment-specific factors [3]. In addition, this study also treats localisation as the backbone for real-time asset monitoring, allowing ground support equipment, tooling, and spares to be tracked seamlessly in the bay. On the opposite scale, artefact localisation is addressed, with an exploration of how sensing can facilitate the identification of surface defects and other critical features on the airframe. The framing of these macroscopic and microscopic needs together sets the stage for the optimisation framework, comparative experiments, and cost analyses developed in the remainder of the paper.\n\nDespite technical advances, a notable lack of comprehensive, real-world techno-economic analyses remains, specifically focused on aircraft hangar deployments. The available literature provides only partial benchmarking or component-level comparisons for individual or hybrid localisation modalities [4, 5], with little empirical evidence on their robustness to the full spectrum of hangar-specific challenges such as dynamic occlusion, specular reflections, and integration with existing maintenance workflows. To date, no studies have presented a holistic side-by-side evaluation of MoCap, UWB, and vision-based solutions in an operational metallic aircraft hangar context. This persistent gap underscores an urgent need for domain-specific comparative studies, including rigorous assessments of cost, accuracy, and integration, to inform the reliable and scalable deployment of robotic inspection systems in aviation environments. The economic impact of daily maintenance practices has begun to be quantified by recent hangar-focused studies; for instance, significant rework costs in narrow-body bays can be avoided by improving technicians’ awareness of composite repair, as demonstrated by Jong et al. [6]. Across the timeline, Moenck et al. [7] outline how the forthcoming Industry 5.0 automation could reshape the trade-offs of labour hours and logistics on large MRO campuses. At the same time, the classic aerodynamic analysis of the enclosed engine test hangars by Wallis and Ruglen still provides a valuable historical baseline for energy throughput economics [8].\n\nIn summary, this study offers five significant contributions: (i) it presents the inaugural techno-economic roadmap that evaluates MoCap, UWB, and ceiling-camera vision in parallel for full-scale aircraft hangars; (ii) it introduces a dual-layer optimisation framework that combines market-driven camera-lens selection with a Mixed-Integer Linear Programming-based set-cover placement, resulting in the minimal number of cameras needed; (iii) it supplies quantified design-to-cost case studies converting three typical MRO tasks into specific bills of materials and cost estimates; (iv) determines the optimal balance for defect-detection accuracy, illustrating how ceiling cameras and drone close-ups converge at various defect sizes; and (v) provides the first cost/accuracy comparison between camera localisation and commercial UWB/MoCap systems for a conventional 40 × 50 m bay. Collectively, these contributions deliver an actionable and comprehensive methodology for MRO decision-makers to select, size, and cost localisation and inspection systems within large hangars.\n\nInspired by the aviation industry’s shift towards Industry 5.0, which sees mobile robots and AI-based decision support systems taking on routine maintenance duties, the hangar should transition from a passive shelter to a dynamic sensing platform. The end goal is a ceiling infrastructure dense enough to localise robots, track assets, and even surface defects in real-time but lean enough to be economically retrofitted into legacy bays. Against this backdrop, the remainder of the paper is organised as follows. Section 2 reviews the state-of-the-art in MoCap, UWB and ceiling-camera vision, clarifying their respective accuracy, cost, and integration trade-offs. Section 3 introduces a dual-layer optimisation framework that first selects a market-ready camera–lens pair and then solves a set-cover problem to minimise hardware while meeting resolution targets. Section 4 translates those algorithms into three design-to-cost frameworks: robot localisation, asset tracking, and defect detection, each with a detailed bill of materials for practitioner guidance. It also presents MoCap and UWB implementation options for benchmarking.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在GPS-denied环境中实现准确、可靠的机器人定位和检测？  \n2. 如何在航空维护和检修（MRO）环境中平衡成本、准确性和集成性？  \n3. 当前的定位技术在复杂环境中的经济可行性和性能如何？  \n\n【用了什么创新方法】  \n本文提出了首个针对飞机机库的技术经济路线图，比较了运动捕捉（MoCap）、超宽带（UWB）和天花板摄像头网络在机器人定位、资产跟踪和表面缺陷检测中的应用。引入了一个双层优化框架，结合市场驱动的摄像头镜头选择与混合整数线性规划的设置覆盖问题，最小化所需硬件数量，同时满足准确性目标。研究表明，优化的视觉架构能够实现强大且具有成本效益的传感能力，为下一代智能机库的部署提供了可行的方法。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "A Biomimetic Vertebraic Soft Robotic Tail for High-Speed, High-Force Dynamic Maneuvering",
            "authors": "Sicong Liu,Jianhui Liu,Fang Chen,Wenjian Yang,Juan Yi,Yu Zheng,Zheng Wang,Wanchao Chi,Chaoyang Song",
            "subjects": "Robotics (cs.RO)",
            "comment": ". Submitted Under Review",
            "pdf_link": "https://arxiv.org/pdf/2509.20219",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20219",
            "arxiv_html_link": "https://arxiv.org/html/2509.20219v1",
            "abstract": "Robotic tails can enhance the stability and maneuverability of mobile robots, but current designs face a trade-off between the power of rigid systems and the safety of soft ones. Rigid tails generate large inertial effects but pose risks in unstructured environments, while soft tails lack sufficient speed and force. We present a Biomimetic Vertebraic Soft Robotic (BVSR) tail that resolves this challenge through a compliant pneumatic body reinforced by a passively jointed vertebral column inspired by musculoskeletal structures. This hybrid design decouples load-bearing and actuation, enabling high-pressure actuation (up to 6 bar) for superior dynamics while preserving compliance. A dedicated kinematic and dynamic model incorporating vertebral constraints is developed and validated experimentally. The BVSR tail achieves angular velocities above 670°/s and generates inertial forces and torques up to 5.58 N and 1.21 Nm, indicating over 200% improvement compared to non-vertebraic designs. Demonstrations on rapid cart stabilization, obstacle negotiation, high-speed steering, and quadruped integration confirm its versatility and practical utility for agile robotic platforms.",
            "introduction": "The tail is a masterful evolutionary solution for dynamic locomotion, enabling animals to achieve remarkable feats of stability and agility [1]. The functional utility of this appendage is rooted in the principles of classical mechanics, particularly the conservation of angular momentum. This is often accomplished through a process known as inertial adjustment, where rapid, controlled movements of the tail generate reaction forces and torques that are imparted onto the main body to regulate its orientation and momentum in real time. For instance, cheetahs, during their high-speed bounding gait, use their long, muscular tails as aerodynamic rudders and inertial counterweights to modulate yaw and roll, allowing for exceptionally sharp turns and stable braking [2, 3]. In the aerial realm, animals like geckos and lizards execute rapid mid-air self-righting maneuvers by swinging their tails, inducing a counter-rotation in their bodies to ensure a safe landing orientation [4, 5]. Even during terrestrial or arboreal locomotion, animals from kangaroos to squirrels leverage their tails for a spectrum of dynamic tasks, from providing a “fifth leg” for postural stability to recovering from unexpected falls and slips [6, 7]. These biological archetypes, which demonstrate a sophisticated functional integration of sensing, neural control, and musculoskeletal actuation, provide a rich foundation for designing robotic tails that augment the dynamic performance of mobile robots [8, 9].\n\nInspired by these natural mechanisms, roboticists have long sought to replicate their function to enhance the agility and robustness of mobile systems. Early pioneering work included the Uniroo, a monopedal hopping robot that employed a simple tail for pitching stabilization [10]. Subsequent research in this domain has primarily focused on rigid, articulated tails, which typically manifest as pendulum-like mechanisms. These designs range from single-degree-of-freedom (DOF) systems for planar regulation in the pitch, yaw, or roll axes [2, 11, 12, 13, 14, 3, 15, 16, 17], to 2-DOF mechanisms that provide more generalized spatial control [18, 19]. The high stiffness and well-defined kinematics of these rigid systems allow them to generate significant and predictable inertial effects at high speeds. However, their fundamental design presents a critical set of trade-offs. The primary limitation is their lack of compliance, which restricts their utility in unstructured environments and poses significant safety risks for any physical human-robot interaction [20]. Furthermore, the high reflected inertia of a rigid tail necessitates large, powerful actuators, which adds considerable mass and increases power consumption. This demand for high-bandwidth torque control to manage large inertial loads and mitigate potentially destabilizing impact forces adds significant complexity to the control system.\n\nTo address the inherent safety and compliance limitations of rigid systems, the field of soft robotics presents a compelling alternative paradigm. The intrinsic compliance of soft robots, derived from their deformable materials and structures, offers inherent safety, adaptability to uncertain environments, and robustness to physical impacts [21, 22, 23, 24, 25]. This has motivated the development of a new class of soft robotic tails, including hyper-redundant continuum structures for inertial adjustment [26], aquatic robots with flexible tails that emulate fish locomotion for propulsion and maneuvering [27], and even novel wearable tails for human balance assistance [28]. However, while these pioneering systems validate the potential of soft appendages, they also reveal a persistent performance gap. This deficit is rooted in the fundamental properties of the soft materials themselves, which typically exhibit low stiffness and significant viscoelasticity, leading to challenges in generating sufficient force at high frequencies. The low actuation authority of most soft systems, combined with the immense difficulty of accurately modeling and controlling their near-infinite degrees of freedom, limits their ability to produce the rapid, high-magnitude accelerations necessary for effective dynamic regulation of a large robotic platform. Consequently, the field is faced with a critical unmet need: a robotic appendage that unites the raw inertial authority of rigid systems with the inherent safety and adaptability of soft structures. The absence of such a system currently precludes the deployment of agile, dynamic robots in unstructured, human-centric environments.\n\nThis paper introduces a Biomimetic Vertebraic Soft Robotic (BVSR) tail, as shown in Fig. 1, specifically designed to address the trade-offs above by establishing a novel hybrid design approach. Our method involves the functional integration of a compliant pneumatic body with an internal, passively jointed vertebral column. This central element performs a crucial dual role: it acts as a structural backbone that bears the tensile loads from high-pressure actuation, allowing the system to generate large forces without material failure, and it serves as a kinematic constraint that reduces the complex, high-dimensional deformation of the soft body into a predictable, low-dimensional bending motion. This architectural choice makes the system’s modeling and control more tractable while enabling a level of dynamic performance previously unattainable in soft robotic appendages. The primary contributions of this work are threefold:\n\nThe formulation and physical realization of a vertebraic soft robotic design principle, wherein a passive kinematic constraint enables high-pressure (6 Bar) actuation in an otherwise compliant structure to achieve superior angular velocity and inertial output.\n\nA comprehensive Euler-Lagrange dynamic model that, among the first in this class of robots, explicitly incorporates the kinematic constraints imposed by an internal vertebral structure, demonstrating high fidelity between theoretical predictions and experimental results.\n\nRigorous experimental validation of the tail’s performance envelope and a demonstration of its functional efficacy in dynamic tasks, including inertial assistance for a wheeled mobile robot and successful integration with a quadrupedal platform, validating its versatility.\n\nThe remainder of this paper is organized as follows. Section 2 describes the concept and modeling of the BVSR tail. Section 3 presents its physical implementation and characterization. Section 4 details the experimental validation and functional demonstrations. Section 5 discusses the implications of our findings, limitations, and comparisons to the state of the art. Finally, Section 6 concludes the paper and discusses future work.\n\n1. The formulation and physical realization of a vertebraic soft robotic design principle, wherein a passive kinematic constraint enables high-pressure (6 Bar) actuation in an otherwise compliant structure to achieve superior angular velocity and inertial output.\n\n2. A comprehensive Euler-Lagrange dynamic model that, among the first in this class of robots, explicitly incorporates the kinematic constraints imposed by an internal vertebral structure, demonstrating high fidelity between theoretical predictions and experimental results.\n\n3. Rigorous experimental validation of the tail’s performance envelope and a demonstration of its functional efficacy in dynamic tasks, including inertial assistance for a wheeled mobile robot and successful integration with a quadrupedal platform, validating its versatility.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何设计一种既能提供高速度和高力量又具备安全性的机器人尾部？  \n2. 现有的刚性和柔性尾部设计之间的性能差距如何弥补？  \n3. 如何有效建模和控制具有高维自由度的软体机器人？  \n\n【用了什么创新方法】  \n本研究提出了一种生物仿生的脊椎软体机器人尾部（BVSR），结合了顺应性气动体和被动关节脊柱，解决了刚性和柔性系统之间的权衡。该设计通过高压驱动（最高6 Bar）实现了超过670°/s的角速度和5.58 N的惯性力，显示出200%以上的性能提升。通过建立和验证包含脊柱约束的动态模型，实验结果表明该尾部在动态任务中的有效性，成功集成于四足机器人，展示了其多功能性和实用性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving",
            "authors": "Pengxiang Li,Yinan Zheng,Yue Wang,Huimin Wang,Hang Zhao,Jingjing Liu,Xianyuan Zhan,Kun Zhan,Xianpeng Lang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20109",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20109",
            "arxiv_html_link": "https://arxiv.org/html/2509.20109v1",
            "abstract": "End-to-End (E2E) solutions have emerged as a mainstream approach for autonomous driving systems, with Vision-Language-Action (VLA) models representing a new paradigm that leverages pre-trained multimodal knowledge from Vision-Language Models (VLMs) to interpret and interact with complex real-world environments. However, these methods remain constrained by the limitations of imitation learning, which struggles to inherently encode physical rules during training. Existing approaches often rely on complex rule-based post-refinement, employ reinforcement learning that remains largely limited to simulation, or utilize diffusion guidance that requires computationally expensive gradient calculations. To address these challenges, we introduce ReflectDrive, a novel learning-based framework that integrates a reflection mechanism for safe trajectory generation via discrete diffusion. We first discretize the two-dimensional driving space to construct an action codebook, enabling the use of pre-trained Diffusion Language Models for planning tasks through fine-tuning. Central to our approach is a safety-aware reflection mechanism that performs iterative self-correction without gradient computation. Our method begins with goal-conditioned trajectory generation to model multi-modal driving behaviors. Based on this, we apply local search methods to identify unsafe tokens and determine feasible solutions, which then serve as safe anchors for inpainting-based regeneration. Evaluated on the NAVSIM benchmark, ReflectDrive demonstrates significant advantages in safety-critical trajectory generation, offering a scalable and reliable solution for autonomous driving systems.",
            "introduction": "Autonomous driving (AD) is guiding the transportation industry toward a safer and more efficient future (Tampuu et al., 2020). Within this trend, End-to-End (E2E) systems (Hu et al., 2023; Chen et al., 2023) have emerged as the mainstream alternative to traditional modular designs (Bansal et al., 2018), which are prone to error accumulation between interdependent modules. They have also largely replaced rule-based methods (Fan et al., 2018; Treiber et al., 2000) that demand extensive human engineering effort. Meanwhile, Vision-Language-Action (VLA) models (Kim et al., 2024; Hwang et al., 2024) offer a new solution by incorporating pre-trained knowledge from Vision-Language Models (VLMs) (Hurst et al., 2024; Bai et al., 2025). Equipped with enhanced generalization capabilities, VLA models can interpret visual scenes and understand human instructions to directly output planning trajectories, thereby improving adaptability in challenging situations.\n\nHowever, eixsting learning-based methods does not resolve the core challenge in imitation learning-based driving systems. Specifically, behavior cloning fails to inherently encode inviolable physical rules, such as collision avoidance or adherence to drivable areas (Lu et al., 2023). As a result, a generated trajectory may be highly probable under the model’s distribution yet still violate critical safety constraints. Consequently, existing deployed solutions often rely on significant human priors, such as trajectory anchors (Li et al., 2024) or rule-based generated paths (Dauner et al., 2023). These priors offer a reliable initial solution for the learning system, but they also necessitate substantial post-processing, particularly in complex scenarios. Concurrently, more advanced solutions are emerging. Some methods integrate reinforcement learning (Kaelbling et al., 1996; Kendall et al., 2019; Jaeger et al., 2025; Cusumano-Towner et al., 2025) with human-designed reward functions to enhance causal reasoning. However, most existing studies remain confined to the simulation level. From a deployment perspective, these approaches typically require unsafe online rollouts and suffer from training instability, especially in large-scale models (Zheng et al., 2024). Although guidance mechanisms in diffusion models provide a promising alternative by enabling controllable generation during inference (Zheng et al., 2025; Jiang et al., 2023; Zhong et al., 2023), they often experience slow sampling speeds due to gradient computations and are highly sensitive to parameter tuning, which can lead to numerical instability.\n\nTo address these challenges, we pioneer the use of discrete diffusion (Austin et al., 2021) for planning to meet the demand for verifiable and controllable E2E driving systems. A key advantage of this approach is its operation in a discrete action space, which facilitates the seamless incorporation of critical safety constraints through search, masking, and sampling techniques during trajectory generation. This results in a hybrid framework in which learned behaviors can be rigorously guided by prior knowledge, shifting away from black-box planning toward trustworthy and interpretable decision-making. Inspired by these insights, we propose ReflectDrive, a novel learning-based framework that integrates a reflection mechanism for safe trajectory generation via discrete diffusion. Specifically, we first discretize the two-dimensional driving space to construct a action codebook, enabling the representation of vehicle trajectories through discrete codebook embeddings. This representation allows us to leverage a pre-trained Diffusion Language Models (DLMs) (You et al., 2025; Nie et al., 2025) for planning tasks via fine-tuning. The approach facilitates parallel decoding and bidirectional feature fusion within a unified architecture that supports scalable training. Based on this fine-tuned model, our reflection mechanism begins with goal-conditioned generation, where the goal point guides the generation process to capture diverse multi-modal driving behaviors. Furthermore, the framework integrates safety metrics to evaluate the generated multi-modal trajectories. For unsafe waypoints, we perform a local search to identify a feasible solution, which then serves as a safe anchor token for trajectory inpainting. The entire process operates without gradient computation, enabling parallel generation and the injection of safety constraints during trajectory regeneration. Evaluations on the real-world autonomous driving benchmark NAVSIM (Dauner et al., 2024) demonstrate the feasibility of employing discrete diffusion for trajectory generation. Equipped with our reflection mechanism, ReflectDrive achieves near human-level closed-loop performance. Our contributions are summarized as follows:\n\nWe pioneer the application of discrete diffusion for E2E autonomous driving trajectory generation and integrate it into a VLA model for scalable training.\n\nWe introduce reflection mechanism, a novel inference-time guidance framework specifically designed for the denoising process in discrete diffusion, integrating external safety validation with efficient discrete token optimization.\n\nWe evaluate our method on real-world driving benchmarks, proving that the framework can enforce hard safety constraints without compromising behavioral coherence.\n\n1. We pioneer the application of discrete diffusion for E2E autonomous driving trajectory generation and integrate it into a VLA model for scalable training.\n\n2. We introduce reflection mechanism, a novel inference-time guidance framework specifically designed for the denoising process in discrete diffusion, integrating external safety validation with efficient discrete token optimization.\n\n3. We evaluate our method on real-world driving benchmarks, proving that the framework can enforce hard safety constraints without compromising behavioral coherence.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在自主驾驶中有效地生成安全的轨迹以满足物理规则？  \n2. 现有的模仿学习方法如何克服对复杂场景的依赖和不稳定性？  \n3. 如何在不依赖梯度计算的情况下实现高效的轨迹生成？  \n\n【用了什么创新方法】  \n本研究提出了ReflectDrive，一个基于离散扩散的学习框架，集成了反射机制以实现安全的轨迹生成。首先，我们将二维驾驶空间离散化，构建动作代码本，并通过微调利用预训练的扩散语言模型进行规划任务。核心是一个安全意识的反射机制，能够在不进行梯度计算的情况下进行迭代自我修正。通过目标条件的轨迹生成，我们能够捕捉多模态驾驶行为，并通过局部搜索识别不安全的轨迹点，确保生成的轨迹符合安全约束。评估结果表明，ReflectDrive在NAVSIM基准上表现出显著的安全性优势，提供了一种可扩展且可靠的自主驾驶解决方案。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Hybrid Safety Verification of Multi-Agent Systems using $ψ$-Weighted CBFs and PAC Guarantees",
            "authors": "Venkat Margapuri,Garik Kazanjian,Naren Kosaraju",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20093",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20093",
            "arxiv_html_link": "https://arxiv.org/html/2509.20093v1",
            "abstract": "This study proposes a hybrid safety verification framework for closed-loop multi-agent systems under bounded stochastic disturbances. The proposed approach augments control barrier functions with a novel ψ\\psi-weighted formulation that encodes directional control alignment between agents into the safety constraints. Deterministic admissibility is combined with empirical validation via Monte Carlo rollouts, and a PAC-style guarantee is derived based on margin-aware safety violations to provide a probabilistic safety certificate. The results from the experiments conducted under different bounded stochastic disturbances validate the feasibility of the proposed approach.",
            "introduction": "Safety within multi-agent systems is essential for real-world applications such as autonomous driving [1, 2] and robotic swarm deployments in agriculture [3, 4], manufacturing [5, 6], and search and rescue operations [7], where agents must navigate safely through their environment. Safety in a stochastic multi-agent dynamical system requires that all agent trajectories remain within a predefined safe set under specified control inputs and time horizons.\nTraditional approaches include reachability-based formulations [8, 9], where a Hamilton-Jacobi partial differential equation is solved to characterize the backward-reachable set that avoids unsafe regions. However, these methods are computationally expensive and scale poorly to high-dimensional or multi-agent systems. More recently, barrier certificates [10, 11] and control barrier functions (CBFs) [12, 13] have emerged as tractable alternatives for certifying safety. By enforcing forward invariance of a safe set via control-affine constraints, CBFs offer real-time safety guarantees under deterministic assumptions. Yet, such guarantees may fail in the presence of noise or unmodeled disturbances. While stochastic CBF variants address this, they often rely on strong distributional assumptions or chance-constrained formulations.\n\nTo bridge the gap, this work introduces a hybrid safety verification framework that unifies ψ\\psi-weighted CBFs for forward invariance with finite-sample probably approximately correct (PAC)-style guarantees for margin-aware safety under bounded stochastic disturbances, where ψ\\psi is a term inspired by quantum walk dynamics [14] to promote pairwise safety among different agents. Rather than assuming complete knowledge of the noise distributions, the proposed method combines deterministic admissibility with empirical validation via Monte Carlo rollouts under bounded stochasticity, yielding a distribution-free safety certificate with high-probability guarantees. The proposed method is feasible in multi-agent applications where uncertainty is prevalent and exact noise modeling is infeasible.",
            "llm_summary": "【关注的是什么问题】  \n1. 多智能体系统在随机干扰下的安全性验证问题。  \n2. 现有方法在高维或多智能体系统中的计算复杂性和可扩展性不足。  \n3. 如何在不完全知识的情况下提供概率安全证书。  \n\n【用了什么创新方法】  \n本研究提出了一种混合安全验证框架，结合了ψ加权控制障碍函数（CBFs）与基于有限样本的PAC风格保证。该方法通过引入量子行走动态启发的ψ项，促进不同智能体间的方向控制一致性。通过Monte Carlo回放进行经验验证，结合确定性可接受性，最终实现了在有界随机干扰下的分布无关安全证书。实验结果表明，该方法在多智能体应用中具有良好的可行性和高概率保证。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "C-3TO: Continuous 3D Trajectory Optimization on Neural Euclidean Signed Distance Fields",
            "authors": "Guillermo Gil(1),Jose Antonio Cobano(1),Luis Merino(1),Fernando Caballero(1) ((1) Service Robotics Laboratory, Universidad Pablo de Olavide, Seville, Spain)",
            "subjects": "Robotics (cs.RO)",
            "comment": "submitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.20084",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20084",
            "arxiv_html_link": "https://arxiv.org/html/2509.20084v1",
            "abstract": "This paper introduces a novel framework for continuous 3D trajectory optimization in cluttered environments, leveraging online neural Euclidean Signed Distance Fields (ESDFs). Unlike prior approaches that rely on discretized ESDF grids with interpolation, our method directly optimizes smooth trajectories represented by fifth-order polynomials over a continuous neural ESDF, ensuring precise gradient information throughout the entire trajectory. The framework integrates a two-stage nonlinear optimization pipeline that balances efficiency, safety and smoothness. Experimental results demonstrate that C-3TO produces collision-aware and dynamically feasible trajectories. Moreover, its flexibility in defining local window sizes and optimization parameters enables straightforward adaptation to diverse user’s needs without compromising performance. By combining continuous trajectory parameterization with a continuously updated neural ESDF, C-3TO establishes a robust and generalizable foundation for safe and efficient local replanning in aerial robotics. The source code is open source and can be found at: https://anonymous.4open.science/r/icra2026_neural_trajectory_planner_C3TO_anon/",
            "introduction": "Aerial robots have become increasingly popular for a wide range of real-world applications due to their ability to perform hazardous tasks more efficiently and, most importantly, more safely than humans [1][2]. Fast trajectory replanning remains a critical area of research, particularly in dynamic and unstructured environments. Equally important is maintaining a continuously updated representation of the drone’s surroundings, which is essential for generating continuous, safe, and smooth 3D local trajectories in real time. This paper presents a framework for planning a continuous local trajectory on an online, neurally-generated, distance field.\n\nChosing an adequate map representation is key. Having an efficient calculation of the free space and the direction to the closest obstacle are some of those desirable features for such representation. Euclidean Signed Distance Fields (ESDFs) have become increasingly popular as a method for representing and modeling robot surroundings for planning purposes [3, 4] and present many very useful properties that are desirable for planning: it is continuous, differentiable everywhere except at the cut-locus, its gradient is Lipschitz-continuous everywhere except at the cut-locus, and the norm of its spatial gradient is one [5]. Representations of ESDFs are typically based on discrete voxel grids or neural networks. Discrete methods for online ESDF representation, such as Voxblox [6], FIESTA [3], and Voxfield [7], have gained popularity, but require interpolation to produce continuous ESDF values. In contrast, neural networks can represent ESDFs in a continuous manner [8]. HIO-SDF [9] introduced an incremental, online, and global ESDF model represented by a Sinusoidal Representation Neural Network (SIREN) [10]. Compared to iSDF [8], HIO-SDF can capture finer details, producing smoother surfaces and incorporating more geometric information throughout the environment.\n\nRegarding path and trajectory planning for drones, the traditional approaches are sampling-based or searching-based planners. They can generate optimal paths, but overlook path safety, which makes them undesirable for real-world operations in occluded spaces without post-processing. There are several state-of-the-art entries showing that ESDFs can be very convenient and useful tools for path planning methods [4, 11, 12, 13]. Heuristic search planners, by integrating ESDFs and leveraging their properties, can inherently address the safety problem and have demonstrated the capability to compute feasible, safe, and fast paths. However, these discretized paths, defined by a sequence of intermediate waypoints, are neither continuous nor smooth, and do not take into account kinematic or dynamic constraints [14].\nTrajectory planning through non-linear optimization is the next step, as it results in trajectories that can comply to complex restrictions, which can be assessed in the form of cost functions and can be easily customized depending on the needs of the user. Although trajectory replanning has been widely investigated, most methods depend on discretized ESDFs with interpolation, limiting gradient accuracy and trajectory quality.\n\nThis work focuses on performing local continuous 3D trajectory planning using non-linear optimization directly on an online generated neural ESDF. We present a framework that starts by building the drone’s environment representation using 3D LIDAR measurements to train a SIREN-like network, based on the network described in [9]. The framework then leverages the properties of the neural representation to perform trajectory optimization on that online ESDF, taking into account distance to obstacles as an indication of safety in addition to other restrictions.\n\nThe main contribution of the proposed framework is to maintain a level of computational optimization sufficient for it to be suitable for use in trajectory replanning, while highlighting its robustness and flexibility compared to existing approaches. The novelty of the framework lies in the optimization of continuous trajectories over continuous ESDFs that are updated online.\n\nWe have organized this paper into six sections. Section II describes the current state of the art. Section III provides an overview of the framework implemented. Section IV provides a detailed description of the trajectory planner on the neural ESDF. The experimental validation can be found in Section V and conclusions are presented in Section VI.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在动态和复杂环境中进行连续的3D轨迹优化？  \n2. 如何利用神经欧几里得有符号距离场（ESDF）提高轨迹规划的安全性和效率？  \n3. 如何实现对障碍物距离的实时更新以优化轨迹？  \n\n【用了什么创新方法】  \n本研究提出了一种新颖的框架C-3TO，通过在线神经ESDF实现连续的3D轨迹优化。该方法使用第五阶多项式表示平滑轨迹，并通过两阶段非线性优化管道，平衡效率、安全性和光滑性。实验结果表明，C-3TO能够生成碰撞感知和动态可行的轨迹，并且其灵活性使其能够适应不同用户需求而不影响性能。通过将连续轨迹参数化与持续更新的神经ESDF相结合，C-3TO为安全高效的局部重规划奠定了稳健且可推广的基础。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots",
            "authors": "Surov Maksim",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20082",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20082",
            "arxiv_html_link": "https://arxiv.org/html/2509.20082v1",
            "abstract": "This paper presents a control methodology for achieving orbital stabilization\nwith simultaneous time synchronization of periodic trajectories in\nunderactuated robotic systems. The proposed approach extends the classical\ntransverse linearization framework to explicitly incorporate time-desynchronization\ndynamics. To stabilize the resulting extended transverse dynamics,\nwe employ a combination of time-varying LQR and sliding-mode control.\nThe theoretical results are validated experimentally through the implementation\nof both centralized and decentralized control strategies on a group\nof six Butterfly robots.",
            "introduction": "The problem of trajectory tracking for underactuated robots has been\naddressed in a series of publications [1, 2, 3, 4, 5, 6, 7].\nMost of these works focus on designing control algorithms for orbital\nstabilization, where the system state converges to a reference periodic\ntrajectory up to a phase shift. Formulating the control objective\nin this way has enabled the development of algorithms that have demonstrated\neffectiveness in real-world applications [8, 9, 10, 11].\nHowever, in some practical scenarios, orbital asymptotic stability\nalone may be insufficient. For example, in cooperative or synchronized\ntasks involving multiple underactuated robots, it may be necessary\nto ensure asymptotic stability of the full state rather than only\nthe orbit itself, particularly when the robots share the same clock.\n\nA straightforward method for tracking a reference trajectory is based\non linearization of the tracking error dynamics, followed by the design\nof an LQR for the resulting linear time-varying (LTV) system. This\nmethod is described in Chapter 12 of [12] and has been\nshown to achieve asymptotic stability for small tracking errors, as\ndemonstrated in experiments with a triple pendulum on a cart [13].\nCompared to orbital stabilization methods, this approach is sensitive\nto initial time shifts, and the control system may lose stability\nif the robot becomes desynchronized.\n\nAlternative approaches for synchronization of closed orbits in underactuated\nrobots involve modifications of orbital tracking algorithms to ensure\nsynchronization between robots in a group [2, 14, 15, 16, 17].\nFor example, in [2], the authors employ transverse\nlinearization of the dynamics of a group of three robots to design\na centralized control law that achieves synchronization. In [17]\norbital stabilization together with synchronization is attained using\nthe dynamic virtual holonomic constraints approach. In [15],\nthe authors propose an ad-hoc modification of the transverse–linearization\napproach, and demonstrate its effectiveness experimentally on the\nsynchronization of two real robots.\n\nOur method for orbital stabilization with simultaneous time synchronization\nalso represents a modification of orbital stabiliation feedback. It\nbuilds on the transverse linearization framework [18, 2, 1].\nFor a given periodic trajectory, we augment the transverse dynamics\nwith the dynamics of robot desynchronization, defined as the difference\nbetween the physical time and the reference time corresponding to\nthe “closest” point on the trajectory. As we show, the linearization\nof this extended transverse dynamics takes the form of an LTV system,\nwhich can be stabilized using a combination of LQR and sliding-mode\ncontrol, similarly to [7]. The resulting feedback\nlaw naturally decomposes into an orbital stabilization component and\na synchronization component. The synchronization term is bounded,\nwith the desynchronization variable entering through the signum function.\nThis structure allows the control law to preserve the benefits of\norbital stabilization while providing bounded corrective actions,\neven for large desynchronizations.\n\nThe remainder of the paper is organized as follows. Section II\nformulates the problem of periodic trajectory tracking for a class\nof nonlinear control systems. Section III\nbriefly reviews the orbital-stabilization algorithm based on the transverse\nlinearization approach. The main results are presented in Section IV,\nwhere the transverse dynamics are extended with a desynchronization\nvariable and two methods for extended dynamics stabilization are proposed.\nThe first method applies an LQR design to linearization of the extended\ntransverse dynamics, while the second employs a sliding-mode control\nmethodology. Section V reports experimental\nresults obtained on a group of six butterfly robots [26].\nConcluding remarks are provided in Section VI.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在欠驱动机器人中实现轨迹的轨道稳定性与时间同步。  \n2. 现有方法在处理多机器人同步任务时的局限性。  \n3. 需要在轨道稳定性之外确保全状态的渐近稳定性。  \n\n【用了什么创新方法】  \n本研究提出了一种控制方法，通过扩展经典的横向线性化框架，结合时间变化的LQR和滑模控制，实现欠驱动机器人周期轨迹的轨道稳定性与时间同步。通过将机器人去同步化动态纳入考虑，形成了一个线性时变系统，从而能够同时处理轨道稳定性和同步性。实验结果表明，该方法在六个蝴蝶机器人上有效验证了理论成果，展示了良好的稳定性和同步性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping",
            "authors": "Jose E. Maese,Luis Merino,Fernando Caballero",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20081",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20081",
            "arxiv_html_link": "https://arxiv.org/html/2509.20081v1",
            "abstract": "This paper presents a high-efficiency, CPU-only volumetric mapping framework based on a Truncated Signed Distance Field (TSDF). The system incrementally fuses raw LiDAR point-cloud data into a voxel grid using a directional bitmask-based integration scheme, producing dense and consistent TSDF representations suitable for real-time 3D reconstruction. A key feature of the approach is that the processing time per point-cloud remains constant, regardless of the voxel grid resolution, enabling high resolution mapping without sacrificing runtime performance. In contrast to most recent TSDF/ESDF methods that rely on GPU acceleration, our method operates entirely on CPU, achieving competitive results in speed. Experiments on real-world open datasets demonstrate that the generated maps attain accuracy on par with contemporary mapping techniques. The source code is publicly available at https://github.com/robotics-upo/DB-TSDF",
            "introduction": "Volumetric mapping is a fundamental capability in mobile robotics, supporting tasks such as collision avoidance, motion planning, and the construction of consistent world models under real-time constraints. Point clouds and occupancy grids remain widely used on CPU-only platforms, as their simple data structures allow efficient processing without specialized hardware. However, they are prone to aliasing at high resolutions and often produce geometric artifacts that hinder downstream processing. Truncated Signed Distance Fields (TSDFs) address these limitations by storing per-voxel distances to the nearest surface and providing smooth proximity information. Despite their advantages, many existing TSDF and ESDF (Euclidean Signed Distance Fields) pipelines rely heavily on GPU acceleration or exhibit computational costs on the CPU that grow unfavorably with map resolution and update rate.\n\nThis work introduces DB-TSDF, a mapping method that integrates TSDFs using a directional bitmask representation specifically designed for fast operation on a discrete voxel grid using only the CPU. Each voxel encodes a compact 32-bit distance mask, a sign flag, and a hit counter. For each LiDAR return, the system selects a precomputed, direction-dependent kernel applied over a fixed neighborhood. A single bitwise AND operation per voxel updates the mask, while a directional shadow mechanism assigns occupied or free-space evidence. Since the kernel size remains constant, the integration time per scan is bounded and remains largely unaffected by the total grid dimensions. Increasing resolution increases memory usage but does not compromise real-time performance. The implementation is fully parallelized using multi-threading and relies exclusively on integer operations.\n\nDB-TSDF builds upon the Truncated Distance Field mapping backend initially developed for the D-LIO framework [1], which was primarily focused on localization. In this work, the mapping formulation is expanded and refined: the field representation is extended from unsigned to signed distances, directional evidence accumulation is introduced, and the memory layout is optimized for improved cache efficiency. These changes result in a high-resolution mapping method capable of maintaining stable runtimes on CPU-constrained platforms and delivering robust performance even in feature-sparse environments.\n\nThe method is evaluated on public LiDAR datasets, with results reported in terms of geometric accuracy, runtime, and the trade-off between resolution, and update latency. The experiments show that DB-TSDF achieves mapping quality comparable to established volumetric approaches while maintaining competitive performance on CPU at resolutions typically requiring GPU acceleration. An example of such high-resolution reconstruction is shown in Figure 1, generated from the Newer College dataset.\n\nThe main contributions of this work are: (i) a TSDF integration scheme based on directional kernels and bitmask distance encoding, implemented entirely on CPU; (ii) a mapping approach with constant per-scan computational cost that is independent of the global grid size; (iii) a signed and memory-efficient voxel structure designed for high-resolution mapping and ROS2 integration; and (iv) an experimental evaluation that quantifies the method’s accuracy, speed, and memory requirements.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在CPU上实现高效的体积映射，避免GPU加速的依赖。  \n2. 如何在高分辨率下保持实时性能，同时减少计算成本和内存使用。  \n3. 如何利用方向性位掩码提高TSDF的集成效率和准确性。  \n\n【用了什么创新方法】  \n本研究提出了一种基于方向位掩码的TSDF集成方案，专门设计用于在离散体素网格上快速操作。每个体素编码了一个紧凑的32位距离掩码、符号标志和命中计数器。通过预计算的方向依赖内核，系统对每个LiDAR返回进行处理，使用位与操作更新掩码，确保每次扫描的集成时间保持恒定。该方法在公共LiDAR数据集上的实验表明，DB-TSDF在几何准确性和运行速度上与现有的体积映射方法相当，同时在CPU平台上实现了高分辨率映射，展示了其在特征稀疏环境中的稳健性能。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning",
            "authors": "Xun Li,Rodrigo Santa Cruz,Mingze Xi,Hu Zhang,Madhawa Perera,Ziwei Wang,Ahalya Ravendran,Brandon J. Matthews,Feng Xu,Matt Adcock,Dadong Wang,Jiajun Liu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20077",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20077",
            "arxiv_html_link": "https://arxiv.org/html/2509.20077v1",
            "abstract": "To enable robots to comprehend high-level human instructions and perform complex tasks, a key challenge lies in achieving comprehensive scene understanding: interpreting and interacting with the 3D environment in a meaningful way. This requires a smart map that fuses accurate geometric structure with rich, human-understandable semantics.\nTo address this, we introduce the 3D Queryable Scene Representation (3D QSR), a novel framework built on multimedia data that unifies three complementary 3D representations: (1) 3D-consistent novel view rendering and segmentation from panoptic reconstruction, (2) precise geometry from 3D point clouds, and (3) structured, scalable organization via 3D scene graphs. Built on an object-centric design, the framework integrates with large vision-language models to enable semantic queryability by linking multimodal object embeddings, and supporting object-level retrieval of geometric, visual, and semantic information. The retrieved data are then loaded into a robotic task planner for downstream execution.",
            "introduction": "For robots to perform complex tasks in 3D environments under human instruction, they must relate high-level semantics in natural language commands to actual content in their surrounding environment. Even a simple instruction such as “Robot, I’m thirsty.” demands the ability to infer intent, locate relevant items (e.g., a water bottle), assess affordances, and plan a path for retrieval. Although trivial for humans, the tasks are exceptionally challenging for robots, as they must simultaneously reason about the spatial structure and semantic meaning of the environment based on human queries. While it is essential to enhance the robot’s intelligence for navigating and manipulating complex environments, it is equally important to make the environment more understandable. We address this dual necessity by introducing the concept of a queryable 3D scene representation (QSR), which embeds intelligence directly into the scene. This enables both robots and humans to interact with their surroundings in a more collaborative, context-aware, and semantically grounded manner.\n\nTraditional 3D maps for robotic systems, such as voxel-based occupancy grids, point clouds, and mesh models (Fredriksson et al., 2024; Liu, 2015; Chen et al., 2021; Bandyopadhyay et al., 2024), are predominantly geometric and often constructed using SLAM algorithms (Taheri and Xia, 2021; Ramezani et al., 2022). However, these representations lack the semantic information necessary for understanding and interacting with the scene. Semantic understanding, on the other hand, is typically derived from 2D object detection/segmentation models. The central challenge is aligning 2D semantics with 3D geometry to form a unified representation that enables complex reasoning and interaction. Moreover, human queries often span multiple levels of granularity and conceptual domains (e.g., “a pillow with a tree pattern”), requiring far richer semantics than conventional models can provide.\nFinally, human understanding of environments is inherently structural, involving hierarchical organisation and inter-object relationships. Capturing and reflecting the structural organisation in the map is essential for enhancing analytical capabilities and enabling more intuitive interaction with complex environments.\n\nTo address these challenges, we introduce 3D QSR, a scene-understanding multimodal framework built using multimedia data. It combines state-of-the-art 3D reconstruction techniques, such as NeRF (Mildenhall et al., 2021) and point clouds, with advanced semantic understanding through panoptic segmentation and vision-language embeddings. We also incorporate a 3D scene graph as an abstract layer, providing a structured, explicit, and lightweight representation enriched with object properties and inter-object relationships. Unlike single-modality systems, 3D QSR supports object-level queries involving location, appearance, function, and relational context, significantly enhancing scene understanding and interaction.\nWith a Large Language Model (LLM), our framework supports advanced language querying and reasoning grounded in QSR content. We demonstrate the capability of this representation through various downstream robotic task planning scenarios simulated in Unity (Juliani, 2018) using the Replica dataset (Straub et al., 2019). In summary, the 3D QSR framework provides:\n\nUnified alignment of semantic, geometric, and structural information, enabling robots to reason over spatial and semantic context simultaneously.\n\nUnified alignment of semantic, geometric, and structural information, enabling robots to reason over spatial and semantic context simultaneously.\n\nNatural language-driven interaction, supporting intuitive query-answering for object retrieval.\n\nComprehensive support for robotic task planning, such as autonomous navigation, object retrieval, and adaptive decision-making in complex scenarios.\n\n1. Unified alignment of semantic, geometric, and structural information, enabling robots to reason over spatial and semantic context simultaneously.\n\n2. Natural language-driven interaction, supporting intuitive query-answering for object retrieval.\n\n3. Comprehensive support for robotic task planning, such as autonomous navigation, object retrieval, and adaptive decision-making in complex scenarios.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现机器人对3D环境的全面理解，以执行复杂任务。  \n2. 如何将2D语义信息与3D几何信息对齐，形成统一的表示。  \n3. 如何支持自然语言驱动的交互，以增强机器人与环境的互动能力。  \n\n【用了什么创新方法】  \n提出了3D Queryable Scene Representation (3D QSR)框架，该框架结合了3D重建技术、全景分割和视觉-语言嵌入，形成了一个多模态的场景理解系统。通过3D场景图的引入，3D QSR能够支持对象级查询，增强场景理解和交互能力。该框架通过与大型语言模型的结合，实现了基于自然语言的查询和推理，展示了在复杂场景下的机器人任务规划能力。实验结果表明，3D QSR显著提高了机器人在复杂环境中的导航、物体检索和决策能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "LLM Trainer: Automated Robotic Data Generating via Demonstration Augmentation using LLMs",
            "authors": "Abraham George,Amir Barati Farimani",
            "subjects": "Robotics (cs.RO)",
            "comment": ". Submitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.20070",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20070",
            "arxiv_html_link": "https://arxiv.org/html/2509.20070v1",
            "abstract": "We present LLM Trainer, a fully automated pipeline that leverages the world knowledge of Large Language Models (LLMs) to transform a small number of human demonstrations (as few as one) into a large robot dataset for imitation learning. Our approach decomposes demonstration generation into two steps: (1) offline demonstration annotation that extracts keyframes, salient objects, and pose–object relations; and (2) online keypose retargeting that adapts those keyframes to a new scene, given an initial observation. Using these modified keypoints, our system warps the original demonstration to generate a new trajectory, which is then executed, and the resulting demo, if successful, is saved. Because the annotation is reusable across scenes, we use Thompson sampling to optimize the annotation, significantly improving generation success rate. We evaluate our method on a range of tasks, and find that our data annotation method consistently outperforms expert-engineered baselines. We further show an ensemble policy that combines the optimized LLM feed-forward plan with a learned feedback imitation learning controller. Finally, we demonstrate hardware feasibility on a Franka Emika Panda robot. For additional materials and demonstration videos, please see the project website: https://sites.google.com/andrew.cmu.edu/llm-trainer",
            "introduction": "Recent advances in Large Language Models (LLMs) have revolutionized the field of robot learning, with applications ranging from task planning [1], to tool use in long horizon tasks [2], to deformable object manipulation [3].\nAt the core of these works is the LLM’s broad base of world knowledge, gathered from training on internet-scale data, which allows these agents to be extremely generalizable. In this work, we seek to leverage the world knowledge of LLMs to fully automate demonstration generation through human demo augmentation. To do this, we employ a similar pipeline as [4] and [5] for data generation: first, identify key robot poses in a demonstration, then generate a new environment and modify the key poses based on an initial observation, and finally, use the new key poses to warp the demonstration trajectory, resulting in a new trajectory, which is rolled out in the new environment. However, unlike prior works which rely on human annotation and hard-coded methods to identify key poses and modify them in response to the new environments [4, 5, 6], our system seeks to fully automate this process by leveraging large language models (LLMs). An outline of our method can be seen in Fig. 1.\n\nOur method for LLM-based data generation has two main steps: First, the LLM annotates the human demonstration, identifying keyframes (timesteps that are important inflection points for the task), listing relevant objects at each keyframe, and explaining the relationship between the robot and these objects. Second, the LLM uses this annotation, along with an initial observation of a newly initialized scene, to determine how the robot’s pose should be adjusted at each keypoint. Because the first step of this process does not require information from the new scene, we can reuse these annotations, saving compute cost and opening the door for optimization. By employing a multi-armed bandit-based method, we are able to optimize the demo annotation step, improving data generation success rate by 2-3 times.\n\nOnce the data generation process is complete, we can use the generated data to train imitation learning agents. Additionally, thanks to our annotation optimization process, we develop a highly effective LLM-based feed-forward policy during data generation. In addition to serving as a viable agent on its own, we show that this feed-forward policy, when combined with the feedback agent, can form an effective ensembled policy, combining the long-horizon planning and generalizability of LLMs with the feedback control of imitation learning.\n\nThis work has three main contributions:\n\nAn LLM-based data generation method which can autonomously generate data using only a single, unannoted demonstration and a short (one sentence) description of the task.\n\nA multi-armed bandit-based optimization method which significantly improves demo generation success rate, allowing our method to outperform baselines that rely on expert annotations.\n\nAn ensembling strategy to combine a learned IL policy with the optimized LLM-based feedforward controller developed during data collection.\n\n1. An LLM-based data generation method which can autonomously generate data using only a single, unannoted demonstration and a short (one sentence) description of the task.\n\n2. A multi-armed bandit-based optimization method which significantly improves demo generation success rate, allowing our method to outperform baselines that rely on expert annotations.\n\n3. An ensembling strategy to combine a learned IL policy with the optimized LLM-based feedforward controller developed during data collection.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用LLMs自动生成机器人模仿学习所需的数据？  \n2. 如何优化演示生成过程以提高成功率？  \n3. 如何将LLM生成的策略与反馈控制策略结合以提升性能？  \n\n【用了什么创新方法】  \n本研究提出了LLM Trainer，一个自动化的数据生成管道，利用大型语言模型（LLMs）将少量人类演示转化为大量机器人数据。该方法分为两个步骤：首先，通过LLM进行离线演示注释，提取关键帧、显著对象和姿态-对象关系；其次，进行在线关键姿态重定向，根据初始观察调整关键帧。通过使用Thompson采样优化注释过程，成功率显著提高。实验结果表明，该方法在多项任务中优于专家设计的基线，并展示了将优化的LLM前馈计划与学习的反馈模仿学习控制器结合的有效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping",
            "authors": "Yinzhao Dong,Ji Ma,Liu Zhao,Wanyue Li,Peng Lu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20036",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20036",
            "arxiv_html_link": "https://arxiv.org/html/2509.20036v1",
            "abstract": "Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have demonstrated impressive performance on challenging terrains, allowing robots to execute complex skills such as climbing, running, and jumping. However, existing blind locomotion controllers often struggle to ensure safety\nand efficient traversal through risky gap terrains, which are typically highly complex, requiring robots to perceive terrain information and select appropriate footholds during locomotion accurately. Meanwhile, existing perception-based controllers still present several practical limitations, including a complex multi-sensor deployment system and expensive computing resource requirements. This paper proposes a DRL controller named MAstering Risky Gap Terrains (MARG), which integrates terrain maps and proprioception to dynamically adjust the action and enhance the robot’s stability in these tasks. During the training phase, our controller accelerates policy optimization by selectively incorporating privileged information (e.g., center of mass, friction coefficients) that are available in simulation but unmeasurable directly in real-world deployments due to sensor limitations. We also designed three foot-related rewards to encourage the robot to explore safe footholds. More importantly, a terrain map generation (TMG) model is proposed to reduce the drift existing in mapping and provide accurate terrain maps using only one LiDAR, providing a foundation for zero-shot transfer of the learned policy. The experimental results indicate that MARG maintains stability in various risky terrain tasks.",
            "introduction": "Legged robots have significantly advanced locomotion capabilities, demonstrating impressive skills across various movement modes, such as climbing stairs [1, 2], descending ramps [3], high-speed running [4], parkour [5], bipedal locomotion [6], and backflipping [7]. These abilities enable robots to perform well in continuous and highly challenging terrains, including rugged mountain paths, narrow passages, stairwells, slippery or unstable surfaces, etc. However, existing blind locomotion controllers often struggle to overcome risky gap terrains due to shortcomings in ensuring the safety and balance of quadruped robots.\n\nRisky gap terrains exhibit numerous complex characteristics, imposing nearly stringent demands on robots regarding footholds and balance capabilities during locomotion. As shown in Fig. 1 (a), robots must not only strive to maintain the stability of their center of gravity on a narrow single-plank bridge but also respond in real time to potential lateral disturbances. Once a robot makes errors during locomotion, such as slipping or shifting its center of gravity, it may quickly step on the air or lose stability, leading to a fall and potentially causing severe damage to the robot. When traversing balance beams, the quadruped robot must not only accurately perceive terrain information such as height variations, gap width, and edges, but also select appropriate landing footholds and timing for exertion based on its locomotion capabilities and current state to avoid missteps, as shown in Figs. 1 (b-c).\n\nThe majority of existing quadrupedal locomotion controllers are blind, which means that they do not utilize perception sensors like cameras and LiDARs [8, 9, 10, 11, 12]. It is nearly impossible for these controllers to traverse risky terrains as shown in Fig. 1. Recently, perception sensors have been used to obtain an elevation map of the environment [13, 1]. However, they do not take risky terrains into consideration. Only a few studies consider risky terrains, and they either rely on multiple sensors [14], which significantly increases the complexity of hardware deployment, or use motion capture systems to obtain prior information about the terrain [15]. In this paper, we only use one sensor to construct a robot-centered map and do not rely on motion capture systems.\n\nExisting model-based controllers rely on precise modeling of robots to calculate the optimal joint torques or footholds required for locomotion. For example, Singh et al. [16] compute second-order derivatives of rigid-body inverse and forward dynamics, achieving significant speed-ups over automatic differentiation in optimization-driven robot control. The CAFE-MPC framework [17] employs a cascaded-fidelity model predictive control scheme paired with a tuning-free whole-body controller, enabling quadruped robots to execute agile maneuvers without manual parameter tuning. Meduri et al. [18] splits the nonlinear MPC problem into biconvex centroidal dynamics and full-body kinematics, enabling real-time generation of dynamic whole-body motions for legged robots. These models can generate accurate control commands, enabling the robot to achieve stable and efficient locomotion in an ideal simulation and simple terrains [19]. However, uncertainty factors in real-world environments, such as terrain irregularity, changing friction, and external disturbances, present significant challenges to model-based methods. These factors are difficult to accurately incorporate into models, leading to potential mismatches between the model and the real world. Even slight discrepancies can cause robot locomotion failures, especially in risky gap terrains.\n\nTo address these challenges, researchers [20, 21] have attempted to simplify the dynamics model by utilizing Nonlinear Model Predictive Control (NMPC) to enhance the locomotion of robots in complex and dynamic environments. Yin et al. [22] propose an optimization algorithm to improve the robot’s locomotion performance by transforming the discrete terrain height map into a continuous cost map to adjust the footholds dynamically. [23] proposes a novel control system that integrates adaptive control into a force-based control system for legged robots, enabling them to dynamically locomotion on uneven terrains. However, the computational complexity and slow convergence rates limit the applicability of robots in dynamic environments.\n\nIn addition, studies [24, 25] are also exploring the use of multiple sensors to enhance the accuracy and reliability of the model. Alongside the robot’s inertial measurement unit (IMU), external perception devices such as depth cameras and LiDARs are employed to gather environmental information, including terrain width, height, and edge shape, and integrate this information into the dynamic model to assist robot control [26]. The synchronization of sensor data, the design of fusion algorithms for different sensor inputs, and the computational burden of data processing will further adversely affect the real-time control performance of robots.\n\nModel-free methods, such as deep reinforcement learning (DRL), have shown promise in enabling legged robots to adapt to complex terrains without relying on precise dynamic models. These methods focus on training robots to learn optimal policies through trial and error, allowing them to manage uncertainties and dynamic changes in their environment effectively [27]. The blind locomotion controllers [2, 10] have shown impressive progress in enabling robots to traverse challenging continuous terrains. However, these controllers often struggle in risky terrains due to the absence of environmental perception.\n\nIntegrating data from other external sensors, such as depth cameras, motion capture, etc, into the DRL framework is an effective way to help robots comprehensively understand their surrounding environment. Pioneering works [28, 5, 29] utilize deep learning models, such as GRU [30] and LSTM [31], to process depth images and extract terrain features, including height, slope, and distribution. Robots can successfully perform high-difficulty parkour tasks by incorporating these terrain factors into their decision-making processes. Challenges such as lighting changes, occlusion issues, high dimensionality, and complexity of images [32] may lead to inaccurate terrain feature extraction or high computational complexity during the training process, ultimately affecting the real-time performance of the robot [33]. Meanwhile, [34] and [15] use motion capture and an offline map to derive the height map around the robot’s feet, which limits the practical applicability of this algorithm.\n\nTo obtain more accurate terrain information in the real world, previous DRL controllers [13, 35, 14] utilize multiple depth cameras or LiDARs simultaneously for elevation mapping, which can significantly enhance the accuracy of terrain representation. However, this approach increases the complexity of hardware deployment, as it requires sophisticated processing capabilities to handle the data from multiple sensors. Additionally, existing localization technologies  [36, 37, 38, 39] heavily rely on the pose estimation of floating bases within the global frame. Any inaccuracy in this estimation may lead to map drift, thereby affecting the movement of legged robots in risky terrains. Thus, designing safe and reliable controllers, developing efficient algorithms to simplify deployment processes, and obtaining precise terrain maps remain challenging in risky gaps tasks.\n\nIn summary, we propose a DRL controller for quadrupedal locomotion—MAstering Risky Gap (MARG)—which integrates terrain maps, privileged information, and proprioceptive into the policy to enhance the locomotion performance of quadrupedal robots in risky terrains. The key contributions of this work can be listed as follows:\n\nWe propose a safe and robust robot controller for locomotion, which can predict the body velocity and the contact state of feet on each step, significantly enhancing the robot’s stability in risky gap terrains.\n\nFor risky tasks, we have designed three foot-related rewards: feet air time, feet stumble, and feet center, which promote the policy to explore safe footholds, enhancing the safety of movement.\n\nWe propose a terrain map generation model that uses a single LiDAR to obtain the robot-centered height map. Our method minimizes drift compared to the traditional localization approaches while achieving zero-shot transfer capability and optimal computational efficiency.\n\nThe MARG controller empowers quadruped robots to adeptly handle risky gap terrains in the real world, including 65 cm large gaps, 18 cm narrow single-plank bridges, and balance beams with varying sizes, heights, and inclinations.\n\n1. We propose a safe and robust robot controller for locomotion, which can predict the body velocity and the contact state of feet on each step, significantly enhancing the robot’s stability in risky gap terrains.\n\n2. For risky tasks, we have designed three foot-related rewards: feet air time, feet stumble, and feet center, which promote the policy to explore safe footholds, enhancing the safety of movement.\n\n3. We propose a terrain map generation model that uses a single LiDAR to obtain the robot-centered height map. Our method minimizes drift compared to the traditional localization approaches while achieving zero-shot transfer capability and optimal computational efficiency.\n\n4. The MARG controller empowers quadruped robots to adeptly handle risky gap terrains in the real world, including 65 cm large gaps, 18 cm narrow single-plank bridges, and balance beams with varying sizes, heights, and inclinations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高四足机器人在复杂和危险的间隙地形中的稳定性和安全性。  \n2. 现有的盲目运动控制器在感知和选择合适的落脚点方面的局限性。  \n3. 如何减少传统定位方法中的地图漂移并实现零-shot转移能力。  \n\n【用了什么创新方法】  \n本文提出了一种名为MAstering Risky Gap Terrains (MARG)的深度强化学习控制器，集成了地形图和本体感知，以动态调整行动并增强机器人在危险地形中的稳定性。通过选择性地整合特权信息（如重心和摩擦系数），加速了策略优化。设计了三种与脚相关的奖励机制，以鼓励机器人探索安全的落脚点。此外，提出了一种地形图生成模型，仅使用一个LiDAR来生成准确的地形图，从而减少了映射中的漂移。实验结果表明，MARG在各种危险地形任务中保持了稳定性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "An effective control of large systems of active particles: An application to evacuation problem",
            "authors": "Albina Klepach,Egor E. Nuzhin,Alexey A. Tsukanov,Nikolay V. Brilliantov",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19972",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19972",
            "arxiv_html_link": "https://arxiv.org/html/2509.19972v1",
            "abstract": "Manipulation of large systems of active particles is a serious challenge across diverse domains, including crowd management, control of robotic swarms, and coordinated material transport. The development of advanced control strategies for complex scenarios is hindered, however, by the lack of scalability and robustness of the existing methods, in particular, due to the need of an individual control for each agent.\nOne possible solution involves controlling a system through a leader or a group of leaders, which other agents tend to follow. Using such an approach we develop an effective control strategy for a leader, combining reinforcement learning (RL) with artificial forces acting on the system. To describe the guidance of active particles by a leader we\nintroduce the generalized Vicsek model.\nThis novel method is then applied to the problem of\nthe effective evacuation by a robot-rescuer (leader) of large groups of people from hazardous places. We demonstrate, that while a straightforward application of RL yields suboptimal results, even for advanced\narchitectures, our approach provides a robust and efficient evacuation strategy.\nThe source code supporting this study is publicly available at: https://github.com/cinemere/evacuation.",
            "introduction": "The manipulation of large systems of active particles, especially controlling their collective behavior, has become a fundamental problem in recent decades, initiated by emerging new areas of application.\nThe examples, across diverse domains,\ninclude crowd management [1, 2], controlling of robotic swarms\n[3], coordinated material transport [4], etc. The concept of active matter [5] offers an appropriate framework for modeling such ensembles,\ncomprised of agents that consume energy to move, interact with their environment, and adjust their direction of motion subject to external signals [6, 7, 8, 9].\nSynthetic microswimmers, swarming robots, colonies of bacteria, fish schools, flocks of birds, groups of humans [10, 9] – all these systems of living and non-living agents represent an active matter.\n\nThe primary goals of controlling active particle systems include navigating them in complex environments and supporting synchronized collective behavior [11, 12, 13]. This can be addressed by classical optimal control theory, which requires a complete knowledge of the environment and dynamics (similar to Zermelo’s navigation problem [14]), or by reinforcement learning (RL), where an agent learns strategies through trial-and-error [15, 16]. The latter is particularly applicable when an agent has only partial, local information or when the impact of noise is significant. A key limitation of such control problems\nis that their solution relies on the individual manipulation of each participating particle. For ensembles comprising hundreds of agents a straightforward application of RL, with individual-level control, becomes computationally intractable.\n\nTo overcome this limitation, a common strategy involves guiding a group through a leader or multiple leaders, when active particles just follow the leader(s) [17, 18, 19, 20]. This leader-follower paradigm significantly simplifies the control problem by focusing strategies solely on the leader(s). Nevertheless, complexities persist, particularly when managing multiple groups of different size and location, or when groups cannot be manipulated simultaneously or possess non-coinciding goals [21, 22]. Among\nimportant examples of such challenging problems is an effective guidance of large groups of people, especially – their evacuation from hazardous places. Here a leader (rescuer)\nhas the goal to evacuate all people\nguiding them to the exit(s) in shortest time.\nGiven the inherent randomness and non-negligible noise within these systems, reinforcement learning (RL) seems to be the most\nsuitable approach. However, a straightforward application of standard RL\nis neither computationally efficient nor effective in achieving the ultimate goal.\n\nTo this end, we propose an application of auxiliary (artificial)\n“pseudo-gravitational” forces acting on the system, as a part of the environment; this helps the leader to find the most effective guidance strategy.\nSuch a combination of RL with artificial forces [23] results in a very effective method to control large ensembles of active particles.\nTo implement our new method for the evacuation problem, we utilize the generalized Vicsek model. This model takes into account\nnot only interactions between active particles\n(as in the conventional Vicsek model [5]), but also between particles and the leader.\n\nThe rest of the paper is organized as follows. In the next Sec. II we formulate the problem of an effective evacuation from a hazardous place by an informed rescuer-leader. In Sec. III we describe the methods for training optimal evacuation policy and propose an approach based on “pseudo-gravitational” forces, viz the new method of RL with artificial fields. In Sec. IV we report the results of our numerical experiments. Finally, in Sec. V, we summarize our findings.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效控制大规模活性粒子系统以实现集体行为的协调。  \n2. 如何在复杂环境中进行有效的人员疏散，特别是在危险场所的疏散。  \n3. 现有方法在处理多个群体时的可扩展性和鲁棒性问题。  \n\n【用了什么创新方法】  \n本研究提出了一种结合强化学习（RL）与人工“伪引力”力的控制策略，通过引导者控制活性粒子群体的行为。利用广义Vicsek模型，研究者能够有效地指导大规模人群的疏散。实验结果表明，尽管传统RL方法效果不佳，采用新方法后，疏散策略表现出显著的鲁棒性和效率。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Generalist Robot Manipulation beyond Action Labeled Data",
            "authors": "Alexander Spiridonov,Jan-Nico Zaech,Nikolay Nikolov,Luc Van Gool,Danda Pani Paudel",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted at Conference on Robot Learning 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.19958",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19958",
            "arxiv_html_link": "https://arxiv.org/html/2509.19958v1",
            "abstract": "Recent advances in generalist robot manipulation leverage pre-trained Vision–Language Models (VLMs) and large-scale robot demonstrations to tackle diverse tasks in a zero-shot manner. A key challenge remains: scaling high-quality, action-labeled robot demonstration data, which existing methods rely on for robustness and generalization. To address this, we propose a method that benefits from videos without action labels—featuring humans and/or robots in action—enhancing open-vocabulary performance and enabling data-efficient learning of new tasks. Our method extracts dense, dynamic 3D point clouds at the hand or gripper location and uses a proposed 3D dynamics predictor for self-supervision. This predictor is then tuned to an action predictor using a smaller labeled dataset for action alignment. We show that our method not only learns from unlabeled human and robot demonstrations—improving downstream generalist robot policies—but also enables robots to learn new tasks without action labels (i.e., out-of-action generalization) in both real-world and simulated settings.",
            "introduction": "Robust zero-shot manipulation across diverse tasks and environments is one of the biggest bottlenecks towards truly autonomous robots. Inspired by the open-world reasoning capabilities of Large-Language (LLM) and Vision-Language Models (VLM), Vision-Language-Action (VLA) models have emerged for generalist robot manipulation. Approaching this challenge, VLAs extend the semantic reasoning abilities of VLMs with embodied understanding and adapt them for robotic control by training on large datasets of teleoperated robot demonstrations [1, 2, 3, 4, 5, 6, 7]. This has led to impressive progress in learning robust manipulation policies. However, most success is centered around in-domain settings, and performance quickly degrades as the tasks move outside the training distribution. While collecting yet larger robot datasets seems straightforward, it remains unclear what resources would be required to achieve generalist manipulation.\n\nMultimodal training with videos of human demonstrations is a promising alternative to prohibitively expensive robot demonstrations. Such videos contain valuable spatiotemporal information highly relevant to learning robotic control, are readily available at internet scale, and provide diverse tasks and environments. However, learning from human demonstration datasets comes with a range of challenges; Videos provide no direct action labels supervision, exhibit human-to-robot domain gaps, and include redundant or distracting features irrelevant to robotic control.\n\nBeing a fundamental challenge, learning motion priors from humans and unlabeled data has been widely explored. Yet existing work remains confined to specialist, small-scale policies. Some focus on visual representations [8, 9, 10, 11], not considering unseen motions. Others predict visual plans that require a bespoke inverse-dynamics model for execution [12, 13, 14, 15, 16, 17]. Another line retargets human hands to robot grippers [18, 19, 20, 21, 22], but suffers from a large domain gap.\n\nIn this work, we bridge this gap and present MotoVLA, a generalist robot manipulation policy that enables new tasks from human and robot videos without action labels. To achieve this, we propose a VLA model and two-stage training approach using a combination of large-scale labeled and unlabeled\n111Unlabeled refers to non-action-labeled, as action labels are the main challenge in acquiring manipulation data.\nhuman and robot videos.\nIn the first training stage, a dynamic point cloud predictor is trained on the unlabeled data, which establishes a common embodiment-agnostic action representation. Since the dynamic point cloud strongly correlates with the end-effector actions up to hand-eye calibration, the second stage training of an action expert on action-labeled data is simplified. This natural correspondence between dynamic point clouds and 3D robot actions makes our approach particularly effective for learning from unlabeled data. An overview of our method is shown in Figure 1.\nIn summary, our contributions are:\n\nMotoVLA, the first end-to-end VLA model that allows the use of unlabeled data for learning motion priors required for the generalist robot manipulation.\n\nA two-stage training approach enabling the use of dynamic point clouds as a common embodiment-agnostic representation, which is both scalable and intuitive.\n\nExtensive real and simulated evaluations of our model for in-domain, out-of-domain, and transfer learning tasks, demonstrating the effective use of unlabeled data by our model.\n\n1. MotoVLA, the first end-to-end VLA model that allows the use of unlabeled data for learning motion priors required for the generalist robot manipulation.\n\n2. A two-stage training approach enabling the use of dynamic point clouds as a common embodiment-agnostic representation, which is both scalable and intuitive.\n\n3. Extensive real and simulated evaluations of our model for in-domain, out-of-domain, and transfer learning tasks, demonstrating the effective use of unlabeled data by our model.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在缺乏高质量、标注动作的数据情况下实现通用机器人操作？  \n2. 如何利用无标签视频数据提升机器人在多样任务中的表现？  \n3. 如何解决人类与机器人之间的领域差距以实现有效学习？  \n\n【用了什么创新方法】  \n提出了MotoVLA，一个通用机器人操作策略，利用无标签的人类和机器人视频数据进行学习。方法包括一个两阶段的训练流程：第一阶段在无标签数据上训练动态点云预测器，建立通用的动作表示；第二阶段在小规模标注数据上训练动作专家，实现动作对齐。通过这种方式，MotoVLA能够有效地从无标签数据中学习运动先验，提升机器人在多种任务中的表现，尤其是在真实和模拟环境中的迁移学习任务。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation",
            "authors": "Pinhao Song,Yurui Du,Ophelie Saussus,Sofie De Schrijver,Irene Caprara,Peter Janssen,Renaud Detry",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19954",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19954",
            "arxiv_html_link": "https://arxiv.org/html/2509.19954v1",
            "abstract": "We propose a probabilistic shared-control solution for navigation, called Robot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe, effective assistance in human–robot interaction. RT-V2 jointly models a user’s long-term behavioral patterns and their noisy, low-dimensional control signals by combining a prior intent model with a posterior update that accounts for real-time user input and environmental context. The prior captures the multimodal and history-dependent nature of user intent using recurrent neural networks and conditional variational autoencoders, while the posterior integrates this with uncertain user commands to infer desired actions. We conduct extensive experiments to validate RT-V2 across synthetic benchmarks, human–computer interaction studies with keyboard input, and brain–machine interface experiments with non-human primates. Results show that RT-V2 outperforms the state of the art in intent estimation, provides safe and efficient navigation support, and adequately balances user autonomy with assistive intervention. By unifying probabilistic modeling, reinforcement learning, and safe optimization, RT-V2 offers a principled and generalizable approach to shared control for diverse assistive technologies. Code will be available in https://mousecpn.github.io/RTV2_page/.",
            "introduction": "Shared control is a collaborative approach between a human operator and a robot, designed to reduce operator workload and facilitate the more efficient and safer completion of complex tasks with the robot. This approach is widely used in various fields, such as subsea maintenance, surgery, driving, and assistive devices. In the context of assistive devices, it enables individuals with disabilities to regain autonomy through technologies like robotic wheelchairs and manipulators. The key challenge here is that while robots have many degrees of freedom, the input devices available to disabled users are typically low-DoF and noisy due to the nature of their disabilities. Examples include chin joysticks (Rulik et al., 2022) and neural implants (Hochberg et al., 2012). Using these devices to control a wheelchair or robot arm can be slow, tiring, and prone to errors. Shared control addresses these issues by identifying the user’s intent and facilitating smoother, more effortless goal achievement. To enhance this process, shared control often utilizes additional sources of information to interpret user input in context. For instance, cameras can provide images of the surrounding environment to aid in this interpretation.\n\nAccurately assisting and executing a user’s desired action requires understanding their intent.\nPredicting user intent is particularly challenging due to three defining characteristics:\n(i) Multi-modal: a user may approach a goal through different sub-optimal paths;\n(ii) Non-Markovian: past experiences continue to influence current actions (e.g., a previous car accident may make a driver more cautious);\n(iii) Non-stationary: user performance fluctuates, improving when they are focused and declining when they are fatigued.\nThis complexity creates a dynamic interplay between the user’s and the assistive controller’s authority. Users often require more control to effectively convey their intent, while assistive controllers may need increased authority to reduce the user’s effort. This tension gives rise to two critical dilemmas frequently faced by assistive controllers:\n\n(i) Action–noise dilemma:\nAn assistive controller must execute the user’s intended actions while filtering out noise from the user interface. However, this task is complicated by the challenge of distinguishing noise from the effect of the three defining characteristics of user intent listed above.\nOver-reliance on the assistive controller may reduce noise effectively but risks suppressing genuine changes in the user’s intent. Conversely, relying more on the user preserves their autonomy but fails to alleviate their control burden or input noise. For example, as illustrated in Fig. 1 (a), at the current timestep, the assistive controller predicts an action toward goal 1, while the user issues a command toward goal 2. Treating the user’s command as noise and filtering it out may result in the user reaching an unintended goal if the command represents their true intent. On the other hand, fully relying on the user’s command offers no assistance if the command is indeed noise.\nSome approaches attempt to address this dilemma heuristically (Demeester et al., 2008; Song et al., 2024a). By introducing a disagreement threshold, the system differentiates between intended actions and noise, returning control to the user when the threshold is exceeded. However, this method does not fundamentally solve the problem, as there is no guarantee that a user command exceeding the threshold represents true intent. Additionally, the process of setting such a threshold lacks theoretical justification.\nTo fully resolve the action–noise dilemma, the assistive controller must adaptively blend actions by considering the uncertainties of both the user and the environment.\n\n(ii) Disagreement Dilemma: A common approach to shared control involves linearly blending the user’s commands with those of the controller, facilitated by an arbitrator (Dragan and Srinivasa, 2013; Song et al., 2024a; Maeda, 2022). However, as Trautman points out (Trautman, 2015), tasks such as collision avoidance often allow for multiple equally optimal trajectories due to the multi-modal nature of human intent. The user may select any of these trajectories, which can differ from the controller’s prediction.\nWhen the user disagrees with the assistive controller at a given timestep, blending a safe user command with a safe controller-proposed action may unintentionally result in an unsafe shared action. For example, as shown in Fig. 1 (b), the assistive controller predicts an action to the left, while the user commands movement to the right. A linear blend of these two actions could lead to a collision with the obstacle.\nExisting approaches, such as those based on probabilistic models (Trautman, 2015), constraint-based shared control (Iregui et al., 2021), and model predictive control (MPC) (Lu et al., 2019), attempt to address this issue by implicitly blending policies. However, these methods often reduce to linear blending, limiting their ability to fully resolve the disagreement dilemma.\nTo effectively tackle this challenge, the assistive controller should adopt a multi-modal blending strategy. For instance, it could generate multiple trajectory proposals and blend the user’s command with the proposal most aligned with their intent, ensuring both safety and responsiveness.\n\nIn this paper, we propose an assistive controller named Robot Trajectron V2 (RT-V2) for navigation tasks. RT-V2 is designed within a Bayesian framework, as:\n\nIn summary:\n\nOur paper proposes Robot Trajectron V2 (RT-V2), a Bayesian-based assistive controller designed for navigation tasks. RT-V2 models user behavior using a prior trained on past data and a posterior that adapts to real-time user commands, addressing both action-noise and disagreement dilemmas.\n\nThe prior model is built using a recurrent neural network and a conditional variational autoencoder (CVAE), enabling it to capture multi-modal and non-Markovian aspects of human intent. This enhances the controller’s ability to accurately interpret and blend user commands in a dynamic shared control setting.\n\nTo overcome causal confusion in imitation learning, we introduce Imagined Rollout Reinforcement Learning, where RT-V2 simulates future interactions to receive reward signals and refine its autonomous navigation capabilities.\n\nA sampling-based trajectory optimization method with safety constraints is employed to ensure the controller’s actions are collision-free.\n\nThe novel contributions of the paper are:\n\nA shared-control model grounded in a probabilistic formulation of the intention prior and posterior, and their acquisition from data via a combination of imitation learning, reinforcement learning, and sampling-based optimization.\n\nExtensive experiments show that RT-V2 achieves high accuracy in intent estimation and safe, efficient navigation. Tests with human users (keyboard interface) and monkey users (BMI interface) demonstrate its effectiveness in optimizing shared autonomy with respect to agreeability, safety, and efficiency.\n\n1. Our paper proposes Robot Trajectron V2 (RT-V2), a Bayesian-based assistive controller designed for navigation tasks. RT-V2 models user behavior using a prior trained on past data and a posterior that adapts to real-time user commands, addressing both action-noise and disagreement dilemmas.\n\n2. The prior model is built using a recurrent neural network and a conditional variational autoencoder (CVAE), enabling it to capture multi-modal and non-Markovian aspects of human intent. This enhances the controller’s ability to accurately interpret and blend user commands in a dynamic shared control setting.\n\n3. To overcome causal confusion in imitation learning, we introduce Imagined Rollout Reinforcement Learning, where RT-V2 simulates future interactions to receive reward signals and refine its autonomous navigation capabilities.\n\n4. A sampling-based trajectory optimization method with safety constraints is employed to ensure the controller’s actions are collision-free.\n\n1. A shared-control model grounded in a probabilistic formulation of the intention prior and posterior, and their acquisition from data via a combination of imitation learning, reinforcement learning, and sampling-based optimization.\n\n2. Extensive experiments show that RT-V2 achieves high accuracy in intent estimation and safe, efficient navigation. Tests with human users (keyboard interface) and monkey users (BMI interface) demonstrate its effectiveness in optimizing shared autonomy with respect to agreeability, safety, and efficiency.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何准确预测用户意图以实现安全有效的导航支持。  \n2. 如何解决助理控制器在用户命令和控制器建议之间的行动噪声和不一致性困境。  \n\n【用了什么创新方法】  \n本研究提出了Robot Trajectron V2 (RT-V2)，一个基于贝叶斯框架的助理控制器，旨在解决导航任务中的用户意图建模问题。RT-V2结合了先验模型和后验更新，利用递归神经网络和条件变分自编码器捕捉用户的多模态和非马尔可夫特性。通过引入想象滚动强化学习，RT-V2能够模拟未来交互以优化自主导航能力。此外，采用基于采样的轨迹优化方法确保安全性。实验结果表明，RT-V2在意图估计和安全导航方面优于现有方法，展示了其在共享自主性优化中的有效性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference",
            "authors": "Zijun Che,Yinghong Zhang,Shengyi Liang,Boyu Zhou,Jun Ma,Jinni Zhou",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19916",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19916",
            "arxiv_html_link": "https://arxiv.org/html/2509.19916v1",
            "abstract": "Autonomous exploration in structured and complex indoor environments remains a challenging task, as existing methods often struggle to appropriately model unobserved space and plan globally efficient paths.\nTo address these limitations, we propose GUIDE, a novel exploration framework that synergistically combines global graph inference with diffusion-based decision-making.\nWe introduce a region-evaluation global graph representation that integrates both observed environmental data and predictions of unexplored areas, enhanced by a region-level evaluation mechanism to prioritize reliable structural inferences while discounting uncertain predictions.\nBuilding upon this enriched representation, a diffusion policy network generates stable, foresighted action sequences with significantly reduced denoising steps. Extensive simulations and real-world deployments demonstrate that GUIDE consistently outperforms state-of-the-art methods, achieving up to 18.3% faster coverage completion and a 34.9% reduction in redundant movements.",
            "introduction": "Autonomous exploration remains a cornerstone of modern robotics research, with pivotal applications in scenarios where unknown environment coverage is critical: environmental monitoring, warehouse logistics, and search-and-rescue operations [1, 2]. A defining challenge in these tasks is efficiently covering all reachable areas under stringent constraints—limited time, finite energy, and constrained computational resources. Despite decades of progress, existing exploration strategies still struggle to appropriately model unobserved space and plan globally efficient paths.\n\nCurrent exploration methodologies can be broadly categorized into model-based and learning-based approaches, each exhibiting fundamental limitations in addressing the global coverage challenge. Early model-based techniques, including frontier-based methods [3, 4] and sampling-based exploration strategies [5], rely exclusively on observed map information to determine exploration directions. While these approaches demonstrate reasonable performance in structured environments, their inherent myopia toward unobserved areas frequently results in redundant revisits, inefficient path planning, and suboptimal coverage—particularly in environments with complex topologies. Coverage path-based methods [6, 7] attempt to address this limitation by incorporating explicit coverage objectives; however, their reliance on uniform grid decomposition implicitly assumes environmental regularity, leading to performance degradation in spaces with irregular layouts or varying structural complexity.\n\nMore recently, learning-based approaches have emerged as promising alternatives, yet they too face significant challenges in achieving comprehensive spatial understanding. The first category employs neural networks to directly map observed environments to exploration actions [8, 9, 10]. Although these methods improve adaptability to specific environments, they fundamentally operate with limited information—encoding only observed areas while remaining agnostic to the structure of unknown spaces. This inherent constraint severely limits their capacity to achieve globally efficient exploration, often requiring extensive training across diverse environments to achieve moderate performance. The second category explicitly predicts unobserved areas and associated information gain [11, 12]. While conceptually promising, these approaches typically utilize predicted maps only for local planning rather than incorporating them into a comprehensive global planning framework, thereby failing to fully leverage the predictive information for long-horizon path optimization.\n\nThese limitations collectively highlight a critical research gap: the absence of a unified framework that effectively integrates predictions of unknown areas with globally optimized exploration planning. Specifically, existing methods lack mechanisms to (1) construct a comprehensive environmental representation that coherently combines observed information with predictions of unexplored areas, (2) leverage credible predictions to guide exploration decisions, and (3) generate stable, long-horizon trajectories that maximize coverage efficiency while minimizing redundant movements.\n\nTo address these challenges, we propose GUIDE, a novel exploration framework that synergistically combines global graph inference with diffusion-based decision-making. At its core, GUIDE constructs a region-evaluation global graph representation that integrates both observed environmental data and predictions of unexplored areas. This representation is enhanced through a region-level evaluation mechanism that prioritizes significant regional structural inferences, effectively creating an informative yet compact environmental model that prioritizes credible structural inferences while appropriately discounting uncertain predictions. Building upon this enriched representation, GUIDE employs a diffusion policy network that generates stable, foresighted action sequences with significantly reduced denoising steps compared to conventional approaches—enabling efficient long-horizon planning that effectively balances immediate information gain with comprehensive coverage objectives. The reduced computational overhead ensures real-time responsiveness—a critical advantage for resource-constrained robotic platforms.\n\nWe rigorously evaluate GUIDE across diverse simulation environments with varying structural complexities and through real-world deployments on physical robotic platforms. Quantitative results demonstrate consistent improvements over state-of-the-art methods, with our approach achieving up to 18.3% faster coverage completion and a 34.9% reduction in redundant movements across benchmark environments. Qualitative analysis further showcases GUIDE’s superior capability in structural inference and adaptive exploration behavior.\nThe main contributions of this work are threefold:\n\n1) We introduce a region-evaluation global graph inference module that constructs a unified environmental representation by integrating observed information with predictions of unexplored areas. It incorporates a novel region-evaluation mechanism that assesses the reliability and decision relevance of predicted areas, enabling robust and reliable exploration planning under uncertainty.\n\n2) We develop a diffusion-based decision-making framework that leverages the global graph representation to generate stable, long-horizon exploration trajectories, significantly reducing the computational burden of conventional diffusion policies while producing foresighted and efficient exploration paths.\n\n3) We conduct comprehensive evaluations across multiple simulation environments and real-world scenarios, demonstrating GUIDE’s superior performance in both structural inference accuracy and exploration efficiency metrics, establishing a new benchmark for autonomous exploration systems.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效建模未观察空间以实现自主探索？  \n2. 现有方法在复杂环境中的路径规划效率不足。  \n3. 缺乏统一框架整合已知与未知区域的预测信息。  \n\n【用了什么创新方法】  \n本研究提出了GUIDE框架，结合全球图推理与扩散决策。首先，构建了一个区域评估的全球图表示，整合已观察环境数据与未探索区域的预测。通过区域级评估机制，优先考虑可靠的结构推断。然后，利用扩散策略网络生成稳定的长远行动序列，显著减少去噪步骤。实验结果显示，GUIDE在覆盖完成速度上提高了18.3%，并减少了34.9%的冗余移动，展现出优越的探索效率和结构推断能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "D3Grasp: Diverse and Deformable Dexterous Grasping for General Objects",
            "authors": "Keyu Wang,Bingcong Lu,Zhengxue Cheng,Hengdi Zhang,Li Song",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19892",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19892",
            "arxiv_html_link": "https://arxiv.org/html/2509.19892v1",
            "abstract": "Achieving diverse and stable dexterous grasping for general and deformable objects remains a fundamental challenge in robotics, due to high-dimensional action spaces and uncertainty in perception. In this paper, we present D3Grasp, a multimodal perception-guided reinforcement learning framework designed to enable Diverse and Deformable Dexterous Grasping. We firstly introduce a unified multimodal representation that integrates visual and tactile perception to robustly grasp common objects with diverse properties. Second, we propose an asymmetric reinforcement learning architecture that exploits privileged information during training while preserving deployment realism, enhancing both generalization and sample efficiency.\nThird, we meticulously design a training strategy to synthesize contact-rich, penetration-free, and kinematically feasible grasps with enhanced adaptability to deformable and contact-sensitive objects.\nExtensive evaluations confirm that D3Grasp delivers highly robust performance across large-scale and diverse object categories, and substantially advances the state of the art in dexterous grasping for deformable and compliant objects, even under perceptual uncertainty and real-world disturbances. D3Grasp achieves an average success rate of 95.1% in real-world trials—outperforming prior methods on both rigid and deformable objects benchmarks.",
            "introduction": "Dexterous robotic hands, with their human-like kinematic structures and multi-finger adaptability, hold transformative potential across industrial assembly, elderly care, and hazardous material handling. Recent advances in hardware design, exemplified by Shadow Dexterous Hand Robot (2025), Allegro Hand Hand (2025), and Paxini Dexhand13 Paxini (2025), have enabled 16+ degree-of-freedom (DoF) manipulation capabilities approaching human-level dexterity. However, two fundamental challenges persist in bridging this mechanical potential to real-world applications Xiao et al. (2025); An et al. (2025): multimodal perception integration and data-efficient policy learning, particularly for long-horizon manipulation tasks.\nContemporary robotic manipulation systems primarily depend on single sensing modality, each with inherent limitations: vision enables global localization but struggles with transparency or occlusion; tactile sensing offers precise contact feedback, yet lacks global awareness; proprioception monitors internal states but provides minimal environmental understanding. Hybrid architectures, such as visual-tactile fusion networks  Li et al. (2024b); Akinola et al. (2024); Dave et al. (2024); Ferrandis et al. (2024); Jin et al. (2023); Parsons et al. (2022), attempt to address these constraints through direct sensor concatenation. However, this approach induces a high-dimensional observation space, hindering policy convergence Tao et al. (2024). Crucially, fixed fusion weights cannot adapt to the varying sensory dominance across manipulation phases Li et al. (2022a); Wang et al. (2025), often resulting in conflicting signals that degrade control stability Akinola et al. (2024).\n\nFor data-efficient policy learning, modern simulation platforms such as IsaacSim Mittal et al. (2023), PyBullet Coumans and Bai (2016–2021), Genesis Authors (2024), Robotwin Mu et al. (2025) allow safe parallelized reinforcement learning (RL) exploration Li (2017). However, sim2real transfer is fundamentally hindered by sparse rewards and exponential exploration complexity in long-horizon tasks Wang et al. (2022), and catastrophic error propagation across sequential subtasks owing to compounding inaccuracies. Consequently, data-driven approaches leveraging imitation learning (IL) Hussein et al. (2017) and policy distillation (PD) Rusu et al. (2015) are gaining traction for improved sample efficiency Mandlekar et al. (2021), although scaling high-quality teleoperation data remains prohibitively expensive due to human-robot morphological differences Darvish et al. (2023). While integrated RL/IL/PD strategies Zhang et al. (2025a); Wan et al. (2023) mitigate data costs, they often overlook the critical influence of diverse object properties and grasp configurations in sim2real deployment.\n\nTo overcome these limitations, we introduce a multimodal learning framework for dexterous manipulation. Our primary contributions are threefold: (1) We develop a tactile-based multimodal perception representation capable of maximally leveraging environmental information and proprioception, while dynamically selecting optimal contact force outputs based on object material texture. (2) We construct an asymmetric actor-critic (AAC) network architecture utilizing privileged information; this framework employs simulated privileged data (e.g., deformable object deformation states) for policy value estimation within simulation, enabling optimal control mode selection and reducing the excessive reliance on perceptual precision in contact-intensive operations. (3) We propose a hybrid training paradigm that incorporates multiple category of objects and defines task-specific grasping postures, enabling the agent to acquire enhanced generalization capabilities.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现对一般和可变形物体的多样化和稳定的灵巧抓取。  \n2. 如何有效整合多模态感知以提高抓取性能。  \n3. 如何在长时间操作任务中实现数据高效的策略学习。  \n\n【用了什么创新方法】  \n本研究提出了D3Grasp，一个多模态感知引导的强化学习框架，旨在实现多样化和可变形的灵巧抓取。首先，构建了一个统一的多模态表示，集成视觉和触觉感知，以增强对不同物体的抓取能力。其次，设计了一个不对称的强化学习架构，利用特权信息进行训练，同时保持部署的现实性，从而提高了泛化能力和样本效率。最后，开发了一种训练策略，合成接触丰富、无穿透和运动学可行的抓取，增强了对可变形和接触敏感物体的适应性。D3Grasp在真实世界试验中取得了95.1%的平均成功率，显著超越了先前方法。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "SAGE:State-Aware Guided End-to-End Policy for Multi-Stage Sequential Tasks via Hidden Markov Decision Process",
            "authors": "BinXu Wu,TengFei Zhang,Chen Yang,JiaHao Wen,HaoCheng Li,JingTian Ma,Zhen Chen,JingYuan Wang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19853",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19853",
            "arxiv_html_link": "https://arxiv.org/html/2509.19853v1",
            "abstract": "Multi-stage sequential (MSS) robotic manipulation tasks are prevalent and crucial in robotics. They often involve state ambiguity, where visually similar observations correspond to different actions. We present SAGE, a state-aware guided imitation learning framework that models tasks as a Hidden Markov Decision Process (HMDP) to explicitly capture latent task stages and resolve ambiguity. We instantiate the HMDP with a state transition network that infers hidden states, and a state-aware action policy that conditions on both observations and hidden states to produce actions, thereby enabling disambiguation across task stages. To reduce manual annotation effort, we propose a semi-automatic labeling pipeline combining active learning and soft label interpolation. In real-world experiments across multiple complex MSS tasks with state ambiguity, SAGE achieved 100% task success under the standard evaluation protocol, markedly surpassing the baselines. Ablation studies further show that such performance can be maintained with manual labeling for only about 13% of the states, indicating its strong effectiveness.",
            "introduction": "Robotic manipulation tasks have attracted significant attention due to their broad applications. Vision-based strategies have been widely adopted [1], and have demonstrated remarkable performance across a variety of real-world scenarios [2, 3, 4, 5, 6]. However, a particular class of tasks—Multi-Stage Sequential (MSS) tasks—introduces distinctive challenges to vision-based policies. MSS tasks are characterized by a sequence of interdependent stages that must be executed in a prescribed temporal order, often requiring the policy to perform long-horizon reasoning, retain contextual information from prior steps, and ensure coherent progression across successive stages.\n\nIn many MSS tasks, conventional vision-based policies struggle in scenarios involving state ambiguity. In such cases, visually similar observations may correspond to different actions, resulting in ambiguity during action selection. An illustrative case is the Push Buttons task shown in Fig. 1. The visual observations at stages 1-1, 2-1, and 3-1 are nearly indistinguishable; however, the correct action—pressing the yellow, pink, or blue button—requires knowledge of the current task stage to be correctly determined. This requires the policy to map similar observations to distinct actions, a phenomenon we refer to as State Ambiguity. Similar challenges also arise in other real-world contexts, such as assessing whether a container has been filled in a warehouse packaging task, or judging whether a cloth is wet or dry during household cleaning. These examples highlight the inherent difficulty of resolving state ambiguity when relying solely on visual input.\n\nTo handle state ambiguity in MSS tasks, existing methods mainly fall into two categories: Memory-based approaches and hierarchical task decomposition. (1) In robotic manipulation, memory-based methods use models like recurrent neural networks [7], attention mechanisms [8], Transformer-XL [9] to capture historical context, as incorporating earlier observations can help distinguish visually similar states that lead to ambiguity. Although these methods are flexible, they often struggle with redundant information, high computational cost, and difficulties in deciding how much history to retain. (2)Hierarchical approaches, on the other hand, structure the policy into multiple levels of controllers. A high-level controller manages stage transitions, while low-level policies are responsible for executing specific actions [10, 11, 12, 13]. While this structure helps reduce ambiguity, designing the high-level controller typically requires extensive manual effort. For example, some methods use a nine-layer decision tree to handle transitions [11]. Moreover, since low-level modules directly execute action primitives, they are often constrained to be simple and modular, which limits flexibility. Transitions between primitives further introduce delays, reducing execution efficiency and stability in real-world scenarios.\n\nTo address the issue of State Ambiguity, we propose SAGE, a State-Aware Guided End-to-End Imitation Learning framework based on the Hidden Markov Decision Process (HMDP). Specifically, in Section III, we provide a theoretical analysis. The key idea is to treat observations as partial manifestations of a latent environment state and to explicitly model this state as a hidden variable. With this formulation, the HMDP can distinguish between different underlying physical states that share similar visual appearances, thereby resolving ambiguity. The HMDP consists of two components: hidden state estimation and decision-making agent. In Section IV, guided by the theoretical formulation, we implement these components as two neural networks. The state transition network infers hidden states from the current observation together with the previously estimated state, while the state-aware action policy generates actions by conditioning jointly on visual observations and the inferred states. We integrate them into an End-to-End architecture and train the framework using actions from expert demonstrations and human-labeled states as supervision. Furthermore, as detailed in Section V, to reduce the manual annotation cost of state labels required by supervised learning, we propose a semi-automatic labeling pipeline that integrates active learning with soft label interpolation. It substantially reduces manual labeling effort by annotating only a small subset of representative segments and automatically labeling the remaining data.\n\nExtensive real-world experiments on three MSS tasks, all of which involve state ambiguity, were conducted to evaluate the effectiveness of SAGE. The results show that it achieves up to a 100% stage success rate under standard evaluation settings, significantly outperforming competitive baselines. Our method remains robust under visually distracting conditions and continuous execution, completing 50-step sequences without error. Ablation studies show that our semi-automatic annotation strategy in SAGE achieves full success with only 13% of episodes manually labeled, demonstrating efficient annotation. These results validate the effectiveness and generality of our approach in tackling real-world state ambiguity.\n\nTherefore, we propose a unified imitation learning framework. Our main contributions are as follows:\n\nTo the best of our knowledge, we are the first to formulate MSS tasks with state ambiguity as a Hidden Markov Decision Process, offering a principled framework to address this challenge.\n\nWe realize the HMDP formulation through two dedicated neural modules for hidden state inference and state-aware action generation, which are jointly integrated into an End-to-End training pipeline.\n\nWe propose a semi-automatic state annotation strategy that substantially reduces the human effort required to provide state supervision signals for the HMDP.\n\nWe conduct extensive real-world experiments on multiple MSS tasks with state ambiguity, demonstrating the superior performance and robustness of SAGE.\n\n1. To the best of our knowledge, we are the first to formulate MSS tasks with state ambiguity as a Hidden Markov Decision Process, offering a principled framework to address this challenge.\n\n2. We realize the HMDP formulation through two dedicated neural modules for hidden state inference and state-aware action generation, which are jointly integrated into an End-to-End training pipeline.\n\n3. We propose a semi-automatic state annotation strategy that substantially reduces the human effort required to provide state supervision signals for the HMDP.\n\n4. We conduct extensive real-world experiments on multiple MSS tasks with state ambiguity, demonstrating the superior performance and robustness of SAGE.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何处理多阶段顺序（MSS）任务中的状态模糊性问题。  \n2. 如何通过隐马尔可夫决策过程（HMDP）建模MSS任务以捕捉潜在任务阶段。  \n3. 如何减少对手动标注的依赖，提高状态标注的效率。  \n\n【用了什么创新方法】  \n提出了一种名为SAGE的状态感知引导模仿学习框架，通过隐马尔可夫决策过程（HMDP）建模MSS任务，显式捕捉潜在状态并解决状态模糊性。该方法结合了状态转移网络和状态感知动作策略，能够根据观察和隐状态生成动作。通过引入半自动标注管道，结合主动学习和软标签插值，显著降低了手动标注的工作量。实验结果显示，SAGE在多个复杂MSS任务中实现了100%的任务成功率，远超基线方法，并且在仅标注13%状态的情况下仍能保持高效性能。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "DynaFlow: Dynamics-embedded Flow Matching for Physically Consistent Motion Generation from State-only Demonstrations",
            "authors": "Sowoo Lee,Dongyun Kang,Jaehyun Park,Hae-Won Park",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19804",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19804",
            "arxiv_html_link": "https://arxiv.org/html/2509.19804v1",
            "abstract": "This paper introduces DynaFlow, a novel framework that embeds a differentiable simulator directly into a flow matching model. By generating trajectories in the action space and mapping them to dynamically feasible state trajectories via the simulator, DynaFlow ensures all outputs are physically consistent by construction. This end-to-end differentiable architecture enables training on state-only demonstrations, allowing the model to simultaneously generate physically consistent state trajectories while inferring the underlying action sequences required to produce them.\nWe demonstrate the effectiveness of our approach through quantitative evaluations and showcase its real-world applicability by deploying the generated actions onto a physical Go1 quadruped robot.\nThe robot successfully reproduces diverse gait present in the dataset, executes long-horizon motions in open-loop control and translates infeasible kinematic demonstrations into dynamically executable, stylistic behaviors. These hardware experiments validate that DynaFlow produces deployable, highly effective motions on real-world hardware from state-only demonstrations, effectively bridging the gap between kinematic data and real-world execution.",
            "introduction": "Generative models, such as Diffusion Models and Flow Matching, have recently achieved unprecedented success across various domains, including image[1, 2, 3], audio[4], and text generation[5]. They have demonstrated a remarkable ability to learn intricate data distributions from large-scale datasets, producing highly natural and diverse outputs. Inspired by this success, these models are increasingly being recognized as powerful tools for generating complex motion trajectories in fields like robotics and computer graphics[6, 7, 8].\nIndeed, their application to kinematic motion generation has seen significant progress, largely driven by the increasing availability of state demonstration data from sources such as motion capture and raw video.\n\nHowever, directly applying these generative models to character animation and robot control presents significant challenges.\nA primary limitation is the lack of physical consistency.\nMost generative models learn a statistical approximation of the data distribution from a finite set of examples, rather than the underlying physical principles governing the data. Consequently, there is no guarantee that the generated outputs will adhere to intrinsic physical principles or dynamic constraints.\nThis means that when generating novel behaviors, the resulting motions can be either physically implausible, with artifacts like ground penetration, character floating, and foot sliding, or dynamically inconsistent, making them unsuitable to execute in the physical world.\nThe issue becomes particularly pronounced when models are trained on datasets with inherent physical inconsistencies, such as those sourced from motion capture or generated through kinematic retargeting.\n\nAnother major hurdle is the scarcity of action-labeled data. While state trajectories can often be obtained from motion capture or other sources, the corresponding action sequences (e.g., joint torques or motor commands) are rarely available and are costly to collect, typically being hardware-specific. Prior diffusion-based control approaches can be categorized into two principal ways. One option is to directly train a policy to predict actions from true action data, but this is rarely viable given the limited availability of true action data. Alternatively, a hierarchical framework can be employed, where the diffusion model generates desired state trajectories that are then executed by a low-level tracking controller. However, this hierarchical strategy has its own drawback: the tracking controller usually requires extensive fine-tuning for each new motion, and ensuring robustness against the distributional gap between planned trajectories and the controller remains a significant challenge [9].\n\nTo overcome these limitations, we propose DynaFlow, a novel framework that guarantees physically consistent motion generation by embedding dynamics directly into the generation process, while simultaneously inferring actions from state-only demonstration data.\nThe core idea of DynaFlow is to integrate a differentiable simulator at the output of the flow matching prediction module. This simulator layer acts as a mapping from the space of action trajectories to the space of dynamically feasible state trajectories, ensuring that the model’s output strictly adheres to the laws of physics by construction. Furthermore, its differentiable nature allows the entire model to be trained end-to-end. During this process, the model naturally discovers the action trajectory required to reconstruct a given state trajectory in a dynamically consistent manner, even without explicit action labels.\n\nWe conduct a series of experiments to validate the effectiveness of DynaFlow. Our quantitative analysis compares DynaFlow against several baselines, evaluating both dynamic feasibility and distributional similarity on two distinct datasets: a rich, strictly feasible dataset of quadruped locomotion and a challenging single-trajectory dataset from retargeted motion capture.\nOur results demonstrate that DynaFlow consistently generates strictly feasible trajectories, even when trained on a physically inconsistent dataset, while remaining competitive in distributional similarity. To showcase its real-world applicability, we deploy action trajectories generated by DynaFlow on a physical Go1 quadruped robot.\nThe robot successfully reproduces diverse gaits observed in the training data and executes long-horizon motions with high accuracy in challenging open-loop experiments, validating the precision and coherence of the generated actions.\nFurthermore, we demonstrate its ability to translate infeasible retargeted motions into dynamically executable and stylistic behaviors on hardware, bridging the gap between kinematic demonstration and real-world execution.\n\nThe main contributions of this paper are as follows:\n\nDynamics-embedded generative modeling:\nWe propose DynaFlow, a novel generative model that embeds a differentiable simulator into a flow matching framework to guarantee all generated trajectories are strictly dynamically consistent by construction.\n\nLearning from state-only data:\nOur method overcomes the common challenge of unavailable action data by leveraging analytical gradients, learning action directly from state-only demonstrations.\n\nReal-world validation:\nWe demonstrate the practical viability of our approach through successful hardware deployment of the generated motion on a physical quadruped robot, bridging the gap between motion generation and real-world execution.\n\n1. Dynamics-embedded generative modeling:\nWe propose DynaFlow, a novel generative model that embeds a differentiable simulator into a flow matching framework to guarantee all generated trajectories are strictly dynamically consistent by construction.\n\n2. Learning from state-only data:\nOur method overcomes the common challenge of unavailable action data by leveraging analytical gradients, learning action directly from state-only demonstrations.\n\n3. Real-world validation:\nWe demonstrate the practical viability of our approach through successful hardware deployment of the generated motion on a physical quadruped robot, bridging the gap between motion generation and real-world execution.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何确保生成的运动轨迹在物理上是一致的？  \n2. 如何从仅有的状态演示数据中推断出相应的动作序列？  \n3. 如何将生成的动作有效地应用于现实世界的机器人？  \n\n【用了什么创新方法】  \nDynaFlow框架通过将可微分模拟器嵌入流匹配模型中，确保生成的运动轨迹在物理上始终一致。该方法允许从状态演示数据中直接学习动作序列，克服了缺乏动作标注数据的挑战。通过一系列实验，DynaFlow在动态可行性和分布相似性方面表现出色，并成功在物理四足机器人上部署生成的动作，验证了其在真实环境中的有效性和精确性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training",
            "authors": "Rushuai Yang,Hangxing Wei,Ran Zhang,Zhiyuan Feng,Xiaoyu Chen,Tong Li,Chuheng Zhang,Li Zhao,Jiang Bian,Xiu Su,Yi Chen",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19752",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19752",
            "arxiv_html_link": "https://arxiv.org/html/2509.19752v1",
            "abstract": "Vision-language-action (VLA) models have shown strong generalization across tasks and embodiments; however, their reliance on large-scale human demonstrations limits their scalability owing to the cost and effort of manual data collection. Reinforcement learning (RL) offers a potential alternative to generate demonstrations autonomously, yet conventional RL algorithms often struggle on long-horizon manipulation tasks with sparse rewards. In this paper, we propose a modified diffusion policy optimization algorithm to generate high-quality and low-variance trajectories, which contributes to a diffusion RL-powered VLA training pipeline. Our algorithm benefits from not only the high expressiveness of diffusion models to explore complex and diverse behaviors but also the implicit regularization of the iterative denoising process to yield smooth and consistent demonstrations. We evaluate our approach on the LIBERO benchmark, which includes 130 long-horizon manipulation tasks, and show that the generated trajectories are smoother and more consistent than both human demonstrations and those from standard Gaussian RL policies. Further, training a VLA model exclusively on the diffusion RL-generated data achieves an average success rate of 81.9%, which outperforms the model trained on human data by +5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight our diffusion RL as an effective alternative for generating abundant, high-quality, and low-variance demonstrations for VLA models.",
            "introduction": "Vision-language-action (VLA) is a promising model toward general-purpose robots capable of generalizing across a wide array of manipulation tasks [1, 2, 3]. However, this paradigm is critically dependent on massive datasets of human demonstrations, such as the Open X-Embodiment dataset [4]. The process of collecting this data via manual teleoperation is notoriously expensive and labor-intensive. The reliance on manual data collection fundamentally caps the scalability of VLA models, presenting a major bottleneck to further progress.\n\nReinforcement learning (RL) has emerged as a powerful paradigm for enabling robots to acquire sophisticated physical skills directly through environmental interaction.\nThe fundamental strength of reinforcement learning stems from its trial-and-error process: by optimizing for a reward signal, an agent can autonomously discover highly effective and efficient strategies that often surpass what can be learned by simply mimicking human demonstrations.\nHowever, a significant limitation of this approach is that the resulting policies are often highly specialized.\nA policy trained to excel under one specific set of conditions typically struggles to adapt or generalize its skills when faced with new task variations or different environmental setups [5].\nHowever, making a general RL algorithm effective enough to generate high-quality data across diverse, complex manipulation tasks is challenging.\nThe long-horizon, sparse-reward tasks prevalent in benchmarks like LIBERO [6] expose critical weaknesses of conventional RL algorithms, often leading to unstable learning process or high-variance and suboptimal trajectories [7].\n\nTo this end, we propose a general framework that utilizes a modified diffusion policy optimization algorithm for diffusion RL-powered data generation.\nWe find that diffusion offer a superior alternative for this problem.\nFirst, diffusion policy provide good expressiveness to fit complex expert distribution.\nCompared with Gaussian RL, diffusion-based RL provides more space for RL exploration when interacting with environment.\nSecond, the inherent structure of the iterative denoising process acts as a powerful implicit regularizer on the action space, The model is trained to predict the noise for the entire action chunk at every step of the denoising process. This forces the model to learn the underlying structure of smooth, physically plausible motions. A single, jerky movement in the final action would require a very specific and complex sequence of denoising steps, which is less likely to be learned than a smooth, coherent refinement process. This naturally encourages the generation of temporally smooth, low-variance motion. We further enhance this process with a stabilized fine-tuning regimen, incorporating modifications to the architecture and training strategies to ensure robust performance across the 130 challenging tasks in LIBERO. This property allows our RL agent to explore more effectively and converge to near-optimal and low-variance policies.\n\nOur experiments yield a clear message:\nA VLA model trained exclusively on our RL-generated data consistently and significantly surpasses the ones trained on human data and Gaussian RL, both on in-distribution tasks and in challenging OOD generalization. A quantitative analysis reveals the mechanism behind this success: Our generated trajectories are smoother and less variable, providing a more stable learning signal for VLA training.\nOur contributions are threefold:\n\nA diffusion RL-powered VLA training pipeline for autonomously generating high-quality and low-variance data for VLA training, including validated effective modifications on the model architecture and training strategies.\n\nCompelling empirical evidence on the 130 complex manipulation tasks of the LIBERO benchmark shows that our synthetic data provides superior training signal to human demonstrations, significantly improving both the in-distribution success rates and out-of-distribution generalization of VLA models.\n\nAn in-depth quantitative analysis that relates trajectory-level properties (e.g., efficiency, smoothness, and consistency) with the performance of fine-tuned VLA, providing a clear explanation for why optimized data is more effective.\n\n1. A diffusion RL-powered VLA training pipeline for autonomously generating high-quality and low-variance data for VLA training, including validated effective modifications on the model architecture and training strategies.\n\n2. Compelling empirical evidence on the 130 complex manipulation tasks of the LIBERO benchmark shows that our synthetic data provides superior training signal to human demonstrations, significantly improving both the in-distribution success rates and out-of-distribution generalization of VLA models.\n\n3. An in-depth quantitative analysis that relates trajectory-level properties (e.g., efficiency, smoothness, and consistency) with the performance of fine-tuned VLA, providing a clear explanation for why optimized data is more effective.",
            "llm_summary": "【关注的是什么问题】  \n1. VLA模型依赖于大量人工示范数据，限制了其可扩展性。  \n2. 传统的强化学习算法在长时间跨度的稀疏奖励任务中表现不佳。  \n3. 如何生成高质量、低方差的演示数据以提高VLA训练效果。  \n\n【用了什么创新方法】  \n提出了一种改进的扩散策略优化算法，以生成高质量、低方差的轨迹，构建了一个基于扩散强化学习的VLA训练管道。该算法利用扩散模型的高表达能力探索复杂行为，并通过迭代去噪过程的隐式正则化生成平滑、一致的演示。实验结果表明，使用扩散RL生成的数据训练的VLA模型在130个长时间跨度的操作任务中，成功率达到81.9%，超越了基于人工数据和标准高斯RL生成数据的模型，展示了扩散RL在生成丰富、高质量演示数据方面的有效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Trajectory Planning Using Safe Ellipsoidal Corridors as Projections of Orthogonal Trust Regions",
            "authors": "Akshay Jaitly,Jon Arrizabalaga,Guanrui Li",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19734",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19734",
            "arxiv_html_link": "https://arxiv.org/html/2509.19734v1",
            "abstract": "Planning collision free trajectories in complex environments remains a core challenge in robotics. Existing corridor based planners which rely on decomposition of the free space into collision free subsets scale poorly with environmental complexity and require explicit allocations of time windows to trajectory segments. We introduce a new trajectory parameterization that represents trajectories in a nonconvex collision free corridor as being in a convex cartesian product of balls. This parameterization allows us to decouple problem size from geometric complexity of the solution and naturally avoids explicit time allocation by allowing trajectories to evolve continuously inside ellipsoidal corridors. Building on this representation, we formulate the Orthogonal Trust Region Problem (Orth-TRP), a specialized convex program with separable block constraints, and develop a solver that exploits this parallel structure and the unique structure of each parallel subproblem for efficient optimization. Experiments on a quadrotor trajectory planning benchmark show that our approach produces smoother trajectories and lower runtimes than state-of-the-art corridor based planners, especially in highly complicated environments.",
            "introduction": "Collision free trajectory optimization is a core challenge in robotics, useful in mobile robot navigation and mobile manipulation [1, 2, 3]. By mathematically defining sets of feasible trajectories that a system can undergo, and assigning a cost to each element of the set, trajectory optimization can be posed as a mathematical programming problem. Several methods exist to characterize these sets to facilitate efficient searches for optimal feasible trajectories.\n\nConventional sampling based planners [4, 5, 6] build tree or graph like structures in the configuration space, with nodes corresponding to collision free states and edges representing locally feasible motions. Paths through these structures define potential trajectories, which can then be searched efficiently using graph search or shortest path algorithms. On the other hand, some optimization based approaches [7, 8, 9, 10, 11, 12] impose constraints on robot’s states (enforcing that a barrier between collision objects is respected) to define sets of feasible solutions, then minimize a cost function within this nonconvex feasible set. These enable powerful nonconvex optimization solvers for trajectory optimization. In more complicated scenarios, with larger dimensional spaces or with numerous obstacles, the above methods inevitably face the curse of dimensionality.\n\nRecent work has introduced convex approximations of free space through “safe corridors”, using either polytopic or ellipsoidal regions [13, 14, 15, 16]. In these approaches, a trajectory is divided into segments, and each segment is constrained to lie within a corresponding convex subset of the obstacle free space at a specific point in time. By enforcing convex constraints on the robot’s configuration, these methods convert a difficult global planning problem into convex optimization, which can be solved using efficient convex programming techniques [1, 15, 16].\n\nHowever, a key challenge in these methods is allocating time across the convex regions. Each region requires a specified time interval, and if this interval is too short, the system must accelerate sharply to meet continuity constraints, while overly long intervals produce inefficient motions. As the number of convex subsets increases, these timing decisions become increasingly difficult to manage, often introducing sensitivity and limiting the scalability of convex corridor methods.\n\n[17] introduced Differentiable Collision Free Parametric Corridors which model free space as a smooth nonconvex corridor, made of a continuously deforming convex set rather than discrete segments, offering a more unified description of collision free regions. Building on this idea, we view these nonconvex corridors as time-varying projections of orthogonal trust regions in a higher dimensional parameter space. By lifting trajectories into a space where feasible solutions form a cartesian product of high dimensional balls, each point in this lifted space naturally corresponds to a collision free trajectory within the corridor. This decouples problem size from environmental complexity while allowing representations of paths in the safe corridor as points in the lifted space.\n\nThis representation results in a favorable convex feasible set for trajectory optimization. We pose the resulting problem as the Orthogonal Trust Region Problem (Orth-TRP), which can be expressed as a collection of interconnected trust region subproblems (TRPs). Because each block of variables has its own seperable constraint, the Orth-TRP naturally supports a parallelizable algorithm, where each block can be updated efficiently using trust region steps that resemble simple one dimensional line searches.\n\nIn summary, our contributions are:\n\nWe develop a convex representation of sets of trajectories within a nonconvex set of configurations using a product of multiple high dimensional balls, decoupling the solution complexity from the size of the optimization problem.\n\nWe develop a convex representation of sets of trajectories within a nonconvex set of configurations using a product of multiple high dimensional balls, decoupling the solution complexity from the size of the optimization problem.\n\nWe create a solver that exploits both, the parallelizable structure and the unique Trust-Region-like structure of our resulting problem to solve quadratically constrained quadratic optimization problems where constraints are enforced on separable variables.\n\nTaken together, these components create a scalable and geometrically intuitive framework for collision-free trajectory optimization in complex environments. Our experiments show that this approach consistently outperforms state-of-the-art implementations on challenging benchmarks. In direct comparisons with the method of [15], even when using a powerful solver like OSQP [18], our method solves long-horizon planning problems faster while producing comparable or smoother results. To the best of our knowledge, this is the first continuous parameterization of trajectories that enables optimization within a collision-free corridor where problem size and runtime are agnostic to horizon length and solution complexity. We achieve this by combining a new convex programming formulation with a solver that fully exploits the problem’s separable trust-region structure, eliminating the scaling and time-allocation issues inherent to existing corridor-based approaches and enabling both speed and robustness at scale.\n\n1. We develop a convex representation of sets of trajectories within a nonconvex set of configurations using a product of multiple high dimensional balls, decoupling the solution complexity from the size of the optimization problem.\n\n2. We create a solver that exploits both, the parallelizable structure and the unique Trust-Region-like structure of our resulting problem to solve quadratically constrained quadratic optimization problems where constraints are enforced on separable variables.",
            "llm_summary": "【关注的是什么问题】  \n1. 复杂环境中的无碰撞轨迹规划仍然是机器人技术中的核心挑战。  \n2. 现有基于走廊的规划方法在环境复杂性增加时表现不佳，且需要显式的时间分配。  \n3. 如何有效地优化非凸走廊中的轨迹，同时避免时间分配问题。  \n\n【用了什么创新方法】  \n本研究提出了一种新的轨迹参数化方法，通过将轨迹表示为高维球体的笛卡尔积，形成非凸走廊的凸表示。这种方法使得问题规模与解决方案的几何复杂性解耦，并允许轨迹在椭圆走廊内连续演变。我们将其形式化为正交信任区域问题（Orth-TRP），并开发了一种利用该问题并行结构的求解器。实验结果表明，该方法在复杂环境中产生了更平滑的轨迹，并且运行时间低于现有的走廊规划方法，尤其在高复杂度环境中表现优越。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Simultaneous estimation of contact position and tool shape with high-dimensional parameters using force measurements and particle filtering",
            "authors": "Kyo Kutsuzawa,Mitsuhiro Hayashibe",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted to The International Journal of Robotics Research (IJRR)",
            "pdf_link": "https://arxiv.org/pdf/2509.19732",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19732",
            "arxiv_html_link": "https://arxiv.org/html/2509.19732v1",
            "abstract": "Estimating the contact state between a grasped tool and the environment is essential for performing contact tasks such as assembly and object manipulation.\nForce signals are valuable for estimating the contact state, as they can be utilized even when the contact location is obscured by the tool.\nPrevious studies proposed methods for estimating contact positions using force/torque signals; however, most methods require the geometry of the tool surface to be known.\nAlthough several studies have proposed methods that do not require the tool shape, these methods require considerable time for estimation or are limited to tools with low-dimensional shape parameters.\nHere, we propose a method for simultaneously estimating the contact position and tool shape, where the tool shape is represented by a grid, which is high-dimensional (more than 1000 dimensional).\nThe proposed method uses a particle filter in which each particle has individual tool shape parameters, thereby to avoid directly handling a high-dimensional parameter space.\nThe proposed method is evaluated through simulations and experiments using tools with curved shapes on a plane.\nConsequently, the proposed method can estimate the shape of the tool simultaneously with the contact positions, making the contact-position estimation more accurate.",
            "introduction": "When robots manipulate objects or use them as tools, they often need to recognize the contact states between the grasped objects/tools and the environment.\nFor instance, when inserting a key into a keyhole, the robot must know the contact position and conditions of the key and lock.\nIn addition, when cutting the bone-in meat with a knife, the robot must detect where the knife contacts with the bone.\nIn such situations, robots need to detect contact states indirectly because tools usually have no sensors.\nMoreover, because tools hide the contact location, robots must estimate the contact information from force signals instead of vision.\n\nConventional methods for contact-position estimation from force signals require the shape and position of the tools.\nA technique for contact-position estimation (Salisbury, 1984) often requires the shape of the tool surface to be known.\nHowever, shape measurements using cameras generally exhibit large errors in the depth direction and are sensitive to occlusion, reflection, and transparency.\nAlthough there are several methods for contact-position estimation without shape information (Tsuji et al., 2017; Koike et al., 2017), the estimation is slow, and these methods require that contact force constantly fluctuates during estimation.\nAs those drawbacks are unavoidable unless using shape information, it is beneficial to estimate tool shape for estimating the contact position.\nAdditionally, the shape of the object/tool is necessary to assemble tasks and plan a control strategy (von Drigalski et al., 2020).\n\nFor tools made of transparent or reflective materials, it would be helpful to be able to estimate the tool shape from force signals instead of vision.\nRecently, a method that simultaneously estimates the contact position and tool shape from force signals was proposed (Kutsuzawa et al., 2020).\nThis method gradually estimates the contact position and tool shape under uncertainty using an unscented particle filter (UPF) (van der Merwe et al., 2000a, b).\nHowever, it requires the tool shape to be expressed using a small number of parameters.\nIt is practically impossible to apply that method to general shapes because the dimensionality of the tool-shape parameters becomes high, which requires an exponential number of particles for a reliable estimation.\nThere is another method that can detect the contact position while estimating the tool shape of voxels from force measurements (Bimbo et al., 2022), but this method requires the geometry of the environment being static.\n\nHere, we propose a method to estimate the tool shape with a large number of parameters from force signals while simultaneously estimating the contact position, based on the Rao–Blackwellized particle filter (RBPF) (Murphy, 1999) used in SLAM (Murphy, 1999; Grisetti et al., 2005, 2007).\nThe conventional method (Kutsuzawa et al., 2020) is affected by the curse of dimensionality owing to the application of a particle filter to high-dimensional parameters.\nBy contrast, the proposed method avoids this issue by associating shape information with individual particles rather than scattering particles into the shape-parameter space.\nThus, the proposed method enables the tool shape to be expressed using a high-dimensional grid representation (voxels or pixels).\nIn addition, in contrast with Bimbo et al. (2022), the proposed method does not need any assumption of environment geometry.\nTherefore, the proposed method is available even for the contact object in the environment being deformable and movable, e.g., a human touching the tool.\nIn this study, we address the simple case of a curved object on a plane.\nAlthough simple, this setup can demonstrate the effectiveness of the proposed method because the challenge of this study is the high-dimensional shape parameter.\nThe contributions of this study are listed as follows:\n\nThis study proposes a force-signal-based estimation method for tool shapes represented by high-dimensional variables (grid representation) without any assumptions of environment geometry.\n\nWe formulated a probabilistic model for contact-position and tool-shape estimation from force signals.\n\nThis study also proposes a method for updating the tool-shape parameters represented by a grid using the estimated contact position and measurements.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何同时估计接触位置和高维工具形状。  \n2. 现有方法在高维参数空间处理时的效率问题。  \n3. 在缺乏工具几何信息的情况下进行接触状态估计的挑战。  \n\n【用了什么创新方法】  \n提出了一种基于力信号的估计方法，通过粒子滤波（RBPF）同时估计接触位置和工具形状，工具形状用高维网格表示。该方法避免了直接处理高维参数空间的问题，通过将形状信息与个别粒子关联，显著提高了接触位置的估计准确性。实验结果表明，该方法在处理曲面工具时表现出色，能够在不依赖环境几何假设的情况下进行有效估计。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Towards Autonomous Robotic Electrosurgery via Thermal Imaging",
            "authors": "Naveed D. Riaziat,Joseph Chen,Axel Krieger,Jeremy D. Brown",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted for publication in the proceedings of the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)",
            "pdf_link": "https://arxiv.org/pdf/2509.19725",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19725",
            "arxiv_html_link": "https://arxiv.org/html/2509.19725v1",
            "abstract": "Electrosurgery is a surgical technique that can improve tissue cutting by reducing cutting force and bleeding. However, electrosurgery adds a risk of thermal injury to surrounding tissue. Expert surgeons estimate desirable cutting velocities based on experience but have no quantifiable reference to indicate if a particular velocity is optimal. Furthermore, prior demonstrations of autonomous electrosurgery have primarily used constant tool velocity, which is not robust to changes in electrosurgical tissue characteristics, power settings, or tool type. Thermal imaging feedback provides information that can be used to reduce thermal injury while balancing cutting force by controlling tool velocity. We introduce Thermography for Electrosurgical Rate Modulation via Optimization (ThERMO) to autonomously reduce thermal injury while balancing cutting force by intelligently controlling tool velocity. We demonstrate ThERMO in tissue phantoms and compare its performance to the constant velocity approach. Overall, ThERMO improves cut success rate by a factor of three and can reduce peak cutting force by a factor of two. ThERMO responds to varying environmental disturbances, reduces damage to tissue, and completes cutting tasks that would otherwise result in catastrophic failure for the constant velocity approach.",
            "introduction": "Electrosurgery is a surgical technique for cutting tissue using energy from a high-frequency voltage source. Eight in ten surgical procedures use electrosurgery, often to remove diseased tissue [1]. Monopolar electrosurgery uses a grounding pad to dissipate current through the body from a tool, directly heating the local tissue. However, electrosurgery can significantly damage nearby healthy tissue as well. Excess thermal damage can adversely affect surgery outcomes. Surgeons try to avoid generating thermal damage by modulating cut speed while ensuring cut accuracy.\n\nWhen using electrosurgery to remove tissue, surgeons aim to spare as much healthy tissue as possible. Since signs of excess thermal energy are often not obvious until irreversible damage has been done, surgeons minimize dwell time by moving faster, as fast motions reduce the heat deposited in a specific area. However, fast motion comes at the cost of decreased cut accuracy and increased cutting force, which can also damage tissue. The ideal electrosurgical cut technique creates a small denatured tissue zone ahead of the tool tip that reduces mechanical stiffness and, thus, the required cut force. However, as the tool speed increases, this denatured tissue zone becomes smaller and eventually disappears, allowing tissue to accumulate on the tool tip and increasing the force needed to cut. This results in increased tissue damage, bulk tissue deformation, and lowered cut quality. Therefore, the optimal cutting velocity should maintain a balanced speed to reduce thermal damage and minimize excess force on the tissue.\n\nWhile there have been no prior demonstrations of techniques for optimizing cut velocity in electrosurgery, there have been recent investigations into sensor-based automation for controlling thermal damage. Bao and Mazumder showed that thermal imaging can measure the denaturation zone and control it to a specific size using a novel computer-controllable electro electrosurgical unit (ESU) [2]. This method, however, contrasts with the fixed power level typically used by surgeons. El-Kebir et al. used thermal sensing and data-driven models to control the thermal damage boundary along a cut by pausing at discrete decision points [3]. Unfortunately, neither approach simultaneously considers cut deformation and thermal damage to optimize the cut velocity.\n\nThe same velocity modulation problem exists in autonomous robotic electrosurgery, which has had recent success in medical robotics research. While these autonomous approaches promise improved cut accuracy and decreased surgeon error, they have largely avoided real-time velocity optimization. Opfermann et al.  [4] demonstrated a visual servoing approach for electrosurgical cutting on the Smart Tissue Autonomous Robot (STAR) [5]. Saeidi et al. similarly showed that STAR could perform tissue-cutting tasks using predetermined cut depth, power, and speed [6]. Ge et al. demonstrated tumor detection and excision using a suction gripper [7]. Each approach relies on a predetermined cut velocity, chosen based on clinical observation or simulation. Researchers have generally opted to move slowly to avoid excess tissue deformation. However, these approaches fail to account for the excess heat damage caused by slow cutting velocities.\n\nHere, we introduce ThERMO. ThERMO uses thermal imaging to determine the optimal cut velocity by 1) dually identifying thermal and mechanical parameters online via a “Truncated Unscented Kalman Filter” (TUKF) and 2) minimizing a parametrized cost function of denaturation width and cutting force. Thermal measurements inform the adaptation of thermal and mechanical parameters, which are applied to generic thermal and mechanical models to maximize cut accuracy and minimize thermal damage with respect to cut velocity. Collectively, we contribute first steps towards 1) infrared (IR)-camera-based tissue denaturation and force measurement method with accompanying validation, 2) a combined adaptive identification and optimization-based approach to control velocity based on thermal spread and cutting force, and 3) rigorous comparisons to the current cutting-edge autonomous electrosurgery approach.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在电外科手术中优化切割速度以减少热损伤？  \n2. 现有电外科技术未能实时优化切割速度，导致组织损伤和切割质量下降。  \n3. 传统的电外科方法依赖于固定的切割速度，缺乏对环境变化的适应性。  \n\n【用了什么创新方法】  \n本研究提出了“热成像电外科速率调制优化”（ThERMO）方法，通过热成像反馈实时调整工具速度，以减少热损伤并平衡切割力。ThERMO使用“截断无味卡尔曼滤波器”（TUKF）在线识别热和机械参数，并最小化与组织变性宽度和切割力相关的参数化成本函数。实验结果表明，ThERMO在组织模型中将切割成功率提高了三倍，并将峰值切割力降低了一半，能够有效应对环境干扰，减少组织损伤，完成传统固定速度方法无法实现的切割任务。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies",
            "authors": "Liquan Wang,Jiangjie Bian,Eric Heiden,Animesh Garg",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19712",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19712",
            "arxiv_html_link": "https://arxiv.org/html/2509.19712v1",
            "abstract": "Robotic manipulation tasks involving cutting deformable objects remain challenging due to complex topological behaviors, difficulties in perceiving dense object states, and the lack of efficient evaluation methods for cutting outcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for multi-step robotic cutting tasks, that integrates cutting environment and generalized policy learning. TopoCut is built upon three core components: (1) We introduce a high-fidelity simulation environment based on a particle-based elastoplastic solver with compliant von Mises constitutive models, augmented by a novel damage-driven topology discovery mechanism that enables accurate tracking of multiple cutting pieces.\n(2) We develop a comprehensive reward design that integrates the topology discovery with a pose-invariant spectral reward model based on Laplace–Beltrami eigenanalysis, facilitating consistent and robust assessment of cutting quality. (3) We propose an integrated policy learning pipeline, where a dynamics-informed perception module predicts topological evolution and produces particle-wise, topology-aware embeddings to support PDDP—Particle-based Score-Entropy Discrete Diffusion Policy—for goal-conditioned policy learning.\nExtensive experiments demonstrate that TopoCut supports trajectory generation, scalable learning, precise evaluation, and strong generalization across diverse object geometries, scales, poses, and cutting goals. Project page: https://topocut.github.io/.",
            "introduction": "Robotic manipulation involving the cutting of deformable objects plays a critical role across diverse domains such as food processing, medical surgery, and manufacturing. Many real-world tasks require not just a single cut, but a sequence of cutting actions to segment objects into complex or structured shapes. From slicing ingredients into uniform pieces in culinary automation, to performing multi-incision procedures in robotic surgery, and executing multi-pass segmentation in industrial workflows, multi-step cutting is essential for achieving fine-grained precision. The ability to reliably plan and execute these sequential cutting operations significantly enhances efficiency, safety, and quality in autonomous systems.\n\nDespite recent progress in robotic cutting of single-material deformable objects with fixed trajectories [1, 2, 3], goal-conditioned multi-step cutting of complex deformable geometries remains a major challenge. Deformable objects often fail to separate cleanly after each cut, making outcome evaluation ambiguous [1]. Existing evaluation metrics are sensitive to pose variations and typically require explicit alignment [3]. Furthermore, dense topological changes resulting from sequential cuts are difficult to perceive from sparse or noisy observations [4, 3], hindering the effectiveness of policy learning in such settings.\n\nTo address these challenges, we introduce TopoCut, a unified framework for multi-step robotic cutting that combines high-fidelity simulation, robust evaluation, and goal-conditioned policy learning. At its core, TopoCut features a particle-based elastoplastic simulator equipped with a novel damage-driven topology discovery mechanism that enables precise tracking of multiple cutting-induced topological changes. We further design a pose-invariant spectral reward based on Laplace–Beltrami eigenanalysis to evaluate cutting outcomes consistently across varying object geometries and poses. Finally, we propose a learning pipeline that leverages a dynamics-informed perception module to produce topology-aware, particle-wise embeddings—explicitly designed to operate on sparse visual input, making it suitable for real-world robotic settings—and supports PDDP, a discrete diffusion policy model for scalable and generalizable multi-step cutting.\n\nOur contributions are organized into three core components:\n\nHigh-fidelity Simulation and Topology Discovery: We develop a robust simulation environment utilizing a novel particle-based elastoplastic solver with compliant von Mises constitutive models, coupled with an advanced particle-based topology discovery method to precisely capture and track topological changes during cutting.\n\nPose-invariant Spectral Reward: We introduce a novel reward formulation integrating the real-time topology discovery with a spectral reward function based on Laplace–Beltrami eigenanalysis, enabling consistent, pose-invariant evaluation of cutting outcomes across arbitrary object poses.\n\nDynamics-informed Policy Learning: We propose a goal-conditioned policy learning framework that employs dynamics-informed perception modules to predict topology evolution and generate particle-level, topology-aware embeddings. These embeddings support conditional score-based discrete diffusion models, enhancing the robustness and generalizability of the learned cutting strategies.\n\n1. High-fidelity Simulation and Topology Discovery: We develop a robust simulation environment utilizing a novel particle-based elastoplastic solver with compliant von Mises constitutive models, coupled with an advanced particle-based topology discovery method to precisely capture and track topological changes during cutting.\n\n2. Pose-invariant Spectral Reward: We introduce a novel reward formulation integrating the real-time topology discovery with a spectral reward function based on Laplace–Beltrami eigenanalysis, enabling consistent, pose-invariant evaluation of cutting outcomes across arbitrary object poses.\n\n3. Dynamics-informed Policy Learning: We propose a goal-conditioned policy learning framework that employs dynamics-informed perception modules to predict topology evolution and generate particle-level, topology-aware embeddings. These embeddings support conditional score-based discrete diffusion models, enhancing the robustness and generalizability of the learned cutting strategies.",
            "llm_summary": "【关注的是什么问题】  \n1. 多步切割任务中的复杂拓扑行为如何有效处理。  \n2. 如何在稀疏观测下准确评估切割结果。  \n3. 现有的切割策略学习方法在复杂可变形几何体上的局限性。  \n\n【用了什么创新方法】  \nTopoCut框架结合了高保真模拟、稳健评估和目标条件策略学习。首先，开发了一种基于粒子弹塑性求解器的模拟环境，配合损伤驱动的拓扑发现机制，精确跟踪切割引起的拓扑变化。其次，设计了一种基于拉普拉斯-贝尔特拉米特征值分析的姿态不变谱奖励，确保在不同物体几何和姿态下的一致评估。最后，提出了一种动态感知模块，生成粒子级拓扑感知嵌入，支持条件分数基础离散扩散策略（PDDP），增强了切割策略的稳健性和泛化能力。实验结果表明，TopoCut在轨迹生成、可扩展学习、精确评估和强泛化能力方面表现出色。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks",
            "authors": "Noah Geiger,Tamim Asfour,Neville Hogan,Johannes Lachner",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19696",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19696",
            "arxiv_html_link": "https://arxiv.org/html/2509.19696v1",
            "abstract": "Learning methods excel at motion generation in the information domain but are not primarily designed for physical interaction in the energy domain. Impedance Control shapes physical interaction but requires task-aware tuning by selecting feasible impedance parameters. We present Diffusion-Based Impedance Learning, a framework that combines both domains. A Transformer-based Diffusion Model with cross-attention to external wrenches reconstructs a simulated Zero-Force Trajectory (sZFT). This captures both translational and rotational task-space behavior. For rotations, we introduce a novel SLERP-based quaternion noise scheduler that ensures geometric consistency. The reconstructed sZFT is then passed to an energy-based estimator that updates stiffness and damping parameters. A directional rule is applied that reduces impedance along non-task axes while preserving rigidity along task directions.\nTraining data were collected for a parkour scenario and robotic-assisted therapy tasks using teleoperation with Apple Vision Pro. With only tens of thousands of samples, the model achieved sub-millimeter positional accuracy and sub-degree rotational accuracy. Its compact model size enabled real-time torque control and autonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller achieved smooth parkour traversal within force and velocity limits and 30/30 success rates for cylindrical, square, and star peg insertions without any peg-specific demonstrations in the training data set.\nAll code for the Transformer-based Diffusion Model, the robot controller, and the Apple Vision Pro telemanipulation framework is publicly available. These results mark an important step towards Physical AI, fusing model-based control for physical interaction with learning-based methods for trajectory generation.",
            "introduction": "Robotic behavior emerges at the interface of two fundamentally different domains. Motion planning belongs to the information domain, where learning-based methods have recently shown remarkable progress. In contrast, physical interaction is governed by the energy domain, and model-based Impedance Control has been widely adopted to guarantee stability [1] and safety [2]. Robots that manipulate in unstructured environments must bridge these domains: motions must be inferred as information, yet executed through stable, energy-consistent exchange with the environment [3]. This requirement is critical in assembly, rehabilitation, and other contact-rich tasks where visual feedback is limited and success depends on regulating interaction rather than simply following a path.\n\nWhile Impedance Control provides the framework for shaping robot–environment interaction [4], its performance critically depends on selecting appropriate stiffness and damping parameters (which may need to be adapted online) [5]. Too much stiffness can lead to jamming, too little can prevent task execution.\n\nRecent advances in contact-rich manipulation can be divided into two main strands. Model-based approaches incorporate explicit contact dynamics like friction cones, complementarity constraints, and mixed-integer formulations [6, 7, 8, 9, 10, 11, 12, 13, 14]. These methods produce behavior consistent with physical contact but face challenges: high computational cost, parameter sensitivity, and limited robustness in unstructured settings. Learning-based approaches, in contrast, operate in the information domain. Reinforcement learning and sampling-based optimization [15, 16, 17, 18], along with more recent generative models such as diffusion policies [19], flow-matching policies [20, 21], and Transformer-based policy models [22, 23], excel at motion generation across tasks and embodiments [24]. However, execution in these frameworks often relies on velocity-based or fixed-gain PD controllers. While fixed-gain PD controllers can avoid hardware damage, apparent compliance is only a byproduct of low control gains. In unstructured environments, where vision may be unreliable or occluded, explicit impedance regulation is essential to control interaction.\n\nThe Adaptive Compliance Policy of [25] is a first step toward combining generative models with compliance adaptation, but it is formulated within an admittance-control framework. Admittance control is known to struggle with transitions into and out of contact [4, 26], making it unsuitable for many assembly tasks such as peg-in-hole insertion. To our knowledge, no approach has successfully combined learning-based trajectory generation (information domain) with impedance adaptation (energy domain).\n\nThis paper addresses that gap. We present Diffusion-Based Impedance Learning, a framework that unifies generative modeling with energy-consistent Impedance Control. A conditional Transformer-based Diffusion Model reconstructs simulated Zero-Force Trajectories111The Zero-Force Trajectory (ZFT), introduced by Hogan [5], refers to the commanded equilibrium motion in the unconstrained case: the unique end-effector pose at which the interaction wrench vanishes. from contact-perturbed displacement and external wrench signals. The sZFT serves as a reconstructed equilibrium used to modulate task-space impedance. A directional adaptation scheme adapts stiffness primarily along non-task-relevant directions and preserves rigidity where needed for execution. In this way, our approach combines the strengths of learning-based methods (motion generation in the information domain) with those of model-based methods (impedance regulation in the energy domain).\n\nWe validate the framework on a KUKA LBR iiwa in two contact-rich scenarios: parkour-style obstacle traversal and multi-geometry peg-in-hole insertion. Training data were collected through telemanipulation, using Apple Vision Pro (AVP) equipped with a markerless pose tracking framework [27] integrated into the robot controller. Both tasks highlight the shortcomings of fixed impedance and simple adaptation schemes, which either jam at obstacles or fail to achieve insertion. In contrast, Diffusion-Based Impedance Learning achieves consistent success. The outcomes of Table I preview the central result of this work: bridging the information and energy domains is key to robust manipulation in unstructured environments.\n\nAll code for the Transformer-based Diffusion Model, the robot controller,\nand the Apple Vision Pro telemanipulation framework is available on our\nGitHub repository222https://github.com/StrokeAIRobotics/DiffusionBasedImpedanceLearning. Demonstration videos of all\nexperiments can be found on the project website333https://strokeairobotics.github.io/DiffusionBasedImpedanceLearning.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效结合学习方法与阻抗控制以实现物理交互？  \n2. 如何在接触丰富的任务中实现稳定的运动生成和阻抗调节？  \n3. 如何在不确定环境中提高机器人操作的成功率和鲁棒性？  \n\n【用了什么创新方法】  \n提出了一种Diffusion-Based Impedance Learning框架，结合了生成建模与能量一致的阻抗控制。通过条件Transformer-based Diffusion Model重建模拟的零力轨迹（sZFT），并采用方向适应方案调节任务空间的阻抗。该方法在KUKA LBR iiwa机器人上进行验证，展示了在公园风格障碍物穿越和多几何体插入任务中的成功率，达到了亚毫米的位置精度和亚度的旋转精度。结果表明，信息域与能量域的结合是实现不确定环境中稳健操作的关键。\n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Formal Safety Verification and Refinement for Generative Motion Planners via Certified Local Stabilization",
            "authors": "Devesh Nath,Haoran Yin,Glen Chou",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG); Systems and Control (eess.SY); Optimization and Control (math.OC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19688",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19688",
            "arxiv_html_link": "https://arxiv.org/html/2509.19688v1",
            "abstract": "We present a method for formal safety verification of learning-based generative motion planners. Generative motion planners (GMPs) offer advantages over traditional planners, but verifying the safety and dynamic feasibility of their outputs is difficult since neural network verification (NNV) tools scale only to a few hundred neurons, while GMPs often contain millions. To preserve GMP expressiveness while enabling verification, our key insight is to imitate the GMP by stabilizing references sampled from the GMP with a small neural tracking controller and then applying NNV to the closed-loop dynamics. This yields reachable sets that rigorously certify closed-loop safety, while the controller enforces dynamic feasibility. Building on this, we construct a library of verified GMP references and deploy them online in a way that imitates the original GMP distribution whenever it is safe to do so, improving safety without retraining. We evaluate across diverse planners, including diffusion, flow matching, and vision-language models, improving safety in simulation (on ground robots and quadcopters) and on hardware (differential-drive robot).",
            "introduction": "Motion planning has been transformed by generative models like diffusion and conditional flow matching (CFM) [1, 2], which learn multimodal trajectory distributions and enable generative motion planners (GMPs) that produce diverse plans from inputs like language or images [3, 4, 5, 6].\nHowever, ensuring that GMP-generated trajectories satisfy safety and dynamic feasibility is difficult: GMPs often contain millions of parameters, making neural network verification (NNV) [7] intractable, limiting their use in safety-critical settings [6]. NNV provides hard guarantees via set-based reachability but only scales to controllers with a few hundred neurons [8, 9, 10, 11]. More scalable statistical methods [12, 13, 3, 14] yield weaker probabilistic guarantees or require prohibitive samples over long horizons. Thus, existing work trades off between expressive large models lacking hard guarantees and small verifiable models unable to capture complex behaviors.\n\nTo bridge this gap, we propose SaGe-MP (Safe Generative Motion Planning), a method that provides hard safety and dynamic feasibility guarantees for GMP-generated motion plans. Our key insight is that while NNV tools cannot directly certify the GMP, they can certify a small neural tracking controller that locally stabilizes the system around GMP-sampled references.\nReachability analysis of the resulting closed-loop system yields hard assurances of safety and dynamic feasibility over a continuum of inputs. Here, the GMP acts only as an open-loop plan generator, while verification is performed on the closed-loop dynamics induced by tracking a fixed GMP plan, resulting in a smaller computational graph that makes NNV tractable. By tracking GMP references under the true dynamics, the controller also projects potentially dynamically-infeasible GMP plans onto feasible trajectories. To preserve the original GMP behavior if possible, we develop a trajectory-library approach: multiple GMP references are sampled offline, certified as safely trackable via NNV, and deployed online in a way that mimics the potentially multimodal GMP output. In this sense, our method is a lightweight GMP refinement that enhances safety and dynamic feasibility without costly GMP retraining. Our contributions are:\n\nAn NNV-based method for formal safety verification of large GMPs that decouples trajectory generation from neural feedback loop verification, preserving planner expressiveness while providing hard safety guarantees.\n\nAn NNV-based method for formal safety verification of large GMPs that decouples trajectory generation from neural feedback loop verification, preserving planner expressiveness while providing hard safety guarantees.\n\nA method to certify a neural trajectory tracking controller that stabilizes dynamically-infeasible GMP references, producing safe, feasible trajectories.\n\nA trajectory-library method that stores certified-safe GMP plans and executes them online in a way that preserves the original behavior whenever safe. We prove sample complexity bounds for a target imitation error.\n\nExtensive simulation and real-world validation demonstrating safe stabilization of references from diverse generative models (diffusion, flow matching, VLMs, neural ODEs) on challenging nonlinear systems (e.g., 12D quadcopter, and learned NN dynamics).",
            "llm_summary": "【关注的是什么问题】  \n1. 如何确保生成运动规划器（GMP）生成的轨迹满足安全性和动态可行性？  \n2. 如何在不重新训练GMP的情况下提高其安全性？  \n\n【用了什么创新方法】  \n提出了一种名为SaGe-MP的方法，通过使用小型神经跟踪控制器对GMP生成的参考轨迹进行局部稳定化，从而实现对GMP的形式安全验证。该方法将轨迹生成与神经反馈回路验证解耦，允许使用神经网络验证（NNV）工具对闭环动态进行可行性和安全性分析。通过构建一个经过验证的轨迹库，能够在安全时在线执行这些轨迹，保持GMP的原始行为。该方法在多种生成模型上进行了广泛的仿真和实际验证，展示了在复杂非线性系统（如12维四旋翼和学习的神经网络动态）上的安全稳定化效果。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains",
            "authors": "Dongzhe Zheng,Wenjie Mei",
            "subjects": "Robotics (cs.RO); Dynamical Systems (math.DS)",
            "comment": "Accepted by NeurIPS 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.19672",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19672",
            "arxiv_html_link": "https://arxiv.org/html/2509.19672v1",
            "abstract": "Stochastic optimal control methods often struggle in complex non-convex landscapes, frequently becoming trapped in local optima due to their inability to learn from historical trajectory data. This paper introduces Memory-Augmented Potential Field Theory, a unified mathematical framework that integrates historical experience into stochastic optimal control. Our approach dynamically constructs memory-based potential fields that identify and encode key topological features of the state space, enabling controllers to automatically learn from past experiences and adapt their optimization strategy. We provide a theoretical analysis showing that memory-augmented potential fields possess non-convex escape properties, asymptotic convergence characteristics, and computational efficiency. We implement this theoretical framework in a Memory-Augmented Model Predictive Path Integral (MPPI) controller that demonstrates significantly improved performance in challenging non-convex environments. The framework represents a generalizable approach to experience-based learning within control systems (especially robotic dynamics), enhancing their ability to navigate complex state spaces without requiring specialized domain knowledge or extensive offline training.",
            "introduction": "Stochastic optimal control has proven highly effective for handling nonlinear systems and uncertain environments, finding widespread application in robotics, reinforcement learning, and complex system control. Among these approaches, Model Predictive Path Integral (MPPI) control stands out for its ability to handle continuous state-action spaces through stochastic sampling and exponentially weighted averaging. However, these methods still face significant theoretical and practical challenges when confronting highly non-convex value function landscapes.\n\nFrom an optimization perspective, stochastic optimal control problems can be viewed as trajectory optimization over a value function landscape. When this landscape exhibits complex non-convex characteristics, optimization processes may become trapped in local optima, unable to reach global solutions. While introducing noise sampling (as in MPPI’s random perturbations) can somewhat mitigate this issue, significantly non-convex features often lead to inefficient sampling or control instability when noise is simply increased.\n\nFrom a dynamical systems perspective, non-convex value functions correspond to systems with multiple attractors and unstable equilibrium points. Control algorithms need to identify these features and, when necessary, guide the system across energy barriers to escape suboptimal attractor regions. Traditional stochastic control methods have limited capabilities in this regard, as they lack awareness and memory of the state space’s topological structure.\n\nTraditional stochastic optimal controllers lack memory—operating solely on current states without learning from past trajectories. This design means controllers might repeatedly fall into the same suboptimal regions, failing to extract experience from previous \"failures.\" In contrast, advanced cognitive systems (like humans) dynamically adjust decision strategies based on prior experience when exploring complex environments.\n\nThis paper addresses a fundamental question: How can we integrate \"memory\" mechanisms into stochastic optimal control frameworks, enabling controllers to automatically learn state space topological features from historical trajectories and adjust optimization strategies accordingly? We introduce Memory-Augmented Potential Field Theory, integrating historical state experience into stochastic optimal control through dynamic potential fields that automatically identify and encode topological features of the state space during execution. These fields act as correction terms to reshape the value function landscape, enabling adaptive navigation of non-convex optimization problems. Our framework provides: 1) automatic detection and encoding of problematic regions like local minima and low-gradient areas, 2) dynamic reshaping of value functions for efficient escape from suboptimal attractors, 3) theoretical guarantees for convergence to global optima, and 4) significant performance improvements in complex control tasks without requiring extensive offline training.\n\nOur approach uniquely integrates memory mechanisms with dynamical systems theory and stochastic optimal control, analyzing memory’s impact on non-convex optimization topologically. Beyond simply storing experiences, our method automatically identifies key state space features and dynamically reshapes value function landscapes, enabling \"meta-optimization\" capabilities. The code has been anonymized and is available at https://anonymous.4open.science/r/MA_MPPI-6555.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将“记忆”机制整合到随机最优控制框架中，以便自动学习状态空间的拓扑特征？  \n2. 如何在复杂的非凸环境中提高控制器的优化策略和性能？  \n\n【用了什么创新方法】  \n本研究提出了记忆增强潜力场理论，通过动态构建基于记忆的潜力场，自动识别和编码状态空间的关键拓扑特征。该方法允许控制器从历史轨迹中学习，调整优化策略。理论分析表明，记忆增强潜力场具有非凸逃逸特性和渐近收敛性。通过实现记忆增强模型预测路径积分（MPPI）控制器，在复杂非凸环境中显著提高了性能，展示了经验学习在控制系统中的广泛适用性。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "RoboSSM: Scalable In-context Imitation Learning via State-Space Models",
            "authors": "Youngju Yoo,Jiaheng Hu,Yifeng Zhu,Bo Liu,Qiang Liu,Roberto Martín-Martín,Peter Stone",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19658",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19658",
            "arxiv_html_link": "https://arxiv.org/html/2509.19658v1",
            "abstract": "In-context imitation learning (ICIL) enables robots to learn tasks from prompts consisting of just a handful of demonstrations. By eliminating the need for parameter updates at deployment time, this paradigm supports few-shot adaptation to novel tasks.\nHowever, recent ICIL methods rely on Transformers, which have computational limitations and tend to underperform when handling longer prompts than those seen during training.\nIn this work, we introduce RoboSSM, a scalable recipe for in-context imitation learning based on state-space models (SSM).\nSpecifically, RoboSSM replaces Transformers with Longhorn – a state-of-the-art SSM that provides linear-time inference and strong extrapolation capabilities, making it well-suited for long-context prompts.\nWe evaluate our approach on the LIBERO benchmark and compare it against strong Transformer-based ICIL baselines.\nExperiments show that RoboSSM extrapolates effectively to varying numbers of in-context demonstrations, yields high performance on unseen tasks, and remains robust in long-horizon scenarios.\nThese results highlight the potential of SSMs as an efficient and scalable backbone for ICIL.\nOur code is available at https://github.com/youngjuY/RoboSSM.",
            "introduction": "Imitation Learning (IL) is a powerful framework that enables robots to learn behaviors from demonstrations without explicit programming or reward design [1, 2].\nWhile IL has achieved notable success in manipulation and navigation tasks, a key limitation of conventional imitation learning lies in its restricted adaptation capability, particularly when faced with new tasks.\nEven with models trained on large multi-task datasets [3, 4, 5, 6], adapting to novel tasks still requires collecting a large amount of task-specific data and retraining, which can be computationally costly and often unstable [7, 8].\nTo address this challenge, In-Context Imitation Learning (ICIL) introduces a new paradigm, inspired by the success of large language models (LLMs) [9, 10, 11] in adapting to unseen language tasks through few-shot learning [12].\nICIL integrates the concept of prompting into imitation learning [13, 14, 15, 16, 12, 17, 18, 19], allowing the model to infer and perform tasks based on a prompt composed of demonstrations, with no post-demonstration training.\n\nGiven that ICIL formulates imitation learning as a sequence modeling problem, recent ICIL approaches have naturally adopted Transformer-based models as their primary architecture [17, 19, 18].\nAlthough Transformers are the dominant architecture for sequence modeling [20], their time complexity scales quadratically with sequence length, and they struggle to extrapolate beyond training lengths [21, 22].\nFor ICIL to handle long prompts efficiently at test time, it is essential to adopt alternatives to Transformers that enhance scalability with input length.\n\nIn this paper, we introduce RoboSSM, a scalable in-context learning framework that replaces Transformers with state-space models (SSMs).\nSpecifically, RoboSSM utilizes Longhorn [23], a state-of-the-art SSM with linear inference time and strong extrapolation capability for long-context sequences.\nLeveraging these properties, RoboSSM can process substantially longer prompts at test time compared to previous Transformer-based ICIL methods.\nWe also investigate adapting Longhorn to ICIL via β\\beta-scaling ablations, which encourage the model to attend to demonstration prompts.\n\nOn the LIBERO [24] benchmark, RoboSSM uniquely benefits from using more in-context examples, maintaining high success rates on unseen tasks when trained with only a few demonstrations.\nFor instance, on the task pick up the plate and place it in the tray, where the plate object was unseen during training, RoboSSM achieves its highest performance when prompted with 32 demonstrations, despite being trained on only two.\nFurthermore, our framework performs well on unseen long-horizon tasks, which we simulate by repeating frames in the demonstrations to create time-dilated scenarios.\nConsequently, RoboSSM handles test-time demonstration prompts up to 16 times longer than those seen in training while maintaining linear inference time, whereas Transformer-based ICIL methods sharply degrade once the test prompt exceeds the training length.\nThese findings confirm that RoboSSM establishes a scalable in-context imitation learning framework by effectively leveraging long-range contextual information.\nOur project video is available at RoboSSM-video.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高机器人在新任务上的适应能力，特别是在少量示范的情况下。  \n2. 传统的Transformer模型在处理长上下文提示时的计算限制和性能不足。  \n3. 如何在不进行参数更新的情况下实现高效的在-context模仿学习。  \n\n【用了什么创新方法】  \nRoboSSM引入了一种基于状态空间模型（SSM）的可扩展在-context模仿学习框架，替代了计算复杂度高的Transformer。使用Longhorn SSM，RoboSSM实现了线性推理时间和强大的外推能力，能够有效处理长上下文提示。实验表明，RoboSSM在LIBERO基准测试中表现优异，能够在仅用少量示范的情况下，成功适应未见过的任务，并在长时间范围任务中保持高性能。具体而言，在未见物体的任务中，RoboSSM在使用32个示范时表现最佳，且能够处理比训练时长16倍的测试提示。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Minimalistic Autonomous Stack for High-Speed Time-Trial Racing",
            "authors": "Mahmoud Ali,Hassan Jardali,Youwei Yu,Durgakant Pushp,Lantao Liu",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "The data associated with this paper is available atthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.19636",
            "code": "https://doi.org/10.5281/zenodo.17187680",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19636",
            "arxiv_html_link": "https://arxiv.org/html/2509.19636v1",
            "abstract": "Autonomous racing has seen significant advancements, driven by competitions such as the Indy Autonomous Challenge (IAC) and the Abu Dhabi Autonomous Racing League (A2RL). However, developing an autonomous racing stack for a full-scale car is often constrained by limited access to dedicated test tracks, restricting opportunities for real-world validation.\nWhile previous work typically requires extended development cycles and significant track time, this paper introduces a minimalistic autonomous racing stack for high-speed time-trial racing that emphasizes rapid deployment and efficient system integration with minimal on-track testing.\nThe proposed stack was validated on real speedways, achieving a top speed of 206 km h−1206\\text{\\,}\\mathrm{km}\\text{\\,}{\\mathrm{h}}^{-1} within just 11 hours’ practice run on the track with 325 km325\\text{\\,}\\mathrm{km} in total. Additionally, we present the system performance analysis, including tracking accuracy, vehicle dynamics, and safety considerations, offering insights for teams seeking to rapidly develop and deploy an autonomous racing stack with limited track access.",
            "introduction": "Autonomous racing has gained significant traction in recent years, advancing both research and real-world deployment in high-speed autonomy. Competitions such as\nIAC [1] and A2RL [2] provide a platform for testing cutting-edge autonomous systems in extreme conditions. These events have driven advancements in perception, planning, and control algorithms [3, 4, 5, 6], leading to fully autonomous race cars competing at 290 km h−1290\\text{\\,}\\mathrm{km}\\text{\\,}{\\mathrm{h}}^{-1} [7].\nDespite these achievements, developing an autonomous racing stack for full-scale vehicles remains a resource-intensive endeavor due to the limited availability of dedicated racetracks for testing. Prior work has demonstrated impressive results but often relies on years of development and extensive track-testing time, making rapid deployment difficult for new teams.\n\nTo address this challenge, we present a minimalistic autonomous racing stack designed for high-speed time-trial racing with a focus on single-car speed performance and rapid deployment.\nOur approach strategically maximizes track time utilization, enabling a fully functional autonomy stack with minimal on-track testing.\nThe proposed system was implemented on the IAC AV-24 race car [8] and validated on real speedways, achieving a top speed of 206 km h−1206\\text{\\,}\\mathrm{km}\\text{\\,}{\\mathrm{h}}^{-1} with only 11 track hours and 325 km325\\text{\\,}\\mathrm{km} of practice runs.\nThe key contributions of this work include:\nI) A minimalistic autonomous racing stack tailored for high-speed solo racing.\nII) A system integration of the proposed stack into the AV-2424 racing car.\nIII) An evaluation of the proposed stack\nwith emphasis on controllers performance and dynamics analysis.\nIV) A discussion of the safety measures incorporated into the system, along with an analysis of failure cases.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在有限的赛道测试条件下快速开发和部署自主赛车系统？  \n2. 如何提高自主赛车在高速度下的性能和安全性？  \n\n【用了什么创新方法】  \n本研究提出了一种简约的自主赛车堆栈，专注于高速度单车时间试验，旨在通过最大化赛道时间利用率来实现快速部署。该系统在真实赛道上进行了验证，仅用11小时的练习运行便达到了206 km/h的最高速度，总计325 km的测试里程。研究还分析了系统性能，包括跟踪精度、车辆动态和安全性，为寻求快速开发和部署自主赛车的团队提供了宝贵的见解。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data",
            "authors": "Ryan Punamiya,Dhruv Patel,Patcharapong Aphiwetsa,Pranav Kuppili,Lawrence Y. Zhu,Simar Kareer,Judy Hoffman,Danfei Xu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "Accepted at 39th Conference on Neural Information Processing Systems (NeurIPS 2025) and Oral at Conference on Robot Learning (CoRL 2025)",
            "pdf_link": "https://arxiv.org/pdf/2509.19626",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19626",
            "arxiv_html_link": "https://arxiv.org/html/2509.19626v1",
            "abstract": "Egocentric human experience data presents a vast resource for scaling up end-to-end imitation learning for robotic manipulation. However, significant domain gaps in visual appearance, sensor modalities, and kinematics between human and robot impede knowledge transfer. This paper presents EgoBridge, a unified co-training framework that explicitly aligns the policy latent spaces between human and robot data using domain adaptation. Through a measure of discrepancy on the joint policy latent features and actions based on Optimal Transport (OT), we learn observation representations that not only align between the human and robot domain but also preserve the action-relevant information critical for policy learning.\nEgoBridge achieves a significant absolute policy success rate improvement by 44% over human-augmented cross-embodiment baselines in three real-world single-arm and bimanual manipulation tasks. EgoBridge also generalizes to new objects, scenes, and tasks seen only in human data, where baselines fail entirely. Videos and additional information can be found at https://ego-bridge.github.io/",
            "introduction": "Supervised imitation learning methods such as behavior cloning have emerged as a promising path to scaling robot performance across diverse objects, tasks, and environments. However, while large-scale models in vision and language have achieved remarkable generalization through Internet-sourced data, replicating this success in robotics remains challenging due to the labor-intensive nature of collecting teleoperated demonstrations. Deploying physical robots to many new environments to collect data with enough coverage and diversity is economically and practically intractable.\n\nIn this work, we aim to enable robots to learn from egocentric recordings of natural human behavior, collected by increasingly ubiquitous wearable devices (e.g., XR devices and smart glasses). Without a robot in the loop, such data is cheap and scalable to collect and captures natural human interactions with the world. More importantly, it reflects the embodied human experience, as it contains both observations (e.g., egocentric RGB images) and actions (e.g., hand motions). Unlike unstructured data sources such as Internet videos, the rich embodied information allows us to treat human data and robot data as equal parts in a continuous spectrum of demonstration data and potentially learn from both with a unified learning framework.\n\nHowever, the multitudes of domain gaps between human and robot pose significant challenges in designing such a framework. Human bodies and robots have different visual appearances. Even within a shared action space, kinematic differences can lead to behavior distribution shifts. Robots also have additional sensing modalities such as wrist cameras that are often missing from embodied human data. While recent works such as EgoMimic kareer2024egomimicscalingimitationlearning  have attempted to bridge the embodiment gaps with techniques such as visual masking, data normalization, and motion retargeting, such domain gaps still largely remain. More broadly, simply co-training from cross-domain data does not automatically yield effective knowledge transfer, as suggested by recent studies wei2025empirical . Such challenges prevent policies from scaling their performance primarily with human data.\n\nWe formalize the human-robot cross-embodiment learning problem as a domain adaptation problem, where human and robot data represent two labeled distributions with significant covariate shifts in observations due to embodiment gaps. Standard domain adaptation approaches often rely on global distribution alignment techniques such as adversarial training tzeng2017adversarialdiscriminativedomainadaptation  and maximum mean discrepancy minimization long2017deeptransferlearningjoint . However, they primarily address high-level tasks such as image classification and fail to preserve detailed action-relevant information—a critical requirement for robot learning where actions and observations are temporally correlated under compounding covariate shift.\n\nTo address these challenges, we propose EgoBridge, a novel domain adaptation approach that uses Optimal Transport (OT) to align latent representations from human and robot domains as part of the policy co-training objective. Unlike conventional domain alignment methods, our OT formulation explicitly exploits the inherent relationship between motion similarities in human and robot domains to form pseudo-pairs as supervision for the adaptation process. Concretely, we use the dynamic time warping (DTW) distance among human and robot motion trajectories to shape the OT ground cost. This encourages the transport map to find a minimal-cost coupling between human and robot data exhibiting similar behaviors. As such, EgoBridge aligns policy representations across domains via a differentiable OT loss (Sinkhorn distance), while preserving action-relevant information for policy learning. Importantly, we show that EgoBridge learns a shared latent representation that generalizes beyond the paired data. This enables the policy to learn behaviors observed only within the human dataset, effectively enabling the policy to scale primarily with human data.\n\nWe evaluate EgoBridge on both a reproducible simulation benchmark task and three challenging real-world manipulation tasks. Our results show that EgoBridge consistently improves policy success rates compared to human-augmented cross-embodiment baselines, for up to 44% absolute success rate improvement, and effectively transfers behaviors from diverse human demonstrations to robotic execution in tasks requiring spatial, visual, and task generalization.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效地从人类的第一人称数据中学习模仿行为以提升机器人操作能力。  \n2. 人类与机器人之间在视觉外观、传感器模态和运动学上的显著领域差距如何影响知识转移。  \n3. 现有的领域适应方法如何未能有效保留与动作相关的信息，导致机器人学习性能受限。  \n\n【用了什么创新方法】  \n本研究提出了EgoBridge，一个统一的共训练框架，通过最优传输（Optimal Transport, OT）对人类和机器人数据的策略潜在空间进行显式对齐。该方法利用动态时间规整（Dynamic Time Warping, DTW）距离来构建人类与机器人运动轨迹之间的伪配对，从而在适应过程中形成监督信号。EgoBridge通过可微分的OT损失（Sinkhorn距离）对跨领域的策略表示进行对齐，同时保留与动作相关的信息。实验结果表明，EgoBridge在三个真实世界的单臂和双臂操作任务中实现了高达44%的绝对成功率提升，并能够有效地将人类演示中的行为转移到机器人执行中，展示了其在新物体、场景和任务上的泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots",
            "authors": "Qingxi Meng,Emiliano Flores,Carlos Quintero-Peña,Peizhu Qian,Zachary Kingston,Shannan K. Hamlin,Vaibhav Unhelkar,Lydia E. Kavraki",
            "subjects": "Robotics (cs.RO)",
            "comment": "under review",
            "pdf_link": "https://arxiv.org/pdf/2509.19610",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19610",
            "arxiv_html_link": "https://arxiv.org/html/2509.19610v1",
            "abstract": "In this work, we address the problem of planning robot motions for a high-degree-of-freedom (d\n\no\nf) robot that effectively achieves a given perception task while the robot and the perception target move in a dynamic environment. Achieving navigation and perception tasks simultaneously is challenging, as these objectives often impose conflicting requirements. Existing methods that compute motion under perception constraints fail to account for obstacles, are designed for low-d\n\no\nf robots, or rely on simplified models of perception. Furthermore, in dynamic real-world environments, robots must replan and react quickly to changes and directly evaluating the quality of perception (e.g., object detection confidence) is often expensive or infeasible at runtime. This problem is especially important in human-centered environments such as homes and hospitals, where effective perception is essential for safe and reliable operation. To address these challenges, we propose a GPU-parallelized perception-score-guided probabilistic roadmap planner with a neural surrogate model (ps-prm). The planner explicitly incorporates the estimated quality of a perception task into motion planning for high-d\n\no\nf robots. Our method uses a learned model to approximate perception scores and leverages GPU parallelism to enable efficient online replanning in dynamic settings. We demonstrate that our planner, evaluated on high-d\n\no\nf robots, outperforms baseline methods in both static and dynamic environments in both simulation and real-robot experiments.",
            "introduction": "Achieving mobile manipulation (e.g., navigating to a table to grasp a tool) alongside perception tasks (e.g., tracking an object or a human) remains highly challenging. In this work, we address the problem of planning robot motions for a high-degree-of-freedom (d\n\no\nf) robot that effectively achieves a given perception task while the robot and the perception target move in a dynamic environment. A fundamental challenge emerges: the objectives of motion and perception often conflict—perception tasks impose non-trivial constraints on robot motion, as performance varies with object shape, occlusion, distance, and viewing angle [1, 2, 3]. For example, detecting human faces often requires maintaining a frontal view, while reliable object tracking typically benefits from close proximity. At the same time, motion planning for high-degree-of-freedom (d\n\no\nf) robots must follow kinematic constraints, avoid obstacles and nearby humans, and account for the complex, nonlinear relationship between the robot’s configuration and the camera pose.\n\nAs an example, consider the situation in Fig. 1. A mobile robot with wrist-mounted camera must navigate a cluttered environment while observing a monitor at the end of the hallway. The robot must navigate through clutter with a differential drive base while simultaneously maintaining tracking performance, i.e., it avoids occlusions and attains high tracking confidence.\n\nThese challenges are further compounded in dynamic environments, where objects and people move. In such settings, robots must continuously adapt to changes while preserving high perception quality [4, 5, 6, 7].\nFor example, a key motivation of this work comes from human-centered environments such as nurse training settings [8, 9], where robotic tutors are used to support skill acquisition. In such a system [10], the robot must continuously monitor a trainee to detect compliance with sterile techniques and provide timely feedback while simultaneously navigating the environment. Fig. 7 and Fig. 10 illustrate this challenge, where the robot must continuously track nurses’ faces while navigating through a cluttered clinical environment.\nHowever, directly evaluating perception performance (e.g., detection confidence) in such settings can be prohibitively expensive or even impossible. This motivates the need for efficient mechanisms to approximate perception quality and support fast, responsive replanning in high-dimensional spaces.\n\nHow do existing planning methods tackle planning for perception in high d\n\no\nf setting?\nExisting planning methods that consider both motion and perception have several limitations. Informative path planners [11, 12, 13, 14] aim to find paths that maximize information gain while respecting constraints like path length or budget, but they often become computationally expensive, especially when the problem size grows. Perception-aware path planners are typically designed for low-d\n\no\nf systems [15, 16, 17, 18, 19, 20] or rely on overly simplified perception models [21], such as keeping a point of interest centered in the field of view. Active perception approaches [22, 23, 24, 25, 26, 27, 28, 29, 30] focus on selecting discrete viewpoints to maximize scene understanding, rather than ensuring consistent perception quality along a motion trajectory. Moreover, most of these approaches are not designed to operate in dynamic environments where objects or humans may move and where continuous adaptation is needed. As a result, they lack the responsiveness required for real-time perception-aware planning in practical settings.\n\nIn this paper, we propose Perception-Score-guided Probabilistic Roadmap Planning (PS-PRM)—a GPU-parallelized, roadmap-based planner that explicitly incorporates the estimated quality of a given perception task (e.g., object detection confidence) into motion planning for high-degree-of-freedom (DOF) robots. The perception score is predicted by a neural surrogate model trained to approximate perception scores for specific tasks.\nps-prm jointly optimizes for both motion and perception by associating each configuration with an estimated perception score and using these scores to guide roadmap construction and path selection. To efficiently approximate perception quality during planning, we introduce a neural surrogate model trained on data from various objects and human targets. To account for occlusions in cluttered environments, we incorporate a ray-casting-based pipeline that dynamically adjusts perception estimates. Finally, to enable fast replanning in dynamic environments, we develop a GPU-parallelized framework that executes collision checking, forward kinematics (FK), and perception evaluation in batch. These components allow our method to scale to high-dimensional planning problems while maintaining robust perception performance in both static and dynamic scenarios.\n\nIn this work, we make the following key contributions:\n\nA sampling-based roadmap planner, ps-prm, that integrates perception quality into motion planning for high-d\n\no\nf robots.\n\nA perception score estimation pipeline that uses a neural surrogate model and ray casting to efficiently approximate detection quality while accounting for occlusions in cluttered scenes.\n\nA batch-processing infrastructure that accelerates collision checking, forward kinematics, and perception evaluation using GPU parallelism, enabling real-time replanning.\n\nExtensive simulation and real-robot experiments on three different high-d\n\no\nf robots, demonstrating that ps-prm consistently improves both perception performance and planning efficiency compared to baseline methods.\n\n1. A sampling-based roadmap planner, ps-prm, that integrates perception quality into motion planning for high-d\n\no\nf robots.\n\n2. A perception score estimation pipeline that uses a neural surrogate model and ray casting to efficiently approximate detection quality while accounting for occlusions in cluttered scenes.\n\n3. A batch-processing infrastructure that accelerates collision checking, forward kinematics, and perception evaluation using GPU parallelism, enabling real-time replanning.\n\n4. Extensive simulation and real-robot experiments on three different high-d\n\no\nf robots, demonstrating that ps-prm consistently improves both perception performance and planning efficiency compared to baseline methods.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在动态环境中为高自由度机器人规划运动和感知任务？  \n2. 现有方法在处理高自由度机器人运动和感知时的局限性是什么？  \n3. 如何有效评估感知质量以支持快速的在线重规划？  \n\n【用了什么创新方法】  \n提出了一种名为ps-prm的GPU并行化感知评分引导概率路网规划器，该方法将感知任务的质量估计纳入高自由度机器人的运动规划中。通过训练神经代理模型来近似感知评分，并利用GPU并行性实现动态环境中的高效在线重规划。实验结果表明，ps-prm在静态和动态环境中均优于基线方法，显著提高了感知性能和规划效率。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting",
            "authors": "Sander Tonkens,Nikhil Uday Shinde,Azra Begzadić,Michael C. Yip,Jorge Cortés,Sylvia L. Herbert",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "The first three authors contributed equally. This work has been accepted for publication at the Conference on Robot Learning",
            "pdf_link": "https://arxiv.org/pdf/2509.19597",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19597",
            "arxiv_html_link": "https://arxiv.org/html/2509.19597v1",
            "abstract": "The widespread deployment of autonomous systems in safety-critical environments such as urban air mobility hinges on ensuring reliable, performant, and safe operation under varying environmental conditions.\nOne such approach, value function-based safety filters, minimally modifies a nominal controller to ensure safety.\nRecent advances leverage offline learned value functions to scale these safety filters to high-dimensional systems.\nHowever, these methods assume detailed priors on all possible sources of model mismatch, in the form of disturbances in the environment – information that is rarely available in real world settings.\nEven in well-mapped environments like urban canyons or industrial sites, drones encounter complex, spatially-varying disturbances arising from payload-drone interaction, turbulent airflow, and other environmental factors.\nWe introduce space2time, which enables safe and adaptive deployment of offline-learned safety filters under unknown, spatially-varying disturbances.\nThe key idea is to reparameterize spatial variations in disturbance as temporal variations, enabling the use of precomputed value functions during online operation.\nWe validate space2time on a quadcopter through extensive simulations and hardware experiments, demonstrating significant improvement over baselines.",
            "introduction": "Autonomous systems are increasingly deployed in safety-critical environments subject to variable conditions, where ensuring reliable and safe operation is of paramount importance.\nFor instance, a drone operating in mapped environments such as urban canyons or shipyards must remain within a known safe region despite complex, spatially-varying wind disturbances.\nRather than designing a bespoke performant, yet safe, controller for each task, a more modular approach uses a safety filter.\nThese filters monitor a nominal, high-performance controller in real-time and intervene minimially-only when necessary to enforce guarantees without unduly compromising task performance [1].\nPopular approaches for constructing such filters include Control Barrier Functions (CBFs) [2] and Hamilton-Jacobi Reachability (HJR) analysis [3].\nA recent line of work merges these two paradigms, leveraging reachability-based value functions as barrier certificates to construct safety filters with formal guarantees [4, 5, 6].\n\nHowever, these methods face significant practical challenges. A primary limitation is their reliance on an accurate, pre-specified model of the system’s dynamics and its operational domain-the set of conditions, such as expected wind patterns, the system is designed to operate in.\nSecond, each method faces inherent hurdles: HJR analysis is limited by the curse of dimensionality, making it intractable for high-dimensional systems, while the systematic synthesis of a valid CBF for general nonlinear systems remains an open problem.\n\nTo overcome these practical limitations, learning-based approaches have gained prominence, seeking to approximate safety value functions or barrier certificates directly from data [7, 8, 9].\nHowever, these learned approaches often assume a static operational domain that is known beforehand.\nThis makes them brittle when faced with environmental conditions that shift during and across deployments, forcing a choice between unsafe behavior in the face of novelty or an overly conservative policy designed for the worst case [10].\n\nOffline learning of a value function for a safety filter relies on a joint system-environment model that captures the true system’s runtime behavior.\nSuch a model is infeasible in environments with spatially varying disturbances, e.g., wind in urban canyons [11, 12], which are unknown a priori and even differ across deployments.\nA compounding challenge arises because disturbance measurements are typically obtained at a slower rate than control inputs, due to practical sensing and computational constraints.\nHowever, this slower update rate means unmodeled spatial variation can cause significant changes between consecutive measurements, leading to safety violations if ignored.\nOur insight is that spatial variations in disturbance appear as temporal variations along a trajectory.\nBy learning a time-varying safety value function that explicitly accounts for disturbance evolution over time, we implicitly capture spatial variations along trajectories, enabling their use as online safety filters.\nThis work takes a step towards bridging offline-learned value functions with online adaptation in evolving operational domains.\nOur main contributions are:\n\nWe introduce a safety value function formulation that is explicitly conditioned on a disturbance’s temporal rate of change.\n\nWe introduce a safety value function formulation that is explicitly conditioned on a disturbance’s temporal rate of change.\n\nWe use this value function formulation to propose space2time.\nOur approach reparameterizes spatial variations as temporal variations in disturbance.\nThis ensures safety in the presence of unknown, spatially varying disturbances through the use of an adaptive safety filter that leverages our offline-learned value functions.\n\nWe validate space2time on a quadcopter through extensive simulations and hardware experiments demonstrating substantial improvements in safety compared to existing approaches without significantly sacrificing performance.\n\n1. We introduce a safety value function formulation that is explicitly conditioned on a disturbance’s temporal rate of change.\n\n2. We use this value function formulation to propose space2time.\nOur approach reparameterizes spatial variations as temporal variations in disturbance.\nThis ensures safety in the presence of unknown, spatially varying disturbances through the use of an adaptive safety filter that leverages our offline-learned value functions.\n\n3. We validate space2time on a quadcopter through extensive simulations and hardware experiments demonstrating substantial improvements in safety compared to existing approaches without significantly sacrificing performance.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在未知的空间变化干扰下确保自主系统的安全性和性能。  \n2. 现有的安全过滤方法依赖于准确的系统动态模型，如何克服这一限制。  \n3. 如何将离线学习的价值函数与在线适应相结合，以应对不断变化的操作环境。  \n\n【用了什么创新方法】  \n本文提出了space2time方法，通过将空间变化的干扰重新参数化为时间变化，利用离线学习的安全价值函数进行在线适应。该方法显著提高了在复杂环境中的安全性，经过广泛的仿真和硬件实验验证，结果显示与现有方法相比，安全性有显著改善，同时性能损失不大。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping",
            "authors": "Chad R. Samuelson,Abigail Austin,Seth Knoop,Blake Romrell,Gabriel R. Slade,Timothy W. McLain,Joshua G. Mangelson",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19579",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19579",
            "arxiv_html_link": "https://arxiv.org/html/2509.19579v1",
            "abstract": "Outdoor intelligent autonomous robotic operation relies on a sufficiently expressive map of the environment. Classical geometric mapping methods retain essential structural environment information, but lack a semantic understanding and organization to allow high-level robotic reasoning. 3D scene graphs (3DSGs) address this limitation by integrating geometric, topological, and semantic relationships into a multi-level graph-based map. Outdoor autonomous operations commonly rely on terrain information either due to task-dependence or the traversability of the robotic platform. We propose a novel approach that combines indoor 3DSG techniques with standard outdoor geometric mapping and terrain-aware reasoning, producing terrain-aware place nodes and hierarchically organized regions for outdoor environments. Our method generates a task-agnostic metric-semantic sparse map and constructs a 3DSG from this map for downstream planning tasks, all while remaining lightweight for autonomous robotic operation. Our thorough evaluation demonstrates our 3DSG method performs on par with state-of-the-art camera-based 3DSG methods in object retrieval and surpasses them in region classification while remaining memory efficient. We demonstrate its effectiveness in diverse robotic tasks of object retrieval and region monitoring in both simulation and real-world environments.",
            "introduction": "Autonomous robotic systems within large-scale outdoor environments have the potential to address a wide range of fundamental societal problems, including search and rescue, forest fires, food delivery, and others.\nHowever, such robotic systems require the ability to robustly and reliably localize within, map, and interpret outdoor scenes at large scales. In this context, we focus on the development of metric-semantic mapping techniques that enable large-scale autonomy in outdoor scenes.\n\nDue to its range, accuracy, and 360∘360^{\\circ} field-of-view, LiDAR has become a standard sensor for large-scale outdoor geometric mapping [1, 2, 3]. Beyond geometry, some approaches train models to semantically classify points in LiDAR scans [4, 5, 6, 7]. While promising, these models are still largely closed-set (restricted to a fixed set of semantic classes).\n\nCameras, in contrast, provide rich visual data and have achieved remarkable success in semantic scene understanding. Recent techniques enable more general open-set scene understanding and even enable grounding of visual data with natural language through vision-language models (VLMs) [8, 9, 10].\n\nOver the last several years, 3D scene graphs (3DSGs) have emerged as a structured approach to build semantically and hierarchically organized metric-semantic maps. Many 3DSG-based methods utilize VLMs and large language models (LLMs) for both scene graph construction and autonomous task planning. Most existing 3DSG methods focus on indoor environments using camera imagery and depth data to build a semantically-classified mesh that forms the base layer of the 3DSG. However, constructing mesh maps over large areas is both computationally and memory intensive. Additionally, camera-derived depth has limited range (≤20\\leq 20 meters). These both restrict indoor 3DSG techniques from scaling to large outdoor settings.\n\nIn this work, we combine indoor 3DSG techniques with geometrically robust outdoor LiDAR SLAM methods, enabling metric-semantic mapping in large-scale outdoor environments, we term our method Terra (see Fig. 1).\nWe structure the resulting map into a hierarchical scene graph specifically designed to support autonomous outdoor robotic tasks. In particular, since terrain is a key factor for outdoor navigation, we integrate a terrain layer into our 3DSG.\nThe key contributions of our paper are:\n\nA novel, memory-efficient, and task-agnostic approach for open-set metric-semantic mapping in large-scale outdoor environments,\n\nA terrain layer in the outdoor 3DSG that supports terrain-aware tasks where VLMs alone struggle,\n\nHierarchical region layers to handle multiple levels of task abstraction,\n\nAn in-depth evaluation on simulated and real world data comparing Terra with state-of-the-art (SOTA) indoor 3DSG methods.\n\nThe rest of our paper is outlined as follows. Section II provides a brief overview of research in semantic mapping and 3DSG methods. Section III gives an explanation for the Terra method. Section IV provides experiments across simulation and real-world datasets demonstrating the capabilities of our Terra method compared to SOTA methods and techniques. Section V concludes our findings and explores future work in the area of outdoor 3DSG generation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在大规模户外环境中实现有效的metric-semantic mapping？  \n2. 如何结合几何和语义信息以支持自主机器人的任务规划？  \n3. 如何处理户外环境中的地形信息以提高导航能力？  \n\n【用了什么创新方法】  \n本研究提出了一种名为Terra的创新方法，将室内3D场景图（3DSG）技术与户外几何映射和地形感知推理相结合，生成任务无关的稀疏地图。该方法通过构建层次化的场景图，集成地形层以支持户外导航任务，显著提高了区域分类的性能，并在内存效率上优于现有的相机基础3DSG方法。经过全面评估，Terra在物体检索和区域监控等多种机器人任务中表现出色，验证了其在模拟和真实环境中的有效性。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning",
            "authors": "Zachary Olkin,Kejun Li,William D. Compton,Aaron D. Ames",
            "subjects": "Robotics (cs.RO)",
            "comment": "Submitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.19573",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19573",
            "arxiv_html_link": "https://arxiv.org/html/2509.19573v1",
            "abstract": "Achieving highly dynamic behaviors on humanoid robots, such as running, requires controllers that are both robust and precise, and hence difficult to design. Classical control methods offer valuable insight into how such systems can stabilize themselves, but synthesizing real-time controllers for nonlinear and hybrid dynamics remains challenging. Recently, reinforcement learning (RL) has gained popularity for locomotion control due to its ability to handle these complex dynamics. In this work, we embed ideas from nonlinear control theory, specifically control Lyapunov functions (CLFs), along with optimized dynamic reference trajectories into the reinforcement learning training process to shape the reward. This approach, CLF-RL, eliminates the need to handcraft and tune heuristic reward terms, while simultaneously encouraging certifiable stability and providing meaningful intermediate rewards to guide learning. By grounding policy learning in dynamically feasible trajectories, we expand the robot’s dynamic capabilities and enable running that includes both flight and single support phases. The resulting policy operates reliably on a treadmill and in outdoor environments, demonstrating robustness to disturbances applied to the torso and feet. Moreover, it achieves accurate global reference tracking utilizing only on-board sensors, making a critical step toward integrating these dynamic motions into a full autonomy stack.",
            "introduction": "Humanoid running is a challenging task that involves executing highly dynamic motion on a nonlinear and hybrid system. Achieving performant and robust running demands controllers that can reject disturbances arising from model mismatch and environmental uncertainty, all while operating near the limits of the robot’s dynamic capabilities. Running inherently involves alternating between a flight phase, where both feet are off the ground, and a single-support phase, where only one foot is in contact. Effectively handling control across these hybrid domains is critical, as improper treatment can lead to instability.\n\nBipedal running has been studied for decades with early examples including the Raibert heuristic [1]. In the 2010’s, a number of planar bipeds were developed and running was achieved [2, 3, 4]. These methods fall under the category of Hybrid Zero Dynamics (HZD) [5] where offline trajectory optimization leveraging the idea of virtual constraints is used to generate a stable and periodic trajectory. Then, online, the trajectories are tracked using tools from nonlinear control theory such as feedback linearization and control Lyapunov functions (CLFs). These controllers have been shown to be certifiably stable if the convergence to the virtual constraints is sufficiently quick relative to the destabilizing effect of the foot-ground impact [6]. In general, these methods optimize for a steady-state motion (i.e. a periodic orbit) while the ability to get to the steady state motion is entirely dependent on the region of attraction of the tracking controller. Because these controllers operate only on the continuous dynamics, their capacity to generate transient and robust behaviors is inherently limited.\n\nOne of the difficulties with generating transients motions lies in the ability to reason through contact, specifically determining the subsequent contact schedule. A number of model predictive control (MPC) schemes have attempted to solve this issue through various numerical methods. Contact implicit MPC (CI-MPC) uses gradient-based optimization to implicitly yield a hybrid domain sequence, but have not yielded bipedal running and are quite computationally intensive in general [7, 8, 9]. Sample-based methods circumvent the need for an explicit domain sequence by rolling out sampled inputs and optimizing the inputs in an MPC fashion [10, 11, 12]. Yet these methods still require large amounts of on-board compute and have not yet produced humanoid running.\n\nReinforcement learning (RL) has recently emerged as a dominant approach for controlling legged robots [13, 14, 15, 16, 17]. RL methods are attractive due to their robustness, ability to generate diverse motions, lightweight on-board execution requirements, and their capacity to learn contact-rich behaviors directly from experience rather than requiring explicit contact modeling. Notably, RL has achieved bipedal running [18, 19]. However, many RL schemes require extensive hand tuning of rewards to produce a performant and robust policy. Poorly shaped rewards can lead to unstable learning, failure to achieve the desired behaviors, or prohibitively long training times. To mitigate these issues, a number of works have merged pre-computed trajectories with RL. This includes reduced-order model trajectories [20, 21] and full-order model trajectories such as from the HZD framework [22]. Yet, even in these cases, the rewards generally incentivize being close to the trajectories in an ad-hoc manner. Building on the ideas of [23], we embed a CLF tracking controller’s stability condition and Lyapunov function into the reward to provide meaningful intermediate rewards and incentivize certifiably stable behavior.\n\nMany prior works focus on bipeds, not humanoids, and have not shown running with low positional drift required for treadmill operation. Hand designed rewards are often used and the policies may fail to match the desired velocity [19]. Even when fast locomotion is achieved, a flight phase is not necessarily attained [20]. Alternative approaches use RL to imitate human motion data [24, 25, 26, 27]. Although such methods can reproduce the demonstrated motion, they have not shown the ability to produce dynamically stable steady-state motions, like running, with tracking capabilities ready for use in an autonomy stack. In contrast, our proposed method provides a principled way of synthesizing running controllers for full humanoids, not just bipeds, with minimal tuning and without relying on human data. The resulting policy produces both transient and steady state running motions with accurate position and velocity tracking.\n\nIn this paper, we develop a model-guided approach to enable running on a humanoid. We leverage multi-domain trajectory optimization, control Lyapunov functions (CLFs), and reinforcement learning (RL) to create a robust and performant controller. Trajectory optimization is used to generate nominal motions, which are then incorporated into CLF-based tracking controllers. These CLFs are embedded directly into the RL reward, eliminating the need for heuristic reward design. The resulting RL policy no longer requires trajectories or CLFs at runtime. Fig. 1 shows an overview of the framework. We demonstrate this controller on a Unitree G1 humanoid robot both on a treadmill and outdoors. The resulting policy shows accurate position and velocity tracking on the hardware, and exhibits robustness to various objects on the ground all while maintaining running speeds and achieving a full flight phase.\n\nThe rest of the paper is organized as follows: in section II the mathematical preliminaries are presented, including the hybrid system model and CLFs. Section III describes how multi-domain trajectories can be generated and embedded into the RL via CLFs. Then, section IV showcases the simulation and hardware experiments, demonstrating the performance and robustness of the policy. Lastly section V gives the paper’s conclusions.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何设计一个既稳健又精确的人形机器人跑步控制器。  \n2. 如何利用强化学习（RL）处理复杂的非线性和混合动力学。  \n3. 如何消除手工调节奖励项的需求，同时确保控制的稳定性。  \n\n【用了什么创新方法】  \n本研究提出了一种控制Lyapunov函数（CLF）引导的强化学习（CLF-RL）方法，通过将非线性控制理论的思想与优化的动态参考轨迹嵌入到强化学习训练过程中，形成了一种新的奖励机制。该方法消除了对手工设计和调节奖励项的需求，同时鼓励可证明的稳定性，并提供有意义的中间奖励以指导学习。通过基于动态可行轨迹的策略学习，扩展了机器人的动态能力，使其能够在跑步中有效地处理飞行和单支撑阶段。实验结果表明，该策略在跑步机和户外环境中均表现出可靠性，能够抵御对机器人躯干和脚部施加的干扰，并实现准确的全局参考跟踪，仅依赖于机载传感器。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action",
            "authors": "Sacha Morin,Kumaraditya Gupta,Mahtab Sandhu,Charlie Gauthier,Francesco Argenziano,Kirsty Ellis,Liam Paull",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.19571",
            "code": "https://montrealrobotics.ca/agentic-scene-policies.github.io/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19571",
            "arxiv_html_link": "https://arxiv.org/html/2509.19571v1",
            "abstract": "Executing open-ended natural language queries is a core problem in robotics. While recent advances in imitation learning and vision-language-actions models (VLAs) have enabled promising end-to-end policies, these models struggle when faced with complex instructions and new scenes. An alternative is to design an explicit scene representation as a queryable interface between the robot and the world, using query results to guide downstream motion planning. In this work, we present Agentic Scene Policies (ASP), an agentic framework that leverages the advanced semantic, spatial, and affordance-based querying capabilities of modern scene representations to implement a capable language-conditioned robot policy. ASP can execute open-vocabulary queries in a zero-shot manner by explicitly reasoning about object affordances in the case of more complex skills. Through extensive experiments, we compare ASP with VLAs on tabletop manipulation problems and showcase how ASP can tackle room-level queries through affordance-guided navigation and a scaled-up scene representation. We encourage readers to visit our project page.",
            "introduction": "Generalist language-conditioned robot policies need to manage the complex interplay between language, space, and action. Much of the recent progress on this problem has been driven by vision-language models (VLMs) trained on internet-scale data and showing strong general visual understanding in the open-world. Applying VLMs to the robotics domain has broadly followed two paradigms. In the first paradigm, VLMs can serve as backbones for end-to-end policy learning, yielding “vision-language actions” models (VLAs) that directly map sensor data and language commands to robot actions [1, 2, 3, 4, 5]. In the second paradigm, VLMs are primarily used for perception in the construction and querying of structured scene representations with advanced capabilities for object retrieval and spatial reasoning [6, 7, 8, 9, 10, 11, 12, 13, 14, 15].\n\nVLAs are increasingly showing zero-shot potential on new tasks [16, 4] but in practice still require task-specific fine-tuning to be truly proficient, which poses challenges in terms of data collection and infrastructure that limit overall deployment. For their part, scene representations preserve the generality of VLMs—they can practically represent any object—but do not offer a direct solution to the motion problem and are often constrained to navigation and pick-and-place tasks as a result [6, 17, 18].\n\nWe observe that a large number of language queries can be solved through a (potentially repeated) three-step process consisting of 1) object grounding, 2) spatial reasoning, and 3) part-level interaction. In this work, we demonstrate that state-of-the-art zero-shot performance can be achieved across a wide range of robotics tasks by implementing all three steps as scene queries. We expose querying functionalities as tools that can be freely called by a large language model (LLM) agent to execute language commands. For interaction, we design an expressive set of skill primitives supported by the strong affordance detection capabilities of VLMs. Our modular policy can map language queries (Ring the desk bell, Remove the thumbtack) to specific affordances and affordance-based skills (tip_push, pinch_pull), as well as solve a range of mobile manipulation queries. In summary, our key contributions include:\n\nAgentic Scene Policies (ASP), a language-conditioned manipulation policy that can solve a broad range of queries involving specific semantics, spatial reasoning, and affordances.\n\nAn extensive empirical comparison with leading VLAs on 15 manipulation tasks, providing a valuable data point in the ongoing debate between modular and end-to-end methods.\n\nA mobile version of ASP that tackles room-level queries through affordance-guided navigation and a scaled-up scene representation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何执行开放式自然语言查询以实现机器人操作。  \n2. 现有的视觉-语言-动作模型在复杂指令和新场景下的局限性。  \n3. 如何设计一个明确的场景表示作为机器人与世界之间的可查询接口。  \n\n【用了什么创新方法】  \n本研究提出了Agentic Scene Policies (ASP)，通过结合语义、空间和可供性查询能力，创建了一个语言条件的机器人操作政策。ASP通过三个步骤（对象定位、空间推理和部分交互）实现复杂任务的零-shot执行。实验表明，ASP在15个操作任务中表现优于现有的视觉-语言-动作模型，能够有效处理房间级查询并实现基于可供性的导航。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space",
            "authors": "Sankalp Agrawal,Junwon Seo,Kensuke Nakamura,Ran Tian,Andrea Bajcsy",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19555",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19555",
            "arxiv_html_link": "https://arxiv.org/html/2509.19555v1",
            "abstract": "Recent works have shown that foundational safe control methods, such as Hamilton–Jacobi (HJ) reachability analysis, can be applied in the latent space of world models. While this enables the synthesis of latent safety filters for hard-to-model vision-based tasks, they assume that the safety constraint is known a priori and remains fixed during deployment, limiting the safety filter’s adaptability across scenarios. To address this, we propose constraint-parameterized latent safety filters that can adapt to user-specified safety constraints at runtime. Our key idea is to define safety constraints by conditioning on an encoding of an image that represents a constraint, using a latent-space similarity measure. The notion of similarity to failure is aligned in a principled way through conformal calibration, which controls how closely the system may approach the constraint representation. The parameterized safety filter is trained entirely within the world model’s imagination, treating any image seen by the model as a potential test-time constraint, thereby enabling runtime adaptation to arbitrary safety constraints. In simulation and hardware experiments on vision-based control tasks with a Franka manipulator, we show that our method adapts at runtime by conditioning on the encoding of user-specified constraint images, without sacrificing performance. Video results can be found on the project website.",
            "introduction": "World models offer a promising paradigm for generalizing robot control to hard-to-simulate physical tasks by learning compact latent state spaces and dynamics directly from high-dimensional observations [1, 2, 3, 4]. Recent works have demonstrated that foundational safe control methods, such as Hamilton–Jacobi (HJ) reachability analysis [5, 6], can be applied directly in a world model’s latent space, enabling safety analysis directly from high-dimensional sensor inputs. By computing robot policies that anticipate and avoid future failures within the world model’s imagination, these latent safety filters can proactively steer robots away from hard-to-model constraints, such as spilling the contents of deformable bags [7] or toppling complex rigid-body structures [8].\n\nHowever, most safe control frameworks assume that the state constraints that robots should avoid are determined a priori and remain fixed during deployment [9, 6]. In practice, this assumption is overly restrictive: at deployment time, a robot may need to adapt its notion of what is a safety constraint based on changing environments or end-user requirements. For example, consider the robot manipulator in Fig. 1 that must sweep clutter from a table. In one scenario, it needs to avoid sweeping objects in a particular region (top row), but later it may be tasked with intentionally collecting objects into that same region while avoiding a different one (bottom row). This raises the central question of our work:\n\nIn this work, we design constraint-parameterized latent safety filters (called AnySafe). The core challenge with parameterizing safety constraints in the latent space is that, unlike in hand-designed state spaces, the structure needed to represent and optimize against a suite of safety constraints does not naturally emerge. In hand-designed state spaces, one can design a low-dimensional parameterization of the constraint set (e.g., a circle by its center and radius) alongside a dense distance measure for guiding policy optimization (e.g., signed distance to the constraint set); this allows for the safety filter to be effectively computed for all possible constraint variations. In latent spaces, by contrast, constraints are typically only implicitly defined by classifiers on the latent states [7, 8] which do not admit a continuous parameterization to represent diverse safety constraints nor yield a notion of proximity from a state to such constraints.\n\nWe propose three key ingredients that enable constraint-parameterization in latent safety filters. First, we specify safety constraints via a similarity measure between the embedding of a constraint image and the robot’s current latent state; this provides a dense signal of how close the policy is to failure. Then, we calibrate the resulting constraint set with conformal prediction [10, 11] to align with an end-user’s semantic notion of failure. Lastly, we train the safety filter by treating any image in the world model dataset as a possible test-time safety constraint. At runtime, we adapt the latent safety filter by conditioning it on an encoding of a user-specified constraint image, thereby adapting it to the runtime safety specification.\n\nWe evaluate our framework on vision-based safe-control tasks, including a simulated vehicle collision-avoidance domain and real-world object sweeping with a Franka manipulator. Our results highlight four key findings: (1) by parameterizing the safety filter with constraint representations, AnySafe can adapt to arbitrary constraints provided as images; (2) this adaptability does not come at the cost of performance, as for a given constraint, the parameterized safety filter achieves performance comparable to a specialized filter trained solely on that constraint;\n(3) AnySafe generalizes to constraints beyond those that specialized safety filters can model; and (4) since AnySafe learns from continuous latent similarity signals, conformal calibration allows us to control how conservatively the robot avoids specified constraints by adjusting the effective size of the failure set.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在运行时适应用户指定的安全约束以提高机器人控制的灵活性？  \n2. 如何在潜在空间中有效地参数化安全约束以支持多样化的任务？  \n3. 如何在不牺牲性能的情况下实现安全过滤器的适应性？  \n\n【用了什么创新方法】  \n本研究提出了一种名为AnySafe的约束参数化潜在安全过滤器。其核心方法包括通过相似性度量将安全约束与机器人的潜在状态进行关联，使用符合预测来校准约束集，并在训练过程中将任何图像视为潜在的测试时安全约束。在模拟和硬件实验中，AnySafe展示了其在运行时根据用户指定的约束图像进行适应的能力，且在给定约束下，其性能与专门训练的过滤器相当。此外，AnySafe能够超越专门安全过滤器的建模能力，提供更广泛的适应性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots",
            "authors": "Min Dai,Aaron D. Ames",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19545",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19545",
            "arxiv_html_link": "https://arxiv.org/html/2509.19545v1",
            "abstract": "We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots. RoMoCo’s modular architecture unifies state-of-the-art planners and whole-body locomotion controllers under a consistent API, enabling rapid prototyping and reproducible benchmarking. By leveraging reduced-order models for platform-agnostic gait generation, RoMoCo enables flexible controller design across diverse robots.\nWe demonstrate its versatility and performance through extensive simulations on the Cassie, Unitree H1, and G1 robots, and validate its real-world efficacy with hardware experiments on the Cassie and G1 humanoids.",
            "introduction": "Bipedal locomotion remains one of the central challenges in robotics, given bipeds’ high-dimensional, non-linear, hybrid, and underactuated nature. A widely adopted strategy to address this complexity is the use of reduced-order models (ROM) that capture the essential dynamics of walking while abstracting full-body details. These models, including the linear inverted pendulum (LIP) [1] and its variants [2, 3], have enabled the design of theoretically grounded and robust locomotion controllers. However, despite their success in research, deploying ROM-based planners and integrating them with whole-body controllers (WBC) remains a significant challenge, requiring expertise in contact and state estimation, robot kinematics and dynamics, and nonlinear control.\n\nIn recent years, there has been a surge of interest in learning-based locomotion, largely fueled by the release of frameworks such as IsaacLab [4] and IsaacGym [5], which provide scalable reinforcement learning (RL) environments and simulation infrastructure. These platforms, combined with open-source RL algorithms implementations such as RSL-RL [6], have lowered the barrier to entry for training locomotion policies at scale, enabling impressive demonstrations of locomotion in simulation and on hardware.\n\nIn contrast to learning-based approaches, model-based methods, though more interpretable, computationally efficient, and theoretically grounded, lack equivalent open-source support. Researchers typically face steep implementation hurdles, not only in developing reduced-order planners but also in integrating them with whole-body controllers and simulators. To date, only isolated efforts exist, such as the ALIP controller for Cassie [7] implemented in Simulink Real-Time, which, while effective, is tied to a specific robot platform and simulation ecosystem, preventing its generalization. This lack of a standardized, extensible framework limits reproducibility, comparative benchmarking, and rapid prototyping across diverse robotic platforms.\n\nA number of open-source libraries provide essential components for the planning and control of robots. FROST [8] supports trajectory optimization for bipedal robots, though it emphasizes offline trajectory generation rather than real-time controllers. Drake [9] offers a versatile platform for trajectory optimization and dynamics simulation, but its focus remains primarily on manipulation. OCS2 [10] implements efficient optimal control solvers for switched systems, but it requires significant integration effort to connect with whole-body controllers or reduced-order abstractions. Other optimal control tools, such as OpenSoT [11], Crocoddyl [12], and TSID [13], operate largely in isolation from dynamic locomotion planning. Collectively, these libraries form a rich ecosystem of building blocks; yet, researchers still assemble them manually, as there remains no unified framework that seamlessly couples reduced-order planning with whole-body control in a reproducible, extensible manner.\n\nTo address these challenges, we introduce RoMoCo—an open-source software toolbox designed to unify the development, evaluation, and deployment of ROM-based planners and WBC algorithms for bipedal and humanoid locomotion. The key contributions of this work are:\n\nA unified mathematical formulation of popular LIP-based planners (ALIP, H-LIP, MLIP, DCM), enabling their modular implementation and direct comparison.\n\nA modular software architecture that decouples planners, output mappings, whole-body controllers, and robot interfaces, allowing for rapid prototyping across different hardware.\n\nAn open-source library with integrated MuJoCo [14] simulation, demonstrated hardware deployments on multiple bipedal and humanoid platforms, and a fully available anonymized repository111https://anonymous.4open.science/r/RoMoCo-6E85 for review purposes.222The repository will be linked to a permanent public release upon acceptance.\n\nA comparative analysis of different ROM planners and whole-body controllers on the Cassie and Unitree G1, offering insights into their performance trade-offs.\n\n1. A unified mathematical formulation of popular LIP-based planners (ALIP, H-LIP, MLIP, DCM), enabling their modular implementation and direct comparison.\n\n2. A modular software architecture that decouples planners, output mappings, whole-body controllers, and robot interfaces, allowing for rapid prototyping across different hardware.\n\n3. An open-source library with integrated MuJoCo [14] simulation, demonstrated hardware deployments on multiple bipedal and humanoid platforms, and a fully available anonymized repository111https://anonymous.4open.science/r/RoMoCo-6E85 for review purposes.222The repository will be linked to a permanent public release upon acceptance.\n\n4. A comparative analysis of different ROM planners and whole-body controllers on the Cassie and Unitree G1, offering insights into their performance trade-offs.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效整合减少阶模型（ROM）与全身控制器（WBC）以实现双足和类人机器人运动控制？  \n2. 现有的开源工具缺乏统一框架，限制了不同机器人平台间的可重复性和快速原型开发。  \n\n【用了什么创新方法】  \nRoMoCo是一个开源C++工具箱，旨在统一ROM规划器和WBC算法的开发、评估和部署。它采用了模块化的软件架构，允许快速原型设计，并提供了对多种双足和类人机器人的硬件实验验证。通过比较不同的ROM规划器和全身控制器，RoMoCo展示了其在多种平台上的灵活性和性能，促进了对运动控制的深入理解。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Autonomous Elemental Characterization Enabled by a Low Cost Robotic Platform Built Upon a Generalized Software Architecture",
            "authors": "Xuan Cao,Yuxin Wu,Michael L. Whittaker",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19541",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19541",
            "arxiv_html_link": "https://arxiv.org/html/2509.19541v1",
            "abstract": "Despite the rapidly growing applications of robots in industry, the use of robots to automate tasks in scientific laboratories is less prolific due to lack of generalized methodologies and high cost of hardware.\nThis paper focuses on the automation of characterization tasks necessary for reducing cost while maintaining generalization,\nand proposes a software architecture for building robotic systems in scientific laboratory environment.\nA dual-layer (Socket.IO and ROS) action server design is the basic building block, which facilitates the implementation of a web-based front end for user-friendly operations and the use of ROS Behavior Tree for convenient task planning and execution.\nA robotic platform for automating mineral and material sample characterization is built upon the architecture, with an open source, low-cost three-axis computer numerical control gantry system serving as the main robot.\nA handheld laser induced breakdown spectroscopy (LIBS) analyzer is integrated with a 3D printed adapter, enabling automated 2D chemical mapping. We demonstrate the utility of automated chemical mapping by scanning of the surface of a spodumene-bearing pegmatite core\nsample with a 1071-point dense hyperspectral map acquired at a rate of 1520 bits per second.\nAutomated LIBS scanning enables controlled chemical quantification in the laboratory that complements field-based measurements acquired with the same handheld device, linking resource exploration and processing steps in the supply chain for lithium-based battery materials.",
            "introduction": "The rapid development of robotics in recent years has given a boost of its applications in industry, such as machine tending [1], palletizing [2], and assembly [3]. The operational stock of industrial robots worldwide increased from about 1.3 million in 2013 to 4.3 million in 2023 [4].\n\nSimilarly, robotic automation in research laboratories has become an emerging field, since “Robotics and automation can enable scientific experiments to be conducted faster, more safely, more accurately, and with greater reproducibility, allowing scientists to tackle large societal problems in domains such as health and energy on a shorter timescale” [5].\nAlthough there have been successful applications of robotic automation in laboratories [6, 7, 8, 9, 10], the use of robots to automate laboratory operations is still limited in general due to the automation gap caused by the variety of tasks and protocols [11], ultimately resulting in high costs.\n\nThis work sheds some light on the automation of characterization tasks in labs, which determine the properties, composition, and behavior of substances (e.g. spectrometry, microscopy, thermal analysis, etc.), and hence are essential in scientific research.\nOne common pattern in characterization tasks is sample-move-instrument-stay (SMIS), where a sample is placed to a specific position for an analytical instrument to start working. Automating this pattern using robots requires precise pick-and-place operations and enough degrees of freedom.\n\nBy contrast, this paper focuses on the sample-stay-instrument-move (SSIM) pattern, where an instrument is held by a robot and moved around a sample during characterization. Automating this pattern does not require pick-and-place operations since the instrument is mounted on the robot all the time. Sample standardization, such as positioning on a 2-d horizontal plane, reduces the robot’s required degrees of freedom to reach the samples, which could potentially lower the hardware cost.\n\nTowards this end, this paper introduces a robotic platform for automating SSIM characterization tasks for mineral and material samples. The platform consists of: (1) a low-cost 3D (translational movements in X, Y, and Z directions) gantry system commonly used in traditional computer numerical control (CNC) machining as the primary robot, (2) an analytical instrument mounted to the gantry system for sample characterization, and (3) a stereo camera capable of depth sensing for locating samples to be measured. All components, and samples to be measured, are placed on a benchtop. The general workflow consists of the following steps: (1) a sample location is either predefined, or else identified by the camera; (2) the gantry system takes the analytical instrument to the sample location; (3) the instrument starts characterization and collects raw data; (4) the raw data are processed and optional feedback is generated.\n\nThe core of the software is a generalized custom-designed architecture for automation systems in laboratory environments. The basis of the architecture is a dual-layer action server design for every hardware component, which monitors incoming operation requests through both Socket.IO [12] and Robot Operating System (ROS) [13] communication protocols and commands the hardware to act accordingly.\nOn top of all action servers lies a Behavior Tree (BT) [14] which orchestrates the hardware components by interacting with their action servers to automate the characterization workflow.\nA web-based front end is developed to ensure user-friendly operations of the platform, including both manual control of each individual hardware and execution of the BT.\n\nTo showcase the efficacy of the platform, we integrate a handheld laser induced breakdown\nspectroscopy (LIBS) analyzer to the gantry system and use the platform to perform dense LIBS scanning on the surface of a spodumene-bearing pegmatite core sample with 1071 measurement points, each containing optical emission spectra between 190 nm and 950 nm with 0.03 nm resolution, corresponding to 22800 data channels per measurement. The resulting 2×1072\\times 10^{7} data are automatically quantified using a custom algorithm, yielding spatially-resolved, comprehensive chemical analysis with parts-per-million levels for most chemical elements.\nThe automated LIBS scanning (1) frees researchers from tedious operations,\n(2) accelerates LIBS characterization by at least 3 times the rate of manual operations, and (3) provides crucial information about downstream processing chemistry.\n\nThis paper makes three contributions.\nFirst, a generalized software architecture for building robotic automation systems in scientific laboratory environment is proposed.\nSecond, a low-cost gantry system commonly used in CNC machining is shown to be capable of working as a robot for the automation of SMIS characterization tasks.\nThird, automated dense LIBS scanning using the developed robotic platform and automatic data reduction is achieved.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何降低科学实验室中自动化任务的成本和复杂性？  \n2. 如何实现对矿物和材料样本的自动化表征任务？  \n3. 如何设计通用的软件架构以支持实验室机器人系统的开发？  \n\n【用了什么创新方法】  \n本研究提出了一种通用的软件架构，采用双层（Socket.IO和ROS）动作服务器设计，支持实验室环境中的机器人自动化。通过集成手持激光诱导击穿光谱（LIBS）分析仪，构建了一个低成本的三轴计算机数控（CNC）龙门系统，实现了对矿物样本的自动化化学映射。该平台能够以1520比特每秒的速度扫描样本，生成1071点的密集高光谱图，显著提高了实验效率，减少了人工操作的繁琐性，并提供了重要的下游处理化学信息。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot",
            "authors": "James Avtges,Jake Ketchum,Millicent Schlafly,Helena Young,Taekyoung Kim,Allison Pinosky,Ryan L. Truby,Todd D. Murphey",
            "subjects": "Robotics (cs.RO)",
            "comment": "Published at IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.19525",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19525",
            "arxiv_html_link": "https://arxiv.org/html/2509.19525v1",
            "abstract": "Closed-loop control remains an open challenge in soft robotics. The nonlinear responses of soft actuators under dynamic loading conditions limit the use of analytic models for soft robot control. Traditional methods of controlling soft robots underutilize their configuration spaces to avoid nonlinearity, hysteresis, large deformations, and the risk of actuator damage. Furthermore, episodic data-driven control approaches such as reinforcement learning (RL) are traditionally limited by sample efficiency and inconsistency across initializations. In this work, we demonstrate RL for reliably learning control policies for dynamic balancing tasks in real-time single-shot hardware deployments. We use a deformable Stewart platform constructed using parallel, 3D-printed soft actuators based on motorized handed shearing auxetic (HSA) structures. By introducing a curriculum learning approach based on expanding neighborhoods of a known equilibrium, we achieve reliable single-deployment balancing at arbitrary coordinates. In addition to benchmarking the performance of model-based and model-free methods, we demonstrate that in a single deployment, Maximum Diffusion RL is capable of learning dynamic balancing after half of the actuators are effectively disabled, by inducing buckling and by breaking actuators with bolt cutters. Training occurs with no prior data, in as fast as 15 minutes, with performance nearly identical to the fully-intact platform. Single-shot learning on hardware facilitates soft robotic systems reliably learning in the real world and will enable more diverse and capable soft robots.",
            "introduction": "Soft robots offer the potential for improved adaptability and safety compared to their rigid counterparts due to compliance and material redundancies [1]. However, soft-actuated robots are traditionally difficult to control due to their nonlinear dynamics and high degrees of freedom [2, 3]. Soft-actuated robot dynamics—which may be impractical or impossible to model classically—are often stochastic and vary with a number of factors including strain, actuator fatigue, and even manufacturing processes [4, 5].\n\nData-driven methods such as reinforcement learning (RL) have been used to circumvent many of the modeling difficulties associated with controlling soft robots [6]. Prior applications of RL in this domain have explored various approximations of soft actuator dynamics, including piecewise constant curvature models, Cosserat rod models, and rigid NN-link pendulum approximations [7, 8, 9, 5, 10]. However, lower-dimensional approximations may oversimplify the dynamics of highly nonlinear actuators, which may be compounded by external forces and changes to the dynamics during use. Additionally, models parameterized using these approximations are almost exclusively used to train control policies in simulation, requiring dedicated approaches to address the sim-to-real gap. Existing approaches also often assume quasi-static behavior and focus on relatively simple tasks such as reaching or tracing with continuum arms.\n\nWhen learning approaches are applied to soft robot control, current works often constrain the size or dimensionality of the configuration space to regions with smaller deformations, or where their dynamics are approximately linear [11, 7, 9]. This may include directly constraining actuator outputs, choices of robot orientation, and evaluating control approaches only on undamaged, unloaded actuators.  While these techniques are effective, utilizing the full variety of soft robots’ diverse configuration spaces despite their stochastic and nonlinear dynamics that may be impossible to simulate is paramount for producing behaviors that involve buckling, large deformations, or actuator breakages. Exploiting these unique properties of soft robots has the potential to enable more diverse task learning, biomimetic behaviors, and adaptation to loading conditions or damage.\n\nOur experimental platform is a six degree-of-freedom (DoF), soft-actuated parallel mechanism with a structure similar to that of a Stewart platform [12]. The struts of the platform are motorized soft actuators based on 3D-printed Handed Shearing Auxetics (HSAs) that lengthen and shorten upon the turn of a servo motor [13, 14]. In this work, we balance a sliding puck on the platform, at both the center and at arbitrary coordinates. We utilize deep RL to train closed-loop controllers and benchmark multiple model-based and model-free RL frameworks. Our training occurs fully on-hardware without simulation or bootstrapped data.\n\nLearning in real-time on hardware has its challenges [15, 16], some unique to soft robotics. The limited lifespan of flexible, strain-dependent actuators makes sample efficiency and adaptability valuable attributes  for minimizing and adapting to actuator damage throughout training [9]. Our approach combines a classical kinematics model for a rigid Stewart platform with a deep RL controller to efficiently learn nonlinear control policies for the system. We train all of our policies using single-shot reinforcement learning, a non-episodic problem formulation for continual learning without any environment resets. Learning in a singular real-time episode may be necessary for a variety of tasks well-suited to soft robotics—such as functioning in extreme environments or human interactions—where leveraging prior data, simulations, and reset routines may be undesirable or impossible. While episodic soft robot RL has been implemented on-hardware in works such as [9, 17, 18], and reset-free hardware learning has been implemented in works such as [19, 20], to the best of our knowledge this work is the first to accomplish all of the above in a soft-actuated robot.\n\nAn additional challenge with learning a balancing task in single-shot episodes is that reliably learning control policies is not necessarily an inevitable outcome in reasonable timescales—catastrophic failure modes do exist. While the balancing puck is constrained from falling off the platform, without access to a reset routine, either learned or programmed, the puck can become stuck in a corner during training, providing little to no variance in data required for learning. This is especially  relevant when learning to balance at arbitrary coordinates on the platform, where the reward landscape is parameterized by an observable setpoint that may not be near the platform’s center. Other works conducting soft-actuated RL such as [9, 17, 8, 18] structure their experiments such that providing zero control results in an intervention-free environment reset—such as orienting a continuum arm downwards—stable states such as these do not exist in our experimental platform with an unarticulated sliding puck.\n\nTo overcome this challenge, we employ a curriculum learning approach based on expanding neighborhoods of a known equilibrium: the platform center. With this we reliably learn to balance in a single deployment, without it the task is impossible to accomplish consistently, or at all.\n\nFurthermore, we show how RL can learn to balance the puck despite major changes to the robot’s dynamics during training. We buckle half of the HSAs, introducing singularities, hysteresis, and an out-of-distribution configuration. We also damage the platform by cutting through half of the HSAs with bolt cutters. Despite these alterations, RL attains evaluation performance indistinguishable to the default case.\n\nThe contributions of this work are:\n\nDemonstrations and benchmarking of single-shot learning for dynamic tasks on a soft robot,\n\nDemonstrations and benchmarking of single-shot learning for dynamic tasks on a soft robot,\n\nIntroduction of a curriculum learning procedure to improve single-shot learning outcomes, and\n\nAn illustration of a setting where RL can accommodate changing dynamics such as buckling or breaking actuators during single-shot training.\n\n1. Demonstrations and benchmarking of single-shot learning for dynamic tasks on a soft robot,\n\n2. Introduction of a curriculum learning procedure to improve single-shot learning outcomes, and\n\n3. An illustration of a setting where RL can accommodate changing dynamics such as buckling or breaking actuators during single-shot training.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在软机器人中实现闭环控制以应对非线性动态和高自由度问题？  \n2. 如何提高强化学习在动态平衡任务中的样本效率和一致性？  \n3. 如何在没有模拟或先前数据的情况下实现实时单次学习？  \n\n【用了什么创新方法】  \n本研究采用了一种基于已知平衡点邻域扩展的课程学习方法，以提高软机器人在动态平衡任务中的单次学习效果。通过在真实硬件上进行训练，系统能够在不重置环境的情况下快速学习控制策略。即使在半数执行器失效的情况下，强化学习仍能实现与完整平台相当的性能，展示了软机器人在动态环境下的适应能力和学习效率。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Bioinspired SLAM Approach for Unmanned Surface Vehicle",
            "authors": "Fabio Coelho,Joao Victor T. Borges,Paulo Padrao,Jose Fuentes,Ramon R. Costa,Liu Hsu,Leonardo Bobadilla",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19522",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19522",
            "arxiv_html_link": "https://arxiv.org/html/2509.19522v1",
            "abstract": "This paper presents OpenRatSLAM2, a new version of OpenRatSLAM—a bioinspired SLAM framework based on computational models of the rodent hippocampus. OpenRatSLAM2 delivers low-computation-cost visual-inertial based SLAM, suitable for GPS-denied environments. Our contributions include a ROS2-based architecture, experimental results on new waterway datasets, and insights into system parameter tuning. This work represents the first known application of RatSLAM on USVs. The estimated trajectory was compared with ground truth data using the Hausdorff distance. The results show that the algorithm can generate a semimetric map with an error margin acceptable for most robotic applications.",
            "introduction": "The increasing use of unmanned surface vehicles (USVs) for scientific, military, and commercial purposes requires the development of robust navigation systems [1]. Common applications include oceanographic data collection, oil and gas exploration, environmental surveys, mine countermeasures, and surveillance [2, 3]. To autonomously perform such tasks, a mobile robot must be able to localize itself within its environment [4].\n\nCommon approaches include combining GPS with an inertial measurement unit (IMU) and Kalman filtering algorithms for state estimation in USVs [5]–[6]. However, these methods fail in GPS-denied environments where satellite signals are obstructed [7]. Moreover, GPS signals are vulnerable to various disruptions and cyberattacks, including jamming and spoofing [8].\n\nTo address these limitations, Simultaneous Localization and Mapping (SLAM) is an alternative that enables a vehicle to build a map of its surroundings while estimating its position relative to that map. Many existing SLAM implementations rely on computationally intensive sensors, such as LiDAR or depth cameras. These sensors often require high processing and storage demands, making them less suitable for real-time applications on resource-constrained platforms [9].\n\nMotivated by recent advances in neuroscience, several brain-inspired SLAM systems have been proposed [10]. A pioneering work is the RatSLAM framework, a biologically inspired SLAM algorithm based on computational models of the rodent hippocampus. RatSLAM employs a Continuous Attractor Neural Network (CANN) to construct a cognitive map of an environment using only a low-resolution monocular camera [11]. Compared to probabilistic SLAM approaches, RatSLAM offers reduced computational complexity and efficient memory usage and is well-suited for both indoor and large-scale outdoor mapping.\n\nIn recent years, several RatSLAM-based variants have been proposed [12]. For instance, [13] introduced a MATLAB-based RatSLAM implementation in a rat robot, demonstrating its capability to learn spatial layouts. However, the system’s performance was too slow for real-time operation in large environments. Another approach, OpenRatSLAM, was proposed as an open-source RatSLAM implementation based on the Robot Operating System (ROS) [14]. This version benefits from ROS’s node parallelization and modular integration with diverse robotic architectures [12].\n\nThe emergence of ROS 2 as the dominant middleware for new robotic systems has created integration challenges, as OpenRatSLAM was primarily developed for ROS 1. In this context, xRatSLAM was developed as an extensible, parallel, open-source framework implemented as a C++ library to facilitate the development and testing of RatSLAM algorithm variants [12].\n\nWhile most applications have targeted ground robots, RatSLAM-inspired algorithms have also been explored in other domains. One aerial application, NeuroSLAM, is a neuro-inspired SLAM system with four degrees of freedom (4DoF), based on computational models of 3D grid cells and multilayered head direction cells. It integrates visual and self-motion cues through a dedicated vision system [15]. In underwater environments, two RatSLAM-based systems have been developed: DolphinSLAM [16], a 3D variant, and a more recent system that implements Pose Cells using Spiking Neural Networks (SNNs) [17]. Both were developed using ROS 1 distributions, which are now deprecated and unsupported.\n\nIn summary, the contributions of this work are as follows:\n\nA new version of OpenRatSLAM, implemented using ROS 2 Rolling, referred to as OpenRatSLAM2. This version benefits from ROS 2’s advantages, including improved maintainability and easier integration with modern tools. Additionally, the communication middleware is more robust than ROS 1, providing streamlined transition from simulation to physical robot deployment;\n\nTo the best of our knowledge, this is the first application of RatSLAM to a USV;\n\nA visual-inertial dataset collected using a USV for evaluating SLAM performance in aquatic environments.\n\n1. A new version of OpenRatSLAM, implemented using ROS 2 Rolling, referred to as OpenRatSLAM2. This version benefits from ROS 2’s advantages, including improved maintainability and easier integration with modern tools. Additionally, the communication middleware is more robust than ROS 1, providing streamlined transition from simulation to physical robot deployment;\n\n2. To the best of our knowledge, this is the first application of RatSLAM to a USV;\n\n3. A visual-inertial dataset collected using a USV for evaluating SLAM performance in aquatic environments.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在GPS-denied环境中实现低计算成本的SLAM？  \n2. 如何将RatSLAM算法应用于无人水面车辆（USV）？  \n3. 如何提高SLAM系统在水域环境中的性能评估？  \n\n【用了什么创新方法】  \n本研究提出了OpenRatSLAM2，一个基于ROS 2的生物启发式SLAM框架，利用低分辨率单目相机和视觉惯性传感器进行导航。通过在水域环境中收集的数据集进行实验，结果表明该算法能够生成具有可接受误差范围的半度量地图，适合大多数机器人应用。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion",
            "authors": "Najeeb Ahmed Bhuiyan,M. Nasimul Huq,Sakib H. Chowdhury,Rahul Mangharam",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19521",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19521",
            "arxiv_html_link": "https://arxiv.org/html/2509.19521v1",
            "abstract": "Gesture-based control for mobile manipulators faces persistent challenges in reliability, efficiency, and intuitiveness. This paper presents a dual-hand gesture interface that integrates TinyML, spectral analysis, and sensor fusion within a ROS framework to address these limitations. The system uses left-hand tilt and finger flexion, captured using accelerometer and flex sensors, for mobile base navigation, while right-hand IMU signals are processed through spectral analysis and classified by a lightweight neural network. This pipeline enables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3 manipulator. By supporting simultaneous navigation and manipulation, the framework improves efficiency and coordination compared to sequential methods. Key contributions include a bimanual control architecture, real-time low-power gesture recognition, robust multimodal sensor fusion, and a scalable ROS-based implementation. The proposed approach advances Human–Robot Interaction (HRI) for industrial automation, assistive robotics, and hazardous environments, offering a cost-effective, open-source solution with strong potential for real-world deployment and further optimization.",
            "introduction": "Robot remote control underpins a wide range of transformative technologies, enabling precise operations across domains like space exploration [1, 2, 3], disaster response [4, 5], military operations [6, 7], and assistive robotics [8, 9]. By harnessing human input to guide robotic systems, this field has unlocked capabilities that extend beyond manual reach, adapting to environments where direct intervention is impractical or hazardous. Gesture-based control emerges as a natural evolution of this concept, leveraging intuitive human movements to command robots with minimal training. This approach proves particularly effective in scenarios requiring immediate, instinctual interaction, such as directing medical neuro-arms with laser scalpels for pinpoint accuracy [10] or steering assistive devices to support daily tasks. Its appeal lies in bridging the gap between human intent and robotic action, offering a direct and expressive interface.\n\nBuilding on this foundation, mobile manipulators—systems that pair a mobile base with a robotic arm—amplify the potential of gesture control, combining locomotion with manipulation for versatile applications. In industrial automation, they streamline assembly lines [11, 12]; in healthcare, they assist with patient care [13, 14]; and in search-and-rescue missions, they navigate complex terrains [15]. Their utility extends to extreme conditions: in coal mines, gesture-controlled robots dig under searing heat where workers cannot endure [16]; in bomb defusing, they shield human operators from lethal risks [17]; and in nuclear reactors, they manage radioactive waste without exposing personnel [18]. This synergy of mobility and dexterity makes mobile manipulators a cornerstone of modern robotics. However, gesture-based control, despite its promise, grapples with practical hurdles—unreliable recognition, environmental sensitivity, and resource intensity—that impede its widespread adoption, necessitating innovative solutions.\n\nThese limitations arise from fundamental flaws in existing gesture recognition methods, each posing distinct challenges to effective HRI. Vision-based systems, such as Microsoft’s Kinect or Leap Motion, depend on cameras that falter under variable lighting, occlusions, or when users move beyond a narrow range, disrupting gesture detection [19, 20]. Similarly, approaches employing Deep Neural Networks (DNNs) or Convolutional Neural Networks (CNNs) deliver high accuracy but at the cost of substantial power and computational demands, rendering them impractical for lightweight or real-time systems [21, 22]. Electromyography (EMG)-based methods offer an alternative by capturing muscle signals, with studies like one using the Myo Armband achieving 78.94% accuracy across 10 gestures [23], yet their reliability wanes as muscle fatigue degrades signal quality over time [24]. Such challenges are especially pronounced in rehabilitation and mobility assistance, where intuitive, dependable control is paramount. Stroke, a leading cause of disability per the World Stroke Organization [25], often induces hemiparesis, impairing one side of the body, while paralysis afflicts millions globally, severely restricting mobility. Early robot-assisted therapy with patient-driven devices can enhance recovery, and related efforts, such as a Robot Wheelchair (RW) using sensor-based hand gestures via gloves or handles, underscore the demand for accessible interfaces in these contexts [26].\n\nTo address these shortcomings, a more robust system is essential—one that overcomes environmental constraints, reduces resource demands, and maintains consistency across diverse users and conditions. Wearable sensor-based approaches, enhanced by TinyML and edge computing, provide a compelling solution, offering real-time, low-power performance adaptable to dynamic environments [27, 28, 29]. By integrating sensor fusion—combining data from accelerometers, IMUs, and flex sensors—these systems achieve greater accuracy and robustness, circumventing the pitfalls of vision, neural networks, and EMG [30, 31]. This paradigm shift not only improves technical feasibility but also aligns with the urgent need for intuitive HRI in rehabilitation, hazardous operations, and beyond, setting the stage for advanced robotic control frameworks.\n\nInspired by these insights, we propose a dual-hand gesture control system for a ROS-based mobile manipulator, harnessing TinyML and sensor fusion to deliver seamless, efficient operation. The left hand governs the mobile base using an Arduino Nano equipped with an accelerometer and two flex sensors—tilting to command directional movement (forward, backward, left, right) and flexing to fine-tune acceleration or deceleration. Concurrently, the right hand directs the 7-DOF Kinova Gen3 manipulator via an Arduino Nano 33 BLE Sense with TinyML and an LSM9DS1 IMU, mapping distinct gestures: “Forward-Backward” to a pickup pose, “Left-Right” to placement on the mobile base, “Flat Rectangle” to table placement, “Rectangle” to elevated placement, “Circle” to alternative motion, and “Up-Down” to homing. Built on ROS (ros2-jazzy), the system simulates the mobile base in Gazebo Harmonic and controls the manipulator via RViz with MoveIt!. Our primary contributions include:\n\nA dual-hand gesture control architecture that separates mobile base and manipulator functions, enhancing coordination and natural interaction.\n\nA TinyML-based gesture recognition system for real-time, low-power classification of complex manipulator gestures.\n\nA sensor fusion approach combining accelerometer and flex sensor data for precise mobile base control.\n\nA comprehensive ROS-based simulation framework integrating a custom mobile base and the Kinova Gen3 manipulator.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高移动操控器的手势识别的可靠性和效率。  \n2. 如何实现双手手势控制以同时导航和操作机器人。  \n3. 如何克服现有手势识别方法在环境敏感性和资源消耗上的不足。  \n\n【用了什么创新方法】  \n本文提出了一种基于TinyML和传感器融合的双手手势控制系统，利用左手的倾斜和手指弯曲控制移动底座，右手通过IMU信号进行手势识别。该系统实现了实时、低功耗的手势识别，显著提高了移动操控器的协调性和效率。通过ROS框架的集成，系统在复杂环境中表现出更高的准确性和鲁棒性，推动了人机交互在工业自动化和助理机器人中的应用。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Supercomputing for High-speed Avoidance and Reactive Planning in Robots",
            "authors": "Kieran S. Lachmansingh,José R. González-Estrada,Ryan E. Grant,Matthew K. X. J. Pan",
            "subjects": "Robotics (cs.RO); Distributed, Parallel, and Cluster Computing (cs.DC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19486",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19486",
            "arxiv_html_link": "https://arxiv.org/html/2509.19486v1",
            "abstract": "This paper presents SHARP (Supercomputing for High-speed Avoidance and Reactive Planning), a proof-of-concept study demonstrating how high-performance computing (HPC) can enable millisecond-scale responsiveness in robotic control. While modern robots face increasing demands for reactivity in human–robot shared workspaces, onboard processors are constrained by size, power, and cost. Offloading to HPC offers massive parallelism for trajectory planning, but its feasibility for real-time robotics remains uncertain due to network latency and jitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator must dodge high-speed foam projectiles. Using a parallelized multi-goal A* search implemented with MPI on both local and remote HPC clusters, the system achieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote,  300 km away), with avoidance success rates of 84% and 88%, respectively. These results show that when round-trip latency remains within the tens-of-milliseconds regime, HPC-side computation is no longer the bottleneck, enabling avoidance well below human reaction times. The SHARP results motivate hybrid control architectures: low-level reflexes remain onboard for safety, while bursty, high-throughput planning tasks are offloaded to HPC for scalability. By reporting per-stage timing and success rates, this study provides a reproducible template for assessing real-time feasibility of HPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable pathway toward dependable, reactive robots in dynamic environments.",
            "introduction": "Modern robots are increasingly expected to operate in unstructured and dynamic environments, often in close collaboration with humans. These scenarios demand not only accurate planning but also highly reactive control: a robot must perceive changes and adapt within tens of milliseconds to avoid unsafe or undesirable interactions. Traditionally, such responsiveness has relied on local computing platforms (commodity CPUs or GPUs embedded on the robot or on nearby workstations), which are constrained by size, power, and cost [1, 2]. As robots integrate more resource-intensive artificial intelligence (AI) and machine learning models, these platforms are reaching their limits, creating bottlenecks that threaten truly responsive human–robot interaction (HRI).\n\nHigh-performance computing (HPC), or supercomputing, offers an alternative. HPC systems perform billions of computations per second and excel at large-scale graph search and optimization—problems central to robotics tasks such as inverse kinematics, trajectory planning, and collision avoidance [3, 4]. In principle, HPC can deliver solutions orders of magnitude faster than local compute, enabling behaviours that are more optimal, adaptive, and reactive.\n\nThe main obstacle is latency [5]. HPC resources are typically non-local and accessed via network connections that introduce delays. In time-critical robotics, where milliseconds matter, the central question is whether the raw compute advantage of HPC can overcome communication costs and still enable real-time action [6, 7].\n\nThis paper addresses that question through SHARP—Supercomputing for High-speed Avoidance and Reactive Planning—a proof-of-concept system that evaluates whether HPC offloading can support real-time avoidance. To stress-test responsiveness, we examine a deliberately stringent scenario: a 7-DOF manipulator tasked with dodging high-speed foam projectiles. This setup serves two purposes. First, it creates a controlled, time-critical environment in which HPC planning must deliver trajectories fast enough to influence live robot behaviour. Second, it provides a proxy for broader applications such as collision avoidance in dynamic, human–robot shared workspaces, where comparable millisecond-scale reaction budgets apply.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用高性能计算（HPC）实现机器人在动态环境中的实时反应与规划？  \n2. 网络延迟是否会影响HPC在机器人控制中的实时性和有效性？  \n3. 传统本地计算平台在处理复杂机器人任务时的瓶颈是什么？  \n\n【用了什么创新方法】  \n本研究提出了SHARP系统，通过将高性能计算（HPC）应用于机器人控制，进行高速度规避和反应规划。采用并行化的多目标A*搜索算法，结合MPI在本地和远程HPC集群上进行评估。在应对高速度泡沫投射物的压力测试中，系统实现了平均规划延迟为22.9毫秒（本地）和30.0毫秒（远程），规避成功率分别为84%和88%。这些结果表明，当往返延迟保持在十毫秒以内时，HPC计算不再是瓶颈，从而使机器人能够在低于人类反应时间的情况下实现有效规避。SHARP的结果为混合控制架构提供了动力，低级反应留在机器人本体以确保安全，而高吞吐量的规划任务则可离线处理以实现可扩展性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Crater Observing Bio-inspired Rolling Articulator (COBRA)",
            "authors": "Adarsh Salagame,Henry Noyes,Alireza Ramezani,Eric Sihite,Arash Kalantari",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19473",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19473",
            "arxiv_html_link": "https://arxiv.org/html/2509.19473v1",
            "abstract": "NASA aims to establish a sustainable human basecamp on the Moon as a stepping stone for future missions to Mars and beyond. The discovery of water ice on the Moon’s craters located in permanently shadowed regions, which can provide drinking water, oxygen, and rocket fuel, is therefore of critical importance. However, current methods to access lunar ice deposits are limited. While rovers have been used to explore the lunar surface for decades, they face significant challenges in navigating harsh terrains, such as permanently shadowed craters, due to the high risk of immobilization. This report introduces COBRA (Crater Observing Bio-inspired Rolling Articulator), a multi-modal snake-style robot designed to overcome mobility challenges in Shackleton Crater’s rugged environment. COBRA combines slithering and tumbling locomotion to adapt to various crater terrains. In snake mode, it uses sidewinding to traverse flat or low inclined surfaces, while in tumbling mode, it forms a circular barrel by linking its head and tail, enabling rapid movement with minimal energy on steep slopes. Equipped with an onboard computer, stereo camera, inertial measurement unit, and joint encoders, COBRA facilitates real-time data collection and autonomous operation. This paper highlights COBRA’s robustness and efficiency in navigating extreme terrains through both simulations and experimental validation.",
            "introduction": "Robots have become essential tools in our scientific exploration of space and celestial bodies. Equipped with advanced sensors and designed to withstand severe conditions, they safely explore, sample, and analyze, yielding invaluable insights that would otherwise remain unattainable. Traditionally, robots for space exploration have relied on wheeled locomotion [1]. However, recent innovations have shifted this paradigm. Ingenuity, the Mars Helicopter [2], has successfully demonstrated the first aerial locomotion on another planet, while alternate surface mobility systems such as Dragonfly, EELS, and DuAxel [3, 4, 5] are in advanced stages of development. These solutions are typically tailored to address the locomotion challenges in specific regions of interest, such as recurring slope lineae on Mars for DuAxel or the icy moon Enceladus for EELS.\n\nHowever, some regions of interest, particularly the Moon’s permanently shadowed regions (PSRs), are still unexplored due to limitations of existing surface mobility solutions. In this work, we introduce our bio-inspired mobile robot, COBRA, Crater Observing Bio-inspired Rolling Articulator, designed to address challenges unmet by current solutions for energy-efficient exploration of steep and unknown craters in the Moon’s PSRs. As a case study, we focus on Shackleton Crater (see Fig. 2) at the lunar South Pole, a site of immense scientific potential for water ice discovery [6], but significant challenges for existing exploration methods. We present the envisioned mission scenario and the locomotion solution that led to COBRA winning the 2022 Breakthrough, Innovative, and Game-changing (BIG) Idea Challenge [7], an initiative under NASA’s Space Technology Mission Directorate’s (STMD’s) Game Changing Development Program (GCD), for alternative lunar mobility solutions. This report summarizes the design, dynamical modeling, contact-rich and optimization-based gait discovery, as well as the numerical and experimental evaluation of COBRA’s mobility in unstructured, contested environments. Specifically, the technical contributions of this work are:  (1) Hardware design, including head, tail, and body module mechanical and electronics design. (2) Design of the head-tail docking mechanism to substantiate transitions between crawling and tumbling. (3) Numerical modeling of crawling and tumbling locomotion feats based on Lagrange and mixed Hamilton-Lagrange (partitioned state-space model for tumbling) dynamics. (4) Contact-implicit optimization-based gait discovery and joint motion planning. (5) Simulation and experimental validation of hex-ring tumbling, spiral tumbling, sidewinding, lateral rolling, and vertical undulation. (6) Demonstration of field-tested locomotion modes in dusty, steep, and bumpy environments, with self-sustained dust mitigation. (7) Demonstration of full deployment from a lander, including unmanned transitions between various locomotion modes in experiments.\n\nWith the Artemis program, NASA and collaborating space agencies aim to revitalize lunar and space exploration. One Artemis objective is to create a sustainable human base camp on the Moon with the hope to then propel further missions to Mars and beyond. Accomplishing such a feat is contingent on In-Situ Resource Utilization (ISRU) on the Moon [8]. One resource of significant interest is lunar water ice, which can potentially supply drinking water, oxygen, and rocket propellant. However, the means to access the ice deposits on the Moon still require additional development [9, 10, 11]. In 2018, NASA confirmed the presence of water ice on the Moon’s poles, concentrated chiefly in PSRs [12]. The near-permanent lack of sunlight in these regions results in extremely low temperatures (as low as -238 °C) and allows for the accumulation of water ice and other volatiles [13].\nOne such PSR is Shackleton Crater.\n\nWhile the presence of water ice in Shackleton Crater is evident [6], there are no precise measurements of its quantity or chemical composition. This detailed information, along with topographic maps of crater terrain, is critical to initiate targeted mining operations for ISRU. However, acquiring these measurements requires proximate investigations in extreme lunar environments that pose significant mobility challenges to exploration platforms.\n\nHigh Porosity Regolith Surface: The surface of the lunar South Pole is characterized by high porosity regolith that poses significant locomotion challenges [14]. Due to the high porosity, traditional wheeled rovers suffer sinkage and slipping, reducing energy efficiency and increasing the risk of immobilization. The high porosity lunar dust is also abrasive and invades machinery due to its particulate nature [15].\n\nTerrain Profile: Traversing the immense slopes to reach all areas of scientific interest inside the crater also poses significant challenges. Shackleton Crater is a massive geographic feature 21 kilometers in diameter. The crater slope leading to the crater floor has an average slope of 30.5 degrees and covers a horizontal distance of approximately 8 kilometers. This steep crater slope is difficult to both ascend and descend. On the descent, there is limited traction due to the lower lunar gravity further reducing normal force, and the regolith substrate being more prone to yield, leading to slipping and sinkage.\n\nThis mobility challenge also extends to the uneven crater floor and surrounding areas. The crater slope and floor have a root mean square (RMS) surface roughness of approximately 1 meter. This makes it difficult for wheeled systems to traverse, as the height of the obstacle they can overcome is limited by their wheel diameter. Boulder fields outside the crater, the unknown mechanical properties of regolith with ice, and a lack of detailed topography of the crater floor further raise the requirements for an adaptable system.\n\nA large of number of studies have investigated the mechanics of wheeled and legged locomotion in such conditions, using both analytical terramechanics models and experimental proxies. For instance, Ishigami et al. [16] and Shrivastava et al. [17] analyze wheel-soil interactions using lunar simulants, including deformable media like poppy seeds, to understand sinkage and traction loss. Similarly, recent work by Kolvenbach et al. [18] and Karsai et al. [19] explores how legged robots navigate granular slopes, addressing challenges in slip, footing stability, and dynamic control. However, the systematic consideration of substrate phenomena–such as mass wasting, sinkage, and traction loss–during inclined traversal remains largely unexplored.\n\nLack of sunlight: Shackleton Crater is a PSR, leading to two challenges: power generation and near-absolute zero temperature. Traversal of the crater is limited due to the lack of solar power. Therefore, large distances must be covered with minimal power consumption, or systems must rely on heavy power generation mechanisms such as radioisotope thermoelectric generators (RTGs). Low temperatures interfere with mechanisms that rely on lubricants and liquid batteries. This results in less efficient power systems.\n\nTo overcome locomotion challenges in the Moon’s hostile environment, a robotic system must be designed to minimize sinkage and slippage, while incorporating features to prevent immobilization. It must traverse long distances in an energy-efficient manner while maintaining passive and active regolith mitigation strategies and be able recover from, or continue to operate with, component failures.\n\nCurrent state-of-the-art wheeled rovers [20, 21, 22, 23] shown in Fig. 1 struggle on steep inclines due to inadequate traction on the porous regolith and decreased stability from the reduced normal force. Hybrid systems such as SherpaTT [24] and Scarab [25] partially address this issue by incorporating articulated legs that can lift the wheels off the ground to reposition them. More advanced multi-modal platforms like Robosimian [26] combine wheels with fully articulated legs, enabling both wheeled and legged locomotion, which improves steep slope traversal but remains slow and energy-intensive. An alternative approach is to use tether-assisted mobility, implemented in the DuAxel system [27], where one half of the rover anchors itself while the other rappels down steep terrain using a tether. While effective in certain terrains, the scale and steepness of craters such as Shackleton limit the practicality of tethered solutions.\n\nLegged systems, such as ETH Zurich’s Spacebok [40], JPL’s Llama [38], and ANYmal [39], offer another solution by allowing precise control of foot placement, enabling stable locomotion on loose surfaces. Despite their adaptability, these systems are generally energy-intensive. To address this, recent works like SpaceHopper [37] have explored hopping-based locomotion to leverage the Moon’s low gravity for energy-efficient travel over long distances. For steeper slopes, climbing robots equipped with micro-grippers, such as JPL’s LEMUR 3 [41], have been developed, but these designs are tailored for hard, rocky surfaces and are unsuitable for the soft, granular lunar regolith.\n\nFlying systems can traverse large distances quickly, as demonstrated by Ingenuity [48] on Mars. NASA’s Dragonfly [3], the Mars Science Helicopter [29] and the Sample Return Helicopter [36] are further exploring flight-based solutions for planetary exploration. However, replicating this approach on the Moon, which lacks an atmosphere, would require the use of ion or hydrogen-based propulsion systems. These technologies are extremely energy-intensive, making frequent recharging or carrying significant fuel reserves necessary for sustained operation. Thruster-assisted multi-modal designs like LEONARDO [32], which combine hopping and flight, present a potential compromise, but their exhaust may alter the chemical composition of the lunar regolith, complicating scientific investigations. Similarly, other multi-modal systems such as the M4 [33] and JPL’s Rollocopter [34], which combine wheeled mobility and flight, would encounter similar challenges.\n\nUnconventional morphologies, such as the screw-driven snake robot EELS [4] designed for icy terrains, and micro-scale jumping robots like Frogbot [45], offer new paradigms for locomotion. To achieve safe locomotion down steep crater walls, a particularly promising approach is seen in JPL’s Hedgehog [46], a tumbling robot that uses internal flywheels to generate momentum. Tumbling locomotion takes advantage of gravity to descend slopes, eliminating the need for active actuation and resulting in highly energy-efficient traversal. Despite its long history, tumbling has seen limited application in space exploration, leaving its full potential largely unexplored. Given its simplicity and efficiency, it presents an exciting opportunity for navigating the Moon’s challenging terrain.\n\nThe merits and limitations of tumbling robots are well documented. Passive systems like NASA/JPL’s Mars Tumbleweed Rover [49] are energy-efficient, making them ideal for remote exploration where energy conservation is critical. Active rolling spherical robots, such as MIT’s Kickbot [50], with their low center of gravity and omnidirectional mobility, are robust against external perturbations and excel in tight spaces. However, tumbling robots face challenges. Passive systems often sacrifice controllability for energy efficiency, relying on their shape and external forces for maneuvering. Additionally, rolling robots typically use their entire body for locomotion, leaving no stable platform for sensors, which complicates tasks like localization and perception.\n\nEarly rolling robots like Rollo [51] and the Spherical Mobile Robot (SMR) [52] used a spring-mass system with a driving wheel to create a mass imbalance for movement, but they were unreliable, as the driving wheel often lost contact with the sphere. Furthermore, significant central weight was required to generate enough inertia for propulsion. Other notable designs include the University of Pisa’s Sphericle [53] and Spider Rolling Robot (SRR) [54]. Sphericle used a car inside a sphere to drive the structure, but relied on gravity to keep the car wheels in contact with the inside of the sphere. Large perturbations led by mobility on rough terrain could dislodge the car and incapacitate the robot.\n\nA more precise tumbling method involves shifting internal weights to control the center of mass, as seen in the University of Michigan’s Spherobot [55] and University of Tehran’s August Robot [56], though these systems are not energy-efficient due to the added weight required.\n\nA more energy-efficient means of positioning the center of gravity for tumbling is by using deformable structures. Successful examples [57, 58, 59, 60, 61] can be identified that have attempted rolling by articulated structural designs that allow such deformation. Notable examples are Ritsumeikan University’s Deformable Robot [62], and Ourobot [63] with an articulated closed-loop structure.\n\nNASA’s Hedgehog [46] is a notable example developed specifically for space applications, derived from ETH’s Cubli [47]. It combines hopping, oblique hopping, tumbling, escape motion, and yawing (pointing) to traverse asteroids and comets. While it offers robust locomotion on a flat surface, because of its simple operation principle, Hedgehog has a few serious shortcomings that make it unsuitable for long-distance mobility. First, it can get stuck in soft terrain due to its jagged shape (a cube with protusions at its corners). A tornado (fast rotations around body axes) maneuver is proposed by its designers to escape; however, any fast body movements in these conditions can aggravate the situation. Next, because of its cubic shape, Hedgehog’s tumbling can be rough compared to other smoother circular geometries. Last, Hedgehog’s actuation relies on spinning three internal flywheels with considerable inertia to produce reaction torque. This actuation can be very costly, particularly for larger versions of Hedgehog. Therefore, if large-payload systems are considered in future NASA Moon missions, Hedgehog may suffer from scalability issues.\n\nOur proposed system, COBRA, is a snake-inspired multimodal rover designed for challenging terrain exploration (Fig. 3). COBRA combines the advantages of tumbling locomotion with the morphology afforded by snake robots to effectively address the locomotion challenges of steep crater walls as well as the uneven terrain of the crater floor. COBRA is lightweight at just 7.11 kg, with a compact diameter of 10 cm and a length of 1.7 m. Its eleven actuated degrees of freedom (DOF) enable it to morph its body shape to achieve various locomotion modes that can adapt to unpredictable and rough terrain. To achieve tumbling locomotion, COBRA raises and links its head and tail modules together to transform into a wheel-like structure. We identify these two distinct configurations as (1) Snake Configuration, where the head and tail is unconnected, and (2) Hex-Ring Configuration, where the head and tail are connected to form a closed loop.\n\nIn Snake Configuration, COBRA employs sidewinding and other slithering gaits to move efficiently across flat or uphill terrain. Sidewinding is a gait used by snakes to traverse loose or slippery surfaces like sand, and is particularly relevant for traversing lunar regolith, which shares similar properties. Variations of sidewinding locomotion have been shown to be fast and efficient, able to push off rocks and other obstacles to minimize energy consumption and increase traction [64, 65, 66]. Further, the symmetric snake-like morphology makes it excellent at traversing uneven and unknown terrain by virtue of not needing to reorient itself. The system’s weight is distributed along the entire length of its body which mitigates sinkage, and also allows for a large payload capacity, which can carry sensors such as a spectrometer to determine the concentration of hydrogen in the lunar regolith. By swinging its joints back and forth, the robot can also dig into the ground, exposing scientific samples below the surface or clearing material away if the system is stuck. It can also lift sections of its body into the air to overcome obstacles, climb out of holes, and aim instruments for navigation or communication.\n\nFor tumbling down slopes, COBRA enters the Ring Configuration and shifts its center of gravity to initiate tumbling. Through manipulation of its posture within the ring configuration using its joint actuators, COBRA can actively steer itself with minimal effort, allowing it to track desired paths to reach points of interest along the slope or avoid obstacles. These capabilities are not limited to lunar exploration. As a ground based system, it is a powerful tool for exploration of similar environments on Mars, such as Valles Marineris, a canyon system that features large, sloped terrain that our system can efficiently travel down to perform in-situ measurements in channels that potentially contain water.\n\nSome examples of tumbling platforms employing active deformation have been discussed above. COBRA distinguishes itself from these platforms in two significant ways: (1) Dynamic Tumbling Locomotion: As a tumbling system, it is field-tested and capable of dynamic tumbling locomotion with steering in two dimensions (2) Multimodal Locomotion and Manipulation: COBRA is a multimodal robot that employs the Snake Configuration to perform various locomotion and manipulation tasks on flat ground. This feature extends its operational scope beyond that of typical tumbling robots, enabling it to address diverse mission requirements.\n\nPrevious studies have extensively explored snake locomotion across various robotic platforms [67, 68, 69, 70, 71, 72]. COBRA builds on these works by implementing a broad range of snake gaits, achieving untethered operation, and integrating these capabilities with tumbling locomotion. This enables COBRA to operate effectively across a wide range of environments of various slopes and roughness, making it highly suitable for the exploratory nature of the proposed task.\n\nA comparable platform of note is NASA JPL’s EELS snake robot [4], which employs a screw propulsion mechanism along the length of its body, enabling locomotion on low-friction ice surfaces such as those found on Europa. However, this mechanism is less suited for granular and rocky terrains like those encountered on the Moon. COBRA is designed with locomotion strategies tailored for high-roughness surfaces, utilizing slithering and sidewinding gaits for flat or slightly inclined terrain and tumbling locomotion for rapid movement across steep slopes. This combination makes COBRA uniquely suited for environments where rapid elevation changes and substrate variability are critical considerations, for example lunar sites such as Shackleton Crater.\n\nHere we present the mission scenario envisioned for COBRA’s operation considering the case study of Shackleton Crater. COBRA will be deployed from a Commercial Lunar Payload Services (CLPS) lander near the edge of Shackleton Crater, as illustrated in Fig. 2. Following deployment, the system will navigate to the slope of the crater using sidewinding locomotion. In this mode, segments of the robot’s body are lifted and shifted forward, while others remain in contact with the ground, minimizing shear forces at contact points and preventing slippage on the soft regolith.\n\nNext, COBRA leverages the Moon’s partial gravity to tumble down the steep slope in its wheel-like configuration. Transitioning from the snake configuration, the system shifts its internal weight onto the slope to initiate tumbling. During descent, the joints remain static, conserving energy.\n\nThe system will tumble incrementally, halting every 500 meters to disconnect its head and tail for in-situ measurements. The scientific payload, a spectrometer housed in the tail, can be positioned using COBRA’s joints. This allows the system to create a detailed hydrogen concentration map at various depths within the crater. After collecting data, COBRA transmits the results to a lunar orbiter via a radio antenna in its head module, eliminating the need for the system to climb out of the crater for data transmission.\n\nFollowing communication at the sampling location, COBRA reforms its tumbling structure, tumbles another 500 meters, and repeats the process. Upon reaching the flatter center of Shackleton Crater, it will switch to sidewinding to continue its exploration and data collection until its power is exhausted.\n\nBased on this mission scenario, we come up with four objectives that we focus on for this work. 1) Locomotion over flat or slightly sloped ground using snake-like gaits. 2) Locomotion down slopes using two tumbling configurations. 3) Multi-modal operation allowing seamless switching between the modes of locomotion with no human contact with the robot. 4) Modular design that is robust to potential failures that can occur during remote operation in a space mission.\n\nFor this prototype, we focused on developing COBRA’s locomotion capabilities and designing a robust, modular system. While not using space-grade materials, the prototype is built to withstand outdoor terrestrial conditions. The following sections detail our development and testing process towards achieving these objectives.",
            "llm_summary": "大模型总结失败",
            "llm_score": 0,
            "llm_error": "API 状态码异常：403，响应：{\"error\":{\"message\":\"免费API限制模型输入token小于4096，如有更多需求，请访问 https://api.chatanywhere.tech/#/shop 购买付费API。The number of prompt tokens for free accounts is limited to 4096. If you have additional requirements, please visit https://api.chatanywhere.tech/#/shop to purchase a premium key.(当前请求使用的ApiKey: sk-8l9****i4zt)【如果您遇到问题，欢迎加入QQ群咨询：1048463714】\",\"type\":\"chatanywhere_error\",\"param\":null,\"code\":\"403 FORBIDDEN\"}}"
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "CU-Multi: A Dataset for Multi-Robot Collaborative Perception",
            "authors": "Doncey Albin,Daniel McGann,Miles Mena,Annika Thomas,Harel Biggie,Xuefei Sun,Steve McGuire,Jonathan P. How,Christoffer Heckman",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19463",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19463",
            "arxiv_html_link": "https://arxiv.org/html/2509.19463v1",
            "abstract": "A central challenge for multi-robot systems is fusing independently gathered perception data into a unified representation. Despite progress in Collaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of dedicated multi-robot datasets. Many evaluations instead partition single-robot trajectories, a practice that may only partially reflect true multi-robot operations and, more critically, lacks standardization, leading to results that are difficult to interpret or compare across studies. While several multi-robot datasets have recently been introduced, they mostly contain short trajectories with limited inter-robot overlap and sparse intra-robot loop closures. To overcome these limitations, we introduce CU-Multi, a dataset collected over multiple days at two large outdoor sites on the University of Colorado Boulder campus. CU-Multi comprises four synchronized runs with aligned start times and controlled trajectory overlap, replicating the distinct perspectives of a robot team. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined ground-truth odometry. By combining overlap variation with dense semantic annotations, CU-Multi provides a strong foundation for reproducible evaluation in multi-robot collaborative perception tasks. The dataset, support code, and updates are available at https://arpg.github.io/cumulti.",
            "introduction": "Multi-robot systems significantly enhance capabilities across diverse domains, particularly in large-scale environments, by accelerating exploration through distributed sensing and collaborative decision-making [7]. A central challenge in realizing these advantages lies in fusing perception data collected independently by multiple robots into a unified global representation, complicated by spatial and temporal misalignment. This challenge is further compounded in many practical scenarios where external positioning methods (e.g. GPS, motion capture) are impractical, unreliable, or hazardous. Multi-robot Collaborative SLAM (C-SLAM) algorithms are typically first developed and validated offline using datasets before doing field tests in real-world conditions, where system-level issues often arise. Consequently, the availability of realistic, well-structured multi-robot datasets is essential for supporting reproducible research and bridging the gap between offline development and real-world deployment.\n\nSeveral multi-robot datasets have recently been introduced for C-SLAM verification [3, 6, 1]. These datasets include synchronized multi-agent sensor data collected across both indoor and/or outdoor environments. While these datasets represent strong starting points, there remains a need for longer-trajectory multi-robot datasets with explicitly varied overlap (see Table I). Despite recent advancements in multi-robot datasets, there remains a prevalent practice of artificially segmenting a single trajectory into multiple parts to simulate a multi-robot scenario for verification [8]. Single-robot SLAM datasets still offer a few benefits over many of the existing multi-robot datasets, such as large trajectories with multiple loop closures, as well as the availability of both camera and LiDAR semantics [8, 9]. However, without careful consideration, arbitrary segmentation may not accurately represent realistic observational overlap typically encountered in multi-robot operations [10].\n\nIn this paper, we introduce CU-Multi, a dataset designed to support the evaluation of methods relevant to multi-robot perception. These methods include C-SLAM, LiDAR and visual data association, and multi-session LiDAR-based place recognition. CU-Multi offers the following key features:\n\nA multi-robot dataset consisting of two large-scale environments, each with four robots. Our dataset contains a total of eight diverse trajectories, spanning a combined length of 16.7 km across the CU Boulder campus (Figure 1).\n\nSystematically varied trajectory overlaps and a rendezvous-based trajectory design, enabling evaluation of multi-robot perception tasks under different levels of observational redundancy and variation.\n\nRefined ground truth poses using RTK GPS, a digital elevation model, lidar-inertial SLAM, and a highly accurate scan-matching algorithm, calculated at each LiDAR timestamp.\n\nSemantic labels for all LiDAR scans, dataset tools for interacting with the data, as well as initial benchmarks on a well-known C-SLAM and LiDAR place recognition method to demonstrate dataset usability and utility of the provided tools.",
            "llm_summary": "【关注的是什么问题】  \n1. 多机器人系统在融合独立收集的感知数据时面临的挑战。  \n2. 现有多机器人数据集缺乏长轨迹和标准化，导致评估结果难以比较。  \n3. 需要一个结构良好的多机器人数据集以支持可重复的研究。  \n\n【用了什么创新方法】  \nCU-Multi数据集在两个大型户外环境中收集，包含四个同步运行的机器人，具有控制的轨迹重叠和多样化的重叠变化。数据集提供RGB-D传感、RTK GPS、语义LiDAR和精细的真实轨迹，支持多机器人协作感知任务的评估。通过丰富的语义注释和系统化的轨迹设计，CU-Multi为多机器人感知方法的评估提供了坚实的基础，并展示了其在C-SLAM和LiDAR位置识别方法上的初步基准效果。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Self-evolved Imitation Learning in Simulated World",
            "authors": "Yifan Ye,Jun Cen,Jing Chen,Zhihe Lu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19460",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19460",
            "arxiv_html_link": "https://arxiv.org/html/2509.19460v1",
            "abstract": "Imitation learning has been a trend recently, yet training a generalist agent across multiple tasks still requires large-scale expert demonstrations, which are costly and labor-intensive to collect.\nTo address the challenge of limited supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework that progressively improves a few-shot model through simulator interactions.\nThe model first attempts tasks in the simulator, from which successful trajectories are collected as new demonstrations for iterative refinement.\nTo enhance the diversity of these demonstrations, SEIL employs dual-level augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model to collaborate with the primary model, and (ii) Environment-level, introducing slight variations in initial object positions.\nWe further introduce a lightweight selector that filters complementary and informative trajectories from the generated pool to ensure demonstration quality.\nThese curated samples enable the model to achieve competitive performance with far fewer training examples.\nExtensive experiments on the LIBERO benchmark show that SEIL achieves a new state-of-the-art performance in few-shot imitation learning scenarios.\nCode is available at https://github.com/Jasper-aaa/SEIL.git.",
            "introduction": "Imitation Learning (IL) [1, 2, 3, 4] has demonstrated remarkable success across a variety of tasks by leveraging large-scale expert demonstration datasets [5].\nHowever, collecting such datasets is often time-consuming, labor-intensive [6], and in certain domains (e.g., surgical practice) can even be impractical [7].\nThis motivates the study of IL under limited supervision, which is both a more practical and challenging setting.\nHowever, learning from only a few demonstrations typically results in a substantial performance drop. For instance, in the one-shot setting, a train-from-scratch Diffusion Policy [2] achieves only 0.8% success rate, compared to 50.5% with full training.\n\nInspired by the empirical success of Reinforcement Learning (RL) [8, 9, 10, 11, 12], where agents explore through extensive trial-and-error within simulators, a natural solution is to leverage simulation environments to autonomously generate additional demonstrations for fine-tuning few-shot imitation policies.\nThat is, a policy is initially trained on a limited set of expert demonstrations and then deployed in a simulator, where it interacts with the environment and records successful trajectories based on simple success/failure feedback.\nOnce a sufficient number of demonstrations across tasks are collected, the policy is refined using standard imitation learning updates on the newly acquired data.\nHowever, this approach gives rise to two critical challenges:\n(i) How can we ensure sufficient diversity in the simulated demonstrations to support iterative learning?\n(ii) How can we identify and select the most informative samples to optimize model performance while maintaining computational efficiency?\n\nIn this paper, we investigate a few-shot imitation learning setting, address the aforementioned two key challenges and propose Self-Evolved Imitation Learning (SEIL), a novel framework that leverages simulator to enable policy self-evolution from limited expert demonstrations, often with significant improvements as shown in Figure 1.\nSEIL is built upon two core components: dual-level augmentation, comprising model-level and environment-level strategies, for generating diverse trajectories, and a sample selector tailored to identify and retain the most informative demonstrations for efficient policy refinement.\nNotably, SEIL supports multiple rounds of interaction with the simulator, enabling the policy to evolve progressively over time.\nSpecifically, for model-level augmentation, SEIL introduces an auxiliary model to conduct additional rollouts alongside the primary model.\nTo avoid incurring extra training overhead, particularly important in a multi-stage evolution setting where cost accumulates over iterations, this auxiliary policy is implemented as an Exponential Moving Average (EMA) of the main model.\nThe EMA model maintains diversity in the policy space without the need for separate training, thereby supporting efficient and scalable demonstration generation.\nIn addition to model-level augmentation, we introduce environment-level augmentation, which enables the policy to interact with the simulator under diverse initial conditions.\nThis variation in starting states enhances environmental diversity, thereby enriching the recorded demonstrations and improving the robustness of policy learning.\n\nAfter generating a pool of diverse demonstrations via dual-level augmentation, we introduce a lightweight selector to identify informative samples.\nThe efficiency nature of this selector comes from taking as input only an initial image and a trajectory, rather than full video sequences.\nTrained on few-shot data via a trajectory classification task, the selector learns category-specific feature patterns.\nOnce trained, the selector is frozen and applied to all recorded demonstrations.\nSamples with the lowest confidence, i.e., those most distinct from expert demonstrations, are selected for policy training.\n\nOur contributions are summarized as follows.\n\nWe investigate a few-shot imitation learning setting and propose Self-Evolved Imitation Learning (SEIL), a novel framework that leverages simulation environments to enable policy self-evolution from limited expert demonstrations.\n\nWe propose dual-level augmentations, i.e., model-level and environment-level augmentation, to guarantee the diversity of demonstration generation.\n\nWe propose a lightweight selector to identify the most informative demonstrations, enhancing both policy performance and training efficiency.\n\nExtensive experiments demonstrate that SEIL can effectively evolve weak few-shot trained models, achieving substantial improvements, e.g., a 217.3% performance growth over the 1-shot baseline on Libero-Long.\n\n1. We investigate a few-shot imitation learning setting and propose Self-Evolved Imitation Learning (SEIL), a novel framework that leverages simulation environments to enable policy self-evolution from limited expert demonstrations.\n\n2. We propose dual-level augmentations, i.e., model-level and environment-level augmentation, to guarantee the diversity of demonstration generation.\n\n3. We propose a lightweight selector to identify the most informative demonstrations, enhancing both policy performance and training efficiency.\n\n4. Extensive experiments demonstrate that SEIL can effectively evolve weak few-shot trained models, achieving substantial improvements, e.g., a 217.3% performance growth over the 1-shot baseline on Libero-Long.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在有限的专家演示下进行有效的模仿学习？  \n2. 如何确保模拟演示的多样性以支持迭代学习？  \n3. 如何识别和选择最具信息量的样本以优化模型性能？  \n\n【用了什么创新方法】  \n提出了自我进化模仿学习（SEIL）框架，通过模拟器交互逐步改进少量模型。采用双层增强策略，分别在模型和环境层面生成多样化轨迹，并引入轻量级选择器过滤出有用的演示样本。实验结果表明，SEIL在少样本模仿学习场景中达到了新的最先进性能，尤其在Libero-Long基准上实现了217.3%的性能增长。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation",
            "authors": "Jason Chen,I-Chun Arthur Liu,Gaurav Sukhatme,Daniel Seita",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19454",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19454",
            "arxiv_html_link": "https://arxiv.org/html/2509.19454v1",
            "abstract": "Training robust bimanual manipulation policies via imitation learning requires demonstration data with broad coverage over robot poses, contacts, and scene contexts.\nHowever, collecting diverse and precise real-world demonstrations is costly and time-consuming, which hinders scalability. Prior works have addressed this with data augmentation, typically for either eye-in-hand (wrist camera) setups with RGB inputs or for generating novel images without paired actions, leaving augmentation for eye-to-hand (third-person) RGB-D training with new action labels less explored. In this paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation (ROPA), an offline imitation learning data augmentation method that fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D observations of novel robot poses. Our approach simultaneously generates corresponding joint-space action labels while employing constrained optimization to enforce physical consistency through appropriate gripper-to-object contact constraints in bimanual scenarios. We evaluate our method on 5 simulated and 3 real-world tasks. Our results across 2625 simulation trials and 300 real-world trials demonstrate that ROPA outperforms baselines and ablations, showing its potential for scalable RGB and RGB-D data augmentation in eye-to-hand bimanual manipulation.\nOur project website is available at: https://ropaaug.github.io/.",
            "introduction": "Bimanual manipulation is critical for a wide range of daily tasks [1] such as lifting large objects [2, 3], handling deformable objects [4, 5, 6, 7], and opening containers [8, 9].\nThese activities require coordinated motion of both arms and awareness of nearby objects and surfaces. A third-person camera can capture both arms and the surrounding scene from one viewpoint, which is appealing for vision-based imitation learning [10].\nRecent imitation learning approaches show that large demonstration datasets yield increasingly general bimanual policies [11, 12, 13], but collecting sufficiently large and diverse action-labeled data is costly, which limits scalability.\n\nData augmentation has emerged as a promising strategy to address this bottleneck, particularly in single-arm manipulation, where novel views and corresponding action labels can be synthesized offline [14].\nHowever, extending these techniques to bimanual manipulation, depth images, and beyond wrist-mounted (eye-in-hand) viewpoints [15, 16], introduces new difficulties. These include enforcing scene-wide visual consistency across two robot arms, preserving action label correctness, and handling the higher degrees-of-freedom (DOF).\n\nIn this paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation (ROPA), a data augmentation method for vision-based imitation learning of bimanual manipulation. Our key insight in ROPA is to adapt Pose-Guided Person Image Synthesis techniques [17] (originally developed for human pose generation) by conditioning a diffusion model on robot joint configurations. This enables the diffusion model to yield realistic and consistent third-person views for varied robot arm poses. We maintain a tight correspondence between augmented visual observations and valid actions, so that policies trained on the augmented dataset remain executable on real hardware.\n\nWe evaluate ROPA across 5 simulated and 3 real-world bimanual manipulation tasks, based on the PerAct2 [18] benchmark in simulation and a physical bimanual UR5 setup. See Figure 1 for an example rollout. Compared to an Action Chunking with Transformers (ACT) [19] baseline trained only on raw demonstrations, data augmentation using ROPA enables ACT to achieve higher success rates across bimanual manipulation tasks that require coordinated motion and fine-grained precision.\nBy enabling third-person vision-based data augmentation for bimanual systems, ROPA scales training data and broadens coverage without extra manual data collection.\n\nThe contributions of this paper include:\n\nROPA, a novel pose-guided image synthesis method for offline data augmentation in bimanual manipulation, which supports learning from both RGB and RGB-D data.\n\nDepth image synthesis that generates depth maps consistent with the augmented joint positions and RGB images.\n\nAction-consistent augmentation that outputs images and joint-space labels, with constraints to ensure feasibility.\n\nSimulation and real-world experiments showing that bimanual policies with ROPA achieve significantly improved performance over baseline methods and ablations.\n\n1. ROPA, a novel pose-guided image synthesis method for offline data augmentation in bimanual manipulation, which supports learning from both RGB and RGB-D data.\n\n2. Depth image synthesis that generates depth maps consistent with the augmented joint positions and RGB images.\n\n3. Action-consistent augmentation that outputs images and joint-space labels, with constraints to ensure feasibility.\n\n4. Simulation and real-world experiments showing that bimanual policies with ROPA achieve significantly improved performance over baseline methods and ablations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效生成多样化的机器人姿态以增强双手操作的模仿学习数据？  \n2. 如何在数据增强过程中保持视觉一致性和动作标签的正确性？  \n3. 如何在不增加手动数据收集成本的情况下扩展训练数据的覆盖范围？  \n\n【用了什么创新方法】  \n提出了一种名为ROPA的合成机器人姿态生成方法，通过微调Stable Diffusion模型，生成第三人称RGB和RGB-D观察数据。该方法在生成图像的同时，确保动作标签的一致性，并通过约束优化来维持物理一致性，确保夹持器与物体的接触约束。经过在5个模拟和3个真实任务上的评估，ROPA在2625次模拟试验和300次真实试验中表现优于基线和消融实验，展示了其在双手操作中的数据增强潜力。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames",
            "authors": "Alessandro Saviolo,Jeffrey Mao,Giuseppe Loianno",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19452",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19452",
            "arxiv_html_link": "https://arxiv.org/html/2509.19452v1",
            "abstract": "Search and rescue operations require unmanned aerial vehicles to both traverse unknown unstructured environments at high speed and track targets once detected. Achieving both capabilities under degraded sensing and without global localization remains an open challenge. Recent works on relative navigation have shown robust tracking by anchoring planning and control to a visible detected object, but cannot address navigation when no target is in the field of view.\nWe present HUNT (High-speed UAV Navigation and Tracking), a real-time framework that unifies traversal, acquisition, and tracking within a single relative formulation. HUNT defines navigation objectives directly from onboard instantaneous observables such as attitude, altitude, and velocity, enabling reactive high-speed flight during search. Once a target is detected, the same perception–control pipeline transitions seamlessly to tracking. Outdoor experiments in dense forests, container compounds, and search-and-rescue operations with vehicles and mannequins demonstrate robust autonomy where global methods fail.\nVideo: http://bit.ly/4n5Tp5D",
            "introduction": "Unmanned Aerial Vehicles (UAVs), especially quadrotors, have become essential tools in Search-and-Rescue (SAR) operations, where their maneuverability and speed enable rapid deployment in unstructured, GPS-denied environments. These missions require two critical capabilities: the ability to safely traverse unknown terrain at high speed to search for victims or objects of interest, and the ability to reliably track and follow detected targets once they are identified. Achieving both in degraded sensing and unknown environments remains a fundamental challenge in robotics.\n\nTraditional autonomy relies on global navigation anchored by GPS, Visual-Inertial Odometry (VIO), or Simultaneous Localization and Mapping (SLAM) [1]. These methods degrade in exactly the conditions most critical to SAR: dense forests, collapsed buildings, or urban canyons where GPS is degraded and visual features are sparse, dynamic, or occluded. Their dependence on persistent landmarks and loop closures makes global pose estimation fragile, resulting in drift and unsafe behavior at high speed.\n\nAn alternative paradigm, recently advanced in prior works [2], is to abandon global consistency and instead define navigation directly in a target-relative reference frame. By continuously re-anchoring perception, planning, and control to the instantaneous detection of a moving object, UAVs can robustly pursue targets even in unstructured or degraded environments. This instantaneous relative navigation formulation has demonstrated strong performance for the tracking phase of SAR missions. However, it presumes that a target is visible from the outset and remains continuously observable—an assumption that rarely holds in practice. What is missing is a capability for safe, high-speed traversal in unknown terrain without any target directly in view, and without reverting to the fragile assumptions of traditional GPS, VIO, or SLAM.\n\nThis paper introduces HUNT, a reactive autonomy framework that unifies high-speed traversal, acquisition, and tracking under a single instantaneous relative formulation. At its core is a novel loitering mode, which defines navigation objectives solely from directly observable quantities—attitude, altitude, and instantaneous velocity—allowing the UAV to traverse unknown cluttered environments safely even in the absence of a target. When a target is detected, the same perception–control pipeline seamlessly re-anchors the reference frame to the target, turning tracking into a special case of loitering without requiring global state.\n\nSafety is enforced throughout by embedding high-order Control Barrier Functions (CBFs) directly into a Nonlinear Model Predictive Controller (NMPC), guaranteeing dynamically feasible, collision-free trajectories in real time. Robust transitions between modes are enabled by a confidence-based switching mechanism that prevents oscillations and ensures stability as detections appear and disappear.\n\nWe validate HUNT in extensive outdoor SAR-like missions spanning urban compounds, semi-structured layouts, and dense forests. Experiments include loitering flights over city blocks and forest canopy, dense-clutter traversal below canopy, and full missions where the UAV searches, acquires, and pursues static and dynamic targets such as vehicles and mannequins. Across all settings, HUNT achieves consistent high-speed performance where global methods fail.\n\nTo the best of our knowledge, HUNT is the first framework to integrate traversal, acquisition, and tracking in cluttered, GPS-denied environments under a unified instantaneous relative formulation, eliminating reliance on global pose.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在未知、无结构环境中实现无人机的高速度导航和目标跟踪。  \n2. 传统的全球导航方法在SAR任务中面临的挑战和局限性。  \n3. 在没有目标可见的情况下进行安全的高速度穿越的能力。  \n\n【用了什么创新方法】  \nHUNT框架通过将高速度穿越、目标获取和跟踪统一在一个瞬时相对参考框架下，提出了一种新颖的巡航模式。该模式仅依赖于可直接观察的量（如姿态、高度和瞬时速度）来定义导航目标，使无人机能够在没有目标的情况下安全穿越未知环境。当目标被检测到时，感知-控制管道无缝切换到跟踪模式，确保高效的目标追踪。通过将高阶控制障碍函数嵌入到非线性模型预测控制器中，HUNT保证了实时的动态可行和无碰撞轨迹。实验结果表明，HUNT在复杂的城市和森林环境中表现出一致的高速度性能，超越了传统方法的局限。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models",
            "authors": "JuanaJuana Valeria Hurtado,Rohit Mohan,Abhinav Valada",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20107",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20107",
            "arxiv_html_link": "https://arxiv.org/html/2509.20107v1",
            "abstract": "Hyperspectral imaging (HSI) captures spatial information along with dense spectral measurements across numerous narrow wavelength bands. This rich spectral content has the potential to facilitate robust robotic perception, particularly in environments with complex material compositions, varying illumination, or other visually challenging conditions. However, current HSI semantic segmentation methods underperform due to their reliance on architectures and learning frameworks optimized for RGB inputs. In this work, we propose a novel hyperspectral adapter that leverages pretrained vision foundation models to effectively learn from hyperspectral data. Our architecture incorporates a spectral transformer and a spectrum-aware spatial prior module to extract rich spatial-spectral features. Additionally, we introduce a modality-aware interaction block that facilitates effective integration of hyperspectral representations and frozen vision Transformer features through dedicated extraction and injection mechanisms.\nExtensive evaluations on three benchmark autonomous driving datasets demonstrate that our architecture achieves state-of-the-art semantic segmentation performance while directly using HSI inputs, outperforming both vision-based and hyperspectral segmentation methods. We make the code available at https://hyperspectraladapter.cs.uni-freiburg.de.",
            "introduction": "Advancing robot perception in complex environments requires an understanding of detailed scene characteristics beyond surface appearance [1]. Hyperspectral imaging (HSI), which captures reflectance information across tens to hundreds of narrow spectral bands, provides material-specific information that is significantly richer than conventional RGB imaging [2]. By encoding distinctive spectral signatures, HSI enables fine-grained discrimination of objects and materials, offering the potential to enhance perception tasks [3, 4] under challenging conditions such as varying illumination, occlusions [5], scene clutter [6], and complex material compositions. These properties make HSI particularly promising for autonomous driving, where reliable perception in diverse and visually complex environments is essential.\n\nHyperspectral semantic segmentation aims to classify each pixel of an image into predefined categories based on HSI inputs. Despite its potential, semantic segmentation with hyperspectral data remains a challenging problem. Most segmentation models are tailored for RGB images and fail to fully leverage the spectral richness of HSI. Standard vision semantic segmentation architectures, although proven powerful for extracting spatial semantics from RGB data, are not inherently equipped to model the complex inter-channel dependencies of hyperspectral inputs. As a result, models pretrained on RGB datasets often underperform when directly applied to hyperspectral inputs [7, 8, 9]. Although pseudo-RGB approaches can partially adapt existing models, they overlook the spectral diversity inherent in HSI and fail to address critical challenges such as high dimensionality, limited labeled data, and increased computational costs [2]. We illustrate the difference between these approaches in Fig. 1.\n\nVision foundation models trained on large-scale datasets generalize well across domains [10]. However, their full finetuning is computationally prohibitive, and linear probing lacks capacity for complex tasks such as hyperspectral segmentation. Adapter tuning provides an efficient alternative by inserting lightweight modules into a frozen backbone, allowing for task-specific adaptation without retraining the entire model. However, existing adapters are designed for RGB inputs and, when applied to HSI, they face initialization sensitivity and fail to capture spectral dependencies, limiting their effectiveness.\n\nTo address these limitations, we propose the HSI-Adapter, a modular architecture that enables leveraging strong pretrained vision foundation models with HSI inputs for hyperspectral semantic segmentation. Our approach introduces a spectral transformer and a spectral-enhanced spatial prior module to jointly extract rich spectral and spatial features from hyperspectral inputs. To bridge the modality gap between hyperspectral and RGB representations, we propose a modality-aware interaction block that enables effective bidirectional feature exchange between hyperspectral features and foundational vision features. This topology preserves the strengths of the pretrained foundation model while incorporating the unique characteristics of hyperspectral data. Extensive experiments on HSI-DriveV2 [11], HyperspectralCityV2.0 (HCV2) [12], and HyKo2-VIS [13] benchmark autonomous driving datasets demonstrate that our method achieves state-of-the-art performance, surpassing both RGB-based and hyperspectral segmentation approaches. In addition to quantitative gains, qualitative results show that our method generalizes well to diverse and complex driving environments, handling cluttered scenes, dense urban layouts, and visually similar material classes such as painted and unpainted metal.\n\nOur primary contributions are summarized as follows:\n\nA novel hyperspectral adapter architecture tailored for semantic segmentation using HSI inputs.\n\nA spectral transformer and spectrum-aware spatial prior module to jointly model spatial and spectral context.\n\nA modality-aware interaction block that enables effective feature fusion between hyperspectral and pretrained foundational vision representations.\n\nComprehensive experiments and ablation studies across three benchmark datasets, demonstrating the effectiveness and generalization of our approach.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效利用高光谱成像（HSI）数据进行语义分割？  \n2. 当前基于RGB的语义分割模型在处理HSI数据时表现不佳的原因是什么？  \n3. 如何在不完全重训练的情况下，适应预训练视觉基础模型以处理HSI输入？  \n\n【用了什么创新方法】  \n本研究提出了一种新颖的高光谱适配器架构，结合了光谱变换器和光谱增强空间先验模块，以联合提取高光谱输入的丰富光谱和空间特征。此外，提出的模态感知交互块实现了高光谱特征与预训练视觉特征之间的有效双向特征交换。通过在三个基准数据集上的广泛评估，证明了该方法在语义分割性能上达到了最先进的水平，超越了现有的RGB和高光谱分割方法。定量和定性结果表明，该方法在复杂的驾驶环境中表现良好，能够处理拥挤场景和视觉相似材料类别。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Embodied AI: From LLMs to World Models",
            "authors": "Tongtong Feng,Xin Wang,Yu-Gang Jiang,Wenwu Zhu",
            "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Robotics (cs.RO)",
            "comment": "Accepted by IEEE CASM",
            "pdf_link": "https://arxiv.org/pdf/2509.20021",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20021",
            "arxiv_html_link": "https://arxiv.org/html/2509.20021v1",
            "abstract": "Embodied Artificial Intelligence (AI) is an intelligent system paradigm for achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications and driving the evolution from cyberspace to physical systems. Recent breakthroughs in Large Language Models (LLMs) and World Models (WMs) have drawn significant attention for embodied AI. On the one hand, LLMs empower embodied AI via semantic reasoning and task decomposition, bringing high-level natural language instructions and low-level natural language actions into embodied cognition. On the other hand, WMs empower embodied AI by building internal representations and future predictions of the external world, facilitating physical law-compliant embodied interactions. As such, this paper comprehensively explores the literature in embodied AI from basics to advances, covering both LLM driven and WM driven works. In particular, we first present the history, key technologies, key components, and hardware systems of embodied AI, as well as discuss its development via looking from unimodal to multimodal angle. We then scrutinize the two burgeoning fields of embodied AI, i.e., embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs, meticulously delineating their indispensable roles in end-to-end embodied cognition and physical laws-driven embodied interactions. Building upon the above advances, we further share our insights on the necessity of the joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds. In addition, we examine representative applications of embodied AI, demonstrating its wide applicability in real-world scenarios. Last but not least, we point out future research directions of embodied AI that deserve further investigation.",
            "introduction": "Embodied Artificial Intelligence (AI) originated from the Embodied Turing Test by Alan Turing in 1950 [1], which is designed to explore whether agents can imitate human intelligence to achieve Artificial General Intelligence (AGI). Among them, agents that only solve abstract problems in digital world (cyberspace) are generally defined as disembodied AI, while those that also can interact with the physical world are regarded as embodied AI. Embodied AI builds on foundational insights from cognitive science and neuroscience [2, 3], which claims that intelligence emerges from the dynamic coupling of perception, cognition, and interaction. As shown in Fig. 1, embodied AI includes three key components in a closed-loop manner, i.e., 1) active perception (sensor-driven environmental observation), 2) embodied cognition (historical experience-driven cognition updating), and 3) dynamic interaction (actuator-mediated action control). Besides, hardware embodiment [4, 5, 6] is also critical due to escalating computational and energy demands, particularly under latency and power constraints of devices in real-world deployment scenarios.\n\nThe development of embodied AI has evolved from unimodal to multimodal paradigm. In early stage, embodied AI is primarily studied through focusing on individual components with single modality such as vision, language, or action, where the perception, cognition, or interaction component is driven by one sensory input [7, 8], e.g., perception tends to be dominated by the visual modality [9], cognition tends to be dominated by the language modality [10, 11], and interaction tends to be dominated by the action modality [12, 13]. Although these methods perform well within individual components, they are limited by the narrow scope of information provided by each modality and the inherent gaps between modalities across components. The continued development of embodied AI witnesses the limitations of unimodal approaches, promoting a significant shift toward integration of multiple sensory modalities [14, 15, 16]. As such, multimodal embodied AI [17, 18] naturally arises to create more adaptive, flexible, and robust agents capable of performing complex tasks in dynamic environments.\n\nLarge Language Models (LLMs) empower embodied AI via semantic reasoning [19] and task decomposition [20, 21], bringing high-level natural language instructions and low-level natural language actions into embodied cognition. Representative LLM driven works include SayCan [22], which i) provides a real-world pretrained natural language action library to constrain LLMs from proposing infeasible and contextually inappropriate actions; ii) uses LLMs to convert natural language instructions into natural language action sequences; and iii) utilizes value functions to verify the feasibility of natural language action sequences in a particular physical environment. These works suggest that LLMs are extremely useful to robots which aim at acting upon high-level, temporally extended instructions expressed in natural language. However, LLMs are only a part of the entire embodied AI system (e.g., embodied cognition), which is limited by a fixed natural language action library and a specific physical environment, making it difficult for LLM driven embodied AI to achieve adaptive expansion for new robots and environments.\n\nRecent breakthroughs in Multimodal LLMs (MLLMs) [23, 24] and World Models (WMs) [25, 26, 27] have opened up a new frontier in embodied AI research. MLLMs can act on the entire embodied AI system, bridging high-level multimodal inputting and low-level motor action sequences into end-to-end embodied applications. Semantic reasoning [28, 29, 30] leverages MLLMs’ cross-modal comprehension to interpret semantics from visual, auditory, or tactile inputs, e.g., identifying objects, inferring spatial relationships, predicting environmental dynamics. Concurrently, task decomposition [31, 32, 33] employs MLLMs’ sequential logic to break complex objectives into sub-tasks while dynamically adapting plans based on sensor feedback. However, MLLMs often fail to ground predictions in physics-compliant dynamics [34] and exhibit poor real-time adaptation [35] to environmental feedback.\n\nOn the other hand, WMs empower embodied AI by building internal representations [36, 37, 38, 39, 40] and making future predictions [41, 42, 43, 44] of the external world. Such WM driven embodied AI is able to facilitate physical law-compliant embodied interactions in dynamic environments. Internal representations compress rich sensory inputs into structured latent spaces, capturing object dynamics, physics laws, and spatial structures, as well as allowing agents to reason about “what exists” and “how things behave” in their surroundings. Simultaneously, future predictions simulate potential rewards of sequence actions across multiple time horizons aligned with physical laws, thereby preempting risky or inefficient behaviors. However, WM driven approaches struggle with open-ended semantic reasoning [45] and lack the ability of generalizable task decomposition [26] without explicit priors.\n\nBuilding upon the above advances, we further share our insights on the necessity of developing a joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds. MLLMs enable contextual task reasoning but overlook physical constraints, while WMs excel at physics-aware simulation but lack high-level semantics. The joint of MLLM and WM can bridge semantic intelligence with grounded physical interaction. For instance, EvoAgent [46] designs an autonomous-evolving agent with a joint MLLM-WM driven embodied AI architecture, which can autonomously complete various long-horizon tasks across environments through self-planning, self-reflection, and self-control, without human intervention. We believe that designing joint MLLM-WM driven embodied AI architectures will dominate next-generation embodied systems, bridging the gap between specialized AI agents and general physical intelligence.\n\nWe summarize the representative applications of embodied AI as service robotics, rescue UAVs, industrial Robots, and others etc., demonstrating its wide applicability in real-world scenarios. We also point out potential future directions of embodied AI, including but not limited to autonomous embodied AI, embodied AI hardware, and swarm embodied AI etc.\n\nAs shown in Fig. 2, the rest of this paper is organized as follows. Section II introduces the history, key technologies, key components, and hardware system of embodied AI, discussing the development of embodied AI from unimodal to multimodal angle. Section III presents embodied AI with LLMs/MLLMs, and Section IV presents embodied AI with WMs. Section V introduces our insights on designing a joint MLLM-WM driven embodied AI architecture. Section VI briefly examines applications of embodied AI. Potential future directions are discussed in Section VII.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现从LLMs到世界模型的联合驱动的具身人工智能架构？  \n2. 如何在复杂的物理环境中实现具身AI的高效任务执行？  \n3. 具身AI如何克服单一模态方法的局限性，提升多模态感知与交互能力？  \n\n【用了什么创新方法】  \n本文提出了一种联合MLLM-WM驱动的具身AI架构，通过整合多模态语言模型和世界模型，增强了具身智能的语义推理和物理交互能力。该架构能够在动态环境中实现复杂任务的自我规划、自我反思和自我控制，展现出在真实场景中广泛的应用潜力。研究表明，这种方法显著提高了具身AI在物理法则遵循下的交互能力，并推动了下一代具身系统的发展。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents",
            "authors": "Filippo Ziliotto,Jelin Raphael Akkara,Alessandro Daniele,Lamberto Ballan,Luciano Serafini,Tommaso Campari",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19843",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19843",
            "arxiv_html_link": "https://arxiv.org/html/2509.19843v1",
            "abstract": "Recent advances in Embodied AI have enabled agents to perform increasingly complex tasks and adapt to diverse environments. However, deploying such agents in realistic human-centered scenarios, such as domestic households, remains challenging, particularly due to the difficulty of modeling individual human preferences and behaviors.\nIn this work, we introduce PersONAL  (PERSonalized Object Navigation And Localization), a comprehensive benchmark designed to study personalization in Embodied AI.\nAgents must identify, retrieve, and navigate to objects associated with specific users, responding to natural-language queries such as find Lily’s backpack.\nPersONAL comprises over 2,000 high-quality episodes across 30+ photorealistic homes from the HM3D dataset. Each episode includes a natural-language scene description with explicit associations between objects and their owners, requiring agents to reason over user-specific semantics.",
            "introduction": "In recent years, Embodied AI has significantly advanced, enabling agents to perform complex tasks and interact more naturally with their environments. Modern methods combine end-to-end training with zero-shot capabilities powered by large language models (LLMs), allowing agents to answer dynamically to user input [1, 2, 3, 4, 5, 6, 7].\nYet, their application to user-centric scenarios, where agents must interpret local, implicit information not directly encoded in pretrained models, such as object ownership, remains largely unexplored.\nBridging this gap is key to deploying embodied agents in real-world environments like homes or offices.\nWhile personalized vision-language models (VLMs) [8, 9, 10] have been developed for user-specific visual grounding, they are typically limited to static, image-based contexts.\nIn contrast, embodied agents must operate in complex physical settings, reasoning and acting over time.\n\nRecently, a few works have begun to explore personalization in embodied scenarios [11, 12], but the field remains in its early stages and these works mainly focus on guiding agents using image-based queries or continuous human-robot interaction, which limits scalability and real-world applicability.\n\nTo address this gap, we introduce PersONAL , the first embodied AI benchmark for personalized, user-centric navigation and personalized object grounding (Figure 1).\nAgents must interpret user-specific queries and either navigate to or retrieve the location of objects associated with particular individuals (e.g., “Navigate to Carl’s backpack”).\nEach episode includes a textual scene description specifying object attributes and ownership (e.g., “the upper kitchen cabinet belongs to Linda”), followed by a personalized query.\nUnlike prior work, we also define a grounding task which acts as an embodied memory challenge, requiring agents to recall and localize targets using internal maps.\n\nPersONAL supports two evaluation modes: (i) Personalized Active Navigation (PAN), the agent must navigate an unknown environment to find the target object, and (ii) Personalized Object Grounding (POG), where the goal is to localize objects within a pre-mapped environment.\nWe release a dataset of over 2,000 curated evaluation episodes across three difficulty levels (easy, medium, hard), capturing increasing complexity in human–object associationsWe evaluate the proposed benchmark with several zero-shot, state-of-the-art navigation baselines and introduce a simple zero-shot method for the grounding task, demonstrating in both settings that a substantial performance gap persists between current Embodied AI agents and human capabilities.\n\nIn summary, our main contributions are:\n\nWe present PersONAL , a comprehensive Embodied AI benchmark specifically designed to evaluate embodied personalization, incorporating user-centric queries and object–ownership semantics.\n\nWe release a dataset of 2,000 high-quality episodes sampled from over 30 realistic household environments, divided into three difficulty levels (easy, medium, and hard).\n\nWe provide empirical analyses with state-of-the-art zero-shot baselines, showcasing limitations and possible future research directions toward human-level personalized navigation in Embodied AI.\n\nWe argue that personalization cannot be achieved through repeated LLM-based interactions alone.\nInstead, agents must maintain and recall user-specific preferences internally—ideally through long-term memory and continual learning—an ability crucial in complex households settings where ownership is stable but scenes continually evolve.\n\n1. We present PersONAL , a comprehensive Embodied AI benchmark specifically designed to evaluate embodied personalization, incorporating user-centric queries and object–ownership semantics.\n\n2. We release a dataset of 2,000 high-quality episodes sampled from over 30 realistic household environments, divided into three difficulty levels (easy, medium, and hard).\n\n3. We provide empirical analyses with state-of-the-art zero-shot baselines, showcasing limitations and possible future research directions toward human-level personalized navigation in Embodied AI.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在真实人类中心场景中实现个性化的具身智能代理。  \n2. 现有的个性化视觉语言模型在动态环境中的应用局限性。  \n3. 如何有效地评估和提升具身智能代理的个性化导航能力。  \n\n【用了什么创新方法】  \n本研究提出了PersONAL，一个综合性的基准，专注于个性化的具身智能代理。该基准包含2000个高质量的评估情节，涵盖30多个真实家庭环境，代理需根据用户特定的自然语言查询进行导航和物体定位。研究展示了当前具身智能代理在个性化导航任务中的性能差距，并指出了未来研究方向，强调了内部记忆和持续学习在复杂家庭环境中的重要性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving",
            "authors": "Carlo Bosio,Greg Woelki,Noureldin Hendy,Nicholas Roy,Byungsoo Kim",
            "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19789",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19789",
            "arxiv_html_link": "https://arxiv.org/html/2509.19789v1",
            "abstract": "Human drivers focus only on a handful of agents at any one time. On the other hand, autonomous driving systems process complex scenes with numerous agents, regardless of whether they are pedestrians on a crosswalk or vehicles parked on the side of the road.\nWhile attention mechanisms offer an implicit way to reduce the input to the elements that affect decisions, existing attention mechanisms for capturing agent interactions are quadratic, and generally computationally expensive.\nWe propose RDAR, a strategy to learn per-agent relevance — how much each agent influences the behavior of the controlled vehicle — by identifying which agents can be excluded from the input to a pre-trained behavior model.\nWe formulate the masking procedure as a Markov Decision Process where the action consists of a binary mask indicating agent selection.\nWe evaluate RDAR on a large-scale driving dataset, and demonstrate its ability to learn an accurate numerical measure of relevance by achieving comparable driving performance, in terms of overall progress, safety and performance, while processing significantly fewer agents compared to a state of the art behavior model.",
            "introduction": "Humans, when driving, do not pay equal attention to all agents around them (e.g., other vehicles, pedestrians).\nTransfomer-based attention models offer the promise of attending only to relevant components of the input, but existing attention models are typically quadratic in the size of the input space. Driving models encounter hundreds of input tokens, leading to substantial computational complexity and latency Harmel et al. (2023); Huang et al. (2024); Baniodeh et al. (2025).\n\nIn autonomous driving, there is a tension between the limited available compute resources and the desire to take advantage of scaling laws, large models, and test-time compute. Having access to numerical per-agent relevance scores would not only improve the interpretability of large driving models, but also allow compute resources to be prioritized for the features that are most important.\nIn fact, when agents and other scene elements are represented explicitly as tokens, reasoning about interactions between these tokens (typically through self-attention or graph neural network operations) is quadratic and difficult to reduce using low-rank or other approximations that work well for long-sequence data. Reducing the number of tokens under consideration provides quadratic improvements in FLOPs used.\n\nIn this work, we introduce RDAR (Reward-Driven Agent Relevance), through which we quantify agent relevance through a learned approach. The basic intuition is that if an agent is not relevant towards the driving decisions of the controlled vehicle, then its absence would not change the controlled vehicle’s driving behavior significantly. Thus, we quantify per-agent relevance by learning which agents can be masked out from the controlled vehicle’s planner input while maintaining a good driving behavior.\nWe formulate agent selection as a reinforcement learning (RL) problem where an action is a binary mask indicating which agents to include in the driving policy input, and which not to.\nThe RDAR scoring policy is trained in the loop with a frozen, pre-trained driving policy and a simulator. At each time step, based on the relevance scores, an agent mask is fed to the driving policy, making the controlled vehicle blind to the lower score agents. As it will be clear from the following sections, this is not a binary classification problem over agents due to the underlying system dynamics (e.g., not observing an agent now could lead to a collision later) and to the unavailability of ground truth labels.\nSome examples of relevance (color-coded) computed by our method are shown in Fig. 1.\nOur main contributions are:\n\nA novel reinforcement learning formulation for agent relevance estimation;\n\nA sampling-based mechanism for agent selection that enables efficient training and inference;\n\nA comprehensive evaluation showing that we can maintain driving performance while processing only a handful of surrounding agents.\n\n1. A novel reinforcement learning formulation for agent relevance estimation;\n\n2. A sampling-based mechanism for agent selection that enables efficient training and inference;\n\n3. A comprehensive evaluation showing that we can maintain driving performance while processing only a handful of surrounding agents.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效评估每个代理对自动驾驶行为的影响？  \n2. 如何减少输入代理的数量以降低计算复杂性？  \n3. 如何在保持驾驶性能的同时提高模型的可解释性？  \n\n【用了什么创新方法】  \n提出了RDAR（Reward-Driven Agent Relevance），通过强化学习方法量化每个代理的相关性，形成一个二进制掩码来选择输入代理。该方法在保持良好驾驶行为的同时，显著减少了处理的代理数量，从而降低了计算复杂性。评估表明，RDAR在大规模驾驶数据集上表现出与最先进的行为模型相当的驾驶性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "VIMD: Monocular Visual-Inertial Motion and Depth Estimation",
            "authors": "Saimouli Katragadda,Guoquan Huang",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19713",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19713",
            "arxiv_html_link": "https://arxiv.org/html/2509.19713v1",
            "abstract": "Accurate and efficient dense metric depth estimation is crucial for 3D visual perception in robotics and XR.\nIn this paper, we develop a monocular visual-inertial motion and depth (VIMD) learning framework to estimate dense metric depth by leveraging accurate and efficient MSCKF-based monocular visual-inertial motion tracking.\nAt the core the proposed VIMD is to exploit multi-view information to iteratively refine per-pixel scale,\ninstead of globally fitting an invariant affine model as in the prior work.\nThe VIMD framework is highly modular, making it compatible with a variety of existing depth estimation backbones.\nWe conduct extensive evaluations on the TartanAir and VOID datasets and demonstrate its zero-shot generalization capabilities on the AR Table dataset.\nOur results show that VIMD achieves exceptional accuracy and robustness, even with extremely sparse points—as few as 10-20 metric depth points per image.\nThis makes the proposed VIMD a practical solution for deployment in resource-constrained settings,\nwhile its robust performance and strong generalization capabilities offer significant potential across a wide range of scenarios.",
            "introduction": "For applications like robotics and extended reality (XR), accurate and efficient metric dense depth estimation is critical for 3D visual perception, which is essential for tasks such as obstacle avoidance and motion planning.\nMonocular methods, which estimate depth from a single RGB image, are particularly appealing because they use a compact, inexpensive, and common camera. However, purely monocular vision suffers from an inherent scale ambiguity: it can capture the relative shape of a scene but cannot determine the true distances to objects.\nKnowing exactly how far an obstacle is from the camera can be the difference between safe passage and collision.\nIntegrating inertial data can (partially) resolve this ambiguity.\nAs most mobile devices and robots are already equipped with an IMU,\nvisual–inertial odometry (VIO) or SLAM prevails but typically yields only sparse metric depth from a small set of tracked landmarks [1],\nwhich alone cannot provide the dense metric depth map required in safety-critical operations.\n\nAlthough recent advances in monocular depth prediction have achieved high-quality relative depth estimation [3, 4], these methods still lack the ability to produce outputs with absolute metric scale.\nA common approach assumes an invariant affinity model and aligns the predicted depth to the sparse VIO depth via per-frame least-squares fitting for a global scale and offset (e.g., see [5]).\nHowever, as shown in Fig. 1, the fitted scale and offset vary significantly across frames—particularly the offset, which exhibits much larger fluctuations than the scale.\nThis temporal instability indicates that the predicted relative depths are not truly affine with respect to the metric depths; that is, they cannot be accurately modeled by a single global scale and offset per frame without unmodeled errors.\nBesides the unreliable per-frame global fitting, the optimization may become poorly conditioned when sparse points are unevenly distributed or noisy, leading to sensitivity in the fitted parameters and potential error amplification in low-density regions.\nIt has been empirically observed that predicting offset along with scale can hurt depth accuracy [6].\n\nFrom Fig. 1, it is clear that the offset exhibits much higher variance across frames than the scale, implying that offsets may also vary more significantly across pixels within a single frame (due to factors like viewpoint changes, occlusions, or non-uniform scene structure), thus making them harder to learn reliably at the per-pixel level compared to the more stable scale.\nMotivated by this observation, we propose to exploit multi-view constraints and learn to optimize iteratively a per-pixel scale, formulated as learning approach that moves beyond the global fitting affine parameters.\nIn particular, the proposed monocular Visual-Inertial Motion and Depth (VIMD)\nleverages an accurate and efficient MSCKF-based visual-inertial motion estimation module to form multi-view geometric constraints\nand then refines and predicts dense depth and its uncertainty.\nAs a result, the proposed VIMD improves accuracy and robustness in low-density depth regions.\nFig. 2 shows some exemplar performance of the proposed VIMD in both outdoor and indoor scenes, producing high-quality dense metric depths.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在单目视觉中准确高效地估计稠密的度量深度。  \n2. 解决单目视觉方法中的尺度模糊问题，以提高深度估计的准确性和鲁棒性。  \n3. 如何利用视觉惯性数据来增强深度估计的性能，尤其是在稀疏点情况下。  \n\n【用了什么创新方法】  \n提出了一种单目视觉-惯性运动和深度(VIMD)学习框架，通过利用基于MSCKF的视觉惯性运动跟踪，迭代优化每个像素的尺度，克服了传统方法中全局拟合不稳定的问题。该框架兼容多种深度估计模型，并在TartanAir和VOID数据集上进行了广泛评估，展示了其在资源受限环境中的实用性和强大的零-shot泛化能力。VIMD在极少的深度点（仅10-20个）情况下，仍能实现卓越的准确性和鲁棒性，适用于多种场景。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar",
            "authors": "William L. Muckelroy III,Mohammed Alsakabi,John M. Dolan,Ozan K. Tonguz",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19644",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19644",
            "arxiv_html_link": "https://arxiv.org/html/2509.19644v1",
            "abstract": "LiDAR’s dense, sharp point cloud (PC) representations of the surrounding environment enable accurate perception and significantly improve road safety by offering greater scene awareness and understanding. However, LiDAR’s high cost continues to restrict the broad adoption of high-level Autonomous Driving (AD) systems in commercially available vehicles. Prior research has shown progress towards circumventing the need for LiDAR by training a neural network, using LiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds using only 4D Radars. One of the best examples is a neural network created to train a more efficient radar target detector with a modular 2D convolutional neural network (CNN) backbone and a temporal coherence network at its core that uses the RaDelft dataset for training [1]. In this work, we investigate the impact of higher-capacity segmentation backbones on the quality of the produced point clouds. Our results show that while very high-capacity models may actually hurt performance, an optimal segmentation backbone can provide a 23.7% improvement over the state-of-the-art (SOTA).",
            "introduction": "There are numerous reasons why widespread adoption of commercially available autonomous vehicles (AV) and commercially viable autonomous driving (AD) has not occurred, one of which is the cost-prohibitive sensor suites they employ. These systems are designed to significantly improve road safety through their greater awareness and understanding of the scene, allowing them to interact with the world and react to environmental disturbances in a safe manner. However, an essential component of these systems is LiDAR because of the sharp, dense point cloud (PC) representations of the environment they can produce. These PCs are often used with arrays of radars and cameras to detect people, cars, and other objects as the ego vehicle traverses the world. Of these three primary sensors, we see that LiDARs take a significant stake in the overall cost of the sensor suite in AVs, ranging from $4,000 for mid-range LiDARs to $70,000 for long-range high-end LiDARs like that used in the KITTI dataset [2][3]. However, 4D Radar is a relatively new emerging sensor in the AD space that offers promising performance at lower costs, as seen in Table I.\n\nPrevious research has shown that one can build a system that has the potential to replace LiDAR by leveraging concepts from Deep Learning (DL). By training a neural network using LiDAR PCs as ground truth (GT), one can produce LiDAR-like 3D PCs using only 4D radars [1][6][7][8][5][4]. An example of such a network is the neural network proposed in [1] and trained on the RaDelft dataset as a more efficient radar target detector. Using a modular and straightforward 2D convolutional neural network (CNN) backbone for segmentation and a temporal coherence network enabled the authors of [1] to achieve significant improvements in bidirectional chamfer distance (BCD) when compared to other methods like various forms of Constant False Alarm Rate (CFAR) (i.e., CA-CFAR, SOCA-CFAR, GOCA-CFAR, OS-CFAR, ML-CFAR, etc.) [9][10][11].\n\nOur work further investigates the impact of the 2D segmentation backbone and the number of 3D convolutional layers in the temporal coherence network on the overall performance of the predicted point clouds. This allows us to gain insight into how optimizations of the overall architecture can further minimize BCD and increase the viability of this technique as a potential replacement for LiDAR.\n\nThe remainder of this paper is organized as follows. Section II presents the background of the problem investigated and different definitions of key terms used throughout the paper. Section III outlines the methods used in this work for experimentation and how we evaluate the performance of such experiments. Section IV presents the primary findings of our work. Finally, Section V summarizes the discussions that have been held throughout this paper, along with our intentions to further improve the metrics and results in the future.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用4D雷达替代高成本的LiDAR进行3D点云预测。  \n2. 2D分割骨干网络对生成点云质量的影响。  \n3. 高容量模型在点云生成中的性能表现。  \n\n【用了什么创新方法】  \n本研究通过训练神经网络，使用LiDAR点云作为真实值，探索了不同2D分割骨干网络对生成点云质量的影响。重点在于优化网络架构，包括2D CNN骨干和时间一致性网络的3D卷积层数量。结果表明，适当的分割骨干网络可以在性能上比现有技术提高23.7%。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation",
            "authors": "Ramy ElMallah,Krish Chhajer,Chi-Guhn Lee",
            "subjects": "Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "Accepted to the CoRL 2025 Eval&Deploy Workshop",
            "pdf_link": "https://arxiv.org/pdf/2509.19524",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19524",
            "arxiv_html_link": "https://arxiv.org/html/2509.19524v1",
            "abstract": "Robot learning papers typically report a single binary success rate (SR), which obscures where a policy succeeds or fails along a multi-step manipulation task. We argue that subgoal-level reporting should become routine: for each trajectory, a vector of per-subgoal SRs that makes partial competence visible (e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware plug-in evaluation framework that utilizes vision–language models (VLMs) as automated judges of subgoal outcomes from recorded images or videos. Rather than proposing new benchmarks or APIs, our contribution is to outline design principles for a scalable, community-driven open-source project. In StepEval, the primary artifact for policy evaluation is the per-subgoal SR vector; however, other quantities (e.g., latency or cost estimates) are also considered for framework-optimization diagnostics to help the community tune evaluation efficiency and accuracy when ground-truth subgoal success labels are available. We discuss how such a framework can remain model-agnostic, support single- or multi-view inputs, and be lightweight enough to adopt across labs. The intended contribution is a shared direction: a minimal, extensible seed that invites open-source contributions, so that scoring the steps, not just the final goal, becomes a standard and reproducible practice.",
            "introduction": "Robotic manipulation research largely evaluates policies with a single binary metric (success or failure). While simple to report, success rate (SR) provides a general, non-granular view of performance [1, 2]. A policy either achieves the task or not, with no insight into partial progress or which sub-task caused the failure. This is especially problematic for long-horizon tasks composed of multiple sequential subgoals (e.g., grasp, lift, place in a pick-and-place task). A low success rate might indicate failure, but it does not reveal where the policy struggled. As a motivating example, Kress-Gazit et al. [1] describe a pancake-flipping task consisting of several stages (picking up spatulas, flipping the pancake, then plating it). One policy in their study had only 17% overall success, yet it completed the first two subgoals 100% of the time; it consistently grasped the spatulas and flipped the pancake, failing only at the final plating step. If judged solely by overall SR, this policy would be deemed poor, obscuring the fact that it mastered over half the task. Granular evaluation is essential: without it, researchers cannot pinpoint failure modes (e.g., was it the grasp or the pour that failed?), making improvements guesswork.\n\nThe need for fine-grained, nuanced evaluation in robot learning has been increasingly recognized. Recent best-practice guidelines explicitly recommend reporting subgoal completion metrics and failure mode analyses alongside overall success [1, 3, 4]. However, adopting such practices in everyday research is challenging: manually labeling each sub-step success for dozens or hundreds of experiments is labor-intensive and subjective. Some works have automated subgoal checks in simulation (where the state is known) or relied on human labeling for physical trials. Vision-language models (VLMs) offer a promising middle ground, a way to automatically classify outcomes from visual data. Indeed, initial attempts have been made to use learned classifiers [5, 6] or VLMs [7] to detect task failures.\n\nOur goal is to reframe evaluation practice by advocating a subgoal-first view of manipulation metrics. We present StepEval as a blueprint for a cost-aware, plug-in framework that treats the per-subgoal success-rate (SR) vector as the primary artifact for policy evaluation. In this view, a VLM acts as a black-box automated judge that maps recorded imagery to subgoal outcomes. We articulate design principles for a community-built, scalable open-source project. All other quantities (e.g., latency, token/image cost, or confusion matrices based on ground-truth subgoal labels) are framed as framework-optimization diagnostics that help tune evaluation efficiency and accuracy when ground-truth labels are available, rather than as core evaluation metrics.\n\nWe contribute by articulating the following pillars for subgoal-based evaluation:\n\nSubgoal-first evaluation perspective. We formalize the evaluation target as a trajectory-level vector 𝐲∈{0,1}n\\mathbf{y}\\in\\{0,1\\}^{n} of per-subgoal SRs and argue that this granular object, rather than a single binary task SR, should become the standard reported outcome for manipulation studies.\n\nModel-agnostic judging concept. We propose using a VLM as a judge to infer subgoal outcomes from RGB image frames (single- or multi-view) without instrumenting policies or environments, keeping the framework post-hoc and lightweight. The design is intentionally policy- and model-agnostic, with the aim of making scoring the steps a routine, reproducible practice across labs.\n\nSeparation of evaluation vs. framework optimization metrics. We distinguish the primary evaluation artifact (per-subgoal SR vector) from optional diagnostics (e.g., cost/latency; confusion matrices based on ground-truth subgoal labels; or optional prompt-optimization procedures). The latter are intended only to help the community tune efficiency and accuracy when labels exist, not to redefine the core evaluation target.\n\n1. Subgoal-first evaluation perspective. We formalize the evaluation target as a trajectory-level vector 𝐲∈{0,1}n\\mathbf{y}\\in\\{0,1\\}^{n} of per-subgoal SRs and argue that this granular object, rather than a single binary task SR, should become the standard reported outcome for manipulation studies.\n\n2. Model-agnostic judging concept. We propose using a VLM as a judge to infer subgoal outcomes from RGB image frames (single- or multi-view) without instrumenting policies or environments, keeping the framework post-hoc and lightweight. The design is intentionally policy- and model-agnostic, with the aim of making scoring the steps a routine, reproducible practice across labs.\n\n3. Separation of evaluation vs. framework optimization metrics. We distinguish the primary evaluation artifact (per-subgoal SR vector) from optional diagnostics (e.g., cost/latency; confusion matrices based on ground-truth subgoal labels; or optional prompt-optimization procedures). The latter are intended only to help the community tune efficiency and accuracy when labels exist, not to redefine the core evaluation target.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有的机器人学习评估方法仅使用单一的成功率（SR），缺乏对多步骤任务的细粒度分析。  \n2. 研究者无法准确识别失败模式，导致改进过程变得困难和不确定。  \n3. 手动标注每个子步骤的成功与否既费时又主观，亟需自动化解决方案。  \n\n【用了什么创新方法】  \n提出了一种名为StepEval的框架，利用视觉-语言模型（VLM）自动评估机器人操作中的子目标成功率（SR）。该框架以每个子目标的SR向量作为主要评估指标，旨在提供细粒度的任务完成情况反馈。通过将VLM作为黑箱评判者，StepEval能够从记录的图像中推断出子目标结果，且不需要对政策或环境进行额外的仪器化。该方法保持轻量级和模型无关，促进了社区的开放源代码贡献，推动了细化评估的标准化实践。最终，StepEval为机器人操作的评估提供了更高效和准确的途径。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Robust Near-Optimal Nonlinear Target Enclosing Guidance",
            "authors": "Abhinav Sinha,Rohit V. Nanavati",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO); Optimization and Control (math.OC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19477",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19477",
            "arxiv_html_link": "https://arxiv.org/html/2509.19477v1",
            "abstract": "This paper proposes a nonlinear optimal guidance law that enables a pursuer to enclose a target within arbitrary geometric patterns, which extends beyond conventional circular encirclement. The design operates using only relative state measurements and formulates a target enclosing guidance law in which the vehicle’s lateral acceleration serves as the steering control, making it well-suited for aerial vehicles with turning constraints. Our approach generalizes and extends existing guidance strategies that are limited to target encirclement and provides a degree of optimality. At the same time, the exact information of the target’s maneuver is unnecessary during the design. The guidance law is developed within the framework of a state-dependent Riccati equation (SDRE), thereby providing a systematic way to handle nonlinear dynamics through a pseudo-linear representation to design locally optimal feedback guidance commands through state-dependent weighting matrices. While SDRE ensures near-optimal performance in the absence of strong disturbances, we further augment the design to incorporate an integral sliding mode manifold to compensate when disturbances push the system away from the nominal trajectory, and demonstrate that the design provides flexibility in a sense that the (possibly time-varying) stand-off curvature could also be treated as unknown. Simulations demonstrate the efficacy of the proposed approach.",
            "introduction": "Driven by increasing autonomy requirements, unmanned aerial vehicles (UAVs) have become integral to a variety of mission-critical operations such as area coverage, surveillance, environmental monitoring, aerial defense, reconnaissance, and search and rescue, e.g., [1, 2, 3, 4]. A common control objective in these applications involves regulating the motion of a UAV designated as the pursuer to maintain a prescribed geometric configuration or trajectory with respect to a reference entity, termed the target. The target may correspond to another vehicle, a stationary beacon, or a dynamically evolving point of interest. This objective, referred to as target enclosing, entails enforcing spatial constraints on the relative position of the pursuer with respect to the target, and is central to both civilian and military guidance and coordination strategies.\n\nOne of the earliest investigations into target enclosing using teams of mobile robots was presented in [5], where the agents were modeled under the assumption of holonomic dynamics. Subsequent works extended this framework, leveraging the relative simplicity of control synthesis afforded by holonomic vehicle models. However, in practical scenarios, the dynamics of mobile platforms are more accurately represented by non-holonomic constraints, which significantly complicate both analysis and control design. Target enclosing under non-holonomic vehicle dynamics has thus been examined in a more realistic setting in works such as [6, 7], often within the broader context of distributed formation control. Within this domain, a task of interest is maintaining circular motion around a target (circumnavigation). This specific formation behavior has attracted substantial research attention due to its relevance in surveillance, tracking, and protection missions, as evidenced by a wide body of literature (e.g., [8, 9, 10, 11, 12]).\n\nOne approach is to leverage vector fields defined around the desired trajectory or geometric pattern to regulate the vehicle’s heading and generate convergence behavior. In [13], the authors employed a Lyapunov-based vector field to coordinate the circumnavigation of a stationary target at multiple altitudes using a fleet of UAVs. The work in [14] extended this methodology to account for external disturbances, proposing a coordinated standoff tracking strategy in the presence of wind. In contrast to purely reactive vector field approaches, the work in [15] presented a nonlinear model predictive control framework for dual-vehicle coordinated standoff tracking, optimizing performance over a finite prediction horizon. An alternative formulation in [11] utilized spherical pendulum-inspired dynamics, wherein backstepping and Lyapunov-based techniques were applied to regulate position and velocity tracking errors for robust guidance law synthesis. Beyond vector field and predictive control approaches, several recent strategies have explored guidance law design under partial or minimal state information, often motivated by scenarios with limited sensing or communication capabilities. These methods typically employ either relative range and or bearing measurements (see, for example, [12, 10, 16, 17, 8]).\n\nDespite such solutions, several key challenges remain unaddressed. This work advances the state of the art in target-enclosing guidance by addressing limitations of existing methods, which we summarize below:\n\nMany of the aforementioned methods assume slow or negligible target motion, which may become limited under adversarial scenarios if the target moves aggressively. The proposed framework explicitly accommodates a general maneuvering target without requiring knowledge of its guidance law or future maneuvers.\n\nWhile previous studies rely on curvature information of the enclosing geometry, our formulation has the flexibility to eliminate this requirement, thereby broadening applicability to arbitrary geometric patterns provided the prescribed range-parameterized curve is at least ℂ2\\mathbb{C}^{2}.\n\nWe introduce a robust nonlinear optimal guidance design for this problem class by integrating an SDRE framework with a supertwisting sliding mode controller defined over an integral sliding manifold. This hybrid design leverages the optimality of SDRE while inheriting robustness and finite-time convergence properties from sliding mode control, where the reaching phase is absent.\n\n1. Many of the aforementioned methods assume slow or negligible target motion, which may become limited under adversarial scenarios if the target moves aggressively. The proposed framework explicitly accommodates a general maneuvering target without requiring knowledge of its guidance law or future maneuvers.\n\n2. While previous studies rely on curvature information of the enclosing geometry, our formulation has the flexibility to eliminate this requirement, thereby broadening applicability to arbitrary geometric patterns provided the prescribed range-parameterized curve is at least ℂ2\\mathbb{C}^{2}.\n\n3. We introduce a robust nonlinear optimal guidance design for this problem class by integrating an SDRE framework with a supertwisting sliding mode controller defined over an integral sliding manifold. This hybrid design leverages the optimality of SDRE while inheriting robustness and finite-time convergence properties from sliding mode control, where the reaching phase is absent.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有方法假设目标运动缓慢，无法应对快速移动目标的情况。  \n2. 许多研究依赖于包围几何形状的曲率信息，限制了应用范围。  \n3. 如何设计一个鲁棒的非线性最优引导法则以应对复杂动态环境。  \n\n【用了什么创新方法】  \n本研究提出了一种非线性最优引导法则，能够在任意几何模式下包围目标，扩展了传统的圆形包围方法。该方法基于状态依赖里卡提方程(SDRE)框架，结合超扭滑模控制器，确保在存在扰动时的鲁棒性和有限时间收敛性。通过仿真验证了该方法的有效性，显示出其在动态环境中对目标的灵活适应能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Learning from Observation: A Survey of Recent Advances",
            "authors": "Returaj Burnwal,Hriday Mehta,Nirav Pravinbhai Bhatt,Balaraman Ravindran",
            "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO); Machine Learning (stat.ML)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19379",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19379",
            "arxiv_html_link": "https://arxiv.org/html/2509.19379v1",
            "abstract": "Imitation Learning (IL) algorithms offer an efficient way to train an agent by mimicking an expert’s behavior without requiring a reward function. IL algorithms often necessitate access to state and action information from expert demonstrations. Although expert actions can provide detailed guidance, requiring such action information may prove impractical for real-world applications where expert actions are difficult to obtain. To address this limitation, the concept of learning from observation (LfO) or state-only imitation learning (SOIL) has recently gained attention, wherein the imitator only has access to expert state visitation information. In this paper, we present a framework for LfO and use it to survey and classify existing LfO methods in terms of their trajectory construction, assumptions and algorithm’s design choices. This survey also draws connections between several related fields like offline RL, model-based RL and hierarchical RL. Finally, we use our framework to identify open problems and suggest future research directions.",
            "introduction": "Imitation learning, often studied as part of reinforcement learning (RL), has emerged as a promising paradigm for training autonomous agents to perform tasks effectively by mimicking demonstrator behavior (Hussein et al., 2017a). In contrast to RL, which relies on environment reward, IL has the distinct advantage of learning solely from expert guidance, which alleviates the need to design a reward function. This feature proves particularly beneficial in complex environments where crafting a suitable reward function can be challenging. Standard methods developed in this framework often require access to demonstrations, in the form of state-action (st,at)(s_{t},a_{t}) pairs. While expert actions can provide detailed guidance, requiring such action information may be restrictive in practical scenarios. For instance, a human expert may demonstrate the object-manipulation task to a robot by manually moving the robot’s arm. The robot can record both its joint angles (state) and joint torques (action) induced by the human expert. Collecting such state-action pair demonstrations can be challenging or, in some cases, impossible; such as environments with limited access to physical demonstrators.\n\nLearning from demonstration (LfD) aims to replicate the expert’s actions for a given state; this introduces a constraint that both the expert and the imitator must share the same dynamics model: specifically, they must have identical action spaces and the exact next-state transition probabilities for all feasible state-action pairs. This assumption brings severe limitations, as imagine that a robot with a low-speed limit imitates another robot that moves fast; then, using the LfD framework, a slow robot cannot learn to mimic the behavior of the fast-moving robot. The LfD framework does not support transfer learning across dynamically different agents (Liu et al., 2020a).\n\nWhile IL research is inspired by the way humans learn from observations of another human, there exists a notable difference in the imitation process employed by humans. Unlike LfD, which often relies on demonstrator’s action information to guide behavior, humans typically imitate without requiring explicit action information. A more general framework, known as learning from observation (LfO) or state-only imitation learning (SOIL), involves an expert communicating solely the state sequence information using raw sequences of true agent states, or partial observations like images or videos. In comparison to LfD, LfO aligns more closely with the natural way humans learn to imitate in the physical world, a phenomenon referred to as observational learning in psychology (Bandura and Walters, 1977). Moreover, the LfO framework can support transfer learning across dynamically different agents, as it does not rely on demonstrator’s action information.\n\nThe recent works on LfO have demonstrated significant success in both simulated environments and real-world tasks, however the literature lacks a systematic review of this field. Though a novel taxonomy for LfO was proposed by Torabi et al. (2019b), we found that the recent LfO algorithms either do not neatly fit or cannot be categorised based on the established taxonomy (see section 9 for a detailed discussion of related work). Hence, this article presents a survey of the literature based on the fundamental questions on trajectory dataset construction and algorithm’s design choices.\n\nOverview\n\nThe goal of this article is to present a comprehensive overview of learning from observation and to provide an over-arching framework to formalize this class of methods. We aim to define classification criteria based on the trajectory data set’s construction and algorithms’ design choices. We address the following fundamental questions to structure our classification:\n\nWho qualifies as an expert? An expert can be a human, a distinct dynamical agent from the imitator, or the same dynamical agent as the imitator. The formulation of the algorithm varies depending on the identity of the expert with respect to the imitator. (Section 3.1)\n\nHow are expert trajectories collected? These trajectories can be collected in two primary ways: i) in a first-person viewpoint, where the state trajectories are collected from the agent-centric viewpoint. Collecting first-person demonstrations can be challenging or, in some instances, impossible. A more natural way would be to collect demonstrations from a ii) third-person viewpoint. (Section: 3.2)\n\nWhat are the different trajectory datasets? In section 3.3, we survey different LfO algorithms based on the trajectory dataset they use to learn from expert demonstrations. A trajectory dataset can contain demonstrations from a single or multiple experts. Additionally, we provide a discussion of how the trajectory dataset varies depending on whether the algorithm operates online or offline.\n\nHow are algorithms formulated to facilitate learning from state-only demonstrations? In section 4, we define and discuss our framework for classifying LfO approaches based on algorithm design choices.\n\nIn the next section, we will provide the necessary background for LfO algorithms. Section 3, 4 forms the survey’s core and will provide a comprehensive overview of the recent LfO methodologies. In Section 5, we discuss how these algorithms are used to address learning across different viewpoints and non-identical agents. In Section 6, we discuss the datasets, benchmarks and algorithms used in LfO research. Following that, in Section 7, we will discuss the connections between LfO algorithms and other related fields in RL. Subsequently, in Section 8, we will use our framework to identify open problems and discuss future directions. Lastly, we will conclude the article by discussing related surveys in Section 9 and summarize this article in Section 10.\n\n1. Who qualifies as an expert? An expert can be a human, a distinct dynamical agent from the imitator, or the same dynamical agent as the imitator. The formulation of the algorithm varies depending on the identity of the expert with respect to the imitator. (Section 3.1)\n\n2. How are expert trajectories collected? These trajectories can be collected in two primary ways: i) in a first-person viewpoint, where the state trajectories are collected from the agent-centric viewpoint. Collecting first-person demonstrations can be challenging or, in some instances, impossible. A more natural way would be to collect demonstrations from a ii) third-person viewpoint. (Section: 3.2)\n\n3. What are the different trajectory datasets? In section 3.3, we survey different LfO algorithms based on the trajectory dataset they use to learn from expert demonstrations. A trajectory dataset can contain demonstrations from a single or multiple experts. Additionally, we provide a discussion of how the trajectory dataset varies depending on whether the algorithm operates online or offline.\n\n4. How are algorithms formulated to facilitate learning from state-only demonstrations? In section 4, we define and discuss our framework for classifying LfO approaches based on algorithm design choices.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在缺乏专家动作信息的情况下进行模仿学习？  \n2. 如何分类和评估现有的学习观察（LfO）方法？  \n3. LfO方法如何支持不同动态代理之间的迁移学习？  \n4. 当前LfO领域存在哪些开放问题和未来研究方向？  \n\n【用了什么创新方法】  \n本文提出了一个框架用于学习观察（LfO），并对现有的LfO方法进行分类，重点关注轨迹构建、假设和算法设计选择。通过对不同专家身份、轨迹收集方式和数据集的分析，本文系统性地整理了LfO领域的研究进展。此外，文章还探讨了LfO与其他强化学习相关领域的联系，并识别了当前研究中的开放问题和未来的研究方向。通过这些分析，本文为LfO的研究提供了一个全面的视角和结构化的分类标准。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Scensory: Automated Real-Time Fungal Identification and Spatial Mapping",
            "authors": "Yanbaihui Liu,Erica Babusci,Claudia K. Gunsch,Boyuan Chen",
            "subjects": "Signal Processing (eess.SP); Robotics (cs.RO)",
            "comment": "Our project website is at:this http URL",
            "pdf_link": "https://arxiv.org/pdf/2509.19318",
            "code": "http://generalroboticslab.com/Scensory",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19318",
            "arxiv_html_link": "https://arxiv.org/html/2509.19318v1",
            "abstract": "Indoor fungal contamination poses significant risks to public health, yet existing detection methods are slow, costly, and lack spatial resolution. Conventional approaches rely on laboratory analysis or high-concentration sampling, making them unsuitable for real-time monitoring and scalable deployment. We introduce Scensory, a robot-enabled olfactory system that simultaneously identifies fungal species and localizes their spatial origin using affordable volatile organic compound (VOC) sensor arrays and deep learning. Our key idea is that temporal VOC dynamics encode both chemical and spatial signatures, which we decode through neural architectures trained on robot-automated data collection. We demonstrate two operational modes: a passive multi-array configuration for environmental monitoring, and a mobile single-array configuration for active source tracking. Across five fungal species, our system achieves up to 89.85% accuracy in species detection and 87.31% accuracy in localization under ambient conditions, where each prediction only takes 3–7 s sensor inputs. Additionally, by computationally analyzing model behavior, we can uncover key biochemical signatures without additional laboratory experiments. Our approach enables real-time, spatially aware fungal monitoring and establishes a scalable and affordable framework for autonomous environmental sensing.",
            "introduction": "Indoor fungal contamination poses significant risks to human health and building safety [1]. Mold thrives in damp and poorly ventilated environments, especially in older homes and infrastructure [2]. Exposure to airborne fungal emissions has been linked to asthma, allergies, and other respiratory illnesses, particularly in children and people with weakened immune systems [3, 4]. Yet current detection methods are slow, invasive, and expensive. They require laboratory processing and offer limited information about the spatial distribution of contamination. The lack of scalable, real-time, and spatially aware detection systems continues to hinder timely intervention and effective mitigation of fungal exposure risks.\n\nTraditional fungal detection methods are constrained by high cost, long processing time, and technical complexity. Culture-based assays aim to grow fungal colonies from surface or air samples and identify species through morphological or physiological features [5, 6]. While widely used, these approaches require incubation periods that can delay results, and the subsequent identification process is often manual, requiring expert interpretation or additional laboratory analysis.\nMicroscopic identification of spores and hyphae requires expert interpretation, offers limited species resolution, and lacks sensitivity at low contamination levels [7]. Molecular diagnostics such as qPCR and ITS-based sequencing improves sensitivity and specificity [8], but require sample preprocessing and specialized laboratory infrastructure, and can be confounded by DNA from nonviable or nonpathogenic organisms [9]. While accurate, these methods are impractical for routine monitoring or rapid field deployment.\n\nAs a non-invasive alternative, volatile organic compound (VOC) analysis has gained attention. Fungi emit diverse VOCs during metabolism, including alcohols, ketones, and sulfur-based compounds, that serve as chemical fingerprints [10, 11, 12]. Electronic nose (eNose) systems [13] use arrays of cross-sensitive sensors to capture these emissions, increasingly paired with machine learning classifiers. Recent advances have enabled real-time fungal detection across food, agriculture, and environmental applications [14, 15, 16, 17]. With appropriate training, ML-enhanced eNoses have achieved species-level accuracy exceeding 90% in controlled settings [18, 19]. However, these systems remain spatially blind and typically require sealed chambers, pre-concentration, or extended exposure times [20]. They cannot localize fungal sources in ambient real-world environments [21].\n\nIn parallel, mobile robots and autonomous agents have made progress in gas plume tracking and odor source localization under turbulent or outdoor conditions [22, 23, 24, 25]. These strategies leverage visible or high-concentration plumes and rely on structured airflow dynamics [26, 27], making them unsuitable for detecting the subtle, low-density, and highly variable emissions characteristic of indoor fungal growth.\n\nDespite substantial advances, existing technologies can either identify fungal species without spatial resolution, or they can localize odor sources but cannot handle complex and low-level VOC signatures from fungi. Such a disconnect highlights a critical gap: we lack a scalable and real-time approach that can simultaneously determine “what” fungal species are present and “where” they are distributed under natural and low concentration conditions.\n\nWe present Scensory, a robot-enabled chemical sensing system that performs simultaneous species identification and source localization using affordable VOC sensor arrays and deep learning. The key insight is that complex temporal VOC dynamics encode both chemical identity and spatial information, which can be decoded using neural networks. We demonstrate this system in two operational modes: a passive multi-array configuration for environmental monitoring, and a mobile single-array configuration for real-world deployments where only limited resources are available, or rapid, mobile, or cost-effective solutions are required. Furthermore, our modeling framework allows us to infer which sensor inputs are most biochemically relevant for species identification.\n\nTo enable scalable model training, we automate high-throughput data collection using a robotic arm that samples VOCs under various species and spatial arrangements. Across five phylogenetically diverse fungal species, our system achieves up to 89.85% accuracy in species classification and 87.31% spatial localization accuracy under ambient conditions, where each prediction only takes 3–7 s sensor inputs. We further demonstrate the potential of our approach through real-world deployment on a mobile robot in a residential environment. Our results establish a scalable and practical framework for autonomous real-time fungal monitoring that bridges chemical perception and spatial inference, paving a way to advance the state of the art in environmental sensing, health diagnostics, and autonomous robotic inspection.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有室内真菌检测方法速度慢、成本高且缺乏空间分辨率。  \n2. 传统方法无法实时监测和定位真菌源，影响及时干预和有效减轻风险。  \n3. 现有技术无法同时识别真菌种类和其空间分布。  \n\n【用了什么创新方法】  \n我们提出了Scensory，一个机器人辅助的化学传感系统，能够同时进行真菌种类识别和源定位。该系统利用经济实惠的挥发性有机化合物（VOC）传感器阵列和深度学习，解码复杂的VOC动态信号。通过自动化的高通量数据收集，我们的系统在五种真菌物种上实现了高达89.85%的种类分类准确率和87.31%的空间定位准确率。该方法在真实环境中展示了其可扩展性和实用性，为自主实时真菌监测奠定了基础。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        }
    ],
    "2025-09-26": [
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds",
            "authors": "Luis Augenstein,Noémie Jaquier,Tamim Asfour,Leonel Rozo",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21281",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21281",
            "arxiv_html_link": "https://arxiv.org/html/2509.21281v1",
            "abstract": "Human-like motion generation for robots often draws inspiration from biomechanical studies, which often categorize complex human motions into hierarchical taxonomies. While these taxonomies provide rich structural information about how movements relate to one another, this information is frequently overlooked in motion generation models, leading to a disconnect between the generated motions and their underlying hierarchical structure. This paper introduces the  Gaussian Process Hyperbolic Dynamical Model (GPHDM), a novel approach that learns latent representations preserving both the hierarchical structure of motions and their temporal dynamics to ensure physical consistency.\nOur model achieves this by extending the dynamics prior of the Gaussian Process Dynamical Model (GPDM) to the hyperbolic manifold and integrating it with taxonomy-aware inductive biases. Building on this geometry- and taxonomy-aware frameworks, we propose three novel mechanisms for generating motions that are both taxonomically-structured and physically-consistent: two probabilistic recursive approaches and a method based on pullback-metric geodesics. Experiments on generating realistic motion sequences on the hand grasping taxonomy show that the proposed GPHDM faithfully encodes the underlying taxonomy and temporal dynamics, and generates novel physically-consistent trajectories.",
            "introduction": "Designing robots with human-like capabilities is a long-standing goal in robotics, often drawing inspiration from biomechanics to achieve realistic and functional motions [2]. A critical aspect in this process is analyzing human movements, for which researchers often structure complex actions into hierarchical classifications known as taxonomies. These taxonomies, which categorize hand postures [3], full-body poses [4], and manipulation primitives [5], among others, carry a rich hierarchical structure that reflects the relationships among different movements. However, this crucial structural information is frequently ignored in motion generation literature. Recent works [6, 7] have shown that leveraging this structural inductive bias into motion generation models may alleviate the demand of training data, lead to human-like movements, and ultimately generate new motions that comply with the hierarchical structure of a given taxonomy.\n\nEarly approaches to generating motions within these taxonomies showed promise but did not directly leverage their hierarchical nature. For example, Romero et al. [8] used a  Gaussian Process Latent Variable Model (GPLVM) [9], where clusters corresponding to various grasp types of the GRASP taxonomy [3] emerged naturally, despite that the taxonomy’s structure was not an explicitly considered during training. They identified the potential for generating new motions via latent space interpolation as potential future work. Separately, Mandery et al. [10] abstracted the motion generation problem as a linguistic task, using probabilistic nn-gram language models to reproduce whole-body motions based on the types of whole-body poses proposed in [4]. Discrete body poses were represented by words and sequenced using sentences. However, this discrete representation struggled to capture the continuous nature of movement and overlooked the entire hierarchical structure of the associated taxonomy.\n\nTo address this gap, recent work by Jaquier et al. [6] introduced the  Gaussian Process Hyperbolic Latent Variable Model (GPHLVM), which explicitly accounts for the hierarchical structure of taxonomy data. Their key insight was to leverage hyperbolic geometry [11], a natural fit for embedding tree-like structures [12, 13, 14], to create a continuous latent representation of the hierarchically-organized taxonomy data. In the GPHLVM, high-dimensional observations (e.g., joint angles of a human body or hand) belonging to the same taxonomy node were embedded closely together, forming distinct clusters in the hyperbolic space. Crucially, they demonstrated that hyperbolic geodesics between these clusters correctly traversed intermediate clusters, mirroring the parent–child relationships of the original taxonomy. These insights confirmed that the GPHLVM could successfully learn a latent space that preserves the hierarchical structure of complex motion data.\nHowever, a significant limitation remains: While the GPHLVM can generate novel motions by decoding latent geodesics back to the high-dimensional joint space, some of the resulting motions can be physically impractical. This issue arises because the GPHLVM is trained on data concentrated within the clusters (i.e., static poses) of the taxonomy, leaving data-sparse regions in-between. In these regions, the model lacks information about valid trajectories and thus its predictions revert to non-informative mean, failing to capture the underlying dynamics of target movements.\n\nThis paper tackles this challenge. First, we propose the Gaussian Process Hyperbolic Dynamical Model (GPHDM), which learns latent representations that preserve not only the hierarchical structure of motions but also their temporal dynamics to ensure physical consistency (see Fig. LABEL:fig:teaser). To achieve this, we extend the dynamics model of the  Gaussian Process Dynamical Model (GPDM) [15] to the hyperbolic manifold and integrate it into the taxonomy-aware framework of GPHLVM. Second, we introduce three novel motion generation mechanisms that are both taxonomically-structured and physically-consistent, namely: two probabilistic recursive approaches, and a pullback-metric geodesics method. We test our approach on the hand grasping taxonomy [1], showing that the proposed GPHDM successfully preserves both the hierarchical structure and temporal dynamics of the data, while allowing us to generate novel physically-consistent motions.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将人类运动的层次结构有效融入机器人运动生成模型中。  \n2. 如何在生成运动时确保物理一致性，避免生成不切实际的运动轨迹。  \n\n【用了什么创新方法】  \n本文提出了高斯过程超曲面动力学模型（GPHDM），通过扩展高斯过程动力学模型（GPDM）到超曲面，并结合层次结构的归纳偏置，学习保留运动的层次结构和时间动态的潜在表示。提出了三种新颖的运动生成机制，包括两种概率递归方法和基于回拉度量测地线的方法。实验结果表明，GPHDM能够忠实地编码运动的层次结构和时间动态，同时生成新颖且物理一致的运动轨迹。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "\\LARGE GMP$^{3}$: Learning-Driven, Bellman-Guided Trajectory Planning for UAVs in Real-Time on SE(3)",
            "authors": "Babak Salamat,Dominik Mattern,Sebastian-Sven Olzem,Gerhard Elsbacher,Christian Seidel,Andrea M. Tonello",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21264",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21264",
            "arxiv_html_link": "https://arxiv.org/html/2509.21264v1",
            "abstract": "We propose GMP3\\text{GMP}^{3}, a multiphase global path planning framework that generates dynamically feasible three-dimensional trajectories for unmanned aerial vehicles (UAVs) operating in cluttered environments. The framework extends traditional path planning from Euclidean position spaces to the Lie group SE​(3)\\mathrm{SE}(3), allowing joint learning of translational motion and rotational dynamics. A modified Bellman-based operator is introduced to support reinforcement learning (RL) policy updates while leveraging prior trajectory information for improved convergence. GMP3\\text{GMP}^{3} is designed as a distributed framework in which agents influence each other and share policy information along the trajectory: each agent refines its assigned segment and shares with its neighbors via a consensus-based scheme, enabling cooperative policy updates and convergence toward a path shaped globally even under kinematic constraints. We also propose DroneManager, a modular ground control software that interfaces the planner with real UAV platforms via the MAVLink protocol, supporting real-time deployment and feedback. Simulation studies and indoor flight experiments validate the effectiveness of the proposed method in constrained 3D environments, demonstrating reliable obstacle avoidance and smooth, feasible trajectories across both position and orientation. The open-source implementation is available at https://github.com/Domattee/DroneManager.",
            "introduction": "Recent advances in control systems have driven the widespread adoption of unmanned vehicles—including underwater, ground, and aerial systems (UAVs)—for a variety of applications. Classical approaches to motion planning, such as sampling-based methods, trajectory interpolation [1] or control based [2], often rely on complete prior knowledge of the workspace and can struggle to scale or adapt to high dimensional or dynamic scenarios. As a result, many recent studies have focused on integrating machine learning and reinforcement leaning (RL) techniques into path planning to enable adaptability, improved performance, and broader applicability [3, 4, 5].\n\nActive Simultaneous Localization and Mapping (SLAM) methods (e.g., [6]) have explored how to acquire new information about an unknown environment, actively balancing exploration and exploitation. The relevance of these works brings attention to the necessity of policy design for navigation under limited or uncertain global knowledge. However, many active SLAM frameworks are not specifically designed for fully autonomous path planning in dynamic or cluttered spaces; they often focus on maintaining accurate localization and building maps, rather than enforcing strict dynamic feasibility or advanced multi-agent coordination.\n\nDeep neural networks have opened new avenues in path planning, as demonstrated in [7], where Motion Planning Networks leverage a learned heuristic to generate sub-optimal paths. These architectures significantly reduce execution time compared to classical methods like RRT∗ [8] or A∗ [9]. On the other hand, they may require extensive offline training data and can suffer from domain shift when deployed in previously unseen contexts or dynamic environments. Similarly, cooperative motion planning in multi-robot or congested scenarios has been addressed through congestion-aware RL-based approaches [10], which improves real-time decision-making yet often requires delicate reward tuning and can face challenges with global convergence under high robot density. In unknown and partially observable workspaces, an integral RL framework has been proposed to unify mapping, localization, and policy improvement under uncertain conditions [11], but many solutions must still combat high sample complexity and may lack strong safety guarantees. For ground vehicles operating in harsh terrains, inverse RL has shown promise in learning sophisticated cost maps directly from expert trajectories [12], although it can be sensitive to distribution mismatches between training and execution conditions. Other learning-based planners—such as those that combine global and local policies [13], exploit dynamic graph representations for online adaptation [13], or focus on UAV data gathering using time-varying Q-learning [14]—highlight the versatility of learning methods in a range of planning problems. Nevertheless, these approaches can be limited by high-dimensional action spaces, restrictive sensor assumptions, and the risk of suboptimal exploration strategies. However, developing and rigorously validating advanced control algorithms [15, 16]—and, by extension, inertial-navigation and sensor-fusion schemes [17, 18]—demands access to the full system state. This comprises not only the UAV’s translational motion (its position and the time evolution thereof) but also its rotational attitude, conventionally expressed by the pitch, roll, and yaw angles. To the best of the authors’ knowledge, no publicly available simulator can yet generate feasible representative full six–degree–of–freedom (6-DoF) trajectories that capture both translational and rotational dynamics with realistic characteristics, thereby limiting systematic controller design and benchmarking.\n\nRecently, there has also been a surge of interest in adaptive RL-based UAV planners for search and rescue [19, 20] and in hybrid optimization learning schemes for accurate multi-joint manipulator motion [21]. While these methods show improved responsiveness and adaptability, they may encounter convergence hurdles or suffer from excessive computational costs when the planning horizon and action dimensionality grow.\n\nIn recent advancements in reinforcement learning (RL) for trajectory optimization, the problem of autonomous path planning has been approached using Markov Decision Processes (MDPs) to minimize cumulative loss functions that account for trajectory smoothness, obstacle avoidance, and motion constraints. Traditional methods often rely on optimization-based approaches such as polynomial trajectory planning, which may not generalize well across dynamic environments. Instead, RL-based methods allow for adaptive learning of trajectory policies, where agents collaborate to modify and refine trajectories in real-time. Building on our previous work [22] that introduced the Global Multi-Phase Path Planning (GMP3\\text{GMP}^{3}) concept in 2D environments, this paper significantly extends the framework and presents several new contributions:\n\nExtension of the GMP3\\text{GMP}^{3} framework to the Lie group SE​(3)\\mathrm{SE}(3), enabling full six-degree-of-freedom (6-DoF) trajectory planning that incorporates both translation and rotation.\n\nWe introduce an influence-aware policy update rule that balances local adaptation, historical best performance, and global alignment.\n\nA consensus-based interaction protocol is employed to align neighboring agents through weighted policy sharing over a dynamic graph.\n\nGMP3\\text{GMP}^{3} offers a direct formulation and solution for policy optimization updates.\n\nImplementation of a modular, real-time drone control interface, DroneManager, to deploy GMP3\\text{GMP}^{3} on physical UAV platforms with minimal reconfiguration.\n\nExtensive validation in both simulation and real-world indoor experiments, demonstrating the robustness, adaptability, and practical viability of the proposed method in complex 3D environments.\n\nThe structure of the paper is as follows. Section II defines the formal problem and introduces the multi-phase trajectory learning setup in a three-dimensional environment. Section III presents the reinforcement learning-based update mechanism, including the formulation of a modified Bellman operator tailored to the trajectory optimization framework. Section IV describes five optimization strategies employed to enhance convergence speed and learning stability. Section V reports simulation results that evaluate the effectiveness of the proposed method under various scenarios. Section VI introduces the custom-developed DroneManager software, which facilitates real-time control and coordination of physical UAVs. Section VII presents the results of real-world experiments that validate the practicality and robustness of the proposed approach. The paper concludes with a summary of findings and outlines potential directions for future research.\n\nNotations: The notation ⟨X⟩\\langle X\\rangle denotes the mean of the quantity XX.\n\n1. Extension of the GMP3\\text{GMP}^{3} framework to the Lie group SE​(3)\\mathrm{SE}(3), enabling full six-degree-of-freedom (6-DoF) trajectory planning that incorporates both translation and rotation.\n\n2. We introduce an influence-aware policy update rule that balances local adaptation, historical best performance, and global alignment.\n\n3. A consensus-based interaction protocol is employed to align neighboring agents through weighted policy sharing over a dynamic graph.\n\n4. GMP3\\text{GMP}^{3} offers a direct formulation and solution for policy optimization updates.\n\n5. Implementation of a modular, real-time drone control interface, DroneManager, to deploy GMP3\\text{GMP}^{3} on physical UAV platforms with minimal reconfiguration.\n\n6. Extensive validation in both simulation and real-world indoor experiments, demonstrating the robustness, adaptability, and practical viability of the proposed method in complex 3D environments.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在动态和复杂的3D环境中为无人机生成可行的轨迹？  \n2. 如何结合强化学习和传统路径规划方法以提高路径规划的适应性和性能？  \n3. 如何实现多智能体之间的协作以优化路径规划？  \n\n【用了什么创新方法】  \n提出了GMP3框架，扩展到Lie群SE(3)，实现六自由度的轨迹规划，结合了局部适应性和全局一致性的影响感知策略更新规则。通过共识基础的交互协议，智能体之间共享策略信息，促进协作更新。实现了一个模块化的实时控制接口DroneManager，支持在真实无人机平台上的部署。经过广泛的仿真和实际飞行实验验证，GMP3展示了在复杂3D环境中的鲁棒性和适应性，能够可靠地避开障碍并生成平滑的轨迹。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "BiNoMaP: Learning Category-Level Bimanual Non-Prehensile Manipulation Primitives",
            "authors": "Huayi Zhou,Kui Jia",
            "subjects": "Robotics (cs.RO)",
            "comment": "under review",
            "pdf_link": "https://arxiv.org/pdf/2509.21256",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21256",
            "arxiv_html_link": "https://arxiv.org/html/2509.21256v1",
            "abstract": "Non-prehensile manipulation, encompassing ungraspable actions such as pushing, poking, and pivoting, represents a critical yet underexplored domain in robotics due to its contact-rich and analytically intractable nature. In this work, we revisit this problem from two novel perspectives. First, we move beyond the usual single-arm setup and the strong assumption of favorable external dexterity such as walls, ramps, or edges. Instead, we advocate a generalizable dual-arm configuration and establish a suite of Bimanual Non-prehensile Manipulation Primitives (BiNoMaP). Second, we depart from the prevailing RL-based paradigm and propose a three-stage, RL-free framework to learn non-prehensile skills. Specifically, we begin by extracting bimanual hand motion trajectories from video demonstrations. Due to visual inaccuracies and morphological gaps, these coarse trajectories are difficult to transfer directly to robotic end-effectors. To address this, we propose a geometry-aware post-optimization algorithm that refines raw motions into executable manipulation primitives that conform to specific motion patterns. Beyond instance-level reproduction, we further enable category-level generalization by parameterizing the learned primitives with object-relevant geometric attributes, particularly size, resulting in adaptable and general parameterized manipulation primitives. We validate BiNoMaP across a range of representative bimanual tasks and diverse object categories, demonstrating its effectiveness, efficiency, versatility, and superior generalization capability.",
            "introduction": "Non-prehensile manipulation refers to a class of robotic actions that do not rely on firm grasping but instead leverage physical interactions such as poking, or pivoting, or pushing to achieve manipulation goals Zhou et al. (2019); Hogan & Rodriguez (2020); Sun et al. (2020); Zhou & Held (2023); Zhang et al. (2023). These skills are not merely complementary to traditional grasp-based tasks; they are often essential in scenarios where grasping is physically infeasible or inefficient. In dual-arm robotic systems Liu et al. (2022); Wu & Kruse (2024); Yamada et al. (2025); Lu et al. (2025), non-prehensile manipulation becomes especially relevant when dealing with objects that are too fragile, too flat, or lack sufficient geometry for reliable grasping.\n\nDespite its importance, current non-prehensile manipulation faces two core bottlenecks. First, most existing works operate under the simplifying assumption of a unimanual setting, often coupled with highly structured environments Zhou et al. (2023); Cho et al. (2024); Wu et al. (2024); Lyu et al. (2025); Li et al. (2025a). To compensate for the lack of control authority, these methods rely on artificial aids such as vertical walls, inclined planes, or boundaries to stabilize and direct object motion. However, in real-world deployments, such assumptions are rarely satisfied due to environmental unpredictability or object fragility. Consider a scenario where a thin rectangular cardboard box lies flat on a tabletop—walls and ramps are unavailable, and poking the box may damage its contents. In such cases, a more general solution is to exploit bimanual coordination Krebs & Asfour (2022); Grannen et al. (2023), where one arm can serve as a stabilizing reference while the other executes the non-prehensile action. This configuration not only replaces inflexible external constraints with adaptive internal ones but also enables complex skills such as dual-arm wrapping Grotz et al. (2024); Lu et al. (2025); Zhou et al. (2025); Liu et al. (2025a) or cluttered object singulation Jiang et al. (2024b); Xu et al. (2025a), which are inaccessible to single-arm systems.\n\nThe second bottleneck lies in the heavy reliance on reinforcement learning (RL) frameworks Schulman et al. (2017); Haarnoja et al. (2018); Fujimoto et al. (2018). Most advanced approaches require constructing task-specific simulators that model manipulators, top-tables, and object dynamics, followed by lengthy policy training with dense environment interactions and carefully engineered reward functions. These RL pipelines are often sensitive to hyperparameter tuning and face substantial sim-to-real gaps due to inaccuracies in simulated physics, including mass distributions, contact dynamics, or friction coefficients. While recent works attempt to mitigate this gap via world models or differentiable simulators Lyu et al. (2025); Li et al. (2025a); Huang et al. (2025), they still inherit the inherent limitations of RL, including slow convergence and poor generalization. To the best of our knowledge, our work is the first to propose a fully RL-free paradigm for learning bimanual non-prehensile manipulation skills through imitation and geometric reasoning.\n\nTo this end, we present a three-stage paradigm that combines hardware generality with algorithmic efficiency. We employ a dual-arm setup with parallel-jaw grippers, which not only supports unimanual non-prehensile skills but also enables more complex bimanual ones. In the first stage, inspired by prior work on learning from human demonstrations Grauman et al. (2022; 2024); Chen et al. (2025); Papagiannis et al. (2025), we extract primitive bimanual motion trajectories from human hand videos for task-specific non-prehensile behaviors. Unlike grasp-based tasks, where 1–3 cm errors in hand-object alignment may be tolerable, non-prehensile tasks are extremely sensitive to deviations: even 3–5 mm misalignment can lead to premature contact loss or over-compression, causing instability or failure. To address this, our second stage introduces a geometry-aware post-optimization algorithm that leverages object shape priors to refine these noisy trajectories into smooth, task-specific motion primitives. These refined trajectories, which we term Bimanual Non-Prehensile Manipulation Primitives (BiNoMaP), exhibit high success rates and are robust to object pose variations. In the third stage, we further extend these primitives to unseen objects within the same category by parameterizing them with object-specific geometric attributes, such as the length and width of a box or the diameter of a sphere. This results in a family of Parameterized Manipulation Primitives that are adaptive and transferable across diverse category-level instances.\n\nWe extensively evaluate BiNoMaP on a diverse set of non-prehensile dual-arm skills, including poking, pivoting, pushing, and wrapping (Fig. 1). We test them on various objects with varying shapes, materials, and physical properties. To prove the effectiveness and efficiency, we compare against strong baselines Zhao et al. (2023a); Chi et al. (2023); Ze et al. (2024); Zhou et al. (2023); Cho et al. (2024); Lyu et al. (2025). Our results show that BiNoMaP consistently achieves higher success rates across tasks. Furthermore, we demonstrate how BiNoMaP can be integrated with high-level vision-language models (VLMs) Xiao et al. (2024); Ravi et al. (2025) to support advanced robotic behaviors, such as pre-grasping under ungraspable conditions, tabletop rearrangement, and error recovery—bridging the gap between low-level skills and high-level autonomy.\n\nOur main contributions are three-fold: (i) We propose the first RL-free framework for learning Bimanual Non-Prehensile Manipulation Primitives directly from human video demonstrations. (ii) We introduce a parameterization scheme that enables category-level generalization of non-prehensile skills across diverse object instances. (iii) We demonstrate the effectiveness, efficiency, versatility, and generality of BiNoMaP across a variety of tasks, objects, and strong baselines.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现双臂非抓取操作的有效学习与应用。  \n2. 现有方法依赖于强化学习，导致训练过程复杂且难以推广。  \n3. 如何从人类演示中提取并优化非抓取动作轨迹。  \n\n【用了什么创新方法】  \n本研究提出了一种三阶段的RL-free框架，首先从人类视频中提取双臂运动轨迹，然后通过几何感知后优化算法将这些轨迹精炼为可执行的非抓取操作原语（BiNoMaP），最后通过参数化对象几何属性实现类别级别的推广。该方法在多种非抓取任务中展现了高成功率和强泛化能力，超越了传统的单臂设置和强化学习方法，展示了其在复杂场景中的有效性和适应性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models",
            "authors": "Jiyeon Koo,Taewan Cho,Hyunjoon Kang,Eunseom Pyo,Tae Gyun Oh,Taeryang Kim,Andrew Jaeyong Choi",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21243",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21243",
            "arxiv_html_link": "https://arxiv.org/html/2509.21243v1",
            "abstract": "Recent Vision-Language-Action (VLA) models demonstrate remarkable generalization in robotics but are restricted by their substantial size and computational cost, limiting real-world deployment. However, conventional lightweighting methods often sacrifice critical capabilities, particularly spatial reasoning. This creates a trade-off between efficiency and performance. To address this challenge, our work reuses Register Tokens, which were introduced for artifact removal in Vision Transformers but subsequently discarded. We suppose that these tokens contain essential spatial information and propose RetoVLA, a novel architecture that reuses them directly by injecting them into the Action Expert.",
            "introduction": "By integrating vast web-scale knowledge into robotic control, Vision-Language-Action (VLA) models such as RT-2 [1] and OpenVLA [2] have demonstrated remarkable generalization in understanding complex language instructions. However, this success relies on massive, billion-parameter models, which require substantial computational costs. This fundamental issue presents a significant challenge to their practical deployment on real-world robotic platforms with limited on-board computing resources.\n\nPrevious efforts to address this efficiency problem have primarily concentrated on physically reducing the model size, as seen in approaches like SmolVLA [4]. However, this inevitably leads to a trade-off between performance and efficiency. This requires a compromise on the extensive representational power of VLMs, particularly their ability to comprehend complex spatial relationships and long-term contexts. Simply discarding information risks compromising fundamental aspects of robotic intelligence in exchange for computational gains.\n\nThis paper examines the underlying issue, seeking a solution not in the ‘deletion’ of information, but in its ‘active reuse’. Our starting point is the work of [3], which revealed that modern large-scale Vision Transformers (ViT) [5] like DINOv2 [6] inherently produce artifacts known as high-norm outlier tokens during training. These tokens typically emerge from image patches relatively low informational content, such as blank walls or the sky. The model repurposes the representational space of these patches to serve as a temporary scratchpad for storing and processing internal global information. While this is a natural learning mechanism, it has been shown to deteriorate the local information of the corresponding patch tokens, significantly impairing performance on dense prediction tasks.\n\nThe proposed solution to this issue, the Register Token [3], provides an explicit ‘scratchpad’ for the model, thereby preventing the misuse of image patch tokens and refining the attention maps. However, after absorbing these artifacts, the Register Tokens [3] have been systematically discarded in downstream tasks. This leads us to a critical question that diverges from previous work: What is the true nature of this ‘global information,’ and is it merely noise to be discarded?\n\nIn the context of robotic manipulation, we propose that this information constitutes the essential Spatial Context of a scene—encompassing the overall layout, the 3D relationships between objects, and the structure of the workspace. For a robot, this information is not a disposable asset; it is critical.\n\nBuilding on this hypothesis, we propose RetoVLA (Reusing Register Tokens [3] VLA), which achieves both efficiency and high performance through the active reuse of information. As summarized in Figure 1, our core contributions are as follows: (1) Redefining the Role of Register Tokens [3] for Spatial Context Injection: A novel VLA architecture, detailed in Figure 2, that redefines the register token [3] from a passive ‘purifier’ used for artifact removal to an active ‘spatial context provider’. We have designed a novel module to directly inject these tokens as Key-Value pairs into the Action Expert’s attention mechanism, allowing it to leverage the global spatial context of the entire scene until the final phase of action generation. (2) Analyzing the Synergy with Efficient VLAs: We analyze how register token [3] injection compensates for the information loss that occurs in efficient models like SmolVLA [4], which reduce the depth of the VLM. Our approach proves to be an effective pathway for maintaining a high level of spatial reasoning with significantly lower computational overhead. (3) Experimental Validation: Through rigorous experiments on the LIBERO simulation benchmark and with the custom-built 7-DOF robot arm we developed for this research, we demonstrate that RetoVLA significantly outperforms the baseline model, particularly on long-horizon tasks that require a complex understanding of 3D spatial structures or multi-step sequential manipulation. In our real-robot experiments, we achieved an increase in the average success rate from 50.3% to 67.4%, a 17.1%p absolute improvement.\n\nThis research reconstructs conventional design principle by re-evaluating internal information flow, enabling a new class of low-cost, high-performance robotic intelligence.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在保持高性能的同时减少Vision-Language-Action模型的计算成本？  \n2. 如何有效利用Register Tokens以增强空间推理能力而不牺牲信息？  \n3. 如何在机器人操控任务中实现复杂的3D空间理解？  \n\n【用了什么创新方法】  \n本研究提出了RetoVLA架构，通过重新利用Register Tokens作为空间上下文提供者，直接将其注入到Action Expert的注意力机制中，从而增强模型的空间推理能力。通过这一方法，我们实现了在LIBERO仿真基准和自建7-DOF机器人臂上的实验验证，结果显示RetoVLA在长时间任务中的表现显著优于基线模型，成功率从50.3%提升至67.4%。这一创新方法有效解决了在高效模型中信息损失的问题，为低成本高性能的机器人智能提供了新的设计思路。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "FSGlove: An Inertial-Based Hand Tracking System with Shape-Aware Calibration",
            "authors": "Yutong Li,Jieyi Zhang,Wenqiang Xu,Tutian Tang,Cewu Lu",
            "subjects": "Robotics (cs.RO)",
            "comment": "Presented at IROS 2025, details are available atthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.21242",
            "code": "https://fsglove.robotflow.ai",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21242",
            "arxiv_html_link": "https://arxiv.org/html/2509.21242v1",
            "abstract": "Accurate hand motion capture (MoCap) is vital for applications in robotics, virtual reality, and biomechanics, yet existing systems face limitations in capturing high-degree-of-freedom (DoF) joint kinematics and personalized hand shape. Commercial gloves offer up to 21 DoFs, which are insufficient for complex manipulations while neglecting shape variations that are critical for contact-rich tasks. We present FSGlove, an inertial-based system that simultaneously tracks up to 48 DoFs and reconstructs personalized hand shapes via DiffHCal, a novel calibration method. Each finger joint and the dorsum are equipped with IMUs, enabling high-resolution motion sensing. DiffHCal integrates with the parametric MANO model through differentiable optimization, resolving joint kinematics, shape parameters, and sensor misalignment during a single streamlined calibration. The system achieves state-of-the-art accuracy, with joint angle errors of less than 2.7∘, and outperforms commercial alternatives in shape reconstruction and contact fidelity. FSGlove’s open-source hardware and software design ensures compatibility with current VR and robotics ecosystems, while its ability to capture subtle motions (e.g., fingertip rubbing) bridges the gap between human dexterity and robotic imitation. Evaluated against Nokov optical MoCap, FSGlove advances hand tracking by unifying the kinematic and contact fidelity. Hardware design, software, and more results are available at: https://sites.google.com/view/fsglove.",
            "introduction": "The human hand, a remarkably dexterous end-effector capable of executing intricate tasks, serves as both the inspiration and benchmark for robotic hand design and manipulation research. Accurate motion capture of the hand during manipulation is critical for collecting data essential to diverse downstream applications, including hand pose estimation, teleoperation, and imitation learning. Achieving high-fidelity hand motion capture hinges on two factors: joint kinematics and hand shape modeling.\n\nCurrently, the most comprehensive commercially available hand MoCap glove [1] can capture at most 21 degrees of freedom (DoFs). However, complex manipulations, such as thumb-index fingertip rubbing, are still beyond the ability to capture, as they require additional torsional DoFs in the proximal or middle phalanges that are often overlooked (Fig. 1), but recently, some dexterous robotic hands [2] tend to support control such DoFs. Similarly, hand shape variations, driven by differences in bone length and soft-tissue composition, demand personalized models for accurate motion reconstruction, particularly during contact-rich tasks involving objects or self-interaction. Existing hand motion capture systems, whether commercial [3, 4] or research-oriented [5, 6, 7], focus primarily on joint angle estimation with limited DoFs and neglect the shape variations. This oversight hinders the transfer of in-manipulation motions to virtual avatars or anthropomorphic robotic hands, where both kinematic and contact fidelity are paramount (Fig. 1).\n\nTo bridge this gap, we present FSGlove, a novel inertial-based hand tracking system simultaneously capturing up to 48-DoF joint kinematics and personalized hand shape. Each finger joint and the dorsal region are instrumented with inertial measurement units (IMUs), enabling high-resolution motion sensing. A key innovation is DiffHCal, a shape-aware calibration method that embeds personalized hand-shape estimation into a streamlined calibration protocol. By leveraging the parametric MANO hand model [8], DiffHCal aligns captured sensor data to MANO’s joint and shape parameters through a differentiable optimization process. This framework uses a series of predefined reference poses to simultaneously resolve skeletal joint parameters, shape parameters, and manual installation errors in a single optimization process. Moreover, this process integrates seamlessly into standard glove calibration workflows, requiring no additional steps compared with commercial systems while delivering contact-consistent hand models. To democratize access, we open-source the hardware design, low-level drivers for sensor calibration, and high-level interfaces for integration with motion capture ecosystems (e.g., OptiTrack, HTC Vive), enabling plug-and-play compatibility with robotics and VR frameworks.\n\nWe evaluate FSGlove across four metrics: (1) joint angle accuracy (≤2.7∘\\leq 2.7^{\\circ}), (2) shape reconstruction precision (mean mesh error ≤3.6\\leq 3.6mm), (3) fingertip tracking error (mean positional error ≤16\\leq 16mm), and (4) hand-object interaction fidelity (mean positional error ≤20\\leq 20mm), benchmarking against commercial glove systems. Despite its low-cost design, FSGlove achieves state-of-the-art performance in shape-aware tracking, surpassing commercial alternatives (e.g., Manus Metaglove Pro, VRTRIX) in contact-rich manipulation tasks.\n\nWe summarize our contributions as follows:\n\nFSGlove, the first open-source, high-DoF (up to 48 DoFs) data glove integrating inertial sensing with shape-aware calibration.\n\nDiffHCal, a differentiable calibration framework that infers personalized hand shape during standard wear-time procedures.\n\nComprehensive validation demonstrating high-precision accuracy in joint measurement and shape reconstruction.\n\n1. FSGlove, the first open-source, high-DoF (up to 48 DoFs) data glove integrating inertial sensing with shape-aware calibration.\n\n2. DiffHCal, a differentiable calibration framework that infers personalized hand shape during standard wear-time procedures.\n\n3. Comprehensive validation demonstrating high-precision accuracy in joint measurement and shape reconstruction.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现高精度的手部运动捕捉，特别是复杂操作中的关节运动和个性化手形？  \n2. 现有手部运动捕捉系统在捕捉高自由度关节运动和形状变化方面的局限性。  \n3. 如何在不增加额外步骤的情况下，优化手部传感器的校准流程？  \n\n【用了什么创新方法】  \nFSGlove是一个基于惯性传感器的手部追踪系统，能够同时捕捉高达48个自由度的关节运动和个性化手形。其核心创新在于DiffHCal，一个形状感知的校准方法，通过可微优化将捕获的传感器数据与MANO模型的关节和形状参数对齐。该方法在标准佩戴程序中推断个性化手形，简化了校准流程。FSGlove在关节角度精度、形状重建精度、指尖跟踪误差和手-物体交互保真度等多个指标上表现出色，超越了商业手套系统，展示了其在接触丰富的操作任务中的优势。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "SEEC: Stable End-Effector Control with Model-Enhanced Residual Learning for Humanoid Loco-Manipulation",
            "authors": "Jaehwi Jang,Zhuoheng Wang,Ziyi Zhou,Feiyang Wu,Ye Zhao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21231",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21231",
            "arxiv_html_link": "https://arxiv.org/html/2509.21231v1",
            "abstract": "Arm end-effector stabilization is essential for humanoid loco-manipulation tasks, yet it remains challenging due to the high degrees of freedom and inherent dynamic instability of bipedal robot structures. Previous model-based controllers achieve precise end-effector control but rely on precise dynamics modeling and estimation, which often struggle to capture real-world factors (e.g., friction and backlash) and thus degrade in practice. On the other hand, learning-based methods can better mitigate these factors via exploration and domain randomization, and have shown potential in real-world use. However, they often overfit to training conditions, requiring retraining with the entire body, and still struggle to adapt to unseen scenarios.\nTo address these challenges, we propose a novel stable end-effector control (SEEC) framework with model-enhanced residual learning that learns to achieve precise and robust end-effector compensation for lower-body induced disturbances through model-guided reinforcement learning (RL) with a perturbation generator. This design allows the upper-body policy to achieve accurate end-effector stabilization as well as adapt to unseen locomotion controllers with no additional training.\nWe validate our framework in different simulators and transfer trained policies to the Booster T1 humanoid robot. Experiments demonstrate that our method consistently outperforms baselines and robustly handles diverse and demanding loco-manipulation tasks.",
            "introduction": "Humanoid robots promise seamless integration into human environments, where they must walk and manipulate simultaneously. From carrying objects while moving to performing collaborative tasks [1, 2, 3], this capability is fundamental for practical humanoid deployment (see the tasks shown in Fig. SEEC: Stable End-Effector Control with Model-Enhanced\nResidual Learning for Humanoid Loco-Manipulation).\nYet, achieving stable and precise control of the arm end-effector during dynamic locomotion remains an open challenge.\nEven modest base movements could induce large accelerations at the arm end-effector, causing tracking errors, destabilizing contact forces, and ultimately limiting the utility of humanoids in real-world settings.\n\nRecently, learning-based approaches [4, 5, 6, 7] have sought to achieve humanoid loco-manipulation by training end-to-end reinforcement learning (RL) policies.\nWhile effective at capturing nonlinearities and handling uncertainty,\nthese policies often rely on imitating joint or task reference trajectories [8, 9, 10], and struggle to ensure accurate end-effector stabilization.\nFor example, in HOVER [10], uncontrolled hand motions emerge as a byproduct of locomotion.\nPrior work [7] has attempted to stabilize end-effector control by directly penalizing its acceleration, but this approach heavily relies on policy optimization to “discover” the right compensation strategy.\nAdditionally, the learned behavior degenerates into static hand-holding motions, limiting general applicability.\nMoreover, when tasks require reactive whole-body coordination, instability in end-effector control is exacerbated by sudden locomotion disturbances. Conventional RL training, as in [7], fails to provide robustness under such out-of-distribution (OOD) scenarios.\n\nInspired by model-based approaches [11, 12, 13, 14, 15], which achieve precise stabilization through dynamics modeling and online estimation, we introduce SEEC: Stable End-Effector Control, a model-enhanced RL framework for humanoid loco-manipulation.\nSEEC leverages model-based expertise to provide analytic acceleration compensation signals during training. Instead of relying on naive penalization, the compensation torque from the model-based formulation is distilled into the RL policy, addressing instability in a more principled manner.\n\nFurthermore, unlike prior works that jointly train manipulation and locomotion policies, we introduce a perturbation generation strategy that exposes the upper-body policy to a wide spectrum of locomotion-induced disturbances. By modeling these disturbances as base movement patterns, the upper-body controller learns to maintain stable arm end-effector control independent of any specific locomotion policy.\nThis modular design not only improves robustness across diverse walking patterns, allowing seamless transfer across different walking patterns, including previously unseen locomotion controllers, but also facilitates integration into complex loco-manipulation tasks that demand coherent whole-body coordination.\n\nOur core contributions can be summarized as follows.\n\nWe propose a model-enhanced residual learning framework that integrates model-based expertise with learning-based adaptability, achieving precise acceleration compensation while effectively addressing model inaccuracies and parameter uncertainties.\n\nWe introduce a base-movement data generation and perturbation generation strategy that exposes the policy to a broad spectrum of locomotion-relevant disturbances during training. This enables the controller to acquire robust compensation behaviors that can transfer to unseen locomotion controllers and gaits without requiring joint re-training.\n\nWe demonstrate the first deployment of such a hybrid framework on a full humanoid Booster T1, validating it both in simulation and on the real hardware via zero-shot transfer. The system achieves more stable and precise end-effector control across a variety of loco-manipulation tasks, compared to the baselines.\n\n1. We propose a model-enhanced residual learning framework that integrates model-based expertise with learning-based adaptability, achieving precise acceleration compensation while effectively addressing model inaccuracies and parameter uncertainties.\n\n2. We introduce a base-movement data generation and perturbation generation strategy that exposes the policy to a broad spectrum of locomotion-relevant disturbances during training. This enables the controller to acquire robust compensation behaviors that can transfer to unseen locomotion controllers and gaits without requiring joint re-training.\n\n3. We demonstrate the first deployment of such a hybrid framework on a full humanoid Booster T1, validating it both in simulation and on the real hardware via zero-shot transfer. The system achieves more stable and precise end-effector control across a variety of loco-manipulation tasks, compared to the baselines.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现人形机器人在动态行走中稳定和精确的末端执行器控制？  \n2. 现有模型基控制器在真实世界中因动态建模不精确而导致的性能下降问题。  \n3. 学习基方法在面对未见场景时的适应性不足和过拟合问题。  \n\n【用了什么创新方法】  \n提出了一种模型增强的残差学习框架（SEEC），结合模型驱动的强化学习和扰动生成策略，能够在动态行走中实现精确的末端执行器补偿。通过在训练中暴露于多种运动扰动，控制器能够在未见的运动控制器上无缝转移，且无需重新训练。实验结果表明，该方法在多种复杂的行走操控任务中表现出更高的稳定性和精确性，超越了基线方法。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Next-Generation Aerial Robots -- Omniorientational Strategies: Dynamic Modeling, Control, and Comparative Analysis",
            "authors": "Ali Kafili Gavgani,Amin Talaeizadeh,Aria Alasty,Hossein Nejat Pishkenari,Esmaeil Najafi",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21210",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21210",
            "arxiv_html_link": "https://arxiv.org/html/2509.21210v1",
            "abstract": "Conventional multi-rotors are under-actuated systems, hindering them from independently controlling attitude from position. In this study, we present several distinct configurations that incorporate additional control inputs for manipulating the angles of the propeller axes. This addresses the mentioned limitations, making the systems \"omniorientational\". We comprehensively derived detailed dynamic models for all introduced configurations and validated by a methodology using Simscape Multibody simulations.\nTwo controllers are designed: a sliding mode controller for robust handling of disturbances and a novel PID-based controller with gravity compensation integrating linear and non-linear allocators, designed for computational efficiency. A custom control allocation strategy is implemented to manage the input-non-affine nature of these systems, seeking to maximize battery life by minimizing the \"Power Consumption Factor\" defined in this study. Moreover, the controllers effectively managed harsh disturbances and uncertainties.\nSimulations compare and analyze the proposed configurations and controllers, majorly considering their power consumption. Furthermore, we conduct a qualitative comparison to evaluate the impact of different types of uncertainties on the control system, highlighting areas for potential model or hardware improvements. The analysis in this study provides a roadmap for future researchers to design omniorientational drones based on their design objectives, offering practical insights into configuration selection and controller design.\nThis research aligns with the project SAC-1, one of the objectives of Sharif AgRoLab.",
            "introduction": "Multi-rotors have become highly popular due to their versatility and agility, finding use in a wide range of applications. These include package transportation [1, 2, 3], search and rescue operations [4, 5, 6, 7], agriculture [8, 9, 10, 11, 12, 13, 14], non-destructive testing [15, 16], aerial surveillance and remote sensing [17, 18, 19, 20].\n\nConventional multi-rotors (e.g., a quadrotor) are under-actuated systems, hindering them from performing complex maneuvers and independently controlling of attitude from position.\nTo overcome these limitations, Ryll et al. [21] and [22] proposed incorporating additional independent actuators to make their drone fully-actuated. Both studies utilized feedback linearization control techniques based on third-order differential equations of motion.\n\nZheng et al. [23] and Badr et al. [24] achieved full actuation in their systems using thrust-vectoring mechanisms, enabling the drone to hover with an inclined body. Allenspach et al. [25] and Kamel et al. [26] constructed hexacopters with tilting rotors, capable of tracking all six  Degrees of Freedom (DoF) independently. Kamel et al. [26] incorporated a PID controller to determine the desired force vector and Two cascade controllers to generate the desired moment vector.\nSantos et al. [27] introduced a fast control allocation method for an over-actuated quadrotor to reduce real-time computation costs. A  Proportional Derivative (PD)  Sliding Mode Controller (SMC) is proposed and simulated by Alkamachi and Erçelebi [28] to control an over-actuated quadrotor, integrating a genetic algorithm to optimally tune the controller parameters. A different over-actuated configuration is proposed by Gavgani et al. [29], introducing a SMC-based controller incorporating a none-linear mapping in control inputs to deal with control allocation problems.\n\nTalaeizadeh et al. [30] introduced a yaw-rate control to mitigate the effects of  Vortex Ring State (VRS).\nHajiAbedini et al. [31] developed a quadrotor with rotor axes tilted inwards, taking a dihedral angle to explore a different approach to overcome VRS. \nSome studies have assessed the different applications of sliding mode control [32, 33, 34].\n\nAlthough previous works have made strides in thrust-vectoring and full actuation, there remains a gap in the literature on comprehensive comparisons of omniorientational configurations and controllers. Such comparisons would allow for a nuanced understanding of how different configurations and control strategies affect energy consumption and performance across the full six-DoF.\n\nWe represent four unique drone configurations featuring rotatable propeller axes. Our target systems are \"omniorientational\", indicating their ability to independently control position and attitude across all six DoF. The terms \"hedral rotation\" and \"tilt rotation\" are used in this article to refer to two distinct methods of axis rotation. Refer to Figure 1 for a graphical representation of hedral and tilt rotations. Detailed comparative analysis examines energy efficiency, robustness, and control efficacy across configurations and controllers.\n\nThe simplified visualization of proposed configurations are represented in Figure 2 emphasizing on the physical parameters, notations and coordinate systems. They involve four  Brushless Direct Current (BLDC) motors with attached propellers along with integrated servo motors for manipulating hedral and/or tilt angles. A conceptual design for the Hedral configuration is illustrated in Figure 3.\n\nThe dynamic equations of motion for each configuration are derived comprehensively using the Newton-Euler method, validated by applying our model-based controller to CAD models in MATLAB Simscape Multibody environment.\n\nWe have developed two effective controllers: a sliding mode controller for robust handling of disturbances and a novel PID-based controller with gravity compensation integrating linear and non-linear allocators, designed for computational efficiency. A custom control allocation strategy is implemented to manage the input-non-affine nature of these systems, seeking to maximize battery life by minimizing the  Power Consumption Factor (PCF) and  Nondimensionalized Power Consumption Factor (NPCF) defined in this study.\n\nA comparison framework assessing power consumption and robustness under uncertainties and disturbances is developed in this study. This framework introduces a power factor and an Uncertainty Impact Factor (UIF) for evaluating configurations and controllers, highlighting areas for potential model or hardware improvements, and offering insights into each configuration’s practical capabilities and operational strengths.\n\nThe main contribution of this paper is exploring a comprehensive assessment of various possible configurations for omniorientational drones —systems designed to overcome the limitations of conventional multi-rotors by enabling independent control across all DoF. This enables a comparative analysis of the different configurations and controllers.\nThe unique analysis in this paper provides a roadmap for future researchers to design omniorientational drones based on their design objectives, offering practical insights into configuration selection and controller design.\n\nThis research is in alignment with the project SAC-1, one of the primary objectives of the Laboratory of Advanced Control Systems and Agricultural Robotics of Sharif University of Technology (Sharif AgRoLab), which is the development of the Sharif Agrocopter. This specialized drone is designed to navigate with ease and precision through dense garden environments abundant with trees and foliage. Its primary functions encompass conducting essential investigations, including the application of artificial intelligence for pest detection and management.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何克服传统多旋翼无人机在姿态与位置控制上的不足？  \n2. 不同的无人机配置和控制策略如何影响能量消耗和性能？  \n3. 如何设计有效的控制器以应对系统中的不确定性和干扰？  \n\n【用了什么创新方法】  \n本研究提出了几种独特的无人机配置，结合额外的控制输入以实现“全向控制”，使其能够独立控制所有六个自由度。通过使用Newton-Euler方法全面推导了动态模型，并在MATLAB Simscape Multibody环境中进行了验证。设计了两种控制器：一种滑模控制器用于强扰动处理，另一种新型PID控制器则集成了重力补偿和线性、非线性分配器，旨在提高计算效率。通过比较不同配置和控制器的能量效率、鲁棒性和控制效果，研究为未来的无人机设计提供了实用的见解和路线图。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Human-like Navigation in a World Built for Humans",
            "authors": "Bhargav Chandaka,Gloria X. Wang,Haozhe Chen,Henry Che,Albert J. Zhai,Shenlong Wang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "CoRL 2025. Project website:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.21189",
            "code": "https://reasonnav.github.io/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21189",
            "arxiv_html_link": "https://arxiv.org/html/2509.21189v1",
            "abstract": "When navigating in a man-made environment they haven’t visited before—like an office building—humans employ behaviors such as reading signs and asking others for directions.\nThese behaviors help humans reach their destinations efficiently by reducing the need to search through large areas. Existing robot navigation systems lack the ability to execute such behaviors and are thus highly inefficient at navigating within large environments. We present ReasonNav, a modular navigation system which integrates these human-like navigation skills by leveraging the reasoning capabilities of a vision-language model (VLM). We design compact input and output abstractions based on navigation landmarks, allowing the VLM to focus on language understanding and reasoning. We evaluate ReasonNav on real and simulated navigation tasks and show that the agent successfully employs higher-order reasoning to navigate efficiently in large, complex buildings. Project website: https://reasonnav.github.io/.",
            "introduction": "Imagine that you are an office worker and are asked to deliver a report to Jane Doe’s office. What steps would you take to complete this task? First, you might search in a directory to find out the building and room number for Jane Doe’s office. Then, you might look for signs that indicate the direction of that room. You can integrate the information you receive from each sign with the layout of the scene you see around you to decide where to look next. Along the way, you might ask people nearby for further clarifications.\n\nOur civilization is built to be easy for humans to navigate within. There is an abundance of knowledge-offering resources around us that we leverage to navigate the world efficiently. Directional signs are placed deliberately at junctions to eliminate the risk of going the wrong way. Room labels follow orderly patterns so that reading a few can allow one to infer the locations of other rooms. Such guidance is necessary in order to deal with the inherent uncertainty of navigation in unseen environments.\n\nExisting robot navigation systems lack the skills needed to leverage these resources and thus lose out in navigation efficiency by spending unnecessary time exploring. We call these skills, which include sign reading and asking for directions, higher-order navigation skills because they require higher-order reasoning abilities and language processing. These skills become increasingly important in larger environments, where exploring in the wrong direction can cost a massive amount of time.\n\nOur key insight is that such higher-order navigation skills can be integrated in a unified manner by taking advantage of recent advances in large vision-language models (VLMs). In this paper, we present ReasonNav, a modular system for human-like navigation that leverages the zero-shot reasoning capabilities of a VLM in an agentic manner. The system is comprised of two streams: a low-level stream that handles localization, mapping, and path planning, and a high-level stream where the VLM performs high-level planning on abstracted observation and action spaces. Specifically, we represent the environment using a memory bank of landmarks (e.g. map frontiers, doors, people, signs) with attached textual information. This simplifies both the input and output spaces for the VLM agent, allowing it to focus on higher-order reasoning.\n\nWe evaluate ReasonNav in real and simulated environments. In both cases, the robot is tasked with finding a given room in a large (unseen) building. This mimics a practical indoor delivery scenario. We show that our abstraction design allows the VLM to interpret information from signs and people and use it to guide its decision-making. We compare our full system with ablated versions and demonstrate that such higher-order navigation skills greatly impact navigation performance. Overall, the results suggest that our VLM agent framework is a promising path forward for achieving human-like navigation efficiency using higher-order reasoning skills.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在未知的人工环境中实现人类般的导航效率？  \n2. 现有机器人导航系统缺乏利用环境信息和人际互动的能力。  \n3. 如何将高阶导航技能整合进机器人导航系统以提高效率？  \n\n【用了什么创新方法】  \n提出了ReasonNav，一个模块化导航系统，利用视觉-语言模型（VLM）的推理能力，整合人类导航技能。系统分为低层流和高层流，低层流处理定位和路径规划，高层流利用VLM进行高层规划。通过使用带有文本信息的地标记忆库，简化了输入和输出空间，使VLM能够专注于高阶推理。实验证明，ReasonNav在真实和模拟环境中有效提升了导航效率，展示了高阶导航技能对机器人导航性能的重要影响。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "DAGDiff: Guiding Dual-Arm Grasp Diffusion to Stable and Collision-Free Grasps",
            "authors": "Md Faizal Karim,Vignesh Vembar,Keshab Patra,Gaurav Singh,K Madhava Krishna",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21145",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21145",
            "arxiv_html_link": "https://arxiv.org/html/2509.21145v1",
            "abstract": "Reliable dual-arm grasping is essential for manipulating large and complex objects but remains a challenging problem due to stability, collision, and generalization requirements. Prior methods typically decompose the task into two independent grasp proposals, relying on region priors or heuristics that limit generalization and provide no principled guarantee of stability. We propose DAGDiff, an end-to-end framework that directly denoises to grasp pairs in the S​E​(3)×S​E​(3)SE(3)\\times SE(3) space. Our key insight is that stability and collision can be enforced more effectively by guiding the diffusion process with classifier signals, rather than relying on explicit region detection or object priors. To this end, DAGDiff integrates geometry-, stability-, and collision-aware guidance terms that steer the generative process toward grasps that are physically valid and force-closure compliant. We comprehensively evaluate DAGDiff through analytical force-closure checks, collision analysis, and large-scale physics-based simulations, showing consistent improvements over previous work on these metrics. Finally, we demonstrate that our framework generates dual-arm grasps directly on real-world point clouds of previously unseen objects, which are executed on a heterogeneous dual-arm setup where two manipulators reliably grasp and lift them. Project Page: dag-diff.github.io/dagdiff/",
            "introduction": "Manipulating large, dual-arm relevant objects such as monitors, boxes, or chairs requires not only feasible grasps, but also reasoning about force balance and stable interaction between both arms. Imagine the task of picking up a monitor. Humans instinctively place their hands on the opposite sides of the monitor instead of grasping it at random points to balance forces and torques, ensuring stability. For robots, however, acquiring this kind of coordination is far more complex [1]. Developing this sense of dual-arm stability is essential for moving beyond single-arm grasping toward coordinated, physically robust manipulation of real-world objects [2, 3, 4, 5].\n\nWhile grasp pose generation has been explored extensively in the community, most efforts largely focus on single-arm settings. Most methods [6, 7, 8, 9, 10, 11] follow a general recipe of curating a paired dataset consisting of ground truth grasps evaluated using physics simulators, followed by training encoder-decoder style models in a supervised setting. Recently, diffusion models have emerged as powerful generative frameworks for robotic grasping [12, 13, 14, 15, 16] due to their ability to model complex multimodal distributions. This enables them to sample diverse, high-quality grasp poses either uniformly across the object or constrained to specific parts.\n\nWhile these methods have improved robustness and grasp quality on complex shapes, they are designed for single-arm grasps and lack mechanisms to ensure dual-arm stability. Moreover, extending these methods to dual-arm grasping is non-trivial, as exhaustive pair search is costly and naive single-arm extensions often yield unstable solutions [17]. Furthermore, these methods do not explicitly account for collisions and often produce grasps that intersect the object surface, a problem that becomes increasingly critical for larger and more geometrically complex shapes. A possible workaround would be to increase the resolution of the point cloud or latent representation to capture finer surface details, but this would greatly increase computation without guaranteeing collision-free grasps.\n\nIn this work, we introduce DAGDiff: Dual-Arm Grasp Diffusion, an end-to-end dual-arm grasp generation framework that leverages diffusion models guided by classifier signals. Our method extends S​E​(3)SE(3) diffusion to the dual-arm setting to generate grasp pairs that are simultaneously stable under dual-arm force closure and collision free with respect to the object’s surface.\n\nWe frame dual-arm grasp generation as the task of generating two grasps on the object point cloud, each falling in a suitable region, such that they are jointly stable and physically valid (by physically valid we mean grasps that are collision free, and make stable surface contact with the object). One of the key challenges is region selection: heuristic approaches such as choosing farthest regions [12] often fail when those regions itself are physically incompatible, while VLM-based reasoning [18] remains limited in 3D and physical understanding [19, 20]. It is further hindered by the fact that graspable regions rarely have semantic names, leaving no reliable basis for prediction. In contrast, our approach does not rely on region-specific training but instead learns suitable grasp regions implicitly from guidance signals, and we observe that it naturally discovers the right pairs of regions for stable dual-arm grasps (given in supplementary video).\nSpecifically, a force-closure module distinguishes stable from unstable grasp pairs and provides gradients that bias the diffusion process toward stability, while a collision module identifies grasp–object intersections and pushes generated grasps away from collisions. Together, these signals guide the diffusion model to diverse, stable, and physically valid dual-arm grasps.\n\nOur evaluation demonstrates the effectiveness of the proposed method in generating stable grasps within a dual-arm setup. Analytical evaluation based on dual-arm force-closure criteria [17] confirms that the generated grasp pairs satisfy fundamental stability requirements, while physics simulation-based evaluation [21] highlights the robustness of our approach across diverse objects and grasp configurations. Finally, real-world demonstrations show that our framework, trained entirely on synthetic data, transfers effectively to real point clouds, producing physically realizable dual-arm grasps on previously unseen objects like cooking utensils, buckets, drones etc as shown in Figure 1. To summarize the contributions:\n\nWe present DAGDiff, the first framework to the best of our knowledge, for dual-arm grasp generation that extends S​E​(3)SE(3) diffusion with guidance signals, enabling the synthesis of grasp pairs that are both force-closure stable and collision-free on large, geometrically complex objects.\n\nUnlike prior methods that rely on region identification using VLMs or geometric heuristics, our architecture employs geometry-, stability-, and collision-aware multi-head outputs that directly guide the diffusion process toward valid regions of the dual-arm grasp space (Figure 2).\n\nWe show substantial improvements over prior methods and adapted baselines through analytical metrics and large-scale simulations (Table I), and further validate reliable zero-shot transfer on a heterogeneous real-world dual-arm setup with real point clouds and previously unseen objects (Figure 1).",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现稳定且无碰撞的双臂抓取？  \n2. 现有方法在双臂抓取中缺乏有效的稳定性和碰撞检测机制。  \n3. 如何在不依赖区域特定训练的情况下生成有效的抓取区域？  \n\n【用了什么创新方法】  \nDAGDiff是一个端到端的双臂抓取生成框架，利用扩散模型并通过分类器信号引导生成过程。该方法将S​E​(3)扩散扩展到双臂设置，生成同时稳定且物理有效的抓取对。通过力闭合模块和碰撞模块的引导，DAGDiff能够生成多样化、稳定且物理有效的双臂抓取。评估结果表明，该框架在生成稳定抓取方面表现优异，并在真实世界的点云上成功实现了对未见物体的抓取。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems",
            "authors": "Junfeng Yan,Biao Wu,Meng Fang,Ling Chen",
            "subjects": "Robotics (cs.RO); Computation and Language (cs.CL)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21143",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21143",
            "arxiv_html_link": "https://arxiv.org/html/2509.21143v1",
            "abstract": "Multimodal agents have demonstrated strong performance in general GUI interactions, but their application in automotive systems has been largely unexplored. In-vehicle GUIs present distinct challenges: drivers’ limited attention, strict safety requirements, and complex location-based interaction patterns. To address these challenges, we introduce Automotive-ENV, the first high-fidelity benchmark and interaction environment tailored for vehicle GUIs. This platform defines 185 parameterized tasks spanning explicit control, implicit intent understanding, and safety-aware tasks, and provides structured multimodal observations with precise programmatic checks for reproducible evaluation. Building on this benchmark, we propose ASURADA, a geo-aware multimodal agent that integrates GPS-informed context to dynamically adjust actions based on location, environmental conditions, and regional driving norms. Experiments show that geo-aware information significantly improves success on safety-aware tasks, highlighting the importance of location-based context in automotive environments. We will release Automotive-ENV, complete with all tasks and benchmarking tools, to further the development of safe and adaptive in-vehicle agents.",
            "introduction": "Autonomous agents that interpret natural language instructions and control graphical user interfaces (GUI) can provide enormous value to users by automating repetitive tasks, augmenting human cognitive capabilities, and accomplishing complex workflows (Gravitas, 2023; Wu et al., 2023; Xie et al., 2023; Yao et al., 2022b; Yang et al., 2023b; Ding, 2024; Park et al., 2023). To realize this potential, current research efforts have primarily focused on building and evaluating GUI agents capable of operating within desktop operating systems, mobile applications, and web environments (Deng et al., 2023; Rawles et al., 2023; Zheng et al., 2024a; Koh et al., 2024; Kim et al., 2024; He et al., 2024), establishing important foundations for GUI automation research. These existing evaluation methods typically rely on static interface screenshots and user instructions as input, measuring performance by comparing agent behaviors with pre-collected human demonstrations (Deng et al., 2023; Rawles et al., 2023; Toyama et al., 2021; Li et al., 2024; Chai et al., 2024; Xie et al., 2024; Baek & Bae, 2016). Such approaches work well in traditional computing environments because desktop and mobile devices operate in relatively stable and controlled scenarios where device state has limited impact on task execution. However, this focus represents only a subset of the diverse interface ecosystems that people interact with daily, notably excluding In-vehicle GUI systems that support navigation, communication, media, and safety functions in millions of automobiles worldwide.\n\nIn-vehicle GUI systems introduce evaluation challenges that existing methods cannot adequately address. First, automotive agents operate in highly dynamic and safety-critical contexts, where factors such as real-time location, driving state, weather, and traffic conditions directly determine correct task execution (Zhou et al., 2023; Koh et al., 2024). For example, as shown in Figure 1, the seemingly simple command “I can’t see through the windshield, it’s all fogged up” requires the agent to first perform contextual reasoning over current driving conditions, and then correctly operate the interface (e.g., enabling the front defroster). Second, because drivers must prioritize road attention, their commands are typically brief, ambiguous, or incomplete, forcing agents to infer intent from limited information. Third, mistakes in automotive tasks can have immediate safety implications: a single incorrect navigation instruction or inappropriate system response may distract the driver or induce hazardous behavior. Existing evaluation frameworks, centered on interface screenshots and static inputs, fail to capture these challenges as they lack awareness of vehicle state, environmental conditions, and safety constraints, and cannot assess an agent’s adaptability or reliability under real-time driving dynamics (Liu et al., 2023; Wu et al., 2024).\n\nTo address these challenges, we introduce Automotive-ENV, a comprehensive evaluation platform built on a real in-car operating system spanning 8 functional modules with 185 parameterized tasks. Unlike prior benchmarks based on synthetic interfaces or static specifications, Automotive-ENV dynamically instantiates tasks with randomly generated parameters, creating millions of unique scenarios that require agents to generalize across diverse interface states and driving contexts. Our platform leverages production-grade automotive software architectures and their embedded event-handling mechanisms to ensure robust reward signal generation under the safety-aware conditions characteristic of real automotive environments. Beyond the core automotive tasks, we extend Automotive-ENV by integrating external geographic, environmental, and sensor-driven scenarios, thereby enriching the diversity of evaluation conditions and enabling comprehensive assessment across varied driving contexts. Meanwhile, this platform is designed for practical deployment and broad accessibility, requiring less than 4 GB of memory and 10 GB of disk space while connecting agents to automotive systems through standard APIs without proprietary hardware requirements.\n\nTo demonstrate the utility of Automotive-ENV, we develop ASURADA (Automotive Multimodal Agent), a prototype multimodal agent designed to address the unique challenges of in-vehicle GUI environments. Unlike desktop or mobile GUIs, automotive tasks are inherently geo-dependent: user needs and system behaviors vary significantly with GPS location, traffic conditions, and regional driving rules. For example, the seemingly simple utterance “Adjust the air conditioning temperature” may require different actions depending on whether the vehicle is driving through a hot coastal city, a cold mountainous region, or a humid rainy environment. Motivated by this, ASURADA incorporates a novel GPS-informed context integration that conducts reasoning over GPS signals to infer environmental context and location-specific driving regulations. We evaluate ASURADA under both GPS-enhanced multimodal input—screenshot, text, and GPS—and GPS-absent input with only screenshots and text, across realistic scenarios ranging from congestion rerouting to climate control adjustments. Results show that while incorporating geographic context enhances robustness in safety-aware tasks, substantial challenges remain: ASURADA achieves a 65% success rate, outperforming adapted web-based GUI agent baselines but still falling far below human performance at 100%, underscoring both the necessity of geo-aware reasoning and the current limitations of reliable automotive GUI automation.\n\nIn summary, our main contributions are as follows:\n\nWe introduce Automotive-ENV, a high-fidelity evaluation platform for in-vehicle GUI systems that balances generality and safety. It supports multimodal interactions, structured observations, and programmatic feedback to comprehensively assess agent robustness and generalization.\n\nWe introduce Automotive-ENV, a high-fidelity evaluation platform for in-vehicle GUI systems that balances generality and safety. It supports multimodal interactions, structured observations, and programmatic feedback to comprehensively assess agent robustness and generalization.\n\nWe develop ASURADA, a structured VLM-based agent architecture that integrates perception, intent understanding, planning, and execution. A GPS reasoning module is incorporated to adapt agent behavior to geographic context and regional driving rules, improving robustness across diverse driving environments.\n\nWe demonstrate that agents can leverage GPS to perceive richer environmental context and support decision-making, leading to significant improvements in reliability and responsiveness on safety-critical tasks.\n\n1. We introduce Automotive-ENV, a high-fidelity evaluation platform for in-vehicle GUI systems that balances generality and safety. It supports multimodal interactions, structured observations, and programmatic feedback to comprehensively assess agent robustness and generalization.\n\n2. We develop ASURADA, a structured VLM-based agent architecture that integrates perception, intent understanding, planning, and execution. A GPS reasoning module is incorporated to adapt agent behavior to geographic context and regional driving rules, improving robustness across diverse driving environments.\n\n3. We demonstrate that agents can leverage GPS to perceive richer environmental context and support decision-making, leading to significant improvements in reliability and responsiveness on safety-critical tasks.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在汽车环境中评估和提升多模态代理的性能？  \n2. 现有的GUI代理评估方法为何无法满足汽车系统的动态和安全需求？  \n3. 如何有效整合地理位置信息以增强汽车代理的任务执行能力？  \n\n【用了什么创新方法】  \n本研究提出了Automotive-ENV，一个高保真评估平台，专为汽车GUI系统设计，涵盖185个参数化任务，支持多模态交互。通过动态生成任务场景，平台能够评估代理在真实驾驶环境中的适应性和可靠性。此外，开发的ASURADA代理集成了GPS感知模块，能够根据地理上下文调整行为，从而在安全关键任务中显著提高成功率。实验结果表明，ASURADA在安全任务上的成功率达到65%，尽管仍低于人类表现，但展示了地理信息在汽车环境中的重要性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Rich State Observations Empower Reinforcement Learning to Surpass PID: A Drone Ball Balancing Study",
            "authors": "Mingjiang Liu,Hailong Huang",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted for presentation at the Advancements in Aerial Physical Interaction Workshop of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.21122",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21122",
            "arxiv_html_link": "https://arxiv.org/html/2509.21122v1",
            "abstract": "This paper addresses a drone ball-balancing task, in which a drone stabilizes a ball atop a movable beam through cable-based interaction. We propose a hierarchical control framework that decouples high-level balancing policy from low-level drone control, and train a reinforcement learning (RL) policy to handle the high-level decision-making. Simulation results show that the RL policy achieves superior performance compared to carefully tuned PID controllers within the same hierarchical structure. Through systematic comparative analysis, we demonstrate that RL’s advantage stems not from improved parameter tuning or inherent nonlinear mapping capabilities, but from its ability to effectively utilize richer state observations. These findings underscore the critical role of comprehensive state representation in learning-based systems and suggest that enhanced sensing could be instrumental in improving controller performance.",
            "introduction": "Aerial manipulation, which aims to endow aerial robots with the capability to physically interact with their surroundings [1], has attracted significant attention in recent years. The most common aerial manipulation systems employ unmanned aerial vehicles (UAVs) as floating bases and equip them with robotic arms or custom grippers. These configurations have been successfully applied to tasks such as millimeter-level peg-in-hole insertion [2], light bulb removal [3], and in-flight grasping [4]. Alternatively, manipulation can also be achieved through direct attachment or cable suspension, with cable-suspended payload transportation being a prominent example [5, 6, 7].\n\nDespite these advances in aerial manipulation, significant challenges persist in real-world applications. Most existing systems are confined to direct physical contact, yet many practical scenarios require indirect manipulation, which involves interacting with objects via an intermediate tool or platform. For instance, during transportation, a robot might need to carry a box using a cable-suspended carrier. Here, the robot must manipulate the carrier rather than the box directly, while actively stabilizing it to prevent falling. Such forms of indirect manipulation introduce new challenges, especially in learning to control intermediary tools.\n\nTo explore the potential of indirect aerial manipulation, this paper studies a drone ball-balancing task (see Fig. 1), where a drone tethered to one beam end via a rope and is required to manipulate a beam to position a ball at a target location. To solve this problem, we proposed a hierarchical control framework comprising: A high-level policy that generates velocity commands according to system state observations and a low-level velocity controller for precise reference tracking.\n\nMotivated by recent advances in RL [8, 9], we formulated the drone ball-balancing task as a Markov Decision Process (MDP) and developed an RL-based high-level controller to address this problem. Specifically, we created a drone ball-balancing simulator using Isaac Lab [10]. To enable GPU-accelerated parallel training, we implemented a PyTorch-based low-level flight controller based on the SE(3) geometry controller [11]. The balancing policy was parameterized by a neural network and trained using the Robust Policy Optimization algorithm (RPO) [12].\n\nSimulation experiments demonstrate that the RL-based policy exhibits excellent performance in the drone ball-balancing task. To further validate the superiority of the RL-based approach, we replaced the high-level RL agent with incremental PID controllers [13] within the same hierarchical framework. Empirical results reveal that PID controllers fail to stabilize the system, even when augmented with additional velocity constraints.\n\nWhile numerous studies demonstrate RL’s superiority over classical control methods (e.g., PID and model predictive control (MPC)) in robotics [14, 15, 16], these findings remain largely empirical, lacking systematic analysis of the fundamental factors contributing to RL’s success or classical methods’ limitations. In contrast, a recent comparative study in autonomous drone racing [17] reveals that RL’s advantage stems primarily from its ability to optimize more effective objectives. This insight provides a principled foundation for future method selection and advancement in similar robotic applications. Inspired by this work, we conducted systematic controlled experiments to investigate why RL outperforms PID control. Our results demonstrate that RL’s superiority stems not from discovering superior control gains or employing nonlinear error mapping, but rather from its ability to leverage richer state observations for decision-making effectively.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何通过强化学习（RL）实现无人机在球平衡任务中的优越表现？  \n2. RL相对于PID控制器的优势源于什么因素？  \n3. 如何设计一个分层控制框架以解决间接操控的挑战？  \n\n【用了什么创新方法】  \n本研究提出了一种分层控制框架，将高层平衡策略与低层无人机控制解耦。通过将无人机球平衡任务建模为马尔可夫决策过程（MDP），并使用强化学习算法（RPO）训练高层控制器，研究展示了RL在处理复杂状态观察时的优势。模拟实验表明，RL策略在无人机球平衡任务中表现优异，超越了传统PID控制器，强调了全面状态表示在学习系统中的重要性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Cross-Modal Instructions for Robot Motion Generation",
            "authors": "William Barron,Xiaoxiang Dong,Matthew Johnson-Roberson,Weiming Zhi",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21107",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21107",
            "arxiv_html_link": "https://arxiv.org/html/2509.21107v1",
            "abstract": "Teaching robots novel behaviors typically requires motion demonstrations via teleoperation or kinaesthetic teaching, that is, physically guiding the robot. While recent work has explored using human sketches to specify desired behaviors, data collection remains cumbersome, and demonstration datasets are difficult to scale. In this paper, we introduce an alternative paradigm, Learning from Cross-Modal Instructions, where robots are shaped by demonstrations in the form of rough annotations, which can contain free-form text labels, and are used in lieu of physical motion. We introduce the CrossInstruct framework, which integrates cross-modal instructions as examples into the context input to a foundational vision–language model (VLM). The VLM then iteratively queries a smaller, fine-tuned model, and synthesizes the desired motion over multiple 2D views. These are then subsequently fused into a coherent distribution over 3D motion trajectories in the robot’s workspace. By incorporating the reasoning of the large VLM with a fine-grained pointing model, CrossInstruct produces executable robot behaviors that generalize beyond the environment of in the limited set of instruction examples. We then introduce a downstream reinforcement learning pipeline that leverages CrossInstruct outputs to efficiently learn policies to complete fine-grained tasks. We rigorously evaluate CrossInstruct on benchmark simulation tasks and real hardware, demonstrating effectiveness without additional fine-tuning and providing a strong initialization for policies subsequently refined via reinforcement learning.",
            "introduction": "Imitation learning, also known as Learning from Demonstration, is a dominant paradigm for teaching robots new skills [1]. Imitation learning requires the collection of a dataset of demonstrations, which are generally provided by a human user teleoperating the robot via a remote controller or by kinesthetic teaching, i.e. physically handling the robot. In both of these cases, collecting a sufficient number of demonstrations for even marginal levels of generalization is challenging. There have also been efforts to reduce the human burden of providing demonstrations, including hardware adaptations [2], and using human sketches as a demonstration interface [3, 4, 5, 6, 7]. Humans have a remarkable ability to adapt the same skill to different environments and setups, even after observing one or two demonstrations. However, imitation learning is far less demonstration-efficient, and it can be challenging to collect a sufficient number of demonstrations to generalize behavior when the scene changes. In this work, we aim to circumvent the cumbersome, large-scale human input required for imitation learning.\n\nWe introduce Learning from Cross-modal Instructions as an alternative paradigm to shape robot behavior. In the proposed method, human operators provide cross-modal instructions, which are free-form sketches and textual labels on an image of an operating environment. The robot infers the intended behavior from these inputs and generalizes execution across varying setups. To learn from cross-modal instructions, we propose the CrossInstruct framework. LABEL:fig:teaser illustrates several examples in both simulation and the real world with cross-modal instructions and subsequent robot behavior generated by CrossInstruct. CrossInstruct takes cross-modal instructions as in-context learning examples [8] for a large VLM. The large model performs high-level task reasoning and delegates pixel-level keypoint localization to a smaller VLM fine-tuned for 2D pointing, which returns precise coordinates for task-critical features identified by the large model. The hierarchical coupling between the large reasoning VLM with the smaller, fine-tuned model enables the efficient and accurate identification of task-relevant keypoints. These are then used to produce motion trajectory sketches over a small set of multi-view images of the setup. The resulting 2D trajectories are lifted into the robot’s workspace to yield a distribution over 3D trajectories. Combined with end-effector orientations and gripper actions generated by the reasoning model, this produces executable motion sequences.\n\nWe demonstrate the capabilities of CrossInstruct to shape behavior to complete and generalize across a wide range of tasks, in both simulation and in the real world, without additional fine-tuning. Beyond directly executing the produced motions, we can enable the robots to achieve an even higher level of precision by joining CrossInstruct with a downstream reinforcement learning pipeline. Concretely, the technical contributions in this paper include:\n\nThe Learning from Cross-Modal Instructions paradigm, circumventing the need for physical demonstrations;\n\nThe CrossInstruct framework to leverage cross-modal instructions in an in-context fashion to synthesize robot behavior. This is achieved by a hierarchical coupling of two VLMs and using raycasting to fuse multi-view 2D sketches into executable robot motion;\n\nEvaluations on benchmark tasks, demonstrating robustness of CrossInstruct out of the box in both simulation and the real world, as well as leveraging the distribution of trajectories to enable efficient reinforcement learning.\n\n1. The Learning from Cross-Modal Instructions paradigm, circumventing the need for physical demonstrations;\n\n2. The CrossInstruct framework to leverage cross-modal instructions in an in-context fashion to synthesize robot behavior. This is achieved by a hierarchical coupling of two VLMs and using raycasting to fuse multi-view 2D sketches into executable robot motion;\n\n3. Evaluations on benchmark tasks, demonstrating robustness of CrossInstruct out of the box in both simulation and the real world, as well as leveraging the distribution of trajectories to enable efficient reinforcement learning.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何减少机器人学习新行为所需的物理演示数据。  \n2. 如何利用跨模态指令来生成机器人运动而无需大量示范。  \n3. 如何实现机器人行为的高效泛化和执行。  \n\n【用了什么创新方法】  \n提出了一种新的学习范式——跨模态指令学习，利用人类提供的草图和文本标签作为输入。开发了CrossInstruct框架，将跨模态指令整合到大型视觉-语言模型中，通过层次化的模型耦合实现任务推理和关键点定位。最终，生成的2D轨迹被提升为3D运动轨迹，能够在多种环境中执行。CrossInstruct在基准任务上表现出色，且无需额外微调，为后续的强化学习提供了强有力的初始化。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Flight Dynamics to Sensing Modalities: Exploiting Drone Ground Effect for Accurate Edge Detection",
            "authors": "Chenyu Zhao,Jingao Xu,Ciyu Ruan,Haoyang Wang,Shengbo Wang,Jiaqi Li,Jirong Zha,Weijie Hong,Zheng Yang,Yunhao Liu,Xiao-Ping Zhang,Xinlei Chen",
            "subjects": "Robotics (cs.RO); Networking and Internet Architecture (cs.NI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21085",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21085",
            "arxiv_html_link": "https://arxiv.org/html/2509.21085v1",
            "abstract": "Drone-based rapid and accurate environmental edge detection is highly advantageous for tasks such as disaster relief and autonomous navigation. Current methods, using radars or cameras, raise deployment costs and burden lightweight drones with high computational demands. In this paper, we propose AirTouch, a system that transforms the ground effect from a stability ”foe” in traditional flight control views, into a ”friend” for accurate and efficient edge detection. Our key insight is that analyzing drone basic attitude sensor readings and flight commands allows us to detect ground effect changes. Such changes typically indicate the drone flying over a boundary of two materials, making this information valuable for edge detection. We approach this insight through theoretical analysis, algorithm design, and implementation, fully leveraging the ground effect as a new sensing modality without compromising drone flight stability, thereby achieving accurate and efficient scene edge detection. We also compare this new sensing modality with vision-based methods to clarify its exclusive advantages in resource efficiency and detection capability. Extensive evaluations demonstrate that our system achieves a high detection accuracy with mean detection distance errors of 0.051m, outperforming the baseline method performance by 86%86\\%. With such detection performance, our system requires only 43 mW power consumption, contributing to this new sensing modality for low-cost and highly efficient edge detection.",
            "introduction": "Rapid and accurate environmental edge detection is crucial in various applications, including disaster response [2, 3, 4], rescue-and-relief [5, 6], and autonomous navigation [7, 8, 9]. A key aspect involves detecting terrain edges, such as sudden changes in height (e.g., steps, cliffs) and variations in ground materials (e.g., water, soil, solid rock). With prior knowledge of these edges, intelligent systems can plan paths for humans and robots more logically, efficiently, and safely  [10, 11, 12, 13]. To boost efficiency and cut costs in large-scale edge detection [14], mainstream systems leverage swarms of lightweight drones [15] (a.k.a., UAV) to execute the task collaboratively as they fly and scan the entire scene [16, 17, 18, 19, 20].\n\nIn disaster relief scenarios, such as earthquake rescue or flood response, the ability to quickly and precisely detect edges, including sudden height changes and material boundaries, can significantly enhance the efficiency and safety of rescue missions [21, 22]. For example, drones equipped with edge detection capabilities can navigate through debris fields, identify safe landing zones, and locate survivors trapped in complex terrains. This enables rescue teams to reach affected areas more rapidly and with greater precision, potentially saving lives [23, 24]. In autonomous navigation, edge detection plays a vital role in enabling drones to operate safely and efficiently in unknown environments. By detecting edges such as cliffs, steps, and material transitions, drones can avoid obstacles, plan optimal flight paths, and make real-time decisions to ensure smooth and collision-free navigation [25, 26]. This is particularly important in scenarios where GPS signals are weak or unavailable, such as in indoor environments or dense forests. Accurate edge detection allows drones to maintain situational awareness and adapt to dynamic surroundings, improving their autonomy and reliability.\n\nExisting drone-based edge detection methods face challenges, categorized into two main types:\n(i) Wireless-signal based methods: These methods utilize signals like mmWave Radar [2], Wi-Fi [27], terahertz radar [28], LiDAR [29], and acoustic signals [30] for contactless edge detection.\nWhile effective, they often rely on infrastructure and struggle with operations in inaccessible areas [31], and are typically power-hungry, making them less suitable for lightweight drones with limited battery capacity.\n(ii) Visual-sensor-based methods: These algorithms aim to accurately detect object boundaries using computer vision techniques or neural networks [32, 33, 34].\nHowever, they require significant computational resources, limiting their deployment on resource-constrained drones [35], which can be a bottleneck for real-time applications. Visual sensors like cameras struggle in low-light conditions or when dealing with surfaces of similar colors and textures, which can compromise their detection accuracy and robustness.\nFurthermore, the existing pioneer study [36] proposes multi-modal fusion sensing, achieving promising accuracy by integrating the information from both wireless and visual domains.\n\nIn this work, we aim to introduce a novel approach for edge detection on lightweight drones, which will serve as a significant complement to the aforementioned two envelopes, especially in situations where computational resources are scarce and scene visibility is low.\nAs illustrated in Fig .1, when drones fly closely to the ground (i.e., a surface): the airflow generated by the rotating rotors bounces off the surface below the drone, creating additional upward lift and leading to disturbances in a drone’s flight state.\nThis phenomenon, widely known as ground effect (GE) [37, 38], varies with the drone’s altitude above the surface and the type of surface material.\n\nGround effect of drones.\nAchieving precise control over drone positions is paramount, yet remains a considerable challenge [39, 40].\nThis challenge is predominantly attributed to the intricate interplay between rotor and wing airflows with the ground surface [41, 42].\nThe aerospace industry has recognized the ground effect for some time, acknowledging its potential to amplify lift forces while decreasing aerodynamic drag [43, 44].\nDespite advantages, they also pose challenges to flight stability [45].\nConsequently, mitigating the impacts of ground effect has been a persistent issue [46].\nIn contrast, this paper diverges from conventional approaches by harnessing the ground effect to detect edges rather than attempting to neutralize it.\nAs far as we are aware, it is the first system to perform edge detection without the use of additional sensors.\n\nNew sensing modality.\nThe key insight behind this work is to translate the physical phenomenon ground effect in flight dynamics into a fresh sensing modality for edge detection on lightweight drones - as shown in Fig. 1(b), by identifying ground effect changes, we can deduce sudden alterations in the drone’s relative height above a surface or in the surface material itself, pinpointing the edge information within the scene. The process has similarities with sensing the surface by touching it using air, but only using the most basic sensing ability on lightweight drones.\nHowever, translating this insight into a practical system still faces two challenges:\n\n∙\\bullet The target discrepancy between sensing and flight control complicates ground effect profiling.\nFor the ground effect, the sensing modalities treat it as a friend, aiming to detect drone flight instability through abrupt changes in sensor readings (e.g., Inertial Measurement Unit (IMU) samples) to measure it.\nHowever, drone flight control systems view it as a foe, striving to minimize its impact on flight dynamic stability. \nThis leads to sensor (e.g., IMU) readings being extensively smoothed out after those complex proportional–integral–derivative (PID) operations [47], challenging the effective profiling of the ground effect.\n\n∙\\bullet The noisy sensing data overwhelms the vital feedback related to the ground effect.\nThe dynamic measurements from inexpensive, low-power sensors on lightweight drones make it difficult to extract actual ground-effect-related fluctuations from those complex noises in raw data.\nThe circumstance is further complicated by the flight control system’s smoothing and attenuating functions on the ground effect.\n\nRemark.\nUnder the premise that the flight control module treats the ground effect as a foe and tries to negate its impact on drone stability, accurately and efficiently measuring and profiling the attenuated ground effect from noisy sensor data is crucial for edge detection.\n\nTo tackle the above challenges, we design and implement AirTouch, the first system that treats the ground effect as a friend and offers methods to extract related data from onboard sensors of lightweight drones, despite the flight control module’s influence.\nBenefiting from AirTouch, the ground effect can be leveraged as a new sensing modality for tasks such as edge detection.\nIn general, AirTouch excels in the following three aspects.\n\n∙\\bullet On the sensory input front.\nWe demonstrate that leveraging the onboard IMU readings and motor commands from flight controllers could effectively profile the ground effect.\nBy examining the complex physical dynamics and drone stability control, we uncover how drone attitudes (i.e., measured by the IMU) and control signals (i.e., indicated by motor commands) interrelate and complement each other.\nTheir combination offers a full insight into the ground effect, even with the flight control module’s adjustments.\n\n∙\\bullet On the algorithm front.\nWe propose a ground effect-informed environmental edge detection pipeline, which comprises\n(i)(i) a fluctuation components feature extraction method and a cascaded cross-spectrum feature fusion technique to facilitate the extraction of ground-effect-related information from noisy IMU measurements and motor commands;\n(i​i)(ii) a compact neural network (NN) designed to detect environmental edges from the extracted features;\nand (i​i​i)(iii) an aerodynamics-instructed physical filter to further improve edge detection performance of neural network.\n\n∙\\bullet On the implementation front.\nTo further boost computational efficiency and enable lightweight drones to run the proposed NN in real-time, we apply techniques such as neural unit pruning and weight quantization on the NN before onboard deployment.\nAdditionally, during the NN training, we introduce a meticulously designed Disturbance Force-Informed loss function by analysis and modeling, incorporating binary cross-entropy loss, to expedite network convergence and make the network learn fine-grained bias.\n\nWe evaluate the performance of AirTouch by conducting extensive experiments and comparing it with the baseline using a real-world testbed. Based on a lightweight drone and its onboard IMU and motors, we conducted abrupt height discontinuity edge detection and material interface transition edge detection, respectively. The results demonstrate that our system achieves a high detection accuracy with mean detection distance errors of 0.051m. Furthermore, our system surpasses the baseline performance by 86%86\\% with the same available sensor information. Additionally, we compare AirTouch with vision-based edge detection methods to clarify its exclusive advantages and discuss some concerns about our system’s capabilities. Note that AirTouch is open-source on GitHub1.\n\n11footnotetext: https://github.com/ChenyuZhaoTHU/AirTouch\n\nThe main contributions of this paper are as follows:\n\nWe propose AirTouch, as far as we are aware, the first system that translates the traditionally negative ground effect into a new, positive sensing modality for accurate and efficient environmental edge detection.\n\nWe demonstrate that combining IMU sampling and motor commands provides an effective sensing paradigm to characterize the ground effect under the influence of the flight control system.\nOn this basis, we present a comprehensive and neural network-based pipeline aided by modeled physical knowledge for profiling, extracting, and utilizing the ground effect for sensing tasks from noisy sensory input.\n\nWe develop a prototype system and evaluate the AirTouch system through real-world data and in-field experiments on a lightweight drone by deploying our system on onboard computing chips. Extensive evaluation results show the effectiveness of our system in impressive edge detection accuracy on low-cost drones and low-energy consumption sensors and represent its exclusive advantages of capabilities by comparison with a vision-based method.\n\nThe remainder of this paper is structured as follows: §2 presents the core intuition and prerequisites underlying the AirTouch system. After introducing the system overview in §3, we elaborate on the two main components of the system in §4 and §5, respectively. In §6, we introduce the implementation. In §7, we evaluate our system. In §8, we have a discussion section. In the last, we conclude this paper in §9.\n\n1. We propose AirTouch, as far as we are aware, the first system that translates the traditionally negative ground effect into a new, positive sensing modality for accurate and efficient environmental edge detection.\n\n2. We demonstrate that combining IMU sampling and motor commands provides an effective sensing paradigm to characterize the ground effect under the influence of the flight control system.\nOn this basis, we present a comprehensive and neural network-based pipeline aided by modeled physical knowledge for profiling, extracting, and utilizing the ground effect for sensing tasks from noisy sensory input.\n\n3. We develop a prototype system and evaluate the AirTouch system through real-world data and in-field experiments on a lightweight drone by deploying our system on onboard computing chips. Extensive evaluation results show the effectiveness of our system in impressive edge detection accuracy on low-cost drones and low-energy consumption sensors and represent its exclusive advantages of capabilities by comparison with a vision-based method.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用无人机的地面效应进行高效的边缘检测。  \n2. 现有的边缘检测方法在资源受限的轻量级无人机上面临的计算负担和能耗问题。  \n3. 如何从噪声传感器数据中提取与地面效应相关的信息以进行边缘检测。  \n\n【用了什么创新方法】  \n本文提出了AirTouch系统，将无人机的地面效应转变为一种新的传感模式，用于环境边缘检测。通过分析无人机的基本姿态传感器读数和飞行命令，系统能够检测地面效应的变化，从而识别材料边界。方法包括利用IMU和电机命令的组合进行地面效应特征提取，设计紧凑的神经网络进行边缘检测，并应用物理滤波器以提高检测性能。实验结果表明，AirTouch在边缘检测准确性上超越基线方法86%，且功耗仅为43 mW，展现了其在低成本和高效能边缘检测中的独特优势。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Normalizing Flows are Capable Visuomotor Policy Learning Models",
            "authors": "Simon Kristoffersson Lind,Jialong Li,Maj Stenmark,Volker Krüger",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21073",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21073",
            "arxiv_html_link": "https://arxiv.org/html/2509.21073v1",
            "abstract": "The field of general purpose robotics has recently embraced powerful probabilistic models, such as diffusion models, to model and learn complex behaviors. However, these models often come with significant trade-offs, namely high computational costs for inference and a fundamental inability to quantify output uncertainty. We argue that a model’s trustworthiness, a critical factor for reliable, general-purpose robotics, is inherently linked to its ability to provide confidence measures.",
            "introduction": "Diffusion Policy (DP) [1] is one of the mainstream methods for visuomotor policy learning using a Denoising Diffusion Probabilistic Model (DDPM) [2]. This approach has secured a place in robot policy learning due to its powerful expressive capabilities.\n\nDespite impressive performance in modeling robotic behaviors, diffusion models exhibit a number of drawbacks. Most importantly, they do not produce any measure of confidence or uncertainty in their outputs.\nAn implication of this is that diffusion models cannot give any guarantees regarding the quality of their outputs, which is a weakness.\n\nFirst and foremost, uncertainty is inevitable in general-purpose robotics, and we argue that a measure of confidence is necessary for a model to be considered trustworthy. Crucially, the model should be able to use this confidence to improve the generated action sequences.\n\nAdditionally, the training and inference costs for diffusion policy are often substantial, primarily because of the sequential denoising steps required for action generation.\n\nWith this work, we propose to use Normalizing Flows (NFs) [3] for visuomotor policy learning.\nWe argue that NFs are a natural alternative to diffusion models,\npossessing neither of the aforementioned flaws.\nNFs provide a measure of confidence for their outputs through a statistically sound approximation to the underlying probability.\nAdditionally, inference in an NFs requires only a single pass through the model,\nwhich makes inference much more efficient.\nWhile providing an output probability, and fast inference,\nNFs also exhibit the same properties that make diffusion models desirable,\nnamely stochastic generative sampling, and the ability to model complex and multimodal distributions.\nThrough experiments within the RoboTwin 2.0 simulation framework [4],\nwe show that NFs are able to learn robotic tasks with a success rate comparable to, and often better than, DP,\nespecially when training data is sparse.\n\nTo summarize, our contributions are as follows:\n\nWe propose to use NFs for visuomotor policy learning and introduce our model, Normalizing Flow Policy \n(NF-P).\n\nThrough experiments with 4 different simulated robotic tasks of varying difficulty, we show that NF-P reaches comparable performance to DP, and provides superior performance compared to DP when little training data is available.\n\nWe demonstrate that NFs can provide an order of magnitude faster inference times compared to diffusion models.\n\nWe validate several key techniques on normalizing flows to achieve strong performance in Visuomotor Policy Learning: visual and action conditioning, action-sequence based generation with stride, and likelihood optimization of the output.\n\nIn an ablation study, we highlight various factors and design choices that affect the performance of NFs for visuomotor policy learning.\n\n1. First and foremost, uncertainty is inevitable in general-purpose robotics, and we argue that a measure of confidence is necessary for a model to be considered trustworthy. Crucially, the model should be able to use this confidence to improve the generated action sequences.\n\n2. Additionally, the training and inference costs for diffusion policy are often substantial, primarily because of the sequential denoising steps required for action generation.\n\n1. We propose to use NFs for visuomotor policy learning and introduce our model, Normalizing Flow Policy \n(NF-P).\n\n2. Through experiments with 4 different simulated robotic tasks of varying difficulty, we show that NF-P reaches comparable performance to DP, and provides superior performance compared to DP when little training data is available.\n\n3. We demonstrate that NFs can provide an order of magnitude faster inference times compared to diffusion models.\n\n4. We validate several key techniques on normalizing flows to achieve strong performance in Visuomotor Policy Learning: visual and action conditioning, action-sequence based generation with stride, and likelihood optimization of the output.\n\n5. In an ablation study, we highlight various factors and design choices that affect the performance of NFs for visuomotor policy learning.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有的扩散模型在输出不确定性量化方面存在缺陷。  \n2. 扩散政策的训练和推理成本高，影响了其在机器人政策学习中的应用。  \n3. 需要一种能够提供输出置信度的模型，以提高生成的动作序列的可靠性。  \n\n【用了什么创新方法】  \n本研究提出使用归一化流（Normalizing Flows, NFs）进行视觉运动政策学习，介绍了我们的模型——归一化流政策（Normalizing Flow Policy, NF-P）。通过在RoboTwin 2.0仿真框架中进行的实验，我们表明NF-P在四个不同难度的机器人任务上达到了与扩散政策相当的性能，且在训练数据稀缺时表现更优。NFs不仅提供了输出概率，还显著提高了推理速度，相较于扩散模型快了一个数量级。此外，我们验证了多项关键技术以实现强大的视觉运动政策学习性能，并通过消融研究揭示了影响NFs性能的设计选择。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models",
            "authors": "Sibo Li,Qianyue Hao,Yu Shang,Yong Li",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21027",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21027",
            "arxiv_html_link": "https://arxiv.org/html/2509.21027v1",
            "abstract": "Robotic world models are a promising paradigm for forecasting future environment states, yet their inference speed and the physical plausibility of generated trajectories remain critical bottlenecks, limiting their real-world applications. This stems from the redundancy of the prevailing frame-to-frame generation approach, where the model conducts costly computation on similar frames, as well as neglecting the semantic importance of key transitions.\nTo address this inefficiency, we propose KeyWorld, a framework that improves text-conditioned robotic world models by concentrating transformers computation on a few semantic key frames while employing a lightweight convolutional model to fill the intermediate frames.\nSpecifically, KeyWorld first identifies significant transitions by iteratively simplifying the robot’s motion trajectories, obtaining the ground truth key frames.\nThen, a DiT model is trained to reason and generate these physically meaningful key frames from textual task descriptions.\nFinally, a lightweight interpolator efficiently reconstructs the full video by inpainting all intermediate frames.\nEvaluations on the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68×\\times acceleration compared to the frame-to-frame generation baseline, and focusing on the motion-aware key frames further contributes to the physical validity of the generated videos, especially on complex tasks.\nOur approach highlights a practical path toward deploying world models in real-time robotic control and other domains requiring both efficient and effective world models. Code is released at https://anonymous.4open.science/r/Keyworld-E43D.",
            "introduction": "Robotic world models are generative frameworks that predict future environment states based on an initial observation and a conditioning input (Ding et al., 2024; Agarwal et al., 2025). Their ability to simulate plausible future trajectories is crucial for a variety of applications, ranging from model-based reinforcement learning (MBRL) (Luo et al., 2023; Hansen et al., 2023) to policy evaluation (Shang et al., 2025; Li et al., 2025; Kawaharazuka et al., 2024).\nHowever, the practical deployment of these models faces two significant challenges. First, their powerful predictive capability comes at a substantial computational cost, severely hindering applications like online planning. Second, the prevailing per-frame generation paradigm often fails to produce physically consistent trajectories, leading to implausible motions that undermine the utility of the simulation for downstream tasks. These bottlenecks severely limit the realism and efficiency of model-based reasoning, calling for more efficient and physically-grounded generation paradigms.\n\nWithin such robotic scenarios, observations are typically captured by a fixed camera, with the robot being the primary moving entity (Liu et al., 2023; Brohan et al., 2022). From the human perspective, it is easy to imagine the video progression by visualizing a few key motions, such as “move left”, “grasp the object”, and “lift”.\nHowever, in stark contrast to this efficient reasoning process, current world models follow a frame-by-frame approach. They incur substantial computational redundancy by generating every frame from scratch with costly image generation modules (Wu et al., 2024; Yang et al., 2024). In addition, the standard practice of applying a uniform reconstruction loss during training forces the model to allocate its capacity equally across all frames, regardless of their semantic importance (Agarwal et al., 2025; Cen et al., 2025). This dilutes the learning signal for critical state transitions and ultimately hinders the generation of physically coherent long-horizon sequences.\n\nAn intuitive solution to reduce this redundancy is to synthesize only a sparse subset of frames (key frames) using the expensive world model, and reconstruct the remaining frames with a lightweight model conditioned on those key frames. However, each step in this roadmap is challenging: (1) Selecting appropriate key frames. Key frames must retain the trajectory’s essential semantics while leaving intermediate motion simple enough for a lightweight interpolator. (2) Generating key frames. Unlike per-frame generation, this task requires the model to synthesize temporally distant anchors while preserving global coherence and physical plausibility, posing distribution-shift and long-range dependency challenges. (3) Reconstructing between key frames. The number of intermediate frames is unknown, and large pose differences between key frames can produce substantial motion gaps that are difficult to model.\n\nTo address the above challenges, we propose KeyWorld, a framework that enhances the efficiency and effectiveness of text-conditioned robotic world models through explicitly focusing the computational load on key-frame reasoning.\nFirst, we construct a motion-aware key frames dataset from robotic poses using the Ramer–Douglas–Peucker (RDP) (Ramer, 1972; Douglas & Peucker, 1973) algorithm, which retains significant motion transitions and discards the steady movements. This ensures that the preserved key frames capture essential semantics and the intervals between them remain simple enough for lightweight interpolation.\nWith the key frame dataset, we train a Diffusion Transformer (DiT) to reason about critical motions from the task description and initial state, and then synthesize the corresponding key frames.\nBy fine-tuning on motion-aware key frames, the model learns to generate semantically critical anchors, significantly reducing the computational burden while enhancing its focus on essential physical interactions.\nFinally, we employ a lightweight Convolutional Neural Network (CNN) model, which is powerful enough for reconstructing the full video sequence by predicting frame gaps and generating intermediate frames between consecutive key frames while also eliminating heavy computational overhead.\nWe evaluate KeyWorld on the representative robotic benchmark LIBERO (Liu et al., 2023), and results demonstrate a 5×\\times acceleration compared to the frame-to-frame model. Furthermore, the motion-aware key frames guide the model to produce trajectories with higher physical plausibility, notably resulting in a substantially increased probability of the robot manipulating the correct target object.\nBy addressing the dual challenges of efficiency and physical fidelity, our approach enables more practical deployment of world models in real-time robotic applications.\nThe contribution of our work is summarized as follows:\n\nWe propose KeyWorld, an efficient and modular framework that decouples text-conditioned robotic world model inference into diffusion-based key frame generation and lightweight intermediate frame interpolation. This design significantly reduces video rollout costs and enhances semantic understanding at critical frames.\n\nWe propose KeyWorld, an efficient and modular framework that decouples text-conditioned robotic world model inference into diffusion-based key frame generation and lightweight intermediate frame interpolation. This design significantly reduces video rollout costs and enhances semantic understanding at critical frames.\n\nWe introduce a motion-aware key-frame detection paradigm that selects semantically critical states directly from robot pose trajectories. By aligning frame selection with meaningful physical transitions, this design not only provides a grounded abstraction of robotic videos but also fosters a sharper representation of physical dynamics within the model.\n\nWe extensively evaluate KeyWorld on the LIBERO benchmark and demonstrate that it achieves up to 5.68×\\times acceleration while maintaining superior video quality across multiple metrics. These results suggest that motion-aware key-frame reasoning offers a viable path for making robotic world models more practical in time-sensitive and physics-sensitive applications\n\n1. We propose KeyWorld, an efficient and modular framework that decouples text-conditioned robotic world model inference into diffusion-based key frame generation and lightweight intermediate frame interpolation. This design significantly reduces video rollout costs and enhances semantic understanding at critical frames.\n\n2. We introduce a motion-aware key-frame detection paradigm that selects semantically critical states directly from robot pose trajectories. By aligning frame selection with meaningful physical transitions, this design not only provides a grounded abstraction of robotic videos but also fosters a sharper representation of physical dynamics within the model.\n\n3. We extensively evaluate KeyWorld on the LIBERO benchmark and demonstrate that it achieves up to 5.68×\\times acceleration while maintaining superior video quality across multiple metrics. These results suggest that motion-aware key-frame reasoning offers a viable path for making robotic world models more practical in time-sensitive and physics-sensitive applications",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高机器人世界模型的推理速度和生成轨迹的物理合理性？  \n2. 当前的逐帧生成方法导致计算冗余和缺乏语义重要性，如何解决这些问题？  \n\n【用了什么创新方法】  \n提出了KeyWorld框架，通过集中计算在少量语义关键帧上，结合轻量级卷积模型填补中间帧，从而提高了文本条件下的机器人世界模型的效率和有效性。首先，通过简化机器人的运动轨迹识别重要的过渡帧，获得真实的关键帧。然后，训练Diffusion Transformer模型生成这些物理上有意义的关键帧。最后，使用轻量级插值器高效重建完整视频。评估结果表明，KeyWorld相比于逐帧生成基线实现了5.68倍的加速，并且在复杂任务中生成的轨迹具有更高的物理合理性，展示了在实时机器人控制等领域的应用潜力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Multi-Robot Vision-Based Task and Motion Planning for EV Battery Disassembly and Sorting",
            "authors": "Abdelaziz Shaarawy,Cansu Erdogan,Rustam Stolkin,Alireza Rastegarpanah",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21020",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21020",
            "arxiv_html_link": "https://arxiv.org/html/2509.21020v1",
            "abstract": "Electric-vehicle (EV) battery disassembly requires precise multi-robot coordination, short and reliable motions, and robust collision safety in cluttered, dynamic scenes. We propose a four-layer task-and-motion planning (TAMP) framework that couples symbolic task planning and cost- and accessibility-aware allocation with a TP-GMM-guided motion planner learned from demonstrations. Stereo vision with YOLOv8 provides real-time component localization, while OctoMap-based 3D mapping and FCL(Flexible Collision Library) checks in MoveIt unify predictive digital-twin collision checking with reactive, vision-based avoidance. Validated on two UR10e robots across cable, busbar, service plug, and three leaf-cell removals, the approach yields substantially more compact and safer motions than a default RRTConnect baseline under identical perception and task assignments: average end-effector path length drops by −63.3%-63.3\\% and makespan by −8.1%-8.1\\%; per-arm swept volumes shrink (R1: 0.583→0.139​m30.583\\rightarrow 0.139\\,\\mathrm{m}^{3}; R2: 0.696→0.252​m30.696\\rightarrow 0.252\\,\\mathrm{m}^{3}), and mutual overlap decreases by 47%47\\% (0.064→0.034​m30.064\\rightarrow 0.034\\,\\mathrm{m}^{3}). These results highlight improved autonomy, precision, and safety for multi-robot EV battery disassembly in unstructured, dynamic environments.",
            "introduction": "Task planning enables autonomous robots to execute complex, multi-step operations under resource constraints by selecting and sequencing high-level actions. When coupled with motion planning, it determines how and when to act amid environmental dynamics and hardware limits. In single-robot settings, learning- and optimisation-based approaches (e.g., ANFIS, GA, RL) improve allocation and ordering under uncertainty [1]. In multi-robot systems, continuous coordination, collision constraints, and shared resources make scheduling more challenging. Recent methods jointly encode geometric feasibility and semantic context for collision-free execution [2, 3]. Electric-vehicle (EV) battery disassembly is a demanding, safety-critical domain: tasks are sequential and tightly coupled (e.g., cable/busbar removal, service plug operations, cell extraction), fixtures and tolerances are strict, and the workspace is cluttered and dynamic [4, 5]. Efficient execution, therefore, hinges on an integrated view of what to plan (symbolic reasoning and allocation) and how to execute (geometrically feasible, collision-safe motions) with the ability to adapt online.\n\nThis work presents a four-layer planning architecture for coordinated multi-robot operation in dynamic environments (Fig. 2). The framework comprises high-level task planning (detecting and localising target objects, and sequencing by priority), low-level task planning (, accessibility, and tool compatibility with cross-robot allocation), high-level motion planning (trajectories consistent with task and spatio-temporal constraints), and low-level motion planning (online updates, collision avoidance, and replanning). Stereo vision with depth cameras integrated with YOLOv8, maintains a 3D OctoMap that imposes spatial constraints on planning. A MoveIt [6]/FCL (Flexible Collision Library) [7] digital twin predicts collisions before execution, while a complementary vision-based module reacts during execution; together they prevent both environment and self-collisions. In EV battery disassembly, the architecture improves safety and efficiency by continuously adjusting sequences and trajectories to environmental feedback, enabling robust dual-robot cooperation with better time, resource, and reliability performance.\n\nThis work introduces a modular TAMP framework that integrates a perception-driven task scheduler with a GMM-informed RRT motion planner. The scheduler handles object sorting, task reasoning, and dual-arm task allocation, while the motion planner exploits demonstrations for guided sampling and enables responsive replanning for collision avoidance and dynamic adaptation. Due to its modular design, the framework is scalable to different robot setups and adaptable to varying execution costs and optimisation criteria.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现多机器人在动态环境中对电动车电池拆解的有效协调与规划。  \n2. 如何在复杂的任务和运动规划中保证安全性和效率。  \n3. 如何结合视觉感知与运动规划以应对环境动态变化。  \n\n【用了什么创新方法】  \n本研究提出了一种四层次的任务与运动规划（TAMP）框架，结合了符号任务规划和基于成本与可达性的分配。通过集成YOLOv8的立体视觉实现实时组件定位，使用OctoMap进行3D映射，并在MoveIt中应用FCL进行碰撞检测。该方法在两个UR10e机器人上验证，显著减少了末端执行器路径长度和任务完成时间，同时提高了多机器人协作的自主性、精确性和安全性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation",
            "authors": "Konstantin Gubernatorov,Artem Voronov,Roman Voronov,Sergei Pasynkov,Stepan Perminov,Ziang Guo,Dzmitry Tsetserukou",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21006",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21006",
            "arxiv_html_link": "https://arxiv.org/html/2509.21006v1",
            "abstract": "We address natural language pick-and-place in unseen, unpredictable indoor environments with AnywhereVLA, a modular framework for mobile manipulation. A user text prompt serves as an entry point and is parsed into a structured task graph that conditions classical SLAM with LiDAR and cameras, metric semantic mapping, and a task-aware frontier exploration policy. An approach planner then selects visibility and reachability aware pre grasp base poses. For interaction, a compact SmolVLA manipulation head is fine tuned on platform pick and place trajectories for the SO-101 by TheRobotStudio, grounding local visual context and sub-goals into grasp and place proposals. The full system runs fully onboard on consumer-level hardware, with Jetson Orin NX for perception and VLA and an Intel NUC for SLAM, exploration, and control, sustaining real-time operation. We evaluated AnywhereVLA in a multi-room lab under static scenes and normal human motion. In this setting, the system achieves a 46%46\\% overall task success rate while maintaining throughput on embedded compute. By combining a classical stack with a fine-tuned VLA manipulation, the system inherits the reliability of geometry-based navigation with the agility and task generalization of language-conditioned manipulation. All code, models, and datasets are open source and are available on the project GitHub repository.",
            "introduction": "Mobile manipulation is accelerating beyond limited indoor workcells towards large unstructured environments, in which robots need to explore unfamiliar cluttered spaces and physically interact with diverse objects and people. The execution of complex mobile manipulation tasks conditional on natural language instructions has gained attention in recent years in the field of service robotics[1]. Research on intelligent robotic systems in fields such as household service [2, 3], retail automation [4, 5], warehouse logistics [6], and manufacturing [7] has gained popularity, highlighting the importance of developing mobile manipulation solutions capable of operating in large-scale and open-plan indoor environments. Recent studies have increasingly focused on natural language processing to enable robots to interpret human instructions and facilitate intuitive task specification [8], positioning language-guided manipulation as a key approach for effective human-robot collaboration [9]. However, unifying language-based control, environment exploration, and manipulation in expansive environments presents a significant challenge [10].\n\nVision-language-action (VLA) models show strong generalization in various mobile manipulation tasks [11, 12], enabling robots to perform complex operations integrating perception, language, and control. Despite these advancements, several critical limitations persist for end-to-end control use for mobile robots. Most VLA models are confined to specific tasks and have limited spatial awareness [11, 12, 13], restricting their operational scope to localized settings and impeding their ability to navigate or manipulate objects in unseen or occluded regions of larger indoor spaces.\n\nVision-Language Navigation (VLN) approaches [14] present an end-to-end VLM-based framework that navigates building-wide environments while manipulating previously unseen household objects. However, [14], as all VLN models, require instructions about the location of the target object within the environment, which is often impractical in dynamic or unexplored settings. In contrast, classical navigation stacks [15] provide robust solutions for mapping and environment exploration, enabling robots to systematically traverse and model unknown spaces. However, these traditional systems lack the advanced language comprehension and semantic reasoning capabilities necessary to interpret complex instructions or contextual cues [8], limiting their ability to perform goal-directed tasks that require understanding natural language or high-level semantic objectives.\n\nIn this paper we introduce AnywhereVLA, a novel modular architecture for large-scale indoor mobile manipulation, addressing the limitations of existing Vision-Language-Action (VLA) models that adapt pretrained vision-language models (VLMs) to enable natural language-driven perception and control, but are often constrained to room-scale environments due to high computational demands. Drawing from advancements in VLA paradigms, AnywhereVLA integrates the rich visual and linguistic knowledge encoded in VLMs—leveraged through co-fine-tuning on robotic trajectory data and Internet-scale vision-language tasks—with the robust traversability afforded by classical navigation stacks, representing robot actions as tokenized sequences to facilitate end-to-end control and emergent semantic reasoning.\n\nAnywhereVLA is a pipeline for large-scale indoor mobile manipulation in unseen environments. As shown in Fig. 1, AnywhereVLA combines robust traversability of classical navigation algorithms and simultaneous localization and mapping (SLAM) with the generalizable scene understanding and task grounding of VLA models. Our pipeline translates high-level language instructions into low-level control commands by generating actions via VLA model for task-specific manipulation and computes navigation trajectories via language-conditioned exploration algorithm, directly actuating the wheels of the mobile base and the joints of the manipulator. Our main contribution is the following:\n\nWe propose a cohesive modular framework that accepts a single language-based task instruction as input, which conditions the environment exploration and navigation modules and simultaneously drives the VLA model for manipulation task execution. Our system achieves real-time performance exceeding 10 Hz across all modules that are deployed on consumer-available edge computing units, ensuring efficient and responsive operation in dynamic settings.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在未知和不可预测的室内环境中实现自然语言驱动的移动操控。  \n2. 如何将语言理解与环境探索和操控任务统一在大规模室内环境中。  \n3. 现有的视觉-语言-动作模型在空间意识和操作范围上的限制。  \n\n【用了什么创新方法】  \nAnywhereVLA是一个模块化框架，通过解析用户的文本提示生成结构化任务图，结合经典的SLAM、语义映射和任务感知的探索策略。该系统使用VLA模型生成低级控制命令，并通过语言条件探索算法计算导航轨迹，确保实时性能超过10Hz。通过将经典导航算法与VLA模型的场景理解相结合，AnywhereVLA在动态环境中实现了46%的整体任务成功率，展示了语言驱动操控的灵活性和几何导航的可靠性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Autoregressive End-to-End Planning with Time-Invariant Spatial Alignment and Multi-Objective Policy Refinement",
            "authors": "Jianbo Zhao,Taiyu Ban,Xiangjie Li,Xingtai Gui,Hangning Zhou,Lei Liu,Hongwei Zhao,Bin Li",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20938",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20938",
            "arxiv_html_link": "https://arxiv.org/html/2509.20938v1",
            "abstract": "The inherent sequential modeling capabilities of autoregressive models make them a formidable baseline for end-to-end planning in autonomous driving. Nevertheless, their performance is constrained by a spatio-temporal misalignment, as the planner must condition future actions on past sensory data. This creates an inconsistent worldview, limiting the upper bound of performance for an otherwise powerful approach.\nTo address this, we propose a Time-Invariant Spatial Alignment (TISA) module that learns to project initial environmental features into a consistent ego-centric frame for each future time step, effectively correcting the agent’s worldview without explicit future scene prediction. In addition, we employ a kinematic action prediction head (i.e., acceleration and yaw rate) to ensure physically feasible trajectories. Finally, we introduce a multi-objective post-training stage using Direct Preference Optimization (DPO) to move beyond pure imitation. Our approach provides targeted feedback on specific driving behaviors, offering a more fine-grained learning signal than the single, overall objective used in standard DPO. Our model achieves a state-of-the-art 89.8 PDMS on the NAVSIM dataset among autoregressive models. The video document is available at https://tisa-dpo-e2e.github.io/.",
            "introduction": "End-to-end autonomous driving aims to learn a driving policy directly from raw sensor inputs [1], bypassing the cascaded perception and planning modules of traditional frameworks and their associated cumulative errors [2]. Prominent end-to-end approaches include trajectory regression [3], autoregressive models [4], and diffusion models [5]. Regression methods are known to suffer from the mode averaging issue [6]. The other methods are generative approaches and can capture multi-modal driving behaviors. Among them, autoregressive models offer unique flexibility by explicitly modeling the action probability distribution, facilitating straightforward integration with techniques like reinforcement learning [7].\n\nHowever, autoregressive end-to-end planning presents a unique challenge: maintaining a consistent worldview for future steps. In modular, two-stage pipelines, this is often resolved by predicting future environmental states, which is a tractable task when operating on lightweight, structured data [8]. This allows the planner to act within a consistently updated, inferred world [6].\nThis strategy, however, is intractable in the end-to-end paradigm where predicting future high-dimensional sensor data is computationally infeasible. Consequently, end-to-end models typically base their entire plan on merely past snapshots of the world [4]. This creates a critical spatio-temporal misalignment: the agent plans for a future step t+kt+k while perceiving the world from a stale viewpoint at time tt. This issue is illustrated in Fig. 1.\n\nTo resolve this spatio-temporal misalignment, we propose a novel alignment mechanism in the latent space. Our approach is motivated by the observation that an ego-view update, which consists of rotation and translation, can be modeled as a time-invariant spatial transformation. We hypothesize that this transformation can be learned and applied within a latent space to align environmental features with future ego states. To realize this, we introduce the Time-Invariant Spatial Alignment (TISA) module. For each future time step, TISA processes a query that fuses static environmental features with the ego-vehicle’s prospective state. In response, the module outputs a spatially-aligned environmental context, ensuring that the model’s subsequent action prediction is always conditioned on a geometrically consistent and relevant worldview.\n\nMoreover, we introduce two additional components to enhance the realism and quality of the learned driving policy.\nFirst, instead of predicting spatial waypoints, our model outputs discretized kinematic actions (i.e., acceleration and yaw rate). This approach not only creates a more compact action space but also ensures that the generated trajectories are inherently physically feasible. To move beyond the limitations of simple imitation learning, we introduce a multi-objective post-training stage using DPO [9]. We construct a preference dataset by generating diverse trajectories and labeling them using multiple safety-focused metrics111Our fine-tuning prioritizes safety, as the model’s kinematic action space already produces trajectories that score highly on comfort metrics.. Fine-tuning on this data provides our model with targeted feedback on distinct driving aspects, offering a more fine-grained and effective learning signal than the single, overall objective used in standard DPO.\n\nWithout bells and whistles, our model achieves a state-of-the-art 89.8 PDMS on the NAVSIM dataset, using the same backbone as competing methods. Comprehensive ablation studies validate the effectiveness of both the TISA module and the multi-objective post-training process in significantly advancing end-to-end planning capabilities.\n\nOur contributions are three-fold:\n\nWe formalize and address the spatio-temporal misalignment problem in autoregressive end-to-end planning, a critical issue caused by conditioning future actions on a stale worldview.\n\nWe propose a novel and efficient TISA module that performs view transformations in the latent space, significantly improving planning performance without notable computational overhead.\n\nWe introduce a multi-objective DPO strategy that uses targeted, fine-grained preference pairs to refine the driving policy, demonstrably outperforming the standard single-objective DPO baseline.\n\n1. We formalize and address the spatio-temporal misalignment problem in autoregressive end-to-end planning, a critical issue caused by conditioning future actions on a stale worldview.\n\n2. We propose a novel and efficient TISA module that performs view transformations in the latent space, significantly improving planning performance without notable computational overhead.\n\n3. We introduce a multi-objective DPO strategy that uses targeted, fine-grained preference pairs to refine the driving policy, demonstrably outperforming the standard single-objective DPO baseline.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何解决自回归端到端规划中由于过时的感知数据导致的时空不一致问题。  \n2. 如何在不增加计算开销的情况下，提高自回归模型的规划性能。  \n3. 如何通过多目标优化策略提升驾驶策略的学习效果。  \n\n【用了什么创新方法】  \n提出了时间不变空间对齐（TISA）模块，通过在潜在空间中进行视图变换，解决了自回归端到端规划中的时空不一致问题。该模块为每个未来时间步生成空间对齐的环境上下文，从而确保模型的动作预测基于一致的世界观。此外，模型输出离散化的运动学动作，确保生成的轨迹物理可行。通过引入多目标后训练阶段，利用直接偏好优化（DPO）策略，提供针对特定驾驶行为的反馈，显著提升了学习信号的细致度。最终，该模型在NAVSIM数据集上达到了89.8的PDMS，展示了其在自回归模型中的领先性能。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "MTRDrive: Memory-Tool Synergistic Reasoning for Robust Autonomous Driving in Corner Cases",
            "authors": "Ziang Luo,Kangan Qian,Jiahua Wang,Yuechen Luo,Jinyu Miao,Zheng Fu,Yunlong Wang,Sicong Jiang,Zilin Huang,Yifei Hu,Yuhao Yang,Hao Ye,Mengmeng Yang,Xiaojian Dong,Kun Jiang,Diange Yang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20843",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20843",
            "arxiv_html_link": "https://arxiv.org/html/2509.20843v1",
            "abstract": "Vision-Language Models(VLMs) have demonstrated significant potential for end-to-end autonomous driving, yet a substantial gap remains between their current capabilities and the reliability necessary for real-world deployment. A critical challenge is their fragility, characterized by hallucinations and poor generalization in out-of-distribution (OOD) scenarios.\nTo bridge this gap, we introduce MTRDrive, a novel framework that integrates procedural driving experiences with a dynamic toolkit to enhance generalization and proactive decision-making.",
            "introduction": "The emergence of Vision-Language Models (VLMs) [1] has propelled the paradigm of end-to-end autonomous driving forward, building a single, cohesive system that mirrors human cognitive processes. Unlike traditional modular pipelines that separate perception[2, 3], prediction[4], and decision-making[5, 6], VLMs break down these divisions by jointly modeling multi-modal inputs and generating holistic driving responses in a unified framework.\nRecent impressive demonstrations show that these models, often enhanced by techniques like Chain-of-Thought (CoT), can achieve strong performance across various tasks, from perception-based Visual Question Answering (VQA) [7, 8, 9, 10] to intricate motion planning [11, 12, 13].\n\nDespite these advances, a significant gap remains between current VLM performance and the reliability required for real-world deployment: The models are inherently fragile, often exhibiting visual hallucinations and failing in out-of-distribution (OOD) scenarios [7, 14]. In autonomous driving, a field where even a minor error can lead to catastrophic outcomes, such limitations present a critical barrier to adoption[15, 16].\n\nFundamentally, a robust driving decision is influenced by two key factors: the accuracy of perception and the soundness of reasoning. Human cognition itself can be viewed as a lifelong feedback loop:\n\nInspired by this human-like process, we propose MTRDrive, a novel framework built on the principle of Interactive Reasoning to address the limitations of current VLM-based driving agents. Unlike traditional methods that treat each input as a one-shot decision, MTRDrive endows the agent with the ability to proactively retrieve driveing experience and employ tools to query its environment. As vividly demonstrated in Figure 1, by shifting from a static decision-making model to a dynamic, interactive one, our approach mitigates the risk of hallucinations and dramatically improves performance in novel, unseen scenarios.\n\nIn summary, our key contributions are:\n\nWe introduce MTRDrive, a novel framework that models autonomous driving as a dynamic, interactive process, moving beyond static, one-shot decision-making paradigms.\n\nWe introduce MTRDrive, a novel framework that models autonomous driving as a dynamic, interactive process, moving beyond static, one-shot decision-making paradigms.\n\nWe propose a memory-tool synergy mechanism that enables the agent to utilize both a knowledge base of past driving experiences and active toolkits for real-time information retrieval.\n\nWe demonstrate that our approach effectively mitigates visual hallucinations and improves generalization in challenging out-of-distribution scenarios, which is critical for real-world deployment.\n\n1. We introduce MTRDrive, a novel framework that models autonomous driving as a dynamic, interactive process, moving beyond static, one-shot decision-making paradigms.\n\n2. We propose a memory-tool synergy mechanism that enables the agent to utilize both a knowledge base of past driving experiences and active toolkits for real-time information retrieval.\n\n3. We demonstrate that our approach effectively mitigates visual hallucinations and improves generalization in challenging out-of-distribution scenarios, which is critical for real-world deployment.",
            "llm_summary": "【关注的是什么问题】  \n1. VLMs在自动驾驶中的脆弱性，尤其是在OOD场景中的表现不佳。  \n2. 如何增强自动驾驶模型的泛化能力和主动决策能力。  \n3. 传统方法的静态决策模型无法应对复杂的驾驶环境。  \n\n【用了什么创新方法】  \nMTRDrive框架通过整合程序化驾驶经验与动态工具，构建了一个互动推理的模型，旨在提升自动驾驶的可靠性。该方法允许代理主动检索驾驶经验，并使用工具实时查询环境，从而减少视觉幻觉的风险，并显著改善在新颖、未见场景中的表现。最终，该框架有效提升了自动驾驶在复杂情况下的决策能力，推动了VLMs在现实世界应用的可行性。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation",
            "authors": "Dekun Lu,Wei Gao,Kui Jia",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "First two authors contribute equally. Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.20841",
            "code": "https://sites.google.com/view/imaginationpolicy",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20841",
            "arxiv_html_link": "https://arxiv.org/html/2509.20841v1",
            "abstract": "End-to-end robot manipulation policies offer significant potential for enabling embodied agents to understand and interact with the world. Unlike traditional modular pipelines, end-to-end learning mitigates key limitations such as information loss between modules and feature misalignment caused by isolated optimization targets. Despite these advantages, existing end-to-end neural networks for robotic manipulation—including those based on large vision-language-action (VLA) models—remain insufficiently performant for large-scale practical deployment. In this paper, we take a step towards an end-to-end manipulation policy that is generalizable, accurate and reliable.\nTo achieve this goal, we propose a novel Chain of Moving Oriented Keypoints (CoMOK) formulation for robotic manipulation. Our formulation is used as the action representation of a neural policy, which can be trained in an end-to-end fashion (details in Sec. III). Such an action representation is general, as it extends the standard end-effector pose action representation and supports a diverse set of manipulation tasks in a unified manner.\nThe oriented keypoint in our method enables natural generalization to objects with different shapes and sizes, while achieving sub-centimeter accuracy.\nMoreover, our formulation can easily handle multi-stage tasks, multi-modal robot behaviors, and deformable objects.\nExtensive simulated and hardware experiments demonstrate the effectiveness of our method.\nVideo demonstration, source code and supplemental materials are available on https://sites.google.com/view/imaginationpolicy",
            "introduction": "During the past decade, deep learning has profoundly reshaped the landscape of robotic manipulation. One popular approach leverages deep neural networks (DNNs) as functional submodules in traditional modularized pipelines. This significantly enhances the capabilities of robot manipulators, particularly in robot perception [46, 37, 31]. Concurrently, a distinct paradigm has emerged, pursuing end-to-end, pixel-to-torque learning strategies [4, 16, 48]. This alternative methodology aims to circumvent the information bottleneck between discrete modules in traditional pipelines by directly mapping raw sensory input to low-level control actions (e.g., robot joint torque commands).\n\nThe success of Large Language Models (LLMs) [1, 2, 36] and Vision-Language Models (VLMs) [40, 21, 20] over the past years has injected new inspiration into robotic manipulation research. A key insight driving this interest is the promising generalization capability of LLMs/VLMs, benefits from large-scale, multiple-task pretraining. Motivated by this potential, researchers are increasingly exploring Vision-Language-Action (VLAs) models [14, 3, 22] – aiming to imbue robots with similar generalization capability. For robotic manipulation, this desired generalization manifests primarily in two critical dimensions: 1) generalization across scenes and objects (robustly handling novel objects with variations on shape, size, and appearance), and 2) generalization across diverse manipulation tasks (adapting to new instructions and tasks without expensive retraining).\n\nDespite significant research advances, current VLA models have not yet seen large-scale deployment in real-world industrial or service applications. Their performance in key operational perspectives, such as reliability and accuracy, can be insufficient for practical deployment and sometimes inferior to well-engineered traditional modularized pipelines. This shortcoming becomes particularly pronounced when these VLAs are deployed in unseen objects or different robot platforms. Consequently, bridging the gap between the potential for broad generalization and the practical demands of real-world performance remains a central challenge.\n\nIn this paper, we propose an alternative approach for end-to-end robot manipulation that emphasizes reliability, accuracy, and interpretability.\nTo achieve this, we propose a novel affordance-based formulation for robotic manipulation, where we define affordance as task-specific, semantically meaningful local object parts.\nTo translate this cognitive concept into executable robotic actions, we propose to represent affordance as semantic, task-specific oriented keypoint(s). Subsequently, the manipulation behavior can be represented as a Chain of Moving Oriented Keypoints (CoMOK), as detailed in Sec. III.\nThis affordance-base formulation is used as the action representation of a neural network based manipulation policy, which can be trained end-to-end.\nThis action representation is general, as it reduces to the standard end-effector pose action in a special case. The proposed formulation provides a unified framework for diverse manipulation tasks, including ones previously studied in isolation (such as robot grasping algorithms).\nThe oriented keypoint affordance enables natural generalization to objects with different shape and size, while achieving sub-centimeter level accuracy. Moreover, the proposed formulation can easily handle multi-stage tasks, multi-modality robot behaviors and deformable objects.\n\nWe also design a new neural network architecture based on the proposed formulation. The network takes sensory observation and task encoding (e.g., language prompt) as inputs. It uses score-matching [38, 4] (a variant of diffusion model) to produce the multi-modal robot actions. The network can be trained end-to-end and the output can be directly used for joint-level motion generation. This joint-level motion generator can be either 1) a traditional motion planning algorithms; or 2) a neural-network based trajectory generation pipeline (which can be trained end-to-end). We attempt both types of motion generators on simulated and real-world experiments. Our network demonstrates promising generalization capability without sacrificing accuracy/reliability.\n\nThis paper is organized as follows: in Sec. II we review related works. Sec. III describes our manipulation formulation and several important extensions. Sec. IV introduces the diffusion-based neural network to address multi-modal, multi-stage manipulation tasks. Sec. V demonstrates our methods on several simulated and real world tasks and shows generalization of our method. Sec. VI concludes.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现通用、精确且可靠的端到端机器人操控策略。  \n2. 当前基于视觉-语言-动作（VLA）模型的机器人操控在实际应用中的表现不足。  \n3. 如何在多阶段任务和多模态机器人行为中保持高效的操控能力。  \n\n【用了什么创新方法】  \n提出了一种新的“移动定向关键点链”（CoMOK）形式作为机器人操控的动作表示，能够在端到端的方式下进行训练。该方法通过定义任务特定的语义关键点，促进了对不同形状和大小物体的自然泛化，同时实现了亚厘米级的精度。此外，设计了一种新的神经网络架构，结合感知观察和任务编码，使用分数匹配生成多模态机器人动作。通过广泛的模拟和硬件实验，验证了该方法在多样化操控任务中的有效性和可靠性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "SemSight: Probabilistic Bird's-Eye-View Prediction of Multi-Level Scene Semantics for Navigation",
            "authors": "Jiaxuan He,Jiamei Ren,Chongshang Yan,Wenjie Song",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20839",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20839",
            "arxiv_html_link": "https://arxiv.org/html/2509.20839v1",
            "abstract": "In target-driven navigation and autonomous exploration, reasonable prediction of unknown regions is crucial for efficient navigation and environment understanding. Existing methods mostly focus on single objects or geometric occupancy maps, lacking the ability to model room-level semantic structures. We propose SemSight, a probabilistic bird’s-eye-view prediction model for multi-level scene semantics. The model jointly infers structural layouts, global scene context, and target area distributions, completing semantic maps of unexplored areas while estimating probability maps for target categories. To train SemSight, we simulate frontier-driven exploration on 2,000 indoor layout graphs, constructing a diverse dataset of 40,000 sequential egocentric observations paired with complete semantic maps. We adopt an encoder–decoder network as the core architecture and introduce a mask-constrained supervision strategy. This strategy applies a binary mask of unexplored areas so that supervision focuses only on unknown regions, forcing the model to infer semantic structures from the observed context. Experimental results show that SemSight improves prediction performance for key functional categories in unexplored regions and outperforms non-mask-supervised approaches on metrics such as Structural Consistency (SC) and Region Recognition Accuracy (PA). It also enhances navigation efficiency in closed-loop simulations, reducing the number of search steps when guiding robots toward target areas.",
            "introduction": "Autonomous robot navigation requires not only local perception but also the ability to predict unobserved areas. However, occlusions and limited sensing range often force robots to rely on egocentric partial observations. As a result, they lack a holistic understanding of the environment’s structural and semantic layout. Critical semantic structures in unobserved regions (e.g., corridors or doorways) have a notable impact on the efficiency of navigation. Therefore, predicting unknown areas from partial observations is a key challenge. The goal is to infer potential traversable structures and semantic distributions that support efficient exploration and navigation.\n\nRecent studies have explored scene reasoning and map prediction, employing learning-based approaches to infer unknown regions from partial observations. Most existing work focuses on geometric occupancy completion[2, 3, 4, 5, 1] and fails to address higher-level semantic reasoning. For example, such methods cannot answer questions like “where is the bedroom or kitchen likely to be?”. Other approaches target specific semantic categories[6, 7, 8, 9, 10] but cannot capture semantic regularities across large spatial extents. As a result, their predictions are often fragmented and localized, without providing room-level semantic priors. High-level semantic structures play an important role in autonomous navigation. For instance, in target-driven tasks, a robot that anticipates “an unknown region is likely a bedroom” or “a corridor connects to other rooms” can plan paths more efficiently and accomplish navigation more effectively. Such semantic prediction capability provides a valuable prior for task-level reasoning. Compared with robots that rely solely on immediate perception, robots capable of scene-structural reasoning and semantic prediction gain the ability to “imagine the unseen world.”\n\nMotivated by this, we propose SemSight, a probabilistic semantic foresight model. It is designed to predict the global semantic structure of an environment from partial observations, rather than being limited to local occupancy estimation. SemSight takes sparse exploration observations and target categories as input, predicts probabilities associated with target-related structures, and infers room-level semantic layouts and structural relations. By formulating semantic map prediction as a probabilistic inference problem, we directly support structured reasoning for navigation decisions, going beyond pure geometric reconstruction.\n\nTo train the SemSight model, we construct a semantic prediction dataset under the BEV representation. Unlike previous methods relying on static annotations, we simulate boundary-driven exploration processes on a large collection of floorplans, This process dynamically collects egocentric observation sequences paired with complete semantic maps as training samples. This dataset covers diverse indoor layouts with rich room types and structural information, effectively supporting the learning of local-to-global semantic reasoning. In summary, the main contributions are as follows:\n\nSemSight model – a probabilistic prediction model that integrates structural layouts, global scene context, and target area distributions for navigation. It maps partial observations and structural target conditions to global semantic maps and target probability maps, supporting target-driven navigation and exploration.\n\nSemSight model – a probabilistic prediction model that integrates structural layouts, global scene context, and target area distributions for navigation. It maps partial observations and structural target conditions to global semantic maps and target probability maps, supporting target-driven navigation and exploration.\n\nDiverse boundary-driven exploration dataset – establishing 40,000 pairings of continuous partial observations and complete semantic maps, covering 2,000 varied indoor layouts. This provides diverse training data for semantic completion and prediction tasks.\n\nSemantic foresight with navigation impact – experiments demonstrate that our mask-constrained supervision enables SemSight to infer room-level layouts and probabilistic target area distributions in unexplored regions. Furthermore, navigation simulations show that incorporating SemSight predictions improves exploration efficiency and target finding success, confirming its practical value for autonomous navigation.\n\n1. SemSight model – a probabilistic prediction model that integrates structural layouts, global scene context, and target area distributions for navigation. It maps partial observations and structural target conditions to global semantic maps and target probability maps, supporting target-driven navigation and exploration.\n\n2. Diverse boundary-driven exploration dataset – establishing 40,000 pairings of continuous partial observations and complete semantic maps, covering 2,000 varied indoor layouts. This provides diverse training data for semantic completion and prediction tasks.\n\n3. Semantic foresight with navigation impact – experiments demonstrate that our mask-constrained supervision enables SemSight to infer room-level layouts and probabilistic target area distributions in unexplored regions. Furthermore, navigation simulations show that incorporating SemSight predictions improves exploration efficiency and target finding success, confirming its practical value for autonomous navigation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效预测未知区域以支持自主导航和环境理解。  \n2. 现有方法缺乏对房间级语义结构的建模能力。  \n3. 如何从部分观察中推断潜在可通行结构和语义分布。  \n\n【用了什么创新方法】  \n提出了SemSight模型，一种概率鸟瞰图预测模型，能够联合推断结构布局、全局场景上下文和目标区域分布。该模型通过掩码约束监督策略，专注于未知区域的推断，利用编码器-解码器网络架构进行训练。实验结果表明，SemSight在未探索区域的关键功能类别预测性能上有所提升，并在导航闭环模拟中提高了效率，减少了引导机器人到目标区域的搜索步骤。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning",
            "authors": "Gawon Lee(1),Daesol Cho(1),H. Jin Kim(1) ((1) Seoul National University)",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "Accepted for publication in the proceedings of the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "pdf_link": "https://arxiv.org/pdf/2509.20766",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20766",
            "arxiv_html_link": "https://arxiv.org/html/2509.20766v1",
            "abstract": "Multi-task reinforcement learning (MTRL) offers a promising approach to improve sample efficiency and generalization by training agents across multiple tasks, enabling knowledge sharing between them.\nHowever, applying MTRL to robotics remains challenging due to the high cost of collecting diverse task data.\nTo address this, we propose MT-Lévy, a novel exploration strategy that enhances sample efficiency in MTRL environments by combining behavior sharing across tasks with temporally extended exploration inspired by Lévy flight [1].\nMT-Lévy leverages policies trained on related tasks to guide exploration towards key states, while dynamically adjusting exploration levels based on task success ratios.\nThis approach enables more efficient state-space coverage, even in complex robotics environments.\nEmpirical results demonstrate that MT-Lévy significantly improves exploration and sample efficiency, supported by quantitative and qualitative analyses.\nAblation studies further highlight the contribution of each component, showing that combining behavior sharing with adaptive exploration strategies can significantly improve the practicality of MTRL in robotics applications.",
            "introduction": "Reinforcement learning (RL) aims to autonomously train an agent to solve complex control tasks in simulated and real-world environments [2].\nNumerous successful applications of RL have been demonstrated in domains such as board games [3], Atari games [4], and continuous control [5].\nHowever, applying RL to real-world tasks remains challenging because most RL algorithms require vast training data and are typically designed to solve only a single task.\nTo mitigate these issues of sample inefficiency and task specificity, multi-task RL (MTRL) has emerged as a promising research area [6, 7, 8].\nIn MTRL, an agent is trained on multiple tasks simultaneously, enabling it to leverage knowledge acquired from one task to benefit learning in others.\nThis approach enhances sample efficiency and leads to more generalizable knowledge across tasks [9, 10, 11, 12].\nDespite its benefits, implementing MTRL in robotics environments continues to encounter substantial hurdles.\nTypical MTRL setups require a diverse set of tasks, often demanding intricate experimental configurations—such as deploying fleets of robots [13] or employing automated reset mechanisms to collect data without human intervention [14].\nWhile these approaches simplify data collection by scaling up experiments and reducing manual effort, they do not directly address the core issue: the inherent sample inefficiency of current MTRL algorithms.\nFrom a robotics standpoint, reducing sample complexity is essential for making these algorithms practically viable.\nOne promising approach to improving sample efficiency is to adopt a more effective exploration mechanism [15, 16, 17].\nHowever, previous works have predominantly focused on exploration strategies for single-task environments, thus directly applying those strategies to MTRL settings can be inefficient and complex because they do not account for the presence of multiple tasks.\nThus, there is a growing need to develop an exploration method that is tailored to MTRL environments.\n\nTo address the data collection challenge by improving sample efficiency, we propose three components that enhance exploration.\nFirst, since tasks in MTRL commonly share features such as state/action spaces, behavior patterns, and reward structures, we propose leveraging policies trained on other tasks to facilitate exploration in a new task—a strategy known as behavior sharing [12, 18].\nAlthough these policies might not yield optimal behavior for the new task, they tend to exhibit purposeful actions that effectively guide exploration [19].\nSecond, executing these exploration policies in a temporally extended manner helps direct the agent toward states of interest [17, 20].\nIn our approach, we employ Lévy flight [1]—a pattern observed in natural phenomena such as food foraging, transportation, and microbial movement—to implement this extended temporal exploration.\nFinally, we utilize an exponential moving average of success ratios and track it to automatically adjust both the onset and duration of exploration.\nThis reduces the distribution mismatch between the data collection policy and the learned policy.\nBy integrating these components, we present MT-Lévy, which combines behavior sharing, temporally extended exploration using Lévy flight, and an automatic exploration adjustment mechanism based on success ratios.\nEvaluations on MTRL benchmarks demonstrate that (1) MT-Lévy discovers more key states that are closely related to solving tasks, and (2) it achieves higher sample efficiency and performance than single-task oriented exploration methods and previously proposed MTRL algorithms.\nIn summary, our key contributions are as follows:\n\nWe propose MT-Lévy, an exploration method that enhances sample efficiency in MTRL environments by integrating behavior sharing, Lévy flight-based temporally extended exploration, and an automatic exploration adjustment mechanism.\n\nWe empirically demonstrate that MT-Lévy facilitates more effective exploration by discovering key states, improves sample efficiency, and outperforms existing methods.\nAdditionally, we provide comprehensive ablation studies to analyze the contribution of each component.\n\n1. We propose MT-Lévy, an exploration method that enhances sample efficiency in MTRL environments by integrating behavior sharing, Lévy flight-based temporally extended exploration, and an automatic exploration adjustment mechanism.\n\n2. We empirically demonstrate that MT-Lévy facilitates more effective exploration by discovering key states, improves sample efficiency, and outperforms existing methods.\nAdditionally, we provide comprehensive ablation studies to analyze the contribution of each component.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在多任务强化学习（MTRL）中提高样本效率和泛化能力。  \n2. 如何有效收集多样化任务数据以应对机器人应用中的高成本问题。  \n3. 现有的探索策略在多任务环境中的适用性不足。  \n\n【用了什么创新方法】  \n提出MT-Lévy，一种新颖的探索策略，通过结合跨任务的行为共享与基于Lévy飞行的时间扩展探索，来增强MTRL环境中的样本效率。该方法利用已训练的相关任务策略引导探索，动态调整探索水平，确保更高效的状态空间覆盖。实验证明，MT-Lévy显著提高了探索和样本效率，发现了更多关键状态，并在MTRL基准测试中超越了现有方法。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "MASt3R-Fusion: Integrating Feed-Forward Visual Model with IMU, GNSS for High-Functionality SLAM",
            "authors": "Yuxuan Zhou,Xingxing Li,Shengyu Li,Zhuohao Yan,Chunxi Xia,Shaoquan Feng",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20757",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20757",
            "arxiv_html_link": "https://arxiv.org/html/2509.20757v1",
            "abstract": "Visual SLAM is a cornerstone technique in robotics, autonomous driving and extended reality (XR), yet classical systems often struggle with low-texture environments, scale ambiguity, and degraded performance under challenging visual conditions. Recent advancements in feed-forward neural network-based pointmap regression have demonstrated the potential to recover high-fidelity 3D scene geometry directly from images, leveraging learned spatial priors to overcome limitations of traditional multi-view geometry methods.\nHowever, the widely validated advantages of probabilistic\nmulti-sensor information fusion are often discarded in these\npipelines. In this work, we propose MASt3R-Fusion,\na multi-sensor-assisted visual SLAM framework that tightly integrates feed-forward\npointmap regression with complementary sensor information, including inertial\nmeasurements and GNSS data. The system introduces Sim(3)-based visual\nalignment constraints (in the Hessian form) into a universal metric-scale SE(3) factor graph for effective information fusion.\nA hierarchical factor graph design is developed, which allows both\nreal-time sliding-window optimization and global optimization with aggressive loop closures,\nenabling real-time pose tracking, metric-scale structure perception and globally consistent mapping.\nWe evaluate our approach on both public benchmarks and self-collected datasets,\ndemonstrating substantial improvements in accuracy and robustness over existing visual-centered multi-sensor SLAM systems.\nThe code will be released open-source to support reproducibility and further research111https://github.com/GREAT-WHU/MASt3R-Fusion.",
            "introduction": "Visual SLAM is a widely utilized technique in applications such as robotics, autonomous driving,\nand extended reality (XR)[1]. SLAM systems generally require comprehensive functionality\nto support diverse spatial tasks. Compared with 3D reconstruction, visual SLAM places a\nstronger emphasis on real-time pose estimation and scene structure perception. In contrast to visual odometry,\nSLAM systems aim to provide a more complete understanding of the environment while\nsimultaneously maintaining long-term global consistency.\n\nAlthough classical visual SLAM frameworks have achieved notable successes,\ntheir practical usability remains limited due to several critical factors.\nFirst, traditional visual SLAM methods based on sparse features or photometric consistency\noften fail to achieve complete spatial perception, particularly in low-texture\nenvironments. Second, the system performance tends to degrade significantly under challenging visual conditions.\nIt also remains an open problem how to achieve life-long consistency across time periods and under varying illumination conditions.\n\nTo overcome these limitations, the integration of deep learning/foundation models has\nemerged as an effective solution[2]. In particular, recent feed-forward neural\nnetwork-based approaches for visual geometry estimation (e.g., DUSt3R[3], MASt3R[4], VGGT[5]),\nhave attracted growing attention. These methods encode the images and decode the latents to directly\nrecover 2D-to-3D pointmaps and other dense information. By leveraging large-scale data\nto learn spatial priors, they alleviate many of the degeneracies encountered in traditional\nmulti-view stereo (MVS) and SLAM systems, which greatly inspire recent SLAM implementations[6, 7].\n\nDespite significant progress, vision-based methods still suffer from inherent\nlimitations, especially regarding scale consistency and performance degradation in visually\ndeprived environments. To address this, a promising direction is to integrate these novel visual paradigms\nwith complementary sensors,\nsuch as inertial measurement units (IMUs) and global navigation satellite system (GNSS),\nthus to exploit the respective strengths of traditional multi-sensor\nfusion and learning-based spatial priors.\n\nIn this work, we propose MASt3R-Fusion,\na framework that integrates multi-sensor information with the\nfeed-forward pointmap regression paradigm, facilitating high-functionality SLAM that supports real-time state estimation,\ncomplete-view structure perception and\nglobally consistent mapping.\n\nThe contributions of this work are listed as follows:\n\nWe propose a framework to fuse feed-forward pointmap regression with multi-sensor information in a tight way,\nbridging Sim(3)-based visual alignment constraints\nand metric-scale SE(3) states and factors.\n\nWe propose a framework to fuse feed-forward pointmap regression with multi-sensor information in a tight way,\nbridging Sim(3)-based visual alignment constraints\nand metric-scale SE(3) states and factors.\n\nBased on the feed-forward model, we develop a real-time visual-inertial SLAM system that supports metric-scale pose estimation and dense perception.\n\nA globally consistent SLAM system that leverages loop closure and GNSS information is developed, in which\ngeometry-based loop closure candidate filtering and full-information iterative optimization are utilized.\n\nBoth public and self-made datasets are employed to comprehensively evaluate the system performance.\n\nThe code is made open-source to benefit the community.",
            "llm_summary": "【关注的是什么问题】  \n1. 传统视觉SLAM在低纹理环境中的性能不足。  \n2. SLAM系统在挑战性视觉条件下的表现下降。  \n3. 如何实现长期一致性和跨时间段的环境理解。  \n\n【用了什么创新方法】  \n本研究提出了MASt3R-Fusion框架，通过将前馈点图回归与IMU和GNSS等多传感器信息紧密集成，克服了传统视觉SLAM的局限性。该系统引入了基于Sim(3)的视觉对齐约束，构建了一个通用的度量尺度SE(3)因子图，支持实时滑动窗口优化和全局优化，增强了姿态跟踪、结构感知和全局一致性映射的能力。通过在公共基准和自收集数据集上的评估，显示出相较于现有视觉中心多传感器SLAM系统的显著准确性和鲁棒性提升。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning",
            "authors": "Guoyang Zhao,Yudong Li,Weiqing Qi,Kai Zhang,Bonan Liu,Kai Chen,Haoang Li,Jun Ma",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20739",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20739",
            "arxiv_html_link": "https://arxiv.org/html/2509.20739v1",
            "abstract": "Conventional SLAM pipelines for legged robot navigation are fragile under rapid motion, calibration demands, and sensor drift, while offering limited semantic reasoning for task-driven exploration. To deal with these issues, we propose a vision-only, SLAM-free navigation framework that replaces dense geometry with semantic reasoning and lightweight topological representations. A hierarchical vision-language perception module fuses scene-level context with object-level cues for robust semantic inference. And a semantic-probabilistic topological map supports coarse-to-fine planning: LLM-based global reasoning for subgoal selection and vision-based local planning for obstacle avoidance. Integrated with reinforcement-learning locomotion controllers, the framework is deployable across diverse legged robot platforms. Experiments in simulation and real-world settings demonstrate consistent improvements in semantic accuracy, planning quality, and navigation success, while ablation studies further showcase the necessity of both hierarchical perception and fine local planning. This work introduces a new paradigm for SLAM-free, vision-language-driven navigation, shifting robotic exploration from geometry-centric mapping to semantics-driven decision making.",
            "introduction": "Autonomous exploration and navigation remain fundamental challenges for mobile robots in open and unstructured environments. These capabilities are critical for applications such as search-and-rescue, warehouse logistics, and environmental monitoring, where robots must explore unknown areas, interpret scene semantics, and reach task-relevant targets [1]. Conventional navigation pipelines typically rely on Simultaneous Localization and Mapping (SLAM) with LiDAR or multi-sensor fusion for pose estimation and dense metric map construction [2]. However, such geometry-driven methods are brittle under the rapid motion and ground impacts of legged robots, where visual odometry often suffers from drift and tracking loss [3]. In addition, multi-sensor calibration and the high computational cost of dense mapping hinder deployment in lightweight, camera-only systems. For many task-driven applications, building dense maps is unnecessary; what is needed is a compact, scalable representation that supports efficient exploration.\n\nTraditional SLAM-based navigation primarily focuses on geometric reconstruction and pose optimization, with limited support for semantic reasoning about the environment [4]. This limitation is especially critical for open-set object search and semantic exploration tasks, where the robot must not only localize itself but also reason about which regions are worth exploring next, which objects are relevant to its mission, and how to plan under uncertainty. Compared with dense maps, topological representations are more compact and lightweight, while still capturing the essential connectivity of the environment and naturally supporting the integration of semantic labels for long-horizon exploration.\n\nRecent advances in Vision-Language Models (VLMs) and Vision-Language Navigation (VLN) have opened new opportunities to endow robots with rich semantic understanding and language grounding [5]. These models can infer task-relevant object categories and scene descriptions from natural language instructions and visual inputs [6]. However, existing VLN approaches largely focus on perception and recognition, while overlooking the integration of global planning, decision-making, and fine-grained obstacle avoidance [7]. Furthermore, most prior works have been evaluated primarily in static or simulated environments and have not addressed the challenges of deploying such systems on real-world, heterogeneous legged platforms.\n\nTo address these challenges, we revisit the design of the robotic navigation stack from a system-level perspective and propose a purely visual, SLAM-free navigation framework (Fig. 1). Our framework relies exclusively on onboard cameras to achieve semantic-driven exploration and is designed with cross-platform deployability in mind. Specifically, we address four core challenges: (i) achieving robust perception and incremental exploration without explicit geometric SLAM; (ii) enabling robust semantic understanding in unstructured scenes; (iii) integrating global reasoning, local obstacle avoidance, and interpretable planning within a unified pipeline; and (iv) supporting rapid deployment and consistent performance across different types of legged robots.\n\nTo tackle these challenges, we propose a SLAM-Free visual navigation framework with hierarchical Vision-language perception and coarse-to-fine semantic topological planning, which integrates semantic perception, topological reasoning, and motion control into a unified architecture. Concretely, we design an adaptive hierarchical perception module that fuses scene-level and object-level outputs from VLMs to achieve robust semantic inference; construct a semantic-probabilistic topological map and perform coarse-to-fine planning via LLM-augmented global reasoning and vision-based local obstacle avoidance, replacing traditional geometry-based global path planning; and train reinforcement learning (RL)-based locomotion policies for different legged robot morphologies, enabling seamless integration and cross-platform deployment. Our main contributions are summarized as follows:\n\nWe present a purely visual, SLAM-free semantic exploration and navigation framework that addresses the fragility of SLAM on legged robots under rapid motion and sensor drift. The framework supports incremental target search in open-world environments while overcoming the limitations of geometry-centric approaches.\n\nWe present a purely visual, SLAM-free semantic exploration and navigation framework that addresses the fragility of SLAM on legged robots under rapid motion and sensor drift. The framework supports incremental target search in open-world environments while overcoming the limitations of geometry-centric approaches.\n\nWe design a hierarchical visual perception module with a lightweight adaptive fusion strategy that combines scene- and object-level VLM outputs for robust, context-aware understanding.\n\nWe construct a semantic-probabilistic topological map and propose a coarse-to-fine planning paradigm combining LLM-driven global reasoning with vision-based obstacle avoidance, serving as an alternative to SLAM.\n\nWe conduct comprehensive experiments in both simulation and real-world settings, showing gains in success rate, semantic accuracy, and path efficiency.\n\n1. We present a purely visual, SLAM-free semantic exploration and navigation framework that addresses the fragility of SLAM on legged robots under rapid motion and sensor drift. The framework supports incremental target search in open-world environments while overcoming the limitations of geometry-centric approaches.\n\n2. We design a hierarchical visual perception module with a lightweight adaptive fusion strategy that combines scene- and object-level VLM outputs for robust, context-aware understanding.\n\n3. We construct a semantic-probabilistic topological map and propose a coarse-to-fine planning paradigm combining LLM-driven global reasoning with vision-based obstacle avoidance, serving as an alternative to SLAM.\n\n4. We conduct comprehensive experiments in both simulation and real-world settings, showing gains in success rate, semantic accuracy, and path efficiency.",
            "llm_summary": "【关注的是什么问题】  \n1. 传统SLAM方法在快速运动和传感器漂移下的脆弱性。  \n2. 任务驱动探索中对语义推理的支持不足。  \n3. 如何实现无SLAM的视觉导航框架以提高灵活性。  \n4. 在不同类型的腿式机器人上实现快速部署和一致性能。  \n\n【用了什么创新方法】  \n提出了一种无SLAM的视觉导航框架，利用分层的视觉-语言感知模块融合场景和物体级信息，进行稳健的语义推理。构建了语义-概率拓扑图，并通过大型语言模型（LLM）驱动的全局推理和基于视觉的局部障碍物规避实现粗到细的规划。结合强化学习的运动控制策略，框架在多种腿式机器人平台上实现了跨平台部署。实验结果显示，该方法在语义准确性、规划质量和导航成功率上均有显著提升，展示了从几何中心映射转向语义驱动决策的新范式。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking",
            "authors": "Zhenguo Sun,Yibo Peng,Yuan Meng,Xukun Li,Bo-Sheng Huang,Zhenshan Bing,Xinlong Wang,Alois Knoll",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20717",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20717",
            "arxiv_html_link": "https://arxiv.org/html/2509.20717v1",
            "abstract": "Long-horizon, high-dynamic motion tracking on humanoids remains brittle because absolute joint commands cannot compensate model–plant mismatch, leading to error accumulation.\nWe propose RobotDancing, a simple, scalable framework that predicts residual joint targets to explicitly correct dynamics discrepancies.\nThe pipeline is end-to-end—training, sim-to-sim validation, and zero-shot sim-to-real—and uses a single-stage reinforcement learning (RL) setup with a unified observation, reward, and hyperparameter configuration.\nWe evaluate primarily on Unitree G1 with retargeted LAFAN1 dance sequences and validate transfer on H1/H1-2.\nRobotDancing can track multi-minute, high-energy behaviors (jumps, spins, cartwheels) and deploys zero-shot to hardware with high motion tracking quality.",
            "introduction": "Humanoid robots are increasingly expected to execute long-horizon, highly dynamic behaviors such as dance, where small tracking errors compound rapidly and destabilize control. A principal source of such drift is the mismatch between idealized reference trajectories and the robot’s true physics (actuation limits, friction, inertia, latency).\nSince the field of animation has shown incredible progress in whole body dynamic control of physics-based characters [1, 2, 3], precise and robust motion tracking has been one of the core challenges in humanoid robotics, particularly when transferring motions from simulation or captured human data to real-world robotic platforms.\n\nRecent advances in physics-based humanoid control [4, 5, 6, 7, 8, 9, 10, 11] have demonstrated the potential of learning-based controllers integrated with physical constraints. Nevertheless, most existing methods generally still predict absolute joint commands, which works well for short, quasi-cyclic skills but remains fragile on long, high-energy sequences, leaving untapped the potential of explicitly modeling and compensating for dynamic discrepancies.\n\nInspired by residual learning paradigms [12, 13], we propose a novel residual-action reinforcement learning (RL) framework RobotDancing for high-quality tracking of long high-dynamic dance motions. Unlike previous methods, our framework explicitly learns the dynamics discrepancy between the reference motion and the robot. Instead of issuing absolute commands, the policy outputs residual actions that correct the reference in joint space, thereby allocating model capacity to physics compensation rather than re-synthesizing the motion.\nThis strategy markedly reduces the accumulation of motion tracking errors and enables robust and accurate motion tracking over long horizons.\n\nOur preliminary experiments conducted on the Unitree G1 humanoid robot platform demonstrate notably superior tracking accuracy compared to absolute actions methods, while yielding high motion quality. Furthermore, we also validated our framework’s generalization capabilities across multiple humanoid robot platforms (Unitree H1 and H1-2).\n\nThe primary contributions of this paper include:\n\nA Simple, Open, and Scalable Motion-Tracking Framework.\nWe present an end-to-end pipeline—training, sim-to-sim validation, and sim-to-real deployment—that achieves long-horizon, high-dynamic tracking with a single-stage RL algorithm (no multi-stage distillation or teacher-student schemes [14]).\n\nResidual-Action Tracking Targets Dynamics Discrepancy.\nA residual-action policy corrects reference joints online, reducing error accumulation and improving stability on demanding sequences (e.g., dance).\n\nEffective Sampling for Long-Tail Motions.\nA two-part strategy (distribution-aware balancing + failure-aware prioritization) improves coverage of rare but informative poses and accelerates progress on difficult segments.\n\nGeneralization across Motions and Platforms.\nA unified set of observations, rewards, and hyperparameters trains diverse motions without per-task tuning, and transfers to multiple humanoid platforms including 2 full-size robots; to the best of our knowledge, this is one of the first demonstrations of high-dynamic, long-horizon motion tracking on full-size humanoids.\n\n1. A Simple, Open, and Scalable Motion-Tracking Framework.\nWe present an end-to-end pipeline—training, sim-to-sim validation, and sim-to-real deployment—that achieves long-horizon, high-dynamic tracking with a single-stage RL algorithm (no multi-stage distillation or teacher-student schemes [14]).\n\n2. Residual-Action Tracking Targets Dynamics Discrepancy.\nA residual-action policy corrects reference joints online, reducing error accumulation and improving stability on demanding sequences (e.g., dance).\n\n3. Effective Sampling for Long-Tail Motions.\nA two-part strategy (distribution-aware balancing + failure-aware prioritization) improves coverage of rare but informative poses and accelerates progress on difficult segments.\n\n4. Generalization across Motions and Platforms.\nA unified set of observations, rewards, and hyperparameters trains diverse motions without per-task tuning, and transfers to multiple humanoid platforms including 2 full-size robots; to the best of our knowledge, this is one of the first demonstrations of high-dynamic, long-horizon motion tracking on full-size humanoids.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现长时间、高动态的类人机器人运动跟踪？  \n2. 如何解决模型与实际植物之间的动态差异导致的跟踪误差积累？  \n3. 如何在不同类人机器人平台上实现运动的通用性和迁移能力？  \n\n【用了什么创新方法】  \n本研究提出了RobotDancing，一个简单且可扩展的框架，通过预测残差关节目标来显式校正动态差异。该流程为端到端，包括训练、sim-to-sim验证和零-shot sim-to-real，采用单阶段强化学习设置，使用统一的观察、奖励和超参数配置。实验表明，RobotDancing能够有效跟踪多分钟的高能行为（如跳跃、旋转、翻滚），并在硬件上实现高质量的运动跟踪，显著优于传统的绝对动作方法。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Digital Twin-Guided Robot Path Planning: A Beta-Bernoulli Fusion with Large Language Model as a Sensor",
            "authors": "Mani Amani,Reza Akhavian",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20709",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20709",
            "arxiv_html_link": "https://arxiv.org/html/2509.20709v1",
            "abstract": "Integrating natural language (NL) prompts into robotic mission planning has attracted significant interest in recent years. In the construction domain, Building Information Models (BIM) encapsulate rich NL descriptions of the environment. We present a novel framework that fuses NL directives with BIM–derived semantic maps via a Beta–Bernoulli Bayesian fusion by interpreting the LLM as a sensor: each obstacle’s design‐time repulsive coefficient is treated as a Beta​(α,β)\\mathrm{Beta}(\\alpha,\\beta) random variable and LLM‐returned “danger scores” are incorporated as pseudo‐counts to update α\\alpha and β\\beta. The resulting posterior mean yields a continuous, context‐aware repulsive gain that augments a Euclidean‐distance‐based potential field for cost heuristics. By adjusting gains based on sentiment and context inferred from user prompts, our method guides robots along safer, more context‐aware paths. This provides a numerically stable method that can chain multiple natural commands and prompts from construction workers and foreman to enable planning while giving flexibility to be integrated in any learned or classical AI framework. Simulation results demonstrate that this Beta–Bernoulli fusion yields both qualitative and quantitative improvements in path robustness and validity. Our code is publicly available on GitHub.",
            "introduction": "The emergence of large language models (LLMs) has fundamentally transformed natural language processing (NLP) capabilities across diverse domains, with robotic planning experiencing similar paradigm shifts through enhanced semantic understanding and contextual reasoning [Dai et al. (2024)][Hu and Zhou (2024)]. While there has been a concentrated focus on the advancement of prompted robotic applications, most of the research on language-commanded robotics is limited to predefined objects and primitives [Arenas et al. (2024)]. This restriction confines robots to a limited set of predefined actions. Sometimes, mission parameters must be adjusted and tuned given unseen and undefined prompts. Furthermore, it is well established that humans expect their collaborators, including AI, to interpret implicit cues beyond explicit wording [Liang et al. (2019)]. This creates a need for a more flexible framework to analyze implicit sentiment information from natural language prompts. Our proposed BIM–LLM Bayesian fusion framework, illustrated in Figure 1, addresses this gap.\nSince implicit and semantic information inherently depend on subjective human interpretation, a probabilistic framework provides a principled approach to quantify and propagate these uncertainties through the planning system. Specifically, the inherent ambiguity and context-dependency of natural language sentiment, combined with the possibility of LLM hallucinations, require a stable probabilistic approach. Therefore, leveraging deep learning models is considered a suitable approach since they have been shown to have high-accuracy sentiment analysis [Aslan (2023)]. LLMs have demonstrated extensive success in sentiment analysis, positioning them as attractive candidates for assessing Theory of Mind-like associations [Zhang et al. (2023)]. Theory of Mind refers to the ability to attribute mental states (like beliefs, desires, and intentions) to oneself and others. In this context, the ’Theory of Mind-like associations’ means that the system is designed to infer and relate implicit emotional or cognitive states from verbal communication and, more generally, observed behavior [Street (2024)]. While these advancements show promise in human-machine interaction, in robotics, the analysis must operate under strict mission constraints and heuristics, requiring that the sentiment data be transformed into actionable insights that can be adapted across various scenarios. This introduces additional complexity in transforming raw sentiment data into an actionable and generalizable format across diverse mission scenarios. Furthermore, for practical applications, it is essential to establish a general mapping between sentiment cues and environmental context to ensure accurate robot operations.\n\nSemantic maps have been extensively studied as a means to integrate qualitative and spatial information [Kostavelis and Gasteratos (2015)]. Typically, these maps assign a class label and descriptive textual information to spatial objects, enabling real-time map generation and planning. In many applied settings, there are existing data formats analogous to semantic maps. For example, BIM is a widely used format that represents both the three-dimensional geometry and associated textual information of a built environment [Cerovsek (2011)]. Given its dual representation of spatial and semantic data, BIM serves as an excellent analog to a semantic map. Indeed, previous research has successfully leveraged BIM to provide the semantic information for robot planning [Park et al. (2025)]. Furthermore, the integration of internet of things (IoT) sensors, real-time data streams, and machine learning has driven an evolution from static BIM representations toward dynamic Digital Twins that continuously update to reflect current site conditions, enabling more responsive robotic navigation in changing construction environments.\n\nPrevious works typically treat the LLM as the primary planning module, leveraging environmental information to generate semantically meaningful plans [Wang et al. (2024)].\nIn contrast, we propose an alternative interpretation: using the LLM as a sensor rather than a planner. In this framework, the LLM provides semantic information that can be embedded into the map or any intermediate representation used during planning. The planning module itself remains agnostic to the LLM, allowing for the use of any classical or learning-based planner in conjunction with the LLM-as-sensor approach. Although visual–language maps have shown great promise [Huang et al. (2023)], most prior work focuses on robot planning from direct commands. In contrast, our contribution demonstrates that LLMs, combined with probabilistic map updates, can account for contextual and implicit sentiments that are not explicitly expressed in the natural-language command.\n\nUsing NL as a cost signal for obstacle avoidance has been explored previously [Oh et al. (2025)]. Prior work shows that VLMs can identify dynamic obstacles and assign danger scores. We extend this line of work by incorporating implicit and contextual information about the environment and by enabling the chaining and consolidation of multiple commands, yielding more flexible and general language-guided costmaps. Semantic planning has also been studied in the realm of visual-language-action models (VLA). However, these models are often expensive to train and deploy. Furthermore, these models often do not have formal verification and guarantees regarding their performance [Sapkota et al. (2025)]. By incorporating NL data as sensor information that can augment costs, we can integrate NL with classical and certifiable AI algorithms such as A* for path-finding, which is commonly used in robotics [Halder et al. (2024)] [Dai et al. (2024)]. In the presented framework, the ideas proposed in studies such as [Dai et al. (2024)] are extended to risk shaping rather than task satisfaction, with BIM‑grounded semantics.\nOur contributions in this paper can be summarized as:\n\nWe propose a novel interpretation of LLMs as semantic sensors for robotic navigation, where natural language prompts about construction site conditions update obstacle danger coefficients through Beta-Bernoulli fusion. This approach leverages BIM family semantics to ground language understanding while maintaining map structure invariance.\n\nWe derive a closed-form Beta–Bernoulli fusion of prompt-derived danger scores into continuous repulsive gains, thereby formulating object avoidance as a repulsive potential-field cost metric.\n\nWe demonstrate that off-the-shelf LLMs correctly interpret domain-specific construction prompts (no domain fine-tuning needed), and that our fusion preserves the bounded-suboptimal guarantees of Multi-Heuristic A*.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将自然语言指令与建筑信息模型（BIM）结合以增强机器人路径规划的灵活性和安全性？  \n2. 如何处理自然语言中的隐含情感信息，以便在机器人导航中实现更好的环境感知？  \n3. 如何利用大语言模型（LLM）作为传感器来更新障碍物的危险系数，从而改进路径规划的鲁棒性？  \n\n【用了什么创新方法】  \n本研究提出了一种新颖的框架，将自然语言指令与BIM派生的语义地图通过Beta-Bernoulli贝叶斯融合进行结合。具体而言，LLM被视为传感器，通过将每个障碍物的设计时间排斥系数视为Beta随机变量，并将LLM返回的“危险分数”作为伪计数来更新这些系数。该方法生成的后验均值提供了一个连续的、上下文感知的排斥增益，增强了基于欧几里得距离的潜在场成本启发式。通过根据用户提示推断的情感和上下文调整增益，机器人能够沿着更安全、上下文感知的路径导航。模拟结果表明，Beta-Bernoulli融合在路径鲁棒性和有效性方面都取得了定性和定量的改善。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework",
            "authors": "Reza Akhavian,Mani Amani,Johannes Mootz,Robert Ashe,Behrad Beheshti",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20705",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20705",
            "arxiv_html_link": "https://arxiv.org/html/2509.20705v1",
            "abstract": "未获取到摘要",
            "introduction": "The adoption of cyber-physical systems and jobsite intelligence that connects design models, real-time site sensing, and autonomous field operations can dramatically enhance digital management in the Architecture, Engineering, and Construction (AEC) industry. This paper introduces BIM2RDT (Building Information Models to Robot-Ready Site Digital Twins), an agentic artificial intelligence (AI) framework designed to transform static Building Information Modeling (BIM) into dynamic, robot-ready digital twins (DTs) that prioritize safety during construction execution. The framework bridges the gap between pre-existing BIM data and real-time site conditions by integrating three key data streams: geometric and semantic information from BIM models, real-time activity data from IoT sensor networks, and visual-spatial data collected by quadruped robots during site traversal. The methodology introduces Semantic-Gravity ICP (SG-ICP), a novel point cloud registration algorithm that leverages large language model (LLM) reasoning. Unlike traditional methods, SG-ICP utilizes an LLM to infer object-specific, physically-plausible orientation priors based on BIM semantics, significantly improving alignment accuracy by avoiding convergence on local minima. This creates an intelligent feedback loop where robot-collected data updates the DT, which in turn optimizes paths for subsequent missions. The framework employs YOLOE open-vocabulary object detection and Shi-Tomasi corner detection to identify and track construction elements while using BIM geometry as robust a priori maps. The framework also integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping sensor-detected safety events to the digital twin using IFC standards for proactive intervention. Major findings from experiments demonstrate SG-ICP’s superiority over standard ICP, achieving RMSE reductions of 64.3%–88.3% in alignment across varied scenarios with occluded or sparse features, ensuring physically plausible orientations. HAV integration triggers real-time warnings and tasks upon exceeding exposure limits, enhancing compliance with such standards as ISO 5349-1.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将静态建筑信息模型（BIM）转化为动态、机器人就绪的数字双胞胎（DT）？  \n2. 如何在建筑施工中优先考虑安全性并实现实时数据集成？  \n3. 如何提高点云注册的准确性，避免传统方法中的局部最小值收敛问题？  \n\n【用了什么创新方法】  \n本研究提出了BIM2RDT框架，通过将几何和语义信息、实时活动数据及机器人收集的视觉空间数据整合，构建机器人就绪的数字双胞胎。采用了新颖的语义引力ICP（SG-ICP）算法，利用大语言模型推断物体特定的物理可行方向，从而显著提高了对齐精度。框架还集成了YOLOE开放词汇物体检测和Shi-Tomasi角点检测，以识别和跟踪施工元素，并使用BIM几何作为稳健的先验地图。此外，实时手臂振动监测与数字双胞胎相结合，能够在超出暴露限制时触发实时警告和任务，增强了安全合规性。实验结果表明，SG-ICP在多种场景下的对齐精度显著优于标准ICP，RMSE减少了64.3%-88.3%。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Joint Flow Trajectory Optimization For Feasible Robot Motion Generation from Video Demonstrations",
            "authors": "Xiaoxiang Dong,Matthew Johnson-Roberson,Weiming Zhi",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20703",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20703",
            "arxiv_html_link": "https://arxiv.org/html/2509.20703v1",
            "abstract": "Learning from human video demonstrations offers a scalable alternative to teleoperation or kinesthetic teaching, but poses challenges for robot manipulators due to embodiment differences and joint feasibility constraints. We address this problem by proposing the Joint Flow Trajectory Optimization (JFTO) framework for grasp pose generation and object trajectory imitation under the video-based Learning-from-Demonstration (LfD) paradigm. Rather than directly imitating human hand motions, our method treats demonstrations as object-centric guides, balancing three objectives: (i) selecting a feasible grasp pose, (ii) generating object trajectories consistent with demonstrated motions, and (iii) ensuring collision-free execution within robot kinematics. To capture the multimodal nature of demonstrations, we extend flow matching to 𝐒𝐄​(3)\\mathbf{SE}(3) for probabilistic modeling of object trajectories, enabling density-aware imitation that avoids mode collapse. The resulting optimization integrates grasp similarity, trajectory likelihood, and collision penalties into a unified differentiable objective. We validate our approach in both simulation and real-world experiments across diverse real-world manipulation tasks.",
            "introduction": "Generating feasible motions is a key challenge in robotics. Traditional approaches address this by formulating and solving optimization problems with carefully designed costs and constraints [1, 2, 3]. An alternative line of work focuses on enabling robots to imitate humans through Learning from Demonstration (LfD) [4, 5]. Historically, this has required collecting demonstrations through teleoperation or precise kinesthetic teaching. More recently, researchers have attempted to leverage videos of human interaction with objects as demonstrations [6, 7]. Although promising, this paradigm introduces challenges: Human arms and robot manipulators differ significantly in morphology, and direct imitation can cause robots to violate joint constraints when tracking human motions. All demonstrations in this paper refer to such human video demonstrations.\n\nIn this paper, we tackle the problem of generating grasp poses and motion trajectories for robot manipulators under the video-based LfD paradigm, where the tracked human motion may often not be kinesthetically feasible for a robot manipulator to execute. Our goal is to identify feasible grasp configurations and ensure that grasped objects move consistently with human video demonstrations. Concretely, given videos of humans interacting with objects, we propose the Joint Flow Trajectory Optimization (JFTO) framework, as shown in Fig.˜1, with three desiderata: (1) identifying a feasible grasp pose; (2) generating trajectories of the grasped object that align with object poses in the videos; (3) ensuring downstream robot motions remain collision-free and within joint limits. To this end, our formulation treats human video demonstrations as object-centric guides rather than strict motion references. By focusing on how objects are manipulated, rather than how the human hand is configured, we allow robots to synthesize grasps and trajectories that are consistent with observed outcomes, while respecting the robot’s own embodiment.\n\nUncertainty is critical to robotics tasks [8, 9, 10]. We take a probabilistic approach and model the distribution of object pose trajectories extracted from videos as a generative model Flow Matching [11] over the Special Euclidean Group 𝐒𝐄​(3)\\mathbf{SE}(3).\nHere, as the grasps and motions of the human from the video-based demonstrations may not be valid for robot execution, our goal is not only to ensure grasp and execution feasibility, but also to mimic the motion patterns of human demonstrations in the videos.\n\nConcretely, the technical contributions of this work are threefold:\n\nWe introduce the Joint Flow Trajectory Optimization (JFTO) framework that integrates grasp similarity, object trajectory likelihood, and collision avoidance into a unified differentiable objective for video-based LfD.\n\nWe extend flow matching to 𝐒𝐄​(3)\\mathbf{SE}(3) for probabilistic modeling of demonstrated object trajectories, enabling density-aware imitation that preserves multi-modality.\n\nWe demonstrate the effectiveness of JFTO in both simulation and real-world experiments on diverse manipulation tasks, showing that joint optimization achieves higher fidelity to demonstrations compared to sequential baselines.\n\n1. We introduce the Joint Flow Trajectory Optimization (JFTO) framework that integrates grasp similarity, object trajectory likelihood, and collision avoidance into a unified differentiable objective for video-based LfD.\n\n2. We extend flow matching to 𝐒𝐄​(3)\\mathbf{SE}(3) for probabilistic modeling of demonstrated object trajectories, enabling density-aware imitation that preserves multi-modality.\n\n3. We demonstrate the effectiveness of JFTO in both simulation and real-world experiments on diverse manipulation tasks, showing that joint optimization achieves higher fidelity to demonstrations compared to sequential baselines.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何从人类视频演示中生成机器人可行的抓取姿势和运动轨迹。  \n2. 如何在视频基础的学习中平衡抓取相似性、物体轨迹一致性和碰撞避免。  \n3. 如何处理人类与机器人在形态上的差异以避免关节约束的违反。  \n\n【用了什么创新方法】  \n提出了联合流轨迹优化（JFTO）框架，将抓取相似性、物体轨迹可能性和碰撞避免整合为一个统一的可微分目标。通过将流匹配扩展到SE(3)，实现了对演示物体轨迹的概率建模，从而支持密度感知的模仿，避免模式崩溃。该方法在模拟和真实世界实验中验证了其有效性，显示出与传统顺序基线相比，联合优化在演示的保真度上具有显著提升。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "RuN: Residual Policy for Natural Humanoid Locomotion",
            "authors": "Qingpeng Li,Chengrui Zhu,Yanming Wu,Xin Yuan,Zhen Zhang,Jian Yang,Yong Liu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20696",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20696",
            "arxiv_html_link": "https://arxiv.org/html/2509.20696v1",
            "abstract": "Enabling humanoid robots to achieve natural and dynamic locomotion across a wide range of speeds, including smooth transitions from walking to running, presents a significant challenge. Existing deep reinforcement learning methods typically require the policy to directly track a reference motion, forcing a single policy to simultaneously learn motion imitation, velocity tracking, and stability maintenance. To address this, we introduce RuN, a novel decoupled residual learning framework. RuN decomposes the control task by pairing a pre-trained Conditional Motion Generator, which provides a kinematically natural motion prior, with a reinforcement learning policy that learns a lightweight residual correction to handle dynamical interactions. Experiments in simulation and reality on the Unitree G1 humanoid robot demonstrate that RuN achieves stable, natural gaits and smooth walk–run transitions across a broad velocity range (0–2.5 m/s), outperforming state-of-the-art methods in both training efficiency and final performance.",
            "introduction": "Humanoid robots are expected to operate in human-centric environments across a broad spectrum of tasks. Among these capabilities, reliable locomotion is the most fundamental and central prerequisite. However, developing a unified locomotion controller that spans a wide range of speeds, from walking to running, and enables smooth transitions between these gaits remains a pivotal challenge.\n\nClassical control methods, such as those based on Zero-Moment Point (ZMP), can ensure stability but often produce rigid, robotic gaits ill-suited for high-speed dynamic tasks [1]. In parallel, Model Predictive Control (MPC) optimizes short-horizon plans under simplified dynamics and preset contact schedules due to real-time and modeling limits, which can restrict agility and robustness on humanoids [2]. In response, Deep Reinforcement Learning (DRL) has emerged as a powerful paradigm for learning complex motor skills end-to-end [3, 4]. However, the effectiveness of DRL depends on reward design, and even small misspecification can steer learning toward easy-to-exploit objectives, yielding efficient yet non-anthropomorphic motions rather than structured, human-like behavior.\n\nTo promote naturalness in DRL, researchers incorporate data-driven motion priors. Paradigms such as the Generative Motion Prior (GMP) [5] employ generative models to synthesize natural reference motions as priors, feed these priors directly into a reinforcement learning policy, and train the policy with imitation and task rewards to produce natural movements. However, this direct-tracking strategy creates a significant learning-complexity challenge in which a single policy must simultaneously satisfy three often competing objectives—imitating the kinematic style of the motion prior, maintaining dynamic stability, and executing task commands. This tight coupling exposes the policy to a large and complex action space, lengthening training time. Additionally, inherent conflicts among the objectives constrain performance and necessitate trade-offs among motion quality, stability, and task accuracy.\n\nTo address the inherent complexities in DRL, we propose RuN, a novel decoupled residual learning framework. The core idea of this framework is to decompose the complex end-to-end control task into two more tractable sub-modules. The first module, a Conditional Motion Generator (CMG), autoregressively generates high-quality kinematic reference trajectories from given velocity commands, which serve as a motion prior. The second module is a residual policy that learns a significantly simplified task, which is providing small and dynamic corrections to the motion prior to compensate for external perturbations. The final control command executed by the robot is the summation of the motion priors generated by the CMG and the residual action output by the policy. This residual formulation drastically narrows the policy’s exploration space, allowing it to focus its learning capacity on mastering complex dynamic interactions, thereby enabling our controller to achieve smooth transitions from walking to running.\n\nWe conduct extensive experiments on the Unitree G1 humanoid robot. The results demonstrate that our framework trains significantly faster and achieves superior final performance compared to state-of-the-art methods across all key metrics, including motion naturalness and velocity tracking error. Our robot successfully performs natural walking and running across a continuous velocity range of 0–2.5 m/s, with smooth gait transitions in the real world.\n\nOur contributions can be summarized as follows:\n\nA novel decoupled residual learning framework, RuN, that simplifies humanoid locomotion by decomposing the action into a motion prior and a learned residual, leading to significant gains in final performance.\n\nA novel decoupled residual learning framework, RuN, that simplifies humanoid locomotion by decomposing the action into a motion prior and a learned residual, leading to significant gains in final performance.\n\nA novel Conditional Motion Generator that autoregressively produces a range of natural motions from walking to running, serving as a high-quality motion prior for the residual policy.\n\nIn extensive real-world and simulation trials on the Unitree G1 humanoid robot, our controller, RuN, demonstrates superior performance over state-of-the-art methods regarding motion naturalness and velocity tracking.\n\n1. A novel decoupled residual learning framework, RuN, that simplifies humanoid locomotion by decomposing the action into a motion prior and a learned residual, leading to significant gains in final performance.\n\n2. A novel Conditional Motion Generator that autoregressively produces a range of natural motions from walking to running, serving as a high-quality motion prior for the residual policy.\n\n3. In extensive real-world and simulation trials on the Unitree G1 humanoid robot, our controller, RuN, demonstrates superior performance over state-of-the-art methods regarding motion naturalness and velocity tracking.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现自然且动态的人形机器人在多种速度下的运动，包括平滑的行走与奔跑过渡。  \n2. 现有深度强化学习方法在运动模仿、速度跟踪和稳定性维护之间的复杂性挑战。  \n3. 如何简化复杂的控制任务以提高训练效率和最终性能。  \n\n【用了什么创新方法】  \n提出了一种新颖的解耦残差学习框架RuN，通过将控制任务分解为两个模块：条件运动生成器（CMG）和残差策略。CMG生成高质量的运动先验，残差策略则学习动态修正以应对外部扰动。该方法显著缩小了策略的探索空间，使其能够专注于复杂动态交互的学习，从而实现平滑的行走与奔跑过渡。实验结果表明，RuN在训练效率和最终性能上均优于现有方法，成功实现了0-2.5 m/s速度范围内的自然行走和奔跑。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Incorporating Human-Inspired Ankle Characteristics in a Forced-Oscillation-Based Reduced-Order Model for Walking",
            "authors": "Chathura Semasinghe,Siavash Rezazadeh",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20689",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20689",
            "arxiv_html_link": "https://arxiv.org/html/2509.20689v1",
            "abstract": "This paper extends the forced-oscillation-based reduced-order model of walking to a model with ankles and feet. A human-inspired paradigm was designed for the ankle dynamics, which results in improved gait characteristics compared to the point-foot model. In addition, it was shown that while the proposed model can stabilize against large errors in initial conditions through combination of foot placement and ankle strategies, the model is able to stabilize against small perturbations without relying on the foot placement control and solely through the designed proprioceptive ankle scheme. This novel property, which is also observed in humans, can help in better understanding of anthropomorphic walking and its stabilization mechanisms.",
            "introduction": "Reduced-order models of legged systems aim to capture the fundamental traits of locomotion within a lower-dimensional framework. Locomotion control, gait generation, and stability remain key challenges in both biomechanical and robotic locomotion, and the complexity and nonlinear dynamics of these systems have motivated the use of simplified models. One of the oldest and most essential models of bipedal walking is the Inverted Pendulum Model (IPM), which has long been used for control and stability analyses of bipedal locomotion [1]. Building on this and by incorporating the Zero Moment Point (ZMP) concept proposed by Vukobratović and Juričić [2], Kajita et al. [3, 4] extended the IPM to the Linear Inverted Pendulum Model (LIPM), a simplified model widely used for humanoid gait generation and control. The LIPM is among the most prevalent templates for bipedal walking, as it enables real-time control through fast, convex trajectory generation of the point mass, which is also the center of mass (CoM) [4, 5]. However, the assumption of constant leg length constrains vertical CoM motion, allowing only horizontal dynamics. With this restriction, the vertical ground reaction force (GRF) remains effectively constant and equal to the weight, which prevents the reproduction of the human-like M-shaped vertical GRF profiles [6, 7]. These limitations raise concerns about the ability of the LIPM template to capture essential characteristics of anthropomorphic walking. Although the original IPM produces attributes more closely resembling human walking, notable differences remain, particularly in GRFs and CoM displacement [8]. More recently, by adding a force actuator to the IPM and applying trajectory optimization methods, these discrepancies have been addressed [9]; however, this extended model is primarily valuable for understanding biomechanical costs, while its application to robotic systems remains an open question.\n\nAnother widely adopted reduced-order model is the Spring-Loaded Inverted Pendulum (SLIP) model, which represents the system as a single point mass supported by one or two massless elastic legs. Originally developed for running, it was later extended to capture key features of human walking [10, 8, 11]. Unlike IPM and LIPM, the energy storage and release mechanism provided by its compliance enables this model to reproduce human-like gait characteristics, including the double-hump (M-shaped) GRF profile and realistic CoM trajectories [8, 12].\n\nOver the years, several variations of the SLIP model have been proposed to improve walking performance and provide closer approximations of human locomotion characteristics. The Variable SLIP (V-SLIP) model [13, 14] incorporates active modulation of leg stiffness to achieve a cost of transport comparable to that of human walking, thereby enabling more energy-efficient and robust bipedal locomotion. To account for upper-body dynamics, Sharbafi and Seyfarth introduced the Bipedal Trunk-SLIP (BTSLIP) model [15, 16], which redirects GRFs to stabilize trunk posture while preserving compliant leg mechanics, resulting in more realistic GRF profiles and CoM dynamics. Pelit et al.[17] extended the framework to the SLIP with Swing Legs (SLIP-SL) model, which incorporates passive swing-leg dynamics without requiring separate swing-leg trajectory planning. This extension enhances stability and cyclic locomotion by producing human-like gait timing and step lengths. Geyer et al. [8] proposed a simplified Bipedal SLIP (BSLIP) model capable of generating stable walking using different combinations of overall leg stiffness and attack angles. Hao et al. [18] further advanced the BSLIP framework by introducing a damper in parallel with the spring on each massless leg, resulting in the D-SLIP model, which allows evaluation of the effects of hip actuator torque on the energetics and stability of bipedal walking.\n\nHowever, the SLIP model and its variations, while useful in capturing human locomotion traits, are highly simplified and underactuated. Hence their deployment to robots remains challenging, often because of underactuation and merely marginal stability in the template level [19, 20]. Notwithstanding, the forced-oscillation-based variation of SLIP (Fig. 1) is a template that has resulted in highly stable gaits on bipedal robots [21, 22]. In this model, a damper and a leg length actuator for setting the spring’s neutral length are added to the bipedal SLIP. The stability of this model and its wide basin of attraction have been proved and its human-like characteristics have been presented in [22]. As such, adopted this paradigm as the foundation for an extension aimed at incorporating more anthropomorphic traits.\n\nA significant shortcoming in the aforementioned point-foot models is that they neglect the kinematics and dynamics of the foot and ankle, which play essential roles in human walking. In particular, ankle flexion contributes a substantial amount of positive mechanical work for forward progression and provides vertical support to sustain body weight [23, 24]. Therefore, incorporating an ankle joint and a finite-sized foot into an extended reduced-order model can potentially enhance stability and robustness in bipedal walking. In addition, the ankle push-off in such models can produce more realistic GRF profiles, CoM dynamics closer to that of humans, human-like forward center-of-pressure (CoP) travel, and ankle behavior, which are features that point-foot templates fail to capture [25, 26]. An example of such an extension is proposed in [25], where the V-SLIP model was equipped with a 1-DoF ankle joint and a finite-sized foot (VSLIP-FF) for each leg, enabling compliant bipedal walking in complex environments. Their approach employed a finite state machine–based gait planner combined with a differential evolution (DE) optimizer to generate periodic gaits. The gait planning method incorporated an adaptive leg stretching and contracting strategy inspired by human walking, with pre-determined footholds used to update step length for placing the foot on known targets. This strategy was controlled by a PD controller and its gains were selected through offline tuning/optimization to realize a periodic gait. Although this method can generate stable gaits, its reliance on offline tuning and optimization limits its flexibility. This challenges the online adaptability and robustness of bipedal walking, particularly for achieving anthropomorphic stance mechanics, real-time foot placement, and step regulation in response to speed variations or disturbances. More importantly, the stability in this framework does not inlcude the distinct separation between ankle and foot placement strategies [27]. In general, many SLIP model variations ultimately produce gaits dictated by the natural dynamics of the system, without explicitly comparing to anthropomorphic walking gaits [20].\n\nIn this paper, we propose an extended version of the forced-oscillation-based reduced-order model for human-like walking, where an ankle and a foot are added. A human-inspired scheme is designed for the ankle characteristics and all the tunable parameters are selected using human biomechanical information. To evaluate the performance of the proposed model, we compare its results against both human data and the point-foot forced-oscillation-based model and show how the ankle stabilization can work without foot placement regulation, similar to humans.\n\nThe rest of this paper is organized as follows. Section II describes the proposed model and its designed characteristics. Section III details the model parameters and the simulation process. Section IV presents the simulation results including comparisons with the point-foot model and the human data. These results and comparisons are discussed in Section V. Finally, Section VI concludes the paper and outlines directions for future works.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在简化模型中有效地整合人类步态的踝关节特性？  \n2. 如何提高步态模型在面对初始条件误差和小扰动时的稳定性？  \n3. 如何通过改进的模型更好地模拟人类步态的地面反作用力和重心动态？  \n\n【用了什么创新方法】  \n本研究提出了一种扩展的强迫振荡基础的简化模型，通过引入人类灵感的踝关节特性，增强了步态模型的稳定性和人类步态的相似性。模型通过结合脚的放置和踝关节策略，实现了在大误差和小扰动下的稳定性。与传统的点足模型相比，新模型在地面反作用力和重心动态上表现出更接近人类的特征。通过与人类数据的比较，验证了该模型在无脚放置调节情况下的踝关节稳定性，展示了其在理解人类步态和稳定机制方面的潜力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "RAM-NAS: Resource-aware Multiobjective Neural Architecture Search Method for Robot Vision Tasks",
            "authors": "Shouren Mao,Minghao Qin,Wei Dong,Huajian Liu,Yongzhuo Gao",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "Joint first authors: Shouren Mao and Minghao Qin. Published in IEEE/RSJ IROS 2024. This arXiv version adds a joint first-authorship note to correct an omission in the IEEE Xplore version. No technical changes. Please cite the IEEE version",
            "pdf_link": "https://arxiv.org/pdf/2509.20688",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20688",
            "arxiv_html_link": "https://arxiv.org/html/2509.20688v1",
            "abstract": "Neural architecture search (NAS) has shown great promise in automatically designing lightweight models. However, conventional approaches are insufficient in training the supernet and pay little attention to actual robot hardware resources. To meet such challenges, we propose RAM-NAS, a resource-aware multi-objective NAS method that focuses on improving the supernet pretrain and resource-awareness on robot hardware devices. We introduce the concept of subnets mutual distillation, which refers to mutually distilling all subnets sampled by the sandwich rule. Additionally, we utilize the Decoupled Knowledge Distillation (DKD) loss to enhance logits distillation performance. To expedite the search process with consideration for hardware resources, we used data from three types of robotic edge hardware to train Latency Surrogate predictors. These predictors facilitated the estimation of hardware inference latency during the search phase, enabling a unified multi-objective evolutionary search to balance model accuracy and latency trade-offs. Our discovered model family, RAM-NAS models, can achieve top-1 accuracy ranging from 76.7%76.7\\% to 81.4%81.4\\% on ImageNet. In addition, the resource-aware multi-objective NAS we employ significantly reduces the model’s inference latency on edge hardware for robots. We conducted experiments on downstream tasks to verify the scalability of our methods. The inference time for detection and segmentation is reduced on all three hardware types compared to MobileNetv3-based methods. Our work fills the gap in NAS for robot hardware resource-aware.",
            "introduction": "The application of deep learning techniques in robot environmental perception is a current focal point of research. However, their demanding resource requirement in terms of processing time challenges their sustained operation on edge devices embedded within robots[1]. Nevertheless, specific visual perception tasks are driven by deep learning, along with related tasks like object detection[2] and semantic SLAM[3]. Consequently, developing lightweight deep learning models becomes crucial for deploying them on edge computing devices embedded within robots. Model lightweighting strives to reduce model parameters while maintaining performance without significant degradation, thus enhancing computational efficiency. Research[4] suggests that numerous parameters within existing deep neural network models are unnecessary, indicating the potential for model lightweighting. This is particularly crucial for deploying deep learning models on robot platforms. NAS by limiting model complexity, uses reinforcement learning[5] or evolutionary algorithms[6] to automatically pick and adjust key parameters like network layers, width, and depth based on task performance, aiming for the lightweighting model design. NAS stands out for its automation and innovation, reducing the need for manual design efforts by automatically finding the optimal neural network structure.\n\nRecently, one-shot NAS [7] became popular due to low computation overhead and competitive performance. It does not require training thousands of individual models from scratch but only trains a single supernet that contains any architecture in the search space, weights are shared among subnets respectively. However, when a supernet shares weights across subnets, it might not guarantee adequate training for each specific subnet. This inconsistency in ranking the architecture could diminish the efficacy of the search process[8]. Retrain-free NAS methods[8, 9] propose simultaneously training multiple networks, slicing different-sized subnets directly from the supernet and synchronously training them during the supernet’s training process.\nIn this paper, we present subnets mutual distillation and use DKD loss to enable the knowledge better transfer between subnets, without requiring an external teacher model. Through these strategies, the consistency in ranking individual subnets.\n\nOn the other hand, previous NAS[10, 11] mainly abstracts the search problem into a single objective optimization problem under given resource constraints, it demands multiple executions, consuming considerable computing resources and time in the pursuit of a suitable model. Compared to single objective optimization, multiobjective optimization not only allows for simultaneous optimization of multiple given objectives but also yields a Pareto-optimal frontier containing multiple optimal solutions[12]. These solutions exhibit different performances across multiple optimization objectives, thereby facilitating more convenient selection and deployment of different robot computational hardware configurations.\nAdditionally, these methods[13, 14], predominantly consider the computing resources of mobile phones, and few take into account the more frequently used edge hardware of robots. However, as the NAS operating on the server side lacks direct latency aware of hardware, inspired by[12, 15], we propose surrogate predictors to assess the architecture’s latency. In this paper, we conduct a neural architecture search to be aware of the resources of actual robot edge hardware devices with surrogate-assisted.\n\nIn summary, to address the limitation of existing NAS methods, which often overlook robot edge device resources and necessitate multiple executions to identify models with varied computing power requirements, we introduce RAM-NAS.\nThe key contributions are summarized below:\n\nWe propose RAM-NAS, a Resource-aware Multiobjective Neural Architecture Search method. This method allows for a single search to compute the Pareto frontier under various constraints, decision makers can deploy models under different resource constraints.\n\nWe adopt a method where multiple subnets mutually distill each other, enabling joint parameter updates across these subnets. It uses DKD Loss to compute the loss function, ensuring more effective convergence of supernet and boosting the consistency in ranking individual subnets.\n\nWe adopt reusable surrogate predictors to predict hardware latency,\nthe predictor demonstrates high sample efficiency and exhibits strong consistency in predicting latency with actual performance.\n\n1. We propose RAM-NAS, a Resource-aware Multiobjective Neural Architecture Search method. This method allows for a single search to compute the Pareto frontier under various constraints, decision makers can deploy models under different resource constraints.\n\n2. We adopt a method where multiple subnets mutually distill each other, enabling joint parameter updates across these subnets. It uses DKD Loss to compute the loss function, ensuring more effective convergence of supernet and boosting the consistency in ranking individual subnets.\n\n3. We adopt reusable surrogate predictors to predict hardware latency,\nthe predictor demonstrates high sample efficiency and exhibits strong consistency in predicting latency with actual performance.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有神经架构搜索（NAS）方法忽视了机器人边缘设备的资源限制。  \n2. 传统NAS方法需要多次执行，耗费大量计算资源和时间。  \n3. 如何在保证模型性能的同时，实现轻量化和资源感知的深度学习模型。  \n\n【用了什么创新方法】  \n提出了RAM-NAS，一种资源感知的多目标神经架构搜索方法，通过引入子网互蒸馏和使用解耦知识蒸馏损失（DKD Loss）来提升子网间的知识转移。同时，采用可重用的延迟预测器来评估硬件延迟，从而在搜索过程中实现模型准确性与延迟的平衡。实验结果表明，RAM-NAS模型在ImageNet上实现了76.7%到81.4%的顶级准确率，并显著降低了在边缘硬件上的推理延迟，验证了方法的可扩展性。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Efficient Construction of Implicit Surface Models From a Single Image for Motion Generation",
            "authors": "Wei-Teng Chu,Tianyi Zhang,Matthew Johnson-Roberson,Weiming Zhi",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20681",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20681",
            "arxiv_html_link": "https://arxiv.org/html/2509.20681v1",
            "abstract": "Implicit representations have been widely applied in robotics for obstacle avoidance and path planning. In this paper, we explore the problem of constructing an implicit distance representation from a single image. Past methods for implicit surface reconstruction, such as NeuS and its variants generally require a large set of multi-view images as input, and require long training times. In this work, we propose Fast Image-to-Neural Surface (FINS), a lightweight framework that can reconstruct high-fidelity surfaces and SDF fields based on a single or a small set of images.\nFINS integrates a multi-resolution hash grid encoder with lightweight geometry and color heads, making the training via an approximate second-order optimizer highly efficient and capable of converging within a few seconds. Additionally, we achieve the construction of a neural surface requiring only a single RGB image, by leveraging pre-trained foundation models to estimate the geometry inherent in the image. Our experiments demonstrate that under the same conditions, our method outperforms state-of-the-art baselines in both convergence speed and accuracy on surface reconstruction and SDF field estimation. Moreover, we demonstrate the applicability of FINS for robot surface following tasks and show its scalability to a variety of benchmark datasets.",
            "introduction": "For autonomous robots to navigate and interact safely with the real world, they must form reliable geometric representations of their surroundings. Distance-based representations are a powerful representation widely used in motion planning and obstacle avoidance [1, 2, 3, 4, 5, 6, 7, 8]. Accurate and efficient SDF estimation is therefore a key enabler of downstream decision-making and control.\n\nRecent neural implicit surface methods, such as NeuS [9] and its successors [10, 11, 12, 13, 14], have demonstrated impressive capability in reconstructing fine-grained object surfaces. However, these approaches suffer from two key drawbacks: (i) they rely on dense multi-view supervision, which is impractical in robotics settings where only sparse observations are available; and (ii) they require long training times, from minutes to hours, making them unsuitable for real-time use in navigation or manipulation. A complementary line of work [15, 16, 17, 18] has sought to improve generalization to sparse views, reducing the dependency on extensive image collections. However, these approaches can often still require a sizeable number of images, and can be relatively inefficient to train from scratch.\n\nIn this paper, we introduce Fast Image-to-Neural Surface (FINS), a lightweight framework that overcomes these limitations. FINS reconstructs high-fidelity surfaces and SDF fields from as few as a single image, or a small set of images, within seconds. Our framework integrates three components: (1) off-the-shelf 3D foundation models, such as DUSt3R [19] and VGGT [20], to lift single-view inputs into point clouds for SDF supervision; (2) a multi-resolution hash grid encoder [21] to enable efficient feature encoding; and (3) lightweight geometry and color heads trained with an approximate second-order optimizer, yielding rapid convergence.\n\nBy leveraging strong priors from pre-trained 3D models, FINS scales naturally from single objects to multi-view, scene-level settings. This flexibility supports deployment on mobile platforms, where continuous observations can be assimilated into an evolving SDF representation. As a result, FINS enables real-time reconstruction and refinement of neural surfaces for downstream robotics tasks such as obstacle avoidance, path planning, and surface following. We empirically evaluate the quality and efficiency of building implicit distance reconstructions of a diverse range of objects, and demonstrate the applicability of these representations for robot surface following [22]. Concretely, this paper presents the following technical contributions:\n\nWe propose FINS, an end-to-end method that achieves high-precision SDF training from a single image in only a few seconds.\n\nWe leverage pre-trained 3D foundation models to generate point clouds for SDF supervision, enabling efficient and complete reconstruction with limited visual input.\n\nWe adopt multi-resolution hash encoding and lightweight geometry/color heads with a mixed optimization strategy to eliminate heavy optimization and enable real-time convergence.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何从单张图像构建隐式距离表示以支持机器人运动生成。  \n2. 现有方法依赖于多视角图像，训练时间长，难以实时应用。  \n3. 如何提高稀疏观察条件下的表面重建精度与效率。  \n\n【用了什么创新方法】  \n提出了Fast Image-to-Neural Surface (FINS)框架，集成了预训练的3D基础模型、一个多分辨率哈希网格编码器和轻量几何/颜色头，能够在几秒内从单张图像重建高保真表面和SDF字段。通过利用强先验，FINS实现了实时的隐式表面重建与优化，适用于机器人避障、路径规划和表面跟随等任务。实验表明，FINS在收敛速度和重建精度上优于现有方法，展示了其在多种基准数据集上的可扩展性。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Equi-RO: A 4D mmWave Radar Odometry via Equivariant Networks",
            "authors": "Zeyu Han,Shuocheng Yang,Minghan Zhu,Fang Zhang,Shaobing Xu,Maani Ghaffari,Jianqiang Wang",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20674",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20674",
            "arxiv_html_link": "https://arxiv.org/html/2509.20674v1",
            "abstract": "Autonomous vehicles and robots rely on accurate odometry estimation in GPS-denied environments. While LiDARs and cameras struggle under extreme weather, 4D mmWave radar emerges as a robust alternative with all-weather operability and velocity measurement. In this paper, we introduce Equi-RO, an equivariant network-based framework for 4D radar odometry. Our algorithm pre-processes Doppler velocity into invariant node and edge features in the graph, and employs separate networks for equivariant and invariant feature processing. A graph-based architecture enhances feature aggregation in sparse radar data, improving inter-frame correspondence. Experiments on the open-source dataset and self-collected dataset show Equi-RO outperforms state-of-the-art algorithms in accuracy and robustness. Overall, our method achieves 10.7% and 20.0% relative improvements in translation and rotation accuracy, respectively, compared to the best baseline on the open-source dataset.",
            "introduction": "Odometry is indispensable for autonomous vehicles and robots, providing accurate support for downstream perception, planning, and control. When Global Positioning System (GPS) signals are degraded, such as under bridges, in tunnels, or during severe rain or snow, it becomes crucial to obtain relative localization using sensors such as LiDARs and cameras.\n\nHowever, the performance of LiDARs and cameras degrades significantly under extreme weather conditions, limiting their applicability for odometry estimation in such scenarios. Recently, 4D millimeter-wave (mmWave) radar—characterized by compact size, cost efficiency, all-weather adaptability, velocity-measuring capability, and long detection range—has been increasingly regarded as a promising solution for robust perception and localization in autonomous driving, particularly under adverse weather conditions [1].\n\nCurrent 4D mmWave radar odometry algorithms are mostly adapted from traditional LiDAR odometry methods. Some exploit unique properties of 4D mmWave radar point clouds like Doppler velocity and Radar Cross Section (RCS) to perform ego-velocity estimation and point cloud registration [2, 3]. Regarding learning-based approaches, a few pioneering works have adapted Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) for stand-alone 4D mmWave radar [4] and radar-camera fusion [5] odometry.\n\nIn learning-based odometry, accurate feature extraction and matching are critical steps. However, features in adjacent frames may become highly inconsistent when the ego-vehicle moves rapidly or turns sharply. Equivariant neural networks have emerged as a promising approach to address this issue [7, 8]. They learn features that transform predictably under specific geometric transformations, such as 3D translations and rotations represented by rigid-body motion group SE(3), thereby enhancing feature matching performance. The geometric symmetry can improve the generalizability and interpretability of learning-based odometry systems.\n\nThe 4D mmWave radar provides additional physical attributes compared to LiDAR point clouds. For example, it measures the RCS of each point, which can be regarded as SE(3)-invariant since it reflects the strength of the returned signal and is determined mainly by the target’s size, shape, and material rather than the sensor’s pose. In addition, the Doppler effect enables estimation of the radial relative velocity of detected targets. However, Doppler velocity is neither equivariant nor invariant under SE(3) transformations, as it depends on both the sensor’s motion and the target’s motion, which limits the applicability of existing equivariant networks to 4D mmWave radar odometry. Moreover, 4D mmWave radar point clouds are inherently noisier, sparser, and more irregular than those captured by scanning LiDARs. Consequently, effectively aggregating point-wise features with those of their spatial neighbors remains a significant challenge for learning-based 4D mmWave radar odometry.\n\nIn this paper, we propose Equi-RO, a novel framework for 4D mmWave radar odometry built upon equivariant neural networks. Compared with existing 4D mmWave radar odometry algorithms, we design a pre-processing pipeline to derive invariant Doppler features and construct a graph representation to mitigate noise and sparsity. The equivariant and invariant features are then processed through separate networks to estimate the relative transformation. The invariant velocity features and equivariant network design enable our method to remain robust in challenging real-world driving scenarios, where existing methods often degrade. Leveraging both a public dataset [6] and a self-collected dataset, we demonstrate consistent improvements over state-of-the-art 4D mmWave radar odometry approaches.\n\nIn summary, our main contributions are as follows:\n\nWe develop a novel 4D mmWave radar odometry algorithm based on equivariant neural networks, capable of producing accurate and robust odometry estimates under large rotations and fast motions, as commonly encountered in sharp turns and challenging driving maneuvers.\n\nWe develop a novel 4D mmWave radar odometry algorithm based on equivariant neural networks, capable of producing accurate and robust odometry estimates under large rotations and fast motions, as commonly encountered in sharp turns and challenging driving maneuvers.\n\nWe design a dedicated pre-processing pipeline for Doppler velocity that derives invariant features, thereby stabilizing motion-related features. And we design a framework that separately extracts equivariant and invariant features to preserve geometric consistency, making it applicable to 4D mmWave radar point clouds with physical attributes.\n\nWe conduct extensive experiments on both an open-source dataset and a self-collected dataset, along with ablation studies, to validate the effectiveness of the proposed method across diverse scenarios.\n\nThe source code will be released after receiving the final decision.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在GPS信号缺失的环境中实现准确的4D mmWave雷达里程计估计。  \n2. 现有的4D mmWave雷达里程计算法在极端天气条件下的性能不足。  \n3. 如何有效处理4D mmWave雷达点云的噪声和稀疏性问题。  \n\n【用了什么创新方法】  \n本研究提出了Equi-RO，一个基于等变网络的4D mmWave雷达里程计框架。该方法通过预处理多普勒速度来提取不变节点和边特征，并采用图形架构增强稀疏雷达数据的特征聚合。通过分别处理等变和不变特征，Equi-RO在快速运动和大旋转情况下保持了鲁棒性。实验结果显示，与现有方法相比，Equi-RO在翻译和旋转精度上分别提高了10.7%和20.0%。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation",
            "authors": "Junzhe Wang,Jiarui Xie,Pengfei Hao,Zheng Li,Yi Cai",
            "subjects": "Robotics (cs.RO)",
            "comment": "submitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.20656",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20656",
            "arxiv_html_link": "https://arxiv.org/html/2509.20656v1",
            "abstract": "Reliable brain–computer interface (BCI) control of robots provides an intuitive and accessible means of human–robot interaction, particularly valuable for individuals with motor impairments. However, existing BCI–Robot systems face major limitations: electroencephalography (EEG) signals are noisy and unstable, target selection is often predefined and inflexible, and most studies remain restricted to simulation without closed-loop validation. These issues hinder real-world deployment in assistive scenarios. To address them, we propose a closed-loop BCI–AR–Robot system that integrates motor imagery (MI)-based EEG decoding, augmented reality (AR) neurofeedback, and robotic grasping for zero-touch operation. A 14-channel EEG headset enabled individualized MI calibration, a smartphone-based AR interface supported multi-target navigation with direction-congruent feedback to enhance stability, and the robotic arm combined decision outputs with vision-based pose estimation for autonomous grasping. Experiments are conducted to validate the framework: MI training achieved 93.1% accuracy with an average information transfer rate (ITR) of 14.8 bit/min; AR neurofeedback significantly improved sustained control (SCI = 0.210) and achieved the highest ITR (21.3 bit/min) compared with static, sham, and no-AR baselines; and closed-loop grasping achieved a 97.2% success rate with good efficiency and strong user-reported control. These results show that AR feedback substantially stabilizes EEG-based control and that the proposed framework enables robust zero-touch grasping, advancing assistive robotic applications and future modes of human–robot interaction.",
            "introduction": "Human–robot interaction (HRI) plays a crucial role in assistive robotics, teleoperation, and human-inspired systems [1, 2, 3], enabling users to control robots naturally and intuitively. For individuals with severe motor impairments, brain–computer interfaces (BCIs) provide a direct communication pathway between the human brain and external devices without requiring any physical movement [4]. Compared to invasive BCIs, non-invasive BCIs are safer, simpler, and more practical for widespread use [5], although they typically suffer from lower information transfer rates (ITRs) [6]. Among non-invasive BCI modalities, electroencephalography (EEG) is widely adopted for its high temporal resolution, portability, and safety [7]. EEG-based paradigms such as steady-state visual evoked potentials (SSVEP) and motor imagery (MI) have demonstrated strong potential for real-time intention decoding in control tasks [8, 9].\n\nIn conventional systems, visual stimuli for EEG-based interaction are often presented on an independent monitor [10]. For complex, multi-target tasks, this setup is inconvenient: the display is usually not within the same field of view as the workspace, forcing users to repeatedly shift their gaze to verify task progress. This attention switching degrades the interaction experience and limits the practical usability of BCIs [11]. To overcome this limitation, augmented reality (AR) has emerged as a powerful tool for enhancing perceptual feedback in robotic control, overlaying virtual elements directly onto the real world [12]. Integrating AR into BCIs allows users to select and manipulate virtual representations of physical objects in a more immersive, context-aware manner, reducing cognitive load compared to traditional monitor-based interfaces [13].\n\nHowever, most existing BCI-AR frameworks rely on fixed stimulus–command mappings, lack adaptability to dynamically changing environments, and are often demonstrated only in simplified simulation settings without a direct bridge to physical robotic execution. AR technologies can be classified as video see-through (VST) or optical see-through (OST) [14]. VST-AR acquires real-world imagery via a camera, fuses it with virtual content, and displays the composite view on a screen, such as a smartphone or tablet [15]. OST-AR employs a head-mounted optical combiner to overlay translucent virtual elements directly onto the real-world [16], allowing simultaneous perception of both without intermediate video processing [17]. In this context, VST-AR realized on a handheld device provides practical advantages over OST-based HMDs, primarily for two reasons. First, using an EEG-based BCI headset together with an OST-AR HMD can place excessive pressure on the scalp, causing discomfort during prolonged operation [18, 19]. Second, placing the AR headset directly over EEG electrodes may cause mechanical displacement or uneven contact pressure, degrading signal quality and stability [20]. In contrast, a handheld VST-AR device avoids physical interference with EEG sensors, ensuring user comfort and reliable neural signal acquisition.\n\nRobotic grasping in complex, multi-target environments poses substantial challenges. EEG signals are inherently noisy, and their effective integration with AR-based real-time visual feedback must be robust to prevent unintended actions (e.g., AR-guided assistance in hybrid gaze–BMI systems has been shown to improve grasping accuracy and reduce user burden significantly [21]). Nevertheless, prior studies on BCI-AR interfaces often rely on predefined and static target layouts, which limit their adaptability to dynamically changing environments [22]. Only a few systems demonstrate a complete closed-loop pipeline—from EEG intention decoding, to AR-based target confirmation, to autonomous robotic grasp execution—while leveraging AR feedback to enhance signal persistence and stability. Moreover, recent surveys emphasize the scarcity of fully integrated, physically realistic BCI–AR–robot systems capable of seamless deployment across both physical and simulated settings [23, 24, 25].\n\nBuilding upon these gaps, we validate a closed-loop framework on the MyCobot 280Pi robot, enabling fully hands-free operation. Users can select and grasp multiple objects solely through EEG control and AR interaction, without the need for manual intervention. The AR feedback mechanism further stabilizes command execution, directly addressing assistive robotics needs by offering an intuitive and accessible interaction modality for mobility-impaired individuals. Experimental results confirm that the system supports flexible multi-object selection, enhances EEG command accuracy, adapts robustly to changing object layouts, and achieves precise grasp execution without requiring manual remapping.\n\nThe main contributions of this study are as follows:\n\nIntegrated framework: A fully developed multimodal closed-loop pipeline combining EEG intention decoding, AR-based target selection, and autonomous robotic grasping across both physical and simulated environments.\n\nIntegrated framework: A fully developed multimodal closed-loop pipeline combining EEG intention decoding, AR-based target selection, and autonomous robotic grasping across both physical and simulated environments.\n\nDynamic interaction: Real-world AR feedback enabling multi-object decisions via EEG commands, ensuring continuity and accuracy in dynamic settings.\n\nRobust grasping: Adaptive strategies based on inverse kinematics control, enhancing stability and transferability across robotic platforms.\n\nHands-free accessibility: Enabling barrier-free human–robot collaboration, allowing users to accomplish object selection and grasping tasks solely through EEG control, fulfilling requirements for assistive robotics applications.\n\n1. Integrated framework: A fully developed multimodal closed-loop pipeline combining EEG intention decoding, AR-based target selection, and autonomous robotic grasping across both physical and simulated environments.\n\n2. Dynamic interaction: Real-world AR feedback enabling multi-object decisions via EEG commands, ensuring continuity and accuracy in dynamic settings.\n\n3. Robust grasping: Adaptive strategies based on inverse kinematics control, enhancing stability and transferability across robotic platforms.\n\n4. Hands-free accessibility: Enabling barrier-free human–robot collaboration, allowing users to accomplish object selection and grasping tasks solely through EEG control, fulfilling requirements for assistive robotics applications.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高EEG信号的稳定性和准确性以实现有效的人机交互？  \n2. 如何在动态环境中实现灵活的多目标选择和无接触抓取？  \n3. 如何将增强现实(AR)与脑机接口(BCI)结合以改善用户体验？  \n4. 如何实现完整的闭环系统以支持物理和模拟环境中的机器人操作？  \n\n【用了什么创新方法】  \n本研究提出了一种闭环BCI-AR-机器人系统，结合了基于运动想象的EEG解码、增强现实反馈和自主抓取。通过14通道EEG头盔进行个性化MI校准，智能手机AR界面支持多目标导航，并提供方向一致的反馈以增强稳定性。实验结果显示，MI训练准确率达到93.1%，闭环抓取成功率为97.2%。AR反馈显著提高了EEG控制的稳定性，推动了助残机器人应用的发展。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Cyber Racing Coach: A Haptic Shared Control Framework for Teaching Advanced Driving Skills",
            "authors": "Congkai Shen,Siyuan Yu,Yifan Weng,Haoran Ma,Chen Li,Hiroshi Yasuda,James Dallas,Michael Thompson,John Subosits,Tulga Ersal",
            "subjects": "Robotics (cs.RO); Human-Computer Interaction (cs.HC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20653",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20653",
            "arxiv_html_link": "https://arxiv.org/html/2509.20653v1",
            "abstract": "This study introduces a haptic shared control framework designed to teach human drivers advanced driving skills. In this context, shared control refers to a driving mode where the human driver collaborates with an autonomous driving system to control the steering of a vehicle simultaneously.\nAdvanced driving skills are those necessary to safely push the vehicle to its handling limits in high-performance driving such as racing and emergency obstacle avoidance. Previous research has demonstrated the performance and safety benefits of shared control schemes using both subjective and objective evaluations.\nHowever, these schemes have not been assessed for their impact on skill acquisition on complex and demanding tasks.\nPrior research on long-term skill acquisition either applies haptic shared control to simple tasks or employs other feedback methods like visual and auditory aids.\nTo bridge this gap, this study creates a cyber racing coach framework based on the haptic shared control paradigm and evaluates its performance in helping human drivers acquire high-performance driving skills.\nThe framework introduces (1) an autonomous driving system that is capable of cooperating with humans in a highly performant driving scenario;\nand (2) a haptic shared control mechanism along with a fading scheme to gradually reduce the steering assistance from autonomy based on the human driver’s performance during training.\nTwo benchmarks are considered: self-learning (no assistance) and full assistance during training.\nResults from a human subject study indicate that the proposed framework helps human drivers develop superior racing skills compared to the benchmarks, resulting in better performance and consistency.",
            "introduction": "Advanced driving skills refer to a set of competencies that go beyond basic driving abilities in terms of situational awareness, hazard perception, risk management, and vehicle handling [1].\nThey are crucial in high-performance driving tasks such as racing, and can also improve safety in everyday driving [2, 1].\nHowever, they can be challenging to learn due to several reasons: (1) Complexity: Advanced driving skills require proficiency in vehicle control, hazard perception, and risk management, making them inherently complex and difficult to master quickly.\nFor instance, accurately gauging handling limits and responding appropriately demands extensive practice under conditions not experienced regularly in everyday driving. (2) Risk: Advanced driving involves higher risk levels than basic driving.\nThis can intimidate learners and requires time to build necessary confidence and decision-making skills. (3) Cognative & physical Demanding: Mastering these skills requires high mental workload, dexterity, and coordination, which can be challenging for learners.\n\nOn the other hand, autonomy algorithms have been witnessed to safely explore high-performance driving limits [3, 4, 5, 6, 7].\nTherefore, it is possible to conceive a framework that leverages highly performant autonomy algorithms to teach human drivers advanced driving skills that enable precise handling at the dynamic limits of vehicles.\nHowever, as the literature review below shows, such a framework has not yet been created.\nThis study addresses this gap by using racing as a context for assessing human drivers’ advanced driving skills.\n\nIn recent advancements in racing training programs and games, it has been established that providing visual and auditory feedback is beneficial for novice drivers [8, 9].\nIn [10], visual feedback, such as illustrating the racing line on the track, assists drivers in determining the optimal path to steer around corners.\nIn [11], the integration of auditory and visual feedback components culminates in the creation of an immersive learning environment tailored for novice players.\nNevertheless, these feedback modalities are often advisory, meaning that they only provide suggestions for humans instead of directly interacting with their behavior.\nWith a motivation to provide physical interactions with humans, the present study examines the viability of haptic feedback in instructing advanced driving techniques.\n\nHaptic feedback allows for physical interactions between the human and autonomous driving system, enabling the human to feel the intentions of autonomy in real time.\nThis continuous mode of collaboration enhances seamless transitions of control authority, which is particularly crucial for applications such as shared control in semi-autonomous driving [12].\nHere, haptic shared control means that both the human driver and autonomy can exert a torque on the steering wheel at the same time, reflecting their respective intentions to steer.\nThis shared control mechanism allows both agents to communicate their control intentions through the steering wheel.\nThis mutual feedback forms a closed-loop system, enhancing coordination and responsiveness, and turning the steering wheel into a negotiation interface.\nIn [13], haptic shared control is shown to reduce the visual demand and shorten the reaction time while performing better in the driving test.\nThese benefits make haptic shared control a valuable tool in enhancing driver safety and efficiency.\nIt has also been shown to reduce the workload of the human driver in performing ordinary driving tasks such as lane keeping [14, 15, 16, 17, 18].\n\nPrior work has primarily focused on characterizing the immediate benefits of haptic shared control.\nMost studies have concentrated on immediate improvements in task performance and safety, without exploring the longer term implications and potential for driver training.\nIn other words, haptic shared control has been initially used as a driver assistance tool rather than as a teaching aid.\n\nWhile haptic shared control effectively supports humans in real-time, there is considerable potential for its further development as a training mechanism for advanced driving skills.\nThe rationale for this hypothesis is grounded in the applications of haptic shared control in normal driving tasks [19, 20, 21, 22, 23, 24].\nIn [20, 23], continuous haptic feedback was provided to guide drivers in a steering task, helping them to follow lanes.\nHaptic feedback has also been utilized as a continuous disturbance to help drivers improve their skills [19, 22].\nNotably, in [19], a fading scheme is created to gradually fade away the torque assist to help the human’s learning process.\nComparatively, as noted in [25], consistent assistance without a fading mechanism may lead to deskilling, as automation can hinder long-term skill retention rather than enhance it.\nHaptic feedback can also be provided in a bandwidth-limited manner, activating only when certain system states exceed a threshold.\nIn [21, 24], researchers demonstrate that both continuous guidance and bandwidth feedback can effectively enhance driver performance.\nAlthough haptic feedback has demonstrated success in teaching longer-term skills to human drivers in driving scenarios, it has only been applied in relatively simple contexts.\nThe potential for utilizing haptic shared control in more complex and performance-oriented driving scenarios has not yet been explored.\n\nTo effectively teach human drivers advanced driving skills via haptic shared control, it is essential to have a highly performant autonomous system that not only possesses such skills itself but also can seamlessly collaborate with humans. In [26], an optimal control-based algorithm is used to track the path with its own steering commands, and the final steering command is a weighted sum of both human and autonomy inputs.\nIn [27, 28], researchers hired a human expert that is utilized as a source of autonomy that generates haptic feedback.\nTo avoid the inconsistency of human experts, other researchers introduced a\nmodel predictive control (MPC) formulation within the haptic shared control framework [16].\nAlthough such studies have shown that autonomy and human drivers can collaborate, the autonomous systems used in current haptic shared control frameworks are not performant enough for advanced driving scenarios such as racing.\n\nRecent efforts resulted in advanced autonomous systems capable of achieving performance comparable to the best human drivers.\nSpecifically in the context of racing, machine learning algorithms, such as reinforcement learning, have been widely used due to their ability to find optimal policies and their general applicability [29, 30, 31].\nOther works utilize feedforward and feedback controllers to calculate the steering, throttle and braking commands over the racetrack [32, 33, 34].\nMPC has also been popular because it captures the vehicle model and maps control actions based on this knowledge [35, 36, 37, 38, 39].\nIn [40], MPC is used in a shared control mode, where it manages the throttle and braking positions while the human driver controls the steering.\nNevertheless, these solutions either rely on a pre-defined global trajectory to inform the local controller or use learned objectives for control. With human intervention, the vehicle can easily deviate from the original lane in trajectory tracking methods or from the safe states set in learning-based MPC, which can cause infeasible solutions.\n\nIn [41], a safe envelope-based MPC method is introduced, which neither requires training nor relies on pre-defined trajectories when planning dynamically feasible paths on the track. However, it is designed to operate autonomously, without consideration for shared control with human intervention.\n\nIn light of the background review above, the potential benefits of haptic shared control in helping human drivers acquire advanced driving skills is identified as an unexplored but important domain.\nTo explore this potential, this paper presents and evaluates a novel haptic shared control framework aimed at teaching humans advanced driving skills.\nThe original contributions of this work are:\n\nA real-time Model Predictive Control (MPC) framework capable of optimizing vehicle trajectories online without requiring a predefined path while being robust to human intervention.\n\nA haptic shared control framework using a fading scheme to smoothly transition control authority by fading away the autonomy assistance based on human driver’s performance during training.\n\nValidation of the efficacy of the proposed framework for teaching advanced driving skills through a human subject study, with self-learning and full assistance teaching methods as the benchmarks.\n\nThe remainder of the paper is organized as follows.\nSec. II covers the methodology, including the design of autonomy, the haptic shared control framework, performance evaluation metrics, and the fading scheme in training.\nSec. III describes the hardware and software setup for the experiment, the test procedure, and the demographics of the human subjects.\nSec. IV discusses the results of the human subject study and further highlights the benefit of utilizing the proposed methods.\nFinally, Sec. V concludes the study, summarizing the key findings and future directions of research.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效地教导人类驾驶员掌握高级驾驶技能。  \n2. 现有的共享控制方案在复杂任务中的技能获取效果尚未评估。  \n3. 如何利用触觉反馈提升驾驶员的学习效果和技能保留。  \n\n【用了什么创新方法】  \n本研究提出了一种基于触觉共享控制的框架，旨在帮助人类驾驶员学习高级驾驶技能。该框架包括一个能够与人类协作的自主驾驶系统，以及一种逐渐减少自主辅助的触觉共享控制机制。通过在人类受试者研究中进行验证，结果显示该框架显著提升了驾驶员的赛车技能，相较于自学和全辅助的基准方法，表现和一致性均有所改善。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Suction Leap-Hand: Suction Cups on a Multi-fingered Hand Enable Embodied Dexterity and In-Hand Teleoperation",
            "authors": "Sun Zhaole,Xiaofeng Mao,Jihong Zhu,Yuanlong Zhang,Robert B. Fisher",
            "subjects": "Robotics (cs.RO)",
            "comment": "An IEEE conference paper currently under review",
            "pdf_link": "https://arxiv.org/pdf/2509.20646",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20646",
            "arxiv_html_link": "https://arxiv.org/html/2509.20646v1",
            "abstract": "Dexterous in-hand manipulation remains a foundational challenge in robotics, with progress often constrained by the prevailing paradigm of imitating the human hand.\nThis anthropomorphic approach creates two critical barriers: 1) it limits robotic capabilities to tasks humans can already perform, and 2) it makes data collection for learning-based methods exceedingly difficult.\nBoth challenges are caused by traditional force-closure which requires coordinating complex, multi-point contacts based on friction, normal force, and gravity to grasp an object.\nThis makes teleoperated demonstrations unstable and amplifies the sim-to-real gap for reinforcement learning.\nIn this work, we propose a paradigm shift: moving away from replicating human mechanics toward the design of novel robotic embodiments.\nWe introduce the Suction Leap-Hand (SLeap Hand), a multi-fingered hand featuring integrated fingertip suction cups that realize a new form of suction-enabled dexterity.\nBy replacing complex force-closure grasps with stable, single-point adhesion, our design fundamentally simplifies in-hand teleoperation and facilitates the collection of high-quality demonstration data.\nMore importantly, this suction-based embodiment unlocks a new class of dexterous skills that are difficult or even impossible for the human hand, such as one-handed paper cutting and in-hand writing.\nOur work demonstrates that by moving beyond anthropomorphic constraints, novel embodiments can not only lower the barrier for collecting robust manipulation data but also enable the stable, single-handed completion of tasks that would typically require two human hands.\nOur webpage is https://sites.google.com/view/sleaphand.",
            "introduction": "Dexterous manipulation, the ability to reconfigure objects within a single hand, remains a grand challenge in robotics [1, 2].\nThe dominant paradigm for achieving this goal has been data-driven learning on anthropomorphic hands, an approach that has led to successes in grasping and reorientation [3, 4, 5].\nHowever, this focus on human-like forms has also fundamentally tethered robotic potential to the limits of human dexterity.\nCurrent research is largely centered on replicating those simple tasks that human hands can do, rather than solving challenging dexterous tasks or exploring what novel robotic morphologies might achieve.\nThis reality raises our core question: to achieve advanced dexterity, must robots merely imitate the human hand, or can they leap beyond these biological constraints by exploiting a unique physical embodiment?\n\nRecent work has begun to challenge this human-centric paradigm by demonstrating that dexterity emerges not only from sophisticated control but also from morphology itself [6]. A systematic exploration of both anthropomorphic and non-anthropomorphic designs revealed that novel configurations can yield unique, emergent manipulation behaviors without complex controllers. This provides compelling evidence that robotic dexterity is not intrinsically bound to the imitation of the human form.\n\nDespite this insight, the current anthropomorphic approach has created two interconnected barriers that impede progress.\nBoth barriers are caused by a deep reliance on force-closure to maintain a stable grasp through a balance of gravity, normal forces, and friction across multiple contact points.\n\nThe first barrier is the extreme difficulty of teleoperating dexterous in-hand manipulation.\nWhether using wearable exoskeletons [7, 8] or motion capture systems [9, 10], the human operator is cognitively burdened with constantly managing multiple contact points to prevent grasp failure during long-horizon tasks.\nThis makes the collection of high-quality, successful demonstrations inefficient and unreliable.\nThe second barrier is the inherent limitation of the anthropomorphic designs themselves.\nDesigns like the Shadow Hand and Allegro Hand are mechanically anthropomorphic, but are also bounded by the functions and dexterity as the human hand.\nThose complex dexterous manipulations are still within human hands’ capabilities, e.g., in-hand arbitrary object reorientation and solving Rubik’s Cube [3, 4].\nThe root cause of both challenges is a reliance on force-closure: the need to form a stable grasp through a precise balance of gravity, normal forces, and friction across multiple contact points.\nAs a result, even the simplest primitive, grasping, typically requires at least two fingers or a finger and the palm.\nFor imitation learning, this makes it incredibly difficult for a human to demonstrate complex in-hand motions without dropping the object, resulting in easily failed demonstrations in long-horizon tasks.\nFor reinforcement learning, this reliance on rich, sensitive contact dynamics massively complicates sim-to-real transfer, requiring much effort for real-world deployment.\nBy continuing to follow this principle, we are not only making data collection harder but also capping robotic dexterity at the human level.\n\nTo overcome these limitations, we propose a paradigm shift from friction-based force-closure to adhesion-based manipulation.\nWe introduce the Suction Leap-Hand (SLeap Hand), a 5-DoF-per-finger robotic hand where each fingertip is augmented with a suction cup, shown in Figure 1.\nThis suction-based embodiment directly addresses the two core challenges.\nFirst, it dramatically simplifies teleoperation.\nA single suctioned fingertip can create a firm grasp, freeing the operator to focus on the manipulation task itself without the constant risk of dropping the object.\nThis lowers the barrier to collecting high-quality data for imitation learning.\nThe teleoperation can be paused at any time to reduce operators’ workload without keeping lifting arms in the air or wearing exoskeletons all the time.\nSecond, and more importantly, it unlocks new dexterous skills.\nBy leveraging adhesion, the SLeap Hand can perform tasks that are exceedingly difficult or impossible for a single human hand, such as cutting paper or writing on a notebook held by the same hand.\n\nThis work shows that by exploring suction-based embodiments that extend beyond human hand capabilities, the SLeap Hand enables teleoperation of complex dexterous tasks.\nThe main contributions of this paper are:\n\nA novel hardware embodiment:\nWe present the design and realization of the SLeap Hand, a 15-DoF, three-fingered manipulator featuring suction cups on each fingertip and the palm.\nThis design facilitates a hybrid manipulation strategy, combining traditional frictional contacts with controlled adhesion.\n\nA novel hardware embodiment:\nWe present the design and realization of the SLeap Hand, a 15-DoF, three-fingered manipulator featuring suction cups on each fingertip and the palm.\nThis design facilitates a hybrid manipulation strategy, combining traditional frictional contacts with controlled adhesion.\n\nAn teleoperation system to collect challenging dexterous demonstrations precisely and comfortably:\nWe develop a teleoperation system that uniquely leverages the hand’s suction capabilities to provide stable and intuitive control.\nThis enables human to generate high-fidelity demonstrations of complex tasks in a quasi-static way without the assistance of learned models or complex controllers on non-anthropomorphic hands.\nBesides, it reduces operators’ workload.\n\nDemonstration of advanced dexterity:\nWe conduct a series of demonstrations on challenging manipulation tasks, such as paper cutting and in-hand writing, to benchmark the system’s performance.\nThe results demonstrate that our SLeap Hand with suction-based embodiment enables the next level of dexterity previously difficult to achieve with conventional robotic hands.\n\n1. A novel hardware embodiment:\nWe present the design and realization of the SLeap Hand, a 15-DoF, three-fingered manipulator featuring suction cups on each fingertip and the palm.\nThis design facilitates a hybrid manipulation strategy, combining traditional frictional contacts with controlled adhesion.\n\n2. An teleoperation system to collect challenging dexterous demonstrations precisely and comfortably:\nWe develop a teleoperation system that uniquely leverages the hand’s suction capabilities to provide stable and intuitive control.\nThis enables human to generate high-fidelity demonstrations of complex tasks in a quasi-static way without the assistance of learned models or complex controllers on non-anthropomorphic hands.\nBesides, it reduces operators’ workload.\n\n3. Demonstration of advanced dexterity:\nWe conduct a series of demonstrations on challenging manipulation tasks, such as paper cutting and in-hand writing, to benchmark the system’s performance.\nThe results demonstrate that our SLeap Hand with suction-based embodiment enables the next level of dexterity previously difficult to achieve with conventional robotic hands.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何克服传统人形手在灵巧操作中的局限性？  \n2. 如何简化远程操作以提高数据收集的效率和质量？  \n3. 如何利用新型物理形态实现超越人类手的灵巧技能？  \n\n【用了什么创新方法】  \n本研究提出了一种新的机器人手设计——Suction Leap-Hand（SLeap Hand），该手具有集成的吸盘，能够实现基于吸附的灵巧操作。通过替代传统的多点接触力闭合抓握，SLeap Hand简化了远程操作过程，使操作员能够专注于任务而无需担心物体掉落。此外，吸附的应用使得执行如单手剪纸和在手中书写等任务成为可能，这些技能是传统人形手难以实现的。通过这些创新，SLeap Hand不仅降低了数据收集的难度，还展示了超越人类手的灵巧能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Learning Terrain-Specialized Policies for Adaptive Locomotion in Challenging Environments",
            "authors": "Matheus P. Angarola,Francisco Affonso,Marcelo Becker",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Accepted to the 22nd International Conference on Advanced Robotics (ICAR 2025).",
            "pdf_link": "https://arxiv.org/pdf/2509.20635",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20635",
            "arxiv_html_link": "https://arxiv.org/html/2509.20635v1",
            "abstract": "Legged robots must exhibit robust and agile locomotion across diverse, unstructured terrains, a challenge exacerbated under blind locomotion settings where terrain information is unavailable. This work introduces a hierarchical reinforcement learning framework that leverages terrain-specialized policies and curriculum learning to enhance agility and tracking performance in complex environments. We validated our method on simulation, where our approach outperforms a generalist policy by up to 16% in success rate and achieves lower tracking errors as the velocity target increases, particularly on low-friction and discontinuous terrains, demonstrating superior adaptability and robustness across mixed-terrain scenarios.",
            "introduction": "Legged robots operating in real-world environments must function reliably across complex and unstructured terrains to achieve task-oriented goals, requiring locomotion controllers that are both agile and adaptable. Traditional approaches based on model predictive control (MPC) and trajectory optimization (TO) have demonstrated effectiveness under nominal conditions but often fail in edge cases, where simplified dynamics and constraint models are insufficient to capture the complexities of robot–environment interactions [1, 2].\n\nTo address these limitations, learning-based methods have seen growing adoption, leveraging the capacity of neural networks to model high-dimensional, nonlinear control problems from data [3]. In particular, deep reinforcement learning (RL) has emerged as a promising paradigm, enabling agents to learn control policies through direct interaction with the environment, without requiring curated datasets—a fundamental bottleneck in supervised learning [4, 5].\n\nIn RL-based locomotion, control is formulated as a sequential decision-making problem, where policies are optimized through trial-and-error to maximize expected cumulative rewards, typically using simulation environments [6]. To represent the diversity of real-world conditions, domain randomization is applied to factors such as terrain profiles, external perturbations, and sensor noise. As a result, legged robots can learn robust mappings from sensory observations to control actions, enabling them to track desired velocity commands while maintaining stability across diverse and challenging terrains [7, 8, 9].\n\nFurthermore, a particular class of locomotion frameworks, known as blind policies, relies solely on proprioceptive information, without access to exteroceptive sensors such as cameras or LiDAR. These policies are typically employed in systems with limited sensing capabilities or constrained computational resources. However, the lack of terrain awareness encourages the controller to operate under worst-case assumptions, as the robot can only sense the terrain after physical contact rather than perceive it in advance. This reduces agility and limits grades overall locomotion performance, especially when tracking velocity commands in challenging environments [10, 11].\n\nIn parallel, prior work has demonstrated the effectiveness of hierarchical learning in solving complex locomotion and navigation tasks, where a high-level policy selects among low-level behaviors, each specialized for a specific context [12, 13]. Nevertheless, this framework remains underexplored in the context of adaptive locomotion with terrain-specialized policies, particularly in blind locomotion settings.\n\nIn this paper, we present a hierarchical RL framework for blind legged locomotion that leverages terrain-specialized control policies (Fig. 1). Our method decomposes the locomotion task into terrain-specific subtasks, each addressed by a dedicated low-level policy trained with proprioceptive inputs and tailored reward shaping. A high-level policy selector uses privileged observations during deployment to identify the terrain type and activate the corresponding expert policy. To enable agile behavior, each policy is trained under a progressive curriculum that gradually expands the range of velocity commands based on performance.\n\nThrough systematic experimentation with terrain-specialized policies, we analyze their performance at high speeds, achieved by curriculum learning strategy, in complex environments. Furthermore, we compare the effectiveness of specialized versus generalized policies under equivalent training conditions, highlighting trade-offs in robustness and adaptability.\n\nThe key contributions of this work are:\n\nHierarchical control with terrain-specialized policies for blind locomotion\n\nCurriculum learning tailored to terrain-specific agility\n\n1. Hierarchical control with terrain-specialized policies for blind locomotion\n\n2. Curriculum learning tailored to terrain-specific agility",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在盲行走环境中实现腿部机器人适应性运动控制。  \n2. 如何利用分层强化学习框架提升在复杂地形中的灵活性和跟踪性能。  \n3. 传统控制方法在复杂和非结构化地形中的局限性。  \n\n【用了什么创新方法】  \n本文提出了一种分层强化学习框架，利用地形专用控制策略和课程学习来解决盲行走中的适应性运动问题。通过将运动任务分解为特定地形的子任务，每个子任务由专门的低级策略处理，并在部署时使用高层策略选择器识别地形类型，从而激活相应的专家策略。实验结果表明，该方法在复杂环境中相较于通用策略成功率提高了16%，并在低摩擦和不连续地形中表现出更低的跟踪误差，展示了其在混合地形场景中的卓越适应性和鲁棒性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation",
            "authors": "Satyajeet Das,Darren Chiu,Zhehui Huang,Lars Lindemann,Gaurav S. Sukhatme",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20623",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20623",
            "arxiv_html_link": "https://arxiv.org/html/2509.20623v1",
            "abstract": "Reinforcement learning has enabled significant progress in complex domains such as coordinating and navigating multiple quadrotors. However, even well-trained policies remain vulnerable to collisions in obstacle-rich environments.\nAddressing these infrequent but critical safety failures through retraining or fine-tuning is costly and risks degrading previously learned skills. Inspired by activation steering in large language models and latent editing in computer vision, we introduce a framework for inference-time Latent Activation Editing (LAE) that refines the behavior of pre-trained policies without modifying their weights or architecture. The framework operates in two stages: (i) an online classifier monitors intermediate activations to detect states associated with undesired behaviors, and (ii) an activation editing module that selectively modifies flagged activations to shift the policy towards safer regimes. In this work, we focus on improving safety in multi-quadrotor navigation.\nWe hypothesize that amplifying a policy’s internal perception of risk can induce safer behaviors. We instantiate this idea through a latent collision world model trained to predict future pre-collision activations, thereby prompting earlier and more cautious avoidance responses. Extensive simulations and real-world Crazyflie experiments demonstrate that LAE achieves statistically significant reduction in collisions (nearly 90% fewer cumulative collisions compared to the unedited baseline) and substantially increases the fraction of collision-free trajectories, while preserving task completion. More broadly, our results establish LAE as a lightweight paradigm, feasible on resource-constrained hardware, for post-deployment refinement of learned robot policies.",
            "introduction": "Advances in robot learning have significantly\npushed the boundaries of autonomy, including multi robot systems,\ndriven primarily by both reinforcement learning (RL) and imitation learning [1, 2, 3, 4]. Despite these successes, most learned models function as black boxes with limited interpretability and explainability [5]. Enhancing specific behaviors or addressing edge cases typically demands expensive retraining or fine-tuning, involving substantial real-world data or large amounts of simulated interactions [6].\nFurthermore, retraining carries the risk of catastrophic forgetting, where policies lose previously learned skills when adapting to new or updated task specifications [7, 8].\nBeyond forgetting, a broader limitation arises from the asymptotic performance plateaus often observed in RL policies. Once a policy achieves strong average performance, further optimization usually yields only marginal gains [9, 10].\nClosing this final remaining performance gap (e.g., from 95% to 99.9%) is essential for robust real-world deployment [10].\nThese challenges\nmotivate the need for methods that target refinement of specific policy behaviors, without incurring the costs and risks of full re-optimization.\n\nRecent advances in natural language processing and computer vision have shown that the behavior of learned models can be modified at inference time without retraining [11, 12, 13, 14, 15]. Activation steering and representation engineering allow precise, inference-time interventions in large language models (LLMs) to guide outputs towards desired characteristics. Latent space editing in generative and diffusion models enables fine-grained control over generated content. The application of these ideas to robotics remains unexplored, primarily due to challenges associated with real-world interactions compared to text or static vision outputs.\n\nInspired by these advances, we propose a framework for altering the behavior of learned robot policies at inference time.\nSpecifically, we focus on the problem of multi-quadrotor navigation in obstacle-rich environments, leveraging a pre-trained RL policy [2]. While this policy achieves strong overall performance, it continues to struggle in certain edge cases and more challenging scenarios. Further retraining or architectural changes do not alleviate these failures, underscoring the need for alternative approaches [2].\n\nWe investigate whether targeted latent activation editing (LAE) (Figure 1) during inference can enhance safety, quantified through collision avoidance, without costly retraining or fine-tuning.\nWe define LAE as the process of modifying hidden activations of a network during inference, without altering its trained weights. By intervening directly in the latent space, LAE temporarily adjusts the policy’s internal representations to steer behavior along desired axes, such as safety (fewer collisions) in cluttered environments.\nOur key insight is that LAE is a promising mechanism to reduce undesirable behaviors or enhance specific desired behaviors in pre-trained models. The specific latent dimensions chosen for editing and the underlying editing logic vary depending on the behavior to be influenced.\n\nLAE (Figure 1) operates in two stages. First, we identify undesired states by passing the selected intermediate latent activations through an online behavior classifier. Second, we perform targeted editing of these flagged latent activations using a principled strategy. To promote safer behavior, we hypothesize that artificially amplifying the robot’s internal perception of environmental risk can trigger earlier and more cautious maneuvers, thereby improving collision avoidance.\nTo realize this idea, we propose a latent collision world model (LCWM), an action-free latent world model [16, 17, 18] that predicts how latent activations evolve along trajectories leading to collisions, using the current activation together with a short history of past activations (Sec. IV-C).\nOur experiments show that among multiple baseline editing strategies, LCWM is the most effective, consistently yielding statistically superior safety performance across extensive evaluations.\nFinally, we demonstrate the real-world feasibility and effectiveness of our approach through deployment on Crazyflie quadrotors, establishing LAE as a practical and effective tool for enhancing the safety of pre-trained RL-based multirobot navigation policies.\n\nOur key contributions are as follows:\n\nWe present LAE, a novel plug-in framework that steers pre-trained policies by modifying intermediate activations at inference time, enabling targeted refinement of specific behaviors. This is the first activation-space intervention demonstrated on learned robot policies.\n\nWe instantiate LAE on the task of navigating multiple quadrotors in cluttered environments, focusing on improving the collision avoidance behavior of a pretrained RL policy.\n\nWe demonstrate the efficacy of LAE through large-scale simulation studies and real-world quadrotor experiments, achieving statistically significant safety improvements while remaining feasible on highly resource-constrained robots.\n\nAblation studies show that effective latent editing must preserve activations relating to the robot’s own dynamics to avoid dynamically infeasible behaviors.\n\n1. We present LAE, a novel plug-in framework that steers pre-trained policies by modifying intermediate activations at inference time, enabling targeted refinement of specific behaviors. This is the first activation-space intervention demonstrated on learned robot policies.\n\n2. We instantiate LAE on the task of navigating multiple quadrotors in cluttered environments, focusing on improving the collision avoidance behavior of a pretrained RL policy.\n\n3. We demonstrate the efficacy of LAE through large-scale simulation studies and real-world quadrotor experiments, achieving statistically significant safety improvements while remaining feasible on highly resource-constrained robots.\n\n4. Ablation studies show that effective latent editing must preserve activations relating to the robot’s own dynamics to avoid dynamically infeasible behaviors.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在不重训练的情况下提高多机器人导航的安全性。  \n2. 如何通过修改中间激活来改善预训练策略的特定行为。  \n3. 如何避免在复杂环境中发生碰撞。  \n\n【用了什么创新方法】  \n提出了一种名为Latent Activation Editing (LAE)的框架，通过在线分类器监测中间激活，识别不良状态，并通过激活编辑模块选择性修改激活，以引导策略向更安全的行为转变。该方法在多旋翼导航任务中实现了近90%的碰撞减少，并显著提高了无碰撞轨迹的比例，同时保持了任务完成率。LAE展示了在资源受限的硬件上进行后期优化的可行性和有效性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Uncertainty-Aware Active Source Tracking of Marine Pollution using Unmanned Surface Vehicles",
            "authors": "Song Ma,Richard Bucknall,Yuanchang Liu",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted for presentation at Oceantech: Marine Robotics & Science Workshop, IROS 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.20593",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20593",
            "arxiv_html_link": "https://arxiv.org/html/2509.20593v1",
            "abstract": "This paper proposes an uncertainty-aware marine pollution source tracking framework for unmanned surface vehicles (USVs). By integrating high-fidelity marine pollution dispersion simulation with informative path planning techniques, we demonstrate effective identification of pollution sources in marine environments. The proposed approach is implemented based on Robot Operating System (ROS), processing real-time sensor data to update probabilistic source location estimates. The system progressively refines the estimation of source location while quantifying uncertainty levels in its predictions. Experiments conducted in simulated environments with varying source locations, flow conditions, and starting positions demonstrate the framework’s ability to localise pollution sources with high accuracy. Results show that the proposed approach achieves reliable source localisation efficiently. This work contributes to the development of full autonomous environmental monitoring capabilities essential for rapid response to marine pollution incidents.",
            "introduction": "Pollution discharged into the marine environment causes severe consequences to ecosystems [1, 2] and human health [3]. The marine pollution problem is a result of various reasons. According to Biswas et al. [4], coastal waters are endangered by sewerage discharge, agricultural, and industrial waste. Even in the far seas, water bodies also face hazards like oil spills and chemical leaks from various maritime activities [5]. To minimise the influence of marine pollution, reliable monitoring and rapid responses play a crucial role.\n\nIn order to better monitor marine pollution and restrict its influence, the use of robotics systems has attracted much attention, which reduces the risks that humans are exposed to [6]. In particular, the use of robotic systems in environmental monitoring has been widely investigated [7], and it demonstrates significantly higher efficiency compared with conventional solutions [8]. A crucial sub-problem within robotic marine monitoring is the task of localising the pollution source, often referred to as source tracking [9].\n\nEarly approaches to pollution source tracking focused on establishing estimation models of the source locations. For example, Pang and Farrel [10] used Bayesian inference to generate a probability map of the source location in a marine environment, while Hutchinson et al. [11] applied a similar principle to airborne source localisation using a particle filter. A key limitation of these methods is their reliance on external control inputs, such as a predefined search pattern (e.g., a lawnmower pattern), to collect data. Consequently, these methods are not fully autonomous and can be considered as a variant of deploying a fixed sensor network [12].\n\nTo overcome this reliance on predefined paths and achieve full autonomy, research has shifted towards active control strategies, often referred to as active information gathering or Informative Path Planning (IPP) [13]. Several researchers have proposed to incorporate the IPP to substitute the external control in the fields of localisation [14], Simultaneous Localisation and Mapping (SLAM) [15], and 3D reconstruction [16]. However, little attention has been paid to the active marine pollution source tracking. A limited number of existing cases [17, 18] applied IPP for gas source localisation, but the scenarios were restricted to indoor applications where their scales were considerably smaller than in the marine environment. Although Bayat et al. [19] put forward active source tracking for marine pollution, the simulated validation was over-simplistic, and their investigated method for estimating the source location in Bayat et al. [19] was restricted to particle filters. Further study in active source tracking in the marine environment using Bayesian-based estimation and the associated validations based on high-fidelity simulations are necessary.\n\nIn the present paper, an uncertainty-aware active pollution source tracking framework using Bayesian estimation is proposed. The proposed framework can be fully integrated into a carefully constructed high-fidelity marine pollution simulator, as shown in Fig. 1.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在海洋环境中有效定位污染源？  \n2. 如何实现无人表面车辆（USVs）的全自主污染源追踪？  \n3. 如何量化预测中的不确定性以提高源定位的准确性？  \n\n【用了什么创新方法】  \n本研究提出了一种不确定性感知的主动污染源追踪框架，结合高保真海洋污染扩散模拟与信息路径规划技术。通过实时处理传感器数据，系统能够更新源位置的概率估计，并逐步优化定位精度，同时量化预测的不确定性。实验结果表明，该框架在不同源位置和流动条件下，能够高效、准确地定位污染源，显著提升了海洋环境监测的自主能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "GraspFactory: A Large Object-Centric Grasping Dataset",
            "authors": "Srinidhi Kalgundi Srinivas,Yash Shukla,Adam Arnold,Sachin Chitta",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20550",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20550",
            "arxiv_html_link": "https://arxiv.org/html/2509.20550v1",
            "abstract": "Robotic grasping is a crucial task in industrial automation, where robots are increasingly expected to handle a wide range of objects. However, a significant challenge arises when robot grasping models trained on limited datasets encounter novel objects. In real-world environments such as warehouses or manufacturing plants, the diversity of objects can be vast, and grasping models need to generalize to this diversity. Training large, generalizable robot-grasping models requires geometrically diverse datasets. In this paper, we introduce GraspFactory, a dataset containing over 109 million 6-DoF grasps collectively for the Franka Panda (with 14,690 objects) and Robotiq 2F-85 grippers (with 33,710 objects). GraspFactory is designed for training data-intensive models, and we demonstrate the generalization capabilities of one such model trained on a subset of GraspFactory in both simulated and real-world settings. The dataset and tools are made available for download at graspfactory.github.io.",
            "introduction": "Large datasets have been a major contributor to the success of AI models. The fields of Computer Vision and Natural Language Processing have seen tremendous progress due to the presence of internet-scale datasets like ImageNet [1] and Laion-5b [2]. Models such as Chat-GPT [3] and Dall-E[4] demonstrate strong generalization capabilities for tasks that were not explicitly represented in their training data, thanks to the use of diverse training datasets and large-scale transformer-based architectures. Similar efforts have been undertaken in robotics to collect large datasets, such as Open X-Embodiment [5] and DROID [6]. These datasets focus on end-to-end training of robots but there is still a need for task-specific datasets. Robot grasping is one such task, and a generalized grasping model remains elusive, in part due to the lack of geometrically diverse objects in existing datasets. In this work, we present an object-centric grasping dataset that offers greater geometric diversity compared to existing datasets.\n\nCurrently, object-centric grasping datasets [7, 8, 9] and scene-based grasping datasets [10, 11, 12] are mostly geared toward domestic robotics applications. These datasets have been used to train robot grasping models such as [13, 14, 15, 16]. The grasping datasets are generated using 3D CAD models111We use the term “CAD models” in this paper to specifically refer to triangular meshes. from 3D datasets such as Shapenet [17], YCB [18], Objaverse [19] and the Princeton Shape Benchmark [20]. These datasets, however, contain objects of low geometric diversity, as they contain only a small number of semantic classes [8]. Some of the recent advancements in 3D generative models, however, are fueled by larger 3D datasets like those presented in [21, 19, 17]. We leverage one such 3D dataset, ABC-Dataset [21], that contains 1M+ high quality geometric models.\n\nWe introduce GraspFactory, a large-scale dataset of 6-DoF parallel-jaw grasps generated in simulation. The dataset provides two-fingered grasps for the Franka Panda and Robotiq 2F-85 grippers. We utilize a scalable robotics simulation and synthetic data generation tool to annotate the objects with 6-DoF grasps. Further, we train an existing diffusion-based grasping model, SE(3)-DiffusionFields [14] on the Franka Panda subset, and evaluate the model’s generalization capabilities on unseen objects. To the best of our knowledge, this is the largest object-centric grasping dataset containing 6-DoF, parallel-jaw grasps for geometrically diverse 3D data.\n\nOur contributions are as follows:\n\nWe present GraspFactory, a large-scale, object-centric dataset of 6-DoF parallel-jaw grasps with corresponding gripper widths, comprising over 109 million grasps in total. The dataset includes grasps for 33,710 objects randomly selected from the ABC dataset [21] for the Robotiq 2F-85 gripper, and 14,690 objects for the Franka Panda gripper, selected as a subset of the Robotiq object set. As part of ongoing work, we plan to extend the dataset with grasps for additional objects from [21].\n\nWe train a diffusion-based grasp generative model [14] on the Franka Panda subset of GraspFactory, and demonstrate that training on geometrically diverse data improves generalization in both simulation and real-world experiments.\n\nThe rest of the paper is organized as follows: In Section 2, we review prior work. In Section 3, we present the method used for generating the dataset. Section 4 describes the experimental setup, both in simulation and real, and the results from training a model with GraspFactory.\n\n1. We present GraspFactory, a large-scale, object-centric dataset of 6-DoF parallel-jaw grasps with corresponding gripper widths, comprising over 109 million grasps in total. The dataset includes grasps for 33,710 objects randomly selected from the ABC dataset [21] for the Robotiq 2F-85 gripper, and 14,690 objects for the Franka Panda gripper, selected as a subset of the Robotiq object set. As part of ongoing work, we plan to extend the dataset with grasps for additional objects from [21].\n\n2. We train a diffusion-based grasp generative model [14] on the Franka Panda subset of GraspFactory, and demonstrate that training on geometrically diverse data improves generalization in both simulation and real-world experiments.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高机器人抓取模型在新物体上的泛化能力。  \n2. 现有抓取数据集缺乏几何多样性，限制了模型的性能。  \n3. 需要一个大规模、面向对象的抓取数据集来支持训练。  \n\n【用了什么创新方法】  \n本研究提出了GraspFactory，一个包含超过1.09亿个6-DoF抓取的对象中心数据集，专为Franka Panda和Robotiq 2F-85抓手设计。我们利用可扩展的机器人仿真和合成数据生成工具生成抓取数据，并在Franka Panda子集上训练了基于扩散的抓取生成模型SE(3)-DiffusionFields。实验结果表明，使用几何多样性数据进行训练显著提高了模型在未见物体上的泛化能力，验证了该数据集的有效性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning",
            "authors": "Anujith Muraleedharan,Anamika J H",
            "subjects": "Robotics (cs.RO)",
            "comment": "Preprint., 1 algorithm. CoRL 2025 style (preprint). Code/data to be released",
            "pdf_link": "https://arxiv.org/pdf/2509.20541",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20541",
            "arxiv_html_link": "https://arxiv.org/html/2509.20541v1",
            "abstract": "Human feedback can greatly accelerate robot learning, but in real-world settings, such feedback is costly and limited. Existing human-in-the-loop reinforcement learning (HiL-RL) methods often assume abundant feedback, limiting their practicality for physical robot deployment. In this work, we introduce SPARQ, a progress-aware query policy that requests feedback only when learning stagnates or worsens, thereby reducing unnecessary oracle calls. We evaluate SPARQ on a simulated UR5 cube-picking task in PyBullet, comparing against three baselines: no feedback, random querying, and always querying. Our experiments show that SPARQ achieves near-perfect task success, matching the performance of always querying while consuming about half the feedback budget. It also provides more stable and efficient learning than random querying, and significantly improves over training without feedback. These findings suggest that selective, progress-based query strategies can make HiL-RL more efficient and scalable for robots operating under realistic human effort constraints.",
            "introduction": "Robots deployed in the real world must adapt to diverse and dynamic environments while operating under safety and efficiency constraints. HiL-RL has emerged as a powerful paradigm to align robot behavior with human intent by incorporating interactive signals such as evaluative feedback [1], corrective actions[2, 3], and preference comparisons [4]. These approaches accelerate learning in tasks ranging from manipulation to navigation [5, 6]. However, their practicality is limited by the cost of human supervision: attention is a scarce resource, constrained by fatigue, multitasking demands, and operational limitations [6, 7]. Continuous querying for feedback overwhelms supervisors, while too few queries slow adaptation and degrade performance.\n\nTo address this tension, interactive strategies have been proposed where robots query humans only when needed. “Human-gated” approaches allow supervisors to intervene when they see fit [3, 8], but require continuous monitoring and cannot scale to multi-robot settings. “Robot-gated” methods shift the responsibility to the agent, enabling it to request feedback when encountering novel or risky states [9, 10]. While these strategies reduce unnecessary interactions, they typically lack explicit mechanisms for managing strict feedback budgets and often struggle in continuous-control tasks where queries must be carefully timed to avoid disrupting smooth execution.\n\nInspired by how humans naturally allocate their effort rationally, we propose SPARQ (Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning), a budget-aware HiL-RL method. Rather than relying on continuous or uncertainty-driven feedback, SPARQ monitors task progress and selectively requests help only when learning stagnates or worsens. By explicitly modeling human attention as a limited budget and enforcing cooldowns between queries, SPARQ balances learning efficiency with supervision cost. An overview of our approach is illustrated in Fig. 1. Panel (a) shows the SPARQ-augmented training pipeline, where the agent selectively queries a human oracle only when needed. Panel (b) zooms into the SPARQ decision rule, which determines when queries are triggered based on progress, patience, and budget constraints.\n\nOur contributions are as follows:\n\nWe propose SPARQ, a resource-rational HiL-RL method that models human attention as a budgeted resource and allocates queries selectively.\n\nWe introduce a progress-aware query rule that triggers feedback requests based on learning stagnation or deterioration, with patience and cooldown to prevent redundancy.\n\nWe empirically evaluate SPARQ on a simulated UR5 cube-picking task, showing that it matches the success rate of full-feedback baselines while using about half the feedback budget, and performs comparably to Random on this task while providing explicit budget control and smoother temporal allocation of queries.\n\n1. We propose SPARQ, a resource-rational HiL-RL method that models human attention as a budgeted resource and allocates queries selectively.\n\n2. We introduce a progress-aware query rule that triggers feedback requests based on learning stagnation or deterioration, with patience and cooldown to prevent redundancy.\n\n3. We empirically evaluate SPARQ on a simulated UR5 cube-picking task, showing that it matches the success rate of full-feedback baselines while using about half the feedback budget, and performs comparably to Random on this task while providing explicit budget control and smoother temporal allocation of queries.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在有限的人类反馈下提高机器人学习效率？  \n2. 现有的HiL-RL方法在实际应用中受限于人类监督的成本。  \n3. 如何有效管理反馈预算以避免过度查询？  \n\n【用了什么创新方法】  \n本文提出了SPARQ（Selective Progress-Aware Querying），一种资源理性的HiL-RL方法，通过监测任务进展，选择性地请求反馈，仅在学习停滞或恶化时进行查询。SPARQ引入了进展感知查询规则，结合耐心和冷却机制，防止冗余查询。通过在模拟UR5立方体抓取任务中的实验，SPARQ显示出与全反馈基线相当的成功率，同时使用约一半的反馈预算，提供了更稳定和高效的学习效果。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Action-Informed Estimation and Planning: Clearing Clutter on Staircases via Quadrupedal Pedipulation",
            "authors": "Prasanna Sriganesh,Barath Satheeshkumar,Anushree Sabnis,Matthew Travers",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20516",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20516",
            "arxiv_html_link": "https://arxiv.org/html/2509.20516v1",
            "abstract": "For robots to operate autonomously in densely cluttered environments, they must reason about and potentially physically interact with obstacles to clear a path. Safely clearing a path on challenging terrain, such as a cluttered staircase, requires controlled interaction. For example, a quadrupedal robot that pushes objects out of the way with one leg while maintaining a stable stance with its three other legs. However, tightly coupled physical actions, such as one-legged pushing, create new constraints on the system that can be difficult to predict at design time. In this work, we present a new method that addresses one such constraint, wherein the object being pushed by a quadrupedal robot with one of its legs becomes occluded from the robot’s sensors during manipulation. To address this challenge, we present a tightly coupled perception-action framework that enables the robot to perceive clutter, reason about feasible push paths, and execute the clearing maneuver. Our core contribution is an interaction-aware state estimation loop that uses proprioceptive feedback regarding foot contact and leg position to predict an object’s displacement during the occlusion. This prediction guides the perception system to robustly re-detect the object after the interaction, closing the loop between action and sensing to enable accurate tracking even after partial pushes. Using this feedback allows the robot to learn from physical outcomes, reclassifying an object as immovable if a push fails due to it being too heavy. We present results of implementing our approach on a Boston Dynamics Spot robot that show our interaction-aware approach achieves higher task success rates and tracking accuracy in pushing objects on stairs compared to open-loop baselines.",
            "introduction": "Successful real-world deployment of quadrupedal robots requires the ability to autonomously perceive, reason about, and physically navigate unpredictable environments. This requires them to go beyond simple avoidance and actively interact with obstacles, such as pushing objects that block their path. This capability becomes critical in complex terrains like a narrow, cluttered staircase, where a robot can neither treat items as static obstacles to be avoided nor step on them without compromising safety. Instead, the robot must reason about the clutter’s physical properties to determine if a path can be safely cleared through interaction. For a quadruped, this action can be achieved by balancing on three legs while using the fourth to manipulate an object, an act termed “pedipulation”. This demands a tight integration of perception and action, as the robot must manage the physical push while dealing with inevitable perceptual challenges like occlusion. In this paper, we present a complete perception-to-action framework that addresses these challenges, enabling a quadruped robot to safely perceive, reason about, and clear movable clutter from a staircase.\n\nMany robotic pushing methods operate by decoupling perception from execution, an approach that is fundamentally limited in real-world scenarios. For example, during a push, the robot’s motion to maintain balance while pushing causes the robot’s body to tilt, shifting the camera’s field-of-view away from the object on the stair. Additionally, the manipulating leg itself physically obstructs any remaining line of sight. Given these geometric constraints, achieving a consistently unobstructed view through additional sensors is infeasible. This loss of exteroception is consequential because push outcomes are not simple binary events. Factors like noisy state estimation and complex object shapes make partial pushes and foot slips common. Such unpredicted displacements can cause standard visual trackers to fail, leading the system to lose track of an object’s state entirely.\n\nThis failure to track presents a new challenge–reasoning about the cause of a failed interaction. The robot must be able to distinguish whether an attempt failed because the foot missed the object or because it contacted an object too heavy to move. A robust system must be able to resolve this ambiguity in the failed push, and learn from it to intelligently update the robot’s world model allowing it to reclassify an object as static or re-attempt the push.\n\nTo address these challenges, we introduce a perception framework that robustly segments clutter on staircases and maintains a low-dimensional world model of the environment. To overcome the challenge of visual occlusion within this model, it employs an interaction-aware state estimation loop. During a push, this loop uses proprioceptive feedback to predict an object’s displacement, guiding the perception system to re-detect the object and update its state. This predict-correct cycle ensures robust tracking through complex interactions and updates key object properties, such as movability, within the world model. This state estimation is integrated into a hierarchical task execution framework. At the high level, a planner manages the robot’s behavior by switching between navigation and pedipulation modes, while using contact feedback to learn from interactions to reclassify static objects, or retry pushes. It directs a low-level planner to compute safe foot trajectories for object pushing. Finally, a policy trained via reinforcement learning executes these trajectories, generating stable joint commands for the maneuver.\n\nThe main contributions of this work are:\n\nA 3D perception framework that robustly segments and localizes clutter on staircases by leveraging geometric priors of the environment.\n\nAn interaction-aware state estimation pipeline that robustly tracks an object’s state through partial pushes and updates its movability by fusing visual measurements with proprioceptive contact feedback.\n\nA hierarchical planning and control architecture where a planning pipeline computes collision-free foot trajectories for pushing, and a learned pedipulation policy to executes these trajectories.\n\n1. A 3D perception framework that robustly segments and localizes clutter on staircases by leveraging geometric priors of the environment.\n\n2. An interaction-aware state estimation pipeline that robustly tracks an object’s state through partial pushes and updates its movability by fusing visual measurements with proprioceptive contact feedback.\n\n3. A hierarchical planning and control architecture where a planning pipeline computes collision-free foot trajectories for pushing, and a learned pedipulation policy to executes these trajectories.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂环境中实现机器人对障碍物的主动交互以清理路径。  \n2. 如何解决在推物体过程中传感器视野被遮挡的问题。  \n3. 如何通过反馈学习更新物体的可移动性状态。  \n\n【用了什么创新方法】  \n提出了一种紧密耦合的感知-动作框架，利用本体反馈预测物体位移，从而在遮挡情况下实现物体的重新检测。该方法通过交互感知状态估计循环，确保在部分推送过程中保持对物体状态的准确跟踪，并更新物体的可移动性。实现了在复杂环境中更高的任务成功率和跟踪精度，特别是在推物体的过程中，展示了相较于开放循环基线的显著优势。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "MELEGROS: Monolithic Elephant-inspired Gripper with Optical Sensors",
            "authors": "Petr Trunin,Diana Cafiso,Anderson Brazil Nardin,Trevor Exley,Lucia Beccai",
            "subjects": "Robotics (cs.RO)",
            "comment": ". SI . Submitted to Wiley Advanced Science",
            "pdf_link": "https://arxiv.org/pdf/2509.20510",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20510",
            "arxiv_html_link": "https://arxiv.org/html/2509.20510v1",
            "abstract": "The elephant trunk exemplifies a natural gripper where structure, actuation, and sensing are seamlessly integrated. Inspired by the distal morphology of the African elephant trunk, we present MELEGROS, a Monolithic ELEphant-inspired GRipper with Optical Sensors, emphasizing sensing as an intrinsic, co-fabricated capability. Unlike multi-material or tendon-based approaches, MELEGROS directly integrates six optical waveguide sensors and five pneumatic chambers into a pneumatically actuated lattice structure (12.5 mm cell size) using a single soft resin and one continuous 3D print. This eliminates mechanical mismatches between sensors, actuators, and body, reducing model uncertainty and enabling simulation-guided sensor design and placement. Only four iterations were required to achieve the final prototype, which features a continuous structure capable of elongation, compression, and bending while decoupling tactile and proprioceptive signals. MELEGROS (132 g) lifts more than twice its weight, performs bioinspired actions such as pinching, scooping, and reaching, and delicately grasps fragile items like grapes. The integrated optical sensors provide distinct responses to touch, bending, and chamber deformation, enabling multifunctional perception. MELEGROS demonstrates a new paradigm for soft robotics where fully embedded sensing and continuous structures inherently support versatile, bioinspired manipulation.",
            "introduction": "The elephant trunk is a remarkably versatile biological manipulator that integrates sensing and actuation within a jointless structure to grasp objects of many shapes, weights, and sizes. In the African elephant, the trunk tip has two asymmetric finger-like projections enable pinching, scooping, and supporting actions [1, 2, 3]. The absence of any division between the continuum arm and the tip, combined with distributed sensory feedback, supports smooth reaching, grasping, and highly dexterous manipulation tasks (e.g., ripping leaves from a wrapped branch). Unlike engineered systems that separate sensing, actuation, and structure, the elephant trunk links them through the integration of muscles, connective tissue, skin, and embedded mechanoreceptors. This arrangement allows the elephant to control movement and respond efficiently to contact with its environment, even with occluded vision during prehensile activity [4]. In this sense, the trunk is monolithic: structure, actuation, and sensing are inseparable.\n\nThis type of integration is still uncommon in robotic systems. Although today robots can deform and adapt to their surroundings through compliant materials and structures [5, 6, 7], many are built by combining separate sensing, actuation, and structural elements [8, 9, 10]. This gap is related to a broader challenge in soft robotics: the lack of truly monolithic systems that combine all functional elements into a single material body. In particular, this problem stems from the fact that common transduction mechanisms, e.g., resistive [11, 12, 13, 14] and capacitive [15, 16, 17, 18] sensors, depend on conductive materials, with inherent mechanical characteristics (e.g., stiffness) different from the ones typically used for the robot’s body. Moreover, sensing elements are often added post-fabrication. This leads to mechanical mismatches that can compromise compliance and induce failure under cyclic loading. Recent efforts have achieved various degrees of actuator–sensor integration [19]. For example, Truby et al. combined EMB3D printing with molding to produce soft somatosensitive actuators by injecting conductive ionogel into elastomeric matrices [20], while Xiao et al. fabricated a fully 3D-printed robotic hand incorporating soft capacitive sensors via dual-extrusion of dielectric and conductive silicones [21]. In addition to transduction strategies, the pressure feedback from fluidic channels has been leveraged to maintain material uniformity [22], though at the expense of increased design complexity (thin hollow channels) and potential performance trade-offs between sensing and actuation. Although these approaches represent meaningful advances toward the monolithic approach, they still rely on multiple materials, involve elaborate fabrication workflows, and restrict design versatility. To fully eliminate material mismatches and post-assembly procedures, a solution is to build sensors with the same material as the actuators and the robot body, a goal attainable by implementing transducers that exploit the optical, rather than the electrical, conductivity of sensing materials. In fact, the monolithic integration of optical sensing set just one requirement, i.e., the material of both sensors and robot must be transparent to light. This feature, even if challenging, is still less constraining than those required from other transduction mechanisms, which may imply the addition of functional fillers (e.g., magnetic, electrically-conductive) stiffening the robot and creating bi-material interfaces. As a first step, our previous work introduced the Monolithic Perceptive Unit (MPU): a fully 3D-printed lattice cell in which the constituent elastomer itself functions as an optical sensor [23].\n\nLattice architectures have emerged as a promising alternative to bulk soft bodies. Although bulk material functionalization is possible, it often comes at the expense of mechanical performance [24]. In contrast, lattice architectures retain the softness of the bulk material while offering internal pathways and anchor points, which simplify the integration of actuators and sensors and enable support-free fabrication by creating in situ supports during 3D printing. For example, tendon-driven lattices have been used to reproduce musculoskeletal behaviors by routing cables through the structure [25, 26]. Alternatively, we have previously introduced lattice-embedded actuators which can achieve bending and jointless behavior [27]. In this work, an IWP-TPMS (triply periodic minimal surface) lattice is adopted. While the underlying topology defines the deformation modes, the stiffness -in addition to the intrinsic material properties- depends on the cell dimensions. Since the lattice serves as a compliant medium for the embedded actuators, and it must enable the gripper to extend, bend, and conform around objects during grasping, a low bending stiffness of the chosen lattice configuration is pursued.\n\nThe draw of the proposed monolithic method in soft robotics lies in simplifying the design process: using a single-material body that combines actuation and sensing without post-processing. However, few materials are capable of delivering all required functionalities (i.e., flexibility, printability, sensing) while remaining compatible with streamlined fabrication. Recent advances in commercial 3D printing [28] have enabled one-step processes to create hybrid systems, yet fully printed monolithic soft systems are still rare.\n\nInspired by the morphology and behavior of the distal region of the African elephant trunk [2, 4], and enabled by an architected design, we introduce the MELEGROS concept: a Monolithic Elephant-Inspired Gripper with Optical Sensors (Figure 1). The system is built from a soft lattice with smoothly connected bladder-shaped actuators, which not only allow the system to elongate, compress, and bend, but act as the body of both gripper and soft optical sensors. The design does not aim to mimic natural muscular arrangements, but rather focuses on functional integration of actuation and sensing. To achieve this objective, and building on our recent results in simulating soft lattice structures [29], our method is based on a workflow linking design and fabrication through simulation in SOFA (Simulation Open Framework Architecture) [30, 31] to design and position the soft optical sensors. The output is a fully-integrated design that is fabricated via a single 3D-printing process. In this work, we focus on investigating the sensing functionality by targeting the discrimination of exteroceptive from proprioceptive information during grasping tasks. We show how the specific monolithic architecture enables MELEGROS to perform enveloping grasps and delicate pinching maneuvers (extending its functionality beyond simple parallel-jaw closure) and to reach and bend independently in an intrinsic workspace, where a specific object can be reached from multiple directions before being grasped, similar to the natural model.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现感知、驱动和结构的单一材料整合以提高软机器人性能。  \n2. 现有软机器人系统在传感器与驱动器集成方面的局限性。  \n3. 如何通过光学传感器实现更高效的多功能感知。  \n\n【用了什么创新方法】  \n本研究提出MELEGROS，一个灵感来源于非洲象鼻的单体抓手，采用光学传感器与气动驱动器的无缝集成。通过使用单一软树脂材料和连续3D打印技术，MELEGROS实现了传感与驱动的内在结合，消除了机械不匹配，简化了设计流程。该抓手能够进行延伸、压缩和弯曲，执行如夹持、舀取和精细抓取等生物启发的动作，且能够区分外部和内部感知信息。最终，MELEGROS展示了一种新型的软机器人设计理念，支持多功能的生物启发操控。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting",
            "authors": "Boqi Li,Siyuan Li,Weiyi Wang,Anran Li,Zhong Cao,Henry X. Liu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20499",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20499",
            "arxiv_html_link": "https://arxiv.org/html/2509.20499v1",
            "abstract": "With the rapid progress of foundation models and robotics, vision-language navigation (VLN) has emerged as a key task for embodied agents with broad practical applications. We address VLN in continuous environments, a particularly challenging setting where an agent must jointly interpret natural language instructions, perceive its surroundings, and plan low-level actions. We propose a zero-shot framework that integrates a simplified yet effective waypoint predictor with a multimodal large language model (MLLM). The predictor operates on an abstract obstacle map, producing linearly reachable waypoints, which are incorporated into a dynamically updated topological graph with explicit visitation records. The graph and visitation information are encoded into the prompt, enabling reasoning over both spatial structure and exploration history to encourage exploration and equip MLLM with local path planning for error correction. Extensive experiments on R2R-CE and RxR-CE show that our method achieves state-of-the-art zero-shot performance, with success rates of 41% and 36%, respectively, outperforming prior state-of-the-art methods.",
            "introduction": "Vision-language navigation (VLN) is the task in which an embodied agent follows a natural language instruction to navigate through an environment and reach a specified destination. It represents an important capability for embodied AI, with applications ranging from search and rescue to autonomous navigation and daily human–robot interaction. However, VLN remains highly challenging: the agent must not only comprehend natural language instructions but also ground them in its surrounding environment [1, 2, 3]. Existing approaches [4, 5, 6] typically rely on RGB-D sensory inputs, which demand both a detailed understanding of complex visual scenes and robust reasoning over the task context.\n\nEarly research on VLN has largely focused on discrete environment settings, where a navigation graph is predefined and the agent’s movement is restricted to a finite set of checkpoints [1, 2, 7]. More recently, it has been extended to continuous environments, where the agent can move freely [3, 8]. This setting is considerably more challenging and has motivated a line of work that employs waypoint prediction models [9]: candidate waypoints are first generated in the continuous space, and a navigation policy then selects one to pursue. This decomposition is intended to ease the learning process by separating spatial grounding from long-horizon reasoning.\n\nDespite their promise, existing waypoint prediction approaches exhibit drawbacks. First, the choice of input representations remains underexplored. The original waypoint model relied on RGB and depth images; however, subsequent work found that RGB inputs can sometimes degrade performance, while others introduced increasingly complex architectures for RGB-D processing, further increasing model complexity [5, 10]. Second, predicted waypoints are not guaranteed to be reachable from the agent’s current position and may be obstructed by intervening obstacles.\n\nRather than designing waypoint prediction models with increasingly complex sensory inputs and neural architectures, we propose a simplified waypoint prediction model that proves rather effective. Our key insight is that providing raw RGB and depth images introduces large amounts of irrelevant information, which can obscure the underlying spatial relationships and hinder generalization. To address this, we abstract the input representation: depth images are first processed into an obstacle map centered on the agent, which is then used as the sole input to the waypoint predictor. This abstraction encourages the model to focus on the dynamic patterns of obstacle distribution, enabling it to better identify critical regions where waypoints should be generated. Empirically, we find that our approach produces waypoints that are both more feasible and reliable than those generated by prior methods.\n\nWe integrate our proposed waypoint model with a zero-shot navigator based on a multimodal large language model (MLLM) [11]. Zero-shot VLN with large language models (LLMs) has attracted increasing attention in recent years [12, 4, 13, 14, 15], as purely learning-based navigation models still suffer from data scarcity and struggle to generalize to unseen environments. By leveraging the broad knowledge encoded in LLMs, zero-shot approaches enable agents to better interpret both natural language instructions and perceptual inputs.\n\nRecent works in VLN-CE have explored textual and visual prompt design for LLM-based navigators [13, 15, 10], incorporating different forms of contextual information. However, most of these approaches overlook the structure of the topological graph, causing the LLM agent to quickly lose track of which waypoints have already been explored. To address this limitation, we propose a dynamically updated topological graph that explicitly encodes node visitation status. This graph is incorporated into the prompt design, providing the LLM with structured knowledge of explored and unexplored regions, alongside perceptual inputs, thereby enabling more consistent and informed reasoning about the agent’s navigation choices.\n\nWe evaluate our proposed waypoint prediction + MLLM navigation framework in the R2R-CE [8] and RxR-CE [7] datasets, achieving 41% and 36% success rates and outperforming recent zero-shot methods. Our contributions are threefold:\n\nAbtract obstacle-map based waypoint prediction: A lightweight predictor using obstacle maps as input, improving waypoint feasibility and reachability.\n\nTopoGraph-and-VisitInfo aware prompting: A dynamically updated topological graph with visited-node tracking, incorporated into prompts to help the MLLM reason over spatial structure and exploration history.\n\nIntegrated zero-shot VLN framework: A unified system combining our waypoint predictor with an MLLM navigator, achieving state-of-the-art results on both R2R-CE and RxR-CE datasets.\n\n1. Abtract obstacle-map based waypoint prediction: A lightweight predictor using obstacle maps as input, improving waypoint feasibility and reachability.\n\n2. TopoGraph-and-VisitInfo aware prompting: A dynamically updated topological graph with visited-node tracking, incorporated into prompts to help the MLLM reason over spatial structure and exploration history.\n\n3. Integrated zero-shot VLN framework: A unified system combining our waypoint predictor with an MLLM navigator, achieving state-of-the-art results on both R2R-CE and RxR-CE datasets.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在连续环境中实现有效的视觉-语言导航（VLN）。  \n2. 现有的路径预测方法在可达性和复杂性方面的不足。  \n3. 如何利用大语言模型（LLM）进行零-shot导航。  \n\n【用了什么创新方法】  \n本研究提出了一种基于抽象障碍地图的轻量级路径预测模型，简化了输入表示，专注于障碍分布的动态模式，从而提高了路径的可行性和可靠性。我们还设计了一个动态更新的拓扑图，将访问状态编码到提示中，帮助多模态大语言模型（MLLM）进行更有效的空间结构和探索历史推理。通过将这两者结合，我们的零-shot VLN框架在R2R-CE和RxR-CE数据集上实现了41%和36%的成功率，超越了现有的零-shot方法，显示出显著的性能提升。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Revisiting Formal Methods for Autonomous Robots: A Structured Survey",
            "authors": "Atef Azaiez,David A. Anisi,Marie Farrell,Matt Luckcuck",
            "subjects": "Robotics (cs.RO)",
            "comment": "Appeal accepted: MOD-66548 This is an appeal request regarding our submission MOD-65174 - 6681725",
            "pdf_link": "https://arxiv.org/pdf/2509.20488",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20488",
            "arxiv_html_link": "https://arxiv.org/html/2509.20488v1",
            "abstract": "This paper presents the initial results from our structured literature review on applications of Formal Methods (FM) to Robotic Autonomous Systems (RAS). We describe our structured survey methodology; including database selection and associated search strings, search filters and collaborative review of identified papers. We categorise and enumerate the FM approaches and formalisms that have been used for specification and verification of RAS. We investigate FM in the context of sub-symbolic AI-enabled RAS and examine the evolution of how FM is used over time in this field. This work complements a pre-existing survey in this area and we examine how this research area has matured over time. Specifically, our survey demonstrates that some trends have persisted as observed in a previous survey. Additionally, it recognized new trends that were not considered previously including a noticeable increase in adopting Formal Synthesis approaches as well as Probabilistic Verification Techniques.",
            "introduction": "Formal Methods have been incorporated in the software production life cycle since the early adoption of computers.\nRelying solely on testing has been shown to be not enough to guarantee the absence of bugs in software. This quote from Dijkstra in 1969 emphasises that the computer science community needed to develop alternative methods to testing \"Testing shows the presence, not the absence of bugs\"  [6].\nAs technology and aspirations have evolved, the use of Robotic Autonomous Systems (RAS) in safety- and/or mission-critical applications has increased, including in the nuclear [4], aerospace [27], agriculture [3, 2], transport [14] and space domains [8]. These sorts of critical systems among others involving safety and security requirements clearly need to be robustly verified using Formal Methods for specification and verification. The use of Formal Methods (FM) is admitted, recommended and can be required by some standards  [16]. The strong verification provided by potentially combinations of distinct FM and testing approaches is advantageous as it guarantees mathematical proof of correctness. This aides in the assurance process in critical settings and helps to provide various stakeholders with sufficient confidence that the systems function as expected.\nVariousFM approaches have been developed to fulfil specific needs of verification.\n\nA 2019 survey provides an overview of these methods and acts as a guidebook for those seeking to apply FM in RAS [17]. Apart from the obvious benefits to developing reliable software, there is a reciprocal benefit to the FM community: the modularity of RAS, as exemplified in the various middleware by which they are supported, fosters creative and interesting opportunities for examining and demonstrating the efficacy of these FM [7]. These observations have given rise to a novel sub-domain called Formal Methods for Autonomous Systems111https://fmasworkshop.github.io/ and many conferences have held special tracks in related topics since. On the other hand, there are some challenges of applying FM to RAS, namely the complexity of the this kind of systems as they usually combine discrete software logic with continuous physical dynamics and that can lead to scalability issues. Moreover, the dynamic nature of the environment where RAS operate makes it difficult to capture all interactions and uncertainties. last but not least, there can be a gap between the trustworthiness of formal verification results and the expectations of regulatory acquirements  [1].\n\nIn this paper, we present the methodology we adopted to conduct our structured literature survey, initial results which examines how the application of FM to RAS has evolved over time. We analyse which trends have persisted since the original survey [17] and identify emerging trends. We examine the relative use of different formal methods and verification approaches, and discuss the role played by Sub-Symbolic AI (SSAI) (e.g. machine learning). We reflect on potential reasons for these various evolutions, providing insight and set the stage for future development in this field.\n\nNext, we present some related work, while the rest of the paper is structured as follows. In Sect. 2 we describe our survey’s methodology, including the scope and search terms. We present the results in Sect. 3 and discuss the implications of the results in Sect. 4. Finally, Sect. 5 gives our concluding remarks.\n\nOur survey builds on a previous survey of FM applied to autonomous robotic systems [17], though we extend the time frame from 2007—2018, to 2007—2024;\nWe also use a different methodology and work-flow, and used Rayyan [20] 222Rayyan: https://www.rayyan.ai/ a dedicated tool for conducting structured surveys.\nThis gives broader coverage of the literature, including both wider search terms and additional search sources, initially returning 20,764 papers. We examine similar research questions to [17] but with an explicit focus on the trends that have emerged over time, and examining the impact of SSAI.\n\nAs the sub-domain of FM for autonomous systems has evolved and become more popular over time, it is no surprise that other related research efforts exist. These include a manifesto for applicable FM that provides ten principles concerning their use [9]. This project however does not specifically focus on RAS, rather it discusses the use and promotion of FM in practice more generally.\n\nLeahy et al. [15] define three categories of grand challenge for verification of autonomous systems: (1) Requirements and Specifications, (2) Models and Abstractions, and (3) Tools, Techniques and Algorithms.\nRelated work in [22] provides a research roadmap for verification of autonomous systems which points to several of these open challenges and emerging standards in the area.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效地应用Formal Methods (FM)于Robotic Autonomous Systems (RAS)的规范和验证？  \n2. FM在RAS中的应用趋势及其演变如何？  \n3. Sub-Symbolic AI (SSAI)在FM与RAS结合中的角色是什么？  \n\n【用了什么创新方法】  \n本研究采用结构化文献综述的方法，使用Rayyan工具进行文献筛选和分析。通过广泛的文献搜索，涵盖2007年至2024年的相关研究，识别出FM在RAS中的应用趋势及新兴方法。结果表明，Formal Synthesis和Probabilistic Verification Techniques的采用显著增加，反映出FM在自适应系统中的重要性和发展潜力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Boosting LiDAR-Based Localization with Semantic Insight: Camera Projection versus Direct LiDAR Segmentation",
            "authors": "Sven Ochs,Philip Schörner,Marc René Zofka,J. Marius Zöllner",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20486",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20486",
            "arxiv_html_link": "https://arxiv.org/html/2509.20486v1",
            "abstract": "Semantic segmentation of LiDAR data presents considerable challenges, particularly when dealing with diverse sensor types and configurations. However, incorporating semantic information can significantly enhance the accuracy and robustness of LiDAR-based localization techniques for autonomous mobile systems. We propose an approach that integrates semantic camera data with LiDAR segmentation to address this challenge. By projecting LiDAR points into the semantic segmentation space of the camera, our method enhances the precision and reliability of the LiDAR-based localization pipeline.",
            "introduction": "Accurate and robust localization is a fundamental requirement for autonomous vehicles in complex environments, such as urban canyons and parking spaces. LiDAR sensors, with their ability to provide high-resolution depth information, play a crucial role in modern localization systems. However, LiDAR-based localization methods often struggle in challenging conditions, such as feature-sparse environments, dynamic scenes, or adverse weather conditions. To address these limitations, semantic information can be integrated into localization frameworks, enhancing the robustness and accuracy of pose estimation.\n\nRecent advancements in machine learning, particularly deep neural networks (DNN), have led to significant improvements in the semantic segmentation of LiDAR point clouds. These methods can classify point cloud data into meaningful categories, providing valuable contextual information for localization. However, LiDAR-only semantic segmentation approaches face inherent challenges, including sensor sparsity, domain adaptation issues, and computational complexity [1].\n\nIn contrast, leveraging semantic information from camera images, it is possible to improve LiDAR-based localization without solely relying on point cloud segmentation. This paper proposes an alternative approach integrating semantic features from camera images into a LiDAR-based localization pipeline through projection techniques. By mapping semantic labels from camera images onto the LiDAR point cloud, we aim to enhance localization performance without requiring direct LiDAR point cloud segmentation.\n\nThis paper evaluates the proposed method compared to state-of-the-art DNN LiDAR point cloud segmentation techniques. Specifically, we analyze the impact of semantic camera integration on localization accuracy and robustness to environmental variations. Our contributions are as follows:\n\nAn approach for LiDAR-based localization that integrates semantic camera information through projection.\n\nAn approach for LiDAR-based localization that integrates semantic camera information through projection.\n\nA comparative analysis of our method with state-of-the-art machine learning-based semantic segmentation of LiDAR point clouds.\n\nExperimental validation demonstrates the advantages and disadvantages of localization accuracy.\n\nThe remainder of this paper is structured as follows: Section II provides an overview of related work, including machine learning-based LiDAR segmentation and camera-LiDAR fusion techniques. Section III details the proposed methodology, including the semantic projection pipeline. Section IV presents experimental results. Finally, Section V concludes the paper with potential future research directions.\n\n1. An approach for LiDAR-based localization that integrates semantic camera information through projection.\n\n2. A comparative analysis of our method with state-of-the-art machine learning-based semantic segmentation of LiDAR point clouds.\n\n3. Experimental validation demonstrates the advantages and disadvantages of localization accuracy.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提升LiDAR数据的语义分割精度以增强定位系统的准确性和鲁棒性。  \n2. 在复杂环境中，LiDAR定位方法如何克服特征稀疏和动态场景等挑战。  \n3. 如何有效整合来自相机的语义信息以改善LiDAR定位性能。  \n\n【用了什么创新方法】  \n本研究提出了一种将相机的语义信息通过投影技术整合到LiDAR定位管道中的方法。该方法通过将LiDAR点映射到相机的语义分割空间，提升了LiDAR定位的精度和可靠性。与传统的LiDAR点云分割方法相比，实验结果表明，整合语义相机信息能够显著改善定位的准确性和对环境变化的鲁棒性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences",
            "authors": "Julius Pesonen,Arno Solin,Eija Honkavaara",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20906",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20906",
            "arxiv_html_link": "https://arxiv.org/html/2509.20906v1",
            "abstract": "3D object localisation based on a sequence of camera measurements is essential for safety-critical surveillance tasks, such as drone-based wildfire monitoring.\nLocalisation of objects detected with a camera can typically be solved with dense depth estimation or 3D scene reconstruction.\nHowever, in the context of distant objects or tasks limited by the amount of available computational resources, neither solution is feasible.\nIn this paper, we show that the task can be solved using particle filters for both single and multiple target scenarios.\nThe method was studied using a 3D simulation and a drone-based image segmentation sequence with global navigation satellite system (GNSS)-based camera pose estimates.\nThe results showed that a particle filter can be used to solve practical localisation tasks based on camera poses and image segments in these situations where other solutions fail. The particle filter is independent of the detection method, making it flexible for new tasks.\nThe study also demonstrates that drone-based wildfire monitoring can be conducted using the proposed method paired with a pre-existing image segmentation model.",
            "introduction": "This work addresses the problem of locating distant objects from a series of camera-based detections from known locations and orientations. At a glance, the problem of locating target objects based on multiperspective imagery seems fairly well-addressed. The earlier proposed methods typically operate as optimisation problems where the object locations of individual keypoints are determined using a set of images and known camera poses. Alternatively, full scene 3D reconstruction or dense depth estimation methods have been used for similar problems.\n\nHowever, the task at hand, presented in this work, is specific to far-away objects which have been detected with separate image segmentation models, such as neural networks which inherently produce noisy segments. Other sources of noise occur from possible target object dynamics and the camera-pose estimation methods.\nIn addition, due to the nature of the target detection models, direct correspondence between features from consecutive frames does not hold.\n\nThe motivation for this work originates from drone-based wildfire detection, in which the position of the drone-carried camera is estimated using GNSS measurements and known dynamics of the drone camera setup. Our previous work showed that wildfire smoke can be detected from almost ten kilometres away using only drone-carried resources [Pesonen_2025_WACV]. Pairing the segmentation model with a lightweight target localisation method enables fully on-board wildfire detection and localisation.\nThis enables the wildfire detection system to be used in areas of poor telecommunication where cloud-based computing can not be relied on.\nThe sketch in Figure 1 illustrates a use case of a UAV scanning for wildfires (sketch by DALL-E 3) with masked RGB images (real data).\n\nTo combat the noise induced by the camera and image observation systems, we focus on Bayesian filters. They enable reliable modelling of various noise sources, and with particle filters in particular, more complex observation and target dynamics can be modelled. The use of particle filters in camera-based 3D localisation literature has been limited. Specifically, for locating distant objects from a moving camera, there appears to be a lack of extensive research.\n\nThis work extends the literature on filter-based target object localisation from a moving camera by addressing the problem of distant objects detected by separate models.\nWe propose using particle filters to iteratively improve the localisation and uncertainty estimation of the target object’s 3D position.\nUsing both simulations and a drone-captured image sequence with GNSS-estimated camera positions, we show that the method can locate multiple target objects from moving camera segmentation sequences despite the various sources of noise.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何从噪声摄像机运动和语义分割序列中定位远处物体。  \n2. 现有的3D重建和深度估计方法在远处物体定位中不适用。  \n3. 如何处理由于目标动态和相机姿态估计引起的噪声。  \n\n【用了什么创新方法】  \n本研究提出了一种基于粒子滤波器的方法，用于从已知位置和方向的摄像机检测序列中定位远处物体。该方法通过迭代改进目标物体的3D位置和不确定性估计，能够有效应对图像分割模型产生的噪声。通过在3D模拟和无人机捕获的图像序列中验证，结果表明该方法在多目标场景中表现出色，能够在其他解决方案失效的情况下进行可靠定位。该方法的灵活性使其能够适应新的任务，特别是在无人机监测野火等安全关键任务中。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning",
            "authors": "Yufan Mao,Hanjing Ye,Wenlong Dong,Chengjie Zhang,Hong Zhang",
            "subjects": "Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20754",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20754",
            "arxiv_html_link": "https://arxiv.org/html/2509.20754v1",
            "abstract": "Navigating complex environments requires robots to effectively store observations as memories and leverage them to answer human queries about spatial locations—a critical yet underexplored research challenge. While prior work has made progress in constructing robotic memory, few have addressed the principled mechanisms needed for efficient memory retrieval and integration. To bridge this gap, we propose Meta-Memory, a large language model (LLM)-driven agent that constructs a high-density memory representation of the environment. The key innovation of Meta-Memory lies in its capacity to retrieve and integrate relevant memories through joint reasoning over semantic and spatial modalities in response to natural language location queries, thereby empowering robots with robust and accurate spatial reasoning capabilities. To evaluate its performance, we introduce SpaceLocQA, a large-scale dataset encompassing diverse real-world spatial question-answering scenarios. Experimental results show that Meta-Memory significantly outperforms state-of-the-art methods on both the SpaceLocQA and the public NaVQA benchmarks. Furthermore, we successfully deployed Meta-Memory on real-world robotic platforms, demonstrating its practical utility in complex environments. Project page: https://itsbaymax.github.io/meta-memory.github.io/.",
            "introduction": "A critical capability in robot navigation is the ability to understand and reason about human queries in order to identify and provide spatial locations as navigation destinations. This requires robots to construct a comprehensive memory from their sensory observations and to perform effective retrieval and reasoning over this memory. However, due to the complexity of real-world environments, building a complete and coherent visual representation from sequential observations remains a significant challenge. Even more demanding is the task of effectively utilizing the constructed memory for accurate spatial reasoning. In this work, we formalize these challenges as the Spatial Localization Question-Answering (SLQA) task—aiming to enable robots to build a holistic memory of large, complex environments and leverage it to answer diverse natural language queries about spatial relationships and locations. To the best of our knowledge, this is the first work to formally define the SLQA task.\n\nCurrent approaches to robot memory construction typically rely on captions generated by Vision-Language Models (VLMs) [1, 2, 3] or semantic embeddings extracted from foundational models [4, 5]. These low-level representations are then used to build higher-level structures such as 3D scene graphs [6, 7, 8, 9], semantic maps [10, 11, 12], or structured databases [13]. However, this paradigm faces several critical limitations.\n\nFirst, significant information loss occurs during the encoding process. Even highly detailed captions and semantic embeddings inevitably fail to capture the full richness of raw sensory observations, leading to incomplete and potentially ambiguous memory representations.\n\nMore importantly, there is a notable lack of principled mechanisms for effective memory retrieval and integration—a core requirement for the SLQA task. Existing methods often treat memories as isolated fragments, lacking coherent organization or contextual linking. While some approaches, such as 3D scene graphs [14, 15] or topological maps [16], impose graph-based structures to encode spatial and semantic relationships, they still struggle with complex, multi-hop spatial reasoning. For instance, when a stranger on campus asks, “Where’s the nearest coffee shop southwest of here?”, humans effortlessly retrieve the relevant memories, plan a route, and translate it into directions—an intuitive demonstration of the cognitive map [17] in action. In contrast, current robotic systems remain far from achieving such flexible, context-aware spatial understanding.\n\nRecent work [18] demonstrates that explicitly generating cognitive maps can significantly enhance the spatial reasoning capabilities of Multimodal Large Language Models (MLLMs). Inspired by this insight, we propose Meta-Memory, an LLM-based agent designed for the SLQA task. Our core idea is to enable the LLM agent to dynamically generate task-specific cognitive maps through comprehensive semantic and spatial retrieval over a rich, structured memory repository. To construct this memory system, we store the robot’s raw sensory observations—comprising images and corresponding positions of the robot—as semantic-spatial memories. Building upon this foundation, the agent is equipped with two memory retrieval tools (semantic and spatial) and one memory integration tool. These tools facilitate thorough and fine-grained memory access, enabling the integration tool to synthesize a tailored cognitive map for each query. This structured representation strengthens the agent’s spatial reasoning, leading to more accurate and contextually grounded responses. As illustrated in Fig. 1, we present a complete reasoning pipeline of Meta-Memory, from perception and memory retrieval to cognitive map generation and response.\n\nTo evaluate our method, we conduct experiments on the spatial position questions from the NaVQA dataset[13]. To more rigorously assess the performance of various approaches on SLQA, we introduce SpaceLocQA, a new benchmark dataset. SpaceLocQA contains a broader and more diverse collection of real-world human spatial queries, enabling a comprehensive evaluation of a robot’s ability to effectively construct, retrieve, and integrate its memories for accurate spatial inference.\n\nThe key contributions of this paper are:\n\nMeta-Memory, an LLM agent capable of comprehensively retrieving the constructed memories and effectively integrating the retrieved memories for spatial reasoning.\n\nSpaceLocQA, a comprehensive spatial-localization dataset that encompasses diverse, realistic human location queries.\n\nWe deploy Meta-Memory on a physical robot and demonstrate its practical success at retrieving and integrating semantic-spatial memories in real-world environments.\n\n1. Meta-Memory, an LLM agent capable of comprehensively retrieving the constructed memories and effectively integrating the retrieved memories for spatial reasoning.\n\n2. SpaceLocQA, a comprehensive spatial-localization dataset that encompasses diverse, realistic human location queries.\n\n3. We deploy Meta-Memory on a physical robot and demonstrate its practical success at retrieving and integrating semantic-spatial memories in real-world environments.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效存储和检索机器人在复杂环境中的观察记忆。  \n2. 如何利用构建的记忆进行准确的空间推理以回答人类查询。  \n3. 如何解决现有方法在记忆检索和整合中的缺陷。  \n\n【用了什么创新方法】  \n提出了Meta-Memory，一个基于大语言模型（LLM）的智能体，能够构建高密度的环境记忆表示。该方法通过对语义和空间模态的联合推理，动态生成任务特定的认知地图，从而增强机器人的空间推理能力。实验结果表明，Meta-Memory在SpaceLocQA和NaVQA基准测试中显著优于现有方法，并成功在真实机器人平台上部署，展示了其在复杂环境中的实用性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration",
            "authors": "Yiyuan Pan,Zhe Liu,Hesheng Wang",
            "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20648",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20648",
            "arxiv_html_link": "https://arxiv.org/html/2509.20648v1",
            "abstract": "Autonomous exploration in complex multi-agent reinforcement learning (MARL) with sparse rewards critically depends on providing agents with effective intrinsic motivation. While artificial curiosity offers a powerful self-supervised signal, it often confuses environmental stochasticity with meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform novelty bias, treating all unexpected observations equally. However, peer behavior novelty, which encode latent task dynamics, are often overlooked, resulting in suboptimal exploration in decentralized, communication-free MARL settings. To this end, inspired by how human children adaptively calibrate their own exploratory behaviors via observing peers, we propose a novel approach to enhance multi-agent exploration. We introduce Cermic, a principled framework that empowers agents to robustly filter noisy surprise signals and guide exploration by dynamically calibrating their intrinsic curiosity with inferred multi-agent context. Additionally, Cermic generates theoretically-grounded intrinsic rewards, encouraging agents to explore state transitions with high information gain. We evaluate Cermic on benchmark suites including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that exploration with Cermic significantly outperforms SoTA algorithms in sparse-reward environments.",
            "introduction": "Achieving effective exploration in complex Multi-Agent Reinforcement Learning (MARL) settings, particularly those characterized by sparse rewards and partial observability, remains a formidable scientific challenge [7, 43]. Intrinsic motivation, instantiated as artificial curiosity, has emerged as a key ingredient for unlocking autonomous learning by providing self-supervised signals in the absence of immediate extrinsic feedback [29, 38]. This internal drive enables agents to acquire skills and knowledge that support robust, adaptive intelligence.\n\nHowever, such novelty-seeking algorithms is susceptible to the stochastic environment dynamics or other unlearnable noises (the “Noisy-TV” problem) [23]. Existing algorithms mitigate this challenge through uncertainty quantification or by exploiting global information in multi-agent systems. However, these strategies prove insufficient for intelligent agents, particularly heterogeneous ones or those in large-scale systems: Firstly, such agents frequently encounter severe partial observability, rendering inaccurate uncertainty estimates due to insufficient replay experiences [20]; Secondly, in decentralized execution without effective communication, agents struggle to form accurate beliefs about others’ latent states, undermining methods that presuppose shared inter-agent information. [13]. Altogether, these limitations highlight a critical need for exploration mechanisms that are robust to partial observable and communication-less environments.\n\nInsights from human cognitive development suggest a pathway forward: children rapidly adapt to new social games not only through solo trial-and-error, but also by observing peers, inferring intentions, and selectively imitating successful strategies [42, 16]. Such form of social learning, often driven by an innate curiosity about ‘why’ others act as they do, allows for swift coordination and an understanding of task dynamics, even without complete information or explicit instruction [21, 30]. Naturally, the success of this human-centric learning process motivates translating its core principles to Multi-Agent Systems (MAS). Therefore, this paper seeks to answer:\n\nTo this end, we propose Curiosity Enhancement via Robust Multi-Agent Intention Calibration (Cermic), a modular, plug-and-play component designed to augment existing MARL exploration algorithms. Based on the Information Bottleneck (IB) principle [41, 40], Cermic learns a multi-agent contextualized exploratory representation that steers exploration toward semantically meaningful novelty, and filters unpredictable and spurious novelty. Specifically, it incorporates a graph-based module to model the inferred intentions of surrounding agents and use the context to calibrate raw individual curiosity signal at a given coverage level. At each episode, Cermic yields a loss for self-training and a theoretically-grounded intrinsic reward for exploration. We empirically validate Cermic by integrating it with various MARL algorithms and evaluating its performance across challenging benchmark suites. In summary, our contributions are threefold:\n\nWe introduce Cermic, a novel framework that empowers MARL agents with socially contextualized curiosity. Inspired by developmental psychology, this offers a novel perspective on the crucial challenges of effective exploration in sparse-reward settings.\n\nWe propose a robust and controllable multi-agent calibration mechanism in challenging partially observable and communication-limited environments. Cermic allows for adaptive tuning based on the learned reliability of the intention graph, effectively dampening exploration instability often plaguing vanilla novelty-seeking agents.\n\nWe deliver Cermic as a lightweight, readily integrable module and demonstrate consistent gains over strong baselines across standard benchmarks under sparse rewards.\n\n1. We introduce Cermic, a novel framework that empowers MARL agents with socially contextualized curiosity. Inspired by developmental psychology, this offers a novel perspective on the crucial challenges of effective exploration in sparse-reward settings.\n\n2. We propose a robust and controllable multi-agent calibration mechanism in challenging partially observable and communication-limited environments. Cermic allows for adaptive tuning based on the learned reliability of the intention graph, effectively dampening exploration instability often plaguing vanilla novelty-seeking agents.\n\n3. We deliver Cermic as a lightweight, readily integrable module and demonstrate consistent gains over strong baselines across standard benchmarks under sparse rewards.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂的多智能体强化学习（MARL）环境中实现有效的探索，尤其是在稀疏奖励的情况下。  \n2. 现有的好奇心机制如何受到环境随机性和统一新奇偏见的影响，导致探索效率低下。  \n3. 如何通过观察同伴行为来增强智能体的内在动机，从而改善探索策略。  \n\n【用了什么创新方法】  \n提出了一种名为Cermic的框架，通过动态校准多智能体的内在好奇心，来增强探索能力。该方法基于信息瓶颈原则，利用图形模块建模周围智能体的意图，并通过上下文信息来过滤噪声信号。Cermic生成理论基础的内在奖励，鼓励智能体探索具有高信息增益的状态转移。实验证明，Cermic在稀疏奖励环境中显著优于现有的最先进算法。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Large Pre-Trained Models for Bimanual Manipulation in 3D",
            "authors": "Hanna Yurchyk,Wei-Di Chang,Gregory Dudek,David Meger",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid Robots",
            "pdf_link": "https://arxiv.org/pdf/2509.20579",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20579",
            "arxiv_html_link": "https://arxiv.org/html/2509.20579v1",
            "abstract": "We investigate the integration of attention maps from a pre-trained Vision Transformer into voxel representations to enhance bimanual robotic manipulation. Specifically, we extract attention maps from DINOv2, a self-supervised ViT model, and interpret them as pixel-level saliency scores over RGB images. These maps are lifted into a 3D voxel grid, resulting in voxel-level semantic cues that are incorporated into a behavior cloning policy. When integrated into a state-of-the-art voxel-based policy, our attention-guided featurization yields an average absolute improvement of 8.2% and a relative gain of 21.9% across all tasks in the RLBench bimanual benchmark.",
            "introduction": "Robotic manipulation remains a core challenge in intelligent robotics [1, 2]. Even in well-structured settings, reliable manipulation requires seamless integration of visual perception, geometric reasoning, task planning, and closed-loop control [3]. These challenges are accentuated in bimanual manipulation, where two robot arms must operate in close coordination while sharing the same workspace. Bimanual systems unlock interactions beyond those of single-arm platforms, including simultaneous grasping, handovers, and in-hand re-orientation of a manipulated object [4]. Depending on the system, each arm or a global agent must reason about both the global scene and the fine-grained affordances of the object it handles, while coordinating with the other arm. Importantly, bimanual dexterity is crucial for humanoid and mobile robots with a camera and a dual-arm set-up. Such robots will be expected to perform assistive tasks such as folding laundry [5, 6, 7], stocking shelves [8], and providing other assistance in unstructured human environments [9, 10, 3]. Humanoids and mobile platforms with manipulators must reason about the global scene while manipulating small objects precisely with each hand, without self-collision. Thus, they impose even stricter requirements on perception and control than conventional industrial manipulators.\n\nIn this paper, we study how semantic information from pre-trained Vision Transformers (ViTs) can enhance voxel-based representations for bimanual manipulation. Building on recent advances in visual reasoning, prior work has shown that features extracted from models like DINO [11] and CLIP [12] significantly improve downstream performance in tasks such as navigation [13] and localization [14], when lifted from 2D images into 3D representations. Since ViTs capture meaningful semantic cues from raw visual inputs, we hypothesize that embedding these features into structured 3D scene representations will improve performance in robotic manipulation.\n\nThis trend has also been explored in imitation learning for manipulation. For instance, Chang et al. [15] showed that DINO-derived keypoints can establish homeomorphic correspondences across object instances, improving generalization in grasping. Di Palo et al. [16] leveraged these keypoints to train a policy from a single demonstration that generalizes effectively to real-world deployment. More broadly, recent works demonstrate that augmenting geometric representations with semantic priors improves the effectiveness of both single and dual-arm policies [17, 18, 19, 20]. Our work continues on this trend by injecting ViT-derived semantic attention into voxel-based policies, improving task performance without modifying the downstream architecture.\n\nWe build on the voxel-based manipulation framework introduced by James et al. [21], who proposed the C2F-ARM policy for single-arm manipulation using coarse-to-fine spatial attention, along with the RLBench benchmark [22]. Shridhar et al. extended this line with PerAct [23], a transformer-based behavioral cloning (BC) agent operating on voxel inputs. More recent methods have further developed voxel policies: Act3D [17] introduced a continuous 3D feature field with coarse-to-fine attention over sampled 3D points, while RVT [24] proposed a multi-view transformer that fuses re-rendered viewpoints to increase robustness to camera viewpoint variations.\n\nIn the bimanual setting, VoxAct-B [25] and PerAct2 [26] extend RLBench to support dual-arm manipulation. We adopt VoxAct-B as our baseline since it incorporates several practical improvements such as role identifiers and the use of the Segment Anything Model (SAM) [27] to crop task-relevant regions.\n\nThese works support a broader insight: combining structured 3D scene representations with high-level semantic priors leads to more robust and generalizable manipulation policies. We propose a lightweight voxel featurization strategy that injects DINOv2 [28] derived semantic priors from a single attention head into voxel-based policy learning, generating per-voxel semantic cues for bimanual manipulation. Our main contributions are:\n\nA pre-processing method that injects ViT-derived attention features into 3D voxel grids with minimal modifications to existing policy frameworks.\n\nExperiments showing that, when combined with VoxAct-B [25], our method achieves an absolute improvement of 8.2% and a relative improvement of 21.9% across all tasks in the bimanual RLBench.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将预训练的视觉变换器（ViT）中的注意力图与体素表示结合，以增强双手机器人操作的性能？  \n2. 在双手操作中，如何有效地利用语义信息来提高行为克隆策略的表现？  \n\n【用了什么创新方法】  \n本研究提出了一种轻量级的体素特征化策略，通过将DINOv2模型提取的语义注意力特征注入到3D体素网格中，增强了双手操作的表现。具体流程包括从RGB图像中提取注意力图，并将其提升至3D体素网格，生成每个体素的语义线索。实验结果表明，该方法在与VoxAct-B结合时，平均绝对提升8.2%，相对提升21.9%，显著改善了在RLBench双手基准测试中的任务表现。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent",
            "authors": "Yandan Yang,Baoxiong Jia,Shujie Zhang,Siyuan Huang",
            "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "Accepted by NeurIPS 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.20414",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20414",
            "arxiv_html_link": "https://arxiv.org/html/2509.20414v1",
            "abstract": "Indoor scene synthesis has become increasingly important with the rise of Embodied AI, which requires 3D environments that are not only visually realistic but also physically plausible and functionally diverse. While recent approaches have advanced visual fidelity, they often remain constrained to fixed scene categories, lack sufficient object-level detail and physical consistency, and struggle to align with complex user instructions. In this work, we present SceneWeaver, a reflective agentic framework that unifies diverse scene synthesis paradigms through tool-based iterative refinement. At its core, SceneWeaver employs a language model-based planner to select from a suite of extensible scene generation tools, ranging from data-driven generative models to visual- and LLM-based methods, guided by self-evaluation of physical plausibility, visual realism, and semantic alignment with user input. This closed-loop reason-act-reflect design enables the agent to identify semantic inconsistencies, invoke targeted tools, and update the environment over successive iterations. Extensive experiments on both common and open-vocabulary room types demonstrate that SceneWeaver not only outperforms prior methods on physical, visual, and semantic metrics, but also generalizes effectively to complex scenes with diverse instructions, marking a step toward general-purpose 3D environment generation.",
            "introduction": "3D scene synthesis Qi_2018_CVPR ; tang2023diffuscene ; yang2024physcene ; wei2023legonet ; echoscene ; yang2025mmgdreamer ; ccelen2024design ; sun2024layoutvlm ; feng2024layoutgpt ; Yang_2024_CVPR ; fu2024anyhome  has been a long-standing research topic in computer vision and graphics, primarily focused on generating visually realistic 3D environments for applications such as interior design, virtual content creation, and gaming asset creation. With the recent rise of  embodied artificial intelligence (EAI), the scope of scene synthesis has naturally expanded to accommodate new functional demands procthor ; khanna2023hssd ; yang2024physcene . Beyond achieving visual realism, scenes are now expected to be physically interactable within simulators and precisely controllable in response to task-specific user instructions, particularly in constructing tailored environments for training and evaluating embodied agents. These extended requirements pose significant new challenges for 3D scene synthesis.\n\nDespite rapid progress, existing methods fall short of holistically addressing the requirements for realistic, controllable, and physically plausible scene synthesis, as summarized in Tab.˜1. Rule-based systems procthor ; infinigen2024indoors  ensure physical validity through hand-crafted constraints, but lack extensibility across diverse scene types and offer limited controllability due to their rigid, manually defined logic. Data-driven generative learning methods paschalidou2021atiss ; tang2023diffuscene ; yang2024physcene , while more flexible, are constrained by the scarcity of high-quality, scene-level 3D datasets (e.g., 3D-Front fu20213d ). As a result, they typically produce visually realistic scenes within pre-defined categories but generalize poorly to novel scene types or layout instructions. Methods based on Large Language Models approaches ccelen2024design ; sun2024layoutvlm ; feng2024layoutgpt ; Yang_2024_CVPR ; fu2024anyhome  offer stronger open-vocabulary understanding and semantic flexibility, yet often struggle with spatial reasoning and 3D awareness, resulting in physically implausible rearrangements. Collectively, these limitations highlight that no single approach is sufficient to meet the combined demands of realism, physical plausibility, and controllability. This motivates the need for a comprehensive and adaptable scene synthesis framework capable of synthesizing high-quality 3D scenes.\n\nInspired by recent advances in LLM-based agents, which demonstrate strong reasoning and planning capabilities in complex tasks, recent works in 3D scene synthesis have begun to move beyond monolithic approaches by decomposing the generation process into sequential compositions of modular synthesis components, forming multi-step pipelines coordinated by LLMs. A common strategy starts with generating coarse, scene-level layouts through interaction with LLMs Yang_2024_CVPR ; sun2024layoutvlm ; feng2024layoutgpt ; ccelen2024design , followed by progressive refinement using pre-trained 2D generative models or Multi-modal LLMs for asset generation wangarchitect ; zhou2024gala3d , object placement yu2025metascenes ; dai2024acdc ; ling2025scenethesis , and texture inpainting fu2024anyhome ; chen2024scenetex . While these pipelines leverage both the specialization of individual models and the semantic flexibility of MLLMs, they remain largely “static”, i.e., their planning and execution are governed by fixed prompts and hard-coded module invocation logic over a limited set of synthesis tools. This design overlooks the potential to couple reasoning with adaptive decision-making based on generation feedback, and the ability to seamlessly integrate diverse synthesis tools through a unified interface. As a result, these systems fall short of enabling self-refining and extensible agents, leaving the full potential of multi-modal foundation models underutilized.\n\nTo address the aforementioned challenges, we propose SceneWeaver, a reflective agentic framework that enables MLLMs to synthesize 3D scenes in a self-refining manner through a set of easily extensible tool interfaces. Specifically, SceneWeaver consists of two core components: 1) a standardized and extensible tool interface that abstracts diverse scene synthesis methods into modular tools operating at different levels of generation granularity; 2) a self-reflective planner that dynamically selects tools and iteratively refines the scene by reasoning over feedback from previous generations, while applying the planned modifications and enforcing physical plausibility with a physics-aware executor. This framework enables closed-loop, feedback-driven scene evolution, where the agent identifies areas for improvement, invokes appropriate tools, and updates the scene under physical constraints. Extensive experiments show that SceneWeaver achieves new state-of-the-art across a broad range of scene types and open-vocabulary instructions, demonstrating strong visual realism, physical plausibility, and precision in instruction following. We also provide ablation studies showing that the self-refining design is critical to achieving high-quality scene synthesis and that integrating diverse tools leads to significant performance improvement compared to monolithic approaches. In summary, our contributions are as follows:\n\nWe propose SceneWeaver, the first reflective agentic framework for 3D scene synthesis, enabling MLLMs to iteratively refine scenes through feedback-driven planning with modular tools.\n\nWe propose SceneWeaver, the first reflective agentic framework for 3D scene synthesis, enabling MLLMs to iteratively refine scenes through feedback-driven planning with modular tools.\n\nSceneWeaver introduces a comprehensive reason-act-reflect paradigm that formalizes the planner’s decision making, reflection, and action protocols, along with a standardized and extensible tool interface for synergizing diverse scene synthesis methods based on their respective strengths.\n\nExtensive experiments on open-vocabulary scene synthesis demonstrate that SceneWeaver outperforms existing methods in both visual realism, physical plausibility, and instruction following. We also provide meticulously designed ablation studies to highlight the effectiveness of the proposed reflective agentic framework.\n\n1. We propose SceneWeaver, the first reflective agentic framework for 3D scene synthesis, enabling MLLMs to iteratively refine scenes through feedback-driven planning with modular tools.\n\n2. SceneWeaver introduces a comprehensive reason-act-reflect paradigm that formalizes the planner’s decision making, reflection, and action protocols, along with a standardized and extensible tool interface for synergizing diverse scene synthesis methods based on their respective strengths.\n\n3. Extensive experiments on open-vocabulary scene synthesis demonstrate that SceneWeaver outperforms existing methods in both visual realism, physical plausibility, and instruction following. We also provide meticulously designed ablation studies to highlight the effectiveness of the proposed reflective agentic framework.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现高质量的3D场景合成，满足视觉真实、物理可行性和功能多样性的要求？  \n2. 现有方法在场景合成中缺乏灵活性和可扩展性，难以处理复杂用户指令。  \n3. 如何将多种合成工具有效整合，以实现自我反思和自我改进的场景生成？  \n\n【用了什么创新方法】  \n本文提出了SceneWeaver，一个反思性代理框架，通过工具驱动的迭代优化实现3D场景合成。核心方法包括一个基于语言模型的规划器，选择多种可扩展的场景生成工具，并通过自我评估物理可行性、视觉真实感和与用户输入的语义一致性来指导生成过程。该框架采用闭环的推理-行动-反思设计，使代理能够识别语义不一致，调用目标工具，并在多次迭代中更新环境。实验表明，SceneWeaver在物理、视觉和语义指标上超越了现有方法，并能有效泛化到复杂场景和多样指令，标志着朝向通用3D环境生成的一步。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment",
            "authors": "Binod Singh,Sayan Deb Sarkar,Iro Armeni",
            "subjects": "Graphics (cs.GR); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20401",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20401",
            "arxiv_html_link": "https://arxiv.org/html/2509.20401v1",
            "abstract": "Aligning 3D scene graphs is a crucial initial step for several applications in robot navigation and embodied perception. Current methods in 3D scene graph alignment often rely on single-modality point cloud data and struggle with incomplete or noisy input. We introduce SGAligner++, a cross-modal, language-aided framework for 3D scene graph alignment. Our method addresses the challenge of aligning partially overlapping scene observations across heterogeneous modalities by learning a unified joint embedding space, enabling accurate alignment even under low-overlap conditions and sensor noise. By employing lightweight unimodal encoders and attention-based fusion, SGAligner++ enhances scene understanding for tasks such as visual localization, 3D reconstruction, and navigation, while ensuring scalability and minimal computational overhead.\nExtensive evaluations on real-world datasets demonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40% on noisy real-world reconstructions, while enabling cross-modal generalization.",
            "introduction": "3D scene understanding is a foundational challenge in robotics and computer vision [1, 2] and serves as the basis for applications in mixed reality, robot navigation, and embodied perception. Recently, cross-modal approaches [3, 4] have gained significant attention due to their ability to bridge different types of data. In particular, tasks such as robot navigation [5], object-centric planning [6], and semantic SLAM [2] require accurate and consistent scene understanding across modalities. As robots operate in dynamic, real-world environments, multimodal fusion becomes crucial. Different sensory data–visual, depth, and textual information–capture distinct aspects of the scene. Fusing these modalities into a coherent spatial-semantic map enables better perception, decision-making, and interaction.\n\nThe 3D semantic scene graph is a structured representation that captures the context of the scene as an attributed and directed graph, enabling unified spatial understanding [7, 8, 9]. Recent work has explored the alignment of 3D scene graphs to support downstream tasks such as 3D registration [10, 11, 12] and visual localization [4]. These capabilities are especially important in robot navigation, for example, when a robot scanning an environment attempts to match current observations with a prior map generated from a different sensing modality. In such cases, alignment remains highly challenging, for instance, when fusing LiDAR scans with CAD-based graphs or grounding objects in a 3D point cloud using natural language descriptions. However, existing methods [11, 10] rely on single-modal (unimodal) sensor data, mainly point clouds, and a fixed label vocabulary for graph annotations. Thus, they struggle with incomplete reconstructions and lack the flexibility to handle text-grounded or multimodal scenes, motivating a lightweight, generalizable framework for cross-modal alignment under noise.\n\nTo address these challenges, we introduce SGAligner++, a method that fuses structural, geometric, and linguistic information into a unified representation space. This enables the model to reason about spatial relationships from language, capture rich semantic context, and resolve ambiguities using multimodal data. Our approach enables efficient, scalable alignment even in noisy or low-overlap settings. Its modular design supports new modalities, and language grounding ensures adaptability to diverse 3D environments. SGAligner++ learns a unified embedding space by representing objects through features from multiple modalities: point clouds, CAD meshes, text captions, and spatial referrals. Furthermore, SGAligner++ handles missing data, such as when sensor inputs vary across environments (e.g., when a robot uses visual data but lacks text referrals). Our approach ensures robust object representation despite such gaps.\n\nUnlike previous methods [10, 11, 4] that depend on fixed semantic labels and simplistic interpretation of object relationships in 3D scene graphs, SGAligner++ enables seamless multimodal scene graph integration that could be used in downstream tasks such as visual localization, 3D reconstruction, and navigation. The approach is fast, lightweight, and annotation-free, making it ideal for embodied perception. We summarize the contributions as follows:\n\nWe enable cross-modal alignment and generate a unified 3D scene graph to achieve semantic consistency.\n\nWe enable cross-modal alignment and generate a unified 3D scene graph to achieve semantic consistency.\n\nWe use lightweight unimodal encoders with attention-based fusion for robust matching on noisy input.\n\nWe achieve state-of-the-art performance while maintaining low runtime and memory overhead, ensuring scalable deployment.\n\nThe data and code will be made public upon acceptance.\n\n1. We enable cross-modal alignment and generate a unified 3D scene graph to achieve semantic consistency.\n\n2. We use lightweight unimodal encoders with attention-based fusion for robust matching on noisy input.\n\n3. We achieve state-of-the-art performance while maintaining low runtime and memory overhead, ensuring scalable deployment.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效对齐3D场景图以支持机器人导航和具身感知的应用。  \n2. 现有方法在处理不完整或噪声输入时的局限性。  \n3. 如何实现跨模态的3D场景图对齐以提高语义一致性。  \n\n【用了什么创新方法】  \nSGAligner++通过学习统一的联合嵌入空间，融合结构、几何和语言信息，解决了部分重叠场景观察的对齐问题。该方法采用轻量级的单模态编码器和基于注意力的融合技术，确保在低重叠和传感器噪声条件下的准确对齐。通过这种方式，SGAligner++在真实世界数据集上的评估显示，其在噪声较大的重建任务中性能提升高达40%。该框架的模块化设计支持新模态的集成，确保了在多样化3D环境中的适应性和鲁棒性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        }
    ],
    "2025-09-27": [],
    "2025-09-28": [],
    "2025-09-29": [
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation",
            "authors": "Chih Yao Hu,Yang-Sen Lin,Yuna Lee,Chih-Hai Su,Jie-Ying Lee,Shr-Ruei Tsai,Chin-Yang Lin,Kuan-Wen Chen,Tsung-Wei Ke,Yu-Lun Liu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "CoRL 2025. Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.22653",
            "code": "https://spf-web.pages.dev",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22653",
            "arxiv_html_link": "https://arxiv.org/html/2509.22653v1",
            "abstract": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs). SPF is capable of navigating to any goal based on any type of free-form instructions in any kind of environment. In contrast to existing VLM-based approaches that treat action prediction as a text generation task, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. SPF harnesses VLMs to decompose vague language instructions into iterative annotation of 2D waypoints on the input image. Along with the predicted traveling distance, SPF transforms predicted 2D waypoints into 3D displacement vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the traveling distance to facilitate more efficient navigation. Notably, SPF performs navigation in a closed-loop control manner, enabling UAVs to follow dynamic targets in dynamic environments. SPF sets a new state of the art in DRL simulation benchmark, outperforming the previous best method by an absolute margin of 63%. In extensive real-world evaluations, SPF outperforms strong baselines by a large margin. We also conduct comprehensive ablation studies to highlight the effectiveness of our design choice. Lastly, SPF shows remarkable generalization to different VLMs.\nProject page: https://spf-web.pages.dev",
            "introduction": "The rapid development of unmanned aerial vehicles (UAVs) has revolutionized applications from environmental monitoring to security patrol. However, autonomous UAV navigation remains challenging due to requirements for strong visual reasoning in unstructured environments, language understanding for user instructions, and high-level task planning with low-level action control [1]. These autonomous UAV navigation tasks are often framed as aerial vision-and-language (AVLN) tasks [2, 3].\n\nThe autonomous UAV navigation tasks are commonly framed as aerial vision-and-language (AVLN) tasks [2, 3]. Conventional methods primalily adopt end-to-end policy learning frameworks which consist of a text and vision encoder that maps language instructions and visual observations into latent representations, followed by a policy head that converts these representations into UAV actions [4, 5, 6, 7, 8, 9, 10]. The entire models are trained on a curated set of expert demonstrations [11, 12, 13, 14]. However, due to the limited scale and diversity of the training data, these methods fail to generalize to unseen environments or task instructions. In contrast, recent works explore a training-free direction that directly converts Vison Large Language Models (VLM) into AVLN policies [15, 16, 17, 18]. As VLMs are trained on large-scale internet data, these models have demonstrated not only rich common-sense knowledge of the world, strong capabilities in visual/language understanding, reasoning and planning, but also, strong generalization to novel environments and tasks [19, 20, 21].\n\nHow to repurpose VLMs that generate texts into embodied agents that generate physical actions has attracted increasing interest in robotics [22, 23, 24, 25], while the research direction is still underexplored in AVLN. Existing VLM-based approaches to AVLN build atop a direct solution, that considers action prediction simply as a text-generation task. VLMs are prompoted to output either continuous actions [16] or pre-defined skills [15, 17, 18], in terms of texts. Despite the simplicity of these methods, they have two obvious problems: (1) embodied agents need to execute fine-grained actions, while texts are not suitable to represent high-precision floating numbers; (2) these VLMs have not been trained on aerial navigation data to predict 3D actions for navigation. In contrast, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. Instead of predicting 3D actions directly, we harnesses VLMs to annotate 2D waypoints [26, 27, 28, 29] on the image, which do not require any domain knowledge of AVLN but general spatial understanding [30, 31]. As these 2D waypoints are grounded in the visual scene, they inherently contain precise action information. These 2D waypoints can then be transformed into 3D actions using the camera information.\n\nNotably, we do not introduce the concept of predicting 2D waypoints for action selection—similar ideas have been explored in both robot manipulation and navigation [28, 26, 27, 20]. For example, RT-Trajectory [26] leverages VLMs to directly label 2D waypoints on the image, which are then used by a separately trained policy network to predict corresponding actions. PIVOT [28], in contrast, samples multiple candidate actions as 2D waypoints and employs a VLM to select the most appropriate one for execution. In this work, we build on this general idea and adapt it to the AVLN setting. Our method requires no additional neural network training, yet it significantly outperforms PIVOT, which is also a training-free approach.\n\nWe introduce See, Point, Fly (SPF), a novel VLM-based AVLN framework that navigates to any goal based on any free-form instructions in any environment. At the core of our method is a VLM [20] that conditions on the current scene and language instructions, and outputs the 2D waypoints in terms of pixel locations. These 2D waypoints are unprojected into unit-length 3D positions based on the camera parameters. These 3D positions denote the relative 3D actions to the current UAV location. To enhance the navigation speed, we propose an adaptive controller module that adjust the scale of the actions based on the distance between the UAV and the target. Since our method naturally enables closed-loop control of the UAV, as shown in Fig. 1, UAVs are capable of following dynamic targets. Moreover, building atop VLMs, our method can easily tackle long-horizon and even ambiguous task instructions in a zero-shot manner.\n\nWe test SPF on a simulation and a real-world benchmark. Our method outperforms prior state-of-the-art, TypeFly [15] by a large margin. We show that our method works well across a wide range of tasks, including long-horizon, abstract, and dynamic navigation tasks. We also conduct an extensive ablation study to validate the effectiveness of each design choice.\n\nIn summary, our contributions are: (1) We propose a state-of-the-art AVLN framework that generalizes to novel scenes and free-form instructions; (2) We set a new state-of-the-art in the DRL simulator [32] simulation benchmark, outperforming prior SOTAs with a margin of 63%63\\% in success rate; (3) We set a new state of the art in the real-world benchmark, outperforming prior SOTAs with a margin of 82%82\\% in success rate.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的无人机导航方法在未见环境或任务指令上缺乏泛化能力。  \n2. 传统的基于文本生成的动作预测方法不适合高精度的动作执行。  \n3. 需要一种无需训练的框架来处理多样化的导航任务。  \n\n【提出了什么创新的方法】  \n本文提出了See, Point, Fly (SPF)框架，利用视觉语言模型（VLM）将模糊的语言指令分解为2D路径点的迭代注释。SPF通过将预测的2D路径点转换为3D位移向量，生成无人机的动作指令，并自适应调整旅行距离以提高导航效率。SPF在DRL模拟基准上设定了新的状态，成功率提高了63%。在实际评估中，SPF也显著超越了强基线，展示了对不同VLM的卓越泛化能力。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Pixel Motion Diffusion is What We Need for Robot Control",
            "authors": "E-Ro Nguyen,Yichi Zhang,Kanchana Ranasinghe,Xiang Li,Michael S. Ryoo",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22652",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22652",
            "arxiv_html_link": "https://arxiv.org/html/2509.22652v1",
            "abstract": "We present DAWN (Diffusion is All We Need for robot control), a unified diffusion-based framework for language-conditioned robotic manipulation that bridges high-level motion intent and low-level robot action via structured pixel motion representation. In DAWN, both the high-level and low-level controllers are modeled as diffusion processes, yielding a fully trainable, end-to-end system with interpretable intermediate motion abstractions.\nDAWN achieves state-of-the-art results on the challenging CALVIN benchmark, demonstrating strong multi-task performance, and further validates its effectiveness on MetaWorld. Despite the substantial domain gap between simulation and reality and limited real-world data, we demonstrate reliable real-world transfer with only minimal finetuning, illustrating the practical viability of diffusion-based motion abstractions for robotic control. Our results show the effectiveness of combining diffusion modeling with motion-centric representations as a strong baseline for scalable and robust robot learning. Project page: https://nero1342.github.io/DAWN/.",
            "introduction": "Multi-stage pixel or point tracking based methods have recently emerged as a promising direction for robot manipulation, offering interpretable intermediate pixel motion and modular control (Yuan et al., 2024a; Gao et al., 2024; Xu et al., 2024; Bharadhwaj et al., 2024b; a; Ranasinghe et al., 2025).\nHowever, despite their promise, approaches such as Im2Flow2Act (Xu et al., 2024), ATM (Wen et al., 2023), and LangToMo (Ranasinghe et al., 2025) still fall short of state-of-the-art vision-language action (VLA) models (Black et al., 2024a; Intelligence et al., 2025) and latent feature–based hierarchical methods (Hu et al., 2024; Nvidia et al., 2025) on established benchmarks.\nWe argue that this performance gap does not arise from limitations in the two-stage intermediate pixel-motion based framework itself. The high-level motion generator in these frameworks does not fully reflect recent advances in visual generative modeling (Ge et al., 2022; Kumari et al., 2023; Zhang et al., 2022; Ren et al., 2022; Chen et al., 2023), while the low-level controllers have not leveraged recent progress in diffusion-based action policies (Janner et al., 2022; Du et al., 2023a; Chi et al., 2023; Shridhar et al., 2024; Li et al., 2024a) in an optimal way.\n\nTo address these limitations, we introduce a two-stage diffusion-based visuomotor framework in which both the high-level and low-level controllers are instantiated as diffusion models and glued by explicit pixel motions as illustrated in Figure 1.\nThe high-level motion director, which is a latent diffusion module, takes current (multiview) visual observations and language instruction, and predicts desired dense pixel motion from a third-person view.\nThis pixel motion could be regarded as a structured intermediate representation of desired scene dynamics to accomplish the language instruction.\nThese pixel motion are then translated into executable actions through a diffusion-based policy head.\nWe highlight how intermediate pixel motion is grounded on visual inputs, endowing the intermediate representations with interpretability.\nTherein, we introduce Diffusion is All We Need for robot control (DAWN), which bridges the strengths of hierarchical motion decomposition and end-to-end visuomotor agents, while maintaining interpretability and modularity.\n\nOur framework illustrated in Figure 1 builds upon insights from prior hierarchical visuomotor approaches. VPP (Hu et al., 2024) employs a video diffusion model to extract predictive feature embeddings, which subsequently condition a downstream action policy. However, it operates in RGB space (with no motion specific representation) and uses the video diffusion model as a feature extractor as opposed to iterative denoising of motion features. LangToMo (Ranasinghe et al., 2025) predicts pixel-space motion trajectories from language instructions, but its high-level motion director uses pixel-level diffusion, limiting the resolution of the generated motion representation and training scalability. Its low-level controller is based on weaker ViT architectures or hand-crafted heuristics. In contrast, DAWN utilizes an efficient pretrained latent diffusion model for motion generation with iterative denoising during inference, and a strong diffusion-based action expert, thus benefiting from powerful vision and language models.\n\nWe evaluate our method on two challenging simulation benchmarks—CALVIN (Mees et al., 2022) and MetaWorld (Yu et al., 2019), as well as across real-world environments with only very limited in-domain training data.\n\nOur results demonstrate that, despite using limited data and substantially smaller model capacity, our method can match or even surpass state-of-the-art VLA models by leveraging explicit structured pixel motion and the strengths of diverse pretrained models, highlighting its high data efficiency.\n\nOur key contributions are as follows:\n\nWe propose DAWN, a two-stage\ndiffusion-based framework that generates structured intermediate pixel motion as an efficient language-conditioned visuomotor policy.\n\nDespite relying on limited data and a substantially smaller model capacity, we achieve competitive or even state-of-the-art performance on CALVIN, MetaWorld, and real-world benchmarks.\n\nOur approach is explicitly designed to leverage pretrained vision and language models, enabling highly data-efficient transfer across domains, while providing interpretability and modularity.",
            "llm_summary": "【论文的motivation是什么】  \n1. 当前的机器人控制方法在高层和低层控制器之间缺乏有效的连接，导致性能不足。  \n2. 现有的视觉生成模型和扩散模型未能充分利用，造成了性能差距。  \n3. 需要一种能够有效桥接高层意图和低层动作的框架，以提高机器人操控的可解释性和模块化。  \n\n【提出了什么创新的方法】  \n提出了一种名为DAWN的两阶段扩散基础框架，通过结构化像素运动表示连接语言条件的机器人操控的高层和低层控制器。高层运动生成器使用潜在扩散模块，从多视角视觉观察和语言指令中预测所需的密集像素运动。这些像素运动被转化为可执行动作，通过扩散基础策略头实现。该方法在CALVIN和MetaWorld基准上取得了最先进的结果，并在现实环境中展示了良好的迁移能力，尽管只使用了有限的训练数据，表明扩散基础运动抽象在机器人控制中的实际可行性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search",
            "authors": "Wenkai Guo,Guanxing Lu,Haoyuan Deng,Zhenyu Wu,Yansong Tang,Ziwei Wang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22643",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22643",
            "arxiv_html_link": "https://arxiv.org/html/2509.22643v1",
            "abstract": "Vision-Language-Action models (VLAs) achieve strong performance in general robotic manipulation tasks by scaling imitation learning. However, existing VLAs are limited to predicting short-sighted next-action, which struggle with long-horizon trajectory tasks due to incremental deviations.\nTo address this problem, we propose a plug-in framework named VLA-Reasoner that effectively empowers off-the-shelf VLAs with the capability of foreseeing future states via test-time scaling.\nSpecifically, VLA-Reasoner samples and rolls out possible action trajectories where involved actions are rationales to generate future states via a world model, which enables VLA-Reasoner to foresee and reason potential outcomes and search for the optimal actions.\nWe further leverage Monte Carlo Tree Search (MCTS) to improve search efficiency in large action spaces, where step-wise VLA predictions seed the root.\nMeanwhile, we introduce a confidence sampling mechanism based on Kernel Density Estimation (KDE), to enable efficient exploration in MCTS without redundant VLA queries.\nWe evaluate intermediate states in MCTS via an offline reward shaping strategy, to score predicted futures and correct deviations with long-term feedback.\nWe conducted extensive experiments in both simulators and the real world, demonstrating that our proposed VLA-Reasoner achieves significant improvements over the state-of-the-art VLAs. Our method highlights a potential pathway toward scalable test-time computation of robotic manipulation.",
            "introduction": "Vision-Language-Action models (VLAs) [1, 2, 3] leverage the grounded perception and commonsense reasoning of large, pre-trained vision–language models (VLMs) to advance general-purpose robot manipulation. Within a supervised imitation learning paradigm, they map visual observations and natural-language instructions directly to sequences of low-level actions using extensive robot demonstration datasets [4, 5, 6]. By decoupling task specification from policy learning, VLAs adapt to diverse manipulation scenarios via language prompts and image conditions. Recent results show stronger generalization across object categories and environments, and reduced dependence on explicit task engineering, positioning VLAs as a promising route toward scalable embodied intelligence [2, 7].\n\nHowever, current VLAs also face critical limitations. As the action prediction of VLAs fundamentally relies on direct mappings from short-horizon environment states to actions, they remain fragile during deployment. This short-sighted prediction discards long-horizon sequential dependencies, becoming a primary cause of incremental deviations across tasks and environments. Consequently, the accuracy and exploration capability of VLAs are significantly constrained.\nThis raises a core question: \"Can VLAs explore the long-horizon future influence of actions at test time, and decide the optimal action?\"\n\nTo this end, we propose a plug-in framework named VLA-Reasoner that empowers off-the-shelf VLAs with the ability to foresee future states via test-time scaling.\nOur proposed VLA-Reasoner can effectively mitigate the incremental deviations of VLAs caused by the lack of considering future impact (Figure˜1).\nSpecifically, VLA-Reasoner samples and rolls out possible action trajectories to generate future states via a world model, where the future states and corresponding actions can reflect the potential outcomes.\nTo enhance search efficiency, we employ MCTS to handle the expansive action space, in which step-wise VLA predictions seed the root node.\nWe introduce a KDE-based confidence distribution that samples candidates in MCTS from an expert-like prior, reducing redundant VLA queries while preserving exploration.\nSince sparse task feedback arrives only at episode ending, we design an offline reward shaping strategy to evaluate intermediate states in MCTS, providing dense feedback signals that correct deviations with stable long-horizon guidance.\nVLA-Reasoner effectively improves the reasoning capability of VLAs in long-horizon trajectory tasks, through enabling structured exploration in expansive action spaces and foreseeing the potential outcomes of the current action.\n\nOur method significantly delivers consistent gains in both simulation and on real robots. On the LIBERO benchmark, wrapping a modest baseline VLA with VLA-Reasoner lifts it beyond competing VLAs. In real-world deployments, our approach achieves higher success rates compared to popular VLAs fine-tuned with a few demonstrations, indicating stronger generalization and adaptivity at test time. Our contributions are summarized as follows:\n\nWe propose a plug-in framework named VLA-Reasoner that empowers VLAs with structured reasoning to address their incremental deviations during deployment.\n\nWe propose a plug-in framework named VLA-Reasoner that empowers VLAs with structured reasoning to address their incremental deviations during deployment.\n\nWe adapt a modified test-time MCTS to search efficiently rather than just use it. We apply KDE for efficient plausible expansion, and provide an offline-based reward shaping method to evaluate intermediate states.\n\nWe conduct extensive experiments that validate the effectiveness of integrating MCTS into VLAs for test-time optimization. We also show the potential to achieve great real-world performance with a few data acquisitions.\n\n1. We propose a plug-in framework named VLA-Reasoner that empowers VLAs with structured reasoning to address their incremental deviations during deployment.\n\n2. We adapt a modified test-time MCTS to search efficiently rather than just use it. We apply KDE for efficient plausible expansion, and provide an offline-based reward shaping method to evaluate intermediate states.\n\n3. We conduct extensive experiments that validate the effectiveness of integrating MCTS into VLAs for test-time optimization. We also show the potential to achieve great real-world performance with a few data acquisitions.",
            "llm_summary": "【论文的motivation是什么】  \n1. 当前的Vision-Language-Action模型在长时间轨迹任务中面临短视预测的限制。  \n2. 现有模型在执行过程中容易产生增量偏差，影响准确性和探索能力。  \n3. 需要一种方法来探索动作的长远影响并决定最优行动。  \n\n【提出了什么创新的方法】  \n本文提出了一种名为VLA-Reasoner的插件框架，旨在通过测试时扩展赋予现有VLA模型预见未来状态的能力。该框架通过采样和展开可能的动作轨迹，利用世界模型生成未来状态，从而使VLA-Reasoner能够推测潜在结果并搜索最优动作。我们采用了蒙特卡洛树搜索（MCTS）来提高在大动作空间中的搜索效率，并引入基于核密度估计（KDE）的置信采样机制，以减少冗余的VLA查询。通过离线奖励塑形策略评估MCTS中的中间状态，提供密集反馈信号以纠正偏差。实验结果表明，VLA-Reasoner在模拟和真实机器人上均显著提升了性能，展示了其在可扩展机器人操作中的潜力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "WoW: Towards a World omniscient World model Through Embodied Interaction",
            "authors": "Xiaowei Chi,Peidong Jia,Chun-Kai Fan,Xiaozhu Ju,Weishi Mi,Kevin Zhang,Zhiyuan Qin,Wanxin Tian,Kuangzhi Ge,Hao Li,Zezhong Qian,Anthony Chen,Qiang Zhou,Yueru Jia,Jiaming Liu,Yong Dai,Qingpo Wuwu,Chengyu Bai,Yu-Kai Wang,Ying Li,Lizhang Chen,Yong Bao,Zhiyuan Jiang,Jiacheng Zhu,Kai Tang,Ruichuan An,Yulin Luo,Qiuxuan Feng,Siyuan Zhou,Chi-min Chan,Chengkai Hou,Wei Xue,Sirui Han,Yike Guo,Shanghang Zhang,Jian Tang",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22642",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22642",
            "arxiv_html_link": "https://arxiv.org/html/2509.22642v1",
            "abstract": "Humans develop an understanding of intuitive physics through active interaction with the world. This approach is in stark contrast to current video models, such as Sora, which rely on passive observation and therefore struggle with grasping physical causality. This observation leads to our central hypothesis: authentic physical intuition of world model must be grounded in extensive, causally rich interactions with the real world.\nTo test this hypothesis, we present WoW, a 14B-parameter generative world model trained on 2 million robot interaction trajectories. Our findings reveal that the model’s understanding of physics is a probabilistic distribution of plausible outcomes, leading to stochastic instabilities and physical hallucinations.\nFurthermore, we demonstrate that this emergent capability can be actively constrained toward physical realism by SOPHIA, where vision language model agents evaluate the DiT’s generated output and guide its refinement by iteratively evolving the language instruction.\nBesides, a co-trained Inverse Dynamics Model translates these refined plans into executable robotic actions, thus closing the imagination-to-action loop.\nWe establish WoWBench, a new benchmark focused on physical consistency and causal reasoning of video, where WoW achieves state-of-the-art performance of both human and autonomous evaluation, demonstrating strong ability on physical causality, collision dynamics, and object permanence.\nOur work provides the systematic evidence that large-scale, real-world interaction is a cornerstone for developing physical intuition in AI. Models, data, and benchmarks will be open-sourced in wow-world-model.github.io",
            "introduction": "“The ladder of causation has three rungs: seeing, doing, and imagining.”\n\nIn contrast, many recent advances in predictive models, particularly in video generation, is predicated on passive observation, a principle fundamentally distinct from active experimentation that fosters accurate causal understanding. While models like Sora (Brooks et al., 2024) and others (Wan et al., 2025) achieve stunning photorealism and demonstrate emergent physical intuition, this intuition remains brittle. Their training objective prioritizes modeling statistical correlations from internet-scale data over inferring the underlying causal mechanisms of physics. Consequently, their grasp of physical laws is often superficial. When tasked with scenarios requiring genuine physical reasoning, they can produce logically and physically inconsistent outcomes. These models master the appearance of our world, but the generative dynamics they learn are an approximation rather than an accurate representation.\n\nThis distinction motivates our core hypothesis, for an embodied model to develop genuine physical intuition, it must learn from large-scale, causally-rich, real-world interaction data, thereby lifting the generative model toward a world model (Ha & Schmidhuber, 2018; Agarwal et al., 2025). To further validate our approach, we first introduce SOPHIA, a novel architectural paradigm that couples the reasoning capabilities of a Vision Language Model (VLM) with the generative power of a Diffusion Transformer (DiT) (Peebles & Xie, 2023). We then present WoW, a concrete instantiation of this paradigm. WoW is a generative world model trained on a large-scale dataset of 2 million real-world robotic interaction trajectories, spanning 5275 tasks and 12 different robots. The objective of WoW is to directly synthesize pixel-level future predictions, learning to imagine and reason through generation itself.\n\nTo close the perception-to-action loop, we designed the Flow-Mask Inverse Dynamics Model (FM-IDM), which functions as the agent’s equivalent of the cerebellum and motor cortex. By analyzing the optical flow and scene context between the current state and the imagined next state, the FM-IDM infers the 7-DoF end-effector action necessary to enact the transition. This module grounds the agent’s imagination in physical reality, translating pixel-level futures into executable actions.\n\nTo empirically validate this complete perception-imagination-reflection-action cognitive architecture, we established WoWBench, a new benchmark focused on physical consistency and causal reasoning. WoWBench is composed of 4 core abilities and 20 sub-tasks, containing 606 samples, each with an initial image and a textual instruction. For a comprehensive evaluation, we comprehend 4 indispensable metrics: Video quality, Planning reasoning, Physical rules, and Instruction following. Our experiments demonstrate that our 14B-parameter WoW achieves SOTA performance on this benchmark, especially 96.53% on Instruction understanding, and 80.16% on Physical law, providing compelling evidence in support of our central hypothesis. To further verify the precision of our WoWBench, we conduct a human evaluation proving that WoWBench is highly correlated with human preference, and WoW achieves SOTA performance on both sides. Beyond its benchmark performance, WoW demonstrates versatility in broader applications. We show it is more than a simple generator, capable of enhancing VLM reasoning, serving as a physical simulator, and enabling 3D-aware representation learning.\n\nIn summary, we propose SOPHIA, a paradigm for developing embodied intelligence through a data-driven feedback loop. This approach involves deploying capable models to collect large-scale corrective feedback from physical interactions, a process that drives a continuous cycle of improvement. Our model, WoW, serves as a powerful instantiation of this paradigm, representing a significant advance from passive video models to an embodied world model that closes the perception-imagination-reflection-action loop. Our main contributions are as follows.\n\nA Unified Architecture for Imagination and Action. We introduce an embodied world model WoW, which instantiates a novel self-optimization framework SOPHIA for imagining physically plausible futures, and incorporates a Flow-Mask Inverse Dynamics Model that infers the corresponding executable actions.\n\nSelf-Supervised Feature Alignment. We are the first to integrate powerful, pre-trained self-supervised visual features into the backbone of a diffusion-based world model. This novel approach significantly boosts the model’s perceptual capabilities, accelerates training convergence, and improves the fidelity and physical consistency of generated futures.\n\nAn Interaction Benchmark. We propose WoWBench, a new public benchmark designed to evaluate the physical consistency and action-generation capabilities of world models. Comprising 606 diverse, high-quality robot trajectories, the benchmark facilitates a rigorous performance comparison between existing methods and proposed WoW across 4 core metrics and 20 associated tasks.\n\nPostraining Application Discussion. We demonstrate that WoW is more than a generator, showcasing its versatility across a range of downstream applications. These applications include synthesizing novel views, generating trajectory-guided videos, producing action-conditioned videos for robot manipulation, enhancing visual style transfer and improving VLM task planning at test-time.\n\nA Scaling Analysis and Open-Source Models. We perform a systematic scaling analysis of our architecture up to 14B parameters, uncovering nascent capabilities and potential precursors to emergence in physical reasoning. This provides strong empirical evidence for the scaling hypothesis in this domain. We will release all trained model checkpoints to provide a foundation for future research in the embodied world model.\n\n1. A Unified Architecture for Imagination and Action. We introduce an embodied world model WoW, which instantiates a novel self-optimization framework SOPHIA for imagining physically plausible futures, and incorporates a Flow-Mask Inverse Dynamics Model that infers the corresponding executable actions.\n\n2. Self-Supervised Feature Alignment. We are the first to integrate powerful, pre-trained self-supervised visual features into the backbone of a diffusion-based world model. This novel approach significantly boosts the model’s perceptual capabilities, accelerates training convergence, and improves the fidelity and physical consistency of generated futures.\n\n3. An Interaction Benchmark. We propose WoWBench, a new public benchmark designed to evaluate the physical consistency and action-generation capabilities of world models. Comprising 606 diverse, high-quality robot trajectories, the benchmark facilitates a rigorous performance comparison between existing methods and proposed WoW across 4 core metrics and 20 associated tasks.\n\n4. Postraining Application Discussion. We demonstrate that WoW is more than a generator, showcasing its versatility across a range of downstream applications. These applications include synthesizing novel views, generating trajectory-guided videos, producing action-conditioned videos for robot manipulation, enhancing visual style transfer and improving VLM task planning at test-time.\n\n5. A Scaling Analysis and Open-Source Models. We perform a systematic scaling analysis of our architecture up to 14B parameters, uncovering nascent capabilities and potential precursors to emergence in physical reasoning. This provides strong empirical evidence for the scaling hypothesis in this domain. We will release all trained model checkpoints to provide a foundation for future research in the embodied world model.",
            "llm_summary": "【论文的motivation是什么】  \n1. 当前视频模型依赖被动观察，难以理解物理因果关系。  \n2. 真实的物理直觉需要通过大量的因果丰富的真实世界交互数据来学习。  \n3. 现有模型在处理需要真实物理推理的场景时，常常产生逻辑和物理不一致的结果。  \n\n【提出了什么创新的方法】  \n本文提出了WoW，一个14B参数的生成世界模型，基于200万机器人交互轨迹进行训练。通过引入SOPHIA架构，将视觉语言模型与扩散变换器结合，WoW能够生成物理上合理的未来预测，并通过Flow-Mask逆动力学模型将这些预测转化为可执行的机器人动作。实验结果表明，WoW在物理因果性、碰撞动态和物体持久性等方面表现出色，尤其在WoWBench基准测试中达到了人类和自主评估的最先进性能，验证了大规模真实交互对AI物理直觉发展的重要性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation",
            "authors": "Yuan Xu,Jiabing Yang,Xiaofeng Wang,Yixiang Chen,Zheng Zhu,Bowen Fang,Guan Huang,Xinze Chen,Yun Ye,Qiang Zhang,Peiyan Li,Xiangnan Wu,Kai Wang,Bing Zhan,Shuo Lu,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22578",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22578",
            "arxiv_html_link": "https://arxiv.org/html/2509.22578v1",
            "abstract": "Imitation learning based policies perform well in robotic manipulation, but they often degrade under egocentric viewpoint shifts when trained from a single egocentric viewpoint. To address this issue, we present EgoDemoGen, a framework that generates paired novel egocentric demonstrations by retargeting actions in the novel egocentric frame and synthesizing the corresponding egocentric observation videos with proposed generative video repair model EgoViewTransfer, which is conditioned by a novel-viewpoint reprojected scene video and a robot-only video rendered from the retargeted joint actions. EgoViewTransfer is finetuned from a pretrained video generation model using self-supervised double reprojection strategy. We evaluate EgoDemoGen on both simulation (RoboTwin2.0) and real-world robot. After training with a mixture of EgoDemoGen-generated novel egocentric demonstrations and original standard egocentric demonstrations, policy success rate improves absolutely by +17.0% for standard egocentric viewpoint and by +17.7% for novel egocentric viewpoints in simulation. On real-world robot, the absolute improvements are +18.3% and +25.8%. Moreover, performance continues to improve as the proportion of EgoDemoGen-generated demonstrations increases, with diminishing returns. These results demonstrate that EgoDemoGen provides a practical route to egocentric viewpoint-robust robotic manipulation.",
            "introduction": "Imitation learning has emerged as a powerful paradigm in robotic manipulation, enabling end-to-end visuomotor policies that map raw observations to control actions. Recent imitation learning policies including Vision-Language-Action models (Chi et al., 2023; Zhao et al., 2023; Zitkovich et al., 2023; Ghosh et al., 2024; Liu et al., 2024; O’Neill et al., 2024; Black et al., 2024) have demonstrated remarkable performance when trained on large and diverse demonstration datasets (Wu et al., 2024; Khazatsky et al., 2024; Walke et al., 2023; O’Neill et al., 2024). However, these policies remain sensitive to distribution shift: policies trained or finetuned from a single egocentric viewpoint often fail to generalize to unseen egocentric viewpoints (Tian et al., 2025; Xing et al., 2025), shown in Figure 1(a). This limitation underscores the need to increase viewpoint diversity.\n\nGenerating novel egocentric viewpoint demonstrations serves as one effective solution to this problem. Existing efforts to mitigate this issue can be broadly categorized into two lines of work. One line of works focus on novel viewpoint synthesis using techniques such as point cloud rendering, 3D reconstruction, or image generation models (Sargent et al., 2024; Xue et al., 2025; Yang et al., 2025). These approaches synthesize novel visual observations but maintain original actions, leading to visual-action mismatch in egocentric setting, shown in Figure 1(b). Another line of works employ world models or action-conditioned video generation to target prediction or planning, rather than observation-action paired demonstration generation (Wang et al., 2025a; Rigter et al., 2024; Bruce et al., 2024; Luo & Du, 2024; Hafner et al., 2025). Moreover, these works do not explicitly model changes in the egocentric viewpoint caused by robot motion. Generating demonstrations from a novel egocentric viewpoint requires coherent synthesis of both the visual observations and the corresponding actions.\n\nOur key insight addresses this fundamental gap: generating novel egocentric demonstrations requires not only synthesizing realistic observations from novel egocentric viewpoints, but also retargeting the original actions to align with the shifted viewpoint. This entails tackling two core challenges: (1) producing kinematically feasible robot actions that achieve the task under the novel egocentric viewpoint, and (2) generating realistic, temporally consistent observation videos that match these retargeted actions. Crucially, the generated demonstrations must preserve the style and intent of the original demonstrations while ensuring visual–action alignment.\n\nTo tackle these challenges, we propose EgoDemoGen, a novel framework for generating demonstrations from novel egocentric viewpoints. First, on the action side, we perform kinematics-based action retargeting to produce joint actions corresponding to novel egocentric viewpoint. Second, on the visual side, we propose EgoViewTransfer, a generative vide repair model that fuses the reprojected scene videos with the robot motion videos rendered under the retargeted actions and generate novel egocentric observation videos. We conducted experiments in the RoboTwin2.0 (Chen et al., 2025b) simulation environment and on a real-world dual-arm robot to evaluate the effectiveness of the generated demonstrations.\n\nOur main contributions can be summarized as follows:\n\nWe present EgoDemoGen, a framework that generates novel egocentric demonstrations with paired retargeted actions and egocentric observation videos, improving policy generalization to egocentric viewpoint shifts.\n\nWe generate novel demonstrations by retargeting actions in the novel egocentric frame and synthesizing corresponding observation videos from a novel-viewpoint reprojected scene video and a robot-only video rendered from the retargeted joint actions. The generated paired demonstrations are used to train downstream policies.\n\nWe introduce EgoViewTransfer, a generative video repair model finetuned from a pretrained video generation model with a double reprojection strategy, which fuses reprojected scene video and rendered robot video to synthesize consistent, realistic egocentric observation video.\n\nExperiments on simulation (RoboTwin2.0) and real-world robot show policy success rate absolute improvements of +17.0% (standard egocentric viewpoint) and +17.7% (novel egocentric viewpoints) in simulation when incorporating demonstrations generated by EgoDemoGen into the training mixture, and +18.3% and +25.8% on real-world robot. Moreover, performance continues to improve as the proportion of EgoDemoGen-generated demonstrations increases, with diminishing returns.\n\n1. We present EgoDemoGen, a framework that generates novel egocentric demonstrations with paired retargeted actions and egocentric observation videos, improving policy generalization to egocentric viewpoint shifts.\n\n2. We generate novel demonstrations by retargeting actions in the novel egocentric frame and synthesizing corresponding observation videos from a novel-viewpoint reprojected scene video and a robot-only video rendered from the retargeted joint actions. The generated paired demonstrations are used to train downstream policies.\n\n3. We introduce EgoViewTransfer, a generative video repair model finetuned from a pretrained video generation model with a double reprojection strategy, which fuses reprojected scene video and rendered robot video to synthesize consistent, realistic egocentric observation video.\n\n4. Experiments on simulation (RoboTwin2.0) and real-world robot show policy success rate absolute improvements of +17.0% (standard egocentric viewpoint) and +17.7% (novel egocentric viewpoints) in simulation when incorporating demonstrations generated by EgoDemoGen into the training mixture, and +18.3% and +25.8% on real-world robot. Moreover, performance continues to improve as the proportion of EgoDemoGen-generated demonstrations increases, with diminishing returns.",
            "llm_summary": "【论文的motivation是什么】  \n1. Imitation learning policies struggle to generalize across different egocentric viewpoints.  \n2. Existing methods either synthesize novel viewpoints without retargeting actions or focus on prediction rather than demonstration generation.  \n3. There is a need for coherent synthesis of visual observations and corresponding actions in novel egocentric contexts.  \n\n【提出了什么创新的方法】  \nEgoDemoGen is introduced as a framework that generates novel egocentric demonstrations by retargeting actions and synthesizing corresponding observation videos. The process involves kinematics-based action retargeting to align actions with novel viewpoints and utilizes EgoViewTransfer, a generative video repair model, to create realistic observation videos. This approach significantly improves policy generalization to egocentric viewpoint shifts, with notable success rate increases of +17.0% and +17.7% in simulation, and +18.3% and +25.8% on real robots. Performance continues to improve with more EgoDemoGen-generated demonstrations, demonstrating its effectiveness in enhancing robotic manipulation.  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data",
            "authors": "Farida Mohsen,Ali Safa",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22573",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22573",
            "arxiv_html_link": "https://arxiv.org/html/2509.22573v1",
            "abstract": "Efficiently detecting human intent to interact with ubiquitous robots is crucial for effective human-robot interaction (HRI) and collaboration. Over the past decade, deep learning has gained traction in this field, with most existing approaches relying on multimodal inputs, such as RGB combined with depth (RGB-D), to classify time-sequence windows of sensory data as interactive or non-interactive. In contrast, we propose a novel RGB-only pipeline for predicting human interaction intent with frame-level precision, enabling faster robot responses and improved service quality. A key challenge in intent prediction is the class imbalance inherent in real-world HRI datasets, which can hinder the model’s training and generalization. To address this, we introduce MINT-RVAE, a synthetic sequence generation method, along with new loss functions and training strategies that enhance generalization on out-of-sample data. Our approach achieves state-of-the-art performance (AUROC: 0.95) outperforming prior works (AUROC: 0.90–0.91.2), while requiring only RGB input and supporting precise frame onset prediction. Finally, to support future research, we openly release our new dataset with frame-level labeling of human interaction intent.",
            "introduction": "Service robots operating in public spaces should be able to recognize when humans intend to interact with them to respond in a timely and socially appropriate manner [1]. In domains such as hotels, shopping centers, and healthcare facilities, this capability is essential for delivering seamless user experiences; for example, a robot receptionist must recognize that a person approaches with the intent to engage before explicit verbal or gestural signals occur [2]. Accurate intent detection improves fluency, safety, and user trust by minimizing delays and avoiding inappropriate or missed responses [3, 4].\n\nRecent research on human–robot interaction (HRI) has focused on human intent prediction for collaborative tasks between humans and robots [7]. These models, typically trained on relations between human motion trajectories and inferred intentions, enable robots to adapt or plan their actions proactively. However, many existing methods rely on more specialized hardware, such as depth cameras or motion capture systems (MOCAP), to capture 3D poses [7, 8], limiting system scalability and leading to more significant equipment costs that are less suited for real-world robot system deployments [9, 10]. Moreover, the high variability of human nonverbal cues constitutes one of the main challenges for the robust detection of human interaction intent, where trained models must generalize well to out-of-distribution test data unseen during training. Humans rely on posture, facial expressions, and proxemics to signal social intent [11], but endowing robots with the ability to interpret such signals requires multimodal perception and temporal reasoning [12].\n\nAnother limitation is the frame labeling resolution. Three common practices are observed: i) Sequence-level labels: most studies provide intent labels for entire data segments, such that all frames within a temporal window are assigned to the same intent label [8, 13]; ii) Action segmentation post-processing: some approaches assign intent labels after segmenting actions or interactions in post-processing, with intent annotation occurring retrospectively based on observed events [14, 15]; iii) Frame-wise outputs with sequence labels (coarse evaluation): other studies produce a probability per frame but train and evaluate their model using the same label replicated across frames, that is, without frame-accurate onset annotation [7, 8, 13]. While suitable for sequence classification, such methods omit the critical moment when intention first emerges (i.e., the onset) and hence, jeopardizes the system’s response time. More broadly, surveys on human motion and intent prediction stress that temporal fidelity is critical for responsive behavior: earlier, more precise predictions enable earlier actions [16]. Recognizing this onset at the frame level (vs. sequence level) is crucial for practical service robots, as it determines the earliest point at which a robot can and should act, thereby leading to an elevated quality of service and robot responsiveness.\n\nAnother central challenge in HRI intent prediction is class imbalance. In public deployments, genuine interaction events are rare relative to prolonged non-interaction; for example, the PAR-D dataset in [17] reports 112 / 4245 interacted vs. non-interacted trajectories. Furthermore, field studies in shopping malls observe engagement rates of ∼3.6%\\sim 3.6\\% [18]. Imbalance manifests at both the sequence level (few sequences contain interactions) and the frame level (positive labels are vastly outnumbered), motivating imbalance-aware training and imbalance-robust metrics (AUROC, macro-F1F_{1}, and balanced accuracy). Common remedies such as undersampling [19] or synthetic oversampling with SMOTE-like methods [20] have limitations in sequential, multimodal settings: undersampling reduces effective data and discards informative negatives, while SMOTE-like oversampling disrupts temporal coherence and multimodal consistency due to their non-learning-based approach (vs. generative models such as variational autoencoders [21] and generative adversarial networks [22]), ultimately degrading the performance of sequential models.\n\nTo address the above limitations and challenges in HRI intent detection contexts, this paper proposes a novel RGB-only pipeline for accurate frame-level detection of human intentions to interact with a robot arm (see Fig. 1). Unlike prior works that mostly focus on sequence-level detection and rely on more expensive sensor suites (e.g., RGB–D) [8, 13], our approach reduces hardware cost (approximately $​350\\mathdollar 350 for an Intel RealSense RGB–D camera vs. $​10\\mathdollar 10 for a commodity USB webcam), while enabling onset-accurate decisions. To tackle class imbalance, we introduce MINT–RVAE, a multimodal recurrent variational autoencoder (VAE) [21] for imbalance-aware sequence augmentation, which significantly improves the generalization of intent detector models. The contributions of this paper are as follows:\n\nDataset: We release a novel dataset (anonymized) with frame-level onset annotations of human intention to interact with a robot arm across diverse indoor public-space scenarios, collected with informed consent and privacy safeguards. To our knowledge, this is the first publicly available dataset that enables the study of frame-level intent prediction, in contrast to previous datasets that provide only sequence-level labels.\n\nMINT-VAE for HRI data re-balancing: We introduce a multimodal recurrent VAE together with custom loss function combinations and training strategies for learning a joint sequential latent representation over pose, emotions, and intent labels, with the goal of generating temporally coherent, cross-modal synthetic sequences for alleviating training data imbalance\n\nRGB-only intent detection: We develop a novel modular pipeline for the detection of human interaction intent (i.e., performing detection before any explicit interaction takes place) using only a single RGB camera, substantially simplifying hardware and cost overheads.\n\nExperiments: We train three different backbone networks (GRU, LSTM, Transformer), while using our proposed MINT-RVAE for data augmentation, and show that the resulting pipeline achieves strong detection performance in both frame-level and sequence-level cases (AUROC: 0.950.95), outperforming previously introduced methods (with typical AUROC: 0.900.90-0.920.92).",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的人机交互意图预测方法依赖于多模态输入，限制了系统的可扩展性和成本效益。  \n2. 真实世界中的人机交互数据集存在类别不平衡问题，影响模型的训练和泛化能力。  \n3. 现有方法在时间精度上不足，无法准确识别意图出现的时刻，影响机器人响应的及时性。  \n\n【提出了什么创新的方法】  \n本文提出了一种新颖的RGB-only管道，通过引入MINT-RVAE（多模态递归变分自编码器）来实现人类意图的帧级检测。该方法通过生成合成序列来解决类别不平衡问题，并结合自定义损失函数和训练策略，显著提高了意图检测模型的泛化能力。实验结果显示，该方法在帧级和序列级的检测性能上均达到了0.95的AUROC，超越了之前的研究（AUROC: 0.90-0.92）。此外，研究还公开发布了带有帧级标注的新数据集，以支持未来的研究。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment",
            "authors": "Xiaoyun Qiu,Haichao Liu,Yue Pan,Jun Ma,Xinhu Zheng",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22550",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22550",
            "arxiv_html_link": "https://arxiv.org/html/2509.22550v1",
            "abstract": "In mixed-traffic environments, where autonomous vehicles (AVs) must interact with diverse human-driven vehicles (HVs), the unpredictability of human intentions and heterogeneous driving behaviors poses significant challenges to safe and efficient lane change maneuvers. Existing methods often oversimplify these interactions by assuming uniform or fixed behavioral patterns.\nTo address this limitation, we propose an intention-driven lane change framework that integrates driving-style recognition with cooperation-aware decision-making and motion-planning. First, a deep learning-based classifier is developed to identify distinct human driving styles from the NGSIM dataset in real time. Second, we introduce a cooperation score composed of intrinsic and interactive components, which estimates surrounding drivers’ intentions and quantifies their willingness to cooperate with the ego vehicle’s lane change. Third, a decision-making module is designed by combining behavior cloning (BC) with inverse reinforcement learning (IRL) to determine whether a lane change should be initiated under current conditions. Finally, a coordinated motion-planning architecture is established, integrating IRL-based intention inference with model predictive control (MPC) to generate collision-free and socially compliant trajectories.\nExtensive experiments demonstrate that the proposed intention-driven BC–IRL model achieves superior performance, reaching 94.2% accuracy and 94.3% F1-score, and outperforming multiple rule-based and learning-based baselines. In particular, it improves lane change recognition by 4–15% in F1-score, highlighting the benefit of modeling inter-driver heterogeneity via intrinsic and interactive cooperation scores.\nBy bridging the gap between real-world human behaviors and automated lane change strategies, this work advances the development of context-aware and human-like AV systems for safe and efficient operations in complex traffic environments.",
            "introduction": "The rapid advancement of autonomous driving technology has accelerated the transition toward mixed-traffic environments where autonomous vehicles (AVs) will inevitably share roadways with human-driven vehicles (HVs) for the foreseeable future (Yurtsever et al. (2020)). Within this operational paradigm, the ability of AVs to accurately interpret and adapt to human drivers’ heterogeneous behavioral patterns during lane change maneuvers emerges as a critical safety imperative (Xing et al. (2021); Chen et al. (2022)). This challenge is particularly acute given the fundamental uncertainty inherent in human decision-making processes and the dynamic complexity of traffic interactions (Schwarting et al. (2018)).\n\nModern lane change algorithms have evolved from classical models (Gipps (1986)) to contemporary data-driven approaches (Zheng (2014); Wang et al. (2019b)), and can be broadly categorized into three paradigms: rule-based, data-based, and incentive-based systems. Recent advances in inverse reinforcement learning (Sun et al. (2022)) and probabilistic modeling (Sun et al. (2018)) have partially incorporated driver intentions into decision-making (Sheng et al. (2022)). Nevertheless, several critical limitations persist. Firstly, existing frameworks often rely on a behavioral homogeneity assumption, which oversimplifies surrounding human drivers by adopting uniform behavioral models and thereby overlooks inter-driver heterogeneity in decision-making. Secondly, although some methods incorporate personalized intentions, they typically fail to capture the temporal evolution of interactions among surrounding vehicles. Lastly, there remains an intention modeling gap: current prediction approaches lack mechanisms to effectively link observable driving styles with latent reward structures, resulting in a disconnect between behavior recognition and motion-planning.\n\nThese limitations are particularly problematic given that sudden lane changes account for about 17.0% of total severe crashes (Shawky (2020)), underscoring the urgent need for AV frameworks capable of simultaneously predicting trajectories and adapting to individual driving styles (Kuderer et al. (2015)).\n\nTo address the identified shortcomings in existing algorithms, we introduce an intention-driven lane change framework that synergistically integrates driving style recognition with adaptive decision-making as shown in Fig.1. In the context of this research, “driving style” is conceptualized as a habitual manner of vehicle operation, distinct to an individual or subset of drivers, that serves as a proxy for underlying personalized driving intentions. Our proposed lane change framework incorporates driving style into two distinct yet interrelated components: intention-prediction and motion-planning. The main contributions of this study are summarized as follows:\n\nWe introduce a cooperation score to quantify human drivers’ cooperative intentions during lane change interactions, where adaptive parameters are optimized to align with diverse driving styles.\n\nWe introduce a cooperation score to quantify human drivers’ cooperative intentions during lane change interactions, where adaptive parameters are optimized to align with diverse driving styles.\n\nWe design a dual-perspective intention-prediction framework that integrates intrinsic vehicle states with interactive contextual cues, thereby improving the interpretability, adaptability, and prediction accuracy of cooperation-aware lane change behavior.\n\nWe develop a BC–IRL integrated decision-making model that produces human-like lane change strategies conditioned on the inferred cooperation score and surrounding traffic context.\n\nWe propose an IRL–MPC pipeline that jointly learns driver-specific reward functions and generates collision-free, kinematically feasible trajectories, enabling safe and personalized maneuver planning.\n\n1. We introduce a cooperation score to quantify human drivers’ cooperative intentions during lane change interactions, where adaptive parameters are optimized to align with diverse driving styles.\n\n2. We design a dual-perspective intention-prediction framework that integrates intrinsic vehicle states with interactive contextual cues, thereby improving the interpretability, adaptability, and prediction accuracy of cooperation-aware lane change behavior.\n\n3. We develop a BC–IRL integrated decision-making model that produces human-like lane change strategies conditioned on the inferred cooperation score and surrounding traffic context.\n\n4. We propose an IRL–MPC pipeline that jointly learns driver-specific reward functions and generates collision-free, kinematically feasible trajectories, enabling safe and personalized maneuver planning.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有方法简化了人类驾驶行为的复杂性，未能有效处理驾驶者之间的异质性。  \n2. 传统算法未能捕捉周围车辆之间的动态交互与时间演变。  \n3. 当前的意图建模方法缺乏将可观察的驾驶风格与潜在奖励结构有效链接的机制。  \n\n【提出了什么创新的方法】  \n本文提出了一种意图驱动的变道框架，结合了驾驶风格识别、合作意识决策和运动规划。首先，开发了一个深度学习分类器，实时识别不同的人类驾驶风格。其次，引入了一个合作评分，评估周围驾驶者的意图和合作意愿。然后，设计了一个结合行为克隆（BC）和逆强化学习（IRL）的决策模块，以确定是否在当前条件下发起变道。最后，建立了一个协调的运动规划架构，将基于IRL的意图推断与模型预测控制（MPC）结合，生成无碰撞且符合社会规范的轨迹。实验结果表明，所提出的BC-IRL模型在变道识别方面的准确率和F1分数均达到94.2%和94.3%，显著优于多个基线方法，强调了通过内在和交互合作评分建模驾驶者异质性的优势。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes",
            "authors": "Katrina Ashton,Chahyon Ku,Shrey Shah,Wen Jiang,Kostas Daniilidis,Bernadette Bucher",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22498",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22498",
            "arxiv_html_link": "https://arxiv.org/html/2509.22498v1",
            "abstract": "Language-specified mobile manipulation tasks in novel environments simultaneously face challenges interacting with a scene which is only partially observed, grounding semantic information from language instructions to the partially observed scene, and actively updating knowledge of the scene with new observations. To address these challenges, we propose HELIOS, a hierarchical scene representation and associated search objective to perform language specified pick and place mobile manipulation tasks. We construct 2D maps containing the relevant semantic and occupancy information for navigation while simultaneously actively constructing 3D Gaussian representations of task-relevant objects. We fuse observations across this multi-layered representation while explicitly modeling the multi-view consistency of the detections of each object.\nIn order to efficiently search for the target object, we formulate an objective function balancing exploration of unobserved or uncertain regions with exploitation of scene semantic information.\nWe evaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and place benchmark in which perception is challenging due to large and complex scenes with comparatively small target objects. HELIOS achieves state-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also transfer to the real world without requiring additional data, as we illustrate by demonstrating it in a real world office environment on a Spot robot.\nVideos and code are available at our project website: https://helios-robot-perception.github.io/",
            "introduction": "Consider an autonomous robot tasked with bringing a mug from a coffee table to the kitchen counter in a home. If that robot sees a coffee table but cannot currently detect a mug on it, should it go closer to investigate if the mug is actually present? Or should it look in new parts of the home? An autonomous robot should be able to efficiently reason through this question using environment cues. In addition, the robot should be able to successfully perform this task of language-specified pick and place for mobile manipulation using the observations it accumulates during this search process.\n\nMethods for embodied physical intelligence can accumulate information about a novel scene and act on it though observation history with no explicit scene representation (Stone et al., 2023; Physical Intelligence et al., 2025; Team et al., 2025), only 2D maps (Yenamandra et al., 2023b; Melnik et al., 2023) or 3D scene graphs (Rana et al., 2023; Liu et al., 2024; Honerkamp et al., 2024). However, these methods all assume dense associations between language, observation, and action. Very different representations for long horizon spatio-temporal reasoning have been developed in problems for semantic search where language grounding is sparse (Georgakis et al., 2021; Yokoyama et al., 2023b; Chang et al., 2023). In order to perform mobile manipulation which includes semantic search, reasoning over vision, language, and action must occur simultaneously in both long and short horizons. Low success rates on new benchmarks targeting open vocabulary pick and place tasks in novel environments have demonstrated that combining this long and short horizon reasoning is still an open challenge (Liu et al., 2024; Yenamandra et al., 2023b).\n\nReasoning jointly over short and long spatio-temporal contexts requires very different policy objectives in addition to the differences in scene representations. Prior work in object search explicitly manages local and global search problems distinctly (Zheng et al., 2023; Schmalstieg et al., 2023; Li et al., 2022). Search policies must figure out when to switch between local and global reasoning by deciding the likelihood of being close to the target object. In addition to exploring unobserved regions, efficient search policies also exploit semantic information about the scene in order to search more likely locations of the target object first (Chaplot et al., 2020b; Ramakrishnan et al., 2022; Ye et al., 2021; Zhang et al., 2023; Georgakis et al., 2021; Yu et al., 2023; Yokoyama et al., 2023b). This exploration-exploitation tradeoff adds additional complexity to the task of performing object search as a component of mobile manipulation.\n\nContributions. We present HELIOS, a hierarchical scene representation and search objective for language specified mobile pick and place tasks in novel environments. We create a hierarchical scene representation using layered 2D value and occupancy maps to efficiently navigate and explore, and sparse collections of 3D Gaussians to represent objects of interest (see fig. 1). We then formulate an objective function on our hierarchical scene representation that balances exploring the scene to find regions which might contain the target object with exploiting observed semantic information. We introduce an uncertainty-weighted object score to take into account the multi-view consistency of the detections of an object before interacting with it. We conduct an ablation study to verify that each of these components increases our method’s performance.\nThrough our experiments, we show the contribution of uncertainty-based reasoning over our novel visual representation in improving robust perception in mobile manipulation. We evaluate HELIOS on the HomeRobot Open-vocabulary Mobile Manipulation benchmark (Yenamandra et al., 2023b; a) in the Habitat simulator (Savva et al., 2019b), achieving state-of-the-art results. We use HELIOS in semantic navigation as a stop decision, improving overall search success on the Habitat-Matterport 3D (Ramakrishnan et al., 2021) object search benchmark. The zero-shot nature of our approach means it can transfer to the real world without requiring additional data, as we show by demonstrating HELIOS in a real world office environment on a Spot robot.",
            "llm_summary": "【论文的motivation是什么】  \n1. 语言指定的移动操作任务在新环境中面临部分观察场景的交互挑战。  \n2. 需要将语言指令中的语义信息与部分观察到的场景进行有效关联。  \n3. 需要主动更新对场景的知识，以应对动态变化的环境。  \n\n【提出了什么创新的方法】  \n提出了HELIOS，一种层次化场景表示和搜索目标的方法，旨在执行语言指定的抓取和放置移动操作任务。该方法构建了包含相关语义和占用信息的2D地图，同时主动构建任务相关对象的3D高斯表示。通过在多层次表示中融合观察结果，并显式建模每个对象检测的多视图一致性，HELIOS能够有效地在未观察或不确定区域进行探索，同时利用场景的语义信息进行目标搜索。实验结果表明，HELIOS在OVMM基准测试中取得了最先进的结果，并且由于其零-shot特性，能够在不需要额外数据的情况下转移到现实世界。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Ontological foundations for contrastive explanatory narration of robot plans",
            "authors": "Alberto Olivares-Alarcos,Sergi Foix,Júlia Borràs,Gerard Canal,Guillem Alenyà",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Logic in Computer Science (cs.LO)",
            "comment": "This version was submitted to the journal Information Sciences and is under review since October 2024",
            "pdf_link": "https://arxiv.org/pdf/2509.22493",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22493",
            "arxiv_html_link": "https://arxiv.org/html/2509.22493v1",
            "abstract": "Mutual understanding of artificial agents’ decisions is key to ensuring a trustworthy and successful human-robot interaction. Hence, robots are expected to make reasonable decisions and communicate them to humans when needed.\nIn this article, the focus is on an approach to modeling and reasoning about the comparison of two competing plans, so that robots can later explain the divergent result. First, a novel ontological model is proposed to formalize and reason about the differences between competing plans, enabling the classification of the most appropriate one (e.g., the shortest, the safest, the closest to human preferences, etc.). This work also investigates the limitations of a baseline algorithm for ontology-based explanatory narration. To address these limitations, a novel algorithm is presented, leveraging divergent knowledge between plans and facilitating the construction of contrastive narratives. Through empirical evaluation, it is observed that the explanations excel beyond the baseline method.",
            "introduction": "Autonomous artificial decision-making in environments with different agents (e.g., robots collaborating with or assisting humans) is complex to model. This is often due to the high degree of uncertainty and potential lack of communication among agents. For instance, robots might need to choose between competing plans (i.e. sequences of actions that would allow the robot to achieve goals), comparing their properties and deciding which one is better. Note that this decision-making problem is different from finding a single plan through automated planning, as here the idea is that there are already two valid plans to execute and the robot shall compare them and identify the best one.\nThis might happen when a human gives an ambiguous command (e.g. ‘can you bring me a drink?’), thus the robot may decompose the abstract command into different concrete goals [1], and find a plan to achieve each of the goals (such as bringing any of the available drinks). Then it would be needed to compare and disambiguate the plans. In these cases, mutual understanding of the ongoing decisions and communication between agents become crucial [2]. Hence, trustworthy robots shall be able to model their plans’ properties to make sound decisions when contrasting them. Furthermore, they shall also be capable of narrating (explaining) the knowledge acquired from the comparison. Note that robots add the possibility of physically executing the plan, which may affect the human, strongly motivating the need for explanations. This may serve two purposes: justifying the robot’s selection of a plan, or asking the human to help in the disambiguation (i.e. the human may prefer the plan that the robot inferred as worse). Reflecting on these thoughts, this work addresses the following research questions:\n\nRQ1 - How could robots model and reason about what differentiates plans, making one better?\n\nRQ2 - How could robots leverage the proposed ontological model to explain (narrate) what differentiates plans?\n\nFirst, an ontological analysis is conducted and a new ontological model is obtained, augmenting the scope of an ontology from the literature (OCRA [3]), which answers RQ1. Specifically, a new theory for plan comparison is formalized, focusing on the properties and relationships that allow comparing plans. The robot’s knowledge about the plans to compare is stored, and together with some logical rules, it is used to infer which plan is better. Second, RQ2 is addressed by introducing a novel Algorithm for Contrastive eXplanatory Ontology-based Narratives (ACXON), extending an existing literature methodology (XONCRA [4]) to contrastive cases. From the robot knowledge, ACXON retrieves the divergent information about the plans, and then it constructs the final textual contrastive narrative. The proposed algorithm produces different types of narratives based on the chosen amount of detail (specificity), addressing different users’ preferences. Based on objective evaluation metrics and using several planning domains, the algorithm is evaluated with respect to the original algorithm proposed in XONCRA, which is used as a baseline. The proposed algorithm outperforms the baseline, using less knowledge to build the narratives (skipping repetitive knowledge), which shortens the time to communicate the narratives. Figure 1 provides an overview of the complete approach. At the end of the article, it is briefly discussed how the proposed algorithm can be slightly modified to enhance and restrict the knowledge selection, which helps to shorten the constructed narratives.111An extended abstract of this paper appears in the Proc. of AAMAS’24 [5].\n\n1. RQ1 - How could robots model and reason about what differentiates plans, making one better?\n\n2. RQ2 - How could robots leverage the proposed ontological model to explain (narrate) what differentiates plans?",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人需要在不确定环境中做出合理决策，以确保人机交互的信任和成功。  \n2. 机器人需要能够比较多个计划并解释其选择，以便与人类有效沟通。  \n\n【提出了什么创新的方法】  \n本文提出了一种新的本体模型，用于形式化和推理竞争计划之间的差异，从而帮助机器人识别最佳计划。通过引入一种新算法（ACXON），该算法利用计划间的差异知识，构建对比叙述，增强了机器人的解释能力。经过实证评估，结果显示该算法在叙述生成上优于基线方法，能够更快地传达信息并减少冗余知识的使用。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards",
            "authors": "Ben Rossano,Jaein Lim,Jonathan P. How",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22469",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22469",
            "arxiv_html_link": "https://arxiv.org/html/2509.22469v1",
            "abstract": "This paper proposes a task allocation algorithm for teams of heterogeneous robots in environments with uncertain task requirements. We model these requirements as probability distributions over capabilities and use this model to allocate tasks such that robots with complementary skills naturally position near uncertain tasks, proactively mitigating task failures without wasting resources. We introduce a market-based approach that optimizes the joint team objective while explicitly capturing coupled rewards between robots, offering a polynomial-time solution in decentralized settings with strict communication assumptions. Comparative experiments against benchmark algorithms demonstrate the effectiveness of our approach and highlight the challenges of incorporating coupled rewards in a decentralized formulation.",
            "introduction": "Heterogeneous multi-robot systems can achieve higher levels of efficiency and effectiveness than individual robots can by leveraging task parallelization and diverse sets of capabilities [1]. At the core of these systems lies multi-robot task allocation (MRTA), the process of determining how to distribute tasks across the team. Many critical missions, such as search and rescue [2] and disaster relief [3], require allocating tasks to robots in unknown environments. In these missions, robots may encounter disturbances that either prevent task completion or delay progress.\nSuch disturbances often arise from underlying uncertainty in the environment or the robots themselves. Making effective decisions in the face of uncertainty remains an open challenge in MRTA problems.\n\nOne approach to addressing uncertainty is to model mission parameters (e.g., task durations/requirements or agent speeds/sensing capabilities) as random variables and reformulate the problem as a probabilistic optimization [4]. A natural question then arises: how are stochastic mission parameters determined? Many methods assume that these parameters are given as a priori knowledge about the mission [5]. For example, in a time-sensitive disaster relief mission with high-value search tasks and lower-value infrastructure repair tasks, there may exist a known probability that search robots encounter untraversable terrain. This renders search tasks infeasible without assistance from larger support robots, which are otherwise assigned to infrastructure repair tasks. Allocation thus becomes a strategic challenge: planners must position support robots so that they can assist the search robots if necessary before task deadlines expire, while still maximizing the completion of infrastructure repair tasks.\n\nA priori knowledge about disturbances may not always be available, highlighting a key distinction in MRTA problem formulations: robustness versus resilience. Robustness is achieved by proactively computing pre-execution plans to anticipate modeled disturbances [6]. Many existing methods achieve robustness through redundancy [7] or idle pre-positioning [8], which can lead to an inefficient use of resources.\nOn the other hand, resilience is the ability of a multi-robot system to adapt to unmodeled disturbances [9]. Unlike robustness, resilience is achieved by reactively modifying plans in response to unanticipated conditions. Existing work under the resilient MRTA framework has primarily focused on detecting unmodeled disturbances and replanning with updated information only after task completion has been declared infeasible [10], [11].\n\nThis paper offers a novel perspective on resilience by emphasizing that reactive planning can still incorporate anticipatory actions. The key insight is that when an unmodeled disturbance is discovered, there is often a delay before the affected task is formally declared infeasible. Rather than treating this delay as wasted time, it can be leveraged to proactively reconfigure resources. For example, in the disaster relief scenario, when a search robot encounters untraversable terrain, it may take time to confirm infeasibility (e.g., the robot may first explore alternative routes). During this period, repositioning a support robot toward a nearby task enables faster intervention if the task ultimately fails, whereas existing reactive planners risk missing the deadline. Capturing this kind of proactive response requires a new way of quantifying uncertainty, since a priori mission parameters cannot account for unmodeled disturbances.\n\nTo encourage strategic allocations, we adjust task utilities to reflect not only intrinsic task value but also the potential benefit of supporting high-value, uncertain tasks (HVUTs). This adjustment incentivizes robots with support capabilities to remain productive on nearby tasks while simultaneously positioning themselves to assist HVUTs if needed. The resulting MRTA formulation is particularly challenging because of strong inter-robot dependencies: while all support robots benefit from positioning near HVUTs, only one will ultimately be reassigned if assistance is needed. Efficient allocation therefore requires capturing this coupling to avoid over-incentivizing robots to cluster around the same HVUTs. To address this, our algorithm introduces a market-based mechanism that resolves these dependencies, providing a polynomial-time solution suitable for decentralized settings with strict communication constraints. Furthermore, we evaluate how the solution quality changes as communication assumptions are relaxed under varying degrees of inter-robot coupling, illustrating the tradeoffs of formulating this problem in a decentralized setting.\nOur work can be summarized by three primary contributions:\n\nWe propose a model for task capability uncertainty that provides a unified framework for solving the multi-robot task allocation problem with both in-distribution and out-of-distribution disturbances.\n\nWe present a market-based algorithm to allocate tasks to robots in the face of these disturbances by strategically prioritizing tasks to maximize the expected mission rewards.\n\nWe demonstrate the utility of our algorithm through a comparative analysis against several benchmark algorithms, demonstrating how the mission performance changes with different assumptions.\n\n1. We propose a model for task capability uncertainty that provides a unified framework for solving the multi-robot task allocation problem with both in-distribution and out-of-distribution disturbances.\n\n2. We present a market-based algorithm to allocate tasks to robots in the face of these disturbances by strategically prioritizing tasks to maximize the expected mission rewards.\n\n3. We demonstrate the utility of our algorithm through a comparative analysis against several benchmark algorithms, demonstrating how the mission performance changes with different assumptions.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有多机器人任务分配方法在面对环境不确定性时效率低下。  \n2. 现有方法往往依赖于先验知识，无法适应未建模的干扰。  \n3. 需要一种新的方法来量化不确定性并优化任务分配。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于市场机制的多机器人任务分配算法，能够在面对不确定任务要求时有效分配任务。该方法通过建模任务能力的不确定性，优化机器人之间的相互依赖关系，确保支持机器人能够在关键时刻迅速响应高价值不确定任务。通过比较实验，验证了该算法在不同通信假设下的有效性，展示了在不确定环境中提升任务完成率的潜力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation",
            "authors": "Zhangyuan Wang,Yunpeng Zhu,Yuqi Yan,Xiaoyuan Tian,Xinhao Shao,Meixuan Li,Weikun Li,Guangsheng Su,Weicheng Cui,Dixia Fan",
            "subjects": "Robotics (cs.RO)",
            "comment": "This paper introduces the first VLA framework for AUVs, featuring a dual-brain architecture and zero-data MPC for real-world underwater navigation",
            "pdf_link": "https://arxiv.org/pdf/2509.22441",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22441",
            "arxiv_html_link": "https://arxiv.org/html/2509.22441v1",
            "abstract": "This paper presents UnderwaterVLA, a novel framework for autonomous underwater navigation that integrates multimodal foundation models with embodied intelligence systems. Underwater operations remain difficult due to hydrodynamic disturbances, limited communication bandwidth, and degraded sensing in turbid waters. To address these challenges, we introduce three innovations. First, a dual-brain architecture decouples high-level mission reasoning from low-level reactive control, enabling robust operation under communication and computational constraints. Second, we apply Vision–Language–Action (VLA) models to underwater robotics for the first time, incorporating structured chain-of-thought reasoning for interpretable decision-making. Third, a hydrodynamics-informed Model Predictive Control (MPC) scheme compensates for fluid effects in real time without costly task-specific training. Experimental results in field tests show that UnderwaterVLA reduces navigation errors in degraded visual conditions while maintaining higher task completion by 19%\\% to 27% over baseline. By minimizing reliance on underwater-specific training data and improving adaptability across environments, UnderwaterVLA provides a scalable and cost-effective path toward the next generation of intelligent AUVs.",
            "introduction": "Autonomous Underwater Vehicles (AUVs) are indispensable tools for marine science and industry, supporting missions ranging from seabed mapping and ecological monitoring to subsea infrastructure inspection and resource exploration [1]. Their ability to operate untethered in extreme environments provides unique advantages over remotely operated vehicles (ROVs), including extended range, reduced human supervision, and lower operational costs [2]. AUVs have enabled breakthroughs in deep-sea research and offshore engineering by delivering high-resolution data from environments that are otherwise inaccessible.\n\nYet, AUV autonomy remains fundamentally constrained by the hostile and unpredictable nature of the ocean. Unlike aerial or terrestrial domains, underwater environments present a combination of challenges:\n\nStrong, nonlinear hydrodynamics, where ocean currents, turbulence, and vortical structures introduce unmodeled disturbances that degrade stability and control [3, 4].\n\nStrong, nonlinear hydrodynamics, where ocean currents, turbulence, and vortical structures introduce unmodeled disturbances that degrade stability and control [3, 4].\n\nExtreme physical conditions, including high hydrostatic pressures, spatially varying salinity, and thermocline effects, which alter vehicle buoyancy and structural reliability [5].\n\nSevere perception difficulties, such as turbidity, backscatter, and non-uniform illumination, which compromise optical sensors; meanwhile, sonar imaging suffers from low resolution and multipath interference [6, 7].\n\nCommunication constraints, where radio is ineffective underwater and acoustic channels are bandwidth-limited, delay-prone, and vulnerable to noise [8].\n\nConventional approaches—including proportional–integral–derivative (PID) controllers, adaptive model-based control, and deep reinforcement learning—have shown partial success, but often lack generalization and robustness in unstructured marine environments [9, 10]. Recent efforts such as factor-graph-based cooperative localization [11] and data-informed domain randomization for reinforcement learning [12, 13] have improved resilience but remain constrained by reliance on accurate modeling and costly task-specific training. This highlights the need for autonomy frameworks capable of generalizing across tasks and adapting to uncertainty without prohibitive data requirements.\n\nThe emerging Vision–Language–Action (VLA) paradigm offers such potential. By unifying perception, natural language understanding, and action generation, VLA systems enable multimodal reasoning and robust task generalization. These models have achieved remarkable success in diverse domains, including industrial manipulation, autonomous driving, aerial robotics, and quadruped locomotion [14, 15, 16, 17]. For instance, OpenVLA demonstrates unified multimodal control and instruction following [18], while DiffusionVLA shows how large-scale diffusion and autoregressive models can enhance general-purpose robot performance [19]. QUAR-VLA has successfully applied VLA models to quadrupeds for locomotion and manipulation [16]. Such advances indicate strong potential for VLA to move beyond task-specific controllers toward general-purpose autonomy with natural language interaction and contextual understanding.\n\nHowever, VLA remains largely unexplored in underwater robotics [20]. Three barriers explain this gap: (i) No underwater-suited hierarchy: Existing end-to-end VLA models cannot effectively decouple high-level mission planning from low-level real-time control under the stringent communication and computational constraints of underwater operations, leading to insufficient system robustness.\n(ii) Data-hungry policies: Conventional VLA require vast amounts of expensive underwater demonstration data to learn policies, severely limiting scalability and generalization due to prohibitive data collection costs [12].\n(iii) No real-time hydro-compensation: Nonlinear fluid dynamics (e.g., turbulence, vortices) significantly disrupt AUV motion; existing VLA or learning-based controllers lack embedded physical priors and cannot perform real-time dynamic compensation without additional task-specific training, resulting in degraded navigation accuracy [21, 5].\nThese challenges have previously hindered the application of VLA frameworks to AUVs, despite their transformative promise. Importantly, recent advances in VLA research increasingly converge toward model-based, zero-data training paradigms — leveraging physical priors [22].\n\nTo address these issues, we present Underwater VLA, the first VLA framework specifically designed for AUVs. Our system systematically addresses the unique challenges of underwater environments by introducing a zero-data training methodology, cross-domain adaptation with physical priors, and a dual-brain architecture that integrates high-level reasoning with low-level control for robust autonomy.\n\nThe main contributions of this paper are summarized as follows:\n\nUnderwater VLA framework: We introduce the first VLA framework tailored for AUVs, systematically addressing underwater-specific challenges such as limited communication, harsh sensing conditions, and costly data collection.\n\nZero-data training methodology: We propose a training strategy that minimizes reliance on underwater demonstrations by leveraging pre-trained multimodal foundation models with underwater-specific transfer learning.\n\nDual-brain architecture: We design a hybrid control structure that decouples high-level deliberative reasoning from low-level reactive control, ensuring reliable autonomy under computational constraints and enhancing safety.\n\nExperimental validation: We demonstrate that Underwater VLA achieves superior language understanding, task generalization, and mission success rates compared to conventional control and previous VLA baseline.\n\nThe remainder of this paper is organized as follows: Section I reviews related work on AUV autonomy and VLA frameworks. Section II details the methodology of Underwater VLA. Section III presents experimental results and discussion. Section IV concludes the paper with future directions.\n\n1. Underwater VLA framework: We introduce the first VLA framework tailored for AUVs, systematically addressing underwater-specific challenges such as limited communication, harsh sensing conditions, and costly data collection.\n\n2. Zero-data training methodology: We propose a training strategy that minimizes reliance on underwater demonstrations by leveraging pre-trained multimodal foundation models with underwater-specific transfer learning.\n\n3. Dual-brain architecture: We design a hybrid control structure that decouples high-level deliberative reasoning from low-level reactive control, ensuring reliable autonomy under computational constraints and enhancing safety.\n\n4. Experimental validation: We demonstrate that Underwater VLA achieves superior language understanding, task generalization, and mission success rates compared to conventional control and previous VLA baseline.",
            "llm_summary": "【论文的motivation是什么】  \n1. AUV autonomy受限于复杂的水下环境，面临多种挑战。  \n2. 现有的控制方法缺乏在不确定性下的泛化能力和鲁棒性。  \n3. VLA框架在水下机器人领域尚未得到充分探索。  \n\n【提出了什么创新的方法】  \n本论文提出了UnderwaterVLA，一个专为AUV设计的VLA框架，解决了水下环境的独特挑战。首先，采用零数据训练方法，减少对昂贵水下示范数据的依赖。其次，设计了双脑架构，将高层推理与低层控制解耦，以确保在计算约束下的可靠自主性。最后，通过实验验证，UnderwaterVLA在语言理解、任务泛化和任务成功率上显著优于传统控制方法和先前的VLA基线，展示了其在复杂水下环境中的有效性和适应性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics",
            "authors": "Margherita Martorana,Francesca Urgese,Ilaria Tiddi,Stefan Schlobach",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22434",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22434",
            "arxiv_html_link": "https://arxiv.org/html/2509.22434v1",
            "abstract": "Personal service robots are increasingly deployed to support daily living in domestic environments, particularly for elderly and individuals requiring assistance. Operating effectively in these settings involves not only physical interaction but also the ability to interpret dynamic environments, understand tasks, and choose appropriate actions based on context. This process requires integrating both hardware components (e.g. sensors, actuators) and software systems capable of reasoning about tasks, environments, and the robot’s own capabilities. Framework such as the Robot Operating System (ROS) provide open-source libraries and tools that help connect low-level hardware data with higher-level functionalities. However, real-world deployments remain tightly coupled to specific hardware and software platforms. As a result, solutions are often isolated and hard-coded, limiting interoperability, reusability, and knowledge sharing. Ontologies and knowledge graphs offer a structured and interpretable way to represent tasks, environments, and robot capabilities. Existing ontologies, such as the Socio-physical Model of Activities (SOMA) and the Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE), provide models for representing activities, spatial relationships, and reasoning structures. However, they often focus on specific domains and do not fully capture the connection between environment, action, robot’s capabilities, and system-level integration. In this work, we propose the Ontology for roBOts and acTions (OntoBOT), which builds upon and extends existing ontologies to provide a unified representation of tasks, actions, environments, and capabilities. Our contributions are twofold: (1) we unify these core aspects into a cohesive ontology to support formal reasoning about task execution, and (2) we demonstrate its generalizability by evaluating competency questions across four distinct embodied agents — TIAGo, HSR, UR3, and Stretch — each with different capabilities, showing how OntoBOT enables context-aware reasoning and facilitates task-oriented execution, thereby promoting knowledge sharing in service robotics.",
            "introduction": "Personal service robots are increasingly deployed to assist in domestic environments, particularly in support of older adults and individuals requiring physical assistance (Macis et al., 2023; Nanavati et al., 2024; Holland et al., 2021; Roy et al., 2000; Sørensen et al., 2024). The tasks that service robots typically perform range from cleaning and tidying to meal preparation and care-giving, often within open-ended, dynamic, and unstructured environments. Effective operation in these settings requires a high-level understanding of the context in which tasks are performed, the specific actions involved, and the robot’s own physical and functional capabilities (Paulius and Sun, 2019; Bajd et al., 2010). This process depends on the integration of hardware components (e.g. sensors, actuators) with software systems capable of representing and reasoning about tasks, environments, and the robot’s own capabilities. Middleware platforms like the Robot Operating System (ROS)111https://www.ros.org/ (Quigley et al., 2009) offer open-source tools and libraries to help connect low-level hardware with higher-level functionalities. However, real-world deployments are often tightly coupled to isolated and platform-dependent solutions (Axelsson, 2015; García et al., 2023; Wang et al., 2024), with hard-coded instructions designed for specific hardware and software stacks (Axelsson, 2015; García et al., 2023), limiting interoperability, knowledge sharing, cross-domain reusability and system integration.\n\nOntologies and Knowledge Graphs (KG) offer a promising approach for representign robotic knowledge in a structured, shareable, and machine-interpretable knowledge (Paulius and Sun, 2019). Several ontologies relevant to robotics have been proposed, including efforts toward standardization such as the IEEE Robotics and Automation Society (RAS) Ontologies (Prestes et al., 2013; Gonçalves et al., 2021; Schlenoff et al., 2017), representations like the Ontology for Collaborative Robotics and Adaptation (OCRA) (Olivares-Alarcos et al., 2022) and the Autonomous Robot Task Processing Framework (ART-ProF) (Ge et al., 2024a), and foundational models such as the Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) (Masolo et al., 2003; Borgo et al., 2022) and the Socio-physical Model of Activities (SOMA) (Beßler et al., 2021). These ontologies provide a formal representation for reasoning about objects, actions, spatial relationships, and activities in physical and social contexts. However, they focus on specific aspects of robotic systems, such as task planning or perception, and do not capture the full interplay between a robot’s capabilities, the actions required to complete a task, and the environment in which those tasks occur. This creates a gap when reasoning about whether a robot is suited to perform a given task in a particular context, especially in heterogeneous, dynamic and collaborative settings. For example, domestic environments such as kitchens often involve recurring tasks and spatial configurations. While the robots deployed in these settings may vary in hardware and capabilities, the structure of the environment and the nature of the tasks often remain comparable. Existing ontologies can partially represent these individual elements – such as actions, objects, or spatial relations – but they lack an integrated way to connect between each other, and to robot-specific capabilities.\n\nIn this work, we introduce the Ontology for roBOts and acTions (OntoBOT)222https://w3id.org/onto-bot, a unified model designed to formally represents the interconnected elements of robotic task execution, namely: the robot and its capabilities, the task it has to perform, and the environment in which it operates. Through OntoBOT, we achieved two key contributions: unifying these core aspects into a cohesive semantic framework, and enabling reasoning about task execution across embodied agents, as demonstrated through the evaluation of competency questions involving four different robots — TIAGo, HSR, UR3, and Stretch — with varying capabilities. All materials are publicly available on GitHub333https://github.com/kai-vu/OntoBOT, including a Jupyter notebook specifically developed for reproducing the competency question evaluation444https://github.com/kai-vu/OntoBOT/blob/main/case-study/cqs.ipynb.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有服务机器人在动态环境中执行任务时缺乏对任务、环境和自身能力的全面理解。  \n2. 现有的本体和知识图谱无法有效整合机器人能力、任务和环境之间的关系。  \n3. 机器人系统的解决方案往往依赖于特定的硬件和软件平台，限制了互操作性和知识共享。  \n\n【提出了什么创新的方法】  \n本文提出了“Ontology for roBOts and acTions (OntoBOT)”，旨在统一表示任务、动作、环境和能力之间的关系。该方法通过构建一个连贯的本体框架，支持对任务执行的形式推理，并通过对四种不同能力的机器人（TIAGo、HSR、UR3和Stretch）进行能力问题的评估，展示了其通用性。OntoBOT促进了上下文感知推理和任务导向执行，从而增强了服务机器人领域的知识共享。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive Grasping",
            "authors": "Leonel Giacobbe,Jingdao Chen,Chuangchuang Sun",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22421",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22421",
            "arxiv_html_link": "https://arxiv.org/html/2509.22421v1",
            "abstract": "Grasping is a core task in robotics with various applications. However, most current implementations are primarily designed for rigid items, and their performance drops considerably when handling fragile or deformable materials that require real-time feedback. Meanwhile, tactile-reactive grasping focuses on a single agent, which limits their ability to grasp and manipulate large, heavy objects.\nTo overcome this, we propose a learning-based, tactile-reactive multi-agent Model Predictive Controller (MPC) for grasping a wide range of objects with different softness and shapes, beyond the capabilities of preexisting single-agent implementations. Our system uses two Gelsight Mini tactile sensors [1] to extract real-time information on object texture and stiffness. This rich tactile feedback is used to estimate contact dynamics and object compliance in real time, enabling the system to adapt its control policy to diverse object geometries and stiffness profiles. The learned controller operates in a closed loop, leveraging tactile encoding to predict grasp stability and adjust force and position accordingly.\nOur key technical contributions include a multi-agent MPC formulation trained on real contact interactions, a tactile-data driven method for inferring grasping states, and a coordination strategy that enables collaborative control.\nBy combining tactile sensing and a learning-based multi-agent MPC, our method offers a robust, intelligent solution for collaborative grasping in complex environments, significantly advancing the capabilities of multi-agent systems.\nOur approach is validated through extensive experiments against independent PD and MPC baselines. Our pipeline outperforms the baselines regarding success rates in achieving and maintaining stable grasps across objects of varying sizes and stiffness.",
            "introduction": "Robotic grasping and manipulation have been the subject of extensive research, with many approaches promising to be valid methods to enhance accuracy in both initial grasp success and the ability to maintain stable grips[2]. However, the majority of these methods have focused on rigid objects, often overlooking the challenges posed by soft or irregularly shaped items. Grasping and manipulating such objects remains difficult due to their compliance, variable surface properties, and the need to apply finely tuned contact forces.\n\nHumans naturally rely on tactile feedback to infer properties such as stiffness, texture, and rigidity, which are crucial information when handling objects. In robotics, tactile sensors offer the potential to replicate this capability, providing dense, high-dimensional data well-suited for learning-based controllers. The rich sensory information embedded in tactile images can be effectively leveraged by neural networks to improve performance in grasping and manipulation in commonplace robotic scenarios.\n\nDespite progress in single-agent tactile control, effective multi-agent grasping coordination remains a fundamental challenge in manipulation tasks, especially in unstructured, cluttered environments.\nRecent work, such as LeTac-MPC [3], proposed a learning-based Model Predictive Control (MPC) layer for tactile-reactive grasping using a single robotic manipulator. While promising, their implementation assumes ideal object positioning and orientation, which may not be a realistic expectation in real-world environments. In real-world deployments, where grasp prediction algorithms such as [4] are employed, performance can decrease significantly, as some objects can be quite challenging to grasp and manipulate with a single contact point if that point is not ideal [5].\n\nEnvironments such as construction sites, where large, heavy and irregularly shaped materials are frequently manipulated, exemplify this need. In those contexts, the inherent limitations of single-agent grasping become apparent. For instance, consider the task of lifting a heavy structural beam. A single robotic manipulator, even when executing an optimal grasp, would struggle due to the beam’s extended length and weight. These factors increase the likelihood of unstable or failed grasps due to contacts being limited to a single point. Other situations where multi-agency could be leveraged include search and rescue missions (with delicate handling of patients), food preparation, and robotic surgery. A multi-agent controller addresses the issue of requiring close to optimal grasping positions by increasing the number of contact points (which improves grasp stability), and by enabling collaborative behaviors that facilitate smoother and more natural manipulation of complex or irregularly shaped objects.\n\nWe propose a multi-agent, collaborative framework of a tactile-reactive Model Predictive Control (MPC) layer. This layer enables coordinated bi-manual manipulation, broadening the range of objects that can effectively be grasped and manipulated, particularly those that are difficult to handle with a single manipulator.\n\nIn spite of the many uses of tactile-based controllers, the deployment of these approaches comes with some inherent limitations:\n\nChallenges with soft objects: Tactile sensors rely on gel deformations to infer qualities about the grasped objects. When interacting with objects that are softer than the sensor’s gel, the resulting deformations are often minimal or imperceptible in the captured tactile images. This can lead the controller to interpret such interactions as no-contact scenarios, thus failing to execute appropriate grasping behaviors.\n\nComputational Complexity: Learning-based control methods are naturally computationally expensive, especially when run at high frequencies. They also require a high amount of data points to operate. Homogeneous multi-agent approaches look to make better use of collected experience by sharing parameters across agents. High-frequency control is crucial in manipulation tasks, especially when rapid changes in conditions occur due to collisions or unexpected dynamics, so elevated computing power is required to make these controllers feasible. Two main factors affect controller runtime: the dimensionality of the tactile image encoding and the length of the MPC prediction horizon. Although increasing either of these would theoretically improve prediction accuracy, it also incurs higher computational cost and may lead to optimization issues such as excessively large initial losses during training that yield a non-converging controller. Thus, a balance must be struck between expressiveness and real-time feasibility.\n\nOur proposed multi-agent tactile-reactive layer addresses the aforementioned challenges as follows:\n\nBi-manual tactile-reactive grasping of soft objects for improved stability: By distributing contact forces across two manipulators, our system reduces the likelihood of applying excessive localized pressure. This not only decreases the risk of permanent damage to grasped objects but also provides more stable manipulation through more contact points.\n\nBi-manual tactile-reactive grasping of soft objects for improved stability: By distributing contact forces across two manipulators, our system reduces the likelihood of applying excessive localized pressure. This not only decreases the risk of permanent damage to grasped objects but also provides more stable manipulation through more contact points.\n\nA collaborative learning based control approach with improved efficiency: According to our experimental evaluations, the introduction of an additional manipulator does not result in a proportional increase in runtime due to the leveraging of parameter sharing across the learnable MPC and CNN layers. This indicates that, although the overall system is slower compared to the baseline single-agent case [3], the per-agent efficiency improves. The relative runtime improvement arises mainly from a more efficient construction of the state transition matrices and lifted system matrices.\n\nReal-world comparative evaluation between baseline models and our proposed approach: In our experiments, two GelSight Mini sensors are mounted on Robotiq 2F-series grippers, with all trials conducted under an identical configuration. An extensive evaluation on various objects across multiple baselines is conducted to showcase the advantage of the proposed pipeline.\n\nTactile Grasping.\nThe main focus of tactile sensors is to mimic the information stream human touch can provide, enabling robotic systems to extract fine-grained information about grasped objects and dynamically adapt their opening and grasping forces. Compared to conventional modalities such as vision or point cloud data, tactile images are significantly richer in encoded contact information, making them especially effective for training data-driven controllers.\nWhile image-based tactile sensors remain the norm, recent research has explored augmenting them with complementary information streams, such as ultrasound [6] and Vision-Language-Action [7] [8], which have demonstrated promising results in expanding the horizons of tactile sensors.\n\nA key challenge, however, lies in the lack of tactile sensing datasets and the difficulty of modeling tactile sensors in simulation. Work such as [9] has addressed this limitation by leveraging measured contact forces at an end-effector to generate realistic synthetic tactile images, helping to bridge the gap between simulation and deployment. These high-fidelity tactile images could prove very useful in the future, as most current learning-based tactile approaches, including ours, rely exclusively on real sensor data, which can be time-consuming to acquire.\n\nBi-manual Manipulation.\nIn recent years, significant progress has been made in bi-manual robotic manipulation, with many approaches aiming to replicate the dexterity and rich sensing capabilities of human arms. Collaboration between the two manipulators is often achieved through Reinforcement Learning [10] or Imitation Learning [11]. A central challenge in this field is accurately discretizing and modeling the environment along with the full range of possible interactions between agents, which makes long-horizon, multi-step tasks particularly complex [12]. This is one of the reasons our approach utilizes parallel-jaw grippers with one degree of freedom: they substantially reduce the action space compared to multi-finger grippers, simplifying the learning process and making it more tractable.\n\nLearning-based MPC.\nModel Predictive Control (MPC) is one of the most robust and widely accepted control frameworks in robotics. However, traditional MPC approaches often face limitations when applied to systems that are inherently difficult to model with high fidelity. Learning-based MPC seeks to overcome these challenges by incorporating real-world (sensory) data to refine system dynamics, enabling the design of more accurate and adaptive controllers. In many cases, learning-based MPC has been shown to achieve superior performance, streamline deployment, and reduce the need for extensive manual controller tuning compared to baseline MPC methods [13]. Applications of MPC span a wide range of domains, including dexterous manipulation [14], autonomous vehicle steering [15], and Unmanned Aerial Vehicles (UAVs) [16].\n\nThe remainder of the paper is organized as follows: In the Data Collection section, we provide an overview of the methods we employed to obtain data for training our learned controller.\nIn the Pipeline Architecture section, we describe the individual components of the proposed system and their interactions, outlining how they collectively enable multi-agent tactile reactive grasping. In the Results section, we report on training and experiment results, comparing the performance of our method against established baselines. Following that, in the Advantages over Single-agent Model section, we present a comparative analysis highlighting the benefits of the proposed layer relative to baseline single-agent approaches. Lastly, in the Conclusion, Limitations and Future Work section, we summarize the contributions, discuss the current limitations, and outline potential directions for future research.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的抓取方法主要针对刚性物体，处理软性或变形物体时性能显著下降。  \n2. 单一智能体的触觉反应抓取限制了对大型重物的抓取和操作能力。  \n3. 多智能体协作抓取在复杂环境中的有效性仍然是一个基本挑战。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于学习的触觉反应多智能体模型预测控制（MPC）框架，旨在实现双手协作抓取。该系统利用Gelsight Mini触觉传感器实时提取物体的纹理和刚度信息，通过丰富的触觉反馈估计接触动态和物体的顺应性，从而自适应控制策略以应对不同物体的几何形状和刚度特征。通过多智能体MPC的协作控制，该方法显著提高了在复杂环境中抓取和操作各种物体的能力。实验结果表明，该方法在稳定抓取的成功率上超越了独立的PD和MPC基线，展示了其在处理不同尺寸和刚度物体时的优势。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation",
            "authors": "Enguang Liu,Siyuan Liang,Liming Lu,Xiyu Zeng,Xiaochun Cao,Aishan Liu,Shuchao Pang",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22356",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22356",
            "arxiv_html_link": "https://arxiv.org/html/2509.22356v1",
            "abstract": "The safety and reliability of embodied agents rely on accurate and unbiased visual perception. However, existing benchmarks mainly emphasize generalization and robustness under perturbations, while systematic quantification of visual bias remains scarce. This gap limits a deeper understanding of how perception influences decision-making stability. To address this issue, we propose RoboView-Bias, the first benchmark specifically designed to systematically quantify visual bias in robotic manipulation, following a principle of factor isolation. Leveraging a structured variant-generation framework and a perceptual-fairness validation protocol, we create 2,127 task instances that enable robust measurement of biases induced by individual visual factors and their interactions. Using this benchmark, we systematically evaluate three representative embodied agents across two prevailing paradigms and report three key findings: (i) all agents exhibit significant visual biases, with camera viewpoint being the most critical factor; (ii) agents achieve their highest success rates on highly saturated colors, indicating inherited visual preferences from underlying VLMs; and (iii) visual biases show strong, asymmetric coupling, with viewpoint strongly amplifying color-related bias. Finally, we demonstrate that a mitigation strategy based on a semantic grounding layer substantially reduces visual bias by approximately 54.5% on MOKA. Our results highlight that systematic analysis of visual bias is a prerequisite for developing safe and reliable general-purpose embodied agents.",
            "introduction": "The safety and reliability of general-purpose robots depend on accurate and unbiased visual perception, which is the primary channelLiu et al. (2025c) through which embodied agentsMa et al. (2024); Li et al. (2024b) perceive and act in the physical world. In hierarchical control, top-level vision-language planners can be biased with respect to color, viewpoint, or scale. Such biases can be amplified as high-level plans are broken into steps and constraints, destabilizing both planning and execution. These observations underscore the need to explore the safety  of multimodal large language models (MLLMs) Liang et al. (2023; 2025b; 2025a); Liu et al. (2025b), motivating research on alignment Ho et al. (2024), robustness Wang et al. (2025); Zhang et al. (2024); Ying et al. (2024), and bias mitigation Xiao et al. (2025) in vision-language systems.\n\nExisting robot manipulation benchmarks primarily evaluate an algorithm’s generalization James et al. (2020); Zhu et al. (2020); Heo et al. (2023); Pumacay et al. (2024); Luo et al. (2025) and robustness Puig et al. ; Xie et al. (2024); Li et al. (2024a); Liu et al. (2025a) under new tasks and environment changes. However, common metrics emphasize average success rates while overlooking variation and instability across visual attributes, thereby hiding failure risks under specific visual conditions. Specifically, they rarely independently isolate and quantify systematic biases from visual attributes, such as color and camera viewpoint, under controlled conditions. They also lack sensitivity and interaction metrics along the perception-to-decision pipeline, as well as fair and clear comparison sets.\n\nWe introduce RoboView-Bias, a benchmark to systematically quantify visual bias in robots using the principle of factorial isolation.\nTo generate evaluation instances, our structured variant-generation framework (SVGF) partitions all variables into two disjoint sets.\n❶ Dimensions of Visual Perturbation (V), comprise the attributes under evaluation: 141 object colors, 9 camera orientations, 21 full camera poses, and 9 distance scales.\n❷ Dimensions of Task Context Generalization (D), includes 4 initial positions, 4 shapes, and 3 language instructions to ensure robust findings across diverse task contexts. This methodology yields 2,127 instances and each instance is further validated for perceptual fairness, ensuring it is visually clear and solvable.\n\nIn the RoboView-Bias benchmark, we comprehensively evaluated two prevailing paradigms of embodied agents. The results show that these agents exhibit pronounced visual bias. In controlled trials where only the camera viewpoint (pose) varied while all other factors were fixed, success rates fluctuated sharply even across nearby viewpoints, identifying viewpoint as the most influential factor. Similarly, color-focused trials revealed a strong performance bias towards high-saturation hues over achromatic and low-saturation ones, with the extent of the bias varying by agents. In factorial (“color × viewpoint”) experiments, analyses of the interaction effect showed that viewpoint changes substantially amplify color-induced performance variation, whereas the reverse effect is weaker. This reveals a strong, asymmetric coupling between the two factors and motivates joint evaluation and mitigation. Based on these observations, we propose the “Semantic Grounding and Perceptual Calibration” (SGL) strategy. We execute pre-training alignment instructions and visible evidence, employing color-invariant calibration to reduce visual bias on MOKA Liu et al. (2024a). This research advances the systematic measurement of visual bias, providing a foundation for bias diagnosis and mitigation to enhance embodied agent stability. Our contributions can be summarized in three key aspects:\n\nWe present RoboView-Bias, a factor-isolated benchmark (color, camera viewpoint) that enables quantitative measurement of visual bias in embodied manipulation.\n\nWe provide cross-paradigm evaluations (VLM-driven, VLA) with fine-grained bias profiles, revealing significant bias and strong, asymmetric color–viewpoint coupling along the perception–decision pipeline.\n\nWe introduce SGL (Semantic Grounding Layer), which aligns commands with visible evidence before execution, reducing visual bias and improving agent stability.\n\n1. We present RoboView-Bias, a factor-isolated benchmark (color, camera viewpoint) that enables quantitative measurement of visual bias in embodied manipulation.\n\n2. We provide cross-paradigm evaluations (VLM-driven, VLA) with fine-grained bias profiles, revealing significant bias and strong, asymmetric color–viewpoint coupling along the perception–decision pipeline.\n\n3. We introduce SGL (Semantic Grounding Layer), which aligns commands with visible evidence before execution, reducing visual bias and improving agent stability.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有基准未能系统量化机器人操作中的视觉偏差。  \n2. 视觉偏差影响决策稳定性，需深入理解其影响。  \n3. 需要开发安全可靠的通用具身智能体，依赖于准确的视觉感知。  \n\n【提出了什么创新的方法】  \n本研究提出了RoboView-Bias基准，采用因素隔离原则系统量化机器人操作中的视觉偏差。通过结构化变体生成框架，创建了2,127个任务实例，评估了不同视觉因素的偏差及其相互作用。研究发现所有代理均表现出显著的视觉偏差，尤其是摄像机视角对成功率影响最大。引入的“语义对齐与感知校准”（SGL）策略有效减少了约54.5%的视觉偏差，提升了代理的稳定性。这一工作为视觉偏差的系统分析和缓解提供了基础，推动了具身智能体的安全性和可靠性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM",
            "authors": "Johan Hatleskog,Morten Nissov,Kostas Alexis",
            "subjects": "Robotics (cs.RO)",
            "comment": "accepted by The 22nd International Conference on Advanced Robotics (ICAR 2025). Supplementary video:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.22288",
            "code": "https://youtu.be/95jeWXBMN7c",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22288",
            "arxiv_html_link": "https://arxiv.org/html/2509.22288v1",
            "abstract": "Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor graph node per measurement to compensate for the lack of time synchronization between radar and LiDAR. For a radar-LiDAR sensor pair with equal rates, this strategy results in a state creation rate of twice the individual sensor frequencies. This doubling of the number of states per second yields high optimization costs, inhibiting real-time performance on resource-constrained hardware. We introduce IMU-preintegrated radar factors that use high-rate inertial data to propagate the most recent LiDAR state to the radar measurement timestamp. This strategy maintains the node creation rate at the LiDAR measurement frequency. Assuming equal sensor rates, this lowers the number of nodes by \\qty50 and consequently the computational costs. Experiments on a single board computer (which has 4 cores each of \\qty2.2\\giga A73 and \\qty2\\giga A53 with \\qty8\\giga RAM) show that our method preserves the absolute pose error of a conventional baseline while simultaneously lowering the aggregated factor graph optimization time by up to \\qty56.",
            "introduction": "Accurate and reliable state estimation is a prerequisite for autonomous navigation in GPS-denied and perceptually challenging environments. Recently, Frequency-Modulated Continuous Wave (FMCW) radar has gained increasing attention as a complementary sensor to LiDAR. FMCW radar systems offer direct Doppler velocity measurements and operate robustly in degraded visual conditions such as dust, fog, rain, or darkness as well as geometrically uninformative environments such as tunnels or open spaces [1]. When fused with LiDAR and inertial measurements, radar measurements can improve resilience in scenarios where LiDAR-only approaches degrade.\n\nIn factor graph-based smoothing for multi-modal sensor fusion, the conventional method to address measurement asynchronicity is to create a distinct state node in the graph for each measurement from each asynchronous sensor. For a typical Radar-LiDAR-Inertial setup with a \\qty10 radar and a \\qty10 LiDAR, this results in 20 state nodes being added to the graph every second. The theoretic computational complexity of factor graph optimization scales quadratically with the number of states in the optimization window [2]. Consequently, this doubling of the state creation rate imposes a significant computational burden, which can inhibit real-time performance, especially on the resource-constrained, embedded systems commonly found on mobile robots.\n\nTo address this limitation, we propose a principled approach that avoids creating state nodes for radar measurements. Instead, we introduce IMU-preintegrated radar factors. Our method creates state nodes only upon the arrival of a LiDAR measurement. When a radar measurement is received, we use high-rate IMU data to propagate the state from the preceding LiDAR node to the timestamp of the radar measurement. This propagation yields a predicted state that is used to evaluate the radar factor residual, creating a constraint on the previous LiDAR node without adding a new state to the graph.\n\nWith \\qty10 radar-LiDAR sensor rates, this strategy reduces the number of nodes in the fixed-lag smoother by \\qty50 compared to the standard approach. Our main contributions are:\n\nA new IMU-preintegrated radar factor that removes the need for dedicated radar state nodes.\n\nA computationally efficient factor graph smoother formulation, utilizing the IMU-preintegrated radar factor, suitable for real-time Radar-LiDAR-Inertial fusion on resource-constrained embedded hardware.\n\nExperimental evaluations on real-world datasets on constrained hardware, demonstrating that our approach substantially reduces computational load with a negligible effect on localization accuracy compared to a conventional baseline.\n\nThe remainder of this work is organized as follows: Section II presents related work and the proposed approach is described in Section III. The evaluation study is presented in Section IV and we conclude in Section V.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的多传感器融合方法在处理异步传感器数据时，导致状态节点数量过多，增加了计算负担。  \n2. 资源受限的嵌入式系统上，实时性能受到影响，难以满足自主导航的需求。  \n\n【提出了什么创新的方法】  \n本文提出了一种新颖的IMU预积分雷达因子方法，避免为雷达测量创建独立的状态节点。该方法仅在接收到LiDAR测量时创建状态节点，并利用高频IMU数据将先前的LiDAR状态传播到雷达测量时间戳。这种策略使得固定延迟平滑器中的节点数量减少了50%，显著降低了计算成本。实验结果表明，该方法在保持定位精度的同时，将因子图优化时间降低了多达56%，有效提升了在资源受限硬件上的实时性能。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment",
            "authors": "Ke Ye,Jiaming Zhou,Yuanfeng Qiu,Jiayi Liu,Shihui Zhou,Kun-Yu Lin,Junwei Liang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22205",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22205",
            "arxiv_html_link": "https://arxiv.org/html/2509.22205v1",
            "abstract": "Generalizing to long-horizon manipulation tasks in a zero-shot setting remains a central challenge in robotics. Current multimodal foundation based approaches, despite their capabilities, typically fail to decompose high-level commands into executable action sequences from static visual input alone.\nTo address this challenge, we introduce Super-Mimic, a hierarchical framework that enables zero-shot robotic imitation by directly inferring procedural intent from unscripted human demonstration videos.\nOur framework is composed of two sequential modules. First, a Human Intent Translator (HIT) parses the input video using multimodal reasoning to produce a sequence of language-grounded subtasks. These subtasks then condition a Future Dynamics Predictor (FDP), which employs a generative model that synthesizes a physically plausible video rollout for each step. The resulting visual trajectories are dynamics-aware, explicitly modeling crucial object interactions and contact points to guide the low-level controller.\nWe validate this approach through extensive experiments on a suite of long-horizon manipulation tasks, where Super-Mimic significantly outperforms state-of-the-art zero-shot methods by over 20%. These results establish that coupling video-driven intent parsing with prospective dynamics modeling is a highly effective strategy for developing general-purpose robotic systems.",
            "introduction": "The pursuit of generalized robotic manipulation aims to develop systems capable of performing a wide array of tasks in diverse, unstructured environments without task-specific training. These systems must comprehend high-level instructions, perceive visual scenes, and generate feasible action plans in a zero-shot manner.\nThe recent success of multimodal large language models (MLLMs)[1, 2, 3], pre-trained on vast internet-scale datasets, has demonstrated their inherent capabilities in perception and reasoning[4, 5, 6].\nLeveraging these models as core components presents a promising pathway toward building such general-purpose robotic systems.\n\nHowever, existing zero-shot approaches [7, 8, 9] based on foundation models are often limited to simple, short-horizon tasks, such as picking up a cup.\nWhile some works [10, 11] have attempted more complex, long-horizon tasks like tidying up the floor, their planning and execution rely heavily on textual descriptions.\nThis is problematic because it forces MLLMs to infer complex action sequences from simple descriptions and cluttered visual scenes, a process that is often unreliable and difficult to control.\nFurthermore, providing detailed instructions for complex, long-horizon tasks is laborious, and the constituent steps of a long sequence often cannot be explicitly segmented or described with language.\n\nWe believe that for a robot to achieve human-level proficiency in complex tasks, it should learn from a more direct and richer source of instruction: human demonstrations.\nTo this end, we propose that unscripted human videos serve as a more effective and natural high-level instruction modality.\nThe rich visual information contained within these videos implicitly encodes the necessary subtasks and corresponding manipulation skills required to complete long-horizon tasks.\nTherefore, we introduce the Human Intent Translator (HIT). This module first efficiently extracts key action frames from the human video.\nIt then leverages MLLMs to build a unified intent translation process that generates a transferable subtask plan based on the parsed keyframe sequence and the robot’s current visual observation.\nThis process can also be refined with additional text prompts for targeted plan modification, enabling effective imitation of the long-horizon task demonstrated in the human video.\n\nFurthermore, in a zero-shot setting, the successful execution of each subtask is critical for completing a long-horizon task.\nPrior works often struggle to extract robust dynamic semantics for subtask execution using only MLLMs, which primarily excel at static scene understanding.\nFor instance, MOKA[9] proposes 2D keypoints on a static image to form a trajectory.\nSimilarly, other approaches [7, 8, 12, 13, 14, 15] attempt to regress end-effector poses directly from static observations, often failing to capture the nuances of object interactions.\nIn this work, to ensure robust subtask planning in open-world scenarios, we introduce the Future Dynamics Predictor (FDP), which employs video generation models[16, 17, 18, 19, 20] to prospectively imagine the execution of each subtask.\nThis practice not only preserves the spatial structure and fine details from the robot’s current visual observation but also accurately infers the complex contact dynamics and constraints between multiple objects.\nThese imagined futures serve as a dense, physically-grounded reference for the subsequent control phase.\nFor low-level control, we extract depth estimates and key trajectories from these generated videos, which are then translated into executable 3D target poses and control signals.\nThis provides fine-grained manipulation and stable performance, particularly in scenes involving multi-object interactions.\n\nIn this paper, we present Super-Mimic, a framework that combines the HIT module for task planning and the FDP module for subtask execution to tackle zero-shot long-horizon robotic manipulation (Fig. LABEL:fig:head).\nOur key contributions are:\n\n1) We propose a novel hierarchical framework to solve zero-shot long-horizon manipulation, by using human demonstration videos to guide high-level task planning and video generation models to inform robust low-level control.\n\n2) We introduce the HIT module, which parses complex human actions into transferable, language-guided robotic plans, and the FDP module, which imagines future states to provide dense, physically-grounded guidance for execution.\n\n3) We conduct extensive experiments demonstrating that Super-Mimic significantly outperforms state-of-the-art methods by over 20% in complex, long-horizon tasks and validate the distinct advantages of our hierarchical video-based approach.",
            "llm_summary": "【论文的motivation是什么】  \n1. 长期操控任务的零-shot泛化仍然是机器人技术中的一个核心挑战。  \n2. 现有的多模态基础方法在将高层指令分解为可执行的动作序列时存在局限性。  \n3. 复杂的长时间任务通常依赖于文本描述，导致推理过程不可靠且难以控制。  \n\n【提出了什么创新的方法】  \n本文提出了Super-Mimic，一个分层框架，通过解析人类演示视频中的程序意图，直接实现零-shot机器人模仿。该框架包含两个模块：人类意图翻译器（HIT）和未来动态预测器（FDP）。HIT模块通过多模态推理解析视频，生成语言引导的子任务序列；FDP模块则利用生成模型为每个步骤合成物理上合理的视频展望。这种结合有效地捕捉了对象交互和接触点，从而指导低级控制。经过广泛实验，Super-Mimic在复杂的长时间操控任务中显著超越了现有的零-shot方法，提升超过20%。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training",
            "authors": "Haoyun Li,Ivan Zhang,Runqi Ouyang,Xiaofeng Wang,Zheng Zhu,Zhiqin Yang,Zhentao Zhang,Boyuan Wang,Chaojun Ni,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang,Zhenbo Song,Xingang Wang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22199",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22199",
            "arxiv_html_link": "https://arxiv.org/html/2509.22199v1",
            "abstract": "Vision Language Action (VLA) models derive their generalization capability from diverse training data, yet collecting embodied robot interaction data remains prohibitively expensive. In contrast, human demonstration videos are far more scalable and cost-efficient to collect, and recent studies confirm their effectiveness in training VLA models. However, a significant domain gap persists between human videos and robot-executed videos, including unstable camera viewpoints, visual discrepancies between human hands and robotic arms, and differences in motion dynamics. To bridge this gap, we propose MimicDreamer, a framework that turns fast, low-cost human demonstrations into robot-usable supervision by jointly aligning vision, viewpoint, and actions to directly support policy training.\nFor visual alignment, we propose H2R Aligner, a video diffusion model that generates high-fidelity robot demonstration videos by transferring motion from human manipulation footage.\nFor viewpoint stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos via homography and inpaints occlusions and distortions caused by warping.\nFor action alignment, we map human hand trajectories to the robot frame and apply a constrained inverse kinematics solver to produce feasible, low-jitter joint commands with accurate pose tracking.\nEmpirically, VLA models trained purely on our synthesized human-to-robot videos achieve few-shot execution on real robots. Moreover, scaling training with human data significantly boosts performance compared to models trained solely on real robot data; our approach improves the average success rate by 14.7%14.7\\% across six representative manipulation tasks.",
            "introduction": "Vision Language Action (VLA) models (Black et al., 2024; 2025; X Square Robot Team, 2025; Cheang et al., 2025a; Bjorck et al., 2025) have shown strong generalization in robotic manipulation, but their progress is constrained by the cost and efficiency of data collection. Meanwhile, large-scale datasets (Khazatsky et al., 2025; Collaboration et al., 2025; AgiBot-World-Contributors et al., 2025) often rely on long teleoperation across heterogeneous hardware, which is time-consuming and limits task diversity. Unlike computer vision and natural language processing that can leverage Internet-scale corpora (Schuhmann et al., 2022; Dodge et al., 2021), robotics lacks cheap and abundant data sources. Human demonstrations (Bahl et al., 2022; Lepert et al., 2025; Grauman et al., 2022b) provide a more efficient and lower-cost path. Hand videos and action trajectories can be gathered quickly without continuous robot execution (Chao et al., 2021; Kwon et al., 2021), reducing hardware dependence and maintenance overhead. More importantly, human motion naturally encapsulates strategies and efficiencies observed in real operations, not brittle, preprogrammed paths, but adaptable procedures. Using human demonstrations as a primary data source, therefore, both reduces collection cost and supplies broadly applicable supervision for VLA training.\n\nExisting mimic methods (Wang et al., 2023; Kareer et al., 2024; Xie et al., 2025; Yang et al., 2025a; Qiu et al., 2025) show that human demonstrations can effectively improve robot policy learning. Most of these methods incorporate human data as auxiliary signals or in limited pipelines, rather than turning them into fully robot-usable supervision for large-scale training. Human demonstrations cannot be used directly (Bahl et al., 2022; Kareer et al., 2024) because of domain and embodiment mismatches. We therefore convert human demonstrations into robot supervision and train VLA models end-to-end on the converted data. Direct transfer, however, still faces three gaps: viewpoint, actions, and vision. (1) For the viewpoint, first-person human operation videos are typically captured by moving cameras with parallax and jitter, which complicates spatiotemporal alignment across sequences and tasks. (2) For actions, humans express intent through end-effector trajectories and dexterous manipulation, whereas robots operate in joint space under kinematic and dynamic constraints, making the semantics-to-control mapping often indirect and difficult to implement. (3) For vision, human hands and robot arms differ significantly in appearance, materials, and motion statistics, limiting the direct transfer of visual representations. Existing methods typically address only one of these issues (Kareer et al., 2024; Yang et al., 2025a), lacking a systematic approach that simultaneously tackles viewpoint stabilization, executable action mapping, and visual consistency.\n\nTherefore, we propose MimicDreamer, a framework that turns fast, low-cost human demonstrations into robot-usable supervision by jointly aligning vision, viewpoint, and actions. To bridge the vision gap, we propose H2R Aligner, a video diffusion model that renders high-fidelity robot-arm videos by transferring motion from human manipulation footage while respecting arm geometry and camera priors (Yang et al., 2025c). Quantitative and qualitative results show realistic arm appearance and contact geometry consistent with the source task. For viewpoint stabilization, EgoStabilizer canonicalizes egocentric frames via homography-based warping to a task-level reference view (estimated by averaging per-category rotations) and inpaints distortions or occlusions introduced by warping (Zhou et al., 2023). Experiment results confirm reduced ego-motion drift and improved cross-sequence comparability. To align the action space, we encode intention as relative end-effector pose increments in the shared frame and execute it via a constrained inverse kinematics (IK) solver with distributional normalization and temporal smoothness, yielding feasible, low-jitter joint trajectories. Visualized rollouts exhibit accurate end-effector tracking while respecting joint and velocity limits.\n\nIn experiments, training the VLA model (Black et al., 2024) solely on MimicDreamer-synthesized human to robot videos enables few-shot execution on real robots. Across six representative manipulation tasks, increasing the scale of human demonstration data yields consistent gains, improving an average success rate by 14.7% over a baseline trained only on real robot data.\nThe primary contributions of this work are as follows:\n\n1. We propose MimicDreamer, a unified human–robot egocentric demonstrations transferring framework that simultaneously reduces the human-to-robot discrepancy along vision, viewpoint, and action dimensions and enables scalable VLA training from low-cost human demonstrations.\n\n2. For vision, we introduce H2R Aligner based on video diffusion and geometric camera priors to synthesize high-fidelity robot arm videos. For viewpoint, we introduce EgoStabilizer, which canonicalizes frames to a task reference view by homography and repairs warping occlusions. For actions, we map human hand trajectories to the robot frame and apply constrained IK to produce feasible, low-jitter joint commands with accurate pose tracking.\n\n3. The VLA policy trained on synthesized robot demonstrations achieves few-shot execution on real robots, and across six manipulation tasks, we realize scalable VLA training, improving an average success rate over the robot data baseline by 14.7%14.7\\%, demonstrating both stronger generalization and higher sample efficiency.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人交互数据收集成本高，限制了VLA模型的训练效率。  \n2. 人类演示视频更易收集，但存在领域差距，影响机器人执行效果。  \n3. 现有方法未能系统性解决人类与机器人演示之间的视觉、视角和动作对齐问题。  \n\n【提出了什么创新的方法】  \n提出了MimicDreamer框架，通过联合对齐视觉、视角和动作，将低成本的人类演示转化为机器人可用的监督信号。具体方法包括：  \n- H2R Aligner：利用视频扩散模型生成高保真机器人演示视频，解决视觉差异问题。  \n- EgoStabilizer：通过单应性变换稳定视角，修复因变换引入的遮挡和失真。  \n- 动作对齐：将人类手部轨迹映射到机器人框架，并应用约束逆运动学求解器生成低抖动的关节指令。  \n实验结果表明，基于合成的人类到机器人视频训练的VLA模型在真实机器人上实现了少量示例执行，并在六个代表性操作任务中，成功率平均提高了14.7%。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting",
            "authors": "Asher J. Hancock,Xindi Wu,Lihan Zha,Olga Russakovsky,Anirudha Majumdar",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22195",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22195",
            "arxiv_html_link": "https://arxiv.org/html/2509.22195v1",
            "abstract": "Fine-tuning vision-language models (VLMs) on robot teleoperation data to create vision-language-action (VLA) models is a promising paradigm for training generalist policies, but it suffers from a fundamental tradeoff: learning to produce actions often diminishes the VLM’s foundational reasoning and multimodal understanding, hindering generalization to novel scenarios, instruction following, and semantic understanding. We argue that this catastrophic forgetting is due to a distribution mismatch between the VLM’s internet-scale pretraining corpus and the robotics fine-tuning data. Inspired by this observation, we introduce VLM2VLA: a VLA training paradigm that first resolves this mismatch at the data level by representing low-level actions with natural language. This alignment makes it possible to train VLAs solely with Low-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and averting catastrophic forgetting. As a result, the VLM can be fine-tuned on robot teleoperation data without fundamentally altering the underlying architecture and without expensive co-training on internet-scale VLM datasets. Through extensive Visual Question Answering (VQA) studies and over 800 real-world robotics experiments, we demonstrate that VLM2VLA preserves the VLM’s core capabilities, enabling zero-shot generalization to novel tasks that require open-world semantic reasoning and multilingual instruction following. Website with additional information, videos, and code: https://vlm2vla.github.io/.",
            "introduction": "The pursuit of generalist robot policies capable of understanding and executing human commands has been significantly advanced by the integration of vision-language models (VLMs) [1, 2, 3] throughout the autonomy stack. Trained on internet-scale datasets of image-text pairs, these models have acquired sophisticated capabilities in perception, semantic understanding, and commonsense reasoning. To endow robots with similar capabilities, the prevailing paradigm involves fine-tuning pretrained VLMs on robot demonstration data, transforming them into vision-language-action models (VLAs) that map from natural language commands and visual observations to robot actions. This approach has yielded impressive results across a wide range of robotic manipulation tasks [4, 5, 6, 7, 8, 9, 10, 11].\n\nHowever, the standard methodology of fundamentally modifying the VLM’s architecture, tokenization vocabulary, or a combination thereof, coupled with full parameter fine-tuning on robot imitation learning data, introduces a crucial yet often overlooked trade-off. In adapting the VLM for robotic control, we risk overfitting to the robot fine-tuning data, thereby overwriting the general-purpose world knowledge acquired during pretraining (see Fig. 2). The consequences of this trade-off are far-reaching: current VLAs often exhibit a diminished ability to generalize to novel objects, handle linguistic variations, be robust to distractions, or reason about concepts outside the narrow scope of their robotic training data [4, 9, 12, 13].\n\nPreserving the VLM’s foundational world knowledge during VLA fine-tuning is essential for creating truly generalist robot policies; consequently, numerous techniques have been developed to address this challenge. The most common approach is to co-train with non-robotic data. This training regimen regularizes the VLM against overfitting to robot datasets, thereby mitigating the loss of its foundational capabilities [9, 6, 14]. While these methods can mitigate knowledge loss, co-training with VLM-scale datasets is inherently expensive and requires a carefully constructed dataset mixture for optimal performance.\n\nThis paper aims to preserve the world knowledge of the VLM while adapting it for robotic control without co-training. We address this issue by resolving the distribution mismatch between the low-level action spaces needed for robotic control and the image-text distributions of the VLM’s pretraining corpus. This mismatch often compels researchers to use full parameter fine-tuning when training VLAs, which contributes to catastrophic forgetting by overfitting to the robot teleoperation data.\n\nOur key insight is that while parameter-efficient methods like Low-Rank Adaptation (LoRA) [15] can avert catastrophic forgetting, their effectiveness relies on the fine-tuning data being sufficiently close to the model’s pretrained representations. We therefore propose resolving this representational mismatch at the data level. Our data-centric approach re-represents robot actions as natural language descriptions, thereby aligning the VLA fine-tuning data directly with the VLM’s pretrained representation space. This alignment enables LoRA to effectively adapt the VLM for robotic control without significantly perturbing its pretrained weights. Fig. 3 illustrates this idea, showing our language-based actions are assigned significantly higher probabilities by the VLM backbone than actions mapped to arbitrary tokens, a common strategy in state-of-the-art VLAs [8, 4, 9, 10, 11]. Our method is model agnostic and simple to implement, obviating the need for sophisticated architectures, complex co-training schemes, or multi-stage training procedures to achieve robust knowledge retention and superior generalization capabilities.\n\nStatement of Contributions. We present VLM2VLA, a data pipeline and training methodology for fine-tuning VLMs into VLAs while preserving their foundational perceptual and reasoning capabilities. Our core contributions are as follows: 1) Representing actions as language: We propose translating low-level robotic imitation data into text, thereby aligning the VLA fine-tuning data with the VLM’s pretraining distribution to mitigate catastrophic forgetting. 2) A data re-labeling and training pipeline for knowledge retention: Building on our action representation, we present a scalable methodology for re-labeling robot teleoperation datasets for fine-tuning a VLM into a VLA through LoRA. 3) Empirical validation of action and reasoning capabilities: We provide extensive empirical validation showing our VLA preserves a suite of crucial capabilities that are often lost in other state-of-the-art models. Specifically, our method’s efficacy is demonstrated through extensive real-world evaluation (over 800 robotic experiments), showing generalization to novel tasks with objects and language instructions unseen during training. Moreover, our policy averts catastrophic forgetting, retaining over 85% of the base model’s performance across challenging VQA benchmarks.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的VLM在机器人控制中容易导致灾难性遗忘，影响其推理和多模态理解能力。  \n2. 机器人控制数据与VLM预训练数据之间存在分布不匹配，导致模型泛化能力下降。  \n3. 传统的共训练方法成本高且复杂，难以有效保留VLM的基础知识。  \n\n【提出了什么创新的方法】  \n本研究提出了VLM2VLA，一个数据驱动的训练方法，通过将低级动作表示为自然语言，解决了VLM与机器人控制数据之间的分布不匹配问题。该方法利用低秩适应（LoRA）技术，最小化对VLM主干的修改，从而避免灾难性遗忘。通过在超过800个真实世界机器人实验中的验证，VLM2VLA展示了在新任务上的零-shot泛化能力，保持了VLM的核心能力，能够处理开放世界的语义推理和多语言指令。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions",
            "authors": "Quanzhou Li,Zhonghua Wu,Jingbo Wang,Chen Change Loy,Bo Dai",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22175",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22175",
            "arxiv_html_link": "https://arxiv.org/html/2509.22175v1",
            "abstract": "Learning to generate dual-hand grasps that respect object semantics is essential for robust hand–object interaction but remains largely underexplored due to dataset scarcity. Existing grasp datasets predominantly focus on single-hand interactions and contain only limited semantic part annotations. To address these challenges, we introduce a pipeline, SymOpt, that constructs a large-scale dual-hand grasp dataset by leveraging existing single-hand datasets and exploiting object and hand symmetries. Building on this, we propose a text-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand Affordance-aware Grasps for unseen objects. Our approach incorporates a novel dual-hand affordance representation and follows a two-stage design, which enables effective learning from a small set of segmented training objects while scaling to a much larger pool of unsegmented data. Extensive experiments demonstrate that our method produces diverse and semantically consistent grasps, outperforming strong baselines in both grasp quality and generalization to unseen objects. The project page is at https://quanzhou-li.github.io/DHAGrasp/.",
            "introduction": "Defining effective grasp targets is fundamental for enabling robust interaction between hands and objects. While extensive research\n[1]\nhas been conducted on single-hand grasping, more sophisticated tasks often require coordinated dual-hand grasps. Moreover, in practical situations, interaction with specific parts of an object is frequently necessary, calling for a generator that generates coordinated two-hand grasps guided by the functional semantics of object parts rather than geometry along.\n\nDespite the importance of dual-hand approaches, research in this area remains underexplored, primarily due to the scarcity of suitable two-hand grasp datasets. Existing datasets, such as [2] and [3], include sequences of two-hand manipulations but focus mainly on manipulation motions rather than the grasping phase itself and feature only a limited number of objects with restricted grasp diversity. In the absence of two-hand grasp datasets, approaches such as [4] employ energy-based optimization to generate dual-hand grasps. These processes, however, tend to be slow and yield relatively low success rates.\n\nTo overcome these limitations, we propose a novel approach that leverages the abundance of existing single-hand grasp datasets. Specifically, we exploit object and hand symmetries to construct a large-scale dual-hand grasp dataset. Most objects exhibit approximate bilateral symmetry; thus, we mirror ground-truth right-hand grasps across an identified pseudo-symmetric plane to obtain corresponding left-hand grasps. These mirrored grasps are then combined into candidate dual-hand proposals. To ensure physical plausibility, we introduce an optimization scheme, termed SymOpt, which eliminates interpenetrations between the hands and the object. Using this pipeline, we derive a large-scale dataset, DualHands-Full, from DexGraspNet [5], and further construct DualHands-Sem, a semantics-augmented subset that associates functional object parts with dual-hand grasps. As summarized in Table I, our dataset features a substantially larger number of grasp configurations per object compared with prior work and uniquely incorporates semantics into dual-hand grasping.\n\nAlthough our dataset contains abundant dual-hand grasps, it inherits the limitations of single-hand datasets, namely the limited availability of consistent semantic part labels, which are costly to obtain at scale. Directly training on dual-hand data with such limited segmentation would lead to suboptimal performance. To address this problem, we propose a novel dual-hand contact representation, which bridges the unsegmented objects to the objects with semantics. Our contact representation consists of a contact map, a part map, and most importantly, a set of affordance directions. By leveraging the representation, our approach learns effectively from a small subset of segmented training objects while scaling to a much larger pool of unsegmented data.\n\nWith the dual-hand contact representation, we introduce a text-guided Dual-Hand Affordance-aware Grasp (DHAGrasp) generator. The key intuition is inspired by how humans recognize the functional parts of novel objects by drawing on prior knowledge of similar items. Specifically, our generator operates in two stages: (1) Text2Dir, a semantics-based affordance module that predicts category-level affordance directions from object geometry and text embeddings; and (2) Dir2Grasp, an affordance-conditioned grasp synthesizer that generates dual-hand grasps aligned with the predicted affordance directions. This design enables effective utilization of limited annotated data while scaling to large unsegmented datasets.\n\nWe conduct extensive experiments to evaluate both our datasets and the grasp generation method. Our data generation approach, SymOpt, produces datasets an order of magnitude larger than those in previous works and yields substantially higher success rates on the generated grasps. We further demonstrate the effectiveness of our generation pipeline, DHAGrasp, which excels in two-hand coordination and outperforms prior methods across multiple metrics.\n\nIn summary, our contributions are fourfold. 1) we introduce a pipeline, SymOpt, that constructs a dual-hand object grasp dataset leveraging the advances of single-hand datasets. 2) We assemble a large-scale dual-hand grasp dataset DualHands-Full, together with a semantics-based sub-dataset, DualHands-Sem. 3) We propose a novel dual-hand contact representation that enables within-category generalization. 4) We design a Dual-Hand Affordance-aware Grasp Model, DHAGrasp, that synthesizes dual-hand grasps based on text instructions.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的双手抓取研究较少，缺乏适合的双手抓取数据集。  \n2. 现有数据集主要集中在单手抓取，且抓取多样性有限。  \n3. 生成双手抓取需要考虑物体的功能语义，而不仅仅是几何形状。  \n\n【提出了什么创新的方法】  \n本文提出了一种名为SymOpt的流程，用于构建大规模的双手抓取数据集，利用现有的单手数据集和物体、手的对称性。基于此，我们设计了一个文本引导的双手抓取生成器DHAGrasp，能够为未见过的物体合成双手抓取。该方法采用了新颖的双手接触表示，并遵循两阶段设计，有效地从少量分割训练物体中学习，同时扩展到更大规模的未分割数据集。实验结果表明，我们的方法在抓取质量和对未见物体的泛化能力上均优于强基线，生成的抓取多样且语义一致。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "DemoGrasp: Universal Dexterous Grasping from a Single Demonstration",
            "authors": "Haoqi Yuan,Ziye Huang,Ye Wang,Chuan Mao,Chaoyi Xu,Zongqing Lu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22149",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22149",
            "arxiv_html_link": "https://arxiv.org/html/2509.22149v1",
            "abstract": "Universal grasping with multi-fingered dexterous hands is a fundamental challenge in robotic manipulation. While recent approaches successfully learn closed-loop grasping policies using reinforcement learning (RL), the inherent difficulty of high-dimensional, long-horizon exploration necessitates complex reward and curriculum design, often resulting in suboptimal solutions across diverse objects. We propose DemoGrasp, a simple yet effective method for learning universal dexterous grasping. We start from a single successful demonstration trajectory of grasping a specific object and adapt to novel objects and poses by editing the robot actions in this trajectory: changing the wrist pose determines where to grasp, and changing the hand joint angles determines how to grasp. We formulate this trajectory editing as a single-step Markov Decision Process (MDP) and use RL to optimize a universal policy across hundreds of objects in parallel in simulation, with a simple reward consisting of a binary success term and a robot–table collision penalty. In simulation, DemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow Hand, outperforming previous state-of-the-art methods. It also shows strong transferability, achieving an average success rate of 84.6% across diverse dexterous hand embodiments on six unseen object datasets, while being trained on only 175 objects. Through vision-based imitation learning, our policy successfully grasps 110 unseen real-world objects, including small, thin items. It generalizes to spatial, background, and lighting changes, supports both RGB and depth inputs, and extends to language-guided grasping in cluttered scenes.",
            "introduction": "Universal dexterous grasping [2, 13] is a fundamental capability for real-world robots. The anthropomorphic design of dexterous robotic hands makes them the most suitable manipulators for real-world manipulation tasks, such as tool use, in-hand reorientation, and bimanual coordination. Universal grasping is therefore an essential prerequisite for enabling these sophisticated interactions. Though basic in concept, learning universal dexterous grasping policies remains far from simple.\nThe high-dimensional action space introduced by dexterous hands with many degrees of freedom (DoFs), together with the long-horizon nature of closed-loop grasping, imposes substantial exploration challenges for reinforcement learning (RL). At the same time, the diverse geometries of objects make universal dexterous grasping a multi-task optimization problem, introducing additional difficulties such as catastrophic forgetting [20, 32] and gradient interference [34, 43].\n\nRecent studies have extensively investigated the use of RL for training universal dexterous grasping policies. Xu et al. [41], Wan et al. [35], Zhang et al. [46], Chen et al. [8] introduce techniques in observation feature design, dense reward shaping, and curriculum learning strategies to facilitate policy learning. UniDexGrasp++ [35] employs an iterative distillation process to improve teacher–student learning. ResDex [16] introduces a two-stage residual RL framework to accelerate multi-task exploration. UniGraspTransformer [37] proposes exhaustive RL on individual objects and distillation with expressive Transformer policies to bypass multi-task RL.\nHowever, many of these approaches train on hands without robot arms [41, 35], use privileged contact information as observations [35, 16], and face a trade-off between collision penalties and other complex reward terms [41, 16], limiting their potential for deployment on real robots. Singh et al. [33], Zhang et al. [46] achieve sim-to-real on a wide variety of objects but still fall short on grasping small, thin objects in tabletop settings. In addition, their reliance on complicated observation design, reward shaping, and multi-stage pipelines increases the barrier to extending these methods to new embodiments and task settings.\n\nIn this research, we propose DemoGrasp, a simple yet powerful framework for universal dexterous grasping that addresses these challenges. Our key insight is that a single demonstration trajectory of grasping a specific object encodes many transferable patterns for universal grasping, such as approaching the object’s grasp center, squeezing the hand pose, and lifting the wrist. To grasp various objects in different poses, we can slightly modify the robot actions within this trajectory and replay the edited actions. For example, to grasp the same object at a different location, we can apply a transformation to the wrist poses in the trajectory, changing where to grasp; to grasp a larger object at the same position, we adjust the grasp poses to be more open, changing how to grasp. In our method, the RL policy explores how to edit the demonstration along these two axes, rather than exploring in the low-level robot action space as in prior methods, resulting in more efficient trial-and-error.\n\nSpecifically, we formulate the demonstration-editing task as a single-step Markov Decision Process (MDP). At each trial, given an arbitrary object placed at a random position, the policy outputs an SE​(3)\\mathrm{SE}(3) transformation and delta hand joint angles, which are used to modify the end-effector poses and hand actions in the demonstration. The edited demonstration is then replayed in simulation, yielding a reward for the whole episode.\nBy restricting the policy to a compact action space and a single-step decision-making horizon, the multi-task exploration burden is significantly reduced, removing the need for complex reward shaping. This enables us to effectively train a universal grasping policy on hundreds or thousands of objects by optimizing a simple combination of binary success reward and a collision penalty. We observe that this design yields both superior performance in simulation and easy sim-to-real transfer with minimal collisions. We train a flow-matching [23] policy on successful rollouts of the learned policy with rendered camera images in simulation, enabling zero-shot deployment on a real robot.\n\nWe conduct large-scale experiments in both simulation and the real world to evaluate DemoGrasp. On 3.4K objects from DexGraspNet [36], DemoGrasp achieves success rates of 95% in state-based settings and 92% in vision-based settings, surpassing previous state-of-the-art methods by a large margin. DemoGrasp also exhibits strong transferability to a wide variety of robotic embodiments and generalization to unseen object categories. Trained on 175 objects, the policies achieve an average success rate of 84.6% on six unseen object datasets across various embodiments, including dexterous hands with different numbers of fingers, grippers, and arm–hand systems.\nIn real-world experiments, DemoGrasp achieves a success rate of 86.5% on 110 unseen objects, covering a wide variety of geometries and visual appearances. For normal-sized objects, it achieves a superior success rate of 95.3%. Benefiting from the simple reward design, the policy is, to our knowledge, the first to grasp previously unseen small, thin objects in tabletop settings without severe collisions, achieving a success rate of 71.1%. DemoGrasp also exhibits generalization to spatial, background, and lighting changes, and is extensible to various camera configurations (RGB and depth) and cluttered scenes, underscoring its practical applicability.\n\nOur contributions are summarized as follows:\n\nWe propose DemoGrasp, a simple yet powerful learning framework that addresses key challenges in learning universal dexterous grasping policies. With a novel formulation of demonstration editing and single-step RL, DemoGrasp enables robust policy learning, minimal reliance on reward shaping, and sim-to-real transferability.\n\nDemoGrasp achieves state-of-the-art performance in large-scale evaluations in both simulation and the real world, demonstrating strong capability in grasping diverse, unseen objects on real robots.\n\nWe demonstrate the strong extensibility of DemoGrasp to novel embodiments, camera configurations, and cluttered scenes, establishing a foundation for future research and applications in dexterous manipulation.\n\n1. We propose DemoGrasp, a simple yet powerful learning framework that addresses key challenges in learning universal dexterous grasping policies. With a novel formulation of demonstration editing and single-step RL, DemoGrasp enables robust policy learning, minimal reliance on reward shaping, and sim-to-real transferability.\n\n2. DemoGrasp achieves state-of-the-art performance in large-scale evaluations in both simulation and the real world, demonstrating strong capability in grasping diverse, unseen objects on real robots.\n\n3. We demonstrate the strong extensibility of DemoGrasp to novel embodiments, camera configurations, and cluttered scenes, establishing a foundation for future research and applications in dexterous manipulation.",
            "llm_summary": "【论文的motivation是什么】  \n1. Universal dexterous grasping is essential for real-world robotic manipulation tasks.  \n2. High-dimensional action spaces and long-horizon exploration pose significant challenges for reinforcement learning in grasping.  \n3. Existing methods often rely on complex reward structures and are limited in their applicability to real robots.  \n\n【提出了什么创新的方法】  \nDemoGrasp introduces a framework for universal dexterous grasping that leverages a single successful demonstration trajectory. By editing the robot's actions in this trajectory—adjusting wrist poses for where to grasp and hand joint angles for how to grasp—it formulates this as a single-step Markov Decision Process (MDP). This approach simplifies the exploration process, allowing for efficient learning and optimization of a universal policy across hundreds of objects. In simulations, DemoGrasp achieves a 95% success rate on DexGraspNet objects and demonstrates strong transferability to unseen objects and diverse robotic embodiments, including successful grasping of small, thin items in real-world settings.  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation",
            "authors": "Xiaohuan Pei,Yuxing Chen,Siyu Xu,Yunke Wang,Yuheng Shi,Chang Xu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22093",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22093",
            "arxiv_html_link": "https://arxiv.org/html/2509.22093v1",
            "abstract": "Robotic manipulation with Vision-Language-Action models requires efficient inference over long-horizon multi-modal context, where attention to dense visual tokens dominates computational cost. Existing methods optimize inference speed by reducing visual redundancy within VLA models, but they overlook the varying redundancy across robotic manipulation stages. We observe that the visual token redundancy is higher in coarse manipulation phase than in fine-grained operations, and is strongly correlated with the action dynamic.\nMotivated by this observation, we propose Action-aware Dynamic Pruning (ADP), a multi-modal pruning framework that integrates text-driven token selection with action-aware trajectory gating. Our method introduces a gating mechanism that conditions the pruning signal on recent action trajectories, using past motion windows to adaptively adjust token retention ratios in accordance with dynamics, thereby balancing computational efficiency and perceptual precision across different manipulation stages.\nExtensive experiments on the LIBERO suites and diverse real-world scenarios demonstrate that our method significantly reduces FLOPs and action inference latency (e.g. 1.35× speed up on OpenVLA-OFT) while maintaining competitive success rates (e.g. 25.8% improvements with OpenVLA) compared to baselines, thereby providing a simple plug-in path to efficient robot policies that advances the efficiency and performance frontier of robotic manipulation. Our project website is: ADP.com.",
            "introduction": "Large vision language models Liu et al. (2023c; b; 2024a); Team et al. (2023); Awadalla et al. (2023) have recently been extended into Vision–Language-Action (VLA) models Kim et al. (2024; 2025); Black et al. (2024); Li et al. (2024); Brohan et al. (2024); Wen et al. (2025b; a); Bjorck et al. (2025) that map both the visual observation and language instruction to executable robot actions. In the mainstream pipeline, a vision encoder produces dense visual tokens from one or more camera views, a projector aligns them to the language space, and an LLM fuses all modalities to predict actions. However, this multi-modal design introduces long input sequences with numerous visual tokens that are only weakly relevant to the current manipulation operation,\nwhich inflates compute, memory footprint, and latency, and it can dilute attention over truly task-relevant cues.\n\nExisting work pursues efficiency via architectural lightening and modality-aware compression, such as RoboMamba Liu et al. (2024b) that focuses on lightweight designs, DeeR-VLA Yue et al. (2024) that aims at structured pruning/reparameterization, Mole-VLA Zhang et al. (2025) that targets conditional layer activation, VLA-Cache Xu et al. (2025) that focuses on cache reuse, and EfficientVLA Yang et al. (2025) that aims to prune visual tokens via attention.\nHowever, a key but underexplored property of robotic manipulation is that visual redundancy in VLAs is action-aware across different manipulation stages.\nAs Fig. 1 shows, during coarse-grained operations (e.g., relocating), global movement dominates and redundant tokens can be pruned; during fine-grained phases (e.g., grasping), local geometry and detailed cues dominate and preserving full vision is preferred.\nMoreover, the relevance of visual patches is not only text conditioned (semantics of the instruction) but also action conditioned (instantaneous end-effector motion and gripper state). Treating all steps uniformly, or ranking tokens solely by mixed attention scores, therefore yields suboptimal pruning schedules that either prune too little (limited savings) or prune too much (accuracy loss), especially in multi-view settings (scene and wrist/gripper cameras) where importance is unevenly distributed across\ntime.\n\nTo address this challenge, we introduce Action-aware Dynamic Pruning (ADP), a plug-and-play strategy that reduces computation while preserving manipulation fidelity. ADP is built on two complementary ideas: (1) Text-driven Pruning evaluates the relevance of visual patch using cross-modal similarities, selecting only the most relevant tokens before entering deep fusion in the subsequent layers. (2) Action-aware Dynamics modulate whether pruning is activated at a given step using a lightweight decision signal derived from the end-effector trajectory within each action window.\nSpecifically, when the recent motion magnitude is relatively low compared to past motion statistics (delicate phases), pruning is disabled to preserve the full visual field for precise control. Conversely, when the motion magnitude is relatively high compared to past motion statistics (coarse phases), pruning is engaged to suppress redundancy and save FLOPs.\nWe implement a gated mechanism that treats recent action statistics as a pruning signal over sliding trajectory windows, adaptively adjusting retention ratios according to motion dynamics and balancing efficiency with precision across manipulation stages. Our contributions can be summarized as:\n(1) We show that the importance of the visual token in VLA models varies within different stages of robotic manipulation. This insight motivates our dynamic pruning method tailored to manipulation phases compared to static pruning approaches.\n(2) We propose text-driven action-aware pruning that combines task instruction relevance with a gating rule based on end-effector motion, enabling adaptive switching between pruned and full-vision states.\n(3) We present a principled complexity analysis and extensive experiments in simulation and real-world settings, demonstrating that our method reduces FLOPs and latency while maintaining fine visual details required for successful manipulation.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有VLA模型在长时间多模态上下文推理中计算成本高。  \n2. 视觉标记的冗余性在不同的操作阶段变化，现有方法未能考虑这一点。  \n3. 视觉冗余与动作动态密切相关，影响了操作的效率和精度。  \n\n【提出了什么创新的方法】  \n提出了Action-aware Dynamic Pruning (ADP)方法，通过文本驱动的标记选择和动作感知的轨迹门控机制，动态调整视觉标记的保留比例。该方法在粗操作阶段启用剪枝以减少冗余，而在精细操作阶段则保留完整视觉信息。通过这种方式，ADP在保持操作精度的同时显著提高了计算效率，实验结果显示在OpenVLA-OFT上实现了1.35倍的速度提升，并在成功率上有25.8%的改善，展示了其在机器人操作中的有效性和实用性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot",
            "authors": "Ethan Fulcher,J. Diego Caporale,Yifeng Zhang,John Ruck,Feifei Qian",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "7+ICRA Submission This work has been submitted to the IEEE for possible publication",
            "pdf_link": "https://arxiv.org/pdf/2509.22065",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22065",
            "arxiv_html_link": "https://arxiv.org/html/2509.22065v1",
            "abstract": "In-situ robotic exploration is an important tool for advancing knowledge of geological processes that describe the Earth and other Planetary bodies.\nTo inform and enhance operations for these roving laboratories, it is imperative to understand the terramechanical properties of their environments, especially for traversing on loose, deformable substrates.\nRecent research suggested that legged robots with direct-drive and low-gear ratio actuators can sensitively detect external forces, and therefore possess the potential to measure terrain properties with their legs during locomotion, providing unprecedented sampling speed and density while accessing terrains previously too risky to sample.\nThis paper explores these ideas by investigating the impact of gait on proprioceptive terrain sensing accuracy, particularly comparing a sensing-oriented gait, Crawl N’ Sense, with a locomotion-oriented gait, Trot-Walk.\nEach gait’s ability to measure the strength and texture of deformable substrate is quantified as the robot locomotes over a laboratory transect consisting of a rigid surface, loose sand, and loose sand with synthetic surface crusts. Our results suggest that with both the sensing-oriented crawling gait and locomotion-oriented trot gait, the robot can measure a consistent difference in the strength (in terms of penetration resistance) between the low- and high-resistance substrates; however, the locomotion-oriented trot gait contains larger magnitude and variance in measurements.\nFurthermore, the slower crawl gait can detect brittle ruptures of the surface crusts with significantly higher accuracy than the faster trot gait.\nOur results offer new insights that inform legged robot “sensing during locomotion” gait design and planning for scouting the terrain and producing scientific measurements on other worlds to advance our understanding of their geology and formation.",
            "introduction": "Robotic exploration plays a crucial role in advancing our understanding of Earth and other planetary bodies by enabling in situ experiments remotely[1].\nMany terrestrial and planetary environments (Figs. 1, 1 and 2) present significant challenges for traditional wheeled rovers[2], as hazardous terrains limit access to areas of scientific interest.\nGiven the high mission costs and risks, scientists must be risk-averse, often foregoing scientific opportunities that pose potential threats to robotic platforms.\nThis is partly due to the challenge in acquiring terramechanical information about surface properties such as regolith strength and texture[3], which are difficult to infer without direct tactile feedback.\nTraditional methods[3] require dedicated sensors that demand stopping the rover for measurements, leading to sparse data collection and operational inefficiencies.\n\nLegged robots[6, 7, 8, 9, 10] offer an alternative mobility paradigm that can expand the operational envelope of planetary exploration.\nRecent work has demonstrated their potential not only for traversing extreme terrain[11, 12, 13, 14] but also for proprioceptively “feeling” surface interactions as they walk, effectively acting as penetrometers and making every step an experiment.\nRecent advances in actuator transparency and torque density have led to accurate force estimation in direct-drive ( i.e., gearless) [15] and quasi-direct-drive (QDD,  i.e., low gear ratio) robotic limbs (Figs. 1 and 1), which has shown promise in enabling high-density ground reaction force measurements [12, 16].\nHowever, most prior work has focused on static or slow-moving test setups[17, 12, 4, 16, 18] (Figs. 1 and 1.1).\nHere, we explore the possibility for proprioceptive terrain sensing to be effectively conducted while a legged robot is in motion (Figs. 1 and 1.2-3).\nThis capability would allow every step to serve as a scientific experiment, significantly increasing the density and speed of geotechnical surveys and planetary explorations.\nFrom a scientific perspective, this could reveal spatial gradients in surface properties[19, 20, 21], providing deeper insight into planetary surface processes[12].\nOperationally, it could enhance mission planning by informing rover and astronaut activities such as excavation, sampling, navigation[22], and construction [23].\n\nTo achieve this, a key question that needs to be answered is: how does gait design affect the accuracy and coverage of proprioceptive regolith sensing?\nHigh-speed locomotion enables rapid data collection but introduces complex regolith reaction force profiles [24] that can complicate signal interpretation, potentially reducing measurement accuracy.\nConversely, low-speed gaits, which keep footsteps in the quasi-static regime, provide higher confidence in force estimation but limit spatial coverage.\n\nTo answer these questions, this study examines how variations in gait design influence a QDD quadrupedal robot’s ability to measure terrain properties such as strength and texture change.\nField experiments with a mounted leg penetrometer (Figs. 1 and 1.1), in an analogue environment (Fig. 2) motivate the need for sensitive, dense measurement of the terrain strength (Sec. II).\nFurthermore, we develop a custom sensing-oriented gait, the Crawl N’ Sense (Fig. 1.2), and compare it to a baseline locomotion-oriented gait, the Trot-Walk (Fig. 1.3), in\nlab experiments (Sec. III).\nAs the robot traverses varied terrains, we characterize the proprioceptive joint signals and propose methods to extract terrain properties from these signals, including a method for proprioceptively estimating the ground surface on soft terrain via a contact-based correction (Sec. III-C).\nUsing the proposed methods, we systematically compare the two gaits in terms of sensing accuracy for (i) characterizing regolith strength (Sec. IV) and (ii) detecting texture and layering (Sec. V).\nOur results provide key insights into the gait design principles needed to optimize both mobility and sensing in legged robots, with implications for deployment in complex earth and planetary surface explorations.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统轮式探测器在极端地形中面临挑战，限制了科学探索。  \n2. 需要高效获取地表机械特性信息，以提高探测效率和数据密度。  \n3. 探索在运动中进行本体感知的可能性，以增强地质调查的速度和准确性。  \n\n【提出了什么创新的方法】  \n本研究开发了一种新的感知导向步态“Crawl N’ Sense”，并与传统的“Tro-Walk”步态进行比较。通过在不同的实验环境中进行测试，量化了两种步态在测量变形基质强度和纹理方面的准确性。结果表明，尽管两种步态都能有效测量基质强度，但“Crawl N’ Sense”在检测表面破裂方面的准确性显著高于“Tro-Walk”。这些发现为优化腿式机器人在复杂地表探测中的步态设计提供了重要见解。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose",
            "authors": "Qifeng Wang,Weigang Li,Lei Nie,Xin Xu,Wenping Liu,Zhe Xu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22058",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22058",
            "arxiv_html_link": "https://arxiv.org/html/2509.22058v1",
            "abstract": "As a key technology for autonomous navigation and positioning in mobile robots, light detection and ranging (LiDAR) odometry is widely used in autonomous driving applications. The Iterative Closest Point (ICP)-based methods have become the core technique in LiDAR odometry due to their efficient and accurate point cloud registration capability. However, some existing ICP-based methods do not consider the reliability of the initial pose, which may cause the method to converge to a local optimum. Furthermore, the absence of an adaptive mechanism hinders the effective handling of complex dynamic environments, resulting in a significant degradation of registration accuracy. To address these issues, this paper proposes an adaptive ICP-based LiDAR odometry method that relies on a reliable initial pose. First, distributed coarse registration based on density filtering is employed to obtain the initial pose estimation. The reliable initial pose is then selected by comparing it with the motion prediction pose, reducing the initial error between the source and target point clouds. Subsequently, by combining the current and historical errors, the adaptive threshold is dynamically adjusted to accommodate the real-time changes in the dynamic environment. Finally, based on the reliable initial pose and the adaptive threshold, point-to-plane adaptive ICP registration is performed from the current frame to the local map, achieving high-precision alignment of the source and target point clouds. Extensive experiments on the public KITTI dataset demonstrate that the proposed method outperforms existing approaches and significantly enhances the accuracy of LiDAR odometry.",
            "introduction": "Light detection and ranging (LiDAR), with its high-precision distance measurement and 3D modeling capabilities, provides significant advantages in low-light environments. Consequently, it has become an essential sensor in autonomous driving and robotic systems [1, 2]. Leveraging its depth perception, LiDAR odometry technology has emerged to enhance autonomous navigation, enabling real-time motion state calculation and high-precision positioning in dynamic, complex environments [3, 4, 5].\n\nAmong the various LiDAR odometry methods, Iterative Closest Point (ICP)-based methods have become the mainstream due to their efficient and accurate point cloud registration capabilities [6, 7, 8]. The ICP-based methods estimate pose by iteratively aligning the source point cloud with the target point cloud through optimization. However, existing ICP-based methods present significant limitations in practical applications. Firstly, these methods often do not adequately consider the reliability of the initial pose estimation, making the algorithm susceptible to converging to local optima when the initial pose is inaccurate, thereby leading to registration failures or decreased accuracy [9, 10, 11]. Secondly, traditional ICP-based methods lack adaptive mechanisms, making it challenging to effectively handle dynamic and complex environmental changes, such as numerous moving objects or rapidly changing conditions. This results in a substantial reduction in registration accuracy and negatively impacts the overall performance of LiDAR odometry [12, 13, 14].\n\nTo address these challenges, this paper proposes an adaptive ICP-based LiDAR odometry method based on a reliable initial pose. The method first preprocesses the source point cloud through distributed coarse registration using density filtering to obtain an initial pose estimation. Subsequently, historical pose information is incorporated for motion prediction and compared with the initial pose obtained in the previous step to select the most reliable initial pose, thereby reducing the initial error between the source and target point clouds. Furthermore, an adaptive threshold is dynamically adjusted based on the current and historical errors, which enables the method to flexibly adjust registration parameters according to different motion states and real-time environments. Finally, an adaptive weight mechanism is employed to weight each pair of points during the point cloud registration process, reducing the influence of outliers and achieving high-precision alignment of the source and target point clouds. Fig. 1 shows the model of our approach. To verify the effectiveness of the proposed method, we conducted extensive experimental evaluations using the public KITTI dataset and compare our approach with the latest mainstream LiDAR odometry methods. The experimental results demonstrate that the proposed method outperforms existing approaches in registration accuracy, effectively enhancing the overall performance of LiDAR odometry in complex dynamic environments.\n\nThe main contributions of our work are as follows:\n\nTo address the issue of unreliable initial poses leading to local optimal solutions, a method for obtaining a more reliable initial pose is proposed. Distributed coarse registration based on density filtering is first employed to estimate the initial pose. The reliable initial pose is then selected by comparing it with the motion prediction pose, thereby minimizing the initial error between the source and target point clouds.\n\nTo solve the problem of insufficient adaptability in dynamic environment, an adaptive frame-to-local map ICP registration method is proposed. The registration parameters were dynamically adjusted by combining current and historical errors to enhance the adaptability of the method in dynamic environments.\n\nTo evaluate the effectiveness of these methods in real-world scenarios, extensive experiments were conducted using the public KITTI dataset [15, 11, 16, 14]. These experiments compared our method with existing mainstream LiDAR odometry methods. The results demonstr ate that the proposed method is superior to other mainstream odometry methods in terms of accuracy.\n\nThe rest of this paper is as follows: Section II introduces related work and reviews the current research progress in LiDAR odometry and the ICP-based methods. Section III elaborates on the theoretical foundation and implementation steps of the proposed method. Section IV presents the experimental results and analysis. Finally, Section V concludes the paper and discusses future research directions.\n\n1. To address the issue of unreliable initial poses leading to local optimal solutions, a method for obtaining a more reliable initial pose is proposed. Distributed coarse registration based on density filtering is first employed to estimate the initial pose. The reliable initial pose is then selected by comparing it with the motion prediction pose, thereby minimizing the initial error between the source and target point clouds.\n\n2. To solve the problem of insufficient adaptability in dynamic environment, an adaptive frame-to-local map ICP registration method is proposed. The registration parameters were dynamically adjusted by combining current and historical errors to enhance the adaptability of the method in dynamic environments.\n\n3. To evaluate the effectiveness of these methods in real-world scenarios, extensive experiments were conducted using the public KITTI dataset [15, 11, 16, 14]. These experiments compared our method with existing mainstream LiDAR odometry methods. The results demonstr ate that the proposed method is superior to other mainstream odometry methods in terms of accuracy.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的ICP方法未考虑初始姿态的可靠性，导致局部最优解问题。  \n2. 传统ICP方法缺乏自适应机制，难以处理动态复杂环境的变化。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于可靠初始姿态的自适应ICP LiDAR里程计方法。首先，通过基于密度过滤的分布式粗注册获得初始姿态估计。然后，通过与运动预测姿态的比较选择最可靠的初始姿态，减少源点云与目标点云之间的初始误差。接着，结合当前和历史误差动态调整自适应阈值，以适应实时变化的动态环境。最后，基于可靠初始姿态和自适应阈值，执行点到平面的自适应ICP注册，实现源点云与目标点云的高精度对齐。实验结果表明，该方法在KITTI数据集上显著提高了LiDAR里程计的准确性，优于现有方法。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "One-DoF Robotic Design of Overconstrained Limbs with Energy-Efficient, Self-Collision-Free Motion",
            "authors": "Yuping Gu,Bangchao Huang,Haoran Sun,Ronghan Xu,Jiayi Yin,Wei Zhang,Fang Wan,Jia Pan,Chaoyang Song",
            "subjects": "Robotics (cs.RO)",
            "comment": ". Accepted by Fundamental Research. For Supplementary Videos, seethis https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.22002",
            "code": "https://bionicdl.ancorasir.com/?p=1668",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22002",
            "arxiv_html_link": "https://arxiv.org/html/2509.22002v1",
            "abstract": "While it is expected to build robotic limbs with multiple degrees of freedom (DoF) inspired by nature, a single DoF design remains fundamental, providing benefits that include, but are not limited to, simplicity, robustness, cost-effectiveness, and efficiency. Mechanisms, especially those with multiple links and revolute joints connected in closed loops, play an enabling factor in introducing motion diversity for 1-DoF systems, which are usually constrained by self-collision during a full-cycle range of motion. This study presents a novel computational approach to designing one-degree-of-freedom (1-DoF) overconstrained robotic limbs for a desired spatial trajectory, while achieving energy-efficient, self-collision-free motion in full-cycle rotations. Firstly, we present the geometric optimization problem of linkage-based robotic limbs in a generalized formulation for self-collision-free design. Next, we formulate the spatial trajectory generation problem with the overconstrained linkages by optimizing the similarity and dynamic-related metrics. We further optimize the geometric shape of the overconstrained linkage to ensure smooth and collision-free motion driven by a single actuator. We validated our proposed method through various experiments, including personalized automata and bio-inspired hexapod robots. The resulting hexapod robot, featuring overconstrained robotic limbs, demonstrated outstanding energy efficiency during forward walking.",
            "introduction": "Designing robotic limbs for a specific application is a challenging optimization problem that involves mechanical configuration, workspace, energy efficiency, payload capacity, and numerous other factors [1]. The open-chain limbs are a common choice by serially connecting rotation motors with specific configurations for agile motion and avoiding self-collision, such as the robotic manipulators [2, 3, 4], quadruped robots [5, 6], and humanoid robots [7, 8, 9]. However, the resulting drawbacks include the redundant inertia of moving components and relatively lower payload capacity. In contrast, the closed-chain limbs usually arrange the motors near the body and leverage the extra links and joints in closure form to achieve lower inertia for moving parts and improved end-effector stiffness, which becomes a preferred choice of legged robots [10, 11, 12, 13, 14, 15]. The increased number of links and joints makes the design of closed-chain robotic limbs more complex when considering the parametric choices for better performance and collision avoidance for larger workspaces concurrently. The scissor-like joint is widely used in robotic limb design [16] due to its simplicity and reliability, which ensures an extensive motion range for the joints. At the same time, the collision between links would still significantly reduce the workspace, leaving a research question regarding the simultaneous optimization of the parameters and geometries of closed-chain robotic limbs for specified tasks. The computational design method provides an alternative solution to task-specified robotic limb optimization [17, 18]. Ha et al. [19] present a computational approach to designing the robotic device by combining modular components for high-level motion specifications. On the other hand, it remains challenging to formulate an optimization problem for generating desired trajectories with collision-free closed-chain configurations and reduced actuation.\n\nAlthough robotic arms can achieve these spatial trajectories, they require integrating multiple actuators using complicated control software [20]. Instead, the one-DoF robotic design can perform receptive motion approximately driven by a single actuator. This single-actuator-driven characteristic has several advantages, including ease of control, robustness, low cost, and lightweight design, making it widely used in various engineering applications, such as modern machines [21], automata [22, 23], and robotics [24]. The RHex robot series provides an instructive insight into robotic limb design by rotating each limb in the sagittal plane and mimicking the behavior of cockroaches [25, 26, 27]. The simplicity of the limb design has resulted in a robust robot platform for learning legged locomotion [28]. With the development of this series of robots, the limb design converged on a planar four-bar structure with full-cycle motion for dynamic locomotion [29].\n\nOn the other hand, the planar four-bar mechanism is also widely adopted in other robotic designs [16]. In contrast, the generalized four-bar linkage (Bennett linkage) still has limited engineering applications as a robotic limb. The early study by Carvalho and Silvestre [30] utilized the Bennett linkage for designing a hexapod robot’s limb, with each limb comprising an active revolute joint and two additional actuators for overcoming obstacles. However, the proposed design is redundant and only demonstrates the limited advantages of the Bennett limb in a simulation environment. Gu et al. [31] investigate the design of the Bennett linkage as a robotic limb with coaxial actuation, enabling omni-directional locomotion. This demonstrates the potential advantage of an overconstrained linkage, which results in a systematic reduction in actuation.\n\nFurthermore, the Bennett robotic limb has also been validated to be energy-efficient for specific tasks by comparing it with other configurations [32, 33]. A research gap remains in leveraging overconstrained linkages for energy-efficient robotic limb design with reduced actuation due to their relatively complicated geometric condition and kinematic constraints [34]. This study aims to develop a computational design model that optimizes the parameters and geometries of overconstrained robotic limbs for efficiently realizing a target trajectory with a single actuator.\n\nLinkage synthesis is a classical kinematic design task that involves constructing a linkage mechanism to transfer an input motion (typically a rotation input from a motor) to an output motion that satisfies a set of specified characteristics [35]. Two typical linkage synthesis problems include rigid body guidance and path generation [36]. The goal of rigid body guidance is to lead a rigid body past a series of given positions and orientations [37]. Several recent works have addressed this problem in designing mechanical automata, including planar mechanical characters [23], multi-pose mechanical objects [38], and drawing machines [39]. Instead of focusing on rigid body motion, path generation aims to generate a mechanical linkage that enables the end-effector point to move along a desired trajectory. Due to its simplicity and ease of fabrication, the 1-DoF planar linkage is the most widely used mechanism for generating 2D paths. Some researchers have developed computational tools to address the 2D path generation problem, such as mechanical character design [23, 40] and interactive editing methods [41]. Additionally, they have explored walking machines [42]. Cheng et al. [43] leverage the 3D cam-follower mechanism to generate a 2D path on a planar surface and a cylindrical or spherical surface.\n\nFurthermore, much effort has been made to generate a 3D path [44, 45, 46]. One of the typical solutions is to use 1-DoF spatial linkages, whose links can move in 3D space and be driven by a single actuator, such as the RCCC mechanism [47] and the RRSS mechanism [48]. Similar to their 2D counterparts, these designs can approximate a desired 3D path but still have limited engineering application cases. While little literature addresses the path generation problem with the overconstrained linkage family, it has been demonstrated for its engineering potential in robotic applications [31] and energy efficiency in robotic limb design [32]. Compared with 1-DoF spatial linkages, Cheng et al. [49] combine the 3D cam-follower with a spatial linkage mechanism for exact path generation. However, the proposed mechanism has limited application in the robotics field due to its higher number of pairs and lack of stiffness. In contrast, our overconstrained mechanism consists of only revolute joints, which are convenient to maintain and assemble and capable of carrying a relatively high payload. As its planar counterpart (planar four-bar linkage) has been widely used in modern machinery [36], the overconstrained linkage should have sufficient potential for advanced robotic design [31].\n\nOne of the most widely studied geometric generation problems is the topology optimization problem, aiming to optimize the geometric shape of static objects under dynamic constraints [50]. There is relatively little literature on optimizing the geometric shapes of dynamic objects under kinematic constraints, such as designing self-collision-free structures. In the graphics community, recent work has focused on the design of transformable objects [51], which are typically realized via 3D printing. For example, Yuan et al. [52] propose a computational approach to generate transformable robots that shape-shift into different forms. However, the design of these objects extends beyond linkages and typically only concerns the initial and final states [53]. The geometric design of the 2D linkage can be resolved by straightforwardly offsetting each component along the directional normal to its motion plane [23].\n\nRegarding spatial linkages, it becomes a challenge to design collision-free geometries due to the increased dimensions [54]. Li et al. [55] leverage the boolean operation to trim the link design for generation collision-free linkage, as well as imply the conjecture of the existence of collision-free spatial linkages over the complete motion circle. However, the resulting link design is defined by the swept volume and can not be directly assembled for a practical linkage mechanism. Zhang et al. [56] transfer the link bar into the deformable chain to obtain the collision-free deployable 3D structure. At the same time, their linkages do not aim at invertible motion and are not directly applicable to robotic structure design. In addition, their method employs a gradient-free approach that requires a relatively longer computing time than our formulation. Therefore, unlike the existing literature, our work aims to generate self-collision-free robotic limb design by a gradient-based formulation for engineering purposes.\n\nThis study presents a computational design method for energy-efficiently realizing user-specified spatial motion via the self-collision-free overconstrained linkage driven by a single actuator. We proposed a generalized method for self-collision-avoidance design by formulating the geometric optimization problem with continuous constraints, which is solvable by non-linear programming problem solvers. Then, we formulated the motion design problem with 1-DoF overconstrained linkages and optimized the design parameters to obtain energy-efficient target trajectories. Finally, we conducted several experiments to validate the proposed method and designs, including kinematic and dynamic evaluations, mechanical character demonstrations, and a hexapod prototype robot. We found that the overconstrained robotic limb design shows the potential to generate energy-efficient motion in specific tasks. The contributions of this study are listed as follows:\n\nProposed a generalized gradient-based design approach for linkage-based robotic limbs with self-collision-free motions.\n\nDeveloped a computational design framework by optimizing the similarity and energy-related metrics for 1-DoF overconstrained robotic limbs with desired trajectories.\n\nValidated the kinematic and dynamic performance of the resulting designs and demonstrated the superior energy efficiency of overconstrained limbs with a hexapod walker.\n\nThe remainder of this study, Section 2, presents the collision-free design method and optimization problem formulation for overconstrained motion generation. Validation with hardware experiments and further discussion are enclosed in Sections 3 and 4. Section 5 presents the conclusion, limitations, and future work, which conclude this work.\n\n1. Proposed a generalized gradient-based design approach for linkage-based robotic limbs with self-collision-free motions.\n\n2. Developed a computational design framework by optimizing the similarity and energy-related metrics for 1-DoF overconstrained robotic limbs with desired trajectories.\n\n3. Validated the kinematic and dynamic performance of the resulting designs and demonstrated the superior energy efficiency of overconstrained limbs with a hexapod walker.",
            "llm_summary": "【论文的motivation是什么】  \n1. 设计具有多个自由度的机器人肢体面临复杂性和自碰撞问题。  \n2. 单自由度设计在控制、成本和效率方面具有优势，但缺乏运动多样性。  \n3. 现有的闭链机器人肢体设计在优化参数和几何形状时存在挑战。  \n\n【提出了什么创新的方法】  \n本研究提出了一种新的计算设计方法，旨在通过优化几何形状和参数，实现单自由度的过约束机器人肢体的自碰撞自由运动。首先，研究者提出了一个几何优化问题的广义公式，以确保设计的自碰撞自由。接着，针对过约束连杆的空间轨迹生成问题进行了优化，确保运动的平滑性和能效。通过一系列实验验证了该方法，结果显示，采用过约束肢体的六足机器人在前进行走时展现出卓越的能效。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Developing Vision-Language-Action Model from Egocentric Videos",
            "authors": "Tomoya Yoshida,Shuhei Kurita,Taichi Nishimura,Shinsuke Mori",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21986",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21986",
            "arxiv_html_link": "https://arxiv.org/html/2509.21986v1",
            "abstract": "Egocentric videos capture how humans manipulate objects and tools, providing diverse motion cues for learning object manipulation. Unlike the costly, expert-driven manual teleoperation commonly used in training Vision-Language-Action models (VLAs), egocentric videos offer a scalable alternative. However, prior studies that leverage such videos for training robot policies typically rely on auxiliary annotations, such as detailed hand-pose recordings. Consequently, it remains unclear whether VLAs can be trained directly from raw egocentric videos.\nIn this work, we address this challenge by leveraging EgoScaler, a framework that extracts 6DoF object manipulation trajectories from egocentric videos without requiring auxiliary recordings. We apply EgoScaler to four large-scale egocentric video datasets and automatically refine noisy or incomplete trajectories, thereby constructing a new large-scale dataset for VLA pre-training.\nOur experiments with a state-of-the-art π0\\pi_{0} architecture in both simulated and real-robot environments yield three key findings: (i) pre-training on our dataset improves task success rates by over 20% compared to training from scratch, (ii) the performance is competitive with that achieved using real-robot datasets, and (iii) combining our dataset with real-robot data yields further improvements.\nThese results demonstrate that egocentric videos constitute a promising and scalable resource for advancing VLA research.",
            "introduction": "Vision-Language-Action models (VLAs) aim to learn general-purpose robot behaviors that follow natural language instructions across environments [3, 4, 5, 6, 7, 8, 9, 10, 11].\nSuch models are pre-trained with large-scale, multi-embodiment datasets [5, 8, 11] and then fine-tuned on embodiment-specific datasets.\nHowever, most pre-training datasets for VLAs heavily rely on human teleoperation, where a number of experts directly manipulate robots to collect instances for imitation learning.\nThis is inherently costly and labor-intensive, leaving a data scarcity problem.\n\nOne promising direction to address this problem is to leverage first-person perspective recordings of humans performing everyday tasks, enabled by the advancement of AR/VR devices and smart glasses [12, 13, 14].\nParticularly, such egocentric videos provide diverse human-object interactions at a close range and inherently provide motion cues for learning object manipulation.\nSeveral studies have begun to explore how to utilize egocentric videos in robot learning [15, 16, 17, 18].\nFor example, EgoMimic [16] and EgoVLA [17] leverage enriched egocentric recordings including hand poses to learn robot policies.\nThese studies demonstrate that utilizing egocentric videos is more time- and scale-efficient than those from teleoperation-based data collection.\nUnfortunately, these approaches depend on dense auxiliary recordings, such as hand poses and action start/end timestamps.\nObtaining these dense auxiliary recordings requires specialized hardware, such as multi-camera systems or depth sensors, as well as extensive manual annotation.\nIn a recent study, LAPA [19] attempted to learn latent action representations from egocentric videos.\nWhile this approach is scalable because it does not require auxiliary labels, such latent representations often struggle to capture fine-grained motions. For example, they showed strong performance on simple actions like pushing but only mediocre performance on more complex skills like pick-and-place.\n\nConsidering the limited scalability of rich egocentric recordings and the lack of fine-grained motion cues in egocentric videos, existing methods provide valuable insights but may fall short of offering sufficiently detailed and diverse action examples for robotic foundation models (see Fig. 1).\nIt is also notable that robot policies trained on diverse real-world egocentric recordings can fall short when evaluated within controlled environments, particularly simulators, due to simplified visual systems [20, 21].\nTherefore, although egocentric recordings are undeniably valuable resources of human motion cues, they remain underexplored in the existing literature.\n\nTo address this issue, we focus on extracting explicit action trajectories, which provide supervision that represents how to move and rotate objects.\nWe leverage EgoScaler [1], a framework designed to extract object manipulation trajectories from egocentric videos.\nEach pose in a trajectory represents the centroid and rotation of the manipulated object, approximated as the end-effector states of a robot, excluding the gripper.\nWe apply this framework to four large egocentric video datasets, including Ego4D [22], Ego-Exo4D [23], HD-EPIC [24], and Nymeria [25].\nThe extracted trajectories are then curated by automatically removing noisy or incomplete instances. After this careful filtering process, we construct a new large-scale dataset for VLA pre-training.\n\nWe conduct our experiments based on a state-of-the-art π0\\pi_{0} [8] architecture.\nFor comparison, we include three real-robot datasets—BC-Z [26], BridgeData V2 [27], and Fractal [3], which match our dataset in scale and diversity.\nWe evaluate performance in both simulated (SIMPLER [20]) and real-robot (ALOHA [28]) environments.\nOur key findings are threefold:\n\nWe successfully train π0\\pi_{0} from egocentric videos without auxiliary labels, achieving significant improvements over both training from scratch and LAPA.\n\nOur dataset achieves performance on par with leading real-robot datasets, while slightly outperforming BC-Z and BridgeData V2.\n\nCombining our dataset with BridgeData V2 yields further gains, surpassing the performance of pre-training on either dataset alone.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的VLA模型训练依赖于昂贵的人工遥控数据，导致数据稀缺问题。  \n2. 现有方法利用第一人称视角视频进行机器人学习，但依赖于密集的辅助标注，限制了其可扩展性。  \n3. 现有的从第一人称视频学习的模型在捕捉细粒度动作方面表现不足，难以支持复杂技能的学习。  \n\n【提出了什么创新的方法】  \n本研究提出了EgoScaler框架，能够从第一人称视频中提取6DoF对象操作轨迹，而无需辅助标注。该方法应用于四个大型第一人称视频数据集，自动过滤噪声或不完整的轨迹，构建了一个新的大规模数据集用于VLA的预训练。实验表明，使用该数据集进行预训练的模型在任务成功率上提高了20%以上，且在模拟和真实机器人环境中表现出色，证明了第一人称视频在VLA研究中的潜力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning",
            "authors": "Sigmund Hennum Høeg,Aksel Vaaler,Chaoqi Liu,Olav Egeland,Yilun Du",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": ". This work has been submitted to the IEEE for possible publication. Seethis https URLfor the project website",
            "pdf_link": "https://arxiv.org/pdf/2509.21983",
            "code": "https://sigmundhh.com/hybrid_diffusion/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21983",
            "arxiv_html_link": "https://arxiv.org/html/2509.21983v1",
            "abstract": "Constructing robots to accomplish long-horizon tasks is a long-standing challenge within artificial intelligence. Approaches using generative methods, particularly Diffusion Models, have gained attention due to their ability to model continuous robotic trajectories for planning and control. However, we show that these models struggle with long-horizon tasks that involve complex decision-making and, in general, are prone to confusing different modes of behavior, leading to failure. To remedy this, we propose to augment continuous trajectory generation by simultaneously generating a high-level symbolic plan. We show that this requires a novel mix of discrete variable diffusion and continuous diffusion, which dramatically outperforms the baselines. In addition, we illustrate how this hybrid diffusion process enables flexible trajectory synthesis, allowing us to condition synthesized actions on partial and complete symbolic conditions. Project website: sigmundhh.com/hybrid_diffusion.",
            "introduction": "In the quest for general-purpose robotics, learning from demonstrations has proven a widely applicable paradigm. The primary task of imitation learning is to absorb a large number of demonstrations involving diverse behaviors. A performant and widely used technique for this task is to apply diffusion models [2, 3] for modeling robotic behavior. In addition to handling multimodal behavior [4], they are stable to train, and allow for flexible guidance through conditioning and composition [1, 5, 6, 7]. They are, as a result, ubiquitous in a number of robotic systems, such as open-loop trajectory modelling [1, 8, 6], closed-loop action inference using image-conditioning [4, 9, 10, 11], or as modules in composite systems [12, 13, 14].\n\nHowever, diffusion models often struggle to form long-horizon, non-smooth plans, which restricts them to only modeling relatively short and dense trajectories in Cartesian space [5]. This limits their ability to do long-horizon decision-making tasks. A motivating example is shown on the left of Figure 1, where a trajectory-level diffusion model is tasked with sorting three blocks. Despite the demonstrations always terminating in a sorted state, sampled trajectories from the planner fail to sort the blocks. This is exacerbated when task complexity is increased, as the diffusion model struggles to account for interdependencies over long time horizons. Indeed, our experiments (Sec. IV) demonstrate that when tasked with sorting an increasing number of blocks, pure diffusion models quickly fail.\n\nA popular paradigm for allowing robots to perform long-horizon decision-making tasks is Task-and-Motion Planning (TAMP). TAMP methods typically exploit the connection between symbolic and continuous motion plans to simplify and reduce the overall size of the search space [15]. For example, symbolic planners can construct symbolic abstracted plans that transfer the system to the goal state, while continuous motion planning can find motion plans that correspond to this symbolic plan [16]. The inclusion of planning in symbolic space not only increases the planner’s efficiency and performance, but it also allows for transparency, unlike that of pure continuous planners. Upon generation, the symbolic plan provides a clear indication of the high-level steps involved, offering clarity. Additionally, their connection between continuous and symbolic plans allows for direct control of the robot by modifying parts of the symbolic plan and having the continuous plan respect these restrictions. For example, when a robot is tasked with moving three boxes from one location to another, we may want to specify at test time that a particular box should be moved first. It would be of interest to combine these techniques with planning using diffusion models to make them more transparent by providing a symbolic description of the robot plan and allowing for guidance and conditioning at a symbolic level.\n\nAs a response, we present Hybrid Diffusion Planner (HDP), a performant method for simultaneously generating both continuous and symbolic plans, as shown on the right of Fig. 1. Its connection to symbolic plans enables unprecedented transparency and guidance functionality. Surprisingly, incorporating modeling of symbolic information with HDP improves the long-horizon planning performance drastically compared to motion-only diffusion. Through our experiments, we show that our formulation of a joint objective consisting of masked diffusion [17, 18] and continuous diffusion [3] is crucial to the success of the method. In addition to the performance gain, HDP enables flexible conditional sampling at inference. By fixing a partial or complete symbolic plan through inpainting, HDP can generate a continuous plan that satisfies the specified constraints. Such flexible conditioning allows HDP to be easily controlled and used for diverse tasks outside of explicit plan generation.\n\nTo further highlight the challenges of long-horizon planning and show the benefits of HDP, we present a novel task suite of simulated and real robotic tasks focused on long-horizon complex planning. Previous IL benchmarks either focus on single-task performance [4, 19] or, when considering long-horizon operations, the subtasks are specified to the policy by an external oracle [20, 21]. In contrast, we focus on long-horizon robotic manipulation tasks where the agent is tasked with determining the sequence of subtasks itself. We find that traditional diffusion-based planning from demonstrations performs poorly in this scenario.\n\nOverall, our contributions are threefold: (1) We introduce a novel Imitation Learning task suite, exhibiting complex and long-horizon planning for robotic manipulation, and show that widely used diffusion-model planning struggles in the face of long-horizon, multimodal demonstrations. (2) We introduce Hybrid Diffusion Planner, a novel diffusion-based planner that uses a coupled discrete and continuous diffusion process for generating both symbolic and continuous motion plans. (3) Lastly, we empirically demonstrate HDP’s flexible conditioning capabilities.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的扩散模型在处理长时间跨度的复杂决策任务时表现不佳。  \n2. 现有方法缺乏将符号规划与连续轨迹生成相结合的能力。  \n3. 需要提高机器人在长时间跨度任务中的透明性和指导功能。  \n\n【提出了什么创新的方法】  \n提出了一种混合扩散规划器（HDP），该方法通过同时生成符号计划和连续轨迹来解决长时间跨度的决策问题。HDP结合了离散变量扩散和连续扩散，显著提高了规划性能。通过在推理过程中进行灵活的条件采样，HDP能够生成满足特定约束的连续计划。这种方法不仅提升了长时间跨度的规划能力，还增强了对复杂任务的控制和适应性。实验结果表明，HDP在复杂的长时间跨度操作中表现优异，超越了传统的扩散模型。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "FlowDrive: moderated flow matching with data balancing for trajectory planning",
            "authors": "Lingguang Wang,Ömer Şahin Taş,Marlon Steiner,Christoph Stiller",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21961",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21961",
            "arxiv_html_link": "https://arxiv.org/html/2509.21961v1",
            "abstract": "Learning-based planners are sensitive to the long-tailed distribution of driving data. Common maneuvers\ndominate datasets, while dangerous or rare scenarios are sparse. This imbalance can bias\nmodels toward the frequent cases and degrade performance on critical scenarios. To tackle this problem,\nwe compare balancing strategies for sampling training data and find reweighting by\ntrajectory pattern an effective approach. We then present FlowDrive, a flow-matching trajectory planner that\nlearns a conditional rectified flow to map noise directly to trajectory distributions with few flow-matching steps.\nWe further introduce moderated, in-the-loop guidance that injects small perturbation between flow steps to\nsystematically increase trajectory diversity while remaining scene-consistent. On nuPlan and the interaction-focused\ninterPlan benchmarks, FlowDrive achieves state-of-the-art results among learning-based planners and approaches methods\nwith rule-based refinements. After adding moderated guidance and light\npost-processing (FlowDrive*), it achieves overall state-of-the-art performance across nearly all benchmark splits.",
            "introduction": "Trajectory planning in autonomous driving requires both safety and efficiency.\nTraditional planners rely on rule-based methods like model predictive control, graph or sampling-based methods, which are interpretable\nand safety-driven but often fail in complex, real-world conditions (Schwarting et al., 2018).\nRecent learning-based planners learn policies from data, capturing nuanced human driving behaviors\n(Wang et al., 2023; Cheng et al., 2024; 2023), and can rival classical systems on large-scale benchmarks.\n\nDespite progress, challenges remain. Real driving data exhibits long-tailed distributions, where common behaviors\nlike lane-following dominate while rare but safety-critical cases are underrepresented (Karnchanachari et al., 2024).\nThis imbalance biases planners toward frequent patterns, reducing reliability in corner cases. Moreover, dataset bias and\nlimited diversity lead to poor generalization—especially in dynamic traffic scenarios unseen\nduring training (Codevilla et al., 2019).\n\nGenerative models provide a promising solution. Diffusion models (Song & Ermon, 2019; Ho et al., 2020) generate trajectories\nvia iterative denoising and can model multi-modal behaviors, but often require many steps or careful guidance for feasibility.\nFlow matching (Lipman et al., 2023; Liu et al., 2023) offers an alternative generative paradigm.\nInstead of iterative denoising, it trains a continuous transformation mapping a simple prior directly to the data\ndistribution (Lipman et al., 2023). This enables fast, few-step sampling, while preserving the ability to model\nmulti-modal behaviors.\n\nMotivated by the need for diverse yet fast trajectory generation, we propose FlowDrive, a flow-matching\nplanner for autonomous driving. Unlike diffusion planners that produce trajectories via many denoising steps, FlowDrive\nlearns a continuous motion flow that directly transforms random initial noise into diverse driving trajectories,\nyielding faster sampling. We also observe that naively training such a planner on a standard driving dataset can\nlead to biased behavior, and the model may overfit to the most common scenarios and neglect underrepresented but critical\ncases. To address this, we analyze how data imbalance in the training set affects planning performance and introduce\na data balancing method that increases the coverage of rare behaviors. Furthermore, we introduce a mechanism to steer\nFlowDrive’s output trajectories in order to systematically diversify the generated candidates. Finally,\nwe evaluate FlowDrive on the nuPlan benchmark (Karnchanachari et al., 2024) and interPlan benchmark\n(Hallgarten et al., 2024). An anonymous code repository was provided to reviewers during peer review; the\nfinal code will be made public upon publication.\n\nIn summary, the contributions of this paper are:\n\nWe identify the impact of unbalanced training data on planning performance and propose a data-balancing\nstrategy to improve robustness to rare scenarios. Furthermore, we present FlowDrive, a flow-matching\ntrajectory planner that efficiently generates feasible driving trajectories for autonomous vehicles.\n\nWe introduce a guidance mechanism that steers FlowDrive’s outputs to produce more diverse trajectory samples.\nThis enables state-of-the-art performance on the nuPlan and interPlan benchmarks, outperforming previous rule-based,\nlearning-based and hybrid baselines.\n\n1. We identify the impact of unbalanced training data on planning performance and propose a data-balancing\nstrategy to improve robustness to rare scenarios. Furthermore, we present FlowDrive, a flow-matching\ntrajectory planner that efficiently generates feasible driving trajectories for autonomous vehicles.\n\n2. We introduce a guidance mechanism that steers FlowDrive’s outputs to produce more diverse trajectory samples.\nThis enables state-of-the-art performance on the nuPlan and interPlan benchmarks, outperforming previous rule-based,\nlearning-based and hybrid baselines.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统规划方法在复杂的真实世界条件下表现不佳。  \n2. 驾驶数据的长尾分布导致常见行为主导，稀有但安全关键的场景被低估。  \n3. 数据集偏差和有限的多样性导致在动态交通场景中的泛化能力差。  \n\n【提出了什么创新的方法】  \n本论文提出FlowDrive，一种流匹配轨迹规划器，通过学习条件修正流直接将噪声映射到轨迹分布，减少流匹配步骤。为了解决数据不平衡问题，提出了一种数据平衡策略，增加稀有行为的覆盖率。此外，引入了引导机制，在流步骤之间注入小扰动，系统性地增加轨迹多样性，同时保持场景一致性。FlowDrive在nuPlan和interPlan基准测试中实现了学习型规划器的最先进结果，并在几乎所有基准分割中达到了整体的最先进性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Learnable Conformal Prediction with Context-Aware Nonconformity Functions for Robotic Planning and Perception",
            "authors": "Divake Kumar,Sina Tayebati,Francesco Migliarba,Ranganath Krishnan,Amit Ranjan Trivedi",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG); Statistics Theory (math.ST)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21955",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21955",
            "arxiv_html_link": "https://arxiv.org/html/2509.21955v1",
            "abstract": "Deep learning models in robotics often output point estimates with poorly calibrated confidences, offering no native mechanism to quantify predictive reliability under novel, noisy, or out-of-distribution inputs. Conformal prediction (CP) addresses this gap by providing distribution-free coverage guarantees, yet its reliance on fixed nonconformity scores ignores context and can yield intervals that are overly conservative or unsafe. We address this with Learnable Conformal Prediction (LCP), which replaces fixed scores with a lightweight neural function sθ​(x)=fθ​(ϕ​(x))s_{\\theta}(x)=f_{\\theta}(\\phi(x)) that leverages geometric, semantic, and model cues. Trained to balance coverage, efficiency, and calibration, LCP preserves CP’s finite-sample guarantees while producing intervals that adapt to instance difficulty, achieving context-aware uncertainty without ensembles or repeated inference. On the MRPB benchmark, LCP raises navigation success to 91.5% versus 87.8% for Standard CP, while limiting path inflation to 4.5% compared to 12.2%. For object detection on COCO, BDD100K, and Cityscapes, it reduces mean interval width by 46–54% at 90% coverage, and on classification tasks (CIFAR-100, HAM10000, ImageNet) it shrinks prediction sets by 4.7–9.9%. The method is also computationally efficient, achieving real-time performance on resource-constrained edge hardware (Intel NUC with footprint 4.6×4.44.6\\times 4.4 inch2 and power <30<30 W) while simultaneously providing uncertainty estimates along with the mean prediction.",
            "introduction": "Learning from data is an inherently ill-conditioned problem that often admits multiple optimal solutions. Selecting a single solution while discarding others is theoretically unjustified and thus limits predictive robustness. Consequently, most learning models that output point predictions or poorly calibrated confidences [1] incur prediction errors that depend heavily on context such as occlusion, clutter, or distribution shift [2]. Moreover, these models are typically optimized for average-case accuracy rather than worst-case reliability in deployment [3]. As a result, while they may deliver strong mean performance, yet can fail catastrophically in rare yet safety-critical corner cases, resulting in a crucial limitation for their deployment for mission/safety-critical robotics.\n\nTwo primary sources of uncertainty exist in learning models: epistemic uncertainty, arising from limited data or model capacity and reducible with additional information, and aleatoric uncertainty, caused by inherent sensor noise or environmental ambiguity which is irreducible even with infinite data [4, 5]. Recent advances have shown promise in autonomous navigation [6, 7] and human-robot collaboration [8]. Recent work has explored separating these uncertainties through conformal inference and evidential learning [9], enabling more nuanced risk assessment. A range of uncertainty quantification (UQ) methods aim to capture these effects. Bayesian neural networks and variational inference estimate epistemic uncertainty but require multiple stochastic passes, making them impractical for time-constrained control [10]. Deep ensembles provide stronger calibration but are computationally expensive [11]. Approximate approaches such as Monte Carlo dropout reduce overhead but often yield poorly calibrated estimates [12], while post-hoc calibration adjusts confidence scores without statistical guarantees [1].\n\nConformal prediction (CP) has recently attracted significant interest as a principled framework for uncertainty quantification [13, 14, 15, 16, 17], with recent extensions to multi-sensor fusion [18] and adaptive abstention policies [19]. Originating in statistical learning theory, CP constructs prediction sets calibrated on held-out data and provides distribution-free, finite-sample coverage guarantees under the assumption of exchangeability. Unlike many heuristic post-hoc calibration methods, CP offers explicit statistical guarantees, ensuring that true outcomes fall within the predicted sets at a user-specified confidence level. Moreover, CP is suited even for legacy prediction models that do not necessarily rely on learning from data. These properties make CP particularly appealing for robotics, where models must operate under distribution shift and safety requires formal reliability bounds on decision-making.\n\nDespite its generality, CP is most often implemented with fixed nonconformity functions that fail to capture the intense interaction of input data, application domain, and context in shaping uncertainty. For instance, in regression, standard CP with residual-based nonconformity produces intervals of constant width across all inputs, ignoring heteroscedasticity [20, 21]. As a result, CP provides valid coverage but does not account for how uncertainty emerges from the joint structure of observations and operating conditions. This limitation is especially critical in robotics, where risk depends not only on the raw input but also on situational context: for example, a partially occluded object may be harmless clutter in a warehouse aisle yet represent a pedestrian entering a crosswalk in an urban scene. Treating both as equally uncertain either wastes efficiency in benign settings or under-protects in safety-critical ones.\n\nWe address this limitation with Learnable Conformal Prediction (LCP) (Fig. 1). Instead of fixed scores, we introduce a feature-driven function sθ​(x)=fθ​(ϕ​(x))s_{\\theta}(x)=f_{\\theta}(\\phi(x)) that adapts to the structure of prediction errors. Features ϕ​(x)\\phi(x) encode geometric, semantic, and model-derived cues, while fθf_{\\theta} is a lightweight neural network trained to balance coverage, efficiency, and calibration. Calibration over these learned scores preserves the finite-sample coverage guarantees of CP [13, 21], while producing intervals that shrink in simple cases and expand in difficult ones.\n\nWe evaluate LCP for (i) robotic path planning under noisy and incomplete sensing on the MRPB benchmark, (ii) object detection with uncertainty calibration on COCO, BDD100K, and Cityscapes, and (iii) image classification on CIFAR-100, HAM10000, and ImageNet. Across these benchmarks, LCP consistently improves the safety–efficiency trade-off across planning, perception, and classification tasks. On the MRPB path-planning benchmark, LCP raises success rates to 91.5% while limiting path inflation to 4.5%, compared to 87.8% success and 12.2% inflation with standard CP. For object detection on COCO, BDD100K, and Cityscapes, LCP reduces mean interval width by 46–54% while sustaining ≈\\approx90% coverage. In classification (CIFAR-100, HAM10000, ImageNet), it cuts prediction set sizes by 4.7–9.9% relative to fixed baselines without losing validity. The proposed framework is also computationally efficient, achieving real-time performance on resource-constrained edge hardware (Intel NUC, area 4.6×4.44.6\\times 4.4 inch2, power <30<30 W) while simultaneously extracting uncertainty estimates and prediction. This aligns with recent advances in edge robotics [22] and intelligent sensing-to-action systems [23].",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有深度学习模型在机器人领域输出的点估计缺乏可靠的置信度量化。  \n2. 固定的非顺应性评分无法捕捉上下文信息，导致置信区间过于保守或不安全。  \n3. 需要在不确定性量化中平衡覆盖、效率和校准，以提高机器人决策的可靠性。  \n\n【提出了什么创新的方法】  \n提出了可学习的顺应性预测（LCP），通过引入特征驱动的神经网络函数来替代固定的非顺应性评分。该方法利用几何、语义和模型线索，经过训练以平衡覆盖、效率和校准，保持了顺应性预测的有限样本保证。LCP在多个基准测试中表现出色，如在MRPB基准上，将导航成功率提高到91.5%，并将路径膨胀限制在4.5%。在COCO、BDD100K和Cityscapes的物体检测中，LCP将均值区间宽度减少了46-54%，在CIFAR-100、HAM10000和ImageNet的分类任务中，预测集大小减少了4.7-9.9%。该方法在资源受限的边缘硬件上实现了实时性能，同时提供不确定性估计和均值预测。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks",
            "authors": "Jialiang Li,Wenzheng Wu,Gaojing Zhang,Yifan Han,Wenzhao Lian",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21928",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21928",
            "arxiv_html_link": "https://arxiv.org/html/2509.21928v1",
            "abstract": "Successfully solving long-horizon manipulation tasks remains a fundamental challenge. These tasks involve extended action sequences and complex object interactions, presenting a critical gap between high-level symbolic planning and low-level continuous control. To bridge this gap, two essential capabilities are required: robust long-horizon task planning and effective goal-conditioned manipulation. Existing task planning methods, including traditional and LLM-based approaches, often exhibit limited generalization or sparse semantic reasoning. Meanwhile, image-conditioned control methods struggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a novel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon Manipulation Tasks. SAGE utilizes semantic scene graphs as a structural representation for scene states. A structural scene graph enables bridging task-level semantic reasoning and pixel-level visuo-motor control. This also facilitates the controllable synthesis of accurate, novel sub-goal images. SAGE consists of two key components: (1) a scene graph-based task planner that uses VLMs and LLMs to parse the environment and reason about physically-grounded scene state transition sequences, and (2) a decoupled structural image editing pipeline that controllably converts each target sub-goal graph into a corresponding image through image inpainting and composition. Extensive experiments have demonstrated that SAGE achieves state-of-the-art performance on distinct long-horizon tasks.",
            "introduction": "Robots are expected to work in complex, real-world environments and perform long-horizon manipulation tasks that involve multiple cascading sub-task phases [1, 2]. Unlike short-horizon operations, these tasks require extended action sequences, with complex dependencies between objects and states. Despite the importance, robustly solving long-horizon manipulation tasks remains a significant challenge. A key difficulty lies in bridging the semantic gap between high-level symbolic planning and low-level continuous control. For example, a high-level plan for making a cup of tea might be put the teabag in the mug, then pour water. While this plan is easy for humans to understand, it is too abstract to be translated into a series of precise, continuous actions, such as correctly grasping the teabag and transfering it into the mug without spilling. Successfully bridging this gap requires two capabilities: (1) generating robust long-horizon task plans that are grounded in the physical world, and (2) providing controllers with actionable goal representations that translate abstract sub-tasks into pixel-level visuo-motor commands, enabling reliable continuous control.\n\nTo address long-horizon task planning, traditional approaches like Sense-Plan-Act (SPA) [3, 4] and Task and Motion Planning (TAMP) [5, 6] rely on manually predefined symbolic rules and known dynamic models, which limits their application in novel real-world scenarios. A more recent line of work leveraging large language models (LLMs) [7, 8, 9, 10] and vision-language models (VLMs) [11, 12, 13, 14] face new challenges including hallucination, uncontrollable generation, and extraction of only low-level sparse semantic information from raw images.\n\nTo provide strong goal guidance, efforts have been made to explore different forms of goals, encompassing language instructions [15, 16, 17] and images [18, 19]. Language is much flexible for human to specify the task goals, but it lacks the precise pixel-level information needed for effective policy learning. Meanwhile, researchers have studied image-conditioned manipulation via text-to-image diffusion-based image synthesis models [20, 21]. However, due to text embeddings, being not aligned with images, these works suffer in generating high-quality, novel images for unseen tasks.\n\nTo address the above challenges, we propose SAGE, a Scene Graph-Aware Guidance and Execution framework for long-horizon manipulation tasks. The key insight underlying SAGE is that many long-horizon manipulation tasks are uniquely defined by their specific temporal execution orderings and spatial object relationships. Based on this, as shown in Figure. 1, SAGE leverages LLMs for planning and generated images for goal guidance, while mitigating their limitations. At its core, SAGE uses semantic scene graphs [22] as a structural representation of the scene state, effectively aligning task-level semantic reasoning with pixel-level visuo-motor control. A scene graph models the physical world by representing objects as nodes and their spatial relationships (e.g., On, In) as edges. We employ this representation for two reasons. First, at the planning level, the scene graphs enable LLMs to extract structural, high-level semantic information, enabling them to generate physically-grounded task plans. Second, for image-conditioned manipulation, by decomposing a scene into a disentangled scene graph with independent objects and their pairwise relations, modifications can be applied locally and consistently, allowing controllable sub-goal image generation for even unseen tasks.\n\nSAGE consists of two key components: (1) a scene graph task planner that uses a VLM to parse the scene into a scene graph and a LLM to reason about a scene graph transition chain, which serves as a physically-grounded task plan; and (2) a decoupled structural image editing pipeline that controllably converts each scene graph of this plan into a corresponding image through image inpainting and composition. These generated images guide a visuo-motor policy to accomplish the entire long-horizon manipulation task by executing each sub-task iteratively. Experimental results have demonstrated that SAGE achieves state-of-the-art performance on various long-horizon tasks.\n\nWe summarize our contributions as follows:\n\nWe introduce SAGE, a framework that uses scene graphs to extract task keyframes, enabling the alignment between task-level semantic reasoning and pixel-level visuo-motor control for long-horizon manipulation.\n\nWe develop a scene graph task planner that robustly decomposes long-horizon tasks into interpretable, physically-grounded scene graph transition chains, facilitating structural symbolic reasoning.\n\nWe design a decoupled structural image editing pipeline that controllably synthesizes sub-goal images from predicted scene graphs, providing reliable visual guidance for visuo-motor control.\n\nWe demonstrate that SAGE achieves SOTA performance across diverse long-horizon tasks with distinct and novel temporal execution orderings and spatial object relationships.\n\n1. We introduce SAGE, a framework that uses scene graphs to extract task keyframes, enabling the alignment between task-level semantic reasoning and pixel-level visuo-motor control for long-horizon manipulation.\n\n2. We develop a scene graph task planner that robustly decomposes long-horizon tasks into interpretable, physically-grounded scene graph transition chains, facilitating structural symbolic reasoning.\n\n3. We design a decoupled structural image editing pipeline that controllably synthesizes sub-goal images from predicted scene graphs, providing reliable visual guidance for visuo-motor control.\n\n4. We demonstrate that SAGE achieves SOTA performance across diverse long-horizon tasks with distinct and novel temporal execution orderings and spatial object relationships.",
            "llm_summary": "【论文的motivation是什么】  \n1. 长期操作任务的复杂性使得高层符号规划与低层连续控制之间存在显著差距。  \n2. 现有任务规划方法在新任务的泛化能力和语义推理方面存在局限性。  \n3. 图像条件控制方法在适应未见任务时表现不佳。  \n\n【提出了什么创新的方法】  \nSAGE框架通过语义场景图作为场景状态的结构化表示，解决了长期操作任务中的规划和控制问题。该框架包含两个关键组件：场景图任务规划器和解耦的结构图像编辑管道。场景图任务规划器利用视觉-语言模型和大型语言模型解析环境并推理物理基础的场景状态转移序列，而图像编辑管道则通过图像修复和合成将目标子目标图转换为相应图像。SAGE在不同的长期任务上实现了最先进的性能，成功地将任务级语义推理与像素级的视觉运动控制对齐。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "WAVE: Worm Gear-based Adaptive Variable Elasticity for Decoupling Actuators from External Forces",
            "authors": "Moses Gladson Selvamuthu,Tomoya Takahashi,Riichiro Tadakuma,Kazutoshi Tanaka",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21878",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21878",
            "arxiv_html_link": "https://arxiv.org/html/2509.21878v1",
            "abstract": "Robotic manipulators capable of regulating both compliance and stiffness offer enhanced operational safety and versatility. Here, we introduce Worm Gear-based Adaptive Variable Elasticity (WAVE), a variable stiffness actuator (VSA) that integrates a non-backdrivable worm gear.\nBy decoupling the driving motor from external forces using this gear, WAVE enables precise force transmission to the joint, while absorbing positional discrepancies through compliance.\nWAVE is protected from excessive loads by converting impact forces into elastic energy stored in a spring.\nIn addition, the actuator achieves continuous joint stiffness modulation by changing the spring’s precompression length.\nWe demonstrate these capabilities, experimentally validate the proposed stiffness model, show that motor loads approach zero at rest–even under external loading–and present applications using a manipulator with WAVE.\nThis outcome showcases the successful decoupling of external forces.\nThe protective attributes of this actuator allow for extended operation in contact-intensive tasks, and for robust robotic applications in challenging environments.",
            "introduction": "Introducing robots into environments not specifically designed for them, such as human living spaces, requires flexibility to mitigate collision forces and prevent damage, while maintaining sufficient rigidity for precise and powerful actuation as shown in Fig. 1 (right).\nOne effective approach is incorporating inherent compliance into the robot design, which helps absorb unexpected collision forces, enhancing safety and tolerance to positional errors.\nHowever, in many scenarios, high payload capacity or accurate position control is also necessary for performing tasks at a level comparable to human capabilities.\nImplementing variable joint compliance is beneficial in integrating these features into a single hardware system.\n\nA Variable Stiffness Actuator (VSA) is a hardware module designed to control compliance, enabling robots to change their joint stiffness in a mechanical manner to perform diverse tasks [1], [2].\nCompared to software-based stiffness control, VSA has a fast response speed and can store impact force energy in the spring, protecting both the robot and the actuator from the force [3].\nBased on actuator configuration as shown in Fig. 2, VSAs can be classified into the antagonistic type [4], [5] and independent type [6].\nAntagonistic VSAs utilize two motors to control both elastic force and joint actuation through differential motion as shown in Fig. 2(a).\nHowever, because the motors are constantly subjected to spring compression forces, additional torque is required beyond what is needed for joint movement. In contrast, as shown in Fig. 2(b), independent VSAs separate the roles of the angle control motor that primarily controls joint actuation and the stiffness control motor that adjusts spring precompression.\nSince the VSA motor does not necessarily need to follow the angle control motor’s speed during joint actuation, it can be smaller, reducing the overall mass of the two motors. Nevertheless, in terms of impact absorption, external impact forces transmitted through the spring can still reach the angle control motor, potentially leading to actuator damage. This study aims to develop a VSA that ensures the independence of force transmission paths for external impact absorption and joint actuation. Worm gears or power screws are known for their non-backdrivable properties and have been utilized in applications where large forces are applied [7].\nA worm gear is a transmission component that converts rotational motion between perpendicular shafts, typically used to achieve high reduction ratios. A worm gear with small lead angle and single-start thread is non-backdrivable, where the worm wheel cannot drive the worm screw because the friction at the contact surface exceeds the torque generated by the reverse force.\n\nIn this paper, we propose a novel VSA mechanism named WAVE (Worm gear-based Adaptive Variable Elasticity) as shown in Fig. 2(c).\nThis mechanism incorporates a worm gear, which is typically fixed in the linear direction but, in our design, is constrained via a spring along that axis.\nThis allows for two distinct motion pathways: (A) rotation of the joint by the angle control motor (M1\\text{M}_{\\text{1}}), shown in red in Fig. 1(a), and (B) precompression of the linear spring by external force feedback or the stiffness control motor (M2\\text{M}_{\\text{2}}), shown in blue in Fig. 1(b).\nAs shown in Fig. 1 (left), this mechanism transmits the angle control motor’s torque to the joint via the worm gear while the spring passively absorbs displacement. This allows for adaptive trajectory generation in response to environmental interactions. Additionally, since the worm gear completely decouples the angle control motor, the impact force is immediately converted into elastic energy of the spring, effectively protecting the actuator. Furthermore, active compliance control–referring to the ability to adjust passive joint deflection in response to external forces by modifying the spring preload–enables the system to support high payloads.\n\nIn this paper, we introduce WAVE as a novel variable stiffness actuator that decouples the angle control motor from external forces through a worm gear mechanism.\nThe main contributions of our work are threefold:\n\nWe propose a compact and integrated hardware design that leverages non-backdrivability to isolate the angle control motor from external forces.\n\nWe demonstrate how active control of the worm gear’s linear axis enables continuous switching between high-stiffness for precise and powerful actuation, and low-stiffness for safe interaction and impact absorption.\n\nThrough experimental evaluation, we show that our mechanism can robustly handle a wide range of tasks, from high-load manipulation to contact-rich operations, without compromising safety or performance.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要在机器人操作中实现灵活性，以减轻碰撞力并防止损坏。  \n2. 机器人在执行任务时必须具备高负载能力和精确的位置控制。  \n3. 现有的可变刚度执行器在抗冲击和安全性方面存在不足。  \n\n【提出了什么创新的方法】  \n本文提出了一种名为WAVE的可变刚度执行器，利用蜗轮机制将角度控制电机与外部力解耦。该机制通过蜗轮传递扭矩，同时允许弹簧被动吸收位移，从而实现环境交互中的自适应轨迹生成。WAVE的设计确保了在高刚度和低刚度之间的连续切换，以适应不同的操作需求。实验结果表明，WAVE能够在高负载操作和接触丰富的任务中稳健地工作，而不影响安全性或性能。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Improved Vehicle Maneuver Prediction using Game Theoretic Priors",
            "authors": "Nishant Doshi",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21873",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21873",
            "arxiv_html_link": "https://arxiv.org/html/2509.21873v1",
            "abstract": "未获取到摘要",
            "introduction": "Conventional maneuver prediction methods use some sort of classification model on temporal trajectory data to predict behavior of agents over a set time horizon. Despite of having the best precision and recall, these models cannot predict a lane change accurately unless they incorporate information about the entire scene. Level-k game theory can leverage the human-like hierarchical reasoning to come up with the most rational decisions each agent can make in a group. This can be leveraged to model interactions between different vehicles in presence of each other and hence compute the most rational decisions each agent would make. The result of game theoretic evaluation can be used as a “prior “ or combined with a traditional motion-based classification model to achieve more accurate predictions. The proposed approach assumes that the states of the vehicles around the target lead vehicle are known. The module will output the most rational maneuver prediction of the target vehicle based on an online optimization solution. These predictions are instrumental in decision making systems like Adaptive Cruise Control (ACC) or Traxen’s iQ-Cruise further improving the resulting fuel savings.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的行为预测模型在准确预测车道变换方面存在局限性。  \n2. 需要更好地建模车辆之间的相互作用以提高预测精度。  \n\n【提出了什么创新的方法】  \n提出了一种结合了层级博弈理论的车辆机动预测方法。该方法通过利用人类般的推理能力来模拟不同车辆之间的相互作用，从而计算出每个代理的最合理决策。通过将博弈理论评估的结果作为“先验”，与传统的基于运动的分类模型相结合，显著提高了预测的准确性。该方法在已知周围车辆状态的基础上，输出目标车辆的最合理机动预测，进而优化决策系统，如自适应巡航控制（ACC），实现更好的燃油节省效果。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors",
            "authors": "Ning Huang,Zhentao Xie,Qinchuan Li",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21810",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21810",
            "arxiv_html_link": "https://arxiv.org/html/2509.21810v1",
            "abstract": "Despite growing interest in developing legged robots that emulate biological locomotion for agile navigation of complex environments, acquiring a diverse repertoire of skills remains a fundamental challenge in robotics. Existing methods can learn motion behaviors from expert data, but they often fail to acquire multiple locomotion skills through a single policy and lack smooth skill transitions. We propose a multi-skill learning framework based on Conditional Adversarial Motion Priors (CAMP), with the aim of enabling quadruped robots to efficiently acquire a diverse set of locomotion skills from expert demonstrations. Precise skill reconstruction is achieved through a novel skill discriminator and skill-conditioned reward design. The overall framework supports the active control and reuse of multiple skills, providing a practical solution for learning generalizable policies in complex environments.",
            "introduction": "In recent years, reinforcement learning (RL) has demonstrated significant potential in enabling robots to master diverse skills. In the domain of quadrupedal locomotion, RL has been successfully applied to allow quadruped robots to perform diverse and challenging tasks in real-world environments [1]–[4]. Typically, RL techniques rely on carefully designed reward functions tailored to specific tasks in order to guide the learning of desired behaviors. Yet engineering rewards for\ncomplex skills necessitates balancing competing objectives an error-prone process where desired behaviors seldom emerge.\n\nAmong various reinforcement learning approaches, imitation-based reinforcement learning offers a convenient and effective means for acquiring complex skills. By leveraging limited number of expert demonstrations, imitation learning enables agents to replicate expert-like behaviors. Escontrela et\nal.[5] have demonstrated that Adversarial Motion Priors (AMP) constitute an effective framework for this purpose. AMP employs a Generative Adversarial Network (GAN) structure, in which a discriminator distinguishes demonstration samples and agent-generated samples, thus providing\nreward signals that motivate the agent to produce motion behaviors stylistically similar to expert demonstrations.\n\nBeyond simply mimicking expert motions, AMP also supports more flexible and generalizable behavior synthesis. It enables quadruped robots to learn user-specific gait patterns by enforcing motion styles specified by reference trajectories, even in the absence of direct low-level control data. This makes AMP particularly suitable for solving well-defined tasks where stylistic fidelity is important, while alleviating the need for dense or fine-grained expert annotations. However,\nthe training process of GANs is known to be highly unstable[12],[15]. When learning multiple motion skills simultaneously, the generator may collapse to producing only a limited subset of behaviors, failing to cover the full data distribution. This lack of diversity in the generated samples undermines the representational richness required to capture the complexity of real expert demonstrations.\n\nIn this study, we propose a novel approach based on the fundamental idea of Conditional Generative Adversarial Networks (CGAN) to address the challenge of multi-skill learning in the Adversarial Motion Priors (AMP) framework. Traditional AMP frameworks usually focus on learning a single target task and cannot effectively handle complex multi-skill learning problems. To overcome this limitation, we introduce additional conditional information to make the learning process more controllable and flexible. Specifically, we use skill categories as conditional inputs to guide the generator in producing specific motion sequences under different skill contexts, thereby enabling simultaneous learning of multiple skills. This method not only allows handling multiple tasks within the same network but also ensures that the generation and learning of each task or skill can be adjusted according to the different requirements of the conditional variables, thus avoiding the task interference problems that may occur in traditional approaches.\n\nTo further improve the quality and controllability of skill-conditioned motion generation, we extend our framework by introducing an additional skill discriminator. While the conditional generator enables the policy to produce diverse motion sequences under different skill contexts, it does not explicitly ensure that the generated behaviors match the characteristics of the intended skill. To address this, the skill discriminator is designed to classify the type of skill expressed\nby the current behavior, using reference samples from the expert dataset as supervision. By incorporating this discriminator, the agent receives informative feedback during training,\nguiding the policy to better align the generated motions with the desired skill characteristics and enhancing both motor capability and control precision.\n\nThis mechanism enhances the policy’s dynamic adaptability to diverse skill requirements while improving robust training stability and cross-skill generalization performance. We validate the proposed approach through extensive experiments in both simulation and on a physical quadruped robot. The results demonstrate that our system is capable of generating diverse motion skills conditioned on user-specified commands, and achieve smooth transitions between skills during motion.",
            "llm_summary": "【论文的motivation是什么】  \n1. 多技能学习在四足机器人中面临挑战，现有方法难以通过单一策略获取多种运动技能。  \n2. 现有的模仿学习方法在技能转换的平滑性和多样性方面存在不足。  \n\n【提出了什么创新的方法】  \n提出了一种基于条件对抗运动先验（CAMP）的多技能学习框架，利用条件生成对抗网络（CGAN）来引导生成器在不同技能上下文中产生特定运动序列。通过引入技能鉴别器，增强了对生成运动的控制和准确性，从而实现了多技能的同时学习。该方法在复杂环境中有效地生成多样化的运动技能，并实现了技能之间的平滑过渡，提升了四足机器人的运动能力和控制精度。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions",
            "authors": "Hyeonseong Kim,Roy El-Helou,Seungbeen Lee,Sungjoon Choi,Matthew Pan",
            "subjects": "Robotics (cs.RO); Human-Computer Interaction (cs.HC)",
            "comment": "for more videos, seethis https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.21776",
            "code": "https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21776",
            "arxiv_html_link": "https://arxiv.org/html/2509.21776v1",
            "abstract": "Playful deception, a common feature in human social interactions, remains underexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice Cream (TIC) vendor routine, we investigate how bounded, culturally familiar forms of deception influence user trust, enjoyment, and engagement during robotic handovers. We design a robotic manipulator equipped with a custom end-effector and implement five TIC-inspired trick policies that deceptively delay the handover of an ice cream-shaped object. Through a mixed-design user study with 91 participants, we evaluate the effects of playful deception and interaction duration on user experience. Results reveal that TIC-inspired deception significantly enhances enjoyment and engagement, though reduces perceived safety and trust, suggesting a structured trade-off across the multi-dimensional aspects. Our findings demonstrate that playful deception can be a valuable design strategy for interactive robots in entertainment and engagement-focused contexts, while underscoring the importance of deliberate consideration of its complex trade-offs. You can find more information, including demonstration videos, on https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/.",
            "introduction": "In Human–Robot Interaction (HRI), deceptive behaviours of robots are often treated as harmful since they often reduce user trust [1, 2, 3, 4]. Prior studies have mainly focused on avoiding deception through predictable behaviours and intent displays [5, 6, 7], or on repairing trust [8, 9, 10]. On the other hand, empirical investigations into the potential benefits of robot deception have been conducted in game contexts where deception is permitted or even encouraged [11, 12, 13].\n\nHowever, beyond game-like contexts where the deception is explicitly allowed, some playful deceptions also enrich interpersonal experiences in everyday human-human interaction. Playful deception can create enjoyable, light-hearted moments, so long as it is interpreted as part of a benign performance rather than as manipulation. For instance, a human handing over an object might playfully delay the delivery or briefly mislead the receiver through unexpected but harmless actions, forming humour that is only possible through physical embodiment.\n\nThe Turkish ice cream (TIC) vendor routine exemplifies this point: it relies on intentional misdirection to entertain while ultimately delivering the treat. The TIC interaction is a short street performance in which a vendor playfully prolongs the handover of an ice cream cone. Using a long spatula-like rod, the cone is presented and then briefly withdrawn, with light feints and showy gestures that tease the customer while keeping the exchange clearly playful. The routine is recognizably performative and always resolves with a successful handover, marking the deception as benign rather than malicious.\n\nIn this paper, we explore the potential role of playful deception as a design component in HRI through a study inspired by the TIC routine. To this end, as shown in Fig. 1, we design and implement a robotic handover system that reproduces TIC-inspired playful deceptive behaviours and conduct a user study with 91 participants to investigate the multi-dimensional effects of playful deception, such as enjoyment, trust, and perceived safety.\n\nIn summary, this paper makes three primary contributions:\n\nWe design and implement a robotic system capable of reproducing TIC-inspired deceptive handovers, introducing a novel playful interaction scenario.\n\nWe conduct a large-scale user study evaluating the multi-dimensional effects of playful TIC deception on user experience, revealing both its positive and negative impacts.\n\nWe emphasize the nuanced trade-offs that playful deception introduces and the importance of situating its use within appropriate contexts, offering guidance for both design and future research.\n\n1. We design and implement a robotic system capable of reproducing TIC-inspired deceptive handovers, introducing a novel playful interaction scenario.\n\n2. We conduct a large-scale user study evaluating the multi-dimensional effects of playful TIC deception on user experience, revealing both its positive and negative impacts.\n\n3. We emphasize the nuanced trade-offs that playful deception introduces and the importance of situating its use within appropriate contexts, offering guidance for both design and future research.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有研究主要关注避免机器人欺骗，缺乏对其潜在益处的探讨。  \n2. 玩耍式欺骗在日常人际互动中能够增添乐趣和轻松感。  \n3. 需要探索如何将玩耍式欺骗作为人机交互设计的组成部分。  \n\n【提出了什么创新的方法】  \n本文设计并实现了一种能够重现土耳其冰淇淋（TIC）风格的欺骗性交接的机器人系统，创造了一种新颖的互动场景。通过对91名参与者进行的大规模用户研究，评估了玩耍式欺骗对用户体验的多维影响，结果显示这种欺骗显著增强了用户的享受和参与感，但同时降低了安全感和信任感，表明在设计中需要权衡这些复杂的影响。研究结果为娱乐和互动为主的机器人设计提供了有价值的策略，同时强调了在适当的上下文中使用玩耍式欺骗的重要性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "VLBiMan: Vision-Language Anchored One-Shot Demonstration Enables Generalizable Robotic Bimanual Manipulation",
            "authors": "Huayi Zhou,Kui Jia",
            "subjects": "Robotics (cs.RO)",
            "comment": "under review",
            "pdf_link": "https://arxiv.org/pdf/2509.21723",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21723",
            "arxiv_html_link": "https://arxiv.org/html/2509.21723v1",
            "abstract": "Achieving generalizable bimanual manipulation requires systems that can learn efficiently from minimal human input while adapting to real-world uncertainties and diverse embodiments. Existing approaches face a dilemma: imitation policy learning demands extensive demonstrations to cover task variations, while modular methods often lack flexibility in dynamic scenes. We introduce VLBiMan, a framework that derives reusable skills from a single human example through task-aware decomposition, preserving invariant primitives as anchors while dynamically adapting adjustable components via vision-language grounding. This adaptation mechanism resolves scene ambiguities caused by background changes, object repositioning, or visual clutter without policy retraining, leveraging semantic parsing and geometric feasibility constraints. Moreover, the system inherits human-like hybrid control capabilities, enabling mixed synchronous and asynchronous use of both arms. Extensive experiments validate VLBiMan across tool-use and multi-object tasks, demonstrating: (1) a drastic reduction in demonstration requirements compared to imitation baselines, (2) compositional generalization through atomic skill splicing for long-horizon tasks, (3) robustness to novel but semantically similar objects and external disturbances, and (4) strong cross-embodiment transfer, showing that skills learned from human demonstrations can be instantiated on different robotic platforms without retraining. By bridging human priors with vision-language anchored adaptation, our work takes a step toward practical and versatile dual-arm manipulation in unstructured settings.",
            "introduction": "Recent years have witnessed rapid progress in embodied robotic manipulation, particularly under the paradigm of visuomotor imitation learning through large-scale teleoperated demonstrations Fang et al. (2024a); Khazatsky et al. (2024); O’Neill et al. (2024); Bu et al. (2025). By collecting thousands of real-world samples for each task and object setting, Vision-Language-Action (VLA) models Team et al. (2024); Kim et al. (2024); Lin et al. (2025) are trained to directly map raw sensory inputs to motor commands. This end-to-end approach avoids explicitly modeling task- or object-specific priors (even for challenging cases involving deformable or articulated objects), by embedding such complexities into high-dimensional latent representations. Such strategies are especially compatible with high-DoF collaborative scenarios like bimanual manipulation, enabling impressive performance on long-horizon tasks, as demonstrated by works such as ALOHA series Zhao et al. (2023a); Fu et al. (2024); Aldaco et al. (2024); Zhao et al. (2024), RDT-1B Liu et al. (2025a), π0\\pi_{0} Black et al. (2024), and FAST Pertsch et al. (2025). However, this line of research is bottlenecked by its reliance on large-scale data collection and retraining cycles: adapting to new objects or tasks typically demands a full demonstration pipeline and model retraining, hindering scalability in open-world settings with unbounded task-object combinations and robot types.\n\nTo alleviate this, recent efforts have embraced modularized VLA pipelines that leverage the generalization capabilities of pre-trained LLMs Achiam et al. (2023) and VLMs Radford et al. (2021); Xiao et al. (2024). These models are repurposed to handle perception and semantic grounding, while downstream motion execution is delegated to either optimization-based controllers or pretrained visuomotor modules such as atomic skills or diffusion policies Chi et al. (2023); Ze et al. (2024); Yang et al. (2024). Reinforcement learning in simulation also serves as a strategy for learning skill-specific controllers Xie et al. (2020); Chen et al. (2022); Yuan et al. (2024b). This modular design allows robotic agents to inherit part of the generalization capability from foundation models, while maintaining flexibility and interpretability. A common practice in these pipelines is to define generalizable representations (e.g., keypoints, affordances and correspondences), as structured anchors between perception and control. For instance, ReKep Huang et al. (2024b) plans robot motion by anchoring on multiple predicted relation points, MOKA Fang et al. (2024b) extracts fine-grained functional regions via multi-modal visual question answering, and RobotPoint Yuan et al. (2024a) identifies object-centric task-relevant point clusters. Such approaches demonstrate that keypoint-affordance abstractions are effective for transferring behavior across objects, viewpoints, or instances, and have become a cornerstone of generalizable manipulation.\n\nBuilding on this insight, we propose VLBiMan for one-shot bimanual manipulation that leverages vision-language anchoring without retraining. Our approach also relies on object-centric representation points, but rather than predicting them via learned networks, we utilize VLMs to perform stable and robust object segmentation, followed by two heuristic strategies for anchor selection: geometric center of masks and plane-contact points. These anchors, though reminiscent of affordances, are far more controllable and lightweight. Unlike prior zero-shot methods Huang et al. (2024b) that require fragile prompt engineering and suffer from unreliable trajectory execution, our framework is demonstration-conditioned: we structure the action plan based on a one-shot, fine-labeled demonstration, then adapt it using language-grounded object anchors and motion optimization techniques. This enables robust execution on complex bimanual tasks while reusing invariant sub-skills.\n\nOur methodology unfolds in three stages: (1) Task-Aware Bimanual Decomposition, which splits the one-shot demonstration into semantically meaningful left/right arm primitives with inter-arm dependencies; (2) Vision-Language Anchored Adaptation, which grounds the invariant motion primitives onto new scenes by aligning demonstration anchors with newly segmented objects via VLMs; (3) Autonomous Trajectory Composition, which composes new robot trajectories through kinematics-aware blending of adapted sub-skills, ensuring smooth coordination under scene variations. The related illustrations can be glimpsed in Fig. 1 and Fig. 2. VLBiMan actually is inspired by a key principle: what to achieve matters more than how to execute it. For instance, rather than mimicking the exact poses or insignificant diversities involved in pouring water, our approach focuses on capturing and re-instantiating the relative spatial relationship between the cup and bottle, emphasizing coordination rather than absolute motion. We validate VLBiMan across ten diverse bimanual tasks (including six basic bimanual skills, two long-horizon tasks consisting of skill combinations, and two multi-stage tool-use tasks), demonstrating superior generalization and minimal engineering overhead compared to prior strong baseline methods.\n\nTo summarize, our contributions are as follows:\n(i) We propose VLBiMan, a novel framework that enables generalizable bimanual manipulation through one-shot demonstration and vision-language anchoring, without retraining.\n(ii) We introduce a task-aware motion decomposition and adaptation mechanism, which reuses invariant sub-skills via object-centric anchors from VLMs and supports cross-embodiment transfer from human demonstrations to different robotic embodiments.\n(iii) We validate VLBiMan on ten diverse bimanual tasks, showing superior generalization, sample efficiency, and robustness compared to strong baselines.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有方法在模仿学习中需要大量示范以覆盖任务变体。  \n2. 模块化方法在动态场景中缺乏灵活性。  \n3. 需要从最小的人类输入中高效学习并适应现实世界的不确定性。  \n\n【提出了什么创新的方法】  \nVLBiMan框架通过任务感知分解从单个示范中提取可重用技能，利用视觉-语言锚定动态适应可调组件。该方法分为三个阶段：任务感知双手分解、视觉-语言锚定适应和自主轨迹组合，确保在场景变化下的平滑协调。通过在十个多样化的双手任务上进行验证，VLBiMan显示出显著的示范需求减少、组合泛化能力和对新物体的鲁棒性，且无需重新训练，展现了在非结构化环境中双手操作的实用性和灵活性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation",
            "authors": "Muqun Hu,Wenxi Chen,Wenjing Li,Falak Mandali,Zijian He,Renhong Zhang,Praveen Krisna,Katherine Christian,Leo Benaharon,Dizhi Ma,Karthik Ramani,Yan Gu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21690",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21690",
            "arxiv_html_link": "https://arxiv.org/html/2509.21690v1",
            "abstract": "Humanoid table tennis (TT) demands rapid perception, proactive whole-body motion, and agile footwork under strict timing—capabilities that remain difficult for unified controllers. We propose a reinforcement learning framework that maps ball-position observations directly to whole-body joint commands for both arm striking and leg locomotion, strengthened by predictive signals and dense, physics-guided rewards. A lightweight learned predictor, fed with recent ball positions, estimates future ball states and augments the policy’s observations for proactive decision-making. During training, a physics-based predictor supplies precise future states to construct dense, informative rewards that lead to effective exploration. The resulting policy attains strong performance across varied serve ranges (hit rate ≥\\geq 96% and success rate ≥\\geq 92%) in simulations. Ablation studies confirm that both the learned predictor and the predictive reward design are critical for end-to-end learning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute joints, the policy produces coordinated lateral and forward–backward footwork with accurate, fast returns, suggesting a practical path toward versatile, competitive humanoid TT.",
            "introduction": "Humanoid robots hold great promise as general-purpose embodied intelligent agents capable of performing diverse real-world tasks. Recent advances in learning and control have substantially expanded their physical capability, with impressive demonstrations of walking [1], jumping [2], running, and dancing [3]. Still, most research has focused on scenarios where humanoids operate in free space or interact only with static objects. Reacting to fast-moving objects with highly coordinated versatile behaviors remains a fundamental challenge in humanoid whole-body control (WBC).\n\nTable tennis (TT), both a cerebral and physically demanding sport, exemplifies this challenge. Competitive TT matches are characterized by breathtakingly dynamic exchanges that demand rapid eye-hand-leg and whole-body coordination as well as high-level strategies that adapt to an opponent’s strengths and weaknesses. Versatile stroke strategies, which requires agile footwork, core stabilization, and precisely coordinated arm swing, play a key role in winning a table tennis match [4, 5]. In contrast to some of the previous dynamic locomotion or quasi-static manipulation tasks, robotic TT is distinguished by the need for high-speed perception, control, and high-level physical versatility – the ball needs to be played very precisely in time and space.\n\nTowards solving these challenges, this paper presents an end-to-end reinforcement learning (RL) framework\nthat maps ball-position observations and robot proprioception directly to whole-body reference motions for coordinated arm swing and footwork with high success rate (Fig. 1).\n\nPrior work on robotic TT control has been predominantly based on analytical models, combining ball-trajectory prediction with inverse kinematics for robot motion planning.\n\nMost approaches adopt the virtual hitting-plane assumption, where a virtual hitting point [6] is computed from a partial ball trajectory in real-time. With the predicted ball velocity at a given time instant, the target racket pose (i.e., position and orientation) and velocity are then specified, and a robot’s motion trajectory is generated to bring the racket to the desired pose at the required time [7, 8, 9, 10].\nYet, the virtual hitting-plane assumption is restrictive.\nIt reduces TT to a predictable machine task, eliminating the need for highly adaptive footwork and hand-leg coordination, and preventing both humans and humanoids from learning the true variability of play.\n\nRecently, the virtual hitting plane assumption has been relaxed to enable model-based controllers to realize combat strategies with competitive success rates [11] and returning speeds across multiple swing types [12].\nStill, most of the existing model-based approaches have been primarily focused on robotic arms with relatively few degrees of freedom (DoFs), making it difficult to directly scale to humanoid whole-body movement, which involves far more DoFs. For example, a humanoid robot has been shown to play TT using impedance control [13], but its motions are constrained to standing still without agile locomotion, thereby limiting the effective hitting range.\n\nLearning-based methods have made substantial progress toward achieving human-level athletic intelligence in robotic TT [14, 15, 16, 17, 18].\n\nConcurrently with this work, HITTER hierarchically integrates an analytical model based interception planner with an RL controller for humanoid whole-body TT, yielding the first demonstration with agile footwork and accurate strikes [19]. However, due to its underlying virtual-hitting plane constraint, the robot relies primarily on lateral footwork.\n\nResearchers at Google DeepMind have introduced an end-to-end, model-free RL framework that relaxes the virtual hitting-plane assumption and learns a perception-to-action policy on a robotic arm [17]. The robot arm has 6 revolute joints, with its base sliding along two orthogonal axes (lateral and forward–backward).\nThis configuration enlarges the reachable workspace and helps enable diverse emergent strokes.\nSubsequent work demonstrates competitive, strategy-aware play [15]. While the end-to-end framework works for robotic arms, its effectiveness on legged humanoids remains an open challenge.\nDue to the high-dimensional action space and inherent locomotion instability of humanoid robots, the training will struggle with efficient exploration. Also, the training can suffer from low sampling efficiency due to the sparse nature of ball-hitting rewards [19].\n\nTowards enabling versatile humanoid TT, this paper introduces an:\n\nEnd-to-end, unified RL formulation that jointly coordinates locomotion and striking directly based on ball-position observations and robot proprioception without modular or hierarchical planning.\n\nAugmentation of the RL pipeline with a ball-hitting position predictor that enables the RL policy to act proactively for returning the ball, as well as with dense, immediate reward design based on physics-based trajectory prediction. Such an augmentation accelerates policy learning toward effective TT strikes and footwork.\n\nTo the best of our knowledge, this is among the first unified RL frameworks that generate whole-body motions and demonstrate successful humanoid TT play, both in simulations and experimentally. The learned policy achieves high success rates at competitive ball speeds across a wide serving range, covering both width (side-to-side) and length (short-long) variations.\n\nThe remainder of this paper is organized as follows. Section II formulates the problem, followed by the RL design details in Section III. Section IV presents the setup for policy training and experimental validation, while the results are presented and discussed in Section V. Section VI concludes the paper and outlines potential future work.\n\n1. End-to-end, unified RL formulation that jointly coordinates locomotion and striking directly based on ball-position observations and robot proprioception without modular or hierarchical planning.\n\n2. Augmentation of the RL pipeline with a ball-hitting position predictor that enables the RL policy to act proactively for returning the ball, as well as with dense, immediate reward design based on physics-based trajectory prediction. Such an augmentation accelerates policy learning toward effective TT strikes and footwork.\n\n3. To the best of our knowledge, this is among the first unified RL frameworks that generate whole-body motions and demonstrate successful humanoid TT play, both in simulations and experimentally. The learned policy achieves high success rates at competitive ball speeds across a wide serving range, covering both width (side-to-side) and length (short-long) variations.",
            "llm_summary": "【论文的motivation是什么】  \n1. Humanoid robots struggle with rapid perception and coordinated motion in dynamic environments like table tennis.  \n2. Existing methods often rely on restrictive assumptions that limit adaptability and performance in real-world scenarios.  \n3. There is a need for an effective learning-based approach that can handle high-dimensional action spaces in humanoid robots.  \n\n【提出了什么创新的方法】  \n本论文提出了一种端到端的统一强化学习框架，直接根据球的位置观察和机器人本体感知来协调运动和击球，而无需模块化或分层规划。通过引入一个球击打位置预测器，增强了RL管道，使得RL策略能够主动返回球，并基于物理轨迹预测设计了密集的即时奖励。这种增强加速了策略学习，提升了乒乓球击打和步伐的有效性。实验结果表明，该策略在多种发球范围内实现了高成功率，展示了在仿真和实际应用中的有效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Generating Stable Placements via Physics-guided Diffusion Models",
            "authors": "Philippe Nadeau,Miguel Rogel,Ivan Bilić,Ivan Petrović,Jonathan Kelly",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "Submitted to the IEEE International Conference on Robotics and Automation 2026, Vienna, Austria, June 1-5, 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.21664",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21664",
            "arxiv_html_link": "https://arxiv.org/html/2509.21664v1",
            "abstract": "Stably placing an object in a multi-object scene is a fundamental challenge in robotic manipulation, as placements must be penetration-free, establish precise surface contact, and result in a force equilibrium.\nTo assess stability, existing methods rely on running a simulation engine or resort to heuristic, appearance-based assessments.\nIn contrast, our approach integrates stability directly into the sampling process of a diffusion model.\nTo this end, we query an offline sampling-based planner to gather multi-modal placement labels and train a diffusion model to generate stable placements.\nThe diffusion model is conditioned on scene and object point clouds, and serves as a geometry-aware prior.\nWe leverage the compositional nature of score-based generative models to combine this learned prior with a stability-aware loss, thereby increasing the likelihood of sampling from regions of high stability.\nImportantly, this strategy requires no additional re-training or fine-tuning, and can be directly applied to off-the-shelf models.\nWe evaluate our method on four benchmark scenes where stability can be accurately computed. Our physics-guided models achieve placements that are 56% more robust to forceful perturbations while reducing runtime by 47% compared to a state-of-the-art geometric method.",
            "introduction": "The task of planning to stably place an object among other objects, henceforth referred to as stable placement planning, is a core element of several higher level tasks, like construction [1, 2], scene rearrangement [3], and dense packing [4].\nThe challenge lies in satisfying strict geometric and physical constraints that characterize the problem.\nA placement pose must establish precise surface contact with objects in the scene while avoiding penetration with any other object. At the same time, it must result in a force equilibrium to prevent toppling.\nIdeally, a placement planner would sample directly from the space of valid poses; however, defining this subset is complex.\nOnly a very small proportion of all poses in the workspace result in valid placements, making random sampling or searching approaches highly inefficient [5].\n\nWe argue that a principled approach to determine stable placements requires: (i) a geometry-aware placement algorithm, (ii) reasoning about scene equilibrium, and (iii) a stability verifier to assess the outcome. To reason about scene equilibrium, existing methods either rely on executing a simulation engine [6, 7], or assess placement quality solely based on appearance [8, 9]. The former is time-consuming, while the latter requires making assumptions about object inertial parameters.\n\nWe propose a simulation-free approach that addresses stability in a principled manner, by merging requirements (i) and (ii): enhancing the placement algorithm with a physics-based feature, obtained from [10], termed robustness. Robustness describes the maximum force that can be applied at a point in a scene before any object moves.\n\nDue to the lack of publicly available datasets that capture the multi-object stability information necessary for our approach, we employ a state-of-the-art sampling-based planner [5] to act as an expert. We construct four scenes, and use the planner offline to generate multi-modal stable placement examples and physics-based robustness features. Our scenes comprise flat placement surfaces, similar to recent works which consider stability [6, 7, 8, 9]. While such scenes do not capture the full complexity of the real world, they allow us to precisely assess the quality of stability reasoning and a certain degree of generalization to unseen data.\n\nDiffusion models are capable of capturing complex distributions and excel at conditional and controllable generation [11, 12].\nTherefore, we introduce a physics-guided score-based generative model for stable placement planning. The compositional nature of score-based generative models allows us to combine multiple likelihood terms—such as the prior and differentiable loss—into a single sampling distribution.\nOur model is trained to generate stable placements conditioned only on scene and object geometry, serving as a prior with no explicit physics information. During sampling, we introduce a stability-aware diffusion guidance to inform the learned prior about scene statics, steering the sampling process toward regions of high stability, as shown in Fig. 1.\nOur guidance is easy to compute and does not affect model training, making it applicable to off-the-shelf models. Compared to our geometry-based learned prior and [5], we observe our guidance can increase the placement validity rate up to 40%40\\% on unseen scenes, and overall scene robustness after placing by 56%56\\%.\nWe also discuss architectural details that we identify as crucial for enabling the model to generalize, independent of guidance.\nTo the best of our knowledge, our work is the first to incorporate a physics-informed prior into a generative stable placement model.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的稳定放置方法依赖于模拟引擎或启发式评估，效率低下。  \n2. 需要一个无模拟的、有效的稳定放置规划方法，以满足几何和物理约束。  \n3. 缺乏公开数据集来捕捉多物体稳定性信息，限制了方法的发展。  \n\n【提出了什么创新的方法】  \n本研究提出了一种物理引导的扩散模型，用于稳定放置规划。通过查询离线采样规划器生成多模态稳定放置示例，并结合几何和稳定性信息，训练扩散模型以生成稳定放置。该模型在采样过程中引入稳定性引导，提升了采样的有效性和稳定性。实验结果表明，所提出的方法在四个基准场景中实现了56%的抗扰动能力提升和47%的运行时间减少，展示了其在实际应用中的有效性和高效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Real-Time Indoor Object SLAM with LLM-Enhanced Priors",
            "authors": "Yang Jiao,Yiding Qiu,Henrik I. Christensen",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21602",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21602",
            "arxiv_html_link": "https://arxiv.org/html/2509.21602v1",
            "abstract": "Object-level Simultaneous Localization and Mapping (SLAM), which incorporates semantic information for high-level scene understanding, faces challenges of under-constrained optimization due to sparse observations. Prior work has introduced additional constraints using commonsense knowledge, but obtaining such priors has traditionally been labor-intensive and lacks generalizability across diverse object categories. We address this limitation by leveraging large language models (LLMs) to provide commonsense knowledge of object geometric attributes, specifically size and orientation, as prior factors in a graph-based SLAM framework.\nThese priors are particularly beneficial during the initial phase when object observations are limited.\nWe implement a complete pipeline integrating these priors, achieving robust data association on sparse object-level features and enabling real-time object SLAM. Our system, evaluated on the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8% over the latest baseline. Additionally, we present real-world experiments in the supplementary video, demonstrating its real-time performance.",
            "introduction": "Object Simultaneous Localization and Mapping (SLAM) builds environment maps by identifying and localizing objects, and using this information to infer the robot’s position. Unlike traditional feature-based SLAM, object-level representations are sparse, focusing on semantic object data. Comparing to semantic segmentation on dense representations, such sparsity improves computational efficiency and reduces storage requirements. Meanwhile, high-level semantic information makes object SLAM useful for downstream tasks, such as 3D semantic scene reconstruction, object-goal navigation, and object retrieval [1].\n\nWhen navigating an unfamiliar environment to find specific objects, the agent must efficiently build a map while moving quickly to cover large areas, often resulting in limited observations of objects from different viewpoints [2]. SLAM with sparse observations commonly occurred in real-world applications, such as when the camera frame rate is low relative to the agent’s motion speed or when loop closure is unavailable. However, due to the inherent sparsity of object-level features, fewer-constraint object SLAM can be extremely challenging. Such sparsity affects the quality of initial mapping and might hinder subsequent optimization and data association.\n\nTo address these issues, we introduce additional constraints from commonsense knowledge. Objects typically exhibit common attributes, such as size, shape, and orientation, which provide valuable priors for mapping systems. These attributes are essential during the initial observation phase, where limited 2D features and uncertain pose estimations bring challenges. By incorporating these priors, the solution space for object estimation can be effectively constrained, enhancing mapping accuracy in sparse observation scenarios. However, acquiring comprehensive object priors has traditionally been labor-intensive, requiring substantial human annotation across diverse object categories [3]. Furthermore, commonsense knowledge about objects can vary by context, leading to potential mismatches between expected and actual observations.\n\nWe leveraged large language models (LLMs) to provide prior information, as commonsense inference is LLM’s strong suit [4].\nWe propose a LLM-enhanced prior-assisted object SLAM framework to address the challenge of under-constrained optimization in object SLAM.\nOur approach models the prior knowledge from LLM as factors using a factor graph [5]. Specifically, we focus on the geometric intrinsics of size and orientation for various object categories.\n\nOur method integrates prior information from LLM and a combined data association method to achieve a full online object SLAM (Fig. 1). We validated it on real-world datasets.\nThe key contributions of this paper are outlined below:\n\nExploration of using commonsense priors, particularly object size and orientation, and encoding them as prior factors for a graph-based SLAM system;\n\nA complete pipeline from embedding LLM-enhanced priors, extracting object-level features, performing data association, to solving incremental optimization;\n\nEvaluation using two real-world datasets: TUM RGB-D and 3RScan, and test of our SLAM method in real-time.\n\n1. Exploration of using commonsense priors, particularly object size and orientation, and encoding them as prior factors for a graph-based SLAM system;\n\n2. A complete pipeline from embedding LLM-enhanced priors, extracting object-level features, performing data association, to solving incremental optimization;\n\n3. Evaluation using two real-world datasets: TUM RGB-D and 3RScan, and test of our SLAM method in real-time.",
            "llm_summary": "【论文的motivation是什么】  \n1. Object-level SLAM faces challenges of under-constrained optimization due to sparse observations.  \n2. Traditional methods for acquiring commonsense knowledge are labor-intensive and lack generalizability across object categories.  \n3. Limited observations during initial mapping hinder accurate data association and optimization.  \n\n【提出了什么创新的方法】  \n本文提出了一种基于大语言模型（LLM）的先验辅助对象SLAM框架，通过将对象的几何属性（如大小和方向）作为图优化中的先验因素，来解决对象SLAM中的欠约束优化问题。该方法集成了LLM增强的先验信息和数据关联技术，形成了一个完整的在线对象SLAM流程。经过在TUM RGB-D和3RScan数据集上的评估，该系统在稀疏对象特征的映射精度上提高了36.8%。此外，实地实验展示了其实时性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control",
            "authors": "HaoZhe Xu,Cheng Cheng,HongRui Sang,Zhipeng Wang,Qiyong He,Xiuxian Li,Bin He",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21571",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21571",
            "arxiv_html_link": "https://arxiv.org/html/2509.21571v1",
            "abstract": "Autonomous docking between Unmanned Aerial Vehicles (UAVs) and ground robots is essential for heterogeneous systems, yet most existing approaches target wheeled platforms whose limited mobility constrains exploration in complex terrains. Quadruped robots offer superior adaptability but undergo frequent posture variations, making it difficult to provide a stable landing surface for UAVs. To address these challenges, we propose an autonomous UAV–quadruped docking framework for GPS-denied environments. On the quadruped side, a Hybrid Internal Model with Horizontal Alignment (HIM-HA), learned via deep reinforcement learning, actively stabilizes the torso to provide a level platform. On the UAV side, a three-phase strategy is adopted, consisting of long-range acquisition with a median-filtered YOLOv8 detector, close-range tracking with a constraint-aware controller that integrates a Nonsingular Fast Terminal Sliding Mode Controller (NFTSMC) and a logarithmic Barrier Function (BF) to guarantee finite-time error convergence under field-of-view (FOV) constraints, and terminal descent guided by a Safety Period (SP) mechanism that jointly verifies tracking accuracy and platform stability. The proposed framework is validated in both simulation and real-world scenarios, successfully achieving docking on outdoor staircases higher than 17 cm and rough slopes steeper than 30∘30^{\\circ}. Supplementary materials and videos are available at: https://uav-quadruped-docking.github.io.",
            "introduction": "Heterogeneous cooperative systems that integrate Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) can expand operational scope and improve efficiency compared to single-domain platforms [1]. Autonomous docking is a key capability for many UAV–UGV collaborative tasks, yet most existing schemes focus on wheeled UGVs, whose mobility is restricted to flat terrain, limiting exploration in complex environments. Moreover, dynamic docking requires UAVs to achieve precise localization and safe landing on moving platforms, imposing high demands on sensor fusion and robust control[2].\n\nQuadruped robots, with their legged morphology, surpass wheeled and tracked UGVs in unstructured terrains and enable UAV collaboration in challenging environments such as mountains or tunnels. Recent advances in deep reinforcement learning (DRL) provide a new paradigm for quadruped locomotion [3, 4, 5]. Unlike model-based controllers (e.g., Model Predictive Control, MPC [6]), DRL maps states and observations directly to joint actions, allowing adaptation to unseen terrains and sensor noise with strong generalization. However, prior studies mainly optimize locomotion of single quadrupeds, neglecting collaborative tasks that require posture and velocity constraints to support UAV docking.\n\nThis work addresses UAV–quadruped docking in complex three-dimensional terrains, where quadrupeds undergo large posture variations. On stairs higher than 15 cm or slopes steeper than 20∘20^{\\circ}, pitch angles often exceed 20∘20^{\\circ}, producing an unstable landing surface for UAVs. Such instability also degrades vision-based tracking (e.g., AprilTag), making it necessary for UAVs to fuse onboard sensing with quadruped state feedback for robust tracking and precise localization.\n\nAutonomous docking Autonomous docking techniques can be broadly categorized into two types: infrastructure-based methods and on-board sensor–based methods. Infrastructure-assisted schemes typically rely on external positioning systems such as GPS, BeiDou, or motion capture systems (MCS) [7, 8]. In [9], LiDAR, IMU, and GPS were tightly fused to improve environment perception and landing point selection for powered parachute UAVs. In [10], the effectiveness of a landing algorithm combining GPS and ArUco marker detection was verified in the Webots simulation environment. However, these methods depend heavily on infrastructure, incur high deployment costs, and lack adaptability. In cluttered environments such as vegetation, water bodies, or indoor spaces, GPS signals are often degraded or lost [11], making such approaches unreliable.\n\nOn-board sensor–based docking methods [12, 13] employ vision, LiDAR, and IMU to operate in GPS-denied settings. In [14], a lightweight vision-based controller enabled quadrotors to land autonomously on unknown moving platforms. In [15], semantic and depth information from cameras and LiDAR were fused to autonomously search and select safe landing sites. These methods, however, are sensitive to sensor quality, perform poorly in low-light or adverse conditions, and impose significant computational overhead.\n\nMost existing docking methods are designed for wheeled UGVs, which are limited to flat or structured terrain. In rugged environments such as rubble fields, steep slopes, or stairs, wheeled platforms struggle to operate effectively, and frequent posture variations of the landing surface further increase the risk of field-of-view (FOV) loss during docking.\n\nLearning-based quadruped locomotion Quadruped robots surpass wheeled UGVs in unstructured terrains through flexible foothold planning and are increasingly employed in exploration tasks. Reinforcement learning (RL) has emerged as a powerful paradigm for quadruped locomotion [3, 5, 16], with policies trained entirely in simulation and transferred to hardware via zero-shot sim-to-real. In [17], the authors introduced an adaptation module within a teacher–student framework to achieve highly robust locomotion. In [18], behavior parameters such as body height command, body pitch command, and footswing height command were incorporated into RL observation and reward functions, resulting in a versatile low-level controller capable of structured behaviors. However, these approaches mainly address mildly uneven terrains and lack active posture control for complex environments.\n\nRecent RL-based strategies have demonstrated superior adaptability to traditional controllers in unstructured 3D terrains [19, 20, 21, 22]. In [19], a context-aided estimator was proposed to jointly infer body state and environmental context, showing strong robustness across complex terrains. In [20], the authors combined internal model control (IMC) [23] with reinforcement learning and proposed a hybrid internal model (HIM) to estimate robot velocity and simulate system response, enabling stable and agile locomotion over stairs and slopes while mitigating noise and sampling inefficiency\n\nNevertheless, most RL frameworks treat body posture alignment as a passive byproduct of terrain adaptation. In UAV–quadruped cooperative tasks such as autonomous docking, quadrupeds must not only maintain robust locomotion but also actively adjust posture to provide a level dorsal surface for UAV landing. Current methods lack such task-oriented posture control, limiting the potential of quadrupeds in collaborative aerial–ground missions.\n\nTo address the landing difficulties caused by the dynamic torso posture of the quadruped robot, we model the UAV-quadruped robot docking task in complex terrain environments as a heterogeneous robot cooperative planning problem. Optimization methods are designed separately for the UAV and the quadruped robot to achieve safe and precise docking.\nOur contributions are as follows:\n\nWe propose an autonomous UAV–quadruped docking framework that integrates multi-sensor fusion with an active posture alignment mechanism to enhance reliability in GPS-denied environments.\n\nWe propose an autonomous UAV–quadruped docking framework that integrates multi-sensor fusion with an active posture alignment mechanism to enhance reliability in GPS-denied environments.\n\nOn the quadruped side, we extend the Hybrid Internal Model (HIM) and introduce HIM with Horizontal Alignment (HIM-HA), a reinforcement learning–based locomotion strategy that enables quadrupeds to actively stabilize their torso and provide a level landing platform in complex 3D terrains.\n\nOn the UAV side, we develop a constraint-aware controller that integrates a Nonsingular Fast Terminal Sliding Mode Controller (NFTSMC) with a logarithmic Barrier Function (BF) to guarantee finite-time error convergence under field-of-view (FOV) constraints, and introduce a Safety Period (SP) strategy to ensure robust and reliable terminal descent.\n\nWe validate the proposed framework in both simulation and real-world experiments across challenging environments, including stairs and steep slopes. To the best of our knowledge, this is the first study to demonstrate successful UAV–quadruped docking in unstructured terrains.\n\n1. We propose an autonomous UAV–quadruped docking framework that integrates multi-sensor fusion with an active posture alignment mechanism to enhance reliability in GPS-denied environments.\n\n2. On the quadruped side, we extend the Hybrid Internal Model (HIM) and introduce HIM with Horizontal Alignment (HIM-HA), a reinforcement learning–based locomotion strategy that enables quadrupeds to actively stabilize their torso and provide a level landing platform in complex 3D terrains.\n\n3. On the UAV side, we develop a constraint-aware controller that integrates a Nonsingular Fast Terminal Sliding Mode Controller (NFTSMC) with a logarithmic Barrier Function (BF) to guarantee finite-time error convergence under field-of-view (FOV) constraints, and introduce a Safety Period (SP) strategy to ensure robust and reliable terminal descent.\n\n4. We validate the proposed framework in both simulation and real-world experiments across challenging environments, including stairs and steep slopes. To the best of our knowledge, this is the first study to demonstrate successful UAV–quadruped docking in unstructured terrains.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的无人机-地面机器人对接方法主要针对轮式平台，限制了在复杂地形中的探索能力。  \n2. 四足机器人在不规则地形中具有更好的适应性，但其姿态变化频繁，导致无人机难以提供稳定的着陆表面。  \n3. 在GPS信号缺失的环境中，现有的对接技术缺乏可靠性和适应性。  \n\n【提出了什么创新的方法】  \n本研究提出了一种自主无人机-四足机器人对接框架，结合多传感器融合与主动姿态对齐机制，以增强在GPS-denied环境中的可靠性。四足机器人方面，扩展了混合内部模型（HIM），引入了HIM-HA策略，通过深度强化学习主动稳定其躯干，提供平坦的着陆平台。无人机方面，开发了一种约束感知控制器，结合了非奇异快速终端滑模控制器（NFTSMC）和对数障碍函数（BF），确保在视野约束下的有限时间误差收敛，并引入安全期（SP）策略以确保可靠的终端下降。该框架在模拟和真实场景中得到了验证，成功实现了在超过17厘米高的户外楼梯和超过30°的陡坡上的对接，展示了在不规则地形中无人机-四足机器人对接的可行性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "PL-VIWO2: A Lightweight, Fast and Robust Visual-Inertial-Wheel Odometry Using Points and Lines",
            "authors": "Zhixin Zhang,Liang Zhao,Pawel Ladosz",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21563",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21563",
            "arxiv_html_link": "https://arxiv.org/html/2509.21563v1",
            "abstract": "Vision-based odometry has been widely adopted in autonomous driving owing to its low cost and lightweight setup; however, its performance often degrades in complex outdoor urban environments. To address these challenges, we propose PL-VIWO2, a filter-based visual–inertial–wheel odometry system that integrates an IMU, wheel encoder, and camera (supporting both monocular and stereo) for long-term robust state estimation. The main contributions are: (i) a novel line feature processing framework that exploits the geometric relationship between 2D feature points and lines, enabling fast and robust line tracking and triangulation while ensuring real-time performance; (ii) an SE(2)-constrained SE(3) wheel pre-integration method that leverages the planar motion characteristics of ground vehicles for accurate wheel updates; and (iii) an efficient motion consistency check (MCC) that filters out dynamic features by jointly using IMU and wheel measurements. Extensive experiments on Monte Carlo simulations and public autonomous driving datasets demonstrate that PL-VIWO2 outperforms state-of-the-art methods in terms of accuracy, efficiency, and robustness.",
            "introduction": "The complex nature of urban environments makes it difficult to ensure long-term stable state estimation in autonomous driving, which continues to attract considerable research attentions. Visual-inertial Navigation Systems (VINS) or Visual-inertial-Odometry (VIO), which integrate cameras and inertial measurement units (IMUs), have been widely regarded as a promising solution for state estimation in unmanned systems due to their low cost and complementary sensing characteristics. Over the past decade, numerous representative VIO have been proposed, including ORB-SLAM3 [1], VINS-Mono [2], and OpenVINS [3].\n\nHowever, the IMU often fails to provide accurate measurements under high-speed motion, such as highway driving, leading to inaccurate estimation [4]. Furthermore, VINS may exhibit additional unobservable degrees of freedom during specific motion patterns, further degrading its performance in autonomous driving scenarios [5]. These challenges can be effectively addressed by incorporating wheel encoder, a standard sensor on ground vehicles, which provides additional observation information [6, 7]. Since the wheel measurements only provide 2D motion information, while the visual-inertial-wheel odometry (VIWO) system estimates a full 3D state, the planar motion characteristics of ground vehicles can be exploited to bridge this gap. Specifically, the 2D wheel measurements are extended into an SE(2) constricted SE(3) state space under planar motion assumption, thereby improving estimation accuracy.\n\nFor vision-based odometry, point features are commonly tracked across different camera poses and constrain the system state by using descriptors [8] or optical flow [9] methods. However, acquiring sufficiently reliable features in urban environments remains challenging due to dynamic objects, poor lighting conditions, and texture-less surfaces. To address this, incorporating additional geometric features, such as lines, has been proven to be effective in previous works [10, 11, 12]. Despite its potential, the application of line features in autonomous driving scenarios faces several limitations that hinder their practical deployment. First, processing line features introduces additional computational overhead, which may compromise the real-time performance. Secondly, conventional line tracking methods typically rely on descriptors, which often perform poorly in complex outdoor environments. For instance, high-speed linear motion or false line detections can lead to unreliable feature correspondences between views. Thirdly, due to the dominant planar motion of ground vehicles, classical triangulation methods may suffer from degenerate motion, resulting in inaccurate triangulation results and ultimately degrading the state estimation accuracy [13].\n\nAnother challenge for state estimation in autonomous driving environments is dynamic objects, such as moving vehicles and pedestrians. The inclusion of dynamic feature points can significantly degrade estimation accuracy. To mitigate this issue, some approaches utilize deep learning techniques to perform semantic segmentation and filter out dynamic points [14, 15]. But such methods are often computationally demanding and may not guarantee real-time performance on resource-limited platforms. Therefore, this paper leverages a lightweight and efficient Motion Consistency Check (MCC) by using the measurement information of IMU and wheel to reject dynamic features. Specifically, the motion of each feature point within a sliding window is compared against the predicted motion derived from inertial and wheel data. Features exhibiting inconsistent motion patterns are classified as dynamic or outliers and further excluded from state estimation.\n\nTo address the aforementioned challenges, we propose a novel visual-inertial-wheel odometry (VIWO) system that integrates both point and line features, named PL-VIWO2, designed for long-term robust and accurate state estimation of autonomous driving in complex outdoor environments. The main contributions of this work are:\n\nA lightweight, fast, and robust VIWO system leveraging both point and line features is proposed for handling complex autonomous driving scenarios, with support for both monocular and stereo configurations.\n\nTwo novel optical-flow-based line feature tracking methods are developed and combined to enable efficient and reliable line tracking in complex outdoor environments.\n\nA robust and accurate 3D line triangulation pipeline exploiting the geometric relationship between point and line features is implemented, including both initialization and optimization-based refinement.\n\nAn SE​(2)\\mathrm{SE}(2)-constrained SE​(3)\\mathrm{SE}(3) wheel pre-integration method is designed based on the planar motion characteristics of vehicles for wheel updates.\n\nA motion consistency check (MCC) module that eliminates the influence of dynamic features by leveraging IMU and wheel measurements.\n\nValidation through both Monte Carlo simulations and real-world experiments, demonstrating robustness, efficiency, and accuracy.\n\nThe source code of the system will be released as open source upon acceptance of the paper.\n\nCompared with our previous publication PL-VIWO [16], the new contributions of PL-VIWO2 are:\n\nPL-VIWO2 is extended to support both monocular and stereo configurations, ensuring compatibility with various hardware setups, whereas PL-VIWO supports only the monocular configuration.\n\nA second-stage optical-flow-based line matching method for tracking line features that cannot be directly tracked using point tracking results.\n\nAn optimization refinement strategy is introduced after 3D line initialization to further improve the accuracy of line triangulation results.\n\nExtensive evaluations are conducted on both simulations and real-world datasets, demonstrating that PL-VIWO2 achieves superior robustness and accuracy compared with PL-VIWO and other SOTA methods.\n\nThe rest of this paper is organized as follows. Section III reviews related work. Section III introduces the system overview and the state estimation method that integrates measurements from three sensors. Section IV details the proposed novel visual processing pipeline, including the point-based line feature tracking, triangulation, stereo configuration and motion consistency check. Section V presents Monte Carlo simulation results and evaluates the proposed line triangulation methods. Section VI reports real-world experimental results and performance evaluation. Finally, Section VII concludes the paper.\n\n1. A lightweight, fast, and robust VIWO system leveraging both point and line features is proposed for handling complex autonomous driving scenarios, with support for both monocular and stereo configurations.\n\n2. Two novel optical-flow-based line feature tracking methods are developed and combined to enable efficient and reliable line tracking in complex outdoor environments.\n\n3. A robust and accurate 3D line triangulation pipeline exploiting the geometric relationship between point and line features is implemented, including both initialization and optimization-based refinement.\n\n4. An SE​(2)\\mathrm{SE}(2)-constrained SE​(3)\\mathrm{SE}(3) wheel pre-integration method is designed based on the planar motion characteristics of vehicles for wheel updates.\n\n5. A motion consistency check (MCC) module that eliminates the influence of dynamic features by leveraging IMU and wheel measurements.\n\n6. Validation through both Monte Carlo simulations and real-world experiments, demonstrating robustness, efficiency, and accuracy.\n\n7. The source code of the system will be released as open source upon acceptance of the paper.\n\n1. PL-VIWO2 is extended to support both monocular and stereo configurations, ensuring compatibility with various hardware setups, whereas PL-VIWO supports only the monocular configuration.\n\n2. A second-stage optical-flow-based line matching method for tracking line features that cannot be directly tracked using point tracking results.\n\n3. An optimization refinement strategy is introduced after 3D line initialization to further improve the accuracy of line triangulation results.\n\n4. Extensive evaluations are conducted on both simulations and real-world datasets, demonstrating that PL-VIWO2 achieves superior robustness and accuracy compared with PL-VIWO and other SOTA methods.",
            "llm_summary": "【论文的motivation是什么】  \n1. 在复杂的城市环境中，视觉惯性导航系统的长期稳定状态估计面临挑战。  \n2. 现有的视觉惯性里程计在高速度运动下IMU的测量不准确，导致估计误差。  \n3. 动态物体的存在会显著降低状态估计的准确性。  \n\n【提出了什么创新的方法】  \n提出了一种轻量级、快速且稳健的视觉-惯性-轮子里程计系统PL-VIWO2，集成了IMU、轮子编码器和相机，支持单目和立体配置。该系统采用了一种新颖的线特征处理框架，结合了光流法进行高效的线跟踪和三角测量，同时实现了实时性能。此外，设计了SE(2)约束的SE(3)轮子预积分方法，以提高轮子更新的准确性。通过运动一致性检查（MCC）模块，有效过滤动态特征，确保了状态估计的鲁棒性和准确性。实验结果表明，PL-VIWO2在准确性、效率和鲁棒性方面超过了现有的最先进方法。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation",
            "authors": "Jinbang Huang,Zhiyuan Li,Zhanguang Zhang,Xingyue Quan,Jianye Hao,Yingxue Zhang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21543",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21543",
            "arxiv_html_link": "https://arxiv.org/html/2509.21543v1",
            "abstract": "Large Language Models (LLMs) have recently shown strong potential in robotic task planning, particularly through automatic planning domain generation that integrates symbolic search. Prior approaches, however, have largely treated these domains as search utilities, with limited attention to their potential as scalable sources of reasoning data. At the same time, progress in reasoning LLMs has been driven by chain-of-thought (CoT) supervision, whose application in robotics remains dependent on costly, human-curated datasets.\nWe propose Plan2Evolve, an LLM self-evolving framework in which the base model generates planning domains that serve as engines for producing symbolic problem–plan pairs as reasoning traces. These pairs are then transformed into extended CoT trajectories by the same model through natural-language explanations, thereby explicitly aligning symbolic planning structures with natural language reasoning. The resulting data extend beyond the model’s intrinsic planning capacity, enabling model fine-tuning that yields a planning-enhanced LLM with improved planning success, stronger cross-task generalization, and reduced inference costs.",
            "introduction": "Large Language Models (LLMs) have shown strong potential in robotic task planning due to their reasoning capabilities and cross-task generalization (Huang et al., 2023, 2022; Wang et al., 2024; Li et al., 2023; Zhao et al., 2024). However, like other learning-based methods, LLMs often suffer from stochastic outputs and error accumulation over long-horizon tasks, leading to failure in verifying action feasibility, state tracking, and dependency verification. To resolve these issues, researchers have combined LLMs with symbolic search-based algorithms for better long-horizon planning performance (Meng et al., 2024; Hu et al., 2023; Liu et al., 2023a). More recently, LLMs have been used to automatically infer planning domains, which, once combined with tree search, yield task-specific planners (Oswald et al., 2025; Byrnes et al., 2024; Guan et al., 2023a; Han et al., 2024b; Huang et al., 2025b). While effective, the domain-inference approaches mainly treat planning domains as mere search utilities, overlooking their potential to serve as generative engines of scalable, verifiable training data (Dalal et al., 2023; Khodeir et al., 2023).\n\nIn parallel, reasoning-focused LLMs have advanced significantly through chain-of-thought (CoT) training, where models learn to decompose complex problems into multi-step reasoning traces (Wei et al., 2022; Cobbe et al., 2021; Zelikman et al., 2022), yet CoT training usually relies on large, manually curated datasets, which are costly to construct, especially for robotics. A promising alternative is to leverage symbolic structures for scalable data production (Dalal et al., 2023). However, while prior research has extensively studied language-to-symbol transformation for robotic planning (Pan et al., 2023a; Han et al., 2024a; Tafjord et al., 2021), the reverse process, symbol-to-language transformation, remains underexplored for model trainingWang et al. (2025a). This gap raises an important question: can LLMs bootstrap their own planning ability by aligning symbolic plans with natural-language reasoning to internalize planning ability?\n\nWe propose Plan2Evolve, a self-evolving framework that reinterprets LLM-generated planning domains as knowledge sources for data generation. Given diverse tasks, the base model first self-generates planning domains that produce problem–plan pairs. These symbolic plans are then explained by the same model in natural language, resulting in long CoT trajectories aligned with formal planning semantics. Together, these outputs form a unified dataset that extends beyond the intrinsic planning capability of the base model, enabling supervised fine-tuning (SFT) to internalize and enhance its planning performance. Our contributions are:\n\nPlan2Evolve Framework: We introduce Plan2Evolve, a novel framework that treats LLM-generated PDDL domains as evolving knowledge sources, whose compositionality enables systematic task generation and the automatic creation of scalable planning supervision.\n\nPlan2Evolve Framework: We introduce Plan2Evolve, a novel framework that treats LLM-generated PDDL domains as evolving knowledge sources, whose compositionality enables systematic task generation and the automatic creation of scalable planning supervision.\n\nSelf-evolving Data Generation: Plan2Evolve enables the base model to generate validated long-horizon robotic planning problem–plan pairs that go beyond its intrinsic planning capacity, thereby eliminating the need for human curation of training data.\n\nSymbolic–Language Alignment: We introduce an automatic self-alignment procedure that translates symbolic PDDL plans and states into natural-language CoT using the base model, and empirically demonstrate that symbolic–language alignment is essential for training LLM to plan, with higher-quality alignment yielding substantial performance gains.\n\nEmpirical Gains: By SFT, Plan2Evolve produces a planning-enhanced LLM that achieves robust planning performance, stronger cross-task generalization, and reduced inference token costs. These results indicate that Plan2Evolve can improve both the efficiency and planning capacity of LLM.\n\n1. Plan2Evolve Framework: We introduce Plan2Evolve, a novel framework that treats LLM-generated PDDL domains as evolving knowledge sources, whose compositionality enables systematic task generation and the automatic creation of scalable planning supervision.\n\n2. Self-evolving Data Generation: Plan2Evolve enables the base model to generate validated long-horizon robotic planning problem–plan pairs that go beyond its intrinsic planning capacity, thereby eliminating the need for human curation of training data.\n\n3. Symbolic–Language Alignment: We introduce an automatic self-alignment procedure that translates symbolic PDDL plans and states into natural-language CoT using the base model, and empirically demonstrate that symbolic–language alignment is essential for training LLM to plan, with higher-quality alignment yielding substantial performance gains.\n\n4. Empirical Gains: By SFT, Plan2Evolve produces a planning-enhanced LLM that achieves robust planning performance, stronger cross-task generalization, and reduced inference token costs. These results indicate that Plan2Evolve can improve both the efficiency and planning capacity of LLM.",
            "llm_summary": "【论文的motivation是什么】  \n1. LLM在机器人任务规划中的潜力尚未充分发挥，尤其是在生成可扩展的推理数据方面。  \n2. 现有的LLM训练依赖于昂贵的人为策划数据集，限制了其在机器人领域的应用。  \n3. 需要一种方法将符号规划与自然语言推理对齐，以增强LLM的规划能力。  \n\n【提出了什么创新的方法】  \n提出了Plan2Evolve，一个自我演化框架，利用LLM生成的规划领域作为知识源，自动生成符号问题-计划对。该模型首先自生成规划领域，然后将这些符号计划转化为自然语言的长链推理轨迹，从而将符号规划结构与自然语言推理明确对齐。通过这种方式，生成的数据超出了模型的内在规划能力，支持了模型的监督微调，显著提升了规划成功率、跨任务泛化能力，并降低了推理成本。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "DroneFL: Federated Learning for Multi-UAV Visual Target Tracking",
            "authors": "Xiaofan Yu,Yuwei Wu,Katherine Mao,Ye Tian,Vijay Kumar,Tajana Rosing",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21523",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21523",
            "arxiv_html_link": "https://arxiv.org/html/2509.21523v1",
            "abstract": "Multi-robot target tracking is a fundamental problem that requires coordinated monitoring of dynamic entities in applications such as precision agriculture, environmental monitoring, disaster response, and security surveillance.\nWhile Federated Learning (FL) has the potential to enhance learning across multiple robots without centralized data aggregation, its use in multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely underexplored.\nKey challenges include limited onboard computational resources, significant data heterogeneity in FL due to varying targets and the fields of view, and the need for tight coupling between trajectory prediction and multi-robot planning.\nIn this paper, we introduce DroneFL, the first federated learning framework specifically designed for efficient multi-UAV target tracking.\nWe design a lightweight local model to predict target trajectories from sensor inputs, using a frozen YOLO backbone and a shallow transformer for efficient onboard training.\nThe updated models are periodically aggregated in the cloud for global knowledge sharing.\nTo alleviate the data heterogeneity that hinders FL convergence, DroneFL introduces a position-invariant model architecture with altitude-based adaptive instance normalization.\nFinally, we fuse predictions from multiple UAVs in the cloud and generate optimal trajectories that balance target prediction accuracy and overall tracking performance.\nOur results show that DroneFL reduces prediction error by 6%-83% and tracking distance by 0.4%-4.6% compared to a distributed non-FL framework. In terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has on average just 1.56 KBps data rate to the cloud.",
            "introduction": "Multi-robot systems are increasingly deployed in real-world applications, with aerial platforms playing a distinct role in large-scale environments such as industry, agriculture, and environmental monitoring [1, 2].\nTeams of UAVs provide wide-area coverage, maintaining visibility even in cluttered, GPS-denied settings like orchards, while adapting quickly to changing operational demands.\nIn agriculture, for example, UAVs enable large-scale monitoring by tracking ground vehicles during operations such as harvesting.\nUnlike static sensors, these targets have dynamic motions influenced by heterogeneous machinery, variable field conditions, and operator-dependent behaviors.\nTherefore, accurate monitoring requires coordinated efforts across multiple UAVs.\n\nIn this paper, we consider multi-robot target tracking as a representative scenario (Fig. 1).\nMost existing research [3, 4, 5, 6, 7] relies on low-dimensional sensors and fixed models or heuristics, which fail to handle high-dimensional inputs such as images and video streams, or to adapt to the variability of real-world environments.\nIn addition, UAVs face strict Size, Weight, and Power (SWaP) constraints that limit both onboard computation and communication. These challenges motivate the need for a multi-robot framework that processes rich visual inputs, delivers high performance (e.g., prediction accuracy and tracking quality) across varying environments, while minimizing resource consumption.\n\nFederated learning (FL) [8] provides a promising solution by allowing distributed UAVs to collaboratively learn a vision-based neural network (NN) model.\nEach FL round begins with a central server (cloud) distributing a global model to the robots. The robots then update their NNs using locally collected sensor data to better predict target trajectories. Finally, the updated models are sent back to the cloud for aggregation, enabling generalization across multiple robots.\nDuring inference, each robot sends its target trajectory predictions to the central server. The server fuses these predictions to form a global understanding of target movements and generate future trajectories for the UAV team.\nCompared to fixed models or heuristics, FL allows on-device adaptation to environmental variations, leading to better long-term performance. Unlike centralized approaches that send all raw data to the cloud, FL supports real-time inference and reduces communication overhead by keeping high-dimensional data local.\nAlthough deploying and training NN models on edge devices was a major challenge in the past, recent advances in efficient algorithms and more capable hardware have made it increasingly feasible.\nFor instance, the YOLOv11n model can process 640×640640\\times 640 images on a Raspberry Pi 5 in around 100 ms per image [9].\nOverall, an FL-enabled multi-robot target tracking framework offers the potential to improve scalability, robustness, and communication efficiency compared to traditional approaches.\n\nWhile FL is promising, several key challenges remain in designing a practical FL framework for multi-UAV target tracking. These challenges include:\n\nLimited device resources: Despite recent advances, deploying NNs on UAVs remains challenging due to their high resource demands for training and inference [10]. Wireless communication with the cloud also consumes significant energy [11].\nThe FL framework should minimize resource consumption and extend the endurance of SWaP-constrained UAVs.\n\nData heterogeneity in FL: Data heterogeneity, namely the varying distributions of data collected by distributed devices, is a long-standing challenge in FL [12, 13, 14].\nIn visual-based target tracking, data heterogeneity may result from variations in monitored targets or differences in the robots’ fields of view.\nIf not properly addressed, data heterogeneity can hinder learning convergence in FL and degrade trajectory prediction performance.\n\nRobust prediction-action coupling:\nMulti-UAV tracking requires integrating trajectory prediction with trajectory planning: accurate predictions are critical for generating effective maneuvers, while the chosen actions in turn influence future observations, making this closed-loop coupling essential for long-term performance.\n\nIn this paper, we propose DroneFL, an end-to-end federated learning framework for multi-robot target tracking using vision-based perception. DroneFL addresses the above challenges through three key designs spanning target trajectory prediction and drone trajectory planning. First, we introduce a lightweight neural network architecture to operate within SWaP constraints. Each robot uses a frozen YOLO model [9] to extract bounding box detections from images. The bounding boxes and recent odometry data are then fed into a shallow transformer model [20] that predicts future target trajectories. During FL, only the transformer layers are updated on the drone, minimizing onboard latency and energy consumption. Onboard storage is also reduced by storing only bounding boxes instead of raw image data. Second, we observe that FL convergence is mainly hindered by data heterogeneity caused by flight altitudes, which produce different field-of-view patterns. To address this, we propose an altitude-based adaptive instance normalization design in the transformer model, making it position-invariant and easier to train across drones with varying views.\nFinally, we develop a centralized trajectory planning module that fuses predictions from multiple robots using an Extended Kalman Filter with adaptive innovation, and generates optimal trajectories by minimizing a custom cost function that balances tracking robustness with perception quality.\nDroneFL is extensively evaluated in a simulated testbed, using an agricultural setting as a case study. DroneFL is further validated for efficiency on a Raspberry Pi 5, a representative computing unit for resource-constrained UAVs.\n\nIn summary, the contributions of this paper are:\n\nDroneFL, the first FL framework for multi-UAV target tracking, which tightly couples closed-loop trajectory prediction and trajectory planning through efficient and effective FL under data heterogeneity\n\nComprehensive evaluations showing that DroneFL reduces prediction error by 6%-83% and tracking distance by 0.4%-4.6 % compared to distributed non-FL framework. DroneFL achieves real-time prediction on a representative computing unit for UAVs, with only 1.561.56 KB/s communication with the cloud.\n\n1. DroneFL, the first FL framework for multi-UAV target tracking, which tightly couples closed-loop trajectory prediction and trajectory planning through efficient and effective FL under data heterogeneity\n\n2. Comprehensive evaluations showing that DroneFL reduces prediction error by 6%-83% and tracking distance by 0.4%-4.6 % compared to distributed non-FL framework. DroneFL achieves real-time prediction on a representative computing unit for UAVs, with only 1.561.56 KB/s communication with the cloud.",
            "llm_summary": "【论文的motivation是什么】  \n1. 多机器人目标追踪需要协调监控动态实体，现有方法无法处理高维输入和环境变化。  \n2. 联邦学习在多无人机目标追踪中的应用尚未得到充分探索，面临计算资源和数据异质性挑战。  \n3. UAV在执行任务时受限于尺寸、重量和功耗（SWaP），需要高效的框架来处理视觉输入。  \n\n【提出了什么创新的方法】  \n本文提出了DroneFL，一个专为多无人机目标追踪设计的联邦学习框架。该框架采用轻量级本地模型，通过冻结YOLO骨干网络和浅层变换器进行高效的目标轨迹预测。为解决数据异质性问题，DroneFL引入了基于高度的自适应实例归一化，使模型架构具有位置不变性。最终，多个无人机的预测结果在云端融合，生成最优轨迹，平衡预测准确性和整体追踪性能。实验结果表明，DroneFL在预测误差上减少了6%-83%，在追踪距离上减少了0.4%-4.6%，并在Raspberry Pi 5上实现实时运行，平均仅需1.56 KB/s的数据传输率。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Developing a Mono-Actuated Compliant GeoGami Robot",
            "authors": "Archie Webster,Lee Skull,Seyed Amir Tafrishi",
            "subjects": "Robotics (cs.RO)",
            "comment": "under-review",
            "pdf_link": "https://arxiv.org/pdf/2509.21445",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21445",
            "arxiv_html_link": "https://arxiv.org/html/2509.21445v1",
            "abstract": "This paper presents the design of a new soft-rigid robotic platform, ”GeoGami”. We leverage origami surface capabilities to achieve shape contraction and to support locomotion with underactuated forms. A key challenge is that origami surfaces have high degrees of freedom and typically require many actuators; we address repeatability by integrating surface compliance. We propose a mono-actuated GeoGami mobile platform that combines origami surface compliance with a geometric compliant skeleton, enabling the robot to transform and locomote using a single actuator. We demonstrate the robot, develop a stiffness model, and describe the central gearbox mechanism. We also analyze alternative cable-driven actuation methods for the skeleton to enable surface transformation. Finally, we evaluate the GeoGami platform for capabilities, including shape transformation and rolling. This platform opens new capabilities for robots that change shape to access different environments and that use shape transformation for locomotion.",
            "introduction": "Compliant origami offers a rich geometric design space where complex 3D behaviors emerge from simple, repeatable folds of a planar sheet [1, 2]. Translating these capabilities into robots that achieve shape change or locomotion with minimal actuation is challenging: underactuated designs must exploit structural physics—distributed stiffness, mass and inertia, and geometry-induced constraints—to route energy and motion effectively. This motivates co-design of crease patterns and compliance so the desired transformations can be achieved and controlled with as little actuation as possible.\n\nCompliance and softness are crucial for safer, more adaptable mechanisms [2] and robotic systems [3], with clear advantages in constrained or hazardous settings. In power-plant inspection and maintenance, where intrusive methods are costly and awkward [4, 5], a robot that can morph to the environment like a soft system yet endure harsh conditions like a rigid platform enables in-situ traversal and stable, on-site operations [6]. Recent reviews emphasize that compliance must be co-designed with geometry, materials, and control to achieve predictable interaction—spanning modeling frameworks for soft robots [7], compliant mechanisms for contact-rich tasks [8], and system-level overviews of soft robotics [9]. In parallel, origami-based robots illustrate how compliant folding structures integrate sensing/decision/actuation for robust interaction [2, 10], and surveys document diverse approaches for crease-pattern design and controllable deployment [11] or even using geometric one-print compliant models [12]. In practice, patterns such as Miura-ori and waterbomb combined with compliant hinges can deliver large, coordinated shape change with minimal actuation, while tendon-, pneumatic-, or SMA-based actuation governs deployment and force transmission. Nevertheless, striking the right balance between rigidity (load-bearing, durability), softness (safe contact, adaptability), and the repeatable “returnability” of origami folds remains non-trivial in practice.\n\nUnderactuated robotics, systems with fewer independent actuators than degrees of freedom, offers advantages in energy efficiency, material use, and compact form factors [13, 14, 15]. When paired with careful morphology and control, such designs can be exceptionally effective in tasks like locomotion [16] and shape transformation in constrained environments [17]. Because fewer actuators reduce mass, volume, and power draw, underactuated platforms are attractive for field deployment [13]. This mirrors biological strategies, where passive dynamics and compliant couplings distribute motion and forces efficiently. The design goal, therefore, is to use the minimum number of actuators while exploiting mechanical couplings, tendon routing, and structural compliance to modulate stiffness and geometry for navigation and manipulation—an idea exemplified by adaptive-synergy hands coordinating many joints with a single input [18]. Nevertheless, integrating geometric compliance (e.g., origami-inspired structures) with underactuation to achieve reversible shape change and size modulation using very few actuators remains an open challenge.\n\nWe address the limited study of actuation strategies for compliant origami surfaces, particularly when the platform must realize multiple shape transformations in addition to rolling. We introduce novel GeoGami as shown in Fig. 1, a mono-actuated robotic platform that integrates a modular geometric compliant skeleton with an origami surface; both structures provide programmable compliance and are manipulated by a centralized cable-driven gearbox. The paper develops and validates a stiffness model for the origami patches and the geometric skeleton, quantifies returnability, and presents an analytical center-of-mass formulation that maps cable retraction through the transmission to mass imbalance and locomotion. We further characterize the transmission and cable cyclic routing for single-actuator operation, and evaluate the resulting motion capabilities of the platform, including size compaction, controllable contact, and rolling.\n\nThe remainder of the paper is organized as follows. Section II introduces the GeoGami hardware, including the compliant skeleton, origami surface, joint measurement protocol, and the mono-actuated cyclic gearbox. Section III develops the planar kinematics, the stiffness aggregation from joint to side, and the center of mass model that links cable retraction to motion. Section IV presents experiments and motion behavior analysis; Section V concludes findings and outlines future work.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的origami表面设计通常需要多个驱动器，难以实现形状变化和运动。  \n2. 需要通过结构物理学来有效路由能量和运动，以实现最小化驱动器的设计目标。  \n3. 在受限或危险环境中，柔性和适应性机制的需求日益增加。  \n\n【提出了什么创新的方法】  \n本文提出了一种新的单驱动GeoGami机器人平台，结合了可编程的origami表面和几何合规骨架。该平台通过中央电缆驱动齿轮箱实现形状变化和运动，利用单个驱动器来控制多种形状变换和滚动能力。通过开发和验证刚度模型，量化可返回性，并提出了一个分析的质心公式，研究了电缆收缩与运动之间的关系。实验结果表明，该平台能够有效实现尺寸压缩、可控接触和滚动等运动能力，展示了其在复杂环境中的应用潜力。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Language-in-the-Loop Culvert Inspection on the Erie Canal",
            "authors": "Yashom Dighe,Yash Turkar,Karthik Dantu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "First two authors contributed equally",
            "pdf_link": "https://arxiv.org/pdf/2509.21370",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21370",
            "arxiv_html_link": "https://arxiv.org/html/2509.21370v1",
            "abstract": "Culverts on canals such as Erie Canal built originally in 1825 require frequent inspections to ensure safe operation.\nHuman inspection of culverts is challenging due to age, geometry, poor illumination, weather and lack of easy access.\nWe introduce VISION, an end-to-end, language-in-the-loop autonomy system that couples a web-scale vision–language model (VLM) with constrained viewpoint planning for autonomous inspection of culverts. Brief prompts to the VLM solicit open-vocabulary ROI proposals with rationales and confidences, stereo depth is fused to recover scale, and a planner—aware of culvert constraints commands repositioning moves to capture targeted close-ups. Deployed on a quadruped in Culvert under the Erie canal, VISION closes the see→decide→move→re-image loop on-board and produces high-resolution images for detailed reporting without domain-specific fine-tuning. In an external evaluation by New York Canal Corporation personnel, initial ROI proposals achieved 61.4% agreement with subject-matter experts, and final post-re-imaging assessments reached 80%, indicating that VISION converts tentative hypotheses into grounded, expert-aligned findings.",
            "introduction": "Canals are artificial waterways built for drainage management and water transport. Largely built in the 19th and 20th century, there are over 4000 miles of canals in the US [1]. Erie canal is one such canal that is 363 miles in length and runs east-west between the Hudson river and Lake Erie. It was initially built in 1825 with several expansions including a major one between 1905 and 1918. Culverts are small cylindrical passages under the canal typically built for draining water after wet spells. The erie canal has over 350 culverts. Most of them were built using concrete, metal or stone. However, over time, these culverts are subject to structural deterioration, as reflected in New York Canal Corporation (NYCC) inspection records. Typical defects include surface corrosion, spalling, and seepage, any of which can escalate to culvert failure with serious consequences for adjacent communities and ecosystems. Currently, these culverts are inspected manually, requiring personnel to enter confined underground spaces in remote locations. Such inspections pose significant risks: the culverts lie beneath the canal in restricted, poorly ventilated environments, where visibility and air quality are uncertain, and structural stability cannot be guaranteed. Reaching these sites is often difficult due to steep slopes, uneven terrain, and waterlogged conditions, all of which increase the likelihood of accidents. These hazards make manual inspection both dangerous and impractical, motivating the development of automated alternatives. Legged robots offer a promising alternative [2] as they can descend embankments, wade through shallow water, and carry multi-modal sensors for close-range imaging.\n\nHowever, despite the ability of legged robots to traverse such constrained environments, culvert inspection remains a challenging task due to unreliable defect detection. The problem is exacerbated by long-tailed, site-specific degradations and highly inconsistent illumination conditions, which limit the effectiveness of closed-set detectors such as  [3, 4]. On the other hand, open vocabulary based methods perform poorly due to out of domain inputs.\nFigure 2 shows the output of three state-of-the-art open-vocabulary baselines (Lang-SAM [5], Grounding DINO [6], and Grounding SAM [7]) for three commonly used terms in inspection reports: rust, ice, and scaling. Grounding DINO yields coarse boxes; Grounding SAM converts those boxes to masks but spreads labels broadly. Further, fine-tuning existing models for this task is infeasible due to the lack of domain-specific data. The limited number of available human inspection reports are often qualitative and expressed in unstructured free text (see Fig. 1), lacking the consistency needed to support fine-tuning.\nBeyond visual detection, inspection methods that rely on 3D or photogrammetric reconstruction are also hindered by the same environmental factors. Poor and inconsistent illumination combined with a feature-sparse, confined geometry renders such reconstructions particularly unreliable. In these conditions, reconstruction algorithms often fail to capture micro-scale structural details, leading to smoothing over real faults like fine cracks. Prevailing challenges in photogrammetric and multi-view reconstruction include low-texture regions and uneven lighting, which systematically degrade geometric fidelity [8]. Moreover, quantitative studies confirm that insufficient surface texture introduces reconstruction noise and reduces accuracy, especially in fine-detail retention [9]. In short, while robots can access these environments, perception and defect detection under challenging visual conditions are one of the major challenges for autonomous culvert inspection.\n\nLarge Vision–Language Models (VLMs) offer a promising alternative in this context. They have shown remarkable adaptability to vague or underspecified prompts, often outperforming hand-crafted baselines in zero- or few-shot settings [10]. Prompt engineering has been demonstrated to effectively steer VLMs across a wide range of tasks without fine-tuning, enabling robust generalization from loosely phrased instructions [11]. Moreover, combining textual and visual prompting can compensate for the ambiguity or sparsity in any single modality, further enhancing performance in under-defined scenarios such as out-of-distribution segmentation [12]. When inspecting a culvert, a human does not carefully analyze every surface in detail from the start; instead, they take a quick glance to judge whether something looks unusual, and only then move closer to examine those regions more carefully. VLMs naturally mirror this behavior: they can be prompted with broad, loosely phrased queries to quickly highlight potentially relevant regions, and then refined with more specific prompts or visual cues to focus attention where it is most needed.\n\nBuilding on this analogy, we present VISION (Visual Inspection System with Intelligent Observation and Navigation), a language-in-the-loop inspection stack that runs onboard the robot. VISION couples vision–language reasoning with constrained viewpoint planning to enable autonomous inspection in culverts.\nGiven a general prompt from an inspector and an image from a forward-facing camera, the VLM proposes candidate defect regions with bounding boxes, confidences, and natural-language rationales.\nThese proposals drive a planner that accounts for culvert geometry and the robot’s limited mobility, commanding short forward motions and pan–tilt adjustments of a second camera to capture close-range, high-resolution imagery suitable for downstream measurement and audit.\nOur contributions in the work are as follows:\n\nWe propose an end-to-end framework for fully autonomous culvert inspection\n\nWe demonstrate a methodology to leverage web-scale vision-language models zero-shot for an abstract inspection task\n\nWe deploy our system onboard a legged robot with a pan-tilt gimbal, and validate its effectiveness through field experiments in an actual culvert.\n\n1. We propose an end-to-end framework for fully autonomous culvert inspection\n\n2. We demonstrate a methodology to leverage web-scale vision-language models zero-shot for an abstract inspection task\n\n3. We deploy our system onboard a legged robot with a pan-tilt gimbal, and validate its effectiveness through field experiments in an actual culvert.",
            "llm_summary": "【论文的motivation是什么】  \n1. 人工检查culverts面临安全风险和环境挑战，难以进行有效的检测。  \n2. 现有的视觉检测方法在复杂环境中表现不佳，缺乏足够的领域特定数据进行微调。  \n3. 需要一种自动化的解决方案来提高culvert检查的效率和安全性。  \n\n【提出了什么创新的方法】  \n提出了VISION，一个结合了视觉-语言推理和受限视角规划的端到端自动化检查系统。该系统通过简要提示向视觉-语言模型（VLM）请求开放词汇的缺陷区域建议，并融合立体深度信息以恢复尺度。规划器根据culvert的几何特征指挥机器人进行短距离移动和摄像头的俯仰调整，从而捕获适合后续测量和审计的高分辨率图像。在实际的culvert检查中，VISION实现了61.4%的初步ROI提案与专家一致性，最终的后重成像评估达到了80%，有效将初步假设转化为与专家一致的发现。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation",
            "authors": "Shuang Zeng,Dekang Qi,Xinyuan Chang,Feng Xiong,Shichao Xie,Xiaolong Wu,Shiyi Liang,Mu Xu,Xing Wei",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.22548",
            "code": "https://miv-xjtu.github.io/JanusVLN.github.io/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22548",
            "arxiv_html_link": "https://arxiv.org/html/2509.22548v1",
            "abstract": "Vision-and-Language Navigation (VLN) requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream.\nRecent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models (MLLMs). However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation.\nInspired by the implicit scene representation in human navigation, analogous to the left brain’s semantic understanding and the right brain’s spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations.\nThis framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input.\nThen, the historical key-value (KV) caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates.\nExtensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data.\nThis indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.",
            "introduction": "Vision-and-Language Navigation (VLN) is a foundational task in embodied AI, requiring an agent to navigate through unseen environments guided by visual inputs and natural language instructions. Recently, capitalizing on the advanced visual perception and semantic understanding capabilities of Multimodal Large Language Models (MLLMs), a new line of research (Zhang et al., 2025b; Wei et al., 2025a) has emerged. These approaches leverage vast-scale training data to adapt MLLMs into VLN models, thereby reshaping the future landscape of VLN research.\n\nTo support navigation models in conducting prolonged and effective exploration, these approaches typically only construct an explicit semantic memory. One class of methods (Zhang et al., 2025c; Zeng et al., 2024) builds a semantic cognitive map using textual descriptions for object nodes and relational edges. However, purely textual descriptions struggle to precisely convey the spatial relationships and orientation of objects, leading to the loss of crucial visual, spatial-geometric, and contextual information. Moreover, repetitive descriptions introduce substantial redundancy and noise. Another class of methods (Zhang et al., 2025b; Wang et al., 2024b) stores historical observation frames, which necessitates reprocessing the entire history of observations along with the current frame at each action prediction step, resulting in significant redundant computation. Finally, in both types of approaches, the explicit semantic memory grows exponentially as navigation time increases. This makes it exceedingly difficult for the model to extract critical information from a vast, cluttered, and fragmented memory, thereby leading to severe inefficiency.\n\nMore importantly, these methods collectively face a fundamental contradiction. Navigation is an inherently 3D physical interaction, yet the visual encoders of existing VLA models almost exclusively inherit the CLIP paradigm pre-trained on 2D image-text pairs. This approach enables these encoders to excel at capturing high-level semantics while leaving them deficient in understanding 3D geometric structures and spatial information. However, a frequently overlooked yet critical insight is that 2D images are not merely isolated planes of pixels but are projections of the 3D physical world, inherently containing a wealth of 3D spatial cues such as perspective, occlusion, and geometric structures. Whereas human observers can effortlessly perceive depth and comprehend spatial layouts from a single static image, existing models neglect this readily available implicit 3D information in their inputs. This oversight fundamentally constrains their spatial reasoning capabilities in complex navigation tasks.\n\nInspired by the human brain’s hemispheric specialization for navigation, where the left hemisphere handles semantic understanding and the right manages 3D spatial cognition to form implicit representations (Gazzaniga, 1967), we propose a fundamental shift from a single, explicit memory to a dual, implicit neural memory. To this end, we introduce JanusVLN, a dual implicit memory framework for VLN that features both spatial-geometric and visual-semantic memory in Figure 1. We model these two types of memory respectively as fixed-size, compact neural memory, whose size does not grow with the trajectory length. This design is analogous to the human brain’s ability to perform efficient memorization within a finite capacity.\n\nTo construct this dual implicit memory, we extend the MLLM into a novel VLN model by incorporating a feed-forward 3D visual geometry foundation model, which provides 3D spatial geometric structural information solely from RGB video input, obviating the need for any explicit 3D data. Unlike the visual encoders of general MLLMs, which are predominantly trained on 2D image-text data, this spatial geometry model is typically trained on pixel-3D point cloud pairs, thereby embedding strong 3D perception priors. We establish implicit spatial-geometric and visual-semantic memory by caching historical key-value (KV) from a 3D spatial geometry encoder and MLLM’s semantic visual encoder, respectively. These dual implicit memory are dynamically and incrementally updated through the initial and sliding window, enabling the progressive integration of historical information for each new frame without recomputing past frames. Extensive experiments demonstrate that JanusVLN significantly enhances spatial comprehension while lowering inference overhead, achieving SOTA performance on VLN-CE benchmarks. It establishes a new paradigm for VLN research, propelling a shift from being 2D semantics-dominated to 3D spatial-semantic synergy. This marks a pivotal direction toward building the next generation of spatially-aware embodied agents.\n\nIn summary, our contributions are as follows:\n\nWe introduce a novel dual implicit memory paradigm for VLN. Inspired by human cognitive science, this framework simultaneously captures visual semantics and spatial geometry to overcome the inherent limitations of existing navigation LLM.\n\nWe introduce a novel dual implicit memory paradigm for VLN. Inspired by human cognitive science, this framework simultaneously captures visual semantics and spatial geometry to overcome the inherent limitations of existing navigation LLM.\n\nOur proposed JanusVLN incrementally updates its dual implicit memory with cached KV from initial and sliding window, which eliminates the need to recompute historical frames for each new frame, substantially lowering computational and inference overhead.\n\nComprehensive experiments on the VLN-CE benchmark demonstrate that JanusVLN achieves SOTA results without requiring auxiliary 3D data. This validates the efficacy of JanusVLN and establishes a new memory paradigm for the field of VLN.\n\n1. We introduce a novel dual implicit memory paradigm for VLN. Inspired by human cognitive science, this framework simultaneously captures visual semantics and spatial geometry to overcome the inherent limitations of existing navigation LLM.\n\n2. Our proposed JanusVLN incrementally updates its dual implicit memory with cached KV from initial and sliding window, which eliminates the need to recompute historical frames for each new frame, substantially lowering computational and inference overhead.\n\n3. Comprehensive experiments on the VLN-CE benchmark demonstrate that JanusVLN achieves SOTA results without requiring auxiliary 3D data. This validates the efficacy of JanusVLN and establishes a new memory paradigm for the field of VLN.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的VLN方法依赖显式语义记忆，导致空间信息丢失和计算冗余。  \n2. 现有模型缺乏对3D几何结构和空间信息的理解，限制了空间推理能力。  \n3. 需要一种新的记忆机制来有效整合视觉语义和空间几何信息。  \n\n【提出了什么创新的方法】  \n本研究提出了JanusVLN，一个具有双隐式神经记忆的VLN框架，分别建模空间几何和视觉语义记忆。该框架通过引入3D空间几何编码器，增强了模型的空间推理能力，并通过初始和滑动窗口动态更新历史键值缓存，避免了冗余计算。实验结果表明，JanusVLN在VLN-CE基准上显著提升了成功率，超越了20种近期方法，展示了其在空间理解和计算效率上的优势。这一创新方法为未来的VLN研究开辟了新的方向。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Learning to Ball: Composing Policies for Long-Horizon Basketball Moves",
            "authors": "Pei Xu,Zhen Wu,Ruocheng Wang,Vishnu Sarukkai,Kayvon Fatahalian,Ioannis Karamouzas,Victor Zordan,C. Karen Liu",
            "subjects": "Graphics (cs.GR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2025). Website:this http URL. Video:this https URL. Code:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.22442",
            "code": "https://youtu.be/2RBFIjjmR2I, http://pei-xu.github.io/basketball, https://github.com/xupei0610/basketball",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22442",
            "arxiv_html_link": "https://arxiv.org/html/2509.22442v1",
            "abstract": "Learning a control policy for a multi-phase, long-horizon task, such as basketball maneuvers, remains challenging for reinforcement learning approaches due to the need for seamless policy composition and transitions between skills. A long-horizon task typically consists of distinct subtasks with well-defined goals, separated by transitional subtasks with unclear goals but critical to the success of the entire task. Existing methods like the mixture of experts and skill chaining struggle with tasks where individual policies do not share significant commonly explored states or lack well-defined initial and terminal states between different phases.\nIn this paper, we introduce a novel policy integration framework to enable the composition of drastically different\nmotor skills in multi-phase long-horizon tasks with ill-defined intermediate states. Based on that, we further introduce a high-level soft router to enable seamless and robust transitions between the subtasks.\nWe evaluate our framework on a set of fundamental basketball skills and challenging transitions.\nPolicies trained by our approach can effectively control the simulated character to interact with the ball and accomplish the long-horizon task specified by real-time user commands, without relying on ball trajectory references.",
            "introduction": "Many real-world tasks consist of complex objectives that can be broken down into sequences of differing subtasks. Successfully executing these multi-phase, long-horizon tasks demands the mastery of heterogeneous skills and the ability to transition seamlessly between them. Basketball provides a compelling example of these challenges. For example, a fundamental maneuver, “shoot-off-the-dribble”, involves distinct subtasks such as dribbling, gathering the ball and shooting, as well as the ability to transition between these skills, ultimately culminating in the ball successfully going into the hoop. However, while dribble and shoot are characterized by well-defined stand-alone goals, gather acts largely as a transition subtask with poorly defined starting and ending states. Thus, executing such multi-phase tasks challenges control methods proposed to date.\n\nReinforcement learning (RL) has shown promise in training policies for individual skills for physics-based character control (peng2018deepmimic; shi2023hci; chentanez2018physics; yin2021discovering; kwiatkowski2022survey),\nbut composing these policies into a cohesive framework remains an open problem. Previous approaches have attempted to address this through methods like a mixture of experts (scadiver2; peng2018sfv). However, this technique relies on sufficiently exploring shared states across individual policies—a condition that does not hold for tasks like dribbling and shooting. Another line of work known as skill chaining (konidaris2009skill; lee2021adversarial; chen2023sequential; liu2017learning; clegg2018learning) allows concatenation of policies but requires each skill to have a well-defined set of terminal states. This limitation renders it ineffective for intermediate tasks where the subtask’s goal depends on the context of the subsequent policy. For example, the gathering motion between dribbling and shooting can be intuitively described as “bringing the agent to a state where shooting is possible.” However, crafting a reward function based solely on state or action variables to reflect this goal is challenging.\n\nTo tackle the problem of building policies for ill-defined, intermediate subtasks, we introduce a policy integration method to compose drastically different and/or ill-defined skills to achieve a multi-phase, long-horizon task. The core idea is to first train policies for well-defined subtasks independently and then use these policies to guide the training of the ambiguous, intermediate subtasks. Specifically, for a task sequence consisting of subtasks A, B, and C, where A and C have well-defined task goals but B does not, we train B using policy A to define a valid initial state distribution and policy C’s state value function to shape the terminal reward. To further improve the transition between B and C, we simultaneously adapt the pretrained policy C to the states generated by B under training. In this process, a state value estimator optimized in tandem with the adapted C will be provided to reflect the up-to-date state value evaluation for policy B optimization.\nWith the primitive policies for all subtasks in place, we finally train a high-level soft-routing policy that directs the execution of those primitive policies based on real-time external commands, such as dribbling destination\nand velocity, or a jump-shot action.\n\nAnother challenging aspect in learning policies for multi-phase, long-horizon tasks is the heterogeneity of movement that demands diverse and extensive human motion data. Previous work on basketball motion synthesis has demonstrated compelling results when using structured data with corresponding full body, fingers, and basketball movements for physics-based character control (liu2018learning; wang2024skillmimic; wang2023physhoi; starke2020local). However, such a special dataset is hard to scale for training a general policy capable of performing under a wide array of conditions.\nIn our work, instead, we demonstrate the generation of policies from unstructured data. We leverage a diverse collection of basketball motion data, including full-body motions without hands, and hand-only motions, as well as motion examples from unstructured videos.\nTo enrich locomotion behaviors, some normal running motions are also included.\nOur method makes no assumptions about the correspondence across datasets or availability of ball trajectories.\n\nOur results show that the proposed method enables the agent to perform smooth and coordinated basketball maneuvers, from gross body movements to fine finger actions, while responding adaptively to user commands. The agent can freely play basketball in real-time—for instance, dribbling to any location at variable speeds and finishing with a jump shot from any direction, achieving a shooting accuracy of 91.8% on a professional court. We further demonstrate team play with multiple agents interacting through catching, passing, rebounding, and defending. Extensive ablation studies validate key design choices, such as soft routing and policy fine-tuning, and expose the limitations of existing methods in handling ambiguous subtasks. By addressing skill integration and phase transitions in long-horizon tasks, our approach advances the capabilities of reinforcement learning in dynamic, interactive environments.",
            "llm_summary": "【论文的motivation是什么】  \n1. 多阶段、长时间任务的控制策略学习面临挑战，尤其是技能之间的无缝过渡。  \n2. 现有方法在处理缺乏明确初始和终止状态的任务时效果不佳。  \n3. 需要有效整合不同的运动技能以完成复杂的任务。  \n\n【提出了什么创新的方法】  \n本文提出了一种新颖的政策集成框架，旨在实现多阶段长时间任务中不同运动技能的组合。首先，独立训练具有明确目标的子任务政策，然后利用这些政策指导模糊中间子任务的训练。通过结合高层软路由策略，确保子任务之间的顺畅过渡。最终，训练出的政策能够在没有球轨迹参考的情况下，实时响应用户命令，成功执行篮球技能，展示出91.8%的投篮准确率，并实现多代理的团队配合。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer",
            "authors": "Zhehao Dong,Xiaofeng Wang,Zheng Zhu,Yirui Wang,Yang Wang,Yukun Zhou,Boyuan Wang,Chaojun Ni,Runqi Ouyang,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang",
            "subjects": "Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22407",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22407",
            "arxiv_html_link": "https://arxiv.org/html/2509.22407v1",
            "abstract": "Vision–language–action (VLA) models increasingly rely on diverse training data to achieve robust generalization. However, collecting large-scale real-world robot manipulation data across varied object appearances and environmental conditions remains prohibitively time-consuming and expensive.\nTo overcome this bottleneck, we propose Embodied Manipulation Media Adaptation (EMMA), a VLA policy enhancement framework that integrates a generative data engine with an effective training pipeline. We introduce DreamTransfer, a diffusion Transformer-based framework for generating multi-view consistent, geometrically grounded embodied manipulation videos. DreamTransfer enables text-controlled visual editing of robot videos, transforming foreground, background, and lighting conditions without compromising 3D structure or geometrical plausibility. Furthermore, we explore hybrid training with real and generated data, and introduce AdaMix, a hard-sample-aware training strategy that dynamically reweights training batches to focus optimization on perceptually or kinematically challenging samples.\nExtensive experiments show that videos generated by DreamTransfer significantly outperform prior video generation methods in multi-view consistency, geometric fidelity, and text-conditioning accuracy. Crucially, VLAs trained with generated data enable robots to generalize to unseen object categories and novel visual domains using only demonstrations from a single appearance.\nIn real-world robotic manipulation tasks with zero-shot visual domains, our approach achieves over a 200% relative performance gain compared to training on real data alone, and further improves by 13% with AdaMix, demonstrating its effectiveness in boosting policy generalization.",
            "introduction": "Vision–language–action (VLA) models have demonstrated remarkable capabilities in enabling robots to perform complex manipulation tasks from natural language instructions and visual inputs (Black et al., 2024; Intelligence et al., 2025; Brohan et al., 2023; Kim et al., 2024; NVIDIA et al., 2025c; Deng et al., 2025). However, their success critically depends on large-scale, diverse training data.\nCollecting real-world robot manipulation data through human teleoperation is labor-intensive and expensive, severely limiting the scale and visual diversity of available datasets.\nWhile simulation offers a scalable alternative for generating annotated trajectories (Geng et al., 2025; Lin et al., 2024; Mu et al., 2025; Katara et al., 2023; Lin et al., 2024), simulated environments often suffer from visual realism gaps and are constrained by limited asset diversity. As a result, policies trained on simulated data frequently underperform when deployed in the real world.\n\nRecently, diffusion models (Wan et al., 2025; Kong et al., 2025; NVIDIA et al., 2025a; Yang et al., 2025; Zheng et al., 2024) have emerged as a promising method for generating realistic and diverse visual video. Several works have explored using diffusion models to generate vision-action data for policy training. Cosmos-Transfer1 (NVIDIA et al., 2025b) generates videos conditioned on semantic segmentation and depth, improving realism for sim-to-real transfer. RoboEngine (Yuan et al., 2025a) provides a flexible toolkit for generating diverse robot interaction scenes by combining background generation with accurate robot segmentation, without requiring camera calibration. RoboTransfer (Liu et al., 2025) further improves multi-view consistency by explicitly modeling 3D geometry using depth maps and surface normals, allowing controllable edits.\n\nDespite these advances, two key challenges remain. First, most methods (NVIDIA et al., 2025b; Yuan et al., 2025a) generate videos from a single view, without ensuring consistency across viewpoints. This limits their usefulness for downstream robot tasks that rely on multi-camera inputs. RoboTransfer takes a step toward multi-view consistency, but its diversity is limited because it often transfers poorly to new domains. Second, existing works treat generated data as a static augmentation, without considering how to use it effectively during training.\n\nIn this work, we propose Embodied Manipulation Media Adaptation (EMMA), a VLA policy enhancement framework that integrates two core components: DreamTransfer and AdaMix.\nDreamTransfer is a diffusion Transformer (DiT)-based framework for generating multi-view consistent, geometrically grounded embodied manipulation videos.\nIt jointly models appearance and geometry across multiple camera views, ensuring spatial and temporal coherence. DreamTransfer supports text-controlled visual transfer: users can edit the foreground objects, background, and lighting conditions of real or simulated demonstrations through natural language, while preserving the underlying 3D structure and geometrical plausibility of the scene.\nAs illustrated in Figure 1, DreamTransfer enables realistic and controllable video generation for both real-to-real and sim-to-real transfer scenarios, making it a powerful tool for scalable robotic policy training.\nTo improve policy learning, we further propose AdaMix, a hard-sample-aware training strategy. We define a set of functions to evaluate the quality of predicted trajectories from the VLA policy and use the performance score to drive an adaptive sampling mechanism. By iteratively refining the training distribution toward challenging cases, our method improves robustness and generalization.\n\nWe evaluate on a variety of robotic manipulation tasks in both video generation quality and real-world robot deployment, including Fold Cloth, Clean Desk, and Throw Bottle. These tasks span a wide range of challenges involving both rigid and deformable objects, short-horizon and long-horizon action sequences, and diverse skills such as grasping, pushing, placing, and draping.\nCompared to the state-of-the-art transfer model, DreamTransfer improves multi-view consistency by 42% and depth consistency by 24%, demonstrating superior geometric fidelity and cross-view coherence.\nIn real-world robotic manipulation tasks involving zero-shot visual appearances, our method achieves over a 200% relative improvement in task success rate compared to training on real data alone, with an additional 13% gain when integrated with AdaMix.\n\nIn summary, our contributions are:\n\nWe propose EMMA, a VLA policy enhancement framework that integrates a generative data engine with an effective training strategy.\nThe data engine generates diverse, multi-view consistent robot manipulation videos for both rigid and deformable objects, while adaptive sample weighting improves VLA policy generalization.\n\nWe propose DreamTransfer, a DiT-based model that generates multi-view consistent, geometrically grounded manipulation videos and supports text-controlled editing of foreground, background, and lighting conditions.\nWe further introduce AdaMix, a hard-sample-aware training strategy that identifies challenging trajectories and adaptively reweights them during training.\n\nEMMA demonstrates strong performance in video generation and real-world robotic deployment. Compared to the state-of-the-art model, DreamTransfer achieves a 42% gain in multi-view consistency and a 24% gain in depth consistency, measured relatively. In zero-shot visual settings, our method achieves over a 200% performance gain compared to real-data training, with AdaMix providing an additional 13% improvement and enhancing cross-domain visual generalization.\n\n1. We propose EMMA, a VLA policy enhancement framework that integrates a generative data engine with an effective training strategy.\nThe data engine generates diverse, multi-view consistent robot manipulation videos for both rigid and deformable objects, while adaptive sample weighting improves VLA policy generalization.\n\n2. We propose DreamTransfer, a DiT-based model that generates multi-view consistent, geometrically grounded manipulation videos and supports text-controlled editing of foreground, background, and lighting conditions.\nWe further introduce AdaMix, a hard-sample-aware training strategy that identifies challenging trajectories and adaptively reweights them during training.\n\n3. EMMA demonstrates strong performance in video generation and real-world robotic deployment. Compared to the state-of-the-art model, DreamTransfer achieves a 42% gain in multi-view consistency and a 24% gain in depth consistency, measured relatively. In zero-shot visual settings, our method achieves over a 200% performance gain compared to real-data training, with AdaMix providing an additional 13% improvement and enhancing cross-domain visual generalization.",
            "llm_summary": "【论文的motivation是什么】  \n1. 收集大规模真实世界机器人操作数据既耗时又昂贵，限制了数据集的规模和视觉多样性。  \n2. 现有的生成方法在多视角一致性和视觉真实感方面存在不足，影响了机器人任务的执行。  \n3. 现有生成数据被视为静态增强，未有效利用于训练过程。  \n\n【提出了什么创新的方法】  \n提出了EMMA框架，结合了DreamTransfer和AdaMix两个核心组件。DreamTransfer是基于扩散Transformer的模型，能够生成多视角一致且几何上合理的机器人操作视频，并支持文本控制的视觉编辑。AdaMix是一种动态重加权的训练策略，专注于优化感知或运动上具有挑战性的样本。实验表明，使用DreamTransfer生成的视频在多视角一致性和几何保真度上显著优于现有方法，且在零-shot视觉领域的真实世界机器人操作任务中，相比仅使用真实数据训练，性能提升超过200%。结合AdaMix后，进一步提升了13%的性能，展示了其在政策泛化方面的有效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation",
            "authors": "Nan Tang,Jing-Cheng Pang,Guanlin Li,Chao Qian,Yang Yu",
            "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22402",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22402",
            "arxiv_html_link": "https://arxiv.org/html/2509.22402v1",
            "abstract": "Reward design remains a critical bottleneck in visual reinforcement learning (RL) for robotic manipulation. In simulated environments, rewards are conventionally designed based on the distance to a target position. However, such precise positional information is often unavailable in real-world visual settings due to sensory and perceptual limitations. In this study, we propose a method that implicitly infers spatial distances through keypoints extracted from images. Building on this, we introduce Reward Learning with Anticipation Model (ReLAM), a novel framework that automatically generates dense, structured rewards from action-free video demonstrations. ReLAM first learns an anticipation model that serves as a planner and proposes intermediate keypoint-based subgoals on the optimal path to the final goal, creating a structured learning curriculum directly aligned with the task’s geometric objectives. Based on the anticipated subgoals, a continuous reward signal is provided to train a low-level, goal-conditioned policy under the hierarchical reinforcement learning (HRL) framework with provable sub-optimality bound. Extensive experiments on complex, long-horizon manipulation tasks show that ReLAM significantly accelerates learning and achieves superior performance compared to state-of-the-art methods.",
            "introduction": "Reward design stands as one of the most fundamental challenges in reinforcement learning (RL), particularly in the domain of vision-based robotic manipulation (Tian et al.,, 2023; Lu et al.,, 2025; Escontrela et al.,, 2023; Huang et al.,, 2024; Pang et al.,, 2025). In simulated environments, a common and often effective approach is to engineer dense reward signals based on precise geometric information, such as the Euclidean distance between a robot’s end-effector and a target position. However, this paradigm faces a critical limitation in real-world applications: exact state information is typically unavailable due to sensory noise, occlusions, and perceptual ambiguities. Consequently, agents must rely on high-dimensional visual observations, making hand-engineered reward design not only labor-intensive but also notoriously challenging. This reward specification bottleneck severely impedes the scalability and adoption of RL in practical robotic settings.\n\nSome prior works overcome this limitation by adopting Learning from Observation (LfO) approaches. A common practice is to employ adversarial frameworks (Ho and Ermon,, 2016; Torabi et al.,, 2018; Kostrikov et al.,, 2019), where a discriminator that does not take action as input is trained and subsequently used as a reward function. However, when dealing with high-dimensional visual inputs, such methods suffer from significant challenges in terms of training difficulty and stability. In recent years, several works (Tian et al.,, 2023; Sontakke et al.,, 2023; Ma et al.,, 2023; Escontrela et al.,, 2023; Huang et al.,, 2024) have instead attempted to design visual rewards based on heuristic strategies. These approaches either yield sparse rewards or lack an explicit structured learning process, making them inefficient for long-horizon tasks with extended periods of partial observability or complex dynamics. Thus, there still remains a need for a framework that can automatically synthesize informative, dense reward signals from readily available video demonstrations, while guiding the agent through a structured and geometrically grounded learning curriculum.\n\nIn this work, we introduce Reward Learning with Anticipation Model (ReLAM), a framework that generates dense and structured rewards from action-free video demonstrations.\nReLAM is built on the recent insight that object keypoints can serve as a powerful intermediate representation for capturing task geometry and progression (Wen et al.,, 2024). ReLAM begins by extracting task-relevant keypoints from video demonstrations: we first use the Segment Anything Model (SAM) (Zhang et al.,, 2024) to isolate objects of interest, then apply a tracking model (Karaev et al., 2024b, ) to follow pixel-level features across frames. A sparse set of representative points is selected and propagated consistently, forming a trajectory of keypoints that encode object motion. From these, we identify keyframes that signify critical stages of the task, and define the keypoint configurations in those frames as subgoals.\nUsing this curated dataset, ReLAM learns an anticipation model capable of predicting a sequence of intermediate keypoint-based subgoals that lead to the final goal. This model acts as a high-level planner, constructing a structured curriculum aligned with the geometric requirements of the task. The anticipated subgoals then enable the computation of a continuous reward signal based on keypoint distance, which is used to train a low-level, goal-conditioned policy under the hierarchical RL (HRL) framework with provable sub-optimality bound.\n\nThrough extensive empirical validation in simulated environments, we demonstrate that this approach not only significantly accelerates learning but also achieves new state-of-the-art performance on long-horizon tasks, thereby offering a robust and practical pathway toward scalable visual reinforcement learning for robotics.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的奖励设计在视觉强化学习中存在瓶颈，尤其是在机器人操作中。  \n2. 实际应用中，精确的状态信息通常不可用，导致奖励设计困难。  \n3. 现有方法在高维视觉输入下训练难度大且稳定性差。  \n4. 需要一个能够从视频演示中自动合成密集奖励信号的框架。  \n\n【提出了什么创新的方法】  \n本研究提出了奖励学习与预期模型（ReLAM），通过从无动作视频演示中生成密集和结构化的奖励。ReLAM首先提取关键点，构建任务几何的中间表示，然后学习一个预期模型，预测中间关键点子目标，形成与任务几何要求一致的结构化学习课程。基于这些预期子目标，计算连续奖励信号来训练低级目标条件策略。通过广泛的实验，ReLAM显著加速了学习过程，并在长时间操作任务上达到了新的最优性能，展示了其在可扩展视觉强化学习中的潜力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems",
            "authors": "Stefano Carlo Lambertenghi,Mirena Flores Valdez,Andrea Stocco",
            "subjects": "Software Engineering (cs.SE); Robotics (cs.RO)",
            "comment": "In proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering (ASE '25)",
            "pdf_link": "https://arxiv.org/pdf/2509.22379",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22379",
            "arxiv_html_link": "https://arxiv.org/html/2509.22379v1",
            "abstract": "Simulation-based testing is a cornerstone of Autonomous Driving System (ADS) development, offering safe and scalable evaluation across diverse driving scenarios. However, discrepancies between simulated and real-world behavior—known as the reality gap—challenge the transferability of test results to deployed systems. In this paper, we present a comprehensive empirical study comparing four representative testing modalities: Software-in-the-Loop (SiL), Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing. Using a small-scale physical vehicle equipped with real sensors (camera and LiDAR), and its digital twin, we implement each setup and evaluate two ADS architectures (modular and end-to-end) across diverse indoor driving scenarios involving real obstacles, road topologies, and indoor environments. We systematically assess the impact of each testing modality along three dimensions of the reality gap: actuation, perception, and behavioral fidelity. Our results show that while SiL and ViL setups simplify critical aspects of real-world dynamics and sensing, MR testing improves perceptual realism without compromising safety or control. Importantly, we identify the conditions under which failures do not transfer across testing modalities and isolate the underlying dimensions of the gap responsible for these discrepancies. Our findings offer actionable insights into the respective strengths and limitations of each modality and outline a path toward more robust and transferable validation of autonomous driving systems.",
            "introduction": "To ensure the safety and reliability of autonomous driving systems (ADS) before deployment in public environments, rigorous system-level testing is indispensable [1, 2, 3]. A common industrial practice for ADS validation follows a two-phase testing pipeline. First, ADS components-such as perception and planning modules-are trained using real-world driving data and evaluated within virtual environments via simulation-based testing, also known as simulation-in-the-loop (SiL). Subsequently, the ADS undergoes real-world testing on real vehicles on closed tracks up to public roads [4, 5, 6, 7].\nReal-world testing, while more faithful, is costly, time-consuming, and constrained in scope and safety.\nDespite their scalability, simulations cannot fully replicate real-world physical phenomena, such as sensor noise, actuator delays, and environmental complexity. The resulting mismatch is known as the reality gap [1] and hinders the transferability of findings to real-world ADS, undermining their trustworthiness [8].\n\nVarious strategies have been proposed to mitigate the reality gap. Some aim to increase simulation fidelity through high-precision modeling (digital twins [9, 10, 11]),\nother address specific gap dimensions, such as the perception gap, by translating simulated sensor outputs into more realistic versions using generative models [12, 13, 14, 8, 15]. However, these methods are limited to model-level testing [16, 17] and do not capture the system-level interactions between perception, planning, and control modules that govern vehicle motion. As a result, they are susceptible to actuation gaps and often miss critical system-level failures [16, 14, 18].\nOther strategies involve vehicle-in-the-loop (ViL) and mixed reality (MR) testing [19], by integrating physical components such as ECUs, small-scale robots, or full vehicles-into simulation loops. While ViL provides closed-loop evaluation, it remains partially virtualized and fails to capture real-world imperfections, such as sensor noise and lighting variability (perception gap) [20]. MR partially mitigates this by injecting virtual elements (e.g., obstacles) into real sensor data, enriching scenario realism.\nPrior system-level studies using small-scale robots investigating failure transferability [8, 21, 22, 23, 24, 25] have primarily documented the existence of the reality gap, without isolating their root causes or comparing how different test modalities influence gap reduction.\n\nTo this aim, in this paper, we conduct an empirical study of the reality gap in autonomous driving by comparing four representative testing modalities: SiL, ViL, MR, and full real-world execution (RW). Our goal is to characterize the dimensions of the reality gap-specifically along actuation, perception, and behavioral fidelity-and to assess the degree to which each testing setup retains ADS behavior relative to real-world ground truth behavior.\nWhile prior work has evaluated or mitigated specific aspects of the reality gap, a broader evaluation spanning multiple testing modalities and ADS architectures remains unaddressed.\n\nTo investigate the impact of different testing modalities, we implemented a modular ROS-based evaluation framework that supports direct comparisons across synthetic, hybrid, and physical testing conditions. Our setup integrates both modular and end-to-end ADS architectures on a small-scale platform equipped with real sensors (camera and LiDAR) and its digital twin.\nWe conduct hundreds of tests across matched driving scenarios with shared road layouts, obstacle placements, and environmental conditions, allowing direct attribution of performance differences to the testing modality.\n\nOur findings reveal that:\n(i) SiL underestimates real-world variability due to idealized dynamics;\n(ii) ViL improves actuation realism but retains perception limitations;\n(iii) MR offers the best perceptual fidelity by blending virtual elements into real sensor data.\nBy isolating the effects of actuation, perception, and behavior on the reality gap, we find that critical failures often manifest differently across configurations, with the perception gap playing a greater role in behavioral divergence than actuation discrepancies. This underscores the importance of testing methods that retain real-world sensor complexity. Our results reveal the limitations of conventional simulation and point to MR as a practical middle ground between fidelity and scalability.\n\nOur paper makes the following contributions:\n\nWe provide a ROS framework for comparing SiL, ViL, MR, and RW testing for E2E and modular ADS, which is available [26].\n\nWe provide a ROS framework for comparing SiL, ViL, MR, and RW testing for E2E and modular ADS, which is available [26].\n\nWe present a systematic analysis of the reality gap in ADS testing across behavior, actuation, and perception fidelity,\nisolating which failures transfer across test modalities. We show that MR testing uniquely replicates real-world system failures, outperforming SiL and ViL across all metrics.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现实差距（reality gap）挑战了模拟测试结果在真实世界中的可转移性。  \n2. 现有方法未能全面评估不同测试模式对现实差距的影响。  \n3. 需要系统性分析以识别不同测试模式下的系统级失败。  \n\n【提出了什么创新的方法】  \n本研究提出了一种基于ROS的评估框架，比较了四种测试模式：SiL、ViL、MR和真实世界（RW）测试。通过在小规模平台上进行数百次测试，研究了各模式在行为、驱动和感知保真度上的表现。结果显示，MR测试在感知保真度上表现最佳，能够有效复制真实世界系统失败，超越了SiL和ViL。研究为理解和减少现实差距提供了新的见解，并为未来的自主驾驶系统验证提供了实用的方法论。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Trust and Human Autonomy after Cobot Failures: Communication is Key for Industry 5.0",
            "authors": "Felix Glawe,Laura Kremer,Luisa Vervier,Philipp Brauner,Martina Ziefle",
            "subjects": "Human-Computer Interaction (cs.HC); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22298",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22298",
            "arxiv_html_link": "https://arxiv.org/html/2509.22298v1",
            "abstract": "Collaborative robots (cobots) are a core technology of Industry 4.0. Industry 4.0 uses cyber-physical systems, IoT and smart automation to improve efficiency and data-driven decision-making. Cobots, as cyber-physical systems, enable the introduction of lightweight automation to smaller companies through their flexibility, low cost and ability to work alongside humans, while keeping humans and their skills in the loop. Industry 5.0, the evolution of Industry 4.0, places the worker at the centre of its principles: The physical and mental well-being of the worker is the main goal of new technology design, not just productivity, efficiency and safety standards. Within this concept, human trust in cobots and human autonomy are important. While trust is essential for effective and smooth interaction, the workers’ perception of autonomy is key to intrinsic motivation and overall well-being. As failures are an inevitable part of technological systems, this study aims to answer the question of how system failures affect trust in cobots as well as human autonomy, and how they can be recovered afterwards. Therefore, a VR experiment (n = 39) was set up to investigate the influence of a cobot failure and its severity on human autonomy and trust in the cobot. Furthermore, the influence of transparent communication about the failure and next steps was investigated. The results show that both trust and autonomy suffer after cobot failures, with the severity of the failure having a stronger negative impact on trust, but not on autonomy. Both trust and autonomy can be partially restored by transparent communication.",
            "introduction": "The fourth industrial revolution, Industry 4.0, refers to the interconnection between machines and production systems enabling increased efficiency and flexibility [1]. While the technologies driving Industry 4.0 are steadily being deployed in today’s production landscape, the European Commission put forth the vision of Industry 5.0. The main idea of Industry 5.0 is to develop a resilient, sustainable, and human-centred industry, harnessing the advances made within Industry 4.0 but to an end that goes beyond the sole increase in efficiency [2, 3]. In production, the physical and mental well-being of workers shall be achieved by providing a motivating and safe environment that meets workers’ needs, including factors such as autonomy, privacy and dignity. [4, 3, 5]. Industry 5.0 can therefore be viewed as the challenge of balancing human well-being and automation and its metrics of success [3].\n\nKey technologies of Industry 4.0 include cloud technologies, smart sensors, simulation, AI and advanced robotics, including collaborative robots (cobots) [6, 7]. Compared to traditional robots, cobots can operate in close proximity to a human partner and are designed to be highly flexible [8]. They are also designed to enhance the capabilities of human workers and not to replace them [9]. Due to their high flexibility, they can easily be adapted to the operators’ needs making them the ideal connecting element between Industry 4.0 and 5.0 [3]. Two measures are crucial to ensure the well-being of the worker and the effectiveness of the human-robot collaboration (HRC): appropriate trust in the collaboration and the satisfaction of the workers’ need for autonomy [10, 11]. Trust between a human and a robotic partner is necessary to ensure the adoption and appropriate use of the technology that enables the full potential of this relationship [12, 13, 14]. This is particularly evident in industrial workplaces, where cobots are increasingly employed. In such contexts, trust becomes even more critical, as the misuse or non-use of these cobots can compromise the safety of workers and also undermine the profitability and efficiency of the technology [15, 16, 17]. The need for autonomy, on the other hand, is a basic psychological need that must be satisfied to ensure the workers’ well-being and to foster intrinsic motivation [18]. However, designing the interaction between humans and robots (HRI) to preserve and foster human autonomy seems to be a non-trivial task. For example, Nikolova et al. 2022 reported a decrease in workers’ perceived autonomy with increasing levels of industrial robotisation. Nevertheless, it is theorized that robots could increase human autonomy by replacing dull, repetitive or even dangerous tasks [19, 20].\nThe question arises of how HRC must be designed to induce trust and fulfil the need for autonomy. One aspect that is expected to play a crucial role is the cobot’s behaviour after a mistake. In human-human relationships, mistakes and unexpected behaviours can occur due to misunderstanding or miscommunication, and in the case of technology, because of faulty software, hardware, or misuse [21]. However, these errors or failures can affect human trust in a cobot, and may also reduce human autonomy by compromising perceived system capabilities [10, 22]. To date, there is no consensus on the best strategies to restore trust after a failure and no study on the impact of failures on human autonomy. Therefore, it is important to investigate how failures in HRC affect human perception and how the cobot’s behaviour after a failure can restore trust and autonomy to allow for future optimal collaboration and the workers’ well-being. This article addresses the research question through a virtual reality (VR) experiment that examines the influence of failures and their severity on trust perception and autonomy satisfaction. Furthermore, it investigates whether transparent communication can serve as a potential way to restore trust and autonomy after a failure.\n\nIn the first part of the article, the theoretical foundations of trust and human autonomy and previous findings on the relationship between trust, autonomy, failures and transparent communication will be discussed. In the second part, we’ll derive the research question and hypotheses, and describe the method used to answer and test them. The results will be presented and discussed in the following two sections. Finally, we’ll discuss the findings and limitations of the study.",
            "llm_summary": "【论文的motivation是什么】  \n1. 如何在协作机器人（cobots）发生故障后恢复人类对其的信任。  \n2. 研究系统故障对人类自主感的影响及其恢复方法。  \n3. 探索透明沟通在恢复信任和自主感中的作用。  \n\n【提出了什么创新的方法】  \n本研究通过虚拟现实实验（n = 39）调查了cobot故障及其严重性对人类信任和自主感的影响。研究发现，cobot故障会导致信任和自主感下降，故障的严重性对信任的负面影响更大，但对自主感的影响较小。透明的沟通可以部分恢复信任和自主感。该研究为人机协作中的信任与自主感的维护提供了新的视角和方法。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning",
            "authors": "Jinkun Hao,Naifu Liang,Zhen Luo,Xudong Xu,Weipeng Zhong,Ran Yi,Yichen Jin,Zhaoyang Lyu,Feng Zheng,Lizhuang Ma,Jiangmiao Pang",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "Accepted by NeurIPS 2025; Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.22281",
            "code": "https://mesatask.github.io/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22281",
            "arxiv_html_link": "https://arxiv.org/html/2509.22281v1",
            "abstract": "The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability of task-relevant tabletop scenes for training.\nHowever, traditional methods for creating these scenes rely on time-consuming manual layout design or purely randomized layouts, which are limited in terms of plausibility or alignment with the tasks.\nIn this paper, we formulate a novel task, namely task-oriented tabletop scene generation, which poses significant challenges due to the substantial gap between high-level task instructions and the tabletop scenes.\nTo support research on such a challenging task, we introduce MesaTask-10K, a large-scale dataset comprising approximately 10,700 synthetic tabletop scenes with manually crafted layouts that ensure realistic layouts and intricate inter-object relations.\nTo bridge the gap between tasks and scenes, we propose a Spatial Reasoning Chain that decomposes the generation process into object inference, spatial interrelation reasoning, and scene graph construction for the final 3D layout.\nWe present MesaTask, an LLM-based framework that utilizes this reasoning chain and is further enhanced with DPO algorithms to generate physically plausible tabletop scenes that align well with given task descriptions.\nExhaustive experiments demonstrate the superior performance of MesaTask compared to baselines in generating task-conforming tabletop scenes with realistic layouts.",
            "introduction": "A fundamental challenge in robotic manipulation is enabling robots to accurately interpret human instructions and successfully execute complex tasks accordingly.\nThe conventional pipeline for achieving this involves task definition, simulatable tabletop scene construction, and policy training.\nHowever, traditional scene construction methods, which rely on manual design or purely randomized layouts, are limited by their labor-intensive nature and the resulting constraints on diversity and plausibility, ultimately hindering the generalization of learned policies.\nTherefore, automatic task-oriented tabletop scene generation emerges as a promising approach for effectively bridging the gap between task descriptions and scenes.\nCrucially, tabletop scene generation must satisfy three key requirements: covering task variables, enabling scene interactivity, and ensuring realistic layouts, thereby facilitating the learning of robust policies.\n\nExisting scene generation methods Wang et al. (2024); Dai et al. (2024); Huang et al. (2024) often start from a single scene image and attempt to recover the corresponding tabletop scene through object retrieval and layout optimization.\nUnfortunately, their ability to understand under-specified task instructions still requires empirical corroboration.\nOther approaches Yang et al. (2024f, e); Çelen et al. (2024) leverage powerful language models (LLMs) to interpret task prompts and then synthesize tabletop scenes in a zero-shot manner.\nNevertheless, these methods are hindered by inherent limitations, no matter the inevitable occlusion in scene images or the lack of fine-tuning on a scene dataset, which significantly impede the modeling of realistic table layouts and complex inter-object relations, such as stacking and containment, within the scene.\nAs a result, task-oriented tabletop scene generation remains a challenging problem due to the scarcity of datasets and the substantial gap between task instructions and scene layouts.\n\nTo tackle these challenges, we collect a first-of-its-kind dataset of synthetic tabletop scenes with manually crafted layouts, dubbed MesaTask-10K.\nAs shown in Figure 1, our dataset comprises approximately 10,70010,700 diverse tabletop scenes, spanning six common indoor table categories, including office tables, dining tables, kitchen counters, and more.\nThe 3D objects in MesaTask-10K originate from a large asset library containing over 12,00012,000 rigid and articulated 3D assets, each with detailed semantic information, such as object category, description, and materials, and featuring a comprehensive taxonomy of over 200 object classes on the tables.\nAs claimed in Wang et al. (2024), pretrained 2D image generative models better capture scene and object configurations both at the scene level and in fine-grained inter-object relations.\nInspired by this, our dataset is built upon diverse tabletop scene images with diversity and realistic layouts, generated by a large text-to-image model Labs (2024) pretrained on massive internet data.\nTo obtain a coarse layout from the scene image, we estimate the depth Yang et al. (2024c) of each image, extract the instance point cloud, and acquire the 3D bounding box of objects.\nWe then leverage the object descriptions labeled by VLM Achiam et al. (2023) to retrieve suitable 3D assets from the library and construct an initial replica of the tabletop scenes.\nSubsequently, human annotators meticulously refine these 3D layouts, adjusting the object size and positions as per the image prompt, addressing inaccuracies from occlusion and ensuring complex inter-object relations.\nUltimately, all the scenes are put into a physical simulator, IsaacSim NVIDIA (2024), to prevent object collisions.\n\nConfronted with the significant gap between tasks and scenes, we propose a novel paradigm referred to as Spatial Reasoning Chain, decomposing task-to-tabletop scene generation into a structured chain of thought (CoT).\nGiven a high-level task description, this chain of thought begins with the inference of requisite objects, accompanied by their semantic attributes and spatial interrelations, based on which a complete scene graph is formed, and finally leads to a concrete 3D layout of objects on the table.\nTo establish trainable spatial reasoning chains with our dataset, we design a set of delicate rules to extract the object attributes and inter-object relations, thus forming a scene graph for each tabletop scene.\nSubsequently, we leverage a multimodal large language model, taking scene graphs and rendered scene images as input, to generate corresponding task information and detailed spatial reasoning descriptions for training.\n\nThanks to our structured reasoning chains, it’s convenient to empower LLM with 3D spatial reasoning and scene generation capability.\nIn this paper, we propose MesaTask, a novel LLM-based framework for task-oriented tabletop scene generation.\nSpecifically, we initially employ the supervised fine-tuning (SFT) strategy on our constructed reasoning data to inject the LLM with 3D spatial reasoning capabilities.\nHowever, MesaTask occasionally generates unsatisfactory tabletop scenes with minor object collisions and misalignment with the given task.\nTo circumvent this hurdle, we devise paired training data and leverage a conventional RL algorithm, namely Direct Preference Optimization (DPO), to boost our MesaTask model, thereby ensuring that the generated scenes are devoid of object collisions and exhibit improved conformity with the provided task descriptions.\n\nFor a more comprehensive performance assessment, we leverage powerful VLMs to evaluate the rendered scene images from multiple perspectives, including task-scene alignment, physical viability, scene layout plausibility, etc.\nThrough extensive experiments, our MesaTask framework is capable of generating physically plausible tabletop scenes with realistic layouts, outperforming baseline methods in terms of FID, VLM-based metrics, and the user study.\nIn particular, our generated tabletop scenes strictly conform to given task instructions and exhibit rich inter-object relations, such as stacking or containing.\nIn summary, our contributions are threefold:\n\nWe pioneer the formulation of Task-to-Scene generation task, which aims to generate physically plausible tabletop scenes directly from high-level task descriptions.\n\nWe introduce MesaTask-10k, a large-scale tabletop scene dataset with human-crafted realistic layouts, characterized by rich inter-object relations and a tremendous amount of synthetic 3D object assets.\n\nAlong with the delicate design of spatial reasoning chains, we propose MesaTask, an LLM-based framework endowed with the capability of 3D spatial reasoning and tabletop scene generation, achieving superior performance across various evaluation criteria.\n\n1. We pioneer the formulation of Task-to-Scene generation task, which aims to generate physically plausible tabletop scenes directly from high-level task descriptions.\n\n2. We introduce MesaTask-10k, a large-scale tabletop scene dataset with human-crafted realistic layouts, characterized by rich inter-object relations and a tremendous amount of synthetic 3D object assets.\n\n3. Along with the delicate design of spatial reasoning chains, we propose MesaTask, an LLM-based framework endowed with the capability of 3D spatial reasoning and tabletop scene generation, achieving superior performance across various evaluation criteria.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的场景生成方法依赖手动设计或随机布局，效率低且缺乏多样性。  \n2. 任务描述与桌面场景之间存在显著差距，导致生成的场景不符合任务要求。  \n3. 现有方法无法有效理解不明确的任务指令，限制了场景生成的真实性和复杂性。  \n\n【提出了什么创新的方法】  \n本文提出了MesaTask，一个基于LLM的框架，旨在通过空间推理链将高层任务描述转化为物理上合理的桌面场景。该方法首先进行对象推理、空间关系推理，然后构建场景图，最终生成3D布局。通过引入MesaTask-10K数据集，框架在生成符合任务要求的桌面场景方面表现优越，超越了基线方法，确保生成的场景不仅符合任务描述，还具备丰富的对象间关系，如堆叠和包含。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Human Autonomy and Sense of Agency in Human-Robot Interaction: A Systematic Literature Review",
            "authors": "Felix Glawe,Tim Schmeckel,Philipp Brauner,Martina Ziefle",
            "subjects": "Human-Computer Interaction (cs.HC); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22271",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22271",
            "arxiv_html_link": "https://arxiv.org/html/2509.22271v1",
            "abstract": "Human autonomy and sense of agency are increasingly recognised as critical for user well-being, motivation, and the ethical deployment of robots in human-robot interaction (HRI). Given the rapid development of artificial intelligence, robot capabilities and their potential to function as colleagues and companions are growing. This systematic literature review synthesises 22 empirical studies selected from an initial pool of 728 articles published between 2011 and 2024. Articles were retrieved from major scientific databases and identified based on empirical focus and conceptual relevance, namely, how to preserve and promote human autonomy and sense of agency in HRI. Derived through thematic synthesis, five clusters of potentially influential factors are revealed: robot adaptiveness, communication style, anthropomorphism, presence of a robot and individual differences. Measured through psychometric scales or the intentional binding paradigm, perceptions of autonomy and agency varied across industrial, educational, healthcare, care, and hospitality settings. The review underscores the theoretical differences between both concepts, but their yet entangled use in HRI. Despite increasing interest, the current body of empirical evidence remains limited and fragmented, underscoring the necessity for standardised definitions, more robust operationalisations, and further exploratory and qualitative research. By identifying existing gaps and highlighting emerging trends, this review contributes to the development of human-centered, autonomy-supportive robot design strategies that uphold ethical and psychological principles, ultimately supporting well-being in human-robot interaction.",
            "introduction": "Human-Robot Interaction (HRI) is a well-studied subject. From robots designed to act as colleagues in assembly tasks, simple cleaning robots, and to pet robots used in care settings, robots are envisioned and studied for a broad range of applications [1, 2, 3]. Robots that are designed to work with or accompany humans must be adapted to the humans’ diverse needs for effective and human-centred interaction and collaboration [4, 5].\n\nNumerous studies have examined how HRI and robots must be modelled and designed to foster user experience, acceptance of robots, trust in robots, or other context-specific goal variables like increased learning or even health benefits [6, 7, 8, 9, 10].\nWhile acceptance of robots and the intended outcome of the HRI are essential for the effective deployment of this technology, this focus neglects ethical concerns.\n\nIn 2017, the Institute of Electrical and Electronics Engineers (IEEE), the largest technical professional association in the world, published the document ”Ethically Aligned Design” to foster discourse and the development of autonomous technology that aligns with specified ethical design principles [11]. One central principle is the optimisation of technology for human well-being. They argue that without well-being as a central aspect of technology design, even the best design intentions could have serious negative consequences. One of the critical aspects of designing technology that upholds human well-being is the issue of maintaining and encouraging human autonomy.\n\nThe concept of human autonomy has a long history as a basic ethical principle, advanced for example by the highly influential groundwork of Immanuel Kant, who described autonomy as freedom of action and choice, without being subject to the control of others or to one’s own inclinations [12, 13].\n\nBesides human autonomy as an ethical principle, autonomy is also identified as a basic psychological need (BPN) in the well-known self-determination theory (SDT) by Ryan and Deci [14]. In SDT, autonomy and the psychological needs for competence and relatedness serve as prerequisites of intrinsic motivation, which in turn is an important factor for well-being. This connection was empirically verified for numerous contexts and across cultures or demographic variables [15]. They define the need for autonomy as the experience of willingness and volition as well as the sense of integrity when actions, feelings or thoughts are authentic and self-endorsed. In contrast, the frustration of autonomy leads to a sense of conflict and pressure [16].\n\nAt this point, it is important to highlight the difference between machine and human autonomy. Machines feature autonomy while humans are in a state of autonomy, but do not possess it [17]. Furthermore, human autonomy does not hinge on complete independence; it builds upon relationships and acknowledges the fact that a human can never be fully independent of others [18]. A well-trusted employer, who recognises and shares the goals and values of his or her employees, can foster autonomy satisfaction, and at the same time, be clear in his or her demands [18, 19]. Human autonomy cannot be oversatisfied [18]. In contrast, there exists an automation conundrum where higher machine autonomy relates to decreased situational awareness and the ability to take over control of a human operator [20]. Therefore, balancing human and machine autonomy is a crucial task.\n\nWith the importance of human autonomy in mind, it is evident why human autonomy serves as a foundational concept in human-computer interaction (HCI) and why the design for human autonomy and psychological needs in general should be placed at the centre of robot design as well [21].\n\nHowever, SDT and BPNs are rarely applied to the context of HRI, although evidence by Nikolova et al. suggests that robotisation at work in its current form might even lead to lowered autonomy, relatedness and meaningfulness [21, 22]. In contrast, theoretical considerations by Formosa highlight that robots could improve autonomy by providing the user with more valuable ends, improved autonomy competencies and more authentic choices [23].\n\nBased on SDT, Peters et al. published the influential METUX framework in 2018 to guide HCI researchers and developers to design technology for well-being based on the psychological needs of autonomy, relatedness and competence [18]. In their framework, they establish the idea of six spheres of experience in all of which the needs can be experienced and affected: adoption, interface, task, behaviour, life and society. The adoption sphere refers to the experience of deciding on the use of a new technology, the interface sphere to the interaction with the technology via its interface, the task sphere to the experience of the task carried out with or by the technology, the behaviour sphere to the overarching behaviour supported or induced by the technology (e.g. cooking or sleep tracking), the life sphere to the experience of how the technology and the induced behaviour shapes the user’s life in general and the society sphere concerned with how the society in general experiences the effects of the technology.\n\nBased on the METUX framework, Janssen and Schadenberg developed the first guidelines for well-being-oriented social robot design [21]. They based these guidelines on empirical studies in HRI concerned with related concepts like empowerment, perceived freedom or self-efficacy. However, they acknowledge that only a few articles have applied theories of well-being and motivation in HRI research.\n\nFor each sphere of experience (except the sphere of adoption and society), they define and give examples of how the individual psychological needs of relatedness, autonomy, and competence can be satisfied in social robot design. Because this literature review is concerned with autonomy, we will only summarise the recommendations given to foster autonomy:\n\nInterface Sphere:\n\nInterface Sphere:\n\nProvide choice and options for interaction modes\n\nApply non-controlling communication techniques\n\nTask Sphere:\n\nProvide choices of tasks and task characteristics performed by the robot\n\nAcknowledge negative feelings towards a task\n\nProvide a meaningful rationale for tasks\n\nBehaviour Sphere:\n\nEmpower decision about type, difficulty and frequency of (users’) behaviour\n\nAcknowledge negative feelings towards a behaviour\n\nProvide meaningful rationale for behaviour\n\nLife Sphere:\n\nPrevent excessive engagement and overreliance on the robot\n\nAssess whether users still act in line with their own values.\n\nJansen and Schadenberg describe their work as a call to action to implement psychological-need-fostering measures when designing robots and to use the framework as a reference point from which to start structured and increased research efforts [21].\n\nThe sense of agency, defined as the ”feeling of control over actions and their consequences” [24, p.1], is often used and treated as a concept synonymous with or highly entangled with autonomy, especially in HCI [25]. Autonomy and agency are even described as ”twin concepts,” highlighted by a shared focus on aspects of self-causality, perception of control, identity, and material independence [26, 25]. Both are valued as intrinsically good and contribute to physical and mental well-being [25, 24]. While the concept of the need for autonomy is grounded in motivational psychology, the sense of agency is often approached from a neuroscientific perspective. Wegner and Wheatley established the Model of Conscious Will, which explains how unconscious causes of our thoughts and actions lead to the actual thoughts and actions we exhibit [27]. While our sense of agency arises from our consciously experienced thoughts and the subsequently performed actions, the real causes of these actions are not our thoughts but the unconscious triggers that occur before we actively think about the intended actions [24].\n\nIn HCI, the sense of agency is acknowledged as an important variable in interface design [24]. Technology should allow users to feel agentic (i.e., in control). However, this conflicts with the rising automation in areas such as self-driving cars and flight assistance systems.\n\nConceptually, one could describe the sense of agency as a prerequisite for satisfying our need for autonomy: without the sense that we are the agents of our actions, it is likely that we do not perceive those actions as being in line with our values and identity [28]. This potential connection, however, remains to be proven. Regardless, in this work, we will apply both concepts to account for their potentially synonymous use and will try to shed light on how they are applied in HRI.\n\nWith this study, we seek to repeat the call to action of Janssen and Schadenberg and to add an empirical base to the theoretical frameworks with a focus on autonomy and agency.\nTherefore, we will identify and aggregate the empirical research on how to maintain and foster human autonomy and agency in HRI and try to identify trends in this research area. The findings of this study will serve as a foundation for future research and as a point of departure for developers and policymakers who seek to integrate human autonomy as a central psychological and ethical principle into the design of HRI.\n\nThe article is structured as follows: First, we describe our methodological approach based on the PRISMA workflow, including the utilised databases, deployed keywords, the process of article identification, characteristics of the final dataset, and the article analysis [29]. Second, we present the results of the analysis. Finally, we discuss the results and the limitations of this study and derive recommendations for subsequent research.\n\n1. Interface Sphere:\n\n2. Provide choice and options for interaction modes\n\n3. Apply non-controlling communication techniques\n\n4. Task Sphere:\n\n5. Provide choices of tasks and task characteristics performed by the robot\n\n6. Acknowledge negative feelings towards a task\n\n7. Provide a meaningful rationale for tasks\n\n8. Behaviour Sphere:\n\n9. Empower decision about type, difficulty and frequency of (users’) behaviour\n\n10. Acknowledge negative feelings towards a behaviour\n\n11. Provide meaningful rationale for behaviour\n\n12. Life Sphere:\n\n13. Prevent excessive engagement and overreliance on the robot\n\n14. Assess whether users still act in line with their own values.\n\n1. Provide choice and options for interaction modes\n\n2. Apply non-controlling communication techniques\n\n1. Provide choices of tasks and task characteristics performed by the robot\n\n2. Acknowledge negative feelings towards a task\n\n3. Provide a meaningful rationale for tasks\n\n1. Empower decision about type, difficulty and frequency of (users’) behaviour\n\n2. Acknowledge negative feelings towards a behaviour\n\n3. Provide meaningful rationale for behaviour\n\n1. Prevent excessive engagement and overreliance on the robot\n\n2. Assess whether users still act in line with their own values.",
            "llm_summary": "【论文的motivation是什么】  \n1. 人类自主性和代理感在机器人与人类互动中对用户的幸福感和动机至关重要。  \n2. 现有的实证研究数量有限且分散，缺乏标准化定义和更强的操作化。  \n3. 需要开发以人类为中心、支持自主性的机器人设计策略，以促进伦理和心理原则。  \n\n【提出了什么创新的方法】  \n本文通过系统文献综述，分析了22项实证研究，揭示了影响人类自主性和代理感的五个关键因素：机器人适应性、沟通风格、人性化、机器人存在感和个体差异。通过心理测量量表和意向绑定范式，研究发现不同应用场景下人们对自主性和代理感的感知存在差异。该研究为未来的HRI设计提供了理论基础和实证支持，强调了将人类自主性作为设计核心的重要性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach",
            "authors": "Seoyoung Lee,Seonbin Yoon,Seongbeen Lee,Hyesoo Kim,Joo Yong Sim",
            "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Multiagent Systems (cs.MA); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22137",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22137",
            "arxiv_html_link": "https://arxiv.org/html/2509.22137v1",
            "abstract": "GUI task automation streamlines repetitive tasks, but existing LLM or VLM-based planner-executor agents suffer from brittle generalization, high latency, and limited long-horizon coherence. Their reliance on single-shot reasoning or static plans makes them fragile under UI changes or complex tasks. Log2Plan addresses these limitations by combining a structured two-level planning framework with a task mining approach over user behavior logs, enabling robust and adaptable GUI automation. Log2Plan constructs high-level plans by mapping user commands to a structured task dictionary, enabling consistent and generalizable automation. To support personalization and reuse, it employs a task mining approach from user behavior logs that identifies user-specific patterns. These high-level plans are then grounded into low-level action sequences by interpreting real-time GUI context, ensuring robust execution across varying interfaces. We evaluated Log2Plan on 200 real-world tasks, demonstrating significant improvements in task success rate and execution time. Notably, it maintains over 60.0% success rate even on long-horizon task sequences, highlighting its robustness in complex, multi-step workflows.",
            "introduction": "Large Language Models (LLMs) now demonstrate such strong capabilities in natural–language understanding and generation that they are being adopted as general purpose agents for personal assistance, software automation, and GUI testing(Raffel et al., 2020; Nguyen et al., 2024). In particular, desktop based agents that can correctly interpret user commands and act inside real software environments have become a key research frontier(Gao et al., 2024; Zhang et al., 2024a; Qin et al., 2025; AGI, 2025).\n\nExisting desktop agent approaches fall into three families: (i) Prompt based zero/ few shot agents exploit pretrained models directly, yielding quick and flexible responses but suffering brittle behavior on multi step tasks(Yao et al., 2023; Yang et al., 2023; Shinn et al., 2023); (ii) Task specific fine tuned models deliver high accuracy within narrow domains yet demand costly retraining and scale poorly to unseen tasks(Li et al., 2020; Hong et al., 2024; Li et al., 2024); (iii) Planner–executor agents couple an LLM planner with external tools, handling complex workflows at the price of engineering overhead and context window explosion(Zhang et al., 2024a; Gao et al., 2024; Qin et al., 2025).\n\nDespite their differences, all three approaches share fundamental limitations in real world GUI environments. They rely on static planners or rigid pipelines, making them fragile when layouts change or personalized reasoning is required. While task mining and RPA research highlight the value of recurring structures in user logs(Choi et al., 2022; Krosnick and Oney, 2022; Huang et al., 2024a; Riva and Kace, 2021), existing agents ignore this opportunity. Replaying-based methods further show that small visual changes cause high failure rates(Deka et al., 2017; Riva and Kace, 2021). Moreover, vision encoding at each step inflates latency, while long plans overflow the model’s context window(Hong et al., 2024).\n\nLog2Plan addresses these gaps with a planning centric LLM framework that bridges symbolic task mining and retrieval augmented generation. It leverages user logs to learn and reuse generalizable task structures(Redis et al., 2024; Yuan et al., 2024). Concretely, Log2Plan (i) segments raw GUI logs into semantically meaningful action–object units, and (ii) mines frequently repeated patterns to synthesize modular automation flows. Compared with record and replay baselines, this yields markedly better generalization and interpret-ability. At runtime, Log2Plan dynamically retrieves and recombines prior workflows to generate robust plans for new commands, enabling fast adaptation without model fine-tuning. By separating high-level intent from low-level execution, the LLM operates over structured, behavior-grounded representations, while a lightweight executor handles GUI interactions.\n\nOur contributions are three fold:\n\nWe formularize GUI interactions as action–object pairs, enabling consistent and interpretable planning from both natural language commands and interaction logs.\n\nWe identify recurring user behaviors in interaction logs and compose them into modular automation units that can be flexibly reused across diverse tasks.\n\nWe propose a two level architecture that cleanly separates user intent from execution, allowing LLMs to plan in a robust and scalable manner.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的LLM或VLM基础的规划执行代理在复杂任务中表现出脆弱的泛化能力。  \n2. 静态计划的依赖使得现有方法在用户界面变化时变得脆弱。  \n3. 现有方法在处理长时间任务序列时成功率低且延迟高。  \n\n【提出了什么创新的方法】  \nLog2Plan提出了一种结合结构化两级规划框架和任务挖掘方法的自适应GUI自动化框架。该方法首先通过将用户命令映射到结构化任务字典来构建高层计划，并通过分析用户行为日志识别用户特定模式以支持个性化和重用。接着，Log2Plan根据实时GUI上下文将高层计划转化为低层行动序列，从而确保在不同界面上的稳健执行。通过在200个真实任务上的评估，Log2Plan显著提高了任务成功率和执行时间，尤其在长时间任务序列中保持超过60.0%的成功率，展示了其在复杂多步骤工作流中的稳健性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics",
            "authors": "Saurav Jha,Stefan K. Ehrlich",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22014",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22014",
            "arxiv_html_link": "https://arxiv.org/html/2509.22014v1",
            "abstract": "Healthcare robotics requires robust multimodal perception and reasoning to ensure safety in dynamic clinical environments. Current Vision-Language Models (VLMs) demonstrate strong general-purpose capabilities but remain limited in temporal reasoning, uncertainty estimation, and structured outputs needed for robotic planning. We present a lightweight agentic multimodal framework for video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model with a SmolAgent-based orchestration layer, it supports chain-of-thought reasoning, speech–vision fusion, and dynamic tool invocation. The framework generates structured scene graphs and leverages a hybrid retrieval module for interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark and a custom clinical dataset show competitive accuracy and improved robustness compared to state-of-the-art VLMs, demonstrating its potential for applications in robot-assisted surgery, patient monitoring, and decision support.",
            "introduction": "Robotics in healthcare has emerged as a critical domain where perception, reasoning, and safe decision-making intersect with high-stakes clinical applications. From robot-assisted surgery [1], to autonomous patient monitoring [2], and collaborative care robots [3], the demand for systems that can robustly interpret complex multimodal environments continues to grow. A central requirement across these applications is scene understanding—the ability to identify objects, infer spatial and temporal relations, and generate structured representations that inform safe robotic actions [4].\n\nRecent advances in Vision-Language Models (VLMs) such as Llava [5] demonstrate strong multimodal reasoning capabilities. These systems have achieved remarkable performance in tasks such as visual question answering, image captioning, and document analysis. However, their application in robotics, especially in healthcare, faces critical challenges. First, most VLMs operate as monolithic end-to-end pipelines, limiting flexibility, explainability, and integration with robotic control loops [6]. Second, temporal and spatial reasoning remains underdeveloped, hindering the accurate interpretation of dynamic surgical or clinical environments [7]. Third, the lack of structured outputs (e.g. scene graphs) complicates downstream integration into robotic planning frameworks [5, 8]. Finally, deployment in clinical workflows is often obstructed by the high computational cost of large proprietary models and unresolved concerns about data privacy [9].\n\nWithin the healthcare domain, these shortcomings are especially problematic. For example, surgical robots require interpretable scene representations to coordinate tool trajectories with evolving anatomical contexts. Assistive robots in hospitals must combine perception with reasoning to safely navigate dynamic environments involving patients and caregivers. Clinical decision-support robots further demand uncertainty estimation and fallback strategies to avoid unsafe actions under ambiguity.\n\nTo address these challenges, we present a leightweight modular agentic multimodal framework designed for video-based scene understanding in clinical and robotic contexts. By integrating the Qwen2.5-VL-3B-Instruct model with a SmolAgent-based orchestration layer, our framwork combines chain-of-thought reasoning, speech-vision fusion, and structured scene graph generation. In addition, a hybrid retrieval mechanism (LightRAG [10]) enables both efficient and interpretable knowledge integration. Together, these components aim to bridge the gap between raw perception and symbolic planning, enabling healthcare robots to act more safely, transparently, and adaptively in high-stakes environments.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的Vision-Language Models在动态临床环境中的时间推理和不确定性估计能力有限。  \n2. 需要生成结构化输出（如场景图）以便于机器人规划的集成。  \n3. 高计算成本和数据隐私问题阻碍了临床工作流中的部署。  \n\n【提出了什么创新的方法】  \n提出了一种轻量级的模块化代理多模态框架，专为临床和机器人场景理解设计。该框架结合了Qwen2.5-VL-3B-Instruct模型与SmolAgent的编排层，支持链式思维推理、语音-视觉融合和结构化场景图生成。此外，采用混合检索机制（LightRAG）实现高效且可解释的知识集成。通过在Video-MME基准和自定义临床数据集上的评估，展示了相较于现有VLM的竞争性准确性和增强的鲁棒性，表明该框架在机器人辅助手术、患者监测和决策支持等应用中的潜力。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation",
            "authors": "Jiahui Wang,Changhao Chen",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "Accepted as a poster in NeurIPS 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.21930",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21930",
            "arxiv_html_link": "https://arxiv.org/html/2509.21930v1",
            "abstract": "Visual navigation is essential for robotics and embodied AI. However, existing foundation models, particularly those with transformer decoders, suffer from high computational overhead and lack interpretability, limiting their deployment in resource-tight scenarios.\nTo address this, we propose DynaNav, a Dynamic Visual Navigation framework that adapts feature and layer selection based on scene complexity. It employs a trainable hard feature selector for sparse operations, enhancing efficiency and interpretability. Additionally, we integrate feature selection into an early-exit mechanism, with Bayesian Optimization determining optimal exit thresholds to reduce computational cost.\nExtensive experiments in real-world-based datasets and simulated environments demonstrate the effectiveness of DynaNav. Compared to ViNT, DynaNav achieves a 2.26×2.26\\times reduction in FLOPs, 42.3% lower inference time, and 32.8% lower memory usage, while improving navigation performance across four public datasets.",
            "introduction": "Visual navigation is a fundamental capability for robotics and embodied AI, enabling autonomous agents to perceive, interpret, and navigate complex 3D environments based on visual inputs [1, 2, 3]. Its applications span real-world scenarios, such as delivery and logistics, as well as virtual domains, including gaming and simulation. By bridging perception and action, visual navigation plays a crucial role in intelligent systems.\nRecently, there has been growing interest in developing foundation models for visual navigation [4, 5, 6, 7, 8, 9, 10, 1]. ViNT [5] is a notable example that learns from large-scale egocentric observations using transformer layers on CNN-extracted features, demonstrating strong generalization across robotic platforms and environments. NoMad [6] further builds on this by incorporating a diffusion policy and a goal-masking mechanism. PixNav [9] utilizes textual heuristics and large language models(LLMs) to explore zero-shot possibility. However, these approaches, particularly those relying on deep neural architectures such as transformer decoders, introduce significant computational overhead, posing challenges for edge deployment where efficiency is paramount.\n\nRobotic applications demand greater efficiency than large cloud-based models. As the trend toward efficient foundation models continues [11, 12], reducing the computational burden of visual navigation models is a key challenge. Additionally, existing models function as \"black boxes,\" raising concerns about interpretability. As humans and intelligent agents increasingly coexist, explainability becomes essential. These challenges lead to two critical research questions:\n\nIs it necessary to activate all transformer layers for every navigation scenario?\n\nWhich features are most important in the decoding process, and can we identify the most salient regions or pixels for navigation?\n\nTo this end, we propose DynaNav, a highly efficient Dynamic Visual Navigation framework that adaptively selects relevant features and neural layers based on visual observations. Our approach employs a trainable hard feature selector to create sparse representations, enabling computationally efficient sparse operations at the feature level.\nThis dynamic feature masking not only lowers computational overhead but also improves the understanding of which regions more relevantly influence the inference of visual navigation models, thereby enhancing explainability. Additionally, we introduce an early-exit strategy for deep Transformer layers by integrating feature selection into the early-exit mechanism, improving stability and computational efficiency. After training the decoder, Bayesian Optimization determines optimal early-exit thresholds. During inference, if a layer’s feature meets its threshold, computation terminates early, significantly reducing overall computational cost.\nExtensive experiments on real-world datasets and in simulated environments demonstrate the effectiveness of our proposed DynaNav. Compared to ViNT [5], DynaNav achieves a 2.26×2.26\\times reduction in FLOPs, 42.3% lower inference time, and 32.8% lower memory usage while improving navigation performance across four public datasets.\nTo the best of our knowledge, this is the first work to introduce dynamic network mechanisms to visual navigation models. To sum up, the main contributions of our work can be summarized as follows:\n\nWe propose DynaNav, a highly efficient and effective dynamic neural model for visual navigation, introducing a novel feature and layer selection strategy to improve efficiency without compromising performance.\n\nWe integrate sparse feature selection into the early exit mechanism, improving the stability and success rate of dynamic layer inference, while the visualized mask enhances the interpretability of the navigation decision process.\n\nExtensive experiments and simulations demonstrate that DynaNav achieves more than twice the efficiency while maintaining comparable success rates.\n\n1. Is it necessary to activate all transformer layers for every navigation scenario?\n\n2. Which features are most important in the decoding process, and can we identify the most salient regions or pixels for navigation?\n\n1. We propose DynaNav, a highly efficient and effective dynamic neural model for visual navigation, introducing a novel feature and layer selection strategy to improve efficiency without compromising performance.\n\n2. We integrate sparse feature selection into the early exit mechanism, improving the stability and success rate of dynamic layer inference, while the visualized mask enhances the interpretability of the navigation decision process.\n\n3. Extensive experiments and simulations demonstrate that DynaNav achieves more than twice the efficiency while maintaining comparable success rates.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有视觉导航模型在资源有限的场景中计算开销过大，限制了其应用。  \n2. 现有模型缺乏可解释性，难以理解模型决策过程。  \n\n【提出了什么创新的方法】  \nDynaNav是一个动态视觉导航框架，根据场景复杂性自适应选择特征和层。它采用可训练的硬特征选择器进行稀疏操作，提升了效率和可解释性。此外，DynaNav将特征选择整合进早退出机制，通过贝叶斯优化确定最佳退出阈值以降低计算成本。实验结果表明，DynaNav在四个公共数据集上相比ViNT实现了2.26倍的FLOPs减少，推理时间降低42.3%，内存使用降低32.8%，同时提升了导航性能。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "ShipwreckFinder: A QGIS Tool for Shipwreck Detection in Multibeam Sonar Data",
            "authors": "Anja Sheppard,Tyler Smithline,Andrew Scheffer,David Smith,Advaith V. Sethuraman,Ryan Bird,Sabrina Lin,Katherine A. Skinner",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO); Image and Video Processing (eess.IV)",
            "comment": "Accepted to OCEANS 2025 Great Lakes",
            "pdf_link": "https://arxiv.org/pdf/2509.21386",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21386",
            "arxiv_html_link": "https://arxiv.org/html/2509.21386v1",
            "abstract": "In this paper, we introduce ShipwreckFinder, an open-source QGIS plugin that detects shipwrecks from multibeam sonar data. Shipwrecks are an important historical marker of maritime history, and can be discovered through manual inspection of bathymetric data. However, this is a time-consuming process and often requires expert analysis. Our proposed tool allows users to automatically preprocess bathymetry data, perform deep learning inference, threshold model outputs, and produce either pixel-wise segmentation masks or bounding boxes of predicted shipwrecks. The backbone of this open-source tool is a deep learning model, which is trained on a variety of shipwreck data from the Great Lakes and the coasts of Ireland. Additionally, we employ synthetic data generation in order to increase the size and diversity of our dataset. We demonstrate superior segmentation performance with our open-source tool and training pipeline as compared to a deep learning-based ArcGIS toolkit and a more classical inverse sinkhole detection method. The open-source tool can be found at https://github.com/umfieldrobotics/ShipwreckFinderQGISPlugin.",
            "introduction": "Recent advances in acoustic sensor technology and marine survey platforms have enabled efficient large area data collection to deliver massive amounts of data to marine scientists. For example, high-resolution mapping projects such as Lakebed 2030 [1] aim to fully map the seafloor of the Great Lakes in the next half decade, greatly increasing the amount of publicly available data. However, this data has yet to be fully leveraged for training machine learning models.\n\nSunken objects such as shipwrecks and airplanes hold important archaeological, historical, and environmental data. Finding shipwrecks in large-area seafloor surveys is a time-consuming task. Typically, this is done by hand with experts who manually inspect statistical anomalies in the data and cross-reference historical shipwreck logs [2]. In the past decade, interest in algorithmic and deep learning approaches to shipwreck detection has increased. A variety of sensors have been used for this problem, ranging from Multibeam Echosounder (MBES) [3, 4, 5] and backscatter data [6], to orbital ocean imagery taken from satellites [7], to sidescan sonar [8], to bathymetry from LiDaR [9]. However, these existing approaches often perform poorly on out-of-distribution data, still require expert oversight, and are rarely open-source.\n\nIn this work, we develop a machine learning-based tool to automate the detection of shipwreck sites from multibeam sonar data (see Fig. 1). Our tool, ShipwreckFinder, is designed to have seamless integration into QGIS [10], a freely available Geographic Information System (GIS) platform, to enable visualization and geo-referencing for detected shipwrecks. We train and validate a shipwreck segmentation model using existing data from Thunder Bay National Marine Sanctuary (TBNMS) [11] and deeper water data collected by the National Oceanic and Atmospheric Administration (NOAA) [12] and Integrated Mapping for the Sustainable Development of Ireland’s Marine Resource (INFOMAR) [13]. ShipwreckFinder aims to improve access to state-of-the-art machine learning methods within the marine archaeology community through an open-source toolset for automatic shipwreck detection.\n\nUltimately, the ShipwreckFinder tool has great potential to reduce the time and cost required to detect archaeological sites from multibeam sonar data collected across our lakes and oceans, accelerating the timeline for discoveries to be made by the scientific community and shared with the public.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的船舶沉没物检测方法耗时且依赖专家分析。  \n2. 现有算法在处理分布外数据时表现不佳，缺乏开源工具。  \n3. 需要提高海洋考古学界对机器学习方法的访问和应用。  \n\n【提出了什么创新的方法】  \n本文提出了ShipwreckFinder，一个开源的QGIS插件，自动检测多波束声纳数据中的船舶沉没物。该工具通过预处理海底地形数据、执行深度学习推理、阈值模型输出，生成像素级分割掩膜或预测的船舶沉没物边界框。使用来自大湖区和爱尔兰海岸的多样化沉没物数据进行训练，并通过合成数据生成增强数据集的规模和多样性。实验结果表明，ShipwreckFinder在分割性能上优于现有的深度学习ArcGIS工具和经典的逆沉洞检测方法，显著提高了检测效率，降低了成本。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        }
    ],
    "2025-09-30": [],
    "2025-10-01": [
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation",
            "authors": "Zhuoyang Liu,Jiaming Liu,Jiadong Xu,Nuowei Han,Chenyang Gu,Hao Chen,Kaichen Zhou,Renrui Zhang,Kai Chin Hsieh,Kun Wu,Zhengping Che,Jian Tang,Shanghang Zhang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.26642",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26642",
            "arxiv_html_link": "https://arxiv.org/html/2509.26642v1",
            "abstract": "Vision-language-action models (VLAs) have shown generalization capabilities in robotic manipulation tasks by inheriting from vision-language models (VLMs) and learning action generation.\nMost VLA models focus on interpreting vision and language to generate actions, whereas robots must perceive and interact within the spatial-physical world.\nThis gap highlights the need for a comprehensive understanding of robotic-specific multisensory information, which is crucial for achieving complex and contact-rich control.\nTo this end, we introduce a multisensory language–action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory objectives to facilitate physical world modeling.\nSpecifically, to enhance perceptual representations, we propose an encoder-free multimodal alignment scheme that innovatively repurposes the large language model itself as a perception module, directly interpreting multimodal cues by aligning 2D images, 3D point clouds, and tactile tokens through positional correspondence.\nTo further enhance MLA’s understanding of physical dynamics, we design a future multisensory generation post-training strategy that enables MLA to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation.\nFor evaluation, the MLA model outperforms the previous state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex, contact-rich real-world tasks, respectively, while also demonstrating improved generalization to unseen configurations.\nProject website: https://sites.google.com/view/open-mla",
            "introduction": "Recent robot imitation learning has achieved remarkable advances in training policies from expert demonstrations to perform diverse vision–language manipulation tasks. Meanwhile, vision–language models (VLMs) [1, 2, 3, 4] pre-trained on internet-scale data have been proven to possess strong capabilities in common-sense reasoning in general scenarios. Building on these progresses, vision–language–action (VLA) models have been proposed [5, 6, 7, 8], which not only inherit the properties of VLMs but also extend them by training with robot demonstrations for action prediction. As a result, VLA models demonstrate impressive generalization and precise manipulation, effectively mapping human instructions and visual observations to the robot control signal.\n\nIn the real world, robots must perceive spatial environments, reason about semantic relationships, and interact with dynamic environment configurations. However, most existing VLA models rely primarily on 2D image integration [9, 6], which is fundamentally inadequate for capturing spatial dependencies and modeling physical dynamics.\nOn the one hand, to address these limitations, several studies enhance VLAs with richer multimodal observations. Specifically, some approaches incorporate 3D inputs to improve geometric scene understanding [10, 11, 12], while others introduce tactile signals to capture interaction feedback from manipulated objects [13, 14, 15].\nAlthough these modalities enrich the perceptual capacity of VLA models, they often require modality-specific encoders, which undermines efficiency. Furthermore, without pre-training on multisensory inputs, the large language model (LLM) backbone of VLAs exhibits limited representation to align with the newly introduced multimodal features.\nOn the other hand, several VLA studies attempt to reason about the physical dynamics by predicting future states, such as subgoal images and camera-view depth maps [16, 17, 18, 19].\nHowever, these approaches remain limited in predicting complete point cloud structures and tactile interaction information, which are essential not only for understanding complex, contact-rich scenes but also for effective motion planning in robotic manipulation.\nConsequently, a critical question arises: “How can multisensory modalities be integrated into a unified representation and predicted in their future states to collaboratively enhance VLA models’ physical-world understanding and action generation?”\n\nTo address this question, we propose MLA, a multisensory language–action model that collaboratively processes diverse sensory inputs and predicts their corresponding future states to enhance physical-world modeling for robotic control.\nTo avoid introducing additional modality-specific encoders that lack pretraining alignment with LLM’s embeddings, MLA adopts an encoder-free multimodal alignment mechanism, repurposing the initial transformer blocks of the LLM as a perception module to directly interpret visual, geometric, and tactile cues.\nIn particular, we project 3D points and the spatial positions of the tactile gripper onto 2D image planes using camera parameters, thereby constructing cross-modal positional mappings. These positional correspondences serve as positive pairs for token-level contrastive learning, aligning multimodal features within the LLM’s embedding space.\nThis position-guided consistency constraint enhances the multimodal representations of our MLA model and supports more comprehensive physical-world perception.\nTo further enhance the LLM’s understanding of physical robotic scenes, we propose a future multisensory generation post-training strategy.\nSpecifically, the lightweight transformer-based decoders and tailored generation scheme are designed to process the LLM’s final-layer features and generate the future states of multiple modalities, including 2D images, 3D point clouds, and tactile signals.\nThrough this predictive process, MLA is able to reason about physical dynamics from multiple dimensions, encompassing semantic information, geometric structures, and object-centric interactions.\nNotably, the proposed methods are applied only during training and do not affect MLA’s inference efficiency, while enriching feature conditions for action generation.\n\nSince existing open-source real-world datasets [20, 21, 22] lack multisensory information, we pretrain the LLM solely on large-scale image–action paired datasets following common practice [23, 7], including more than 570K trajectories.\nSubsequently, we perform supervised fine-tuning (SFT) on downstream task datasets using the proposed encoder-free multimodal alignment mechanism, and finally conduct future multisensory generation post-training, progressively equipping our model with the ability to integrate perception, understanding, and action generation from multisensory inputs in the real physical world.\nTo systematically evaluate our model, we design six complex, contact-rich real-world robotic experiments covering both single- and dual-arm manipulation tasks, where MLA achieves SOTA success rates and demonstrates strong generalization to unseen objects and backgrounds. For reproducibility, we further evaluate MLA on the RLBench [24] simulator and also obtain competitive performance. As tactile sensing in simulation is not realistic, we incorporate tactile signals only in real-world experiments. Our contributions are summarized as follows:\n\nWe propose MLA, a multisensory language-action model with an encoder-free multimodal alignment mechanism, repurposing the LLM itself to directly align with and interpret image, point cloud, and tactile cues.\n\nWe propose MLA, a multisensory language-action model with an encoder-free multimodal alignment mechanism, repurposing the LLM itself to directly align with and interpret image, point cloud, and tactile cues.\n\nTo further strengthen MLA’s understanding of physical dynamics, we introduce a future multisensory generation post-training strategy that enables it to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation.\n\nThrough a progressive pipeline of pretraining, SFT, and post-training, MLA achieves SOTA success rates and strong generalization on complex real-world tasks, including both single- and dual-arm manipulation.\n\n1. We propose MLA, a multisensory language-action model with an encoder-free multimodal alignment mechanism, repurposing the LLM itself to directly align with and interpret image, point cloud, and tactile cues.\n\n2. To further strengthen MLA’s understanding of physical dynamics, we introduce a future multisensory generation post-training strategy that enables it to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation.\n\n3. Through a progressive pipeline of pretraining, SFT, and post-training, MLA achieves SOTA success rates and strong generalization on complex real-world tasks, including both single- and dual-arm manipulation.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的VLA模型主要依赖2D图像，无法充分捕捉空间依赖性和物理动态。  \n2. 现有模型缺乏对多种感知模态的统一表示和未来状态的预测能力。  \n\n【提出了什么创新的方法】  \n本文提出了一种多感知语言-动作模型（MLA），通过无编码器的多模态对齐机制，直接利用大型语言模型（LLM）作为感知模块，解释图像、点云和触觉信号。通过构建跨模态位置映射，MLA增强了对物理世界的感知能力。此外，采用未来多感知生成的后训练策略，使得MLA能够从多个维度推理物理动态，最终在复杂的现实任务中实现了12%和24%的性能提升，展现了强大的泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction",
            "authors": "Lujie Yang,Xiaoyu Huang,Zhen Wu,Angjoo Kanazawa,Pieter Abbeel,Carmelo Sferrazza,C. Karen Liu,Rocky Duan,Guanya Shi",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)",
            "comment": "Project website:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.26633",
            "code": "https://omniretarget.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26633",
            "arxiv_html_link": "https://arxiv.org/html/2509.26633v1",
            "abstract": "A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation.\nTo address this, we introduce OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations.\nWe comprehensively evaluate OmniRetarget by retargeting motions from OMOMO [1], LAFAN1 [2], and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines.\nSuch high-quality data enables proprioceptive RL policies to successfully execute long-horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum.\nAll code, retargeted datasets, and trained policies will be publicly released. Result videos can be found at https://omniretarget.github.io",
            "introduction": "The quest to enable humanoid robots to perform complex whole-body scene- and object-interaction tasks has long been constrained by a fundamental data bottleneck. While deep reinforcement learning (RL) has shown remarkable success in robot control, efficient exploration is highly sensitive to reward engineering [3].\nThis challenge is further amplified on humanoids, whose high-dimensional action spaces and complex dynamics make learning natural, expressive behaviors from scratch both difficult and inefficient.\n\nTo address these challenges, imitating human motions offers a powerful alternative for learning whole-body control, especially for complex scene interactions. Human demonstrations capture dynamic coordination, such as lifting objects while walking on uneven terrain, and have been used effectively in animation [4, 5, 6]. A critical challenge arises in robotics: unlike virtual characters, physical humanoids only approximate human morphology, with significant differences in shape, proportion and degrees of freedom. This embodiment gap means that simply adapting human motions is insufficient; it is essential to also adapt their scene interactions to the robot’s specific form to generate usable references.\n\nTo this end, researchers have pursued two main strategies. The first one is teleoperation [7, 8, 9], where only a human operator’s motions are retargeted to control the robot online. This approach leverages the human operator for real-time adaptation, which sidesteps the need for automatic interaction retargeting. However, despite the advantage of online feedback, the method remains labor-intensive and does not scale well for large-scale data generation. The second and more scalable strategy is offline interaction retargeting, which holistically adapts both the human’s motion and their scene interactions to the robot’s specific embodiment.\n\nHowever, most existing retargeting methods [10, 9, 11] fall short in this regard. They predominantly rely on unconstrained or softly-penalized optimization, resulting in implausible motions with artifacts such as foot skating and penetration. More importantly, they do not explicitly consider interaction preservation—i.e., maintaining spatial and contact relationship—in the retargeting formulation, relying instead on simple keypoint matching. Consequently, the resulting references are of lower quality, which in turn complicates the downstream RL policy training [12, 8, 13].\n\nIn this work, we introduce OmniRetarget, an open-source data generation engine that transforms human demonstrations into diverse, high-quality kinematic references for humanoid whole-body control. By modeling spatial and contact relationships between robots, objects, and terrains via an interaction mesh [14], OmniRetarget preserves essential interactions and generates kinematically feasible variations. While existing methods require separate demonstrations for each variation—making data collection costly and limiting coverage—OmniRetarget addresses this bottleneck directly. Inspired by data augmentation frameworks for contact-rich manipulation [15], our framework automatically augments a single demonstration into a large number of training examples across object configurations, shapes, robot embodiments, and environments.\n\nOur pipeline employs constrained optimization to enforce physical feasibility, including collision avoidance, joint limits, and foot contact stability, while minimizing interaction mesh deformation. The resulting motions are interaction-preserving and exhibit only minimal kinematic artifacts, providing dense learning signals that accelerate RL with minimal reward engineering. On a diverse suite of whole-body interaction tasks such as box lifting, platform climbing, and slope crawling, policies trained on OmniRetarget datasets outperform those from prior retargeting methods in both motion quality and robustness, with successful zero-shot sim-to-real transfer onto a physical humanoid robot.\n\nOur contributions are fourfold:\n\nThe first interaction-preserving humanoid retargeting framework that handles rich robot-object-terrain interactions while enforcing hard physical constraints.\n\nA systematic data augmentation pipeline that transforms a single human demonstration into a diverse, large-scale set of high-quality kinematic trajectories on various robot embodiments.\n\nA large-scale, open-source dataset of retargeted, kinematically-feasible loco-manipulation trajectories.\n\nSuccessful zero-shot sim-to-real transfer of proprioceptive RL policies on a physical humanoid, demonstrating a diverse set of scene-interaction tasks, including a long, agile sequence of object carrying, platform climbing, jumping, and rolling.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的retargeting方法在处理人类与机器人之间的身体差异时，产生不切实际的运动表现。  \n2. 现有方法未能有效保留人类与物体及环境的互动，限制了复杂运动的表达能力。  \n\n【提出了什么创新的方法】  \n本研究提出了OmniRetarget，一个基于交互网格的交互保留数据生成引擎，旨在生成高质量的运动轨迹。通过最小化人类和机器人网格之间的拉普拉斯变形，同时强制执行运动学约束，OmniRetarget能够生成运动学上可行的轨迹。该方法有效地从单一演示中生成多样化的数据，涵盖不同的机器人形态、地形和物体配置。经过全面评估，OmniRetarget生成的运动轨迹在运动学约束满足和接触保留方面优于现有基准，成功实现了长达30秒的复杂运动技能，且无需复杂的奖励工程。所有代码和数据集将公开发布。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Graphite: A GPU-Accelerated Mixed-Precision Graph Optimization Framework",
            "authors": "Shishir Gopinath,Karthik Dantu,Steven Y. Ko",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.26581",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26581",
            "arxiv_html_link": "https://arxiv.org/html/2509.26581v1",
            "abstract": "We present Graphite, a GPU-accelerated non-linear graph optimization framework.\nIt provides a CUDA C++ interface to enable the sharing of code between a real-time application, such as a SLAM system, and its optimization tasks. The framework supports techniques to reduce memory usage, including in-place optimization, support for multiple floating point types and mixed-precision modes, and dynamically computed Jacobians.\nWe evaluate Graphite on well-known bundle adjustment problems and find that it achieves similar performance to MegBA, a solver specialized for bundle adjustment, while maintaining generality and using less memory. We also apply Graphite to global visual-inertial bundle adjustment on maps generated from stereo-inertial SLAM datasets, and observe speed ups of up to 59×59\\times compared to a CPU baseline.\nOur results indicate that our solver enables faster large-scale optimization on both desktop and resource-constrained devices.",
            "introduction": "Nonlinear optimization is a key component of many estimation problems in computer vision, graphics, and robotics. It is especially prevalent in keyframe-based simultaneous localization and mapping (SLAM) systems, which construct a representation of the environment while also determining a device’s position and rotation. In particular, ORB-SLAM3 [1], a visual-inertial SLAM system, widely employs nonlinear optimization over hypergraphs for a broad array of frontend and backend tasks across its tracking, local mapping, and loop closing threads.\n\nFor real-time SLAM, it is necessary to perform these optimizations in a timely manner to remain up to date with changes in the device’s state and surroundings. At the same time, the computational time and amount of memory needed to carry out each optimization may increase as the size of the map grows. This may be especially challenging for resource-constrained platforms when on-device SLAM is just one component of a larger system with several concurrent tasks.\n\nPopular nonlinear optimizers [2, 3, 4] take advantage of multi-core CPU architectures, exploiting the inherent parallelism of sparse linear algebraic operations. However, some platforms used for SLAM [5] are equipped with on-board accelerators such as GPUs, which can be used to perform a variety of learning-based perception tasks efficiently, such as feature extraction, depth-estimation, object detection, and place recognition [6]. GPUs excel at these compute-heavy workloads due to their massively parallel architecture, which also makes them well-suited for nonlinear optimization.\n\nTo date, there have been several efforts to speed up existing optimization libraries by identifying and offloading expensive steps to GPUs. More recently, libraries such as Ceres Solver [4] have implemented GPU acceleration for the linear solver step of trust region optimization algorithms. Other work based on g2o [3] focuses on efficiently reducing the size of the linear system to be solved [7] and accelerating numerical differentiation for computing Jacobian matrices [8].\nYet, while it is possible to improve the performance of existing libraries by offloading individual steps to the GPU, it introduces additional overhead from allocating GPU memory and transferring data between steps.\n\nAn alternative approach is to perform the entire optimization on the GPU. Some methods use Python [9] or a domain-specific language (DSL) [10, 11] to allow users to write simple descriptions of nonlinear optimization problems which are then used to automatically generate efficient solvers using complex code transformations.\nOthers leverage machine learning frameworks to support batched optimization [12] and differentiable optimization [13, 14] for training learning-based tasks such as feature extraction and feature matching.\nIn addition, many solvers are specialized for specific classes of problems such as bundle adjustment [15], achieving large speedups.\n\nHowever, existing GPU-accelerated solvers have several limitations which make them challenging to adapt for applications such as SLAM.\nThey often model optimizable variables as simple numeric data types (e.g. a float, double, vector of doubles, etc.) or only provide data types for a specific optimization problem (e.g. a class representing an SE(3) transformation). Meanwhile, SLAM systems may represent variables as complex classes which consist of other classes.\nFor example, an optimizable pose class may consist of multiple SE(3) transformations and cameras [1].\nExisting solvers cannot represent these classes with a vector of numbers or predetermined types because they cannot model their complex behaviours and data dependencies.\nAdditionally, researchers develop real-time SLAM systems in a specific language of their choice, e.g., C++, making it difficult to use DSLs or Python-based optimizers, since they require optimizable data types and functionality to be faithfully reimplemented for another language.\nA third challenge is that GPU memory is often limited and shared between multiple tasks such as image processing and model inference.\nMoreover, existing libraries may demand additional GPU memory for operations on sparse matrices and linear solver workspace allocations [16].\nEven worse, embedded platforms often have no dedicated GPU memory, so the CPU and GPU must compete for the same memory.\n\nIn this paper, we present Graphite, a GPU-accelerated nonlinear optimization framework for estimation problems, which implements several techniques for balancing runtime performance and GPU memory usage, to enable large-scale optimization for desktop and embedded scenarios.\nIt allows for mixed-precision solving using 64-bit, 32-bit, and 16-bit floating-point precisions, enabling faster and more memory-efficient optimization of graphs.\nTo further reduce memory usage, the library supports dynamically computed Jacobians for matrix-free methods, as well as an automatic differentiation method with equivalent memory overhead to analytic differentiation. In addition, the iterative linear solver is aware of the structure of the graph, bypassing explicit sparse matrix formats and their associated memory costs.\nTo support real-time SLAM and odometry, the framework provides a CUDA C++ interface, and uses a batching model which supports in-place optimization,\nmeaning that users can use their own data types, without having to transform them into a solver-compatible format.\nThis avoids memory usage and runtime overhead from unnecessary copying, as well as explicit data transfers on platforms which share CPU and GPU memory.\n\nTo demonstrate the effectiveness and flexibility of our framework, we evaluate our solver on bundle adjustment problems, and find it performs comparably to MegBA [15] while using up to 78% less GPU memory.\nWe also reimplement visual-inertial bundle adjustment on the GPU inside ORB-SLAM3, which can take hundreds of seconds on the CPU for sufficiently large problems, and demonstrate a speed up of up to 59×59\\times on maps generated by processing stereo-inertial SLAM datasets [17], while using under 1 GiB of GPU memory on the desktop. The code is available at\nhttps://github.com/sfu-rsl/graphite.\n\nTo summarize our contributions:\n\nWe design and implement a general mixed-precision optimization framework for on-device estimation problems which performs all major optimization steps on a GPU.\n\nWe design and implement a general mixed-precision optimization framework for on-device estimation problems which performs all major optimization steps on a GPU.\n\nWe apply our framework to global visual-inertial bundle adjustment inside a SLAM system, which consists of 7 types of constraints and 5 types of variables.\n\nWe evaluate our solver across different precisions and differentiation modes, using well-known bundle adjustment and SLAM datasets.\n\n1. We design and implement a general mixed-precision optimization framework for on-device estimation problems which performs all major optimization steps on a GPU.\n\n2. We apply our framework to global visual-inertial bundle adjustment inside a SLAM system, which consists of 7 types of constraints and 5 types of variables.\n\n3. We evaluate our solver across different precisions and differentiation modes, using well-known bundle adjustment and SLAM datasets.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的非线性优化方法在处理大规模SLAM问题时面临性能和内存限制。  \n2. 现有GPU加速的优化库难以适应SLAM系统的复杂数据结构和实时需求。  \n3. 资源受限平台上的实时SLAM系统需要高效的内存管理和计算性能。  \n\n【提出了什么创新的方法】  \n本文提出了Graphite，一个GPU加速的非线性优化框架，旨在解决SLAM和其他估计问题中的性能和内存挑战。该框架支持混合精度计算，动态计算雅可比矩阵，并提供CUDA C++接口以便于与现有应用的集成。通过在GPU上执行所有主要优化步骤，Graphite实现了显著的性能提升，尤其是在处理视觉惯性SLAM中的全局束调整时，速度提升可达59倍，同时显著降低了GPU内存使用。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Radio-based Multi-Robot Odometry and Relative Localization",
            "authors": "Andrés Martínez-Silva,David Alejo,Luis Merino,Fernando Caballero",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.26558",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26558",
            "arxiv_html_link": "https://arxiv.org/html/2509.26558v1",
            "abstract": "Radio-based methods such as Ultra-Wideband (UWB) and RAdio Detection And Ranging (radar), which have traditionally seen limited adoption in robotics, are experiencing a boost in popularity thanks to their robustness to harsh environmental conditions and cluttered environments. This work proposes a multi-robot UGV-UAV localization system that leverages the two technologies with inexpensive and readily-available sensors, such as Inertial Measurement Units (IMUs) and wheel encoders, to estimate the relative position of an aerial robot with respect to a ground robot. The first stage of the system pipeline includes a nonlinear optimization framework to trilaterate the location of the aerial platform based on UWB range data, and a radar pre-processing module with loosely coupled ego-motion estimation which has been adapted for a multi-robot scenario. Then, the pre-processed radar data as well as the relative transformation are fed to a pose-graph optimization framework with odometry and inter-robot constraints. The system, implemented for the Robotic Operating System (ROS 2) with the Ceres optimizer, has been validated in Software-in-the-Loop (SITL) simulations and in a real-world dataset. The proposed relative localization module outperforms state-of-the-art closed-form methods which are less robust to noise. Our SITL environment includes a custom Gazebo plugin for generating realistic UWB measurements modeled after real data. Conveniently, the proposed factor graph formulation makes the system readily extensible to full Simultaneous Localization And Mapping (SLAM). Finally, all the code and experimental data is publicly available to support reproducibility and to serve as a common open dataset for benchmarking.",
            "introduction": "Heterogeneous multi-robot systems have gained increasing popularity in the last few years. Deploying a team of different robots that work cooperatively - i.e. an Unmanned Ground Vehicle (UGV) and an Unmanned Aerial Vehicle (UAV) - can bring many benefits: one robot may be allowed to access locations that are blocked to the other, enabling the team to cover more terrain in less time. One key challenge is the localization of the robots in the same frame. This paper addresses how information sharing between robots via range-based radio sensors in GPS-denied environments potentially enables more robust multi-robot localization.\n\nIn a multi-robot team, each robot should be able to determine its own position and the relative position of the other robot. Although most of the research focuses on vision- or LiDAR-based methods, the unstructured layout of some areas, together with unfavorable environmental conditions such as fog, dust, or poor lighting critically impact the performance of these methods [1, 2]. Radio-based localization, in contrast, is gaining traction in the literature, especially Ultra-Wideband (UWB) and RAdio Detection And Ranging (radar) methods. UWB sensors provide high-frequency range measurements between anchors and tags with centimetric precision. They are also lightweight, cost-effective, and robust to harsh environmental conditions. Standard UWB-based localization systems require the anchors to be installed in the environment for effective trilateration, but this is often impractical. Deploying a team of robots, on the other hand, makes it possible to place the transceivers onboard to obtain inter-robot distances that can be fused with odometry measurements to determine the positions of both robots in real time. On the other hand, radars are able to estimate the position (in the form of range, azimuth and elevation) and radial velocity of detected objects by leveraging the Doppler effect. However, radar data brings a particular set of challenges regarding sparsity, lower angular and spatial resolution, and multimodal noise coming from reflections or signal scattering, among others [3].\n\nWhile there is extensive research on relative transformation estimation based on range measurements, there is very little work on UWB-based heterogeneous multi-robot localization systems that: 1) leverage multiple on-board transceivers for redundancy and enhanced state observability, 2) fuse UWB with radar odometry to localize both robots with respect to a fixed frame, 3) involve robots operating in 2.5D and 3D spaces and 4) can run in real time.\n\nThe main contribution of this paper is a radio-based multi-robot UGV-UAV localization system that comprises three fundamental modules:\n\nA Nonlinear Least Squares (NLS) optimization framework to compute the relative transformation between the odometry frames of a UAV and a UGV based on multiple anchor-tag distance measurements, in real time.\n\nA Nonlinear Least Squares (NLS) optimization framework to compute the relative transformation between the odometry frames of a UAV and a UGV based on multiple anchor-tag distance measurements, in real time.\n\nA pose-graph optimization framework that simultaneously optimizes the poses of both platforms by fusing all available sensor data, including the inter-robot transformation, the pre-processed radar point-clouds and other readily-available sensors such as IMUs or wheel encoders, in real time.\n\nA simulator plugin for Gazebo Harmonic that replicates UWB range measurements based on a measurement model that has been validated against real sensor data. This plugin is compatible with multi-robot setups in the open-source PX4 Software-In-The-Loop (SITL) framework, narrowing the gap between simulation and real experimentation.\n\nThe system was validated in SITL simulations, and in a real-world experiment involving a UAV and a UGV. All the code and the dataset are publicly available 111https://github.com/robotics-upo/mr-radio-localization\n\nThe remainder of the paper is structured as follows: Section II reviews previous work on radio-based localization in multi-robot systems. Section III formalizes our approach by describing the proposed localization system, focusing on our three main contributions. Section IV validates our implementation in simulation and a real dataset. Finally, Section V presents some final remarks.\n\n1. A Nonlinear Least Squares (NLS) optimization framework to compute the relative transformation between the odometry frames of a UAV and a UGV based on multiple anchor-tag distance measurements, in real time.\n\n2. A pose-graph optimization framework that simultaneously optimizes the poses of both platforms by fusing all available sensor data, including the inter-robot transformation, the pre-processed radar point-clouds and other readily-available sensors such as IMUs or wheel encoders, in real time.\n\n3. A simulator plugin for Gazebo Harmonic that replicates UWB range measurements based on a measurement model that has been validated against real sensor data. This plugin is compatible with multi-robot setups in the open-source PX4 Software-In-The-Loop (SITL) framework, narrowing the gap between simulation and real experimentation.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的机器人定位方法在恶劣环境下表现不佳。  \n2. 需要一种新的方法来实现多机器人系统的相对定位。  \n3. 现有的基于视觉或LiDAR的方法在特定环境中受到限制。  \n\n【提出了什么创新的方法】  \n该论文提出了一种基于无线电的多机器人UGV-UAV定位系统，包含三个核心模块：1) 非线性最小二乘优化框架用于实时计算UAV和UGV之间的相对变换；2) 融合多种传感器数据的姿态图优化框架，实时优化两个平台的姿态；3) Gazebo Harmonic模拟器插件，用于基于真实传感器数据验证的UWB范围测量。该系统在软件仿真和实际实验中得到了验证，展示了其在多机器人定位中的有效性和鲁棒性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Memory-Efficient 2D/3D Shape Assembly of Robot Swarms",
            "authors": "Shuoyu Yue,Pengpeng Li,Yang Xu,Kunrui Ze,Xingjian Long,Huazi Cao,Guibin Sun",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.26518",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26518",
            "arxiv_html_link": "https://arxiv.org/html/2509.26518v1",
            "abstract": "Mean-shift-based approaches have recently emerged as the most effective methods for robot swarm shape assembly tasks. These methods rely on image-based representations of target shapes to compute local density gradients and perform mean-shift exploration, which constitute their core mechanism. However, such image representations incur substantial memory overhead, which can become prohibitive for high-resolution or 3D shapes. To overcome this limitation, we propose a memory-efficient tree map representation that hierarchically encodes user-specified shapes and is applicable to both 2D and 3D scenarios. Building on this representation, we design a behavior-based distributed controller that enables assignment-free shape assembly. Comparative 2D and 3D simulations against a state-of-the-art mean-shift algorithm demonstrate one to two orders of magnitude lower memory usage and two to three times faster shape entry while maintaining comparable uniformity. Finally, we validate the framework through physical experiments with 6 to 7 UAVs, confirming its real-world practicality.",
            "introduction": "In nature, insect groups can assemble into functional structures through purely local interactions [1]. For instance, fire ants dynamically build living bridges to span gaps and transport resources, with stability emerging from simple individual rules. Inspired by such collective behaviors, swarm robotics are extensively explored as a means for large groups of robots to self-organize into desired 2D or 3D configurations, enabling applications from cooperative construction to search-and-rescue in complex environments [2, 3, 4, 5, 6, 7].\n\nExisting swarm shape assembly methods can be categorized into assignment-based and assignment-free paradigms. Assignment-based methods allocate robots to target positions either through centralized optimization [8, 9] or through distributed coordination. The latter can be achieved by rule-based methods (e.g., local constraints and negotiation protocols) [10, 11, 12, 13, 14] or learning-based methods [15]. Centralized schemes provide globally consistent solutions but suffer from poor scalability as swarm size increases and are highly vulnerable to individual failures [6]. Distributed schemes address these limitations by decomposing the centralized assignment into multiple local ones, but conflicts can arise from concurrent local decisions and require additional techniques like task swapping [16] to ensure consistency. In contrast, assignment-free approaches avoid explicit goal allocation and instead exploit emergent behaviors, such as edge growth from seed robots [17], morphogenetic pattern formation [18], artificial potential fields [19], and saliency-driven filling [20]. More recently, image-based approaches have been explored, with mean-shift exploration strategies becoming the state-of-the-art solutions for large-scale swarm shape assembly [21, 22]. Advances in distributed relative localization further support their practical deployment [23]. These methods have been shown to outperform earlier assignment-free approaches in coverage speed and robustness in 2D scenarios [21]. However, the reliance on image maps causes memory demand to grow rapidly with resolution, making direct extension to 3D prohibitively expensive.\n\nWhether assignment-based or assignment-free, most existing methods remain confined to 2D assemblies, and scalable 3D extensions remain rare and technically demanding. For aerial swarms, Morgan et al. proposed a distributed auction algorithm with trajectory planning [8], yet it entails substantial memory and communication costs as each robot must store all target points and iteratively exchange bids. Underwater bio-inspired swarms exhibit 3D patterns via visual signaling [24], but the results are coarse, transient behaviors rather than precisely-defined shapes. Other studies focus on physical 3D construction, including layered aerial robotic block assembly [25], self-climbing assembly (FireAnt3D) [26], shape-changing modular tensegrity blocks [27], and snap-together soft robots (SoftSnap) [28]. However, these approaches typically grow buildings sequentially from fixed bases or seed robots, thus differing fundamentally from the free-space shape assembly problem addressed in this study.\n\nIn this letter, we propose a memory-efficient, assignment-free shape assembly framework for large-scale robot swarms applicable to both 2D and 3D settings. Specifically, we first encode the user-specified target shape into a tree map, which substantially reduces memory requirements. Based on this map, we design a behavior-based distributed shape assembly controller. We validate the framework through extensive 2D and 3D simulations, which demonstrate reliable formation of complex shapes. Compared with a state-of-the-art method [21], our approach reduces memory usage by one to two orders of magnitude and improves shape-entering efficiency by a factor of two to three, while maintaining comparable uniformity. Finally, we confirm the practical feasibility of the proposed method through physical experiments using UAVs.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的机器人群体形状组装方法在内存需求上存在显著限制，尤其是在高分辨率或3D形状的情况下。  \n2. 现有方法大多集中在2D组装，缺乏可扩展的3D扩展方案。  \n\n【提出了什么创新的方法】  \n提出了一种内存高效的树图表示方法，用于编码用户指定的形状，适用于2D和3D场景。基于此表示，设计了一种基于行为的分布式控制器，实现无分配的形状组装。通过与最先进的均值漂移算法的比较，展示了该方法在内存使用上减少了一个到两个数量级，形状进入效率提高了两到三倍，同时保持了可比的均匀性。最终，通过使用无人机的物理实验验证了该框架的实际可行性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Learning from Hallucinating Critical Points for Navigation in Dynamic Environments",
            "authors": "Saad Abdul Ghani,Kameron Lee,Xuesu Xiao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.26513",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26513",
            "arxiv_html_link": "https://arxiv.org/html/2509.26513v1",
            "abstract": "Generating large and diverse obstacle datasets to learn motion planning in environments with dynamic obstacles is challenging due to the vast space of possible obstacle trajectories. Inspired by hallucination-based data synthesis approaches, we propose Learning from Hallucinating Critical Points (LfH-CP), a self-supervised framework for creating rich dynamic obstacle datasets based on existing optimal motion plans without requiring expensive expert demonstrations or trial-and-error exploration. LfH-CP factorizes hallucination into two stages: first identifying when and where obstacles must appear in order to result in an optimal motion plan, i.e., the critical points, and then procedurally generating diverse trajectories that pass through these points while avoiding collisions. This factorization avoids generative failures such as mode collapse and ensures coverage of diverse dynamic behaviors. We further introduce a diversity metric to quantify dataset richness and show that LfH-CP produces substantially more varied training data than existing baselines. Experiments in simulation demonstrate that planners trained on LfH-CP datasets achieves higher success rates compared to a prior hallucination method.",
            "introduction": "Dynamic obstacles present a fundamental challenge for autonomous mobile robots, as their trajectories reside in a vast and high-dimensional space. Describing obstacle trajectories with velocity, acceleration, and higher order derivatives adds dimensions to the space, making it difficult for planners to anticipate and respond to complex motion patterns in real time. Consequently, intelligent navigation strategies must account for these dynamics and react in real time to avoid collisions effectively.\n\nLearning-based models have recently demonstrated success in navigating such environments by leveraging collected data [1]. Two dominant paradigms—Imitation Learning (IL) and Reinforcement Learning (RL)—provide structured ways to gather and learn from experience. However, both paradigms face critical limitations: IL requires large numbers of expert demonstrations, while RL demands extensive trial-and-error exploration. Moreover, robust planners depend on training datasets that are sufficiently diverse to capture the variability of dynamic obstacles. As a result, learning-based planning has not performed to the same extent as in vision and language domains, where internet-scale labeled datasets are readily available.\n\nThe Learning from Hallucination (LfH) paradigm [2] provides a compelling solution to this challenge by synthesizing training data from prior navigation experiences. LfH takes past motion plans from simpler, even fully open environments and generate more constrained and complex scenarios, where those prior plans would be optimal. This process, known as “hallucination”, enables safe and inexpensive creation of large datasets without relying on expensive expert demonstrations or extensive trial-and-error exploration. However, prior methods often produce datasets with limited diversity, constraining their applicability and scalability, especially when facing the vast and high-dimensional space of dynamic obstacles [3, 4].\n\nIn this work, we introduce Learning from Hallucinating Critical Points (LfH-CP), a self-supervised framework for generating rich datasets of dynamic obstacle trajectories. LfH-CP  factorizes hallucination into two stages: first identifying the critical points of obstacle trajectories—specific times and locations obstacles must appear in order to result in an optimal motion plan—and then procedurally generating diverse trajectories with varying orders of complexity that pass through these points while avoiding collisions. These generated obstacle trajectories and original motion plans can then be used to train motion planners in a supervised manner.\nTo quantify the richness of the generated datasets, we introduce a diversity metric and show that LfH-CP produces substantially more varied training data than existing methods. We validate these results in simulated experiments on a ground robot, demonstrating that planners trained on LfH-CP datasets achieve superior navigation performance compared to prior hallucination-based baselines.\nIn summary, this work makes the following contributions:\n\nPropose LfH-CP, a self-supervised framework for generating large and diverse datasets of dynamic obstacle trajectories.\n\nIntroduce a diversity metric to quantify dataset richness.\n\nDemonstrate effectiveness of LfH-CP through simulated robot experiments.\n\nRelease code on GitHub to facilitate future research111https://github.com/Saadmaghani/LfH-CP.\n\n1. Propose LfH-CP, a self-supervised framework for generating large and diverse datasets of dynamic obstacle trajectories.\n\n2. Introduce a diversity metric to quantify dataset richness.\n\n3. Demonstrate effectiveness of LfH-CP through simulated robot experiments.\n\n4. Release code on GitHub to facilitate future research111https://github.com/Saadmaghani/LfH-CP.",
            "llm_summary": "【论文的motivation是什么】  \n1. 动态障碍物的轨迹空间庞大，难以生成多样化的训练数据。  \n2. 现有的学习方法如模仿学习和强化学习面临数据需求高和探索成本大的限制。  \n3. 现有的生成方法常常导致数据集多样性不足，限制了其应用和扩展性。  \n\n【提出了什么创新的方法】  \n提出了LfH-CP，一个自监督框架，通过识别关键点和生成多样化的障碍物轨迹来创建丰富的动态障碍物数据集。该方法分为两个阶段：首先识别障碍物出现的关键时刻和位置，然后生成经过这些关键点的多样化轨迹。通过引入多样性度量，LfH-CP显著提高了生成数据集的丰富性。实验结果表明，使用LfH-CP数据集训练的规划器在导航性能上优于先前的基于幻觉的方法。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Analytic Conditions for Differentiable Collision Detection in Trajectory Optimization",
            "authors": "Akshay Jaitly,Devesh K. Jha,Kei Ota,Yuki Shirai",
            "subjects": "Robotics (cs.RO); Computational Geometry (cs.CG)",
            "comment": ". Accepted to the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.26459",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26459",
            "arxiv_html_link": "https://arxiv.org/html/2509.26459v1",
            "abstract": "Optimization-based methods are widely used for computing fast, diverse solutions for complex tasks such as collision-free movement or planning in the presence of contacts. However, most of these methods require enforcing non-penetration constraints between objects, resulting in a non-trivial and computationally expensive problem. This makes the use of optimization-based methods for planning and control challenging. In this paper, we present a method to efficiently enforce non-penetration of sets while performing optimization over their configuration, which is directly applicable to problems like collision-aware trajectory optimization. We introduce novel differentiable conditions with analytic expressions to achieve this. To enforce non-collision between non-smooth bodies using these conditions, we introduce a method to approximate polytopes as smooth semi-algebraic sets. We present several numerical experiments to demonstrate the performance of the proposed method and compare the performance with other baseline methods recently proposed in the literature.",
            "introduction": "Optimization-based approaches present an effective way to generate rich behavior for robots in the presence of various kinds of constraints [1, 2, 3]. These methods are widely used for planning trajectories of multi-body robotic systems in the presence of obstacles, or in the presence of contact constraints [4]. They are also used in various physics engines\n(e.g., MuJoCo [5], Bullet [6], Drake [7], Dojo [8],\nPhysX [9])\nwhere simulation of contact dynamics is performed by first finding the contacts between objects, then solving a constrained optimization problem.\n\nCentral to these planning or simulation problems is the ability to compute a signed distance function between bodies which can later be used for downstream tasks like collision-free trajectory planning or simulating contact dynamics. Computation of distance functions tends to be computationally challenging. Oftentimes, evaluating constraints on collision or distance values is non-differentiable, leading to challenges when solving problems in various applications like computer graphics, robotics, video games, etc.\n\nIn this paper, we present a formulation that allows us to compute an approximate signed distance function between sets, which we denote as ‘Minimum-Offset-To-Touch’ (MOTT) (see Fig. 1). This can be used to enforce non-penetration constraints during trajectory optimization. Our method embeds computation of the signed distance function alongside the trajectory optimization problem. This is in contrast to the popular bi-level formulations, which require explicit calculation of distance at each solver iteration, as shown in Fig. 1. Other existing single-level optimization methods use a complementarity formulation (see [10], for example) to impose distance function constraints during optimization which are, in general, difficult to solve leading to longer solve times. In contrast, MOTT conditions have continuous, smooth gradients, which allow us to enforce constraints on distances between smooth sets with faster compute efficiency. Through several numerical experiments, we present the computational benefits offered by the proposed method.\n\nContributions.\n\nWe introduce a ‘Minimum-Offset-To-Touch’ (MOTT) metric for the signed distance between collision bodies.\n\nMOTT conditions allow us to derive computationally efficient, differentiable nonlinear equality conditions (compared to non-smooth complementarity conditions used in other approaches). These conditions can be embedded in a single-level optimization problem to enforce non-penetration and perform signed distance computation while performing trajectory optimization.\n\nAdditionally, we propose a method for finding smooth semi-algebraic approximations of (non-smooth) polytopic sets, used in conjunction with our MOTT conditions to simplify optimization problems considering non-penetration of polytopic bodies.\n\n1. We introduce a ‘Minimum-Offset-To-Touch’ (MOTT) metric for the signed distance between collision bodies.\n\n2. MOTT conditions allow us to derive computationally efficient, differentiable nonlinear equality conditions (compared to non-smooth complementarity conditions used in other approaches). These conditions can be embedded in a single-level optimization problem to enforce non-penetration and perform signed distance computation while performing trajectory optimization.\n\n3. Additionally, we propose a method for finding smooth semi-algebraic approximations of (non-smooth) polytopic sets, used in conjunction with our MOTT conditions to simplify optimization problems considering non-penetration of polytopic bodies.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的优化方法在处理非穿透约束时计算复杂且耗时。  \n2. 计算距离函数的非光滑性导致在机器人规划和控制中的困难。  \n3. 需要一种有效的方法来在轨迹优化中强制执行非碰撞约束。  \n\n【提出了什么创新的方法】  \n本文提出了一种“最小接触偏移”（MOTT）度量，用于计算碰撞体之间的签名距离。MOTT条件允许我们推导出计算效率高的可微分非线性等式条件，这些条件可以嵌入单层优化问题中，从而在进行轨迹优化时强制执行非穿透约束并计算签名距离。此外，我们还提出了一种方法，用于找到光滑的半代数近似（非光滑）多面体集，这与MOTT条件结合使用，以简化考虑多面体体非穿透的优化问题。通过数值实验，我们展示了所提出方法的计算优势。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Unwinding Rotations Reduces VR Sickness in Nonsimulated Immersive Telepresence",
            "authors": "Filip Kulisiewicz,Basak Sakcak,Evan G. Center,Juho Kalliokoski,Katherine J. Mimnaugh,Steven M. LaValle,Timo Ojala",
            "subjects": "Robotics (cs.RO)",
            "comment": "24th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)",
            "pdf_link": "https://arxiv.org/pdf/2509.26439",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26439",
            "arxiv_html_link": "https://arxiv.org/html/2509.26439v1",
            "abstract": "Immersive telepresence, when a user views the video stream of a 360∘360^{\\circ}\ncamera in a remote environment using a Head Mounted Display (HMD), has great potential to improve the sense of being in a remote environment. In most cases of immersive robotic telepresence, the camera is mounted on a mobile robot which increases the portion of the environment that the remote user can explore.\nHowever, robot motions can induce unpleasant symptoms associated with Virtual Reality (VR) sickness, degrading the overall user experience.\nPrevious research has shown that unwinding the rotations of the robot, that is, decoupling the rotations that the camera undergoes due to robot motions from what is seen by the user, can increase user comfort and reduce VR sickness. However, that work considered a virtual environment and a simulated robot.\nIn this work, to test whether the same hypotheses hold when the video stream from a real camera is used, we carried out a user study (n=36)(n=36) in which the unwinding rotations method was compared against coupled rotations in a task completed through a panoramic camera mounted on a robotic arm. Furthermore, within an inspection task which involved translations and rotations in three dimensions, we tested whether unwinding the robot rotations impacted the performance of users.\nThe results show that the users found the unwinding rotations method to be more comfortable and preferable, and that a reduced level of VR sickness can be achieved without a significant impact on task performance.",
            "introduction": "Immersive robotic telepresence is a way of embodying a physical robot with the use of an HMD.\nTo effectively leverage the immersive potential of an HMD, a robot is equipped with a 360° camera. Such a combination allows a user to look around freely compared to experiencing the remote location through a view from a non-panoramic camera displayed on a 2-D screen. This wider field of regard can increase immersion, or the extent to which the system can support natural sensorimotor contingencies for perception\n[28].\nAlthough higher levels of immersion can improve the user experience, the use of an HMD for robotic telepresence can come with the disadvantage of inducing VR sickness in users.\n\nVR sickness is comprised of a number of uncomfortable sensations that can arise as the result of using an HMD [35].\nIts deleterious consequences have stimulated research on different methods to improve user comfort [1, 2].\nIn the case of immersive robotic telepresence, previous work to decouple the view of the user from the robot rotations has shown promising results in simulation, with an increase in comfort and a decrease in VR sickness when the rotations are unwound [39, 6].\n\nHowever, in previous work researchers evaluated the unwinding rotations method in environments built with Unity 3D and only allowed certain camera movements, that is, planar motion and a single degree of rotational freedom (yaw rotation about an axis perpendicular to the ground) [39] or with two degrees of rotational freedom (yaw rotation and pitch rotation about an axis parallel to the ground) [6]. In this paper, we seek to answer the question of whether the unwinding rotations method is effective when VR sickness is induced in a real-world context, thus greatly increasing its generalizability. Our research improves upon previous work in three important ways. First, we consider a nonsimulated environment using pre-recorded videos of an office environment. VR sickness is often attributed to sensory conflicts [29], and humans have built up much stronger sensory expectations to real environments over their lifetimes than to three-dimensional rendered environments where things may or may not always closely match their real-world counterparts. Second, we consider rigid body transformations composed of translations along and rotations about three-dimensional vectors.\nFinally, we also evaluate whether unwinding rotations can result in an improvement in task performance.\n\nThe main contribution of this paper is a user study in a nonsimulated environment showing that the unwinding rotations method reduces VR sickness, increases the comfort of an immersive experience, and is preferred by users.\nWe performed the user study using a Remote Environment (RE) based on pre-recorded 360∘ videos taken during robot arm movement around a room for an inspection task. Our findings can improve user experience in real-life robot teleoperation, with particular relevance to environments where motion in any direction is possible, such as underwater, in the air, or in outer space.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人运动引起的VR sickness影响用户体验。  \n2. 需要验证在真实环境中“解旋转”方法的有效性。  \n3. 探索如何在不影响任务表现的情况下提高用户舒适度。  \n\n【提出了什么创新的方法】  \n本研究提出了一种“解旋转”方法，通过在真实环境中进行用户研究，比较了解旋转与耦合旋转的效果。研究结果表明，解旋转方法显著降低了VR sickness，提高了用户的舒适度，并且用户对该方法的偏好明显。尽管用户在执行任务时感到更舒适，但任务表现并未受到显著影响。这一发现为实际机器人遥操作中的用户体验改进提供了重要依据，尤其适用于需要多方向运动的环境。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task Planning",
            "authors": "Zichao Shen,Chen Gao,Jiaqi Yuan,Tianchen Zhu,Xingcheng Fu,Qingyun Sun",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.26375",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26375",
            "arxiv_html_link": "https://arxiv.org/html/2509.26375v1",
            "abstract": "Embodied task planning requires agents to produce executable actions in a close-loop manner within the environment. With progressively improving capabilities of LLMs in task decomposition, planning, and generalization, current embodied task planning methods adopt LLM-based architecture.\nHowever, existing LLM-based planners remain limited in three aspects, i.e., fixed planning paradigms, lack of action sequence constraints, and error-agnostic.\nIn this work, we propose Sda-Planner, enabling an adaptive planning paradigm, state-dependency aware and error-aware mechanisms for comprehensive embodied task planning.\nSpecifically, Sda-Planner introduces a State-Dependency Graph to explicitly model action preconditions and effects, guiding the dynamic revision.\nTo handle execution error, it employs an error-adaptive replanning strategy consisting of Error Backtrack and Diagnosis and Adaptive Action SubTree Generation, which locally reconstructs the affected portion of the plan based on the current environment state.\nExperiments demonstrate that Sda-Planner consistently outperforms baselines in success rate and goal completion, particularly under diverse error conditions.",
            "introduction": "Embodied intelligence entails the capacity of agents to perceive, interpret, and act within the environments [1, 2, 3, 4, 5].\nA crucial ability of the embodied agent is task planning, i.e., decomposing high-level natural language instructions into coherent sequences of intermediate, goal-directed, mid-level actions [6].\nIn recent years, Large Language Models (LLMs) [7, 8, 9, 10, 11, 12] have demonstrated strong generalization capabilities across a wide spectrum of tasks.\nOwing to their ability to encode rich, implicit knowledge about the world[13, 14], LLMs have emerged as promising components of embodied agents, particularly for task planning [15].\n\nExisting LLM-based task planners for embodied agents generally fall into two categories (shown in Fig.1):\n(1) Iterative Planner,\nwhich generates one action at a time based on real-time feedback from the environment,\nand (2) Tree Planner [16],\nwhich generates entire static plans using an LLM and constructs an action tree for execution.\nDespite their promise, these paradigms exhibit several notable limitations:\n❶ Fixed Planning Paradigms:\nIterative Planners prompt an LLM to generate one action at a time, which heavily relies on repeated interactions between LLMs and the environment. Such a redundant and fixed paradigm results in high time and token costs, making the process inefficient.\nConversely, Tree Planner prompts the LLM only once at the initial stage to generate the entire candidate plan, i.e., the action tree. Then, it constrains subsequent trajectory search within the fixed structure of the action tree, limiting adaptability to new information or errors during execution.\n❷ Lack action sequence constraints:\nBoth the Iterative and Tree Planners typically treat actions as isolated steps, without explicitly modeling dependencies between them.\nAs a result, agents may attempt actions like “place tomato” without satisfying preconditions such as “pick up tomato”. Iterative Planner reacts step-by-step but lacks a global view to enforce such constraints, while Tree Planner reasons on the generated action tree and cannot make targeted adaptations if earlier actions invalidate later steps.\n❸ Error-agnostic:\nThe encountered errors during execution can be broadly categorized into two types:\n(1) Environment State Errors, which stem from a mismatch between the agent’s assumptions and the actual environment state.\n(2) Action Precondition Errors, which arise due to internal flaws in the plan itself and often result from implicit or unmodeled dependencies between actions [17].\nSuch errors typically reflect missing or violated preconditions that were not properly accounted for during the original planning phase.\nNeither planner can adjust the plan according to different errors.\nFor example, if the action “pick up tomato” fails, Iterative Planner repeatedly attempts without re-evaluating. Tree Planner can only search for another path in the fixed action tree. Thus, both planners are error-agnostic and cannot detect/correct the root error.\n\nTo overcome these limitations, we propose State-Dependency  Aware Adaptive Planner (Sda-Planner), a novel framework designed to enable error-aware and adaptive embodied task planning.\nSda-Planner is built upon three key components:\nFirst, the State-Dependency Graph Generation module explicitly models action-state dependencies, enforcing structural constraints to ensure actions in the reconstructed subsequence are only executed when their preconditions are satisfied.\nSecond, the Error Backtrack and Diagnosis module enables structured and targeted error handling by identifying root causes (e.g., unmet preconditions) and isolating the minimal action subsequence requiring revision, avoiding full-plan regeneration.\nThird, the Adaptive Action SubTree Generation module reconstructs the affected subsequence using current environment context and constraints from the dependency graph, enabling efficient and localized replanning.\nTogether, these components enable Sda-Planner to reason over complex state-action dependencies, differentiate between diverse error types, and dynamically adjust plans at a fine-grained level.\nWe conduct experiments on the ALFRED benchmark and Sda-Planner achieves a superior success rate and goal-condition success rate compared to existing fixed-paradigm and error-agnostic planners, highlighting its adaptability in embodied task planning.\nOur contributions are as follows:\n\nWe propose Sda-Planner, a novel planning framework for embodied agents that integrates state dependency modeling with error-aware replanning, enabling robust task execution in environments.\n\nWe design a state-aware and error-specific replanning mechanism that leverages a state-dependency graph for modeling action preconditions, and performs localized replan through error-aware diagnosis and adaptive action subtree generation.\n\nExtensive experiments demonstrate that Sda-Planner outperforms strong baselines in both task success and goal completion under various execution error scenarios.\n\n1. We propose Sda-Planner, a novel planning framework for embodied agents that integrates state dependency modeling with error-aware replanning, enabling robust task execution in environments.\n\n2. We design a state-aware and error-specific replanning mechanism that leverages a state-dependency graph for modeling action preconditions, and performs localized replan through error-aware diagnosis and adaptive action subtree generation.\n\n3. Extensive experiments demonstrate that Sda-Planner outperforms strong baselines in both task success and goal completion under various execution error scenarios.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有LLM-based planners在固定规划范式下效率低下。  \n2. 缺乏对动作序列的约束，导致执行错误。  \n3. 现有方法对执行错误缺乏敏感性，无法进行有效调整。  \n\n【提出了什么创新的方法】  \n提出了Sda-Planner，一个集成状态依赖建模与错误感知重规划的框架。该方法通过生成状态依赖图来明确建模动作前提条件，并利用错误回溯与诊断机制处理执行错误。最后，采用自适应动作子树生成模块进行局部重规划。实验表明，Sda-Planner在成功率和目标完成率上显著优于现有方法，尤其在多样化错误条件下表现出色。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Kinodynamic Motion Planning for Mobile Robot Navigation across Inconsistent World Models",
            "authors": "Eric R. Damm,Thomas M. Howard",
            "subjects": "Robotics (cs.RO)",
            "comment": "Presented at the Robotics: Science and Systems (RSS) 2025 Workshop on Resilient Off-road Autonomous Robotics (ROAR)",
            "pdf_link": "https://arxiv.org/pdf/2509.26339",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26339",
            "arxiv_html_link": "https://arxiv.org/html/2509.26339v1",
            "abstract": "Mobile ground robots lacking prior knowledge of an environment must rely on sensor data to develop a model of their surroundings.\nIn these scenarios, consistent identification of obstacles and terrain features can be difficult due to noise and algorithmic shortcomings, which can make it difficult for motion planning systems to generate safe motions.\nOne particular difficulty to overcome is when regions of the cost map switch between being marked as obstacles and free space through successive planning cycles.\nOne potential solution to this, which we refer to as Valid in Every Hypothesis (VEH), is for the planning system to plan motions that are guaranteed to be safe through a history of world models.\nAnother approach is to track a history of world models, and adjust node costs according to the potential penalty of needing to reroute around previously hazardous areas.\nThis work discusses three major iterations on this idea.\nThe first iteration, called Per-Edge Hypothesis (PEH), invokes a sub-search for every node expansion that crosses through a divergence point in the world models.\nThe second and third iterations, called Goal-Edge Hypothesis (GEH) and Goal-Edge Graph Revision Hypothesis (GEGRH) respectively, defer the sub-search until after an edge expands into the goal region.\nGEGRH uses an additional step to revise the graph based on divergent nodes in each world.\nInitial results showed that, although PEH and GEH find more optimistic solutions than VEH, they are unable to generate solutions in less than one-second, which exceeds our requirements for field deployment.\nAnalysis of results from a field experiment in an unstructured, off-road environment on a Clearpath Robotics Warthog Unmanned Ground Vehicle (UGV) indicate that GEGRH finds lower cost trajectories and has faster average planning times than VEH.\nCompared to single-hypothesis (SH) search, where only the latest world model is considered, GEGRH generates more conservative plans with a small increase in average planning time.",
            "introduction": "Robots navigating through unstructured, partially observed environments are often tasked with generating their own models of the world during traversal.\nThese models are used to inform motion planners of environmental hazards, so they can generate safe trajectories.\nTraditionally, motion planners that operate to or beyond the perception horizon build a graph in which edge costs are influenced by the most recent world map.\nSearch algorithms such as A∗ [8] or Anytime Repairing A∗ (ARA∗) [12] are then used to generate feasible motions.\nBy relying on the most recent world representation, the planner attempts to ensure that its output reflects the most accurate approximation of the environment.\n\nIn practice, these maps are subject to uncertainty from sensor noise, occlusions, state estimation errors, and incomplete observations, particularly in complex and partially observable environments [11].\nThis uncertainty results in inconsistencies between successive world models, undermining the assumption that the most recent map is the most accurate.\nThe differences in these maps lead to inconsistent plan generation from deterministic motion planners.\nThis effect was highlighted during a field test on a Clearpath Robotics Unmanned Ground Vehicle (UGV).\nFigure 1 is an illustration from the experiment that shows the variation in perception output by overlaying three successive world maps [14].\nEven in near-field regions where sensor confidence is typically higher, inconsistencies exist in what is classified as lethal in the cost maps.\nThese inconsistencies introduce deviations between successive planner outputs, leading to undesirable behaviors such as abrupt trajectory changes or oscillations around obstacles.\nThis oscillatory behavior, also shown in Figure 1, is described in a topological sense in [15], where trajectories oscillate back and forth due to changes in the cost map.\nSuch oscillations arise when the planner repeatedly generates trajectories that diverge from previous plans, leading to instability in robot motion.\n\nThe planner used for this work leverages the Kinodynamic Efficiently Adaptive State Lattice (KEASL) search space with heuristic based search [3].\nFigure 1 shows the oscillatory plans generated by KEASL.\nThis behavior, where alternating, topologically distinct trajectories (shown in magenta) are generated through the most recent world map, is due to differences in the successive inputs to the motion planner.\nThe deterministic properties of KEASL mean the outputs are predictable, and the behaviors can be explained algorithmically.\nAdditionally, the discrete environment representations mean there is a consistent output with a consistent input.\nThe oscillatory problems arise because the opposite can be true, where noisy inputs tend to result in noisy outputs.\nStochastic processes can take the uncertainty into account, but often require training models or carefully tuning parameters.\nPartially Observable Markov Decision Processes (POMDP), for example, can handle uncertainty in observations, but require updating a policy for each new map.\n\nSince mapping uncertainty can significantly impact planning performance, we propose a more robust approach to edge cost computation that uses multiple temporally sampled environment maps.\nBy considering prior perception decisions and integrating them into the search process, the planner can reason about prior world observations and make more informed decisions.\nWe characterize each successive world map as an evolving hypothesis about the true state of the environment.\nDivergence points between these hypotheses correspond to regions where the perception system’s interpretation of the environment has changed, often leading to inconsistencies in the plans generated by motion planning algorithms.\nEach map in Figure 1 can be viewed as the accumulation of prior hypotheses, each modifying previous interpretations of the environment.\nIn this work, the most recent map is treated as the primary hypothesis, where there cannot be any collisions in the final solution.\nThe additional hypotheses are used to guide the search process.\nFigure 2 shows the output from our multi-hypothesis planning methodology with the same environment map representations as in Figure 1.\nThe Single-Hypothesis (SH) plan (magenta) oscillates around a central clustering of obstacles, while the multi-hypothesis plan (blue) remains consistently to one side.\nWhile our approach does not completely eliminate this behavior, it mitigates the impact by making decisions with consideration of prior world observations.\n\nThis work presents two main contributions aimed at addressing the challenges posed by perceptual uncertainty in the environment representation:\n\nA search algorithm for mobile robot motion planning in off-road environments that considers multiple temporally sampled world model hypotheses when computing edge costs.\n\nAn analysis of the algorithm’s performance with experiments conducted on data collected from field-testing on a UGV in an unstructured, off-road environment.",
            "llm_summary": "【论文的motivation是什么】  \n1. 移动机器人在缺乏环境先验知识时，依赖传感器数据构建周围环境模型。  \n2. 传感器噪声和算法缺陷导致障碍物和地形特征的识别不一致，影响运动规划的安全性。  \n3. 成功的运动规划需要处理多个世界模型之间的差异，避免不稳定的轨迹生成。  \n\n【提出了什么创新的方法】  \n本研究提出了一种多假设规划方法，通过考虑多个时间采样的环境模型来计算边缘成本。首先，使用Per-Edge Hypothesis (PEH)进行节点扩展，随后通过Goal-Edge Hypothesis (GEH)和Goal-Edge Graph Revision Hypothesis (GEGRH)对目标区域进行优化。GEGRH在生成路径时能更好地处理感知不确定性，最终在不规则的离路环境中实现了更低的成本轨迹和更快的规划时间。实验结果表明，GEGRH在规划时间上优于传统的单假设方法，同时生成的计划更为保守。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration and Search",
            "authors": "Ruiyang Wang,Haolun Tsu,David Hunt,Shaocheng Luo,Jiwoo Kim,Miroslav Pajic",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.26324",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26324",
            "arxiv_html_link": "https://arxiv.org/html/2509.26324v1",
            "abstract": "Autonomous exploration and object search in unknown indoor environments remain challenging for multi-robot systems (MRS). Traditional approaches often rely on greedy frontier assignment strategies with limited inter-robot coordination. In this work, we introduce LLM-MCoX (LLM-based Multi-robot Coordinated Exploration and Search), a novel framework that leverages Large Language Models (LLMs) for intelligent coordination of both homogeneous and heterogeneous robot teams\ntasked with efficient exploration and target object search.\nOur approach combines real-time LiDAR scan processing for frontier cluster extraction and doorway detection with multimodal LLM reasoning (e.g., GPT-4o) to generate coordinated waypoint assignments based on shared environment maps and robot states. LLM-MCoX demonstrates superior performance compared to existing methods, including greedy and Voronoi-based planners, achieving 22.7% faster exploration times and 50% improved search efficiency in large environments with 6 robots. Notably, LLM-MCoX enables natural language-based object search capabilities, allowing human operators to provide high-level semantic guidance that traditional algorithms cannot interpret.",
            "introduction": "Coordinated exploration and object search remain a fundamental challenge for multi-robot systems (MRS), particularly in unknown and dynamic environments such as disaster zones, industrial facilities, and subterranean caves [1, 2, 3]. Achieving efficient, scalable exploration with MRS requires sophisticated planning strategies that effectively balance local sensing, task allocation, and global coordination.\n\nClassical sampling-based planners, such as Rapidly-exploring Random Trees (RRT) [4], have been extensively used in robotic exploration. While effective for rapid deployment, they often result in redundant or inefficient trajectories, particularly in cluttered or expansive environments [5, 6]. Frontier-based exploration has emerged as a more information-driven alternative, guiding robots toward the boundary between known and adjacent unknown regions to maximize new information gain. However, most implementations rely on greedy heuristic assignments, typically selecting frontiers based solely on proximity or estimated local utility, without considering global coordination or workload balancing. Moreover, aggressive filtering of small frontier clusters [7, 8] may cause robots to miss narrow passages or less prominent regions, leading to incomplete coverage and suboptimal exploration performance.\n\nRecent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) offer promising solutions to address these limitations, due to remarkable capabilities in multimodal reasoning, contextual understanding, and high-level decision-making across various robotics applications [9, 10, 11]. LLMs excel particularly in interpreting spatial layouts, processing high-level natural language descriptions, and decomposing abstract objectives into executable steps. These capabilities make them well-suited for MRS coordination, especially in tasks that require both spatial efficiency and semantic understanding that traditional geometric or heuristic planners struggle to integrate effectively. Unlike existing approaches that rely on greedy, single-agent strategies [12] or decentralized coordination with limited global awareness [13], LLMs offer the potential for globally informed and adaptive planning.\n\nConsequently, this work introduces LLM-MCoX (LLM-based Multi-robot Coordinated Exploration and Search), a novel framework that leverages a pre-trained multimodal LLM (e.g., GPT-4o) as a centralized high-level planner for efficient multi-robot exploration and object search in unknown environments. LLM-MCoX integrates both structured spatial information (e.g., extracted frontiers and doorways from a shared LiDAR-based occupancy map) and unstructured semantic cues (e.g., natural language hints) to generate meaningful waypoint sequences for each robot. LLM-MCoX processes the global occupancy map as a grayscale image input to LLM, enabling sophisticated visual spatial reasoning.\n\nA key advantage of LLM-MCoX is its ability to incorporate semantic guidance. For instance, when given an instruction such as the object is likely at the far end of the main corridor, the LLM can jointly reason over the spatial layout and instruction to prioritize relevant areas. This enables context-aware and adaptive planning that significantly extends beyond the capabilities of traditional heuristic or geometry-driven approaches, effectively bridging robotic perception and human-like semantic reasoning.\n\nWe evaluate LLM-MCoX in both structured (indoor buildings with straight walls and well-defined room layouts) and unstructured (irregular corridors and random orientations typical of natural caves or disaster zones) environments.\n\nIn summary, the main contributions of this work are:\n\nA sampling-based method to efficiently detect representative frontiers and potential doorways from a shared LiDAR-based occupancy map, enabling efficient and semantically meaningful exploration.\n\nA centralized coordination approach that leverages an LLM to reason over shared spatial and semantic information, and assign informative waypoints to each robot.\n\nWe evaluate LLM-MCoX against three existing approaches, including greedy waypoint assignments with two distinctive frontier selection strategies and Dynamic Voronoi Cells (DVCs) based waypoint assignments, and demonstrate its effectiveness in multi-robot exploration.\n\nWe show that LLM-MCoX uniquely supports language-based object search, significantly outperforming traditional strategies when high-level prior knowledge is available in both simulation and real-world experiments.\n\n1. A sampling-based method to efficiently detect representative frontiers and potential doorways from a shared LiDAR-based occupancy map, enabling efficient and semantically meaningful exploration.\n\n2. A centralized coordination approach that leverages an LLM to reason over shared spatial and semantic information, and assign informative waypoints to each robot.\n\n3. We evaluate LLM-MCoX against three existing approaches, including greedy waypoint assignments with two distinctive frontier selection strategies and Dynamic Voronoi Cells (DVCs) based waypoint assignments, and demonstrate its effectiveness in multi-robot exploration.\n\n4. We show that LLM-MCoX uniquely supports language-based object search, significantly outperforming traditional strategies when high-level prior knowledge is available in both simulation and real-world experiments.",
            "llm_summary": "【论文的motivation是什么】  \n1. 多机器人系统在未知室内环境中的自主探索和物体搜索仍然具有挑战性。  \n2. 传统方法依赖于贪婪的前沿分配策略，缺乏有效的机器人间协调。  \n\n【提出了什么创新的方法】  \n提出了LLM-MCoX框架，利用大型语言模型（LLM）进行多机器人协调探索和搜索。该方法结合实时LiDAR扫描处理和多模态LLM推理，生成基于共享环境地图和机器人状态的协调路径分配。LLM-MCoX在六个机器人的大型环境中实现了22.7%的探索时间提升和50%的搜索效率改善，支持自然语言的物体搜索能力，使人类操作者能够提供传统算法无法解释的高层语义指导。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Anomaly detection for generic failure monitoring in robotic assembly, screwing and manipulation",
            "authors": "Niklas Grambow,Lisa-Marie Fenner,Felipe Kempkes,Philip Hotz,Dingyuan Wan,Jörg Krüger,Kevin Haninger",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.26308",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26308",
            "arxiv_html_link": "https://arxiv.org/html/2509.26308v1",
            "abstract": "Out-of-distribution states in robot manipulation often lead to unpredictable robot behavior or task failure, limiting success rates and increasing risk of damage. Anomaly detection (AD) can identify deviations from expected patterns in data, which can be used to trigger failsafe behaviors and recovery strategies.\nPrior work has applied data-driven AD on time series data for specific robotic tasks, however the transferability of an AD approach between different robot control strategies and task types has not been shown. Leveraging time series data, such as force/torque signals, allows to directly capture robot–environment interactions, crucial for manipulation and online failure detection. Their broad availability, high sampling rates, and low dimensionality enable high temporal resolution and efficient processing.\nAs robotic tasks can have widely signal characteristics and requirements, AD methods which can be applied in the same way to a wide range of tasks is needed, ideally with good data efficiency.\nWe examine three industrial robotic tasks, each presenting several anomalies. Test scenarios in robotic cabling, screwing, and sanding are built, and multi-modal time series data is gathered. Several autoencoder-based methods are compared, and we evaluate the generalization across different robotic tasks and control methods (diffusion policy-, position-, and impedance-controlled). This allows us to validate the integration of AD in complex tasks involving tighter tolerances and variation from both the robot and its environment. Additionally, we evaluate data efficiency, detection latency, and task characteristics which support robust detection. The results indicate reliable detection with AUROC exceeding 0.93 in failures in the cabling and screwing task, such as incorrect or misaligned parts and obstructed targets. In the polishing task, only severe failures were reliably detected, while more subtle failure types remained undetected.",
            "introduction": "As robots are applied to tasks with less structure, variation in the environment, task, or robot often leads to novel, out-of-distribution states. The behavior of the robot - both classical programs and machine learning (ML)-based - in such situations is typically unknown.\nTo avoid robots acting in novel and potentially dangerous situations, anomalies can be detected [1], e.g. triggering a human intervention or failsafe behaviors.\nFor broad deployment of AD in robotic manipulation, it is desired that the same approach can be applied to a variety of tasks and robot control strategies. Furthermore, an understanding of what properties of failures allow their detection can help predict how effective AD will be.\nAnomaly detection in robotic manipulation has been shown in specific tasks, e.g. pick and place applications, validating the ability to detect task- and robot-related deviations in force/torque signals, motor currents, and task state [1, 2]. However, [2] indicate that AD in contact is less accurate, and feasibility studies as well as datasets for contact-rich robotic tasks remain limited.\n\nIn contrast to previous work [3, 2, 1, 4] that focuses solely on analyzing the general feasibility for AD in specific robotic tasks, we evaluate applicability across diverse contact-rich robotic tasks: plug insertion, screwing and polishing.\nUnlike [2, 1, 5], our dataset incorporates subtle failures characterized by low-magnitude force deviations. Moreover, the dataset features multiple robot control strategies to effectively manage disturbances likely to happen in real world applications, with more variation in process time and robot trajectory.\nBased on this dataset, we exclusively benchmark data-driven AD approaches including latency and factors impacting the detection performance, such as data volume or model configuration.\nWe demonstrate their potential for online execution, paving the way to integrate with recovery behaviors or consider within planning processes in the future.\n\nIn Section II related approaches and relevant models are introduced, then in Section III the AD framework is described including the experimental scenarios. Finally in Section IV the experimental results are presented and discussed.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人在操作中面临未知的、分布外的状态，导致不可预测的行为或任务失败。  \n2. 现有的异常检测方法在不同机器人控制策略和任务类型之间的可转移性尚未得到验证。  \n3. 需要一种适用于多种任务的异常检测方法，以提高数据效率和检测可靠性。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于自编码器的异常检测框架，旨在通过多模态时间序列数据（如力/扭矩信号）来捕捉机器人与环境的交互。通过在机器人布线、拧紧和打磨等三种工业任务中进行测试，评估了不同控制策略下的异常检测能力。结果表明，在布线和拧紧任务中，异常检测的AUROC超过0.93，显示出对错误或不对齐部件的可靠检测。然而，在打磨任务中，仅能检测到严重故障，而较微妙的故障类型未被检测到。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "ISyHand: A Dexterous Multi-finger Robot Hand with an Articulated Palm",
            "authors": "Benjamin A. Richardson,Felix Grüninger,Lukas Mack,Joerg Stueckler,Katherine J. Kuchenbecker",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted at IEEE Humanoids 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.26236",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26236",
            "arxiv_html_link": "https://arxiv.org/html/2509.26236v1",
            "abstract": "The rapid increase in the development of humanoid robots and customized manufacturing solutions has brought dexterous manipulation to the forefront of modern robotics.\nOver the past decade, several expensive dexterous hands have come to market, but advances in hardware design, particularly in servo motors and 3D printing, have recently facilitated an explosion of cheaper open-source hands.\nMost hands are anthropomorphic to allow use of standard human tools, and attempts to increase dexterity often sacrifice anthropomorphism.\nWe introduce the open-source ISyHand (pronounced easy-hand), a highly dexterous, low-cost, easy-to-manufacture, on-joint servo-driven robot hand. Our hand uses off-the-shelf Dynamixel motors, fasteners, and 3D-printed parts, can be assembled within four hours, and has a total material cost of about 1,300 USD. The ISyHand’s unique articulated-palm design increases overall dexterity with only a modest sacrifice in anthropomorphism. To demonstrate the utility of the articulated palm, we use reinforcement learning in simulation to train the hand to perform a classical in-hand manipulation task: cube reorientation.\nOur novel, systematic experiments show that the simulated ISyHand outperforms the two most comparable hands in early training phases, that all three perform similarly well after policy convergence, and that the ISyHand significantly outperforms a fixed-palm version of its own design.\nAdditionally, we deploy a policy trained on cube reorientation on the real hand, demonstrating its ability to perform real-world dexterous manipulation.",
            "introduction": "The dexterity, strength, robustness, and tactile sensing of the human hand are crucial to the human ability to perceive, manipulate, and use objects. A person can firmly squeeze and operate a heavy tool in one hand while the other gently holds a delicate object. This vast range of capabilities is crucial to the human ability to physically interact with objects and accomplish tasks across a wide array of environments. Although reproducing the aforementioned traits in a robot hand is challenging, human-like dexterous manipulation will be necessary if robots are to become more ubiquitous and generally capable in human-centered environments.\n\nSome work has pushed simple grippers, such as parallel jaw or underactuated multi-finger grippers, to their manipulation limits [1, 2, 3, 4, 5], but these grippers are fundamentally limited in their dexterity. Parallel jaw grippers can perform pick-and-place tasks, pinch laundry and fold it on a hard surface [4], and even perform simple object rotations around a single axis [3, 5]. Underactuated multi-fingered grippers typically excel at grasping by conforming to the shape of a grasped object, but they lack the dexterity for robust manipulation [1, 2]. The most dexterous grippers are multi-finger hands with fully actuated or almost fully actuated degrees of freedom (DoF).\n\nHigh-DoF multi-finger hands are more dexterous, but this capability comes at the cost of needing many individual actuators to move each joint independently and coordinate finger movements for more complex manipulation tasks. Several different actuation mechanisms have been implemented on multi-fingered hands; these are primarily tendon-driven [6, 7] and on-joint servo-driven [8, 9, 10, 11, 12], although pneumatically actuated hands have also been developed [13, 14].\nTendon-driven hands provide certain advantages: they are similar in size to a human hand, can move very quickly, and can generate high torque at certain joints by using multiple tendons. However, they are typically quite complex to manufacture and maintain, and most of the actuators need to be located outside the hand, typically inside a large forearm. They can also be more difficult to control because of the interactions between tendons, the need to calibrate motor positions, and their complex forward dynamics.\nPneumatically driven hands have similar drawbacks, with large wrists for valves, intricate tubing to direct airflow, and likely an external cabinet to supply pressurized air. They can also be difficult to control, requiring precise valves to accurately control pressure, and their actuation can be noisy.\nOn-joint servo-driven hands offer a balanced approach to design and control. While typically larger than tendon-based and pneumatic hands, these hands are much simpler to manufacture, maintain, and repair. Additionally, they are far easier to control; each joint can simply be commanded to a desired position. Servo-driven hands therefore seem to be the most accessible option for the research community.\n\nTwo of the most prominent servo-driven multi-finger robot hands are the Allegro hand and the LEAP hand [10], both of which are similar in size to our ISyHand (see Figs. 1, 2).\nThe servo-driven, four-fingered, 16-DoF Allegro hand is a popular tool for robotic in-hand manipulation [15, 16] likely because it is easy to use and not prohibitively expensive (17,000 USD).\nHowever, the custom closed-source hardware makes it difficult to repair, and hardware failures have been repeatedly reported [10, 15], although the new version of the hand uses more robust motors.\nThe Tilburg hand is very similar to the Allegro Hand, though less expensive (5,170 USD) [17]. However, it is still closed-source.\nThe LEAP hand has four fingers with 16 DoF and addresses some of the shortcomings of the Allegro hand by introducing a novel kinematic mechanism in the fingers that greatly increases finger dexterity when the hand is open or closed. Because it is fully open-sourced and can be assembled from off-the-shelf components, it can be built and repaired by a larger group of non-experts. Additionally, it is substantially less expensive, with a cost of about 2,000 USD. However, the finger kinematics make teleoperation awkward because they are highly non-anthropomorphic. Additionally, it is unclear if the LEAP hand excels at other manipulation tasks. Finally, because it uses off-the-shelf connectors to link the motors, the design is not easy to modify or customize.\n\nWe propose the ISyHand (pronounced easy-hand), a robust, dexterous, 18-DoF robot hand (Fig. 1). Like the LEAP hand, the ISyHand is inexpensive and made with off-the-shelf and 3D-printed components. It takes approximately four hours to assemble with a material cost of approximately 1,300 USD. Because its body and linkages are 3D-printed, our hand can easily be repaired and broken parts replaced. The ISyHand uses a unique linkage design that routes and houses cabling along the sides of the fingers, reducing wear and risk of damage. Additionally, we introduce a new kinematic mechanism, a 2-DoF articulated palm, that substantially increases the dexterity of the ISyHand while allowing us to maintain anthropomorphism in the finger joints.\n\nBesides evaluating the low-level kinematic and dynamic properties of a robot hand and its actuators (joint repeatability/endurance, thumb opposability, finger manipulability ellipsoids, and grasp strength),\nrelated work also demonstrates more complex manipulation tasks to highlight possible applications for particular robot hands.\nFor example, manipulation dexterity has often been demonstrated by manually teleoperating the robot hand to grasp and interact with a variety of everyday objects [18, 7, 10].\nWhen recorded, these demonstrations can be used to distill autonomous manipulation policies through imitation learning that can solve the demonstrated tasks.\nChristoph et al. [7] use teleoperated robot hand trajectories of a pick-and-place task to train an imitating policy that is later evaluated in repeated task execution over seven hours to showcase the real-world durability and repeatability of their robot hand.\nShaw et al. [10] pretrain manipulation policies for the LEAP hand with imitation learning on internet video data and finetune them with trajectories originating from the Allegro hand; in particular, they train for different pick, place, rotate, and push tasks and evaluate the ratio of successful task executions over multiple trials for both hands.\n\nSince collecting demonstrations for imitation learning is tedious, reinforcement learning (RL) can instead be deployed to train a policy from scratch in simulation and later transfer it to the real robot.\nShaw et al. [10] train an RL policy to rotate a cube in-hand around the vertical z-axis for both the LEAP and the Allegro hands in simulation and transfer the LEAP hand’s policy onto the real system.\nSimilarly, Christoph et al. [7] train a policy to rotate a tennis ball in-hand.\nRecent work demonstrates impressive RL policies for in-hand reorientation of objects, such as a medium-sized cube [15], cylinders [16], and various small items [19], all transferred to the real Allegro hand. While the aforementioned approaches lead to impressive showcases of dexterous manipulation, they mainly reflect the capabilities of the specific robot hand in combination with the particular machine-learning method.\nThere is often no systematic evaluation of how the proposed contributions (e.g., new mechanical design features) influence a hand’s performance in complex object-manipulation tasks in comparison to existing robots hands.\n\nWe propose a systematic approach to evaluate the performance of our new robot hand ISyHand against the Allegro and LEAP hands in the prevalent in-hand cube reorientation task.\nSpecifically, for each robot hand we train for a fixed number of steps a set of RL policies that covers a grid of possible initial and target cube positions (see Section III-A).\nThis strategy enables us to systematically discover the best spatial cube manipulation location for each hand (and thus report the actual best performance for each hand) in early training phases and discover how manipulation performance varies across each hand’s workspace. We further train the best policies for much longer to compare training progress and convergence.\nImportantly, we perform an ablation study of our robot hand without its articulated palm joints, called ISyHand (flat). Our grid evaluation allows us to demonstrate that palm articulation confers a substantial performance advantage for the in-hand cube reorientation task in early training phases. The open-source hand design and code can be found at isyhand.is.mpg.de.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的多指机器人手在灵活性和人类工具使用之间存在权衡。  \n2. 需要一种低成本且易于制造的机器人手，以提高人类中心环境中的操作能力。  \n3. 现有的机器人手在复杂物体操作任务中的表现缺乏系统性评估。  \n\n【提出了什么创新的方法】  \n本研究提出了一种名为ISyHand的开源多指机器人手，采用独特的2-DoF关节掌设计，显著提高了灵活性并保持了人类手的形态。通过在模拟环境中使用强化学习训练ISyHand进行立方体重新定向任务，结果表明ISyHand在早期训练阶段的表现优于其他对比手，并且在真实环境中成功实现了灵巧操作。该手的设计和代码均为开源，便于研究社区使用和改进。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Terrain-Awared LiDAR-Inertial Odometry for Legged-Wheel Robots Based on Radial Basis Function Approximation",
            "authors": "Yizhe Liu,Han Zhang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.26222",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26222",
            "arxiv_html_link": "https://arxiv.org/html/2509.26222v1",
            "abstract": "An accurate odometry is essential for legged-wheel robots operating in unstructured terrains such as bumpy roads and staircases. Existing methods often suffer from pose drift due to their ignorance of terrain geometry.\nWe propose a terrain-awared LiDAR-Inertial odometry (LIO) framework that approximates the terrain using Radial Basis Functions (RBF) whose centers are adaptively selected and weights are recursively updated.\nThe resulting smooth terrain manifold enables “soft constraints” that regularize the odometry optimization and mitigates the zz-axis pose drift under abrupt elevation changes during robot’s maneuver. To ensure the LIO’s real-time performance, we further evaluate the RBF-related terms and calculate the inverse of the sparse kernel matrix with GPU parallelization.\nExperiments on unstructured terrains demonstrate that our method achieves higher localization accuracy than the state-of-the-art baselines, especially in the scenarios that have continuous height changes or sparse features when abrupt height changes occur.",
            "introduction": "Legged-wheel robots combine the speed advantage of wheeled robots with the terrain adaptability advantage of legged robots. Thus, they are well-suited for traversing complex and uneven environments such as bumpy roads, staircases, etc.\nHowever, the uneven surface in these environments will cause impulsive velocity variations during the robot’s maneuver.\nSuch jolts will pose significant challenges to the odometry systems, resulting in pose estimation drifts, particularly in the zz-axis.\nTherefore, the terrain geometry needs to be carefully taken into account when designing the odometry module.\nEarlier studies usually model the terrain as discrete 2.5-D elevation maps. In particular, they capture the overall elevations in a mesh, but ignore the small elevation changes within each grid. Indeed, setting a smaller grid resolution can depict more details\nbut 2.5-D elevation maps are not spatially continuous and hence not spatially differentiable. Thus, it cannot be used directly in the optimization process of an odometry framework and we need to consider a different terrain representation.\n\nTo this end, we use Radial Basis Functions (RBF) to approximate the terrain.\nCompared to 2.5-D elevation maps, it provides a smooth representation for the terrain based on LiDAR point clouds.\nBased on the moment conditions, we further adopt a recursive ridge regression in Kalman filter style to update the RBF weights as new data arrives.\nHence as the robot moves, the new observations will be continuously fused into the terrain approximation.\n\nNow that the RBF-based terrain approximation forms a smooth manifold whose gradient can be explicitly stated, we further introduce “soft constraints” based on the approximated terrain manifold into the optimization of the LiDAR-Inertial odometry (LIO) framework.\nConsequently, the “soft constraints” augments the scan matching objective function with a dedicated cost term to build a more robust odometry.\nIn particular, when abrupt height changes occur, the manifold constraint can anchor the robot’s vertical pose to the approximated terrain surface and reduce zz-axis pose drift.\nTo further improve the real-time performance of our LIO framework, both the RBF-related term evaluation and the sparse kernel matrix inversion are implemented in CUDA-based GPU kernels.\n\nTo summarize, the main contribution of this work is three-fold:\n\nWe approximate the uneven terrains with RBF functions. This gives a smooth terrain approximation whose gradient can be explicitly stated.\nMoreover, based on the moment conditions, a recursive ridge regression is used to update the RBF weights. Furthermore, we use an adaptive RBF center selection strategy and GPU parallelization to accelerate the computation.\n\nWe construct “soft constraints” based on the approximated terrain and introduce them into the LIO optimization to suppress the vertical drift. In addition, we implement the RBF-related terms evaluation using GPU parallelization within the scan matching optimization module.\n\nExperiments have been conducted to test the performance of our proposed LIO framework. Consequently, we outperform ROLO-SLAM [1] and FAST-LIO2 [2] on uneven terrains, particularly in the scenarios that have continuous height changes or sparse features when abrupt height changes occur.\nIn addition, we also release the dataset 111Available at https://zhanghan-tc.github.io/legged_wheel_dataset. collected in this study, which provides a resource for evaluating the SLAM dedicated for legged-wheel robots.\n\n1. We approximate the uneven terrains with RBF functions. This gives a smooth terrain approximation whose gradient can be explicitly stated.\nMoreover, based on the moment conditions, a recursive ridge regression is used to update the RBF weights. Furthermore, we use an adaptive RBF center selection strategy and GPU parallelization to accelerate the computation.\n\n2. We construct “soft constraints” based on the approximated terrain and introduce them into the LIO optimization to suppress the vertical drift. In addition, we implement the RBF-related terms evaluation using GPU parallelization within the scan matching optimization module.\n\n3. Experiments have been conducted to test the performance of our proposed LIO framework. Consequently, we outperform ROLO-SLAM [1] and FAST-LIO2 [2] on uneven terrains, particularly in the scenarios that have continuous height changes or sparse features when abrupt height changes occur.\nIn addition, we also release the dataset 111Available at https://zhanghan-tc.github.io/legged_wheel_dataset. collected in this study, which provides a resource for evaluating the SLAM dedicated for legged-wheel robots.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的里程计方法在不规则地形下容易出现姿态漂移。  \n2. 需要考虑地形几何特征以提高里程计的准确性。  \n\n【提出了什么创新的方法】  \n提出了一种基于径向基函数（RBF）近似地形的LiDAR-惯性里程计（LIO）框架，通过自适应选择RBF中心和递归更新权重，形成平滑的地形流形。引入“软约束”以增强优化过程，减少在急剧高度变化时的zz轴姿态漂移。通过GPU并行化实现实时性能，实验结果显示该方法在不规则地形上优于现有基线，特别是在高度变化连续或特征稀疏的场景中。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Side Scan Sonar-based SLAM for Autonomous Algae Farm Monitoring",
            "authors": "Julian Valdez,Ignacio Torroba,John Folkesson,Ivan Stenius",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.26121",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26121",
            "arxiv_html_link": "https://arxiv.org/html/2509.26121v1",
            "abstract": "The transition of seaweed farming to an alternative food source on an industrial scale relies on automating its processes through smart farming, equivalent to land agriculture. Key to this process are autonomous underwater vehicles (AUVs) via their capacity to automate crop and structural inspections. However, the current bottleneck for their deployment is ensuring safe navigation within farms, which requires an accurate, online estimate of the AUV pose and map of the infrastructure.\nTo enable this, we propose an efficient side scan sonar-based (SSS) simultaneous localization and mapping (SLAM) framework that exploits the geometry of kelp farms via modeling structural ropes in the back-end as sequences of individual landmarks from each SSS ping detection, instead of combining detections into elongated representations.\nOur method outperforms state of the art solutions in hardware in the loop (HIL) experiments on a real AUV survey in a kelp farm. The framework and dataset can be found at https://github.com/julRusVal/sss_farm_slam.",
            "introduction": "Algae has a large potential as a future food source as well as a raw material for fossil-free products [1]. Several types of seaweed are relatively easy to cultivate and there is a considerable amount of suitable area for it along our coastlines. However, for it to become a competitive resource, its production must be scaled up efficiently by means of so-called smart farming. While agriculture on land has seen an enormous increase in automation recently through the use of drones [2] and ground robots [3], marine farming poses several distinctive challenges to the deployment of AUVs [4].\n\nAfter initially laying out the juvenile algae on ropes between moored buoys (see top of Fig. 1 for an aerial view of an algae farm), marine farming only requires monitoring for several months while the algae grows to harvesting length.\nFarmers need to periodically assess the health and growth of the algae as well as the structural status of the farms, which are prone by design to be disrupted by meteorological events or strong currents. Currently, monitoring operations require a vessel, divers, and crew. AUV’s have been proposed in the literature as a tool to reduce the associated risks and costs of such tasks [5].\nHowever, the cost-effective deployment of AUVs in algae farms imposes constraints both in the size of the vehicles that can navigate among the seaweed lines and their value. Smaller, more affordable AUV models provide the higher maneuverability required but at the expense of experiencing a larger degradation in their pose estimate over time. Such navigational drift is currently the main bottleneck for the safe deployment of AUVs in algae farms [4].\n\nSolutions involving installing underwater acoustic beacons, such as ultra short baseline (USBL) [6], result in increasing costs and complexity. Alternatively, SLAM techniques have proved successful at bounding the AUVs localization drift by means of exteroceptive measurements [7].\nThus, in this paper we propose a graph-SLAM method for AUVs equipped with a SSS that utilizes the sonar measurements to both construct a structural model of the farm (top of Fig. 1) and provide an accurate pose estimate of the vehicle online (bottom of Fig. 1) for safe navigation.\n\nOur approach exploits the geometry of the farm lines by dealing with the data association problem over rope detections implicitly in the back-end, instead of through feature-matching methods in the front-end. Instead of trying to associate corresponding single rope segments from SSS beams during a loop closure or to a line-shaped prior of the rope, which are unconstrained problems, our approach models each rope segment detection as independent landmarks.\nBy associating a loose rope prior with high uncertainty along the ropes’ directions to each individual new landmark, ”sliding” edges are added to the graph. These edges constrain the vehicle’s estimate laterally with respect to the ropes while allowing dead reckoning (DR) constraints to anchor the optimization along the direction of the lines. Our experiments show that the resulting sparsely-connected graph of the farm can be optimized incrementally in real time through smoothing techniques such as iSAM2 [8].\n\nWe compare our method against the state of the art techniques presented in [7] and [9], in which measurements arising from the same landmarks are associated in the front-end, to show how our treatment of the measurements on the back-end results in a very simple front-end and very sparsely-connected graphs. We evaluate our approach in a sea trial in terms of mapping quality and AUV trajectory estimates.",
            "llm_summary": "【论文的motivation是什么】  \n1. 当前AUV在海藻农场的安全导航受限于定位和地图估计的准确性。  \n2. 现有的SLAM技术在处理海底结构时存在数据关联问题，影响了导航性能。  \n3. 需要一种高效的方法来提高AUV在海藻农场的定位精度和地图构建能力。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于侧扫声纳的图形SLAM方法，利用声纳测量构建海藻农场的结构模型，并在线提供AUV的准确位姿估计。该方法通过在后端处理数据关联问题，将每个绳索段检测建模为独立的地标，简化了前端处理。通过在海试中进行评估，结果显示该方法在地图质量和AUV轨迹估计方面优于现有技术，能够有效减少定位漂移，提高导航安全性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Evolutionary Continuous Adaptive RL-Powered Co-Design for Humanoid Chin-Up Performance",
            "authors": "Tianyi Jin,Melya Boukheddimi,Rohit Kumar,Gabriele Fadini,Frank Kirchner",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.26082",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26082",
            "arxiv_html_link": "https://arxiv.org/html/2509.26082v1",
            "abstract": "Humanoid robots have seen significant advancements in both design and control, with a growing emphasis on integrating these aspects to enhance overall performance. Traditionally, robot design has followed a sequential process, where control algorithms are developed after the hardware is finalized. However, this can be myopic and prevent robots to fully exploit their hardware capabilities. Recent approaches advocate for co-design, optimizing both design and control in parallel to maximize robotic capabilities.\nThis paper presents the Evolutionary Continuous Adaptive RL-based Co-Design (EA-CoRL) framework, which combines reinforcement learning (RL) with evolutionary strategies to enable continuous adaptation of the control policy to the hardware. EA-CoRL comprises two key components: Design Evolution, which explores the hardware choices using an evolutionary algorithm to identify efficient configurations, and Policy Continuous Adaptation, which fine-tunes a task-specific control policy across evolving designs to maximize performance rewards.\nWe evaluate EA-CoRL by co-designing the actuators (gear ratios) and control policy of the RH5 humanoid for a highly dynamic chin-up task, previously unfeasible due to actuator limitations. Comparative results against state-of-the-art RL-based co-design methods show that EA-CoRL achieves higher fitness score and broader design space exploration, highlighting the critical role of continuous policy adaptation in robot co-design.",
            "introduction": "The field of humanoid robotics is evolving rapidly, yet robot design still faces significant challenges, especially for demanding tasks. These include lifting heavy loads, executing fast and repetitive assembly operations, supporting physical rehabilitation, and mastering highly dynamic movements such as jumping, back-flips, and parkour.\nRelevant literature shows active but also limited humanoid performance for laborious tasks. Past work has dealt with the design of robots capable of heavy-duty operations.\n[1] simulates a set of rescue-relevant tasks on the humanoid COMAN to evaluate joint compliance, but is limited by simplified model and idealized dynamics in the simulator.\n[2] presents DLR humanoid TORO pushing a 50kg table based on a whole-body balancing controller, but ignores the real robot actuators’ limit.\n\nOne of the latest results, [3], introduces a framework that combines lower and upper body reinforcement learning (RL) agents to achieve robust humanoid loco-manipulation tasks with adaptive force control, while respecting actuator limits. While the work is impressive, demonstrating highly dynamic capabilities such as an 82 kg cart-pulling motion, it is limited to improving control only after the robot has been built. The design of the robot model itself is not addressed.\n[4] proposes a push control strategy on a 120kg cabinet for the CENTAURO robot, yet the approach relies on its centaur-type leg design contacting the wall as support.\n[5] utilizes the Unitree H1 model to simulate a benchmark of whole-body manipulation tasks using RL. However, the overall performance presented in this work for high-effort tasks, such as heavy package moving and power lifting, remains very sub-optimal with low task rewards.\nTraditionally, robotic design has followed a sequential approach: first, the system is built, then tested, and subsequently refined through multiple iterations. This process designing, building, testing, and re-designing relies heavily on the intuition and expertise of mechanical designers, with the shortcomings of being often sub-optimal.\nRecent research has increasingly focused on co-design methodologies. The aim is to integrate control and hardware in the design process, enabling a synergistic approach where mechanical and control aspects are co-optimized. This approach seeks to optimize the robot’s structure for its intended goals, behaviors, or motion capabilities.\n\nSeveral co-design strategies have emerged in recent years. One of the earliest approaches involves optimal control (OC) based co-design strategies. For instance, [6] presents a framework where the design is optimized to achieve dynamic motion while minimizing energy consumption in a quadruped robot.\nIn [7], a co-design approach based on OC is developed to achieve an optimized design guided by ergonomic criteria during interactions with various agents. Simulation results using humanoid models demonstrate a reduction in energy consumption.\nAnother approach is the bi-level kineto-static formulation of co-design used in [8], which optimizes a generic manipulator to enhance its range of motion.\n[9] focuses on the co-design of a parallel belt-driven manipulator, incorporating the constraints of the belt’s parallel coupling into the robot’s dynamics and optimizing its transmission ratios.\nAdditionally, [10] focuses on the design optimization of a robotic prosthesis based on its variable stiffness actuators (VSA), demonstrating improved performance and highlighting the potential of co-design applications in VSA-based systems.\nAlthough these methods achieve their intended objectives, they exhibit notable limitations. They are extremely dependent on the underlying model and their applicability falls short in the case of environmental disturbances (e.g., damping, friction, sensor noise and delays), and system uncertainties. This lack of robustness limits their applicability to real-world scenarios and their ability to generalize to model or environment variations. Consequently, the motions generated through OC remain highly dependent on the underlying models, making them vulnerable to deviations from idealized conditions.\n\nReinforcement learning presents a promising alternative by offering a model-free control approach that enhances adaptability and robustness. In co-design, RL leverages domain randomization [11], to expose policies to diverse conditions, improving their resilience to model inaccuracies and environmental uncertainties. This could make RL-based co-design potentially more robust and suitable for real-world deployment compared to methods based on OC [12]. However, RL policies are generally closely linked to the design parameters with which they were trained, requiring adaptation or retraining when the design changes. This coupling of policy and design highlights the need for methods that can generalize over design variations by efficiently adapting policies to new designs.\nA notable contribution in the domain of RL-based co-design is [13], which introduces a model-free meta-RL co-design framework implemented on a quadruped locomotion task. This approach successfully adapts to random velocity commands on diverse rough terrains and has been validated on real-world systems.\nIn [14], the authors co-optimize a parallel elastic joint for quadrupedal robot locomotion.\nSimilarly, [15] addresses highly dynamic quadruped parkour motion by proposing a novel pre-training fine-tuning co-design algorithm that ensures time efficiency while integrating optimization strategies for different robotic configurations.\nFurthermore, [16] combines RL with evolutionary algorithms to achieve structural co-design for a lightweight bipedal robot, optimizing its design specifically for the gait.\nThese advances highlight the growing potential of RL-driven co-design, offering novel pathways for developing robotic co-design platforms beyond conventional techniques [17].\nWhile previous works have predominantly focused on optimizing link lengths within robotic systems, the design space remains vast, requiring more comprehensive studies that consider various other design aspects.\nAnother aspect is to focus on developing more versatile and generalized learning-based co-design frameworks that can be applied to a broader range of robotic systems, ensuring minimal modifications to existing architectures.\n\nIn this work, we propose a generic formalization of an Evolutionary Continuous Adaptive RL-based Co-Design algorithm, referred to in this paper as ”EA-CoRL”. This framework enables RL-based robotic co-design and is validated through a case-study of a whole-body humanoid robot executing a highly dynamic chin-up motion.\nThe main contributions of this work are as follows:\n\nA novel RL-based framework (EA-CoRL) for the co-design of both robot model and control policies.\n\nEA-CoRL integrates a continuous adaptive co-design process that enables broader exploration of the design space, while enhancing performance consistency and reducing the chances of premature convergence compared to a baseline approach.\n\nDevelopment of a model-agnostic humanoid chin-up policy, used as a case study to validate the co-design capabilities for whole-body humanoid motion.\n\nDemonstration that a high-effort task, previously unfeasible due to actuator limitations, can be achieved without significant hardware modifications, through co-design of motor gear ratios and a RL policy optimized on hardware. This approach broadens the scope of co-design while minimizing hardware costs.\n\nSection II presents the methodology of the Evolutionary Continuous Adaptive RL-based Co-Design approach.\nSection III describes the implementation details of the experimental setup used to evaluate the approach.\nSection IV presents the experimental results along with a comparative analysis and discussion.\nFinally, Section V summarizes the key findings and outlines potential future research directions.\n\n1. A novel RL-based framework (EA-CoRL) for the co-design of both robot model and control policies.\n\n2. EA-CoRL integrates a continuous adaptive co-design process that enables broader exploration of the design space, while enhancing performance consistency and reducing the chances of premature convergence compared to a baseline approach.\n\n3. Development of a model-agnostic humanoid chin-up policy, used as a case study to validate the co-design capabilities for whole-body humanoid motion.\n\n4. Demonstration that a high-effort task, previously unfeasible due to actuator limitations, can be achieved without significant hardware modifications, through co-design of motor gear ratios and a RL policy optimized on hardware. This approach broadens the scope of co-design while minimizing hardware costs.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的机器人设计流程过于线性，限制了机器人充分利用硬件能力。  \n2. 现有的RL方法在设计变化时需要重新适应或重新训练，缺乏灵活性。  \n3. 需要一种能够同时优化设计和控制策略的框架，以提高机器人的性能和适应性。  \n\n【提出了什么创新的方法】  \n本文提出了“进化连续自适应RL驱动的共同设计框架”（EA-CoRL），该框架结合了强化学习和进化策略，支持对控制策略和硬件设计的持续适应。EA-CoRL的关键组件包括设计进化和策略连续适应，前者通过进化算法探索硬件配置，后者则针对不断变化的设计微调控制策略。通过对RH5人形机器人进行动态的引体向上任务的共同设计，EA-CoRL显著提高了适应性和性能，展示了在没有重大硬件修改的情况下完成高强度任务的潜力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Conflict-Based Search and Prioritized Planning for Multi-Agent Path Finding Among Movable Obstacles",
            "authors": "Shaoli Hu,Shizhe Zhao,Zhongqiang Ren",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.26050",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26050",
            "arxiv_html_link": "https://arxiv.org/html/2509.26050v1",
            "abstract": "This paper investigates Multi-Agent Path Finding Among Movable Obstacles (M-PAMO), which seeks collision-free paths for multiple agents from their start to goal locations among static and movable obstacles.\nM-PAMO arises in logistics and warehouses where mobile robots are among unexpected movable objects.\nAlthough Multi-Agent Path Finding (MAPF) and single-agent Path planning Among Movable Obstacles (PAMO) were both studied, M-PAMO remains under-explored.\nMovable obstacles lead to new fundamental challenges as the state space, which includes both agents and movable obstacles, grows exponentially with respect to the number of agents and movable obstacles.\nIn particular, movable obstacles often closely couple agents together spatially and temporally.\nThis paper makes a first attempt to adapt and fuse the popular Conflict-Based Search (CBS) and Prioritized Planning (PP) for MAPF, and a recent single-agent PAMO planner called PAMO*, together to address M-PAMO.\nWe compare their performance with up to 20 agents and hundreds of movable obstacles, and show the pros and cons of these approaches.",
            "introduction": "Multi-Agent Path Finding (MAPF) seeks collision-free paths for multiple agents among static obstacles from their start to goal locations while minimizing their arrival times.\nThis paper considers a variant called Multi-agent Path finding Among Movable Obstacles (M-PAMO), where some of the obstacles can be pushed by the agents.\nWhen there are no movable obstacles, M-PAMO becomes MAPF.\nMAPF is NP-hard [1] and so is M-PAMO.\nWhen there is only one agent, M-PAMO becomes the Path planning Among Movable Obstacles (PAMO) [2, 3, 4].\nM-PAMO arises in logistics and warehouses where the passages are blocked by unexpected movable items or shelves [5, 6].\n\nAlthough MAPF [1, 7] and PAMO [2, 3, 4] were both studied, we are not aware of any study on M-PAMO.\nThe introduction of movable obstacles (referred to as objects or boxes hereafter) into MAPF leads to new fundamental challenges, and makes some of the popular techniques for MAPF less effective.\nThe movable obstacles lead to a much larger state space that grows exponentially with respect to the number of both the agents and the objects.\nIn particular, these objects often closely couple the agents together spatially and temporally (Fig. 1): for example, an object that was pushed to location vv by agent ii at time tt may block the only path of agent jj at time t′t^{\\prime} that is much later than tt, forcing the planner to consider different ways of interaction among agents i,ji,j and the objects, to find this only solution.\n\nAs a first attempt, this paper limits its focus to Conflict-Based Search (CBS) [7] and prioritized planning (PP) [8], two popular methods for MAPF, and investigates how to generalize them to handle M-PAMO.\nCBS represents the workspace as a graph and employs a two-level search to handle MAPF, where the low-level addresses single-agent planning and the high-level resolves agent-agent conflicts.\nCBS begins by planning an individually optimal path for each agent while ignoring all other agents, and then detects collisions among the agents along their paths.\nFor a conflict between agents i,ji,j, the high-level of CBS resolves it by adding constraints to either ii or jj and invokes the low-level to replan agent ii’s or jj’s path to satisfy the added constraints.\nCBS repeats this process, alternating between high-level and low-level, until a set of conflict-free paths is found.\n\nWe propose two variants of CBS for M-PAMO, where the first one CBS-MOH (Movable Obstacles on High-level) addresses the objects only at its high-level, while the second one CBS-MOL also modifies the low-level to handle the objects.\nSpecifically, CBS-MOH ignores all objects at its low-level and can leverage any existing single-agent planners used in CBS.\nCBS-MOH handles objects by introducing new types of conflicts and constraints into the search to describe the objects, and adds those constraints to the agents when conflicts of agent-agent, agent-object, object-object and object-environment are detected.\nIn contrast, CBS-MOL further considers the objects at its low-level, where each agent is always planned among all objects.\nFor this purpose, we extend a recent planner for (single-agent) PAMO called PAMO* [2] to plan in space-time, and name the resulting planner ST-PAMO*, which is used as the low-level of CBS-MOL.\nFinally, with ST-PAMO*, we propose a prioritized planner PP-PAMO*, which assigns fixed priority among the agents, plans agents with higher priority at first, and treats them as dynamic obstacles for the lower prioritized agents.\nIn addition, when planning for the agents with lower priority, the lower-prioritized agents should never push objects to block the planned path of agents with higher priority.\n\nAlthough CBS is guaranteed to find an optimal solution for MAPF, we show that our CBS-MOH and CBS-MOL are not guaranteed to find optimal solutions for M-PAMO.\nWe compare all three proposed algorithms in various maps with up to 20 agents and hundreds of objects.\nWe show examples where one approach fails while others succeed, and point out future research directions.\n\nMAPF algorithms fall on a spectrum from coupled [9] to decoupled [10], trading off completeness and optimality for scalability.\nIn the middle of this spectrum lies the popular dynamically coupled methods such as the popular Conflict-Based Search (CBS) [7], which has been extended and improved in many ways [11, 12, 13, 14].\nSome recent work considers modifiable environments where the shelves in the warehouse can be rearranged when needed before planning paths for the agents [5] or when some agents have no tasks [6], which is different from M-PAMO.\n\nPAMO was formulated and studied in different ways, such as in a grid [3], with polygonal obstacles [15], known environments [4], and unknown environments [16], using search [4], sampling [17], and learning [18] methods.\nHowever, none of them provide completeness or solution quality guarantees.\nIn these problems, the difficulty is that the state space includes not only the robot position but also the objects’ positions, and the dimensionality of the state space grows exponentially as the number of objects increases.\nA recent paper [2] presents an efficient planning approach called PAMO*, which has completeness and optimality guarantees, based on the fact that most of the objects are intact during planning, which thus helps limit the actual state space being explored during planning.",
            "llm_summary": "【论文的motivation是什么】  \n1. M-PAMO remains under-explored despite its relevance in logistics and warehouses.  \n2. Movable obstacles introduce new fundamental challenges, complicating the state space for pathfinding.  \n3. Existing MAPF and PAMO techniques are less effective in the presence of movable obstacles.  \n\n【提出了什么创新的方法】  \n本文提出了两种变体的Conflict-Based Search (CBS)方法，分别为CBS-MOH和CBS-MOL，以解决M-PAMO问题。CBS-MOH在高层处理移动障碍，而低层使用现有的单一代理规划器；CBS-MOL则在低层同时考虑移动障碍，使用扩展的PAMO*规划器ST-PAMO*。通过比较这三种算法在多达20个代理和数百个障碍物的不同地图上的表现，展示了各自的优缺点，并指出了未来研究方向。尽管CBS保证找到MAPF的最优解，但我们的CBS-MOH和CBS-MOL不保证找到M-PAMO的最优解。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "S$^3$E: Self-Supervised State Estimation for Radar-Inertial System",
            "authors": "Shengpeng Wang,Yulong Xie,Qing Liao,Wei Wang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25984",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25984",
            "arxiv_html_link": "https://arxiv.org/html/2509.25984v1",
            "abstract": "Millimeter-wave radar for state estimation is gaining significant attention for its affordability and reliability in harsh conditions. Existing localization solutions typically rely on post-processed radar point clouds as landmark points. Nonetheless, the inherent sparsity of radar point clouds, ghost points from multi-path effects, and limited angle resolution in single-chirp radar severely degrade state estimation performance. To address these issues, we propose S3E, a Self-Supervised State Estimator that employs more richly informative radar signal spectra to bypass sparse points and fuses complementary inertial information to achieve accurate localization. S3E fully explores the association between exteroceptive radar and proprioceptive inertial sensor to achieve complementary benefits. To deal with limited angle resolution, we introduce a novel cross-fusion technique that enhances spatial structure information by exploiting subtle rotational shift correlations across heterogeneous data. The experimental results demonstrate our method achieves robust and accurate performance without relying on localization ground truth supervision. To the best of our knowledge, this is the first attempt to achieve state estimation by fusing radar spectra and inertial data in a complementary self-supervised manner.",
            "introduction": "State estimation techniques have garnered widespread attention as crucial facilitators in cutting-edge applications, including autonomous driving [36], indoor localization [27], AR/VR [16, 33], Robotics Navigation [15, 32], and more. Currently, optical sensors such as LiDARs [23] and cameras [26] are considered mainstream sensors for state estimation and external perception. Numerous studies based on these sensors yield highly satisfactory outcomes under ideal conditions. However, optical sensors are not very practical in harsh conditions e.g. dust, fog, rain, snow, haze. In contrast, the millimeter-wave (mmWave) radar, operating longer wavelengths, affords robust measurements unaffected by minuscule particles [20]. Furthermore, mmWave radar can furnish relative radial velocity information for reflected objects, thereby indicating ego-motion and object mobility details. Leveraging these advantages, mmWave radar emerges as a promising catalyst for state estimation in diverse weather conditions.\n\nExisting radar-based studies [2, 39] predominantly employ the Constant False Alarm Rate (CFAR) detector to extract point clouds with Doppler velocities from the processed radar data cube, which encompasses Range-Azimuth-Doppler (RAD) information. On this basis, some studies decouple the relative transformation between two consecutive keyframes through aligning co-observed landmarks [7]. However, multiple objects of different sizes disrupt the independence among CFAR training cells, diminish the probability of detection, and result in missed detections [9]. Additionally, high-intensity cells caused by signal multi-path effects can elevate the chances of false positives, giving rise to “ghost points”. Consequently, these sparse and flawed point clouds struggle with reliable data association, posing a significant challenge for scan-matching techniques. Other studies solve ego velocity from stationary radar points with Doppler velocities [19, 10]. However, the ghost points and dynamic targets without spatial consistency will introduce inaccurate ego velocity estimation, degrading localization performance.\n\nOur motivation is to employ more richly informative Range-Azimuth Spectra (RAS) to bypass sparse points, and then leverage the complementary perception capabilities of the Radar-Inertial System (RIS) to attain robust state estimation. Specifically, as shown in Fig. 1, mmWave radar supplies exteroceptive information for IMU to compensate for motion cumulative drift. The proprioceptive IMU provides inertial data in kinetics to distill landmarks with motion consistency. Moreover, we observe that the quantity of principal energy moving between adjacent RAS depends on the rotational component of the motion transformation matrix. As shown in Fig. 2, we take the maximum power along the azimuth for a Pow-Azimuth curve. It is worth noting that linearly translating the kk-th curve by the motion angle yields its peak position coinciding exactly with that of the (k+1k+1)-th curve. This makes rotation estimation more explicit.\n\nAlbeit inspiring, translating this intuition into a practical and reliable state estimator is non-trivial and encounters significant issues: 1) Fusion Incompatibility. Though RAS make rotation more explicit, unlike point clouds with clear kinematic indicators, radar with inertial data cannot be directly embedded into existing fusion frameworks (e.g. Recursive Gaussian Filter, probabilistic factor graph optimization) due to the absence of explicit observation constraint between RAS and state factors. 2) Limited angle resolution. Commercial single-chip radars are typically equipped with a limited number of antennas, making it difficult to extract reliable landmarks from RAS.\n\nTo address the above issues, we present S3E, the first state estimator that fuses radar signal spectrum and inertial data. S3E fully explores the kinematic association between RAS and inertial data to achieve complementary benefits without localization ground truth. As for the limited angle resolution of radar. we propose a novel cross-fusion technique to enhance the spatial structure information by injecting inertial information into RAS. To sum up, our primary contributions are evident in the following aspects.\n\nWe propose a novel self-supervised state estimator, which leverages the complementary perception capabilities of RIS to attain accurate state estimation. To the best of our knowledge, this is the first attempt to achieve state estimation and landmark extraction from radar signal spectra and inertial data in a self-supervised manner.\n\nWe propose a novel Rotation-based cross-fusion to effectively preserve motion-consistent features and enhance the spatial structure across adjacent spectra.\n\nExperimental results demonstrate that our method outperforms the previous state of the arts and achieves complementary benefits across exteroceptive radar and proprioceptive inertial data.\n\n1. We propose a novel self-supervised state estimator, which leverages the complementary perception capabilities of RIS to attain accurate state estimation. To the best of our knowledge, this is the first attempt to achieve state estimation and landmark extraction from radar signal spectra and inertial data in a self-supervised manner.\n\n2. We propose a novel Rotation-based cross-fusion to effectively preserve motion-consistent features and enhance the spatial structure across adjacent spectra.\n\n3. Experimental results demonstrate that our method outperforms the previous state of the arts and achieves complementary benefits across exteroceptive radar and proprioceptive inertial data.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的雷达点云稀疏性和多路径效应导致状态估计性能下降。  \n2. 需要融合雷达和惯性数据以实现更准确的状态估计。  \n3. 现有的融合框架无法直接处理雷达信号谱和惯性数据的兼容性问题。  \n\n【提出了什么创新的方法】  \n提出了一种自监督状态估计器S3E，利用雷达信号谱和惯性数据的互补感知能力，解决了稀疏点云和融合不兼容的问题。通过引入旋转基础的交叉融合技术，增强了相邻谱之间的空间结构信息。实验结果表明，该方法在没有定位真实值监督的情况下，能够实现鲁棒且准确的状态估计，超越了现有的技术水平。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "MUVLA: Learning to Explore Object Navigation via Map Understanding",
            "authors": "Peilong Han,Fan Jia,Min Zhang,Yutao Qiu,Hongyao Tang,Yan Zheng,Tiancai Wang,Jianye Hao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25966",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25966",
            "arxiv_html_link": "https://arxiv.org/html/2509.25966v1",
            "abstract": "In this paper, we present MUVLA, a Map Understanding Vision-Language-Action model tailored for object navigation. It leverages semantic map abstractions to unify and structure historical information, encoding spatial context in a compact and consistent form. MUVLA takes the current and history observations, as well as the semantic map, as inputs and predicts the action sequence based on the description of goal object. Furthermore, it amplifies supervision through reward-guided return modeling based on dense short-horizon progress signals, enabling the model to develop a detailed understanding of action value for reward maximization.\nMUVLA employs a three-stage training pipeline: learning map-level spatial understanding, imitating behaviors from mixed-quality demonstrations, and reward amplification. This strategy allows MUVLA to unify diverse demonstrations into a robust spatial representation and generate more rational exploration strategies.\nExperiments on HM3D and Gibson benchmarks demonstrate that MUVLA achieves great generalization and learns effective exploration behaviors even from low-quality or partially successful trajectories.",
            "introduction": "未获取到引言",
            "llm_summary": "【论文的motivation是什么】  \n1. 在对象导航任务中，如何有效利用语义地图来提升模型的空间理解能力。  \n2. 如何从多样化的演示中学习，尤其是低质量或部分成功的轨迹。  \n3. 如何通过奖励引导的返回建模来增强监督，从而优化探索策略。  \n\n【提出了什么创新的方法】  \nMUVLA采用三阶段训练流程：首先学习地图级空间理解，其次从混合质量的演示中模仿行为，最后进行奖励增强。该方法通过将历史观察和语义地图作为输入，预测基于目标对象描述的动作序列。MUVLA有效地统一了多样化的演示，生成更合理的探索策略。实验结果表明，MUVLA在HM3D和Gibson基准上表现出色，能够从低质量轨迹中学习有效的探索行为。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Towards Intuitive Human-Robot Interaction through Embodied Gesture-Driven Control with Woven Tactile Skins",
            "authors": "ChunPing Lam,Xiangjia Chen,Chenming Wu,Hao Chen,Binzhi Sun,Guoxin Fang,Charlie C.L. Wang,Chengkai Dai,Yeung Yam",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25951",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25951",
            "arxiv_html_link": "https://arxiv.org/html/2509.25951v1",
            "abstract": "This paper presents a novel human–robot interaction (HRI) framework that enables intuitive gesture-driven control through a capacitance-based woven tactile skin. Unlike conventional interfaces that rely on panels or handheld devices, the woven tactile skin integrates seamlessly with curved robot surfaces, enabling embodied interaction and narrowing the gap between human intent and robot response. Its woven design combines fabric-like flexibility with structural stability and dense multi-channel sensing through the interlaced conductive threads. Building on this capability, we define a gesture–action mapping of 14 single- and multi-touch gestures that cover representative robot commands, including task-space motion and auxiliary functions. A lightweight convolution–transformer model designed for gesture recognition in real time achieves an accuracy of near-100%, outperforming prior baseline approaches. Experiments on robot arm tasks, including pick-and-place and pouring, demonstrate that our system reduces task completion time by up to 57% compared with keyboard panels and teach pendants. Overall, our proposed framework demonstrates a practical pathway toward more natural and efficient embodied HRI.",
            "introduction": "Human–robot interaction (HRI) is central to the deployment of robotic systems in daily life and collaborative settings [1]. For robots to operate effectively alongside humans, the interaction interfaces must be intuitive and user-friendly. Conventional interfaces—such as control panels, handheld devices, or teach pendants—often create a gap between human intent and robot response, thereby hindering natural collaboration [2].\n\nTactile interfaces, also known as robot skins, offer a promising alternative by enabling direct physical interaction with the robot. Tactile skins that can be seamlessly integrated onto curved robot surfaces unlock the possibility of on-surface interaction, where users can engage with the robot body in an intuitive and embodied manner [3]. However, most existing tactile skins respond only to direct force inputs, typically translating touch into compliance control or safety behaviors [4]. While this suffices for basic collaboration, it limits interaction bandwidth and cannot interact with richer contact patterns. Gestures, in contrast, can be viewed as structured spatiotemporal interactions that evolve dynamically across time, space, and multiple contact points [5]. These patterns extend beyond individual points of contact, offering a richer vocabulary for users to express their intentions. By interpreting tactile input as gestures, robotic skins can evolve from basic responsiveness to more intuitive, high-level interaction interfaces.\n\nDespite this potential, gesture-based tactile interaction on robot surfaces remains challenging. One difficulty lies in sensing: most existing tactile skins are fabricated either as flexible films [6], which are difficult to scale up for high-resolution sensing and large-area coverage, or as stretchable fabrics [7], which often suffer from mechanical instability when deployed on curved robot surfaces. Another challenge lies in gesture design: to ensure truly intuitive interaction, gesture sets must naturally align with user intent and effectively represent common robot commands without causing ambiguity [8]. Moreover, achieving robust recognition remains challenging – while vision-based methods have made significant progress in understanding gestures, tactile sensing still lacks mature techniques for directly capturing continuous spatiotemporal patterns on robot surfaces [9].\n\nTo tackle these challenges, we propose a novel human–robot interaction framework for intuitive on-surface control, centered around a capacitance-based woven tactile skin. Its interlaced structure offers structural stability and inherently supports robust, multi-channel sensing of dynamic, multi-touch interactions. Meanwhile, its fabric-like flexibility enables seamless conformity to robot surfaces. Building on this foundation, we introduce a gesture vocabulary comprising 14 dynamic single- and multi-touch patterns—such as swipes, pushes, pinches, and circular strokes involving 1 to 5 contact points. These gestures are directly mapped to robot actions, including translations and rotations in task space, as well as auxiliary functions, forming an intuitive and expressive command set for interaction. To facilitate gesture recognition from tactile input, we design a hybrid architecture that integrates a lightweight convolutional stem for per-frame spatial encoding with a temporal transformer to capture long-range dependencies across frames, enabling accurate and low-latency recognition in real-world scenarios.\n\nWe further validate the proposed framework in real-world, vision-free settings through pick-and-place and pouring tasks, where robot actions are directly controlled by human gestures performed on the robot’s surface. Compared to conventional HRI interfaces like keyboard panels or teach pendants, our approach reduces task completion time by 23–57%, demonstrating the intuitiveness and efficiency of gesture-driven control.\n\nThe main contributions of this work are summarized as follows:\n\nWe introduce a novel embodied gesture-driven human–robot interaction framework using a capacitance-based woven tactile skin, where the interlaced woven structure ensures stable sensing of dynamic and multi-touch tactile inputs such as human gestures (Section III).\n\nWe design a gesture vocabulary of 14 single- and multi-touch patterns with up to five contact points, which are intuitively mapped to representative robot commands, including task-space translation, rotation, and essential auxiliary control functions that improve operational efficiency (Section IV).\n\nWe propose a hybrid convolution–transformer recognition model for classifying gestures from spatiotemporal tactile signals, achieving a near-100% success rate in our tasks and outperforming LSTM-based baselines, while maintaining low latency for real-time human–robot interaction (Section V).\n\n1. We introduce a novel embodied gesture-driven human–robot interaction framework using a capacitance-based woven tactile skin, where the interlaced woven structure ensures stable sensing of dynamic and multi-touch tactile inputs such as human gestures (Section III).\n\n2. We design a gesture vocabulary of 14 single- and multi-touch patterns with up to five contact points, which are intuitively mapped to representative robot commands, including task-space translation, rotation, and essential auxiliary control functions that improve operational efficiency (Section IV).\n\n3. We propose a hybrid convolution–transformer recognition model for classifying gestures from spatiotemporal tactile signals, achieving a near-100% success rate in our tasks and outperforming LSTM-based baselines, while maintaining low latency for real-time human–robot interaction (Section V).",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统人机交互接口无法有效缩小人类意图与机器人响应之间的差距。  \n2. 现有的触觉皮肤技术在动态多点触控的感知和识别上存在挑战。  \n3. 需要一种直观且高效的交互方式以提升人机协作的自然性。  \n\n【提出了什么创新的方法】  \n本研究提出了一种基于电容的编织触觉皮肤的人机交互框架，支持动态多点触控的稳定感知。通过设计14种直观的手势模式，将其映射到机器人命令，实现了高效的任务控制。采用混合卷积-变换器模型进行手势识别，达到了近100%的准确率，显著提高了任务完成效率，减少了23%-57%的时间。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "State Estimation for Compliant and Morphologically Adaptive Robots",
            "authors": "Valentin Yuryev,Max Polzin,Josie Hughes",
            "subjects": "Robotics (cs.RO)",
            "comment": "submitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.25945",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25945",
            "arxiv_html_link": "https://arxiv.org/html/2509.25945v1",
            "abstract": "Locomotion robots with active or passive compliance can show robustness to uncertain scenarios, which can be promising for agricultural, research and environmental industries.\nHowever, state estimation for these robots is challenging due to the lack of rigid-body assumptions and kinematic changes from morphing.\nWe propose a method to estimate typical rigid-body states alongside compliance-related states, such as soft robot shape in different morphologies and locomotion modes.\nOur neural network-based state estimator uses a history of states and a mechanism to directly influence unreliable sensors.\nWe test our framework on the GOAT platform, a robot capable of passive compliance and active morphing for extreme outdoor terrain.\nThe network is trained on motion capture data in a novel compliance-centric frame that accounts for morphing-related states.\nOur method predicts shape-related measurements within 4.2% of the robot’s size, velocities within 6.3% and 2.4% of the top linear and angular speeds, respectively, and orientation within 1.5\\unit\\unit{}.\nWe also demonstrate a 300% increase in travel range during a motor malfunction when using our estimator for closed-loop autonomous outdoor operation.",
            "introduction": "For robots to support outdoor industries and research activities, such as animal monitoring, climate surveillance, and agriculture, they require the capability to operate within and traverse extreme terrain conditions [1][2][3].\nWhile animals demonstrate such capabilities, and can operate on challenging and varied terrains, typical state-of-the art robotic solutions, such as rigid-body quadrupeds, are primarily confined to challenging but man-made terrain [4].\nThis can be partially attributed to their reliance on a singular mode of locomotion and an inability to physically adapt to the wide range of conditions found in outdoor environments [5][6].\nConversely, locomoting animals exploit passive and active compliance to achieve adaptability and beneficial interaction with the environment.\nFor example passive adaptation enables tumbleweeds to roll in the wind, cockroaches to squeeze through tight spaces, and kangaroos to store energy in their legs[7][8].\nActive morphological adaptability is exemplified in nature by armadillos or hedgehogs, which can modify their shape to defend themselves or leverage terrain to their advantage [9][10].\n\nThis biological inspiration has inspired a paradigm shift in robot design toward systems that leverage passive compliance and adaptive morphologies[11], resulting in a new robot design space for traversing varied terrain [12][13].\nTo fully exploit the autonomy of such robots, reliable motion controllers and high-level decision-making modules are required, which are dependent on reliable state estimation. For compliant, morphing robots this becomes challenging due to kinematic changes from active morphology and the lack of rigid-body assumptions.\nOur goal is to produce a reliable state estimator that accommodates the challenges from robots with passive compliance and active morphological adaptability.\nFor this paper, we focus our efforts on the GOAT robot, a elastic-rod shelled robot capable of driving, rolling and swimming by actively changing its morphology [14].\n\nState estimation for rigid-body robots is a widely researched topic [15][16].\nFor legged robots, modern actuation and sensor advancements have enabled precise model-based state estimation [17].\nSimilarly, in autonomous driving, deep-learning methods have provided reliable state estimation in various dynamic environments [18].\nWhile robust and industry-proven, these methods rely on the rigid-body assumption. For compliant, soft robots, the information encoded in the relationship between the robot-centric frame and the rest of the robot’s kinematics—such as the location of the sensors relative to the motors—becomes inconsistent and unobservable.\nThe lack of a rigid frame further complicates state estimation as the computational and sensor payloads can experience large impacts and perturbations.\n\nWith the increasingly prevalence of soft robotics, there have been state-estimation methods developed for compliant systems both on the algorithm side such as kinematic-based filtering [19] and sensor side such as IMU arrays and piezoelectric sensors [20].\nHowever, these primarily focus on minor deformations, not significant morphological change during operation.\nRecent AI advances have produced learning-based approaches that accommodate active morphological variations within a single locomotion mode, such as walking, but not for robots capable of multiple modes like driving and rolling [21].\nTo the best of our knowledge, no state estimation method exists that combines different locomotion modalities with robot compliance.\n\nWe propose a state estimation approach for compliant robots capable of active morphological adaptation, shown on Fig. 1.\nOur method utilizes a machine learning-based history belief encoder, trained on motion capture data, to approximate a robot’s typical rigid-body states (e.g., linear and angular velocities) and states unique to compliant and morphologically adaptive robots (e.g. outer shell shape).\nTo accommodate passive compliance, we introduce a compliance-centric frame based on the overall robot shape, rather than a classical robot-centric frame tied to sensor positions.\nTo support various locomotion modalities, we augment a feedforward PID controller with soft-body-specific information from the state and shape estimator.\n\nWe test our method on the GOAT platform, which features passive terrain compliance and active morphological changes for different locomotion modalities like driving and rolling [14].\nWe demonstrate state estimation for both rigid-body states (e.g., linear and angular velocities) and morphing-robot-specific states (e.g., outer shell shape) across varied morphological scenarios.\nModel performance is validated against motion capture ground truth.\nWe showcase improved control via a morphologically adaptive PID controller that enhances velocity tracking during driving.\nTo demonstrate robustness, we test the history belief encoder with malfunctioning morphology sensors in unforeseen corner cases.\nFinally, we stress-test our closed-loop controller, which relies on our state estimator, in an outdoor scenario against an open-loop baseline.\n\nThe remainder of this paper is structured as follows.\nFirst, we introduce our method.\nWe then present the experimental setup and results.\nFinally, we summarize our findings and propose future research directions.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的刚体机器人在极端环境中的适应性不足。  \n2. 软体机器人在状态估计中面临挑战，尤其是形态变化和缺乏刚体假设。  \n3. 需要可靠的状态估计以支持复杂的运动控制和决策模块。  \n\n【提出了什么创新的方法】  \n本研究提出了一种基于机器学习的状态估计方法，能够同时估计刚体状态和与软体机器人形态相关的状态。该方法利用历史状态信息和合规性中心框架，处理机器人在不同形态和运动模式下的状态变化。通过在GOAT平台上进行测试，结果表明该方法在形状预测、速度和方向估计上具有高精度，并在电机故障情况下提高了300%的行驶范围，展示了其在复杂环境中的鲁棒性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Reinforced Embodied Planning with Verifiable Reward for Real-World Robotic Manipulation",
            "authors": "Zitong Bo,Yue Hu,Jinming Ma,Mingliang Zhou,Junhui Yin,Yachen Kang,Yuqi Liu,Tong Wu,Diyun Xiang,Hao Chen",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25852",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25852",
            "arxiv_html_link": "https://arxiv.org/html/2509.25852v1",
            "abstract": "Enabling robots to execute long-horizon manipulation tasks from free-form language instructions remains a fundamental challenge in embodied AI. While vision–language models (VLMs) have shown promise as high-level planners, their deployment in the real world is hindered by two gaps: (i) the scarcity of large-scale, sequential manipulation data that couples natural language with multi-step action plans, and (ii) the absence of dense, interpretable rewards for fine-tuning VLMs on planning objectives.\nTo address these issues, we propose REVER, a framework that empowers VLMs to generate and validate long-horizon manipulation plans from natural language instructions in real-world scenarios. Under REVER we train and release RoboFarseer, a VLM incentivized to emit chain-of-thought that perform temporal and spatial reasoning, ensuring physically plausible and logically coherent plans. To obtain training data, we leverage the Universal Manipulation Interface framework to capture hardware-agnostic demonstrations of atomic skills. An automated annotation engine converts each demonstration into vision–instruction–plan triplet. We introduce a verifiable reward that scores the generated plan by its ordered bipartite matching overlap with the ground-truth skill sequence. At run time, the fine-tuned VLM functions both as a planner and as a monitor, verifying step-wise completion.\nRoboFarseer matches or exceeds the performance of proprietary models that are orders of magnitude larger, while on open-ended planning it surpasses the best baseline by more than 40%. In real-world, long-horizon tasks, the complete system boosts overall success by roughly 60% compared with the same low-level controller without the planner. We will open-source both the dataset and the trained model upon publication.",
            "introduction": "Embodied planning is a cornerstone of intelligent robotics: an agent must perceive a dynamic environment, reason about latent goals, and produce coherent, long-horizon action sequences that satisfy free-form natural-language instructions [1, 2]. Whereas low-level controllers concentrate on accurate trajectory tracking [3, 4], high-level planning operates at the intersection of vision, language, and action, demanding temporally extended, physically grounded decision-making. Recent large vision–language models (VLMs) unify visual perception and linguistic reasoning in a single backbone [5, 6], yet turning them into active, reliable planners remains elusive. In open-world manipulation, even the strongest VLMs hallucinate unreachable states, violate physical constraints, or derail after the first unexpected observation [7], underscoring a gap between static reasoning and the incremental, feedback-rich nature of real-world task execution.\n\nA fundamental impediment is the learning paradigm itself. Supervised fine-tuning (SFT) on human-collected plan traces teaches VLMs to imitate expert sequences, but provides no mechanism for recovery or creative recombination once the context drifts. Reinforcement learning (RL) promises adaptive, self-improving behaviour, yet embodied RL for VLMs is still nascent: prior work largely targets spatial-reasoning[8, 9, 10] or robotic simulator where reward engineering is trivial. When the objective is open-ended planning, dense, interpretable rewards are notoriously difficult to craft. LLM-as-a-Judge methods [11] supply noisy, unverifiable scalar signals that vary unpredictably across prompts, whereas hand-crafted heuristics break down as scene complexity grows. Compounding the problem is data scarcity: existing real-world collections such as Universal Manipulation Interface (UMI) [12] excel at short, atomic skills but lack the compositional breadth and sequential annotation needed to train and evaluate long-horizon planners. Consequently, existing VLM planners either remain confined to synthetic benchmarks with canned instructions or degrade rapidly when confronted with the visual diversity, action variability, and partial observability of real households.\n\nWe bridge these gaps with REVER (Reinforced Embodied planning with Verifiable Reward)—a framework that turns a VLM into a reliable long-horizon planner and verifier for real-world manipulation.\nInspired by the DeepSeek-R1 paradigm [13], REVER incentivizes the VLM to produce an explicit chain-of-thought that performs temporal and spatial reasoning before committing to any skill, ensuring the ensuing plan is both physically plausible and logically coherent.\nWithin this pipeline we train and release RoboFarseer, fine-tuned by Qwen2.5-VL-7B[14], that learns to generate and self-evaluate such thought-augmented plans.\nCentral to REVER is a deterministic reward that inspects a proposed plan along two axes: (1) syntactic validity—every step must instantiate a predefined, executable skill grammar—and (2) semantic coverage—an ordered bipartite match against the ground-truth skill sequence.\nThis reward is interpretable, and cheap to compute, allowing us to formulate fine-tuning as Reinforcement Learning with Verifiable Reward (RLVR) without human labels or simulator queries.\nDuring RLVR, RoboFarseer learns two complementary capabilities in a single forward pass: emitting an instruction-conditioned multi-step plan and estimating whether the current skill has been completed, yielding a closed-loop system that advances autonomously through extended tasks.\nTo train and evaluate RoboFarseer, we curate a large-scale, sequential dataset.\nWe extend UMI framework with an automated pipeline that converts in-the-wild, kinesthetic video demonstrations into Vision-Instruction-Plan triplets.\nThe pipeline segments videos into atomic skills, labels them with language descriptions, and assembles composite plans.\nOur contributions are summarized as follow:\n\nWe propose REVER, a reinforcement-learning framework that leverages a verifiable reward derived from skill-syntax checks and bipartite matching, and performs reinforcement fine-tuning to yield RoboFarseer.\n\nWe introduce an automated pipeline that converts raw UMI demonstrations into Vision–Instruction–Plan triplets. Building on this pipeline, we present the LEAP (Long-horizon Embodied Action Planning) dataset, which contains two subsets—LEAP-L for long-horizon sequential planning and LEAP-U for instruction-aligned planning.\n\nWe demonstrate a closed-loop, hierarchical system in which RoboFarseer simultaneously generates high-level skill plans and monitors their execution, validated on public benchmarks and through real-robot deployments on complex, long-horizon household tasks.\n\n§II reviews related work on embodied planning, visual reasoning, and real-world robotic datasets. §III details the REVER framework, including data synthesis, verifiable-reward design, and hierarchical execution. §IV presents benchmarks and real-robot evaluations, and §V concludes with future directions.\n\nAll model weights, datasets, and source code will be made publicly available upon acceptance to facilitate reproducibility.\n\n1. We propose REVER, a reinforcement-learning framework that leverages a verifiable reward derived from skill-syntax checks and bipartite matching, and performs reinforcement fine-tuning to yield RoboFarseer.\n\n2. We introduce an automated pipeline that converts raw UMI demonstrations into Vision–Instruction–Plan triplets. Building on this pipeline, we present the LEAP (Long-horizon Embodied Action Planning) dataset, which contains two subsets—LEAP-L for long-horizon sequential planning and LEAP-U for instruction-aligned planning.\n\n3. We demonstrate a closed-loop, hierarchical system in which RoboFarseer simultaneously generates high-level skill plans and monitors their execution, validated on public benchmarks and through real-robot deployments on complex, long-horizon household tasks.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人在执行长时间操作任务时面临语言指令的理解和执行挑战。  \n2. 现有的视觉-语言模型在真实世界中的应用受限于缺乏大规模的操作数据和可解释的奖励机制。  \n3. 现有的学习范式无法有效处理上下文漂移和复杂场景中的计划执行。  \n\n【提出了什么创新的方法】  \n本文提出了REVER框架，通过强化学习与可验证奖励相结合，提升了视觉-语言模型（VLM）在真实世界中的长时间操作规划能力。该方法首先利用自动化管道将原始演示转换为视觉-指令-计划三元组，随后通过可验证的奖励机制进行强化微调，确保生成的计划在语法和语义上均有效。RoboFarseer模型在开放式规划任务中表现优异，成功率比基线提高了60%，并在真实机器人部署中验证了其有效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies",
            "authors": "Jing Wang,Weiting Peng,Jing Tang,Zeyu Gong,Xihua Wang,Bo Tao,Li Cheng",
            "subjects": "Robotics (cs.RO)",
            "comment": "th Conference on Neural Information Processing Systems (NeurIPS 2025)",
            "pdf_link": "https://arxiv.org/pdf/2509.25822",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25822",
            "arxiv_html_link": "https://arxiv.org/html/2509.25822v1",
            "abstract": "Existing imitation learning methods decouple perception and action, which overlooks the causal reciprocity between sensory representations and action execution that humans naturally leverage for adaptive behaviors.\nTo bridge this gap, we introduce Action-Guided Diffusion Policy (DP-AG), a unified representation learning that explicitly models a dynamic interplay between perception and action through probabilistic latent dynamics.\nDP-AG encodes latent observations into a Gaussian posterior via variational inference and evolves them using an action-guided SDE, where the Vector–Jacobian Product (VJP) of the diffusion policy’s noise predictions serves as a structured stochastic force driving latent updates.\nTo promote bidirectional learning between perception and action, we introduce a cycle-consistent contrastive loss that organizes the gradient flow of the noise predictor into a coherent perception–action loop, enforcing mutually consistent transitions in both latent updates and action refinements.\nTheoretically, we derive a variational lower bound for the action-guided SDE, and prove that the contrastive objective enhances continuity in both latent and action trajectories.\nEmpirically, DP-AG significantly outperforms state-of-the-art methods across simulation benchmarks and real-world UR5 manipulation tasks.\nAs a result, our DP-AG offers a promising step toward bridging biological adaptability and artificial policy learning.\nCode is available on our project website: https://jingwang18.github.io/dp-ag.github.io/.",
            "introduction": "Imitation learning (IL) enables agents to replicate expert behavior from demonstrations.\nDirect mapping methods, such as [Codevilla et al., 2018, Mandlekar et al., 2022, Florence et al., 2022], learn a direct mapping between observations and actions.\nIn contrast, generative models such as Diffusion Policy (DP) [Chi et al., 2023] and flow-matching methods [Hu et al., 2024, Zhang et al., 2025] model action distributions to improve continuity across time steps.\nVision-Language-Action (VLA) [Kim et al., 2024, Black et al., 2024] improves perception by leveraging Vision-Language Models (VLMs) to interpret environmental cues and high-level instructions.\nDespite progress, existing methods treat observation features as static during each action sequence generation (extracting them from a single time-point observation and holding them fixed while generating a short sequence of actions), thus overlooking the opportunity for the intermediate action feedback to refine perception.\n\nRobust decision-making relies on a continuous interplay between perception and action [O’regan and Noë, 2001].\nHumans naturally embody this principle by dynamically refining their environmental understanding through feedback from their own actions [Brooks, 1991].\nMotivated by this, we propose Action-Guided Diffusion Policy (DP-AG), a representation learning framework for IL that explicitly models the perception–action interplay through probabilistic latent dynamics.\nWe build upon DP because it models the continuity within each short-horizon action sequence, where intermediate action feedback can be derived from its noise predictions at each diffusion step.\nDP-AG first grounds observation features in a Gaussian posterior via variational inference, capturing uncertainty from observation inputs.\nTo enable dynamic perception, we introduce an action-guided stochastic differential equation (SDE) in which latent features evolve across diffusion steps, driven by action-conditioned noise predictions.\nHere, the Vector–Jacobian Product (VJP) from the diffusion model acts as a structured stochastic force that shape feature evolution.\n\nAlthough raw observations contain all available information, their usefulness for policy learning depends on how they are internally represented.\nUnlike static encoders that keep features fixed over an entire action sequence, DP-AG continuously refines them using action feedback.\nThis process mirrors active perception in biological agents, where even if the external scene remains unchanged, sensory inputs are reinterpreted in the context of ongoing actions.\nThis captures the essence of Act to See, See to Act: the same observation is reinterpreted as actions unfold, with see denoting the evolving latent interpretation of a fixed input refined through action feedback, rather than new external sensing.\nAs shown in Figure 1, DP-AG conditions latent evolution on action-guided noise, leveraging the continuity of diffusion-based action refinement to ensure that perceptual dynamics remain coherent with action denoising.\nA cycle-consistent contrastive loss further aligns latent evolution with action diffusion, preventing drift and reinforcing the perception–action loop.\nCombined with variational inference, these components yield a principled and adaptive representation that produces smoother, more context-aware trajectories, as confirmed by both synthetic and real-robot experiments.\n\nTheoretically, DP-AG introduces an action-guided latent SDE, derives a principled variational lower bound, and rigorously proves that the proposed cycle-consistent contrastive loss enforces continuous and coherent trajectories in both perception and action spaces.\nEmpirically, DP-AG consistently outperforms state-of-the-art methods in success rate, convergence speed, and action smoothness, achieving gains of 6% in Push-T and 13% in Dynamic Push-T benchmarks, and delivering at least 23% higher manipulation success and approximately 60% smoother actions on real-world UR5 robot tasks compared to the baseline DP.\nOur contributions are summarized below:\n\nWe propose a novel observation representation learning that establishes a closed perception–action loop by refining latent features via VJP-guided noise predictions derived from DP.\n\nWe formulate an action-guided latent SDE, derive a variational lower bound, and prove that cycle-consistent InfoNCE enforces mutual smoothness in both latent and action trajectories.\n\nWe validate the effectiveness of DP-AG in both simulation and real-world scenarios, demonstrating consistent and significant improvements in task success rate, convergence speed, and smoothness of generated actions compared to state-of-the-art methods.\n\n1. We propose a novel observation representation learning that establishes a closed perception–action loop by refining latent features via VJP-guided noise predictions derived from DP.\n\n2. We formulate an action-guided latent SDE, derive a variational lower bound, and prove that cycle-consistent InfoNCE enforces mutual smoothness in both latent and action trajectories.\n\n3. We validate the effectiveness of DP-AG in both simulation and real-world scenarios, demonstrating consistent and significant improvements in task success rate, convergence speed, and smoothness of generated actions compared to state-of-the-art methods.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有模仿学习方法将感知与动作解耦，忽视了二者之间的因果互惠关系。  \n2. 需要一种方法来动态地将感知与动作结合，以实现更适应的行为。  \n\n【提出了什么创新的方法】  \n提出了Action-Guided Diffusion Policy (DP-AG)，通过概率潜在动态显式建模感知与动作之间的动态交互。DP-AG利用变分推断将潜在观察编码为高斯后验，并通过动作引导的随机微分方程（SDE）演化这些潜在特征。引入的循环一致对比损失促进了感知与动作之间的双向学习，确保潜在更新与动作精炼之间的一致性。理论上，DP-AG提供了一个变分下界，并证明对比目标增强了潜在和动作轨迹的连续性。实证结果显示，DP-AG在仿真基准和真实世界UR5操作任务中显著优于现有方法，展示了生物适应性与人工策略学习之间的桥梁。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling",
            "authors": "Yixian Zhang,Shu'ang Yu,Tonghe Zhang,Mo Guang,Haojia Hui,Kaiwen Long,Yu Wang,Chao Yu,Wenbo Ding",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25756",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25756",
            "arxiv_html_link": "https://arxiv.org/html/2509.25756v1",
            "abstract": "Training expressive flow-based policies with off-policy reinforcement learning is notoriously unstable due to gradient pathologies in the multi-step action sampling process. We trace this instability to a fundamental connection: the flow rollout is algebraically equivalent to a residual recurrent computation, making it susceptible to the same vanishing and exploding gradients as RNNs. To address this, we reparameterize the velocity network using principles from modern sequential models, introducing two stable architectures: Flow-G, which incorporates a gated velocity, and Flow-T, which utilizes a decoded velocity. We then develop a practical SAC-based algorithm, enabled by a noise-augmented rollout, that facilitates direct end-to-end training of these policies. Our approach supports both from-scratch and offline-to-online learning and achieves state-of-the-art performance on continuous control and robotic manipulation benchmarks, eliminating the need for common workarounds like policy distillation or surrogate objectives. Anonymized code is available at https://github.com/Elessar123/SAC-FLOW.",
            "introduction": "Flow-based policies have shown strong potential on challenging continuous-control tasks, including robot manipulation, due to their ability to represent rich, multimodal action distributions (Black et al., 2024; Lipman et al., 2022; Jiang et al., 2025). Early successes predominantly arose in imitation learning, where a flow-based policy is trained to reproduce expert behavior from static datasets (Luo et al., 2025; Tarasov et al., 2025). However, pure behavior cloning is fundamentally limited: dataset coverage is often sparse and of mixed quality (Kim et al., 2024; Garcia et al., 2025), and the lack of environment interaction prevents exploration, making it difficult to exceed demonstrator performance on hard tasks (Belkhale et al., 2023; Zare et al., 2024).\n\nA natural next step is to train flow-based policies with reinforcement learning. On-policy variants of PPO adapted to flows have demonstrated strong returns, yet they remain sample-inefficient (Schulman et al., 2017; Zhang et al., 2025). Off-policy methods promise much higher data efficiency and early integrations with flow-based policies on MuJoCo and DeepMind Control show encouraging results (Todorov et al., 2012; Tunyasuvunakool et al., 2020; Lv et al., 2025; Park et al., 2025). However, these successes typically come with design compromises that leave a central issue unresolved. Either the update relies on surrogate objectives that avoid differentiating through the rollout of the original flow, or the flow is distilled into a simpler one-step actor that can be optimized with standard off-policy losses. Both strategies reduce gradient stress but decouple optimization from the expressive generator and tend to blunt the benefits of multimodal flow-based policies (Park et al., 2025; Lv et al., 2025).\n\nWe propose a different viewpoint: treat the flow-based policy as a sequential model. Concretely, we show that the Euler integration used to generate actions in the flow-based policy is algebraically identical to the recurrent computation of a residual RNN. This observation explains the instability observed with off-policy training: the same vanishing or exploding gradients known to affect RNNs also afflict the flow rollout. Building on this link, we reparameterize the vanilla velocity network with the cell of modern sequential models that are designed to stabilize deep recurrent computations. We introduce two such novel designs of the flow-based policy: Flow-G, which incorporates a GRU-style gated velocity to regulate gradient flow across rollout steps, and Flow-T, which utilizes a Transformer-style decoded velocity to refine the action-time token via state-only cross-attention and a residual feed-forward network.\n\nOur main contributions are summarized as follows:\n\nA sequential model perspective for stable flow-based policies. We formalize the KK-step flow rollout as a residual RNN computation, providing a clear theoretical explanation for the gradient pathologies that cause instability in off-policy training. This insight allows us to reparameterize the velocity network with modern sequential architectures, leading to two novel, stable designs: Flow-G (GRU-gated) and Flow-T (Transformer-decoded). Our approach resolves critical gradient pathologies, enabling direct end-to-end optimization and eliminating the need for surrogate objectives or policy distillation.\n\nA practical and sample-efficient SAC framework for flow policies. We develop SAC Flow, a robust off-policy algorithm built upon our stabilized architectures. By introducing a noise-augmented rollout, we enable tractable likelihood computation for the SAC objective, a key technical hurdle. This approach yields two robust training procedures: (i) a stable from-scratch trainer for dense-reward tasks and (ii) a unified offline-to-online pipeline for sparse-reward tasks.\n\nState-of-the-art sample efficiency and performance. Our proposed methods, SAC Flow-G and SAC Flow-T, consistently attain state-of-the-art performance across a comprehensive suite of benchmarks, including MuJoCo, OGBench, and Robomimic. They significantly outperform recent flow- and diffusion-based baselines in terms of both final returns and convergence speed, empirically validating the high sample efficiency of our direct off-policy training approach. Ablation studies further confirm the robustness and stability of our designs.\n\n1. A sequential model perspective for stable flow-based policies. We formalize the KK-step flow rollout as a residual RNN computation, providing a clear theoretical explanation for the gradient pathologies that cause instability in off-policy training. This insight allows us to reparameterize the velocity network with modern sequential architectures, leading to two novel, stable designs: Flow-G (GRU-gated) and Flow-T (Transformer-decoded). Our approach resolves critical gradient pathologies, enabling direct end-to-end optimization and eliminating the need for surrogate objectives or policy distillation.\n\n2. A practical and sample-efficient SAC framework for flow policies. We develop SAC Flow, a robust off-policy algorithm built upon our stabilized architectures. By introducing a noise-augmented rollout, we enable tractable likelihood computation for the SAC objective, a key technical hurdle. This approach yields two robust training procedures: (i) a stable from-scratch trainer for dense-reward tasks and (ii) a unified offline-to-online pipeline for sparse-reward tasks.\n\n3. State-of-the-art sample efficiency and performance. Our proposed methods, SAC Flow-G and SAC Flow-T, consistently attain state-of-the-art performance across a comprehensive suite of benchmarks, including MuJoCo, OGBench, and Robomimic. They significantly outperform recent flow- and diffusion-based baselines in terms of both final returns and convergence speed, empirically validating the high sample efficiency of our direct off-policy training approach. Ablation studies further confirm the robustness and stability of our designs.",
            "llm_summary": "【论文的motivation是什么】  \n1. 训练流式策略在多步动作采样过程中不稳定，导致梯度路径问题。  \n2. 现有的强化学习方法在样本效率上表现不佳，无法充分利用环境交互。  \n3. 纯行为克隆方法受限于数据集覆盖不足和环境交互缺失，难以超越演示者表现。  \n\n【提出了什么创新的方法】  \n本文提出了一种将流式策略视为序列模型的方法，通过将流式策略的Euler积分与残差RNN计算等同化，揭示了导致不稳定性的梯度路径问题。基于此，提出了两种稳定的架构：Flow-G（GRU门控）和Flow-T（Transformer解码）。同时，开发了SAC Flow算法，通过引入噪声增强的回滚，支持直接端到端训练，显著提高了样本效率和性能。实验结果表明，SAC Flow-G和SAC Flow-T在多个基准测试中表现优异，超越了现有的流式和扩散基线，验证了其高样本效率和稳定性。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Best of Sim and Real: Decoupled Visuomotor Manipulation via Learning Control in Simulation and Perception in Real",
            "authors": "Jialei Huang,Zhaoheng Yin,Yingdong Hu,Shuo Wang,Xingyu Lin,Yang Gao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25747",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25747",
            "arxiv_html_link": "https://arxiv.org/html/2509.25747v1",
            "abstract": "Sim-to-real transfer remains a fundamental challenge in robot manipulation due to the entanglement of perception and control in end-to-end learning. We present a decoupled framework that learns each component where it is most reliable: control policies are trained in simulation with privileged state to master spatial layouts and manipulation dynamics, while perception is adapted only at deployment to bridge real observations to the frozen control policy. Our key insight is that control strategies and action patterns are universal across environments and can be learned in simulation through systematic randomization, while perception is inherently domain-specific and must be learned where visual observations are authentic. Unlike existing end-to-end approaches that require extensive real-world data, our method achieves strong performance with only 10-20 real demonstrations by reducing the complex sim-to-real problem to a structured perception alignment task. We validate our approach on tabletop manipulation tasks, demonstrating superior data efficiency and out-of-distribution generalization compared to end-to-end baselines. The learned policies successfully handle object positions and scales beyond the training distribution, confirming that decoupling perception from control fundamentally improves sim-to-real transfer.",
            "introduction": "Simulation environments provide a safe, scalable, and cost-effective platform for robot learning [6]. We can run thousands of robots in parallel, automatically reset environments, and access perfect state information in simulation, making large-scale interactive learning feasible. However, reliably deploying simulation-trained policies to the real world remains a central challenge in robot learning [1, 2, 3]. This challenge not only concerns technical feasibility but fundamentally determines whether robot learning methods can transition to practical applications. The difficulty of sim-to-real transfer arises from the entanglement of perception and control.\n\nIn end-to-end learning paradigms, policies must simultaneously handle visual domain shift and dynamics gap. The former arises from discrepancies in texture, lighting, and appearance between simulated and real images, while the latter stems from differences in physical parameters such as friction, mass, and contact forces between simulation and reality [1, 2, 3, 21, 22]. This dual challenge creates a compounding effect: perceptual uncertainties interact with control uncertainties, making the sim-to-real gap grow multiplicatively rather than additively. At deployment, this entanglement makes policies brittle to real-world variations, often requiring extensive fine-tuning to achieve usable performance.\n\nExisting sim-to-real methods predominantly adopt end-to-end learning paradigms, attempting to train unified networks that directly map from pixels to actions [13, 14, 15, 16, 17, 18, 23, 24, 26]. While techniques like domain randomization have improved transfer performance [1, 2, 3], they do not address the fundamental issue of perception-control entanglement. When perception and control are learned jointly, the network must master both visual feature extraction and control strategy generation simultaneously. This coupling means that even simple control behaviors require substantial data to learn under varying visual conditions [21, 22], as the network cannot separate task-relevant control patterns from domain-specific visual features. Moreover, errors in either perception or control can propagate through the network, making failures difficult to diagnose and correct.\n\nWhile privileged state learning has shown success in locomotion through teacher-student distillation [7, 19, 25], manipulation presents distinct challenges. Unlike locomotion where proprioception often suffices, manipulation critically depends on precise visual perception for object localization and grasping [8, 9, 10, 11, 12]. Rather than distilling both perception and control in simulation, we advocate learning perception only in the target domain where visual observations are authentic.\n\nOur core insight can be summarized as: control is consistent, perception is specific. Control strategies and action patterns are governed by consistent physical principles that remain invariant across environments, and they can therefore be effectively acquired in simulation through systematic randomization [1, 2, 3, 5, 6]. In contrast, perception is intrinsically tied to domain characteristics, since lighting conditions, textures, and visual appearances vary substantially across deployment scenarios and cannot be fully captured in simulation [8, 10, 11, 12]. This perspective motivates a rethinking of sim-to-real transfer: instead of pursuing end-to-end policies that attempt to handle all variations simultaneously, we advocate decomposing the problem and training each component in the setting where it can be learned most effectively [20].\n\nBased on this insight, we propose Best of Sim and Real (BSR): learn control where physics is accessible, adapt perception where visual observations are realistic. In the first stage, we train control policies in simulation using privileged state—perfect object poses and spatial relationships—allowing the policy to focus purely on learning robust control strategies through systematic randomization [1, 2, 4, 5, 6]. In the second stage, we freeze the control policy and train only a lightweight visual bridge that maps real observations to the policy’s expected input space [10, 11, 12]. This decomposition transforms the complex sim-to-real problem into two well-defined subproblems: learning universal control patterns in simulation and solving a structured perception alignment task in the real world.\n\nWe validate our approach on tabletop manipulation tasks, demonstrating superior data efficiency and out-of-distribution generalization compared to end-to-end baselines [13, 14, 15, 16, 17, 18]. The learned policies generalize successfully to object positions and scales beyond the training distribution, highlighting that decoupling perception from control substantially improves sim-to-real transfer. More importantly, the learned policies exhibit strong generalization capabilities, handling object positions and scales outside the training distribution. Our main contributions are threefold: (1) proposing a new sim-to-real paradigm for manipulation that decouples perception and control, fundamentally reducing real-world data requirements; (2) designing a two-stage training framework based on privileged state that maximizes the advantages of both simulation and real environments; (3) systematic experimental validation of our method’s advantages in data efficiency and generalization capability, with detailed ablation studies.",
            "llm_summary": "【论文的motivation是什么】  \n1. Sim-to-real transfer remains a fundamental challenge in robot manipulation.  \n2. Existing end-to-end learning approaches struggle with perception-control entanglement, leading to brittle performance in real-world applications.  \n3. Learning perception in the target domain is crucial for effective manipulation tasks.  \n\n【提出了什么创新的方法】  \n提出了一种名为Best of Sim and Real (BSR)的框架，将控制和感知的学习过程解耦。首先，在模拟环境中使用特权状态训练控制策略，以掌握空间布局和操作动态；其次，在真实环境中适应感知，以将真实观察映射到冻结的控制策略。该方法通过将复杂的sim-to-real问题转化为结构化的感知对齐任务，显著减少了对真实世界数据的需求。实验结果表明，该方法在桌面操作任务中展现出优越的数据效率和超出分布的泛化能力，成功处理了训练分布之外的物体位置和尺度，验证了感知与控制解耦的有效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand Object Poses",
            "authors": "Shuaijun Wang,Haoran Zhou,Diyun Xiang,Yangwei You",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25746",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25746",
            "arxiv_html_link": "https://arxiv.org/html/2509.25746v1",
            "abstract": "Despite progress in both traditional dexterous grasping pipelines and recent Vision-Language-Action (VLA) approaches, the grasp execution stage remains prone to pose inaccuracies, especially in long-horizon tasks, which undermines overall performance. To address this “last-mile” challenge, we propose TacRefineNet, a tactile-only framework that achieves fine in-hand pose refinement of known objects in arbitrary target poses using multi-finger fingertip sensing.\nOur method iteratively adjusts the end-effector pose based on tactile feedback, aligning the object to the desired configuration. We design a multi-branch policy network that fuses tactile inputs from multiple fingers along with proprioception to predict precise control updates. To train this policy, we combine large-scale simulated data from a physics-based tactile model in MuJoCo with real-world data collected from a physical system.\nComparative experiments show that pretraining on simulated data and fine-tuning with a small amount of real data significantly improves performance over simulation-only training. Extensive real-world experiments validate the method’s effectiveness, achieving millimeter-level grasp accuracy using only tactile input. To our knowledge, this is the first method to enable arbitrary in-hand pose refinement via multi-finger tactile sensing alone. Project website is available at https://sites.google.com/view/tacrefinenet",
            "introduction": "Dexterous\ngrasping has long been a fundamental and challenging research problem in robotics[1, 2, 3, 4]. Conventional approaches typically decompose the task into sequential stages including object approaching[5], grasp planning[6], and grasp execution[7]. Recently, end to end methods, particularly Vision-Language-Action (VLA) models[8, 9] have shown promising potential in learning direct mapping from visual inputs to grasping policies[10, 11, 12]. Despite advances in both paradigms, a critical challenge persists: the final execution of the grasp often deviates from the planned pose due to compounded errors in perception, algorithmic approximations, and real-world dynamics. Such inaccuracies significantly impair the overall success rate of long-horizon manipulation tasks, especially when manipulating thin or planar objects that demand high precision. Even minor misalignments during grasping can lead to failure or adversely affect subsequent manipulations such as insertion, assembly, or in-hand manipulation. This ”last-mile” grasping problem motivates the need for sensory feedback capable of compensating for such errors.\n\nTactile sensing offers a promising avenue to address this issue[13, 14, 15]. Humans routinely perform fine manipulation tasks in the absence of visual feedback, relying exclusively on the sense of touch[16, 17]. Inspired by this capability, we focus on the grasp execution phase and propose TacRefineNet, which is a tactile-only framework for precise in-hand pose refinement of known objects toward arbitrary target poses using multi-finger fingertip sensing. As shown in Fig. 1, our proposed method relies solely on multi-fingertip tactile sensing and achieves fine-grained grasping of flat objects in specific target poses through iterative adjustments.\n\nTo this end, we integrate custom-designed piezoresistive tactile sensors into a five-fingered dexterous hand with 11 degrees of freedom (DoF). Each fingertip is equipped with an 11×9 taxel array that measures normal contact forces. These high-resolution tactile signals are formatted into tactile images, enabling the use of off-the-shelf visual encoders for feature extraction. Guided by the geometry and size of the target objects, our approach primarily leverages three fingers and integrates their tactile signals to jointly estimate the adjustments required for refining the in-hand pose.\n\nHowever, the dexterous hand is underactuated, with only 6 active DoFs available for control. To overcome this limitation, we exploit external dexterity through iterative regrasping: tactile feedback from the fingertips is used to iteratively update the wrist pose, gradually reducing the misalignment between the current and target configurations. In this way, the system refines the grasp until convergence to the desired pose.\n\nBuilding upon this hardware-software loop, we design TacRefineNet, a novel multi-branch policy network that takes as input the target tactile image, the current tactile image, and the proprioceptive state, and outputs the corrective wrist motion needed for precise in-hand refinement.\nTo train the policy network, we construct a large-scale dataset comprising tactile, proprioceptive, and action data collected from simulation and physical world. Simulation facilitates efficient and scalable data generation using a physics-based tactile model in MuJoCo, while real-world data ensures physical realism. Comparative experiments demonstrate that pretraining on simulated data followed by fine-tuning with a small real-world dataset significantly outperforms simulation-only training, effectively bridging the sim-to-real gap.\n\nTo achieve generalization to arbitrary target in-hand poses, we employ a cross-combination training scheme where both current and target tactile images are randomly sampled from the dataset. This enables the policy to perform precise in-hand regrasping toward any tactile-specified goal.\nIn real-world experiments, our method successfully accomplishes fine in-hand manipulation of known objects from arbitrary initial poses with millimeter-level accuracy. The object is initially placed arbitrarily in the hand, and a target tactile image is provided. Using only tactile feedback, the policy executes fine adjustments until the desired pose is achieved.\n\nIn summary, this work presents TacRefineNet, a novel approach for target-driven in-hand regrasping of known objects using only multi-finger fingertip tactile sensing. We build a large-scale multimodal dataset from simulation and real robots and train a unified policy network that achieves high-precision in-hand manipulation. To the best of our knowledge, this is the first method that performs arbitrary in-hand pose adjustment for specific objects relying solely on tactile feedback. Our contributions are:\n\nWe present TacRefineNet, the first end-to-end tactile-only framework that achieves millimeter-level accuracy in 6-DoF in-hand pose refinement using multi-finger tactile feedback.\n\nWe present TacRefineNet, the first end-to-end tactile-only framework that achieves millimeter-level accuracy in 6-DoF in-hand pose refinement using multi-finger tactile feedback.\n\nWe design a multi-branch fusion policy network that leverages tactile signals from multiple fingers, enabling arbitrary in-hand pose refinement without the need for per-pose retraining.\n\nThe proposed policy demonstrates generalization to some extent to unseen object instances within similar categories.\n\n1. We present TacRefineNet, the first end-to-end tactile-only framework that achieves millimeter-level accuracy in 6-DoF in-hand pose refinement using multi-finger tactile feedback.\n\n2. We design a multi-branch fusion policy network that leverages tactile signals from multiple fingers, enabling arbitrary in-hand pose refinement without the need for per-pose retraining.\n\n3. The proposed policy demonstrates generalization to some extent to unseen object instances within similar categories.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统抓取方法在执行阶段存在姿态不准确的问题，影响整体性能。  \n2. 需要通过感知反馈来补偿抓取中的误差，尤其是在长时间操作任务中。  \n3. 人类在缺乏视觉反馈的情况下，依赖触觉完成精细操作，激发了对触觉感知的研究。  \n\n【提出了什么创新的方法】  \n提出了TacRefineNet，一个基于触觉的框架，通过多指尖触觉感知实现已知物体在任意目标姿态下的精细抓取调整。该方法通过迭代调整末端执行器姿态，利用触觉反馈将物体对齐到期望配置。设计了一个多分支策略网络，融合多指的触觉输入和本体感知，预测精确的控制更新。通过在MuJoCo中使用物理基础的触觉模型生成的大规模模拟数据与少量真实数据的结合进行训练，显著提高了性能。实验证明，该方法在真实世界中实现了毫米级的抓取精度。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning",
            "authors": "Si-Cheng Wang,Tian-Yu Xiang,Xiao-Hu Zhou,Mei-Jiang Gui,Xiao-Liang Xie,Shi-Qi Liu,Shuang-Yi Wang,Ao-Qun Jin,Zeng-Guang Hou",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25718",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25718",
            "arxiv_html_link": "https://arxiv.org/html/2509.25718v1",
            "abstract": "Reinforcement learning (RL) is a promising avenue for post-training vision–language–action (VLA) models, but practical deployment is hindered by sparse rewards and unstable training. This work mitigates these challenges by introducing an action chunk based on proximal policy optimization (PPO) with behavior cloning using self-collected demonstrations. Aggregating consecutive actions into chunks improves the temporal consistency of the policy and the density of informative feedback. In addition, an auxiliary behavior cloning loss is applied with a dynamically updated demonstration buffer that continually collects high-quality task trials during training. The relative weight between the action-chunked PPO objective and the self behavior clone auxiliary loss is adapted online to stabilize the post-training process. Experiments on the MetaWorld benchmark indicate improved performance over supervised fine-tuning, achieving a high success rate (0.93) and few steps to success (42.17). These results demonstrate the viability of RL for VLA post-training and help lay the groundwork for downstream VLA applications.",
            "introduction": "Motivated by advances in multimodal vision–language models (VLMs)[1], recent studies have begun exploring foundation models for robotics[2]. A direct extension augments VLMs with an action modality, yielding vision–language–action (VLA) models. By inheriting strong vision-language understanding of VLMs, VLA models shows strong manipulation potentials [3]. Nevertheless, the scarcity of large-scale manipulation datasets and the heterogeneity of robotic embodiments constrain zero-shot performance, making post-training a practical necessity for deployment [4].\n\nA common post-training strategy is to collect demonstrations and apply supervised fine-tuning to learn a mapping from observations to actions. This approach is straightforward and has shown effectiveness in adapting VLA models to downstream tasks. However, obtaining high-quality demonstrations is costly. Although the required demonstration set for post-training is much smaller than that for pre-training, it typically still comprises on the order of several dozen to about one hundred demonstrations [5, 6], and the acquisition process remains labor-intensive and time-consuming.\n\nRL offers a promising way to address the above problem and has been applied in robotics for decades [7]. Unlike supervised fine-tuning, RL adapts a policy to tasks through iterative interaction with the environment under a reward function. However, deploying a VLA model within an RL framework presents two challenges. First, in many manipulation settings the reward signal is sparse, which complicates credit assignment and slows training [8]. Second, the shift from supervised pre-training to RL-based optimization can introduce instability due to the noises of policy-gradient [9].\n\nTo provide dense reward during RL based post-training, prior work incorporates external expert feedback from either humans [10] or VLMs [11]. Human feedback is typically more reliable, whereas VLM feedback is automated and avoids annotation labor. In both cases, denser signals can accelerate convergence, but at the cost of additional system overhead (human effort or computational expense). To further stabilize training, a small demonstration set (e.g., ∼\\sim10 trials) is commonly collected and a supervised behavior-cloning term is added to the RL objective [9]. Combined with standard stabilization techniques [12, 13][13], this hybrid scheme has shown effectiveness. However, because the demonstrations are limited and often suboptimal, the behavior-cloning term can become detrimental once the VLA policy surpasses demonstration performance.\n\nTo address the above limitations, an action-chunked RL algorithm based on PPO with self-collected demonstrations is proposed for post-training VLA models. The method aggregates consecutive actions into action chunks, increasing the frequency of reward feedback in PPO. A dynamic demonstration buffer integrates high-quality trajectories generated by the agent for constructing an auxiliary supervised loss. The contributions of this study are summarized as follows:\n\nAn action-chunked PPO algorithm is developed for post-training VLA models, increasing the effective density of informative feedback.\n\nAn action-chunked PPO algorithm is developed for post-training VLA models, increasing the effective density of informative feedback.\n\nA self behavior cloning auxiliary loss is constructed based on the dynamically demonstration buffer that consistently collects high-quality task trials during learning.\n\nExperiments show that the method initialized with only 1010 demonstrations surpasses supervised fine-tuning with 100100 demonstrations in both success rate (0.93 vs. 0.89) and steps to success (42.17 vs. 65.65).\n\n1. An action-chunked PPO algorithm is developed for post-training VLA models, increasing the effective density of informative feedback.\n\n2. A self behavior cloning auxiliary loss is constructed based on the dynamically demonstration buffer that consistently collects high-quality task trials during learning.\n\n3. Experiments show that the method initialized with only 1010 demonstrations surpasses supervised fine-tuning with 100100 demonstrations in both success rate (0.93 vs. 0.89) and steps to success (42.17 vs. 65.65).",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的VLA模型在实际应用中面临稀疏奖励和不稳定训练的问题。  \n2. 传统的监督微调方法需要大量高质量的演示，获取成本高且耗时。  \n3. 需要一种有效的后训练策略来提升VLA模型在机器人操作中的表现。  \n\n【提出了什么创新的方法】  \n提出了一种基于PPO的动作分块强化学习算法，旨在提高VLA模型的后训练效果。该方法通过将连续动作聚合为动作块，增加奖励反馈的频率，从而提高策略的时间一致性。同时，动态更新的演示缓冲区用于收集高质量的任务轨迹，构建辅助的行为克隆损失。实验结果表明，该方法在MetaWorld基准测试中表现优异，成功率达到0.93，成功所需步骤为42.17，超越了传统的监督微调方法。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation",
            "authors": "Xinda Xue,Junjun Hu,Minghua Luo,Xie Shichao,Jintao Chen,Zixun Xie,Quan Kuichen,Guo Wei,Mu Xu,Zedong Chu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25687",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25687",
            "arxiv_html_link": "https://arxiv.org/html/2509.25687v1",
            "abstract": "Embodied navigation is a foundational challenge for intelligent robots, demanding the ability to comprehend visual environments, follow natural language instructions, and explore autonomously. However, existing models struggle to provide a unified solution across heterogeneous navigation paradigms, often yielding low success rates and limited generalization. We present OmniNav, a unified framework that handles instruct-goal, object-goal, point-goal navigation, and frontier-based exploration within a single architecture. First, we introduce a lightweight, low-latency policy that predicts continuous-space waypoints (coordinates and orientations) with high accuracy, outperforming action-chunk methods in precision and supporting real-world deployment with control frequencies up to 5 Hz. Second, at the architectural level, OmniNav proposes a fast-slow system design: a fast module performs waypoint generation from relatively short-horizon visual context and subtasks, while a slow module conducts deliberative planning using long-horizon observations and candidate frontiers to select the next subgoal and subtask. This collaboration improves path efficiency and maintains trajectory coherence in exploration and memory-intensive settings. Notably, we find that the primary bottleneck lies not in navigation policy learning per se, but in robust understanding of general instructions and objects. To enhance generalization, we incorporate large-scale general-purpose training dataset including those used for image captioning and visual into a joint multi-task regimen, which substantially boosts success rates and robustness. Extensive experiments demonstrate state-of-the-art performance across diverse navigation benchmarks, and real-world deployment further validates the approach. OmniNav offers practical insights for embodied navigation and points to a scalable path toward versatile, highly generalizable robotic intelligence.",
            "introduction": "Embodied navigation (Gao et al., 2024; Gu et al., 2022) has emerged as a core problem in embodied intelligence: enabling robots to perceive, understand, and explore real-world environments without pre-built maps while following natural language instructions. To act reliably in dynamic, partially observable environments, an agent must not only ground instantaneous visual inputs but also maintain coherent spatiotemporal memory and perform active exploration. Application demands for real-time responsiveness further increase the requirements for low-latency decision-making and cross-environment generalization.\n\nCurrent research largely revolves around three paradigms: point-goal (Liu et al., 2025), instruct-goal (Anderson et al., 2018; Ku et al., 2020), and object-goal (Yokoyama et al., 2024b). point-goal tasks are well-specified and straightforward to evaluate but rely on explicit coordinates rarely available in practice; instruction-goal aligns with human usage but often generalizes poorly to unseen instructions or environments; object-goal is the most practical but requires robust target recognition coupled with efficient path planning, making it the most challenging. Many existing methods remain customized, relying on task-specific data, which limits cross-task transfer and the potential for mutual enhancement. Uni-Navid (Zhang et al., 2024a) proposed a VLM-based discrete action predictor unifying vision-and-language navigation, object-goal navigation, embodied question answering  (Das et al., 2018), and following (Wang et al., 2025a), but its study of LLM long-horizon planning is not sufficiently developed. MTU3D (Zhu et al., 2025) advances a “move to understand” paradigm by coupling frontier exploration with visual localization in a single objective, yet requires constructing 3D object coordinates, leading to deployment complexity. Although recent Video-LLMs (Wei et al., 2025; Qi et al., 2025; Zhang et al., 2024b) and VLAs (Sapkota et al., 2025; Zitkovich et al., 2023; Ma et al., 2024) integrate vision, language, and action prediction end to end, they still face bottlenecks in streaming video input, long-context management, and low-latency inference: discretized action modeling sacrifices precision and flexibility; constrained LLM call frequency and frequent context resets lead to deployment difficulties; besides, in practice, the dominant failure mode often stems from inadequate understanding of generic instructions and open-vocabulary objects rather than policy learning itself. These gaps call for a unified, efficient framework that balances long/short-horizon reasoning with real-time responsiveness.\n\nWe present OmniNav, a unified embodied navigation framework that concurrently covers instruct-goal, object-goal, point-goal, and frontier-based exploration within a single architecture. Inspired by dual-system theory (Figure, 2024; Black et al., 2025), OmniNav coordinates a fast–slow system (Black et al., 2025): a fast system reacts to comparatively short-horizon perception and current tasks or subtasks, generating high-precision waypoints (coordinates and orientations) to support low-latency control up to 5 Hz; a slow system deliberates over long-horizon observations and frontier cues, leveraging a VLM’s chain-of-thought (Wei et al., 2022) to decompose complex goals and select the next subgoal and subtask. The two are coupled through a central memory module that uses a key–value (KV) cache to provide essential spatiotemporal context, yielding decisions that are both locally agile and globally consistent.\n\nOmniNav addresses the triad of real-time operation, fast–slow collaboration, and generalization. A lightweight flow-matching policy (Bjorck et al., 2025) avoids the precision degradation and latency accumulation inherent to action discretization; fast–slow collaboration ensures exploration efficiency and trajectory coherence in long-memory scenarios; and, more importantly, training unifies large-scale generic vision–language data (captioning, referring/grounding, etc.) with multiple navigation tasks, significantly strengthening instruction following and open-vocabulary object perception to improve success rates and robustness. Our contributions are threefold:\n\nThe first unified architecture to simultaneously support instruction, object, and point goals alongside frontier exploration;\n\nAn end-to-end fast–slow coordination with central memory that reconciles low-latency control and high-level deliberation;\n\nA principled strategy to incorporate generic vision–language data into joint training, systematically improving cross-task and cross-environment generalization.\n\n1. The first unified architecture to simultaneously support instruction, object, and point goals alongside frontier exploration;\n\n2. An end-to-end fast–slow coordination with central memory that reconciles low-latency control and high-level deliberation;\n\n3. A principled strategy to incorporate generic vision–language data into joint training, systematically improving cross-task and cross-environment generalization.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有模型在不同导航范式下的统一解决方案不足，成功率低且泛化能力有限。  \n2. 机器人在动态环境中需要理解视觉输入并执行自然语言指令的能力。  \n3. 当前研究主要集中在点目标、指令目标和对象目标导航，缺乏跨任务的通用性。  \n\n【提出了什么创新的方法】  \nOmniNav提出了一个统一的框架，支持指令目标、对象目标、点目标和前沿探索。它采用了快速-慢速系统设计，快速模块生成高精度的连续空间路径点，而慢速模块则进行长远规划。通过结合大规模的通用视觉-语言数据进行联合训练，OmniNav显著提高了指令跟随和开放词汇对象感知的成功率和鲁棒性。实验结果显示，OmniNav在多种导航基准测试中表现出色，并在实际部署中得到了验证。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Hierarchical Diffusion Motion Planning with Task-Conditioned Uncertainty-Aware Priors",
            "authors": "Amelie Minji Kim,Anqi Wu,Ye Zhao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25685",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25685",
            "arxiv_html_link": "https://arxiv.org/html/2509.25685v1",
            "abstract": "We propose a novel hierarchical diffusion planner that embeds task and motion structure directly in the noise model.\nUnlike standard diffusion-based planners that use zero-mean, isotropic Gaussian noise,\nwe employ a family of task-conditioned structured Gaussians whose means and covariances are derived from Gaussian Process Motion Planning (GPMP): sparse, task-centric key states or their associated timings (or both) are treated as noisy observations to produce a prior instance.\nWe first generalize the standard diffusion process to biased, non-isotropic corruption with closed-form forward and posterior expressions.\nBuilding on this, our hierarchy separates prior instantiation from trajectory denoising:\nthe upper level instantiates a task-conditioned structured Gaussian (mean and covariance),\nand the lower level denoises the full trajectory under that fixed prior.\nExperiments on Maze2D goal-reaching and KUKA block stacking show improved success rates, smoother trajectories, and stronger task alignment compared to isotropic baselines. Ablation studies indicate that explicitly structuring the corruption process offers benefits beyond simply conditioning the neural network.\nOverall, our method concentrates probability mass of prior near feasible, smooth, and semantically meaningful trajectories while maintaining tractability. Our project page is available at https://hta-diffusion.github.io.",
            "introduction": "Diffusion models have garnered increasing attention in robotic motion planning and shown strong flexibility across tasks [1, 2, 3, 4, 5].\nHowever, most existing approaches adopt a default corruption process using zero-mean, isotropic Gaussian noise, overlooking the inherent structure of robotic trajectories. In contrast to image or text data,\nrobotic motion possesses intrinsic properties such as temporal smoothness (e.g., consistent evolution across time steps for dynamic feasibility) and can also be shaped by structured external information, such as task-imposed conditions (e.g., start and goal states or phase-specific constraints).\nThese structured sources of information provide inductive biases that diffusion models can exploit, most commonly through conditioning or guidance [3, 6, 7, 8, 1].\n\nIn this work, we propose to encode such structure in the form of a task-aware noise model. This design enables our diffusion planner to respect smoothness and constraint satisfaction by construction. We realize this idea with a two-level hierarchical diffusion framework. The upper level uses a denoising diffusion probabilistic model (DDPM) [9] with isotropic Gaussian noise to produce sparse, task-centric key states.\nFor example, in navigation tasks, key states can be chosen as evenly spaced waypoints between the start and goal. In manipulation tasks, they naturally align with task phases (e.g., approach, grasp, lift), augmented by a binary contact-flag trajectory.111Hereafter, we use key states to denote both waypoints in navigation and keyframe states in manipulation. We treat upper-level key states as soft observations with explicit uncertainty, so these imperfect predictions guide sampling without hard constraints or rejection. The lower-level diffusion then generates the full trajectory under a task-conditioned Gaussian, obtained by conditioning a linear time-varying Gaussian process (LTV-GP) motion prior on the key states, inspired by Gaussian process motion planning (GPMP) [10]. This yields a non-isotropic and temporally correlated noise model whose mean and covariance encourage smooth dynamics between key states, while the key states act as anchors that prevent global oversmoothing and allow the trajectory to adapt at sharp turns or task-phase changes. Accordingly, this framework produces temporally consistent and constraint-aware plans.\n\nPrior diffusion works typically keep the corruption isotropic and inject structural constraints through conditioning or guidance during inference [8, 1, 2]. For instance, conditioning has been used to incorporate visual context from observations or to inject goal information\n[3, 6]. Guidance mechanisms and auxiliary objectives incorporate dynamics, task rewards, or safety specifications into the generative process [11, 12]. In addition to conditioning and guidance, task-centric representations have been leveraged to encode constraints, for example object-centric demonstrations [13] or SE(3)-based cost fields that couple grasp and motion optimization [14]. Projection-based methods have also been explored, where diffusion samples are mapped into feasible sets such as dynamically admissible trajectories [15] or collision-free multi-robot paths [16].\nBeyond these categories, diffusion models have also been integrated with model-based motion planning [17] and with neural dynamics models [18].\n\nStructured corruptions during training have been explored primarily outside robotics and remain rare within it.\nExamples include non-Gaussian but zero-mean choices [19], per-pixel variance schedules without temporal or robotics priors [20], and non-isotropic formulations that capture joint dependencies in human pose but still assume zero-mean noise and omit temporal correlations along the horizon, limiting their use for trajectory state conditioning [21]. Mixture priors have also been proposed for multimodality and controllability, where Gaussian mixture parameters may be predefined through clustering [22] or heuristic structure [23], learned end-to-end to adaptively cover target support [24], or dynamically adjusted during inference by solver-based approaches [25].\nIn robotics, structured corruptions remain rare, with only a few recent efforts—for example, integrating coarse trajectory predictions so that diffusion acts as a refiner [26], introducing temporally correlated noise such as colored Gaussian perturbations [27], or employing diffusion bridges that initiate denoising from informative prior actions in navigation tasks [28].\nWhile these studies highlight the benefits of structured corruptions for distributional expressiveness, they do not leverage structured priors to encode task conditions and temporal smoothness simultaneously. In contrast, our design directly biases the generative process toward trajectories that are both dynamically consistent and task-aware, rather than only improving multi-modality.\n\nHierarchical diffusion has been used to separate coarse-level decisions from finer-level motion, for example: subgoal generation followed by refinement [29], cascaded global and local prediction with online repair [30], progressive coarse-to-fine refinement processes that achieve real-time planning frequencies [31], contact-guided manipulation policies that decompose high-level contact planning from low-level trajectory generation [32], kinematics-aware frameworks with high-level end-effector prediction and low-level joint generation [33], and diffusion–flow hybrids that pair high-level diffusion with low-level rectified flow [34].\nA common pattern feeds high-level outputs to the low level as conditioning, while low-level corruption remains isotropic. This leaves the forward process and loss unchanged, so temporal coupling, boundary consistency, and waypoint fidelity must be relearned at every step. Our hierarchy instead reparameterizes the low-level prior by conditioning a GP on upper-level key states. This yields a trajectory posterior whose mean interpolates the key states and whose covariance couples time steps, so reverse updates follow a Mahalanobis geometry that narrows the search to trajectories consistent between key states and treats imperfect key states probabilistically. This improves feasibility without post hoc constraints or rejection.\n\nWe evaluate on Maze2D goal reaching and KUKA block stacking from the Diffuser benchmarks [2]. We compare (i) a single layer diffuser with isotropic noise, (ii) a hierarchical diffuser that only conditions on upper-level outputs while keeping isotropic noise, and (iii) our full hierarchical model that constructs the structured prior from upper-level key states and denoises under that prior. Across tasks, our model (iii) outperforms the conditioning-only baseline (ii), indicating that conditioning alone is insufficient. Embedding structure in the generative noise yields consistent gains in success and collision or constraint satisfaction.\n\nOur contributions are summarized as follows:\n\nA two-level hierarchical diffusion planner where the upper-level DDPM infers sparse, task-centric key states and the lower level denoises under a GPMP-based motion prior conditioned on these states. Key states are treated as soft observations with explicit uncertainty, encoding smoothness, boundary and phase consistency, and handling non-stationarities without hard constraints.\n\nA diffusion formulation with task-conditioned non-isotropic Gaussian corruption obtained by conditioning a GPMP-based prior on key states. We derive closed-form forward marginals and posteriors for variational training, yielding a Mahalanobis loss that generalizes MSE while preserving the standard DDPM pipeline.\n\nA conceptual and empirical comparison to conditioning-only diffusion hierarchies. Reparameterizing the low-level prior alters the diffusion landscape and concentrates sampling on trajectories consistent with key states, improving feasibility without extra constraints or loss terms.\n\nExperiments on Maze2D goal reaching and KUKA block stacking showing higher success and better constraint satisfaction than baselines with isotropic noise or conditioning alone, with ablations demonstrating the benefits of the structured prior.\n\n1. A two-level hierarchical diffusion planner where the upper-level DDPM infers sparse, task-centric key states and the lower level denoises under a GPMP-based motion prior conditioned on these states. Key states are treated as soft observations with explicit uncertainty, encoding smoothness, boundary and phase consistency, and handling non-stationarities without hard constraints.\n\n2. A diffusion formulation with task-conditioned non-isotropic Gaussian corruption obtained by conditioning a GPMP-based prior on key states. We derive closed-form forward marginals and posteriors for variational training, yielding a Mahalanobis loss that generalizes MSE while preserving the standard DDPM pipeline.\n\n3. A conceptual and empirical comparison to conditioning-only diffusion hierarchies. Reparameterizing the low-level prior alters the diffusion landscape and concentrates sampling on trajectories consistent with key states, improving feasibility without extra constraints or loss terms.\n\n4. Experiments on Maze2D goal reaching and KUKA block stacking showing higher success and better constraint satisfaction than baselines with isotropic noise or conditioning alone, with ablations demonstrating the benefits of the structured prior.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的扩散模型在机器人运动规划中使用零均值、各向同性的高斯噪声，忽视了轨迹的内在结构。  \n2. 机器人运动具有时间平滑性和任务条件等固有属性，这些属性未被充分利用。  \n3. 需要一种方法来同时编码任务条件和时间平滑性，以提高运动规划的有效性和可行性。  \n\n【提出了什么创新的方法】  \n提出了一种两级层次扩散规划器，利用任务中心的稀疏关键状态作为上层的输入，并在下层基于高斯过程运动规划（GPMP）对轨迹进行去噪。通过将关键状态视为具有显式不确定性的软观测，方法能够在不施加硬约束的情况下编码平滑性和一致性。该方法在Maze2D目标到达和KUKA块堆叠任务中表现出更高的成功率和更好的约束满足，表明结构化先验的引入显著提升了运动规划的效果。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought",
            "authors": "Junjie Wen,Minjie Zhu,Jiaming Liu,Zhiyuan Liu,Yicun Yang,Linfeng Zhang,Shanghang Zhang,Yichen Zhu,Yi Xu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "technique report",
            "pdf_link": "https://arxiv.org/pdf/2509.25681",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25681",
            "arxiv_html_link": "https://arxiv.org/html/2509.25681v1",
            "abstract": "Vision-Language-Action (VLA) models are emerging as a next-generation paradigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages a multimodal chain-of-thought to unify visual perception, language reasoning, and robotic control in a single system. dVLA jointly optimizes perception, language understanding, and action under a single diffusion objective, enabling stronger cross-modal reasoning and better generalization to novel instructions and objects. For practical deployment, we mitigate inference latency by incorporating two acceleration strategies—a prefix attention mask and key–value (KV) caching—yielding up to ∼2×\\sim 2\\times speedup at test-time inference. We evaluate dVLA in both simulation and the real world: on the LIBERO benchmark it achieves state-of-the-art performance with a 96.4% average success rate, consistently surpassing both discrete and continuous action policies; on a real Franka robot, it succeeds across a diverse task suite, including a challenging bin-picking task that requires multi-step planning, demonstrating robust real-world performance. Together, these results underscore the promise of unified diffusion frameworks for practical, high-performance VLA robotics.",
            "introduction": "Vision-language-action (VLA) models have emerged as the next-generation framework in robotics, integrating visual perception, language reasoning, and robotic control into unified systems (Black et al., 2024; Brohan et al., 2023; Kim et al., 2024; Hu et al., 2023; Liu et al., 2025a; Intelligence et al., 2025a; Kim et al., 2025; Team et al., 2025; Bjorck et al., 2025; Zhao et al., 2025a; b; Zhen et al., 2024; Wen et al., 2025a; b; Zhou et al., 2025b; Wen et al., 2024). The development of VLA models has undergone two stages of evolution. In the first stage, a pre‑trained vision–language backbone is used purely as a feature extractor, and the extracted features are mapped directly to robot actions. As vanilla VLA architectures proved inadequate for open‑world instruction following and long‑horizon tasks, a second‑stage training paradigm co‑trains on image–text data alongside action trajectories to preserve knowledge from the pre‑trained VLM and, when necessary, to predict both sub-step reasoning and robot actions (Zhou et al., 2025b; a; Intelligence et al., 2025b; Driess et al., 2025). The sub-step reasoning, often referred to as Chain-of-Thought, grounds high-level instructions into low-level sub-steps, thereby offering improved guidance for action prediction. Recent works have also incorporated image generation capabilities into VLAs, enabling the prediction of subsequent images before generating actions, which is a visual form of Chain-of-Thought Zhao et al. (2025a); Cen et al. (2025). Leveraging images as intermediate reasoning steps offers a more detailed description of the next movement. Such approaches have demonstrated remarkable capabilities, enabling models to generalize to novel environments, adapt to new objects, and even complete tasks requiring complex reasoning, such as mathematical puzzle games (Zhou et al., 2025a; Zhao et al., 2025a).\n\nDespite their promise, these models face several limitations. First, co-training visual-text data alongside robotic action data, each with distinct objectives, often results in gradient conflicts. Specifically, the gradients that enhance knowledge preservation and scene understanding may interfere with the model’s ability to effectively learn robot actions, even when a separate module is dedicated to this task. Second, integrating image generation into auto-regressive Vision-Language Models (VLMs) is challenging due to the fundamental gap between training objectives and model architectures, which makes harmonizing multi-modal generation and understanding difficult. Consequently, VLAs struggle to fully exploit knowledge across all modalities, limiting their ability to capture the underlying physical laws that connect actions and generated images, even when equipped with an explicit Chain-of-Thought.\n\nTo address these challenges, we propose dVLA, a framework that jointly optimizes visual reasoning, image generation, and robotic manipulation under a unified diffusion-based objective. dVLA builds on MMaDA (Yang et al., 2025), an advanced model in discrete diffusion language models that unifies multimodal understanding and generation through a consistent discretization strategy, employing modality-specific tokenizers. To extend this foundation to actions, we adopt FAST (Pertsch et al., 2025) to encode action sequences into compact discrete tokens, enabling dVLA to leverage pretrained visual–textual knowledge for generating executable actions. However, simply discretizing actions and applying a unified training objective is insufficient. Such an approach exploits only MMaDA’s multimodal understanding capabilities while neglecting its core strength—multimodal generation. To overcome this limitation, we introduce a multimodal Chain-of-Thought (CoT) training paradigm, in which dVLA is required to simultaneously generate subgoal images (visual CoT), textual reasoning, and action sequences. Concretely, during training we randomly mask tokens not only from actions but also from subgoal images and textual reasoning, and the model is required to reconstruct them across all available modalities. This design encourages dVLA to learn a shared parameter space, ensuring strong consistency between predicted subgoal images and actual execution outcomes. Empirically, we observe that dVLA can even forecast failed execution images that precisely match real-world failures, suggesting that it learns not just to generate fixed sub-goal images but also to capture the underlying physical laws governing action and perception.\n\nIn this paper, we conduct a comprehensive evaluation of dVLA through rigorous experimental analysis. On the LIBERO benchmark, dVLA achieves an average success rate of 96.4%, consistently outperforms both discrete and continuous action policies, and achieves state-of-the-art performance. We further validate our approach on a real Franka robot across a wide range of tasks, including the challenging bin-picking task, which requires multi-step planning to complete. The results demonstrate dVLA’s superior ability to handle the complexities of real-world scenarios, highlighting its potential to significantly advance the capabilities of vision-language-action robotic systems. Since multimodal CoT prediction increases inference cost, we introduce two acceleration strategies: prefix attention mask and KV caching. These optimizations yield up to ∼2×\\sim 2\\times speedup in both real-world tasks and the LIBERO benchmark, with only marginal performance degradation.",
            "llm_summary": "【论文的motivation是什么】  \n1. VLA模型在开放世界指令跟随和长时间任务中表现不足。  \n2. 现有模型在多模态生成和理解之间存在协调困难，限制了其能力。  \n3. 需要一个统一的框架来优化视觉推理、图像生成和机器人操作。  \n\n【提出了什么创新的方法】  \n提出了dVLA，一个基于扩散的VLA模型，利用多模态的Chain-of-Thought（CoT）来统一视觉感知、语言推理和机器人控制。该模型通过联合优化视觉推理、图像生成和机器人操作，克服了传统VLA模型的局限性。dVLA在LIBERO基准测试中实现了96.4%的平均成功率，并在真实的Franka机器人上成功完成了多种任务，包括复杂的多步骤规划的箱子拾取任务，显示了其在真实场景中的强大能力。此外，通过引入前缀注意力掩码和KV缓存等加速策略，dVLA在推理时实现了约2倍的速度提升，确保了高效的实际应用。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Field Calibration of Hyperspectral Cameras for Terrain Inference",
            "authors": "Nathaniel Hanson,Benjamin Pyatski,Samuel Hibbard,Gary Lvov,Oscar De La Garza,Charles DiMarzio,Kristen L. Dorsey,Taşkın Padır",
            "subjects": "Robotics (cs.RO); Image and Video Processing (eess.IV)",
            "comment": "Accepted to IEEE Robotics & Automation Letters",
            "pdf_link": "https://arxiv.org/pdf/2509.25663",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25663",
            "arxiv_html_link": "https://arxiv.org/html/2509.25663v1",
            "abstract": "Intra-class terrain differences such as water content directly influence a vehicle’s ability to traverse terrain, yet RGB vision systems may fail to distinguish these properties.\nEvaluating a terrain’s spectral content beyond red-green-blue wavelengths to the near infrared spectrum provides useful information for intra-class identification. However, accurate analysis of this spectral information is highly dependent on ambient illumination.\nWe demonstrate a system architecture to collect and register multi-wavelength, hyperspectral images from a mobile robot and describe an approach to reflectance calibrate cameras under varying illumination conditions.\nTo showcase the practical applications of our system, HYPER DRIVE, we demonstrate the ability to calculate vegetative health indices and soil moisture content from a mobile robot platform.",
            "introduction": "Understanding where to drive is a critical question for autonomous vehicles [1]. Many systems operate by identifying and defining safe regions for traversability such as road, grass, soil, and sand from RGB camera images [2].\nWhile broad terrain labels are useful in semantic segmentation, they do not capture intra-class differences—an oil slick on a road, ice on soil, or water-logged grass—that may impact vehicle traversability.\nAdding spectral data as a complement to RGB images improves class separation [3], so incorporating intra-class variations into robot intelligence may similarly improve performance. To realize this vision—dynamically identifying these differences onboard a field robot—new sensing and algorithmic approaches are required.\n\nHyperspectral imaging (HSI) is well-established in remote sensing for airborne terrain monitoring, material detection [4], and anomaly identification [5].\nIn contrast to the three intensity values present in RGB cameras, hyperspectral imagers record dozens of discrete wavelengths.\nCharacteristic absorbance peaks across the visible-to-infrared spectrum due to water, organic compounds, and other chemical compositions will then provide classification insight.\nWhile HSI often improves inter-class separability over RGB alone [3], its use has been constrained to environments with well-controlled illumination or frequent camera re-calibration using a target. These constraints are untenable for robots in the field.\n\nWe propose to use field robot-based HSI to identify intra-class terrain variations and dynamically calibrate changes in illumination conditions that would otherwise distort the spectral information. We apply HSI to identify intra-class terrain variations and demonstrate a system, HYPER DRIVE, to capture datacubes (i.e., a multi-dimensional view across spectrum and space) from a moving platform (Fig. 1). We develop and validate a data-driven approach to dynamically calibrate the imaging areas via a learned up-sampling network that references a calibration target continuously sampled by an on-board point spectrometer.\nOur work shows that forward-facing HSI enables low computational cost differentiation of visually similar terrain and is practical for field robotics.\n\nThis work contributes the following to the state-of-the-art:\n\nThe design and development of HYPER DRIVE, a field robot specific hyperspectral camera system,\n\nA method for spectrometer-hyperspectral joint calibration under varying illumination conditions, and\n\nThe evaluation of HYPERDRIVE’s performance in estimating soil moisture content and vegetative health.\n\n1. The design and development of HYPER DRIVE, a field robot specific hyperspectral camera system,\n\n2. A method for spectrometer-hyperspectral joint calibration under varying illumination conditions, and\n\n3. The evaluation of HYPERDRIVE’s performance in estimating soil moisture content and vegetative health.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有RGB视觉系统无法有效区分地形的内部差异，影响自动驾驶车辆的行驶能力。  \n2. 需要新的传感和算法方法来动态识别地形内部差异，以提高机器人智能性能。  \n\n【提出了什么创新的方法】  \n提出了一种基于场地机器人的高光谱成像系统HYPER DRIVE，能够在变化的光照条件下进行反射率校准。该系统通过移动平台捕获多维数据立方体，并利用学习的上采样网络进行动态校准。研究表明，HYPER DRIVE能够以低计算成本有效区分视觉上相似的地形，并在估计土壤湿度和植物健康方面表现出色。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Exhaustive-Serve-Longest Control for Multi-robot Scheduling Systems",
            "authors": "Mohammad Merati,David Castañón",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY); Optimization and Control (math.OC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25556",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25556",
            "arxiv_html_link": "https://arxiv.org/html/2509.25556v1",
            "abstract": "We study online task allocation for multi-robot, multi-queue systems with stochastic arrivals and switching delays. Time is slotted; each location can host at most one robot per slot, service consumes one slot, switching between locations incurs a one-slot travel delay, and arrivals are independent Bernoulli processes. We formulate a discounted-cost Markov decision process and propose Exhaustive-Serve-Longest (ESL), a simple real-time policy that serves exhaustively when the current location is nonempty and, when idle, switches to a longest unoccupied nonempty location, and we prove the optimality of such policy. As baselines, we tune a fixed-dwell cyclic policy via a discrete-time delay expression and implement a first-come–first-serve policy. Across server–location ratios and loads, ESL consistently yields lower discounted holding cost and smaller mean queue lengths, with action-time fractions showing more serving and restrained switching. Its simplicity and robustness make ESL a practical default for real-time multi-robot scheduling systems.",
            "introduction": "Real-time task allocation and routing is a key problem in modern multi-robot systems that arises in logistics, healthcare, maintenance, and urban services. As new tasks arrive, mobile robots repeatedly decide where to go and what to serve, under constraints that require travel/setup times and avoidance of duplication of effort. Our goal is to develop optimal feedback policies for routing and control of multirobot systems in response to randomly arriving spatially distributed tasks.\n\nRecent work on hospital logistics considers task assignment and routing for mobile robots incorporating uncertainty in travel times, service times and energy constraints [1, 2, 3]. However, the tasks in these works are known and do not arrive randomly. The task assignment and routing problem is posed as a dynamic vehicle routing problem (DVRP) with uncertain travel times, and is solved using heuristic techniques. Similar dynamic vehicle routing problems arise in other applications [4, 5, 6, 7, 8], where uncertainty in travel/service times or fuel consumption is incorporated into heuristic search algorithms for routing and task assignment with replanning once information is collected on travel times.\n\nOnline traveling salesperson problems [9, 10] address single robot problems with new task arrivals, but focus primarily on whether arrivals should be accepted or rejected. If accepted, the replanning algorithm is a fast replanning algorithm with the new task. The work [11] considers multi-robot task assignment with distributed, market-based algorithms, but new task arrivals require replanning.\n\nMulti-class queueing networks address the sequencing and routing of tasks in networks as a stochastic control problem. Although optimal control algorithms are difficult except in simple cases, [12] develops bounds on achievable performance. However, servers in these networks are not mobile, and thus fail to capture the inherent tradeoffs in moving servers to address spatially arriving new tasks.\n\nDynamic vehicle routing problems with stochastic customer requests are often posed as Markov Decision Processes [13]. However, the combinatorial complexity of deterministic vehicle routing problems and the curse of dimensionality leads to the use of approximate dynamic programming (ADP), using either sample scenarios or rollout algorithms. Furthermore, these ADP algorithms often use routing heuristics to address the combinatorial complexity of vehicle routing problems [13, 14, 15, 16, 17].\n\nTask allocation with multiple servers and new arrivals is often handled via re-optimization. Our work in [18] solves such problems using dynamic network-flow algorithms, which are fast to allow repeated re-solving as new tasks appear. Reoptimization approaches work well when newly-arrived tasks are a small fraction of the available tasks, and fail to anticipate the stochastic arrival process.\nDistributed auction approaches have been proposed to allocate tasks with deadlines by minimizing total transportation time [19], where new task arrivals trigger replanning with reoptimization.\n\nApproximate dynamic programming using deterministic cost-to-go approximations are used in stochastic multiple knapsack problems [20]. Another task assignment problem with stochastic arrivals arises in in multichannel allocation for mobile networks [21, 22] using index policies for bandit problems. However, these formulations neglect the switching times associated with assigning incoming tasks to channels. Other applications with stochastic event arrivals involve persistent monitoring [23], where stations are visited in a cyclic visit order, and dwell times at stations is controlled based on the observed state of the system.\n\nOur work in this paper is more closely related to feedback control of multi-class queuing networks. For such networks, feedback-regulated policies have been developed using fluid approximations and Markov Decision Process techniques in [24]. Recent work in [25] shows deterministic switching times alone can significantly inflate delays in simple M/M/kk systems, where routing and assignment is done trivially. Hofri and Ross [26] study the optimal control of a single server, two-queue system with server setup times to switch from serving one queue to another.\n\nIn this work, we study a simple model of online multi-robot task allocation with stochastic per-location arrivals to generalize the results of Hour and Ross [26] to multiple robots and multiple arrival locations. We assume independent task arrivals at each location, modeled in discrete time by Bernoulli processes. We pose the feedback control problem as a discounted, infinite horizon Markov Decision Problem. Under simple symmetry conditions, we show that an optimal feedback policy, ESL (Exhaustive-Serve-Longest) takes the form of a simple rule: robots serve tasks in its current location until exhausted, and then switch to the unattended location with the most remaining tasks. We prove the optimality of this policy, and benchmark its performance in simulation against other simple feedback rules proposed in the literature: FCFS per task (prioritizes the oldest waiting task across locations) and a Cyclic policy with optimized fixed dwell and one-slot travel between partitioned location blocks.\nOur experiments show that, across different numbers of robots and arrival rates, the feedback ESL policy consistently achieves the lowest discounted cost and smallest mean queue lengths, as well as increased fractions of time serving tasks.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要解决多机器人系统中的实时任务分配和调度问题。  \n2. 现有方法未能有效处理随机到达的任务和移动服务器的调度。  \n3. 需要开发简单且有效的反馈控制策略以应对随机任务到达。  \n\n【提出了什么创新的方法】  \n提出了一种名为Exhaustive-Serve-Longest (ESL)的实时策略，该策略在当前地点任务未完成时进行全面服务，并在空闲时切换到未被占用且任务最多的地点。通过将问题建模为折扣成本的马尔可夫决策过程，证明了该策略的最优性。实验结果显示，ESL策略在不同的机器人数量和到达率下，始终实现了最低的折扣成本和最小的平均队列长度，同时增加了服务任务的时间比例。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Online Mapping for Autonomous Driving: Addressing Sensor Generalization and Dynamic Map Updates in Campus Environments",
            "authors": "Zihan Zhang,Abhijit Ravichandran,Pragnya Korti,Luobin Wang,Henrik I. Christensen",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "19th International Symposium on Experimental Robotics",
            "pdf_link": "https://arxiv.org/pdf/2509.25542",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25542",
            "arxiv_html_link": "https://arxiv.org/html/2509.25542v1",
            "abstract": "High-definition (HD) maps are essential for autonomous driving, providing precise information such as road boundaries, lane dividers, and crosswalks to enable safe and accurate navigation. However, traditional HD map generation is labor-intensive, expensive, and difficult to maintain in dynamic environments. To overcome these challenges, we present a real-world deployment of an online mapping system on a campus golf cart platform equipped with dual front cameras and a LiDAR sensor. Our work tackles three core challenges: (1) labeling a 3D HD map for campus environment; (2) integrating and generalizing the SemVecMap [1] model onboard; and (3) incrementally generating and updating the predicted HD map to capture environmental changes. By fine-tuning with campus-specific data, our pipeline produces accurate map predictions and supports continual updates, demonstrating its practical value in real-world autonomous driving scenarios.",
            "introduction": "High-definition (HD) maps play a critical role in autonomous driving by supplying precise geometric and semantic information—such as road boundaries, lane dividers, and crosswalks—to enable accurate localization, prediction, and planning [2].\nHowever, producing HD maps remains costly and labor-intensive. Practitioners must manually annotate centimeter-level road geometry, sequence those details into waypoints, and rigorously validate the result. Maintaining such maps is even harder, because the physical world changes constantly and reliably detecting every local modification is still an open problem.\nThis poses a significant barrier to scalability, especially in dynamic or previously unmapped environments. For example, Fig.1 shows an area under construction in our campus environment, where the roads marked by a red cross were undrivable before, and we were unable to obtain the ground truth map for that area.\n\nRecent progress in perception and learning-based systems has enabled a new paradigm: online mapping, where map elements are automatically generated from onboard sensors such as cameras and LiDAR. Despite promising results in static benchmarks [3], these models are rarely evaluated in real-world systems, where sensor generalization and environmental changes pose challenges.\n\nIn this work, we provide a step forward in closing the gap between online mapping research and real-world deployment. We present a complete real-world deployment of an online mapping system [1] on a campus autonomous driving platform equipped with dual front-facing cameras and a LiDAR sensor [4]. Additionally, we propose a clustering-based method to construct HD map priors from online mapping results. Our system enables the generation and continual update of HD maps in previously unseen environments. Our contributions are as follows:\n\nReal-world deployment: We demonstrate the feasibility of using a sensor-generalizable online mapping model in a real-world autonomous driving system, addressing the challenges of sensor variation and environmental dynamics\n\n3D ground truth map labeling: We present our system of generating fine-grained HD annotations, enabling high-quality supervision in unseen environments.\n\nModel fine-tuning and adaptation: We fine-tune a mapping model SemVecNet [1] using campus-specific labels to enhance performance in unseen environments, showing significant improvements across varied road scenarios.\n\nIncremental map update mechanism: We develop a novel pipeline that accumulates multi-frame map predictions to incrementally refine the HD map, enabling the generation new map and detection of changes such as construction and reconfigured lanes.\n\n1. Real-world deployment: We demonstrate the feasibility of using a sensor-generalizable online mapping model in a real-world autonomous driving system, addressing the challenges of sensor variation and environmental dynamics\n\n2. 3D ground truth map labeling: We present our system of generating fine-grained HD annotations, enabling high-quality supervision in unseen environments.\n\n3. Model fine-tuning and adaptation: We fine-tune a mapping model SemVecNet [1] using campus-specific labels to enhance performance in unseen environments, showing significant improvements across varied road scenarios.\n\n4. Incremental map update mechanism: We develop a novel pipeline that accumulates multi-frame map predictions to incrementally refine the HD map, enabling the generation new map and detection of changes such as construction and reconfigured lanes.",
            "llm_summary": "【论文的motivation是什么】  \n1. 高精度地图生成过程成本高、劳动密集，难以维护。  \n2. 动态环境下地图更新的可靠性和实时性仍然是一个开放问题。  \n3. 现有在线映射模型在真实世界应用中的传感器泛化能力不足。  \n\n【提出了什么创新的方法】  \n本研究提出了一种在线映射系统，能够在校园环境中实时生成和更新高精度地图。该系统利用双前置摄像头和LiDAR传感器，解决了传感器变化和环境动态带来的挑战。通过对校园特定数据进行微调，系统能够生成精细的HD注释，并实现地图的增量更新，显著提高了在未见环境中的性能。最终，实验结果表明，该系统在真实世界的自主驾驶场景中具有实用价值。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "CoTaP: Compliant Task Pipeline and Reinforcement Learning of Its Controller with Compliance Modulation",
            "authors": "Zewen He,Chenyuan Chen,Dilshod Azizov,Yoshihiko Nakamura",
            "subjects": "Robotics (cs.RO)",
            "comment": "Submitted to IEEE for possible publication, under review",
            "pdf_link": "https://arxiv.org/pdf/2509.25443",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25443",
            "arxiv_html_link": "https://arxiv.org/html/2509.25443v1",
            "abstract": "Humanoid whole-body locomotion control is a critical approach for humanoid robots to leverage their inherent advantages.\nLearning-based control methods derived from retargeted human motion data provide an effective means of addressing this issue.\nHowever, because most current human datasets lack measured force data, and learning-based robot control is largely position-based, achieving appropriate compliance during interaction with real environments remains challenging.\nThis paper presents Compliant Task Pipeline (CoTaP): a pipeline that leverages compliance information in the learning-based structure of humanoid robots.\nA two-stage dual-agent reinforcement learning framework combined with model-based compliance control for humanoid robots is proposed.\nIn the training process, first a base policy with a position-based controller is trained; then in the distillation, the upper-body policy is combined with model-based compliance control, and the lower-body agent is guided by the base policy.\nIn the upper-body control, adjustable task-space compliance can be specified and integrated with other controllers through compliance modulation on the symmetric positive definite (SPD) manifold, ensuring system stability.\nWe validated the feasibility of the proposed strategy in simulation, primarily comparing the responses to external disturbances under different compliance settings.\nFor detailed experimental results, please see the attached video. 111Attached video: https://drive.google.com/file/d/1Ge08DPEVZRw04pIZNqqBSJQaBJkdImoH/view?usp=sharing",
            "introduction": "In recent decades, humanoid robot technology has made significant advancements.\nParticularly over the past five years, there has been a surge in the development of diverse humanoid robot body designs, including Atlas from Boston Dynamics, Optimus from Tesla, Figure’s humanoid robots, and Unitree’s humanoid robots like H1 and G1.\nMeanwhile, with the rapid progress in the field of artificial intelligence, reinforcement learning (RL) and imitation learning (IL) have been increasingly applied to the control of humanoid robots, leading to significant breakthroughs.\nIn the first step, the imitation motion controller was applied in the simulation for humanoid character control, such as in [1, 2].\nAfter that, this approach was extended into real humanoid robot whoel-body control (WBC) [3, 4].\nBased on the learning method, the controller no longer requires accurate modeling of the robot and environment such as model predictive control (MPC), which demonstrates improved robustness and generalizability in complex environments.\n\nHumanoid robots have the key features that can simultaneously perform locomotion and manipulation, which is abbreviated as loco-manipulation [5].\nIn many studies, the upper and lower bodies of humanoid robots are controlled independently. During manipulation tasks, the legs are typically kept stationary to maintain balance and ensure stability of the center of mass (CoM) [6].\nIn recent studies, IL based on human motion data has also been applied to loco-manipulation. The data sources include publicly available human motion datasets as well as data collected through tele-operation [7, 8].\n\nThere are several key aspects in the current research receiving significant attention.\nFirst, most current research focuses on joint space PD control for humanoid motion control.\nThis control approach may yield satisfactory results in current purely motion control scenarios; however, it fails to implement force control when interactions with the environment (such as manipulation and multi-contact motion) or even human-robot interactions (HRI) occur, thereby making it difficult to achieve desirable outcomes.\nThen, most human even humanoid robot data are only proprioception-based, which lacks sensory input and action output.\nAlthough some studies have already attempted to incorporate contact force data into human motion data collection [9, 10], the overall size and generality of such datasets remain insufficient.\nIn addition, the commonly used physical simulators make sim-to-real transfer more difficult, as they lack accuracy and are computationally expensive when simulating complex contacts.\nTherefore, a key challenge lies in how to leverage the currently limited data resources to achieve force control for humanoid robot loco-manipulation.\n\nTo address these challenges in a comprehensive manner, we turn to a classical topic in traditional robotics: compliance control.\nCompliance control, by adopting a relatively passive mechanism, is capable of adapting to unknown or inaccurate contacts. Such a property is exactly what is required to overcome the training challenges arising from the lack of sufficient contact information;\nmoreover, compliance control inherently provides force control capabilities, which makes it particularly suitable for addressing the challenges of real-world robot–environment interactions in loco-manipulation tasks.\nIn model-based robot control theory, compliance control is a practical method to guarantee robot interaction safety and stability with the environment.\n[11, 12] proposed the hierarchical compliance control method for humanoid robot.\nIn the study [13, 14], the authors optimized the joint-space viscoelasticity matrices by an analytical way.\nThis work has enabled compliance control to achieve promising results in maintaining balance in humanoid robots.\nIn the field of robotic manipulation, compliance control is even more critical. It plays a key role in ensuring the robot’s safety, enhancing HRI, and enabling adaptive manipulation capabilities [15, 16].\n\nCompared with model-based control, the greatest advantage of RL lies in its robustness and generalization in complex and dynamic tasks.\nOn this basis, RL method has also been integrated into compliance control.\nIn [17], the controller achieves compliant behavior by modulating the target position, which is fundamentally akin to admittance control rather than direct force control.\nIn other related studies [18, 19, 20], the RL-based controller typically focuses on optimizing the PD gains of individual joints or a single limb, without considering whole-body compliance.\nRecently, some methods based on model-free RL for compliance control and even force control have been proposed [21, 22, 23], but their accuracy has not been guaranteed.\n\nAccordingly, the immediate task is to establish how parameter adjustability and stability of compliance control can be ensured within the RL framework, and to further verify the compliance effect under external perturbations.\nTherefore, we propose Compliant Task Pipeline (CoTaP), a pipeline leveraging the compliance information in the humanoid robot loco-manipulation control.\nAs illustrated in Fig. 2, in this paper we mainly focus on the compliance modulation on the RL framework of the pipeline.\nIn this study, we present a compliance control approach that integrates the RL control framework with the robot’s kinematic model. By performing stiffness matrix modulation on the symmetric positive definite (SPD) manifold, the stability of the joint-space control is guaranteed.\nFor humanoid whole-body control, a two-stage dual-agent policy training framework is applied in our work.\n\nThe main contributions of this study are as follows:\n\nCombined model-based compliance control with humanoid robot reinforcement learning control framework, designing a learning-based compliance control strategy including dual-agent policy and compliance modulation on SPD manifold for upper-body control;\n\nValidated effectiveness of the proposed compliance control, achieving compliance modulation performance of a humanoid robot in simulation.\n\n1. Combined model-based compliance control with humanoid robot reinforcement learning control framework, designing a learning-based compliance control strategy including dual-agent policy and compliance modulation on SPD manifold for upper-body control;\n\n2. Validated effectiveness of the proposed compliance control, achieving compliance modulation performance of a humanoid robot in simulation.",
            "llm_summary": "【论文的motivation是什么】  \n1. 当前人类数据集缺乏测量的力数据，导致机器人控制难以实现合适的合规性。  \n2. 现有的学习控制方法主要基于位置，难以应对真实环境中的交互。  \n3. 如何利用有限的数据资源实现人形机器人在任务中的力控制是一个关键挑战。  \n\n【提出了什么创新的方法】  \n本研究提出了合规任务管道（CoTaP），结合了基于模型的合规控制与人形机器人强化学习控制框架。通过在对称正定（SPD）流形上进行刚度矩阵调制，确保了关节空间控制的稳定性。该方法采用两阶段双代理策略训练框架，首先训练基于位置的控制策略，然后在蒸馏阶段结合模型基于的合规控制。通过仿真实验验证了该策略的有效性，展示了在不同合规设置下对外部干扰的响应能力。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Parallel Heuristic Search as Inference for Actor-Critic Reinforcement Learning Models",
            "authors": "Hanlan Yang,Itamar Mishani,Luca Pivetti,Zachary Kingston,Maxim Likhachev",
            "subjects": "Robotics (cs.RO)",
            "comment": "Submitted for Publication",
            "pdf_link": "https://arxiv.org/pdf/2509.25402",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25402",
            "arxiv_html_link": "https://arxiv.org/html/2509.25402v1",
            "abstract": "Actor-Critic models are a class of model-free deep reinforcement learning (RL) algorithms that have demonstrated effectiveness across various robot learning tasks. While considerable research has focused on improving training stability and data sampling efficiency, most deployment strategies have remained relatively simplistic, typically relying on direct actor policy rollouts. In contrast, we propose Pachs (Parallel Actor-Critic Heuristic Search), an efficient parallel best-first search algorithm for inference that leverages both components of the actor-critic architecture: the actor network generates actions, while the critic network provides cost-to-go estimates to guide the search. Two levels of parallelism are employed within the search—actions and cost-to-go estimates are generated in batches by the actor and critic networks respectively, and graph expansion is distributed across multiple threads. We demonstrate the effectiveness of our approach in robotic manipulation tasks, including collision-free motion planning and contact-rich interactions such as non-prehensile pushing. Visit p-achs.github.io for demonstrations and examples.",
            "introduction": "Reinforcement Learning (RL) has become a central paradigm for robot control, offering a way to learn behaviors that are difficult to manually specify through cost functions, dynamics models, or action abstractions. Despite this strength, RL methods still face significant challenges in generalization during inference and in solving complex problems that are common in robotic manipulation. Much of the prior work has focused on improving model architectures and training strategies, but comparatively little attention has been given to inference strategies. As a result, RL models are typically deployed as one-step predictors during execution, lacking the ability to perform multi-step forward reasoning or backtracking.\n\nConsider the example shown in Fig. 1, where the objective is to push the T-shaped object to a target pose. While specifying and modeling this task with classical model-based planning algorithms is challenging due to complex contact dynamics, an RL model can learn non-prehensile manipulation behaviors through a simple reward function.\nSuch models perform well in relatively simple settings, for example, when the goal is fixed and the environment is uncluttered. However, as problems become more complex and require inference generality (e.g., when multiple objects must be manipulated sequentially, or when goals depend on environmental constraints), RL often fails to learn sufficiently robust policies. In these cases, search methods provide an alternative: they can explicitly plan multiple steps into the future, enabling exploration of the state space through branching and backtracking.\n\nThe main goal of this work is to combine the advantages of traditional planning algorithms with those of learned models.\nSpecifically, this work integrates a Soft Actor–Critic (SAC) model, trained to control a robotic arm, within a best-first heuristic search framework. In this integration, the actor network proposes candidate actions (i.e., edges) while the critic network serves as a learned heuristic function. The reinforcement learning approach underlying SAC enables the model to capture complex information that is often difficult to represent with hand-crafted action spaces, heuristics and cost functions, such as robot-object interactions, dynamics, and environmental constraints.\nFurthermore, to improve efficiency, we parallelize the exploration phase of the search process by assigning separate threads to each expansion step, and, if possible, by evaluating edges in parallel.\n\nWe evaluate the proposed approach on two different applications. The first scenario involves a shelf environment where the robotic arm must move its end effector to a desired spatial position along a collision-free path.\nThe second scenario presents a push-T task, where the arm must interact with a T-shaped object to push it to a specific position and orientation.\nHere, we trained the model in an obstacle-free environment and then applied the algorithm in environments with added obstacles to study generalizability. The results demonstrate substantial performance improvements, enabling the effective use of imperfectly trained models in complex scenarios through search–a common challenge in RL deployment.\n\nOur contributions are:\n\nA novel algorithm, Pachs, enabling best-first search planning with learned actor-critic RL models.\n\nMulti-layered parallelization strategies (CPU thread-level and GPU batch-level) that achieve significant computational efficiency gains.\n\nExperimental evaluations, demonstrating how Pachs improves the deployment and generalization of RL models by enabling robust performance in complex environments.\n\n1. A novel algorithm, Pachs, enabling best-first search planning with learned actor-critic RL models.\n\n2. Multi-layered parallelization strategies (CPU thread-level and GPU batch-level) that achieve significant computational efficiency gains.\n\n3. Experimental evaluations, demonstrating how Pachs improves the deployment and generalization of RL models by enabling robust performance in complex environments.",
            "llm_summary": "【论文的motivation是什么】  \n1. RL methods face challenges in generalization during inference and solving complex robotic manipulation problems.  \n2. Prior work has focused more on model architectures and training strategies than on inference strategies.  \n3. Existing RL models typically lack multi-step forward reasoning and backtracking capabilities during execution.  \n\n【提出了什么创新的方法】  \n提出了一种名为Pachs的算法，结合了Soft Actor-Critic模型与最佳优先启发式搜索框架。该方法通过演员网络生成候选动作，并利用评论员网络提供成本估计，进行高效的并行搜索。通过在搜索过程中引入多层次并行化策略，Pachs显著提高了计算效率。实验表明，该方法在复杂环境中有效提升了RL模型的部署和泛化能力，尤其在处理障碍物和复杂交互任务时表现优异。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "SARM: Stage-Aware Reward Modeling for Long Horizon Robot Manipulation",
            "authors": "Qianzhong Chen,Justin Yu,Mac Schwager,Pieter Abbeel,Fred Shentu,Philipp Wu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25358",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25358",
            "arxiv_html_link": "https://arxiv.org/html/2509.25358v1",
            "abstract": "Large scale robot learning has recently shown promise in enabling robots to perform complex tasks by integrating perception, control, and optionally, language understanding into a unified framework. However, they continue to struggle with long-horizon, contact-rich manipulation tasks, such as the handling of deformable objects, where supervision from demonstrations is often inconsistent in quality. In such settings, reward modeling offers a natural solution: by providing grounded progress signals, it can transform noisy demonstrations into stable supervision that generalizes across diverse trajectories. In this work, we introduce a stage-aware, video-based reward modeling framework that jointly predicts the high-level task stage and fine-grained progress within each stage. Reward labels are automatically derived from natural language subtask annotations, enabling consistent progress estimation across variable-length and heterogeneous demonstrations. This design overcomes the limitations of frame-index-based labeling, which collapses in long, variable-duration tasks such as folding a T-shirt. Our reward model demonstrates robustness to demonstration variability, generalization to out-of-distribution scenarios, and strong utility for downstream policy training. Building upon this reward model, we propose the Reward-Aligned Behavior Cloning (RA-BC) framework, which selectively filters high-quality data and reweights training samples according to reward estimates. Extensive experiments demonstrate that the reward model outperforms baselines on out-of-distribution real robot policy rollouts and human demonstration validation. Our approach achieves 83% success on folding T-shirts from the flattened state and 67% from the crumpled state—dramatically surpassing vanilla behavior cloning, which attains only 8% and 0% success under the same training dataset, respectively. Overall, our results highlight reward modeling as a key enabler for scalable, annotation-efficient, and robust imitation learning in long-horizon robotic manipulation. Project website: https://qianzhong-chen.github.io/sarm.github.io/.",
            "introduction": "The long-standing vision of enabling robots to seamlessly assist humans in household chores has inspired decades of research in robotics. From tidying living spaces to preparing meals, such capabilities hold the promise of freeing up human time, and improving quality of life. Recent progress in foundation models for robotics, or more generally robot behavior models (RBMs), has sparked renewed optimism toward this goal. By combining visual perception, motor control, and optionally language processing in a single framework, RBMs (Chi et al., 2023; Zhao et al., 2023; Chen et al., 2025; Sun et al., 2024; Huang et al., 2024; Yu et al., 2024; Wang et al., 2023a; Black et al., ; Team et al., 2024; Zitkovich et al., 2023; Shentu et al., 2024; Huang et al., 2025a; b) enable robots to perform complex tasks, making it possible to execute these tasks in unstructured household environments.\n\nDespite their promise, RBMs still struggle with long-horizon, contact-rich manipulation, particularly with deformable objects like T-shirts. Such tasks demand handling changing geometries, occlusions, fabric variations, and error-free multi-step planning—challenges where current models, often tuned for short-horizon rigid-object tasks, fall short. They fail to generalize beyond curated data, lose consistency over time, and misinterpret intermediate states.\nWhile many prior works in RBMs have focused on scaling up data (Barreiros et al., 2025; Lin et al., 2024), far less attention has been given to data quality.\nHowever, high-quality data is difficult to obtain: expert demonstrations are costly and time-intensive, while larger datasets often include noisy or suboptimal trajectories from less experienced operators. Even more challenging, demonstration quality itself is a difficult metric to quantify, since it depends on hidden factors such as action consistency and contact stability that cannot be directly measured, aside from simple proxy heuristics like task duration.\n\nIn light of these challenges, we propose a video-based reward modeling framework that leverages natural language annotations to assign progress labels and enable stable reward estimation for multi-step tasks. The learned reward model drives a Reward-Aligned Behavior Cloning (RA-BC) framework, filtering higher-quality data and improving policy performance in both simulation and the real world. Focusing on the T-shirt folding task, our experiments show that coupling the reward model with RA-BC significantly boosts performance, underscoring the importance of data quality in long-horizon manipulation. Together, these contributions advance scalable and annotation-efficient imitation learning. An overview is shown in Figure 1.\n\nOur contributions can be summarized as follows:\n\nWe present SARM: a stage-aware reward modeling framework that automatically derives task progress labels from natural language annotations. Given any subsequence of RGB frames, the model jointly predicts the current task stage and fine-grained progress within that subtask, achieving robustness, generalization to out-of-distribution scenarios, and strong utility for downstream policy learning.\n\nWe propose the RA-BC framework, which leverages the learned reward model to identify high-quality demonstrations and reweight training data accordingly.\n\nWe validate our approach on the real-world task of T-shirt folding, a challenging long-horizon task that requires manipulation a deformable objects, where it consistently outperforms strong behavior cloning baselines.\n\n1. We present SARM: a stage-aware reward modeling framework that automatically derives task progress labels from natural language annotations. Given any subsequence of RGB frames, the model jointly predicts the current task stage and fine-grained progress within that subtask, achieving robustness, generalization to out-of-distribution scenarios, and strong utility for downstream policy learning.\n\n2. We propose the RA-BC framework, which leverages the learned reward model to identify high-quality demonstrations and reweight training data accordingly.\n\n3. We validate our approach on the real-world task of T-shirt folding, a challenging long-horizon task that requires manipulation a deformable objects, where it consistently outperforms strong behavior cloning baselines.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有机器人学习在处理长时间、接触丰富的操作任务时表现不佳。  \n2. 从人类演示中获得的监督常常质量不一致，影响模型性能。  \n3. 高质量数据难以获取，且难以量化演示质量。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于视频的阶段感知奖励建模框架SARM，能够自动从自然语言注释中推导任务进度标签。该模型通过分析RGB帧序列，联合预测当前任务阶段及其细粒度进展，从而实现对多步骤任务的稳定奖励估计。结合奖励模型的RA-BC框架，能够筛选高质量演示并重新加权训练数据。实验表明，该方法在折叠T恤的真实任务中表现优异，成功率显著高于传统行为克隆方法，强调了数据质量在长时间操作中的重要性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "SRMP: Search-Based Robot Motion Planning Library",
            "authors": "Itamar Mishani,Yorai Shaoul,Ramkumar Natarajan,Jiaoyang Li,Maxim Likhachev",
            "subjects": "Robotics (cs.RO)",
            "comment": "Submitted for Publication",
            "pdf_link": "https://arxiv.org/pdf/2509.25352",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25352",
            "arxiv_html_link": "https://arxiv.org/html/2509.25352v1",
            "abstract": "Motion planning is a critical component in any robotic system.\nOver the years, powerful tools like the Open Motion Planning Library (OMPL) have been developed, offering numerous motion planning algorithms. However, existing frameworks often struggle to deliver the level of predictability and repeatability demanded by high-stakes applications—ranging from ensuring safety in industrial environments to the creation of high-quality motion datasets for robot learning. Complementing existing tools, we introduce SRMP (Search-based Robot Motion Planning), a new software framework tailored for robotic manipulation. SRMP distinguishes itself by generating consistent and reliable trajectories, and is the first software tool to offer motion planning algorithms for multi-robot manipulation tasks. SRMP easily integrates with major simulators, including MuJoCo, Sapien, Genesis, and PyBullet via a Python and C++ API. SRMP includes a dedicated MoveIt! plugin that enables immediate deployment on robot hardware and seamless integration with existing pipelines. Through extensive evaluations, we demonstrate in this paper that SRMP not only meets the rigorous demands of industrial and safety-critical applications but also sets a new standard for consistency in motion planning across diverse robotic systems. Visit srmp.readthedocs.io for SRMP documentation and tutorials.",
            "introduction": "The field of robotics is rapidly becoming more accessible to a wide range of practitioners. As tools for experimenting with robots have become easier to use, programmers with minimal robotics experience can now develop robotic applications in just a few lines of code. For instance, modern physics simulators such as MuJoCo [1], Sapien [2], and Genesis [3] enable users to simulate robotic systems effortlessly using Python, lowering the barrier to entry. This increased accessibility has led to a growing demand for motion planning libraries that are powerful, reliable, and integrable with modern robotic frameworks and simulators.\n\nMany great tools for motion planning have been developed over the past two decades. The Open Motion Planning Library (OMPL) [4], for instance, is widely used for its extensive collection of sampling-based algorithms, efficient implementations, and availability in popular robotics tools [5, 6]. However, OMPL, like many motion planning libraries that use sampling-based algorithms, has certain limitations that hinder its applicability for some real-world robotics use cases. Most notably, the solutions generated by sampling-based planners often exhibit significant variability – a negative property when consistency and interpretability of motions are important.\n\nVariability in generated motions can be particularly problematic in industrial applications and modern robot learning systems. In industrial settings, robots and humans often share a workspace, and for safe collaboration, human workers must be able to anticipate robot motions. This can be achieved through consistent and repeatable motion planning algorithms. Similar properties are also beneficial for robot learning, especially with recent approaches that increasingly rely on motion planners [7, 8, 9]. For example, MPD [8] found success in relying on motion planners for collecting demonstration data for trajectory learning, and PerAct [10] reduced the learning problem to sequences of end-effector key-poses and assumed access to reliable motion planners for moving between key-poses during policy execution. Existing tools, which mainly focus on finding some collision-free trajectory, may lead to unpredictable, potentially dangerous motions, inconsistent datasets, and variable execution results.\n\nTo address these challenges, we introduce Search-based Robot Motion Planning (SRMP): a software framework designed for consistent, safe, reliable and repeatable motion generation (Fig. 1). SRMP achieves this by leveraging state-of-the-art graph search algorithms with strong theoretical guarantees of completeness and bounded sub-optimality, which translate to short and repeatable motions. In practice, SRMP integrates seamlessly with widely used tools, including MoveIt!, various physics simulators, and numerous real-world robotic platforms. Its ease of use and strong algorithmic foundations make SRMP an effective motion planning framework for modern robotics applications, including data collection, industrial automation, and research. Our contributions are as follows:\n\nSRMP Software Framework: a new motion planning library for robotic manipulation that delivers reliable, repeatable, and interpretable plans through powerful search-based algorithms. SRMP directly overcomes key limitation of existing libraries by providing algorithms that are unavailable elsewhere.\n\nSupport for Scale: SRMP is the first motion planning library to support both single-robot and multi-robot manipulation planning.\n\nSeamless Integration: SRMP integrates with modern robotics tools, including MoveIt!, physics simulators (MuJoCo, Sapien, Genesis, etc.), and real hardware.\n\nMulti-Language Support: SRMP’s C++ optimized backend and Python APIs enable accessibility for both researchers and industry practitioners.\n\nSRMP bridges the gap between academic research and practical robotics applications and provides a robust, scalable solution for motion planning in simulation and real-world deployments. We begin by introducing SRMP (Sec. II), proceed with a discussion of experimental results (Sec. III), and close with relevant background (Sec. IV).",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有运动规划库在高风险应用中缺乏可预测性和重复性。  \n2. 运动规划算法在工业应用和机器人学习系统中产生的运动存在显著变异性。  \n3. 需要一个可靠、可重复的运动生成框架以确保人机协作的安全性。  \n\n【提出了什么创新的方法】  \n提出了SRMP（基于搜索的机器人运动规划）软件框架，利用先进的图搜索算法实现一致、安全、可靠和可重复的运动生成。SRMP支持单机器人和多机器人操作，能够与主流工具（如MoveIt!和多种物理模拟器）无缝集成。通过广泛的评估，SRMP展示了在工业和安全关键应用中的有效性，设定了运动规划一致性的新的标准。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "BEV-VLM: Trajectory Planning via Unified BEV Abstraction",
            "authors": "Guancheng Chen,Sheng Yang,Tong Zhan,Jian Wang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25249",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25249",
            "arxiv_html_link": "https://arxiv.org/html/2509.25249v1",
            "abstract": "This paper introduces BEV-VLM, a novel framework for trajectory planning in autonomous driving that leverages Vision-Language Models (VLMs) with Bird’s-Eye View (BEV) feature maps as visual inputs. Unlike conventional approaches that rely solely on raw visual data such as camera images, our method utilizes highly compressed and informative BEV representations, which are generated by fusing multi-modal sensor data (e.g., camera and LiDAR) and aligning them with HD Maps. This unified BEV-HD Map format provides a geometrically consistent and rich scene description, enabling VLMs to perform accurate trajectory planning. Experimental results on the nuScenes dataset demonstrate 44.8% improvements in planning accuracy and complete collision avoidance. Our work highlights that VLMs can effectively interpret processed visual representations like BEV features, expanding their applicability beyond raw images in trajectory planning.",
            "introduction": "In recent years, the pursuit of advanced autonomous driving (AD) has attracted extensive attention, with Vision-Language Models (VLMs) emerging as a promising pathway, owing to their inherent cognitive capabilities from pre-training that enable effective application in real-world scenarios. While existing research has demonstrated the feasibility and reliability of using VLMs for path planning by feeding visual camera images, these approaches suffer from two key limitations: they rely solely on camera data and thus lack integration with other modalities, such as LiDAR point clouds, and they fail to explore VLMs’ potential for planning based on Bird’s-Eye View (BEV) features.\n\nTo address these gaps, this work avoids the direct use of raw visual signals (e.g., camera images) as VLM inputs. Instead, we leverage pre-trained sensor-to-BEV feature models (e.g., BEVFusion [1], LSS [2]) and standard feature map generation techniques to preprocess multi-modal sensor data. This process produces highly compressed yet information-dense BEV feature maps, which offer three critical advantages: they condense key driving environment information into a single representation, reducing computational overhead; provide a unified space for multi-sensor fusion that accommodates sensors with different intrinsic and extrinsic parameters; and exhibit geometric consistency with readily available high-definition maps (HD Maps). After aligning BEV features with HD Maps based on local position, we render the latter onto the former to form a unified BEV-HD Map, and this integration incorporates road topological structure information, enabling VLMs to perform more accurate trajectory planning. We formulate our contributions as follows:\n\nWe present a novel perspective for applying VLMs to AD, in which preprocessed BEV feature maps are visualized and fed into the model, serving as a summary of current driving scenarios. Beyond raw visual images (such as camera images), BEV feature maps can also serve as visual inputs for VLMs.\n\nWe present a novel perspective for applying VLMs to AD, in which preprocessed BEV feature maps are visualized and fed into the model, serving as a summary of current driving scenarios. Beyond raw visual images (such as camera images), BEV feature maps can also serve as visual inputs for VLMs.\n\nBased on this highly compressed yet powerful scene representation, we enable compatibility with diverse sensor configurations while achieving fusion with local HD Maps, and this is accomplished without compromising the inference performance of the downstream planning module.\n\n1. We present a novel perspective for applying VLMs to AD, in which preprocessed BEV feature maps are visualized and fed into the model, serving as a summary of current driving scenarios. Beyond raw visual images (such as camera images), BEV feature maps can also serve as visual inputs for VLMs.\n\n2. Based on this highly compressed yet powerful scene representation, we enable compatibility with diverse sensor configurations while achieving fusion with local HD Maps, and this is accomplished without compromising the inference performance of the downstream planning module.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的路径规划方法仅依赖于相机数据，缺乏多模态传感器的整合。  \n2. 现有研究未探索基于鸟瞰图（BEV）特征的VLM潜力。  \n\n【提出了什么创新的方法】  \n本研究提出了BEV-VLM框架，通过将多模态传感器数据融合生成的BEV特征图作为输入，改善了自主驾驶中的轨迹规划。该方法利用BEV-HD Map格式，提供几何一致且信息丰富的场景描述，使VLM能够进行更准确的轨迹规划。实验结果表明，在nuScenes数据集上，规划准确率提高了44.8%，并实现了完全的碰撞避免。该工作展示了VLM在处理BEV特征方面的有效性，扩展了其在轨迹规划中的应用。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "When and How to Express Empathy in Human-Robot Interaction Scenarios",
            "authors": "Christian Arzate Cruz,Edwin C. Montiel-Vazquez,Chikara Maeda,Randy Gomez",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25200",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25200",
            "arxiv_html_link": "https://arxiv.org/html/2509.25200v1",
            "abstract": "Incorporating empathetic behavior into robots can improve their social effectiveness and interaction quality. In this paper, we present whEE (when and how to express empathy), a framework that enables social robots to detect when empathy is needed and generate appropriate responses. Using large language models, whEE identifies key behavioral empathy cues in human interactions. We evaluate it in human-robot interaction scenarios with our social robot, Haru. Results show that whEE effectively identifies and responds to empathy cues, providing valuable insights for designing social robots capable of adaptively modulating their empathy levels across various interaction contexts.",
            "introduction": "In most scenarios, Large Language Models (LLMs) represent the state-of-the-art approach for classifying empathy [1, 2] and generating empathetic responses [3, 4]. However, the development of robots capable of dynamically adjusting their level of empathy based on the context remains an underexplored area [5]. To this end, we introduce whEE (when and how to express empathy), an empathy framework that provides guidelines on when robots should respond empathetically and how to achieve it. Using our framework, we analyze the utterances of speakers and listeners in dyadic and group conversations with varying levels of empathy. Our analysis identifies key empathy cues that indicate when a speaker seeks an empathetic response and the cues exhibited by listeners displaying high levels of empathy.\n\nWe approach empathy by focusing on observable behaviors that individuals exhibit when demonstrating an understanding of others’ emotions and engaging deeply with their experiences—referred to as behavioral empathy [6].\n\nPrevious research has investigated the detection of empathy direction—whether a person is seeking or providing empathy—using datasets such as iEmpathize [7] and TwittEmp [8]. However, these studies have not included interactions from human-robot interaction (HRI) contexts and have yet to explore the effectiveness of LLMs in identifying empathy direction. This gap motivates our study, in which we apply our framework, whEE, specifically to HRI scenarios involving dyadic and group human-human interactions mediated by our social robot, Haru [9]. We use Haru as it aims to foster safe environments for improving human-human connection.\n\nAdditionally, whEE includes an empathetic text-generation module designed to enhance robot empathy. Using prerecorded human interactions as input, we systematically prompt empathetic responses from LLMs for Haru. We then compare these empathetic responses to Haru’s standard responses to evaluate their differences.\n\nWith the development of our empathy framework for HRI, whEE, we contribute to the literature by offering new insights into empathetic human-human and human-robot interactions. Besides, our findings can support researchers in designing robots that provide better empathy only when needed.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有研究未探讨机器人在动态调整同理心水平方面的能力。  \n2. 需要识别和响应人类交互中的同理心线索以提高机器人社交效果。  \n\n【提出了什么创新的方法】  \n提出了whEE框架，能够检测何时需要同理心并生成适当的响应。该框架利用大型语言模型识别关键的同理心行为线索，并在与社交机器人Haru的互动中进行评估。结果表明，whEE有效识别并响应同理心线索，为设计能够适应不同互动环境的社交机器人提供了宝贵的见解。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Benchmarking Egocentric Visual-Inertial SLAM at City Scale",
            "authors": "Anusha Krishnan,Shaohui Liu,Paul-Edouard Sarlin,Oscar Gentilhomme,David Caruso,Maurizio Monge,Richard Newcombe,Jakob Engel,Marc Pollefeys",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "ICCV 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.26639",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26639",
            "arxiv_html_link": "https://arxiv.org/html/2509.26639v1",
            "abstract": "Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard sensors is critical for wearable devices capturing egocentric data, which exhibits specific challenges, such as a wider diversity of motions and viewpoints, prevalent dynamic visual content, or long sessions affected by time-varying sensor calibration.\nWhile recent progress on SLAM has been swift, academic research is still driven by benchmarks that do not reflect these challenges or do not offer sufficiently accurate ground truth poses.\nIn this paper, we introduce a new dataset and benchmark for visual-inertial SLAM with egocentric, multi-modal data.\nWe record hours and kilometers of trajectories through a city center with glasses-like devices equipped with various sensors.\nWe leverage surveying tools to obtain control points as indirect pose annotations that are metric, centimeter-accurate, and available at city scale.\nThis makes it possible to evaluate extreme trajectories that involve walking at night or traveling in a vehicle.\nWe show that state-of-the-art systems developed by academia are not robust to these challenges and we identify components that are responsible for this.\nIn addition, we design tracks with different levels of difficulty to ease in-depth analysis and evaluation of less mature approaches.\nThe dataset and benchmark are available at lamaria.ethz.ch.",
            "introduction": "Estimating the precise location of a camera over time is a fundamental problem in computer vision.\nAlgorithms like Visual-Inertial Odometry (VIO) or Simultaneous Localization and Mapping (VI-SLAM) can estimate a 6 Degrees-of-Freedom (DoF) pose for each image of a sequence, often aided by inertial sensors.\nPositioning plays a crucial role in ensuring the persistence of digital content over time and enabling seamless sharing across devices, which is especially important for applications like AI assistants and augmented reality.\nProgress in mobile computing has fueled the development of wearable devices that are equipped with various sensors, including multiple color or depth cameras, inertial units, and radio receivers.\nThe egocentric, multi-modal data that they capture presents unique challenges that are often overlooked in computer vision research, which typically relies on curated datasets with controlled viewpoints and motions tailored to algorithms or visual content of interest.\n\nDifferently, egocentric data is passive and accidental: it does not constrain the user’s actions but rather endures them.\nAs a result, this data exhibits significantly more diversity in motion patterns, viewpoints, and environments than typically found in computer vision datasets.\nMoreover, egocentric devices aspire to be all-day wearables that capture data over extended durations, in which factors like sensor calibration can change over time.\nFinally, the wearability and consumer adoption of these devices limits the size, weight, and cost of these sensors, and thus their quality.\n\nAcademic research in VIO/SLAM is mainly driven by benchmarks that do not exhibit the characteristics of egocentric data.\nOften originating from the robotics community [68, 10], their data is recorded by expensive, industrial-grade sensors mounted on robots with limited locomotion capabilities.\nThe robot’s motion can also often be adapted to accommodate the limitations of the perception algorithms, as in active perception.\nAdditionally, datasets that offer sufficiently accurate ground-truth (GT) camera poses are often limited to smaller environments than the ones found in egocentric applications [60, 59, 71, 45].\nThe datasets that offer egocentric data [56, 38, 4, 39] do not have sufficiently accurate GT poses to measure improvements in VIO/SLAM algorithms without saturation.\n\nIn this paper, we introduce LaMAria, a new dataset and benchmark111All data collection, storage, and hosting was performed by ETH Zurich. to track progress in egocentric SLAM (Benchmarking Egocentric Visual-Inertial SLAM at City Scale).\nWe record data with Project Aria devices [21], which capture rich multi-sensor streams in a glasses-like form-factor, such that they can be worn over extended durations and distances without impeding the wearer’s motion.\nThe dataset thus exhibits all key characteristics of egocentric data, with a focus on challenges that break existing algorithms: extremely low illumination, fast motion, large distances, transition between indoors and outdoors, time-varying calibration, and dynamic content – the wearer’s own body, other people, or even moving environments such as elevators and vehicles.\nThe trajectories cover the large area of a city center, with some of them spanning kilometers.\nThey benefit from a metric, centimeter-accurate ground-truth based on sparse control points (CPs) widely used in the surveying community.\n\nWe evaluate state-of-the-art VIO/SLAM systems with over \\qty22 and \\qty70\\kilo of egocentric data under different sensor configurations and across different difficulty levels and types of challenges.\nOur results suggest that the top methods developed by academia are still far from solving this benchmark, while exhibiting a significant gap against\nAria’s SLAM API.\nAdditional sequences offer gradually increasing difficulty levels between controlled hand-held motion, as exhibited by most academic datasets, and challenging unrestricted head-mounted motion.\nAll evaluated methods perform well with controlled motion but significantly break down as it becomes more natural and egocentric.\n\nOur results shed light on the limitations of existing systems while our dataset opens new avenues for multi-sensor SLAM.\nThe dataset and benchmark is publicly released to ease tracking progress in this direction.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有SLAM基准未能反映以自我为中心的数据挑战。  \n2. 现有数据集缺乏足够准确的地面真实位姿，限制了算法评估。  \n3. 以自我为中心的数据具有更高的运动模式和环境多样性。  \n\n【提出了什么创新的方法】  \n本论文提出了LaMAria，一个新的数据集和基准，用于评估以自我为中心的视觉惯性SLAM。数据通过佩戴式设备在城市中心记录，涵盖了多种传感器配置和不同难度级别的轨迹。该数据集提供了厘米级的地面真实位姿，并针对低光照、快速运动和动态内容等挑战进行了评估。结果表明，现有的SLAM系统在处理这些挑战时表现不佳，揭示了其局限性，并为多传感器SLAM开辟了新方向。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance",
            "authors": "Yuyang Liu,Chuan Wen,Yihang Hu,Dinesh Jayaraman,Yang Gao",
            "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.26627",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26627",
            "arxiv_html_link": "https://arxiv.org/html/2509.26627v1",
            "abstract": "Designing dense rewards is crucial for reinforcement learning (RL), yet in robotics it often demands extensive manual effort and lacks scalability.\nOne promising solution is to view task progress as a dense reward signal, as it quantifies the degree to which actions advance the system toward task completion over time.\nWe present TimeRewarder, a simple yet effective reward learning method that derives progress estimation signals from passive videos, including robot demonstrations and human videos, by modeling temporal distances between frame pairs.\nWe then demonstrate how TimeRewarder can supply step-wise proxy rewards to guide reinforcement learning.\nIn our comprehensive experiments on ten challenging Meta-World tasks,\nwe show that TimeRewarder dramatically improves RL for sparse-reward tasks,\nachieving nearly perfect success in 9/10 tasks with only 200,000 interactions per task with the environment. This approach outperformed previous methods and even the manually designed environment dense reward on both the final success rate and sample efficiency.\nMoreover, we show that TimeRewarder can exploit real-world human videos,\nhighlighting its potential as a scalable approach path to rich reward signals from diverse video sources.\n\n\nProject page: https://timerewarder.github.io/",
            "introduction": "Reinforcement learning (RL) has long served as a principal paradigm for robotic skill acquisition (Ibarz et al., 2021; Tang et al., 2025).\nYet, many of its most notable successes so far rely highly on carefully designed reward functions that are dense and task-instructive (Cheng et al., 2024; Nai et al., 2025).\nDesigning such high-quality rewards remains labor-intensive, as they often require significant domain expertise, extensive hyperparameter tuning, or privileged access to ground-truth environments, especially for robotic manipulations (Ng et al., 1999a; Levine et al., 2016; Rajeswaran et al., 2017; Roy et al., 2021).\nThese challenges incurred during manual reward design severely constrain the scalability of RL approaches, motivating the development of automated reward learning mechanisms that can alleviate human effort.\n\nDense reward function design for robotics often exploits explicit prior knowledge of the task’s typical progression, which estimates the distance between the current state and task completion, as well as assesses whether the current action contributes to efficient task accomplishment (Todorov, 2004; Levine et al., 2016; Silver et al., 2021).\nExpert demonstrations provide a natural source of this progression knowledge: the temporal ordering of video frames directly reflects task advancement.\nImportantly, such signals can be derived even from passive videos, which are easy to obtain and require neither action annotations nor privileged supervision.\nAs a result, automatic reward learning from passive videos can significantly expand the scalability of RL.\n\nBuilding on this idea, we introduce TimeRewarder (Figure 1), which comprehends how the task proceeds by learning to predict temporal distances between arbitrary frames from action-free expert demonstrations.\nThe temporal distance reflects the task progress between two frames: which frame is closer to task completion and by how much.\nWhen turning to the RL exploration phase, the predicted progress distances between adjacent frames can naturally serve as dense reward signals.\nThe step-wise reward quantifies exactly how much the agent is advancing or regressing at each moment, guiding the agent toward accomplishing the task by implicitly imitating the expert demonstrations.\n\nWe evaluate TimeRewarder in the imitation-from-observation setting, where only expert videos are available and no expert action labels or dense environment rewards are provided.\nOn 10 Meta-World (Yu et al., 2020) manipulation tasks with 100 demonstrations per task, TimeRewarder surpasses all baselines on 9 tasks in both success rate and sample efficiency.\nThis performance gain highlights the high quality of the reward produced by TimeRewarder:\nit effectively assigns credits to partial progress and penalizes unproductive behaviors even on out-of-distribution transitions along the agent trajectories, thus providing strong instructive guidance to the RL process.",
            "llm_summary": "【论文的motivation是什么】  \n1. 设计稠密奖励函数在强化学习中至关重要，但通常需要大量人工努力。  \n2. 自动化奖励学习机制可以减轻人工设计奖励的负担，提高可扩展性。  \n3. 从被动视频中提取任务进展信号可以显著扩展强化学习的可扩展性。  \n\n【提出了什么创新的方法】  \nTimeRewarder是一种从被动视频中学习稠密奖励的方法，通过建模帧对之间的时间距离来估计任务进展信号。该方法在强化学习探索阶段使用预测的进展距离作为稠密奖励信号，量化智能体在每个时刻的进展或退步。通过在10个Meta-World任务上的实验，TimeRewarder在稀疏奖励任务中显著提高了成功率和样本效率，显示出其在从人类演示中学习的潜力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "The Trajectory Bundle Method: Unifying Sequential-Convex Programming and Sampling-Based Trajectory Optimization",
            "authors": "Kevin Tracy,John Z. Zhang,Jon Arrizabalaga,Stefan Schaal,Yuval Tassa,Tom Erez,Zachary Manchester",
            "subjects": "Optimization and Control (math.OC); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.26575",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26575",
            "arxiv_html_link": "https://arxiv.org/html/2509.26575v1",
            "abstract": "We present a unified framework for solving trajectory optimization problems in a derivative-free manner through the use of sequential convex programming. Traditionally, nonconvex optimization problems are solved by forming and solving a sequence of convex optimization problems, where the cost and constraint functions are approximated locally through Taylor series expansions. This presents a challenge for functions where differentiation is expensive or unavailable.\nIn this work, we present a derivative-free approach to form these convex approximations by computing samples of the dynamics, cost, and constraint functions and letting the solver interpolate between them. Our framework includes sample-based trajectory optimization techniques like model-predictive path integral (MPPI) control as a special case and generalizes them to enable features like multiple shooting and general equality and inequality constraints that are traditionally associated with derivative-based sequential convex programming methods. The resulting framework is simple, flexible, and capable of solving a wide variety of practical motion planning and control problems.",
            "introduction": "Linear dynamical systems of the form x+=A​x+B​ux_{+}=Ax+Bu underpin many of the foundational methods in modern optimal control. Ideas such as the Linear-Quadratic Regulator (LQR) and convex trajectory optimization can reason about dynamical systems of this form in a way that is globally optimal [25, 10, 11]. As a result, these techniques are often applied to nonlinear systems where the dynamics are locally approximated as linear around a linearization point [41]. In many cases, this approximation is appropriate given the function is not being evaluated too far from where the approximation was formed. When used appropriately, this method of linearizing nonlinear systems can be extremely effective in practice, even for highly nonlinear systems. The two caveats here are that the nonlinear system must be both smooth and differentiable.\n\nFor many systems, such as robotic arms, quadrotors, and wheeled vehicles, this assumption of smooth differentiability is reasonable. For rigid-body dynamics, there are specialized methods for computing derivatives of the continuous-time dynamics in a fast and efficient way [18]. However, for more complex dynamics models, there are scenarios where these derivatives are unavailable, prohibitively expensive to compute, or unreliable. If the dynamics model is learned from data, the approximation of the dynamics function may be good, while the approximation of the derivatives may be very poor. This scenario is often explored in the context of model-predictive path-integral (MPPI) control, where a learned simulator is only used to produce parallelized simulations [51, 49].\n\nAnother scenario in which derivatives are unavailable or unusable is in the presence of systems that make or break contact. While there has been a lot of recent interest in making contact simulation differentiable [19, 36, 38, 47, 42, 24], there remains a strong need for optimal control methods that do not rely on these derivatives at all.\n\nA recent trend in robotic simulation is the introduction of simulators that can be run on accelerators for massively parallel simulation. Popular simulators like Isaac Sim [32, 35], Brax [19], and MuJoCo XLA (MJX) [46], are all capable of running thousands of simulations in parallel. This paper leverages the innovations in parallel simulation to motivate a new derivative-free optimal control paradigm where simulation rollouts are used to fully describe the dynamics and cost landscapes present in the problem. We introduce the trajectory bundle method for solving nonconvex trajectory optimization problems, which uses interpolated trajectories instead of derivative-based linearizations to approximate the cost, dynamics, and constraint functions in the problem. The result is a simple and robust trajectory optimization framework that can fully utilize parallelized simulation without requiring any derivatives.\n\nOur specific contributions in this paper are the following:\n\nA unified framework, which we refer to as the trajectory bundle method, for solving general trajectory optimization problems using derivative-free sequential-convex programming.\n\nA method for approximating general nonlinear or non-convex cost and constraint functions through sampling and linear interpolation.\n\nA set of numerical experiments demonstrating the effectiveness of the trajectory bundle method and its equivalence to its SCP and MPPI counterparts.\n\nThe remainder of the paper is organized as follows: we first review related literature on derivative-free optimization, sequential-convex programming, and MPPI in Section II. Next, we introduce relevant background on affine function approximation and its application to constrained optimization in Section III. In Section IV, we describe the trajectory bundle method in a general multiple-shooting framework and a single-shooting special case that is equivalent to MPPI. Finally, we present an array of numerical experiments in Section V and point avenues of future research in VI.\n\n1. A unified framework, which we refer to as the trajectory bundle method, for solving general trajectory optimization problems using derivative-free sequential-convex programming.\n\n2. A method for approximating general nonlinear or non-convex cost and constraint functions through sampling and linear interpolation.\n\n3. A set of numerical experiments demonstrating the effectiveness of the trajectory bundle method and its equivalence to its SCP and MPPI counterparts.",
            "llm_summary": "【论文的motivation是什么】  \n1. 许多复杂动态模型的导数不可用或计算成本高。  \n2. 现有的最优控制方法依赖于导数，限制了其在某些场景下的应用。  \n3. 需要一种新的无导数的最优控制范式，以充分利用并行仿真。  \n\n【提出了什么创新的方法】  \n提出了一种称为“轨迹束方法”的统一框架，用于通过无导数的顺序凸编程解决一般的轨迹优化问题。该方法通过采样和线性插值来近似一般的非线性或非凸成本和约束函数，而不是依赖于导数线性化。通过数值实验展示了轨迹束方法的有效性，并证明其与顺序凸编程（SCP）和模型预测路径积分（MPPI）方法的等价性。该框架简单、灵活，能够解决多种实际的运动规划和控制问题。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Towards Human Engagement with Realistic AI Combat Pilots",
            "authors": "Ardian Selmonaj,Giacomo Del Rio,Adrian Schneider,Alessandro Antonucci",
            "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Robotics (cs.RO)",
            "comment": "13th International Conference on Human-Agent Interaction (HAI) 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.26002",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.26002",
            "arxiv_html_link": "https://arxiv.org/html/2509.26002v1",
            "abstract": "We present a system that enables real-time interaction between human users and agents trained to control fighter jets in simulated 3D air combat scenarios. The agents are trained in a dedicated environment using Multi-Agent Reinforcement Learning. A communication link is developed to allow seamless deployment of trained agents into VR-Forces, a widely used defense simulation tool for realistic tactical scenarios. This integration allows mixed simulations where human-controlled entities engage with intelligent agents exhibiting distinct combat behaviors. Our interaction model creates new opportunities for human-agent teaming, immersive training, and the exploration of innovative tactics in defense contexts.",
            "introduction": "Extending the immense potential of Artificial Intelligence (AI) from strategic tasks to safety-critical domains such as air combat requires realistic simulations and interactive setups that enable humans to influence and evaluate AI agents during training and execution. While several combat environments exist (Salhi et al., 2024; Gorton et al., 2024), only few offer realistic flight dynamics, and even fewer enable direct interaction between AI models and human users (Dantas et al., 2023, 2024; Scukins et al., 2024). Lockheed Martin’s pioneering work at DARPA’s AlphaDogfight Trials first emphasized the significance of human-AI interactions in 1-vs-1 air combat scenarios (Pope et al., 2021). Incorporating humans in the loop is essential to ensure AI systems remain aligned with human judgment and safety constraints, while also fostering trust and leveraging complementary strengths in high-risk domains. However, human-AI interaction in multi-agent defense setups remains relatively unexplored.\n\nThis work contributes a modular framework to bridge this gap. Our system uses Multi-Agent Reinforcement Learning (MARL) to train agents within a custom-built 3D environment ensuring accurate flight dynamics simulation. Using the IEEE standard Distributed Interactive Simulation (DIS) (IEEE, 1993), we developed a communication interface allowing seamless deployment of MARL agents into VR-Forces111mak.com/vr-forces. (VR-F), a tactical simulator used by defense organizations (Figs. 1(a) and 1(b)). This integration enables real-time interactions between human-operated entities and AI-controlled aircraft, fostering competitive agent behavior and enhanced training realism for military personnel. To the best of our knowledge, this is the first approach to integrate MARL agents into VR-F for human-agent teaming. This opens new pathways for tactical innovation and exploration of imaginative strategies in operational tasks.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要在空战场景中实现人机交互，以提高AI系统的安全性和可信度。  \n2. 现有的模拟环境缺乏真实的飞行动态和人机直接互动的能力。  \n3. 多智能体防御设置中的人机交互尚未得到充分探索。  \n\n【提出了什么创新的方法】  \n本研究提出了一个模块化框架，利用多智能体强化学习（MARL）在定制的3D环境中训练代理，确保准确的飞行动态模拟。通过开发与IEEE标准分布式互动模拟（DIS）的通信接口，实现了MARL代理与VR-Forces的无缝集成。这种集成使得人类操作实体与AI控制的飞机之间能够进行实时互动，增强了军事人员的训练真实感。该方法是首次将MARL代理整合到VR-F中，实现人机团队合作，为战术创新和操作任务中的策略探索开辟了新路径。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Boundary-to-Region Supervision for Offline Safe Reinforcement Learning",
            "authors": "Huikang Su,Dengyun Peng,Zifeng Zhuang,YuHan Liu,Qiguang Chen,Donglin Wang,Qinghe Liu",
            "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "NeurIPS 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.25727",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25727",
            "arxiv_html_link": "https://arxiv.org/html/2509.25727v1",
            "abstract": "Offline safe reinforcement learning aims to learn policies that satisfy predefined safety constraints from static datasets. Existing sequence-model-based methods condition action generation on symmetric input tokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry: return-to-go (RTG) serves as a flexible performance target, while cost-to-go (CTG) should represent a rigid safety boundary. This symmetric conditioning leads to unreliable constraint satisfaction, especially when encountering out-of-distribution cost trajectories. To address this, we propose Boundary-to-Region (B2R), a framework that enables asymmetric conditioning through cost signal realignment . B2R redefines CTG as a boundary constraint under a fixed safety budget, unifying the cost distribution of all feasible trajectories while preserving reward structures. Combined with rotary positional embeddings , it enhances exploration within the safe region. Experimental results show that B2R satisfies safety constraints in 35 out of 38 safety-critical tasks while achieving superior reward performance over baseline methods. This work highlights the limitations of symmetric token conditioning and establishes a new theoretical and practical approach for applying sequence models to safe RL. Our code is available at https://github.com/HuikangSu/B2R.",
            "introduction": "Offline reinforcement learning (RL) enables policy learning from static datasets without risky online interactions [28, 7], a critical capability for safety-sensitive applications such as autonomous driving [21, 23, 37], robotics [4, 5], and industrial control systems [36]. While conventional offline RL focuses on maximizing rewards under distributional shift [17, 38, 24], real-world deployments often demand adherence to safety constraints [9, 3]. This necessitates offline safe RL, which seeks policies that maximize cumulative rewards while ensuring expected costs remain below predefined thresholds.\n\nRecent advances in Reinforcement Learning via Supervised Learning (RvS), exemplified by the Decision Transformer (DT) [2], have shown promise by autoregressively generating actions conditioned on historical states, actions, and return-to-go (RTG) signals. However, extending DT to safe RL reveals a fundamental limitation: existing methods naively apply symmetric conditioning mechanisms to both RTG and cost-to-go (CTG), overlooking their inherent asymmetry. Specifically, RTG serves as a flexible performance target to pursue, while CTG represents a rigid safety budget to enforce—a distinction that existing approaches fail to capture.\n\nThis oversight leads to unreliable constraint satisfaction [1], particularly when policies encounter cost trajectories outside the training distribution. Methods like Constrained Decision Transformer (CDT) [26] treat RTG and CTG as equivalent input tokens, conflating the orthogonal objectives of reward maximization and safety assurance. To address this, we propose Boundary-to-Region (B2R), a framework that introduces asymmetric conditioning through CTG realignment. Our core innovation lies in unifying feasible trajectories under a fixed safety budget by redistributing cost signals while preserving reward structure. This redefines CTG as a boundary constraint rather than a variable target, decoupling safety guarantees from reward optimization. Combined with trajectory filtering and rotary positional embeddings [31], B2R enables comprehensive exploration of the safe action space while maintaining strict cost adherence. Figure 1 illustrates how B2R broadens supervision beyond narrow constraint-aligned trajectories, enabling stable learning over the entire safe region.\n\nExperiments on 38 safety-critical tasks demonstrate B2R’s effectiveness: it satisfies safety constraints in 35 environments while achieving competitive rewards [25]. Our findings underscore the necessity of abandoning symmetric token conditioning when applying sequence models to safe RL. The contributions of this work are threefold:\n\n1. Problem Identification: We identify and formalize a fundamental symmetry fallacy in existing RvS methods [2, 26], where the flexible nature of rewards and the rigid nature of costs are improperly treated as symmetric signals.\n\n2. Methodology and Validation: We propose region-level supervision, a new paradigm for offline safe RL. We then introduce B2R, a logically coherent framework designed to instantiate this paradigm, demonstrating how components like trajectory filtering and CTG realignment form a mutually reinforcing system, not a collection of ad-hoc tweaks.\n\n3. Theoretical Foundation:We provide initial theoretical analysis of B2R’s safety compliance [1] under simplified assumptions and empirically validate its superiority in balancing reward and safety across 38 diverse tasks.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的离线安全强化学习方法未能有效处理回报和成本之间的非对称性。  \n2. 对称条件导致在面对分布外成本轨迹时，约束满足不可靠。  \n3. 需要一种新方法来统一可行轨迹并保持奖励结构，以确保安全性。  \n\n【提出了什么创新的方法】  \n提出了Boundary-to-Region (B2R)框架，通过成本信号重新对齐实现非对称条件。B2R将成本视为固定安全预算下的边界约束，统一所有可行轨迹的成本分布，同时保留奖励结构。结合旋转位置嵌入，B2R增强了在安全区域内的探索。实验结果表明，B2R在38个安全关键任务中有35个满足安全约束，同时在奖励表现上优于基线方法。这项工作强调了对称条件的局限性，并为安全强化学习应用序列模型建立了新的理论和实践方法。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "MoReFlow: Motion Retargeting Learning through Unsupervised Flow Matching",
            "authors": "Wontaek Kim,Tianyu Li,Sehoon Ha",
            "subjects": "Graphics (cs.GR); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25600",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25600",
            "arxiv_html_link": "https://arxiv.org/html/2509.25600v1",
            "abstract": "Motion retargeting holds a premise of offering a larger set of motion data for characters and robots with different morphologies. Many prior works have approached this problem via either handcrafted constraints or paired motion datasets, limiting their applicability to humanoid characters or narrow behaviors such as locomotion. Moreover, they often assume a fixed notion of retargeting, overlooking domain-specific objectives like style preservation in animation or task-space alignment in robotics. In this work, we propose MoReFlow, Motion Retargeting via Flow Matching, an unsupervised framework that learns correspondences between characters’ motion embedding spaces. Our method consists of two stages. First, we train tokenized motion embeddings for each character using a VQ-VAE, yielding compact latent representations. Then, we employ flow matching with conditional coupling to align the latent spaces across characters, which simultaneously learns conditioned and unconditioned matching to achieve robust but flexible retargeting. Once trained, MoReFlow enables flexible and reversible retargeting without requiring paired data. Experiments demonstrate that MoReFlow produces high-quality motions across diverse characters and tasks, offering improved controllability, generalization, and motion realism compared to the baselines.",
            "introduction": "Motion retargeting has been the fundamental problem of adapting motions performed by one actor to another with different morphology, such as transferring a human motion sequence to a virtual avatar or a humanoid robot. It holds the premise of an enriched dataset for virtual characters or robots by reducing the cost of content creation in animation and gaming, enabling robots to learn practical skills from human demonstrations, and supporting cross-actor motion analysis in biomechanics. Despite its diverse applications, the central challenge remains the same: establishing a reliable correspondence between the motion spaces of different characters while respecting different kinematic and dynamic capabilities.\n\nMotion retargeting has been studied with diverse approaches, such as optimization methods with handcrafted constraints (Zakka, 2025; Choi & Ko, 2000), reinforcement learning (Reda et al., 2023), and learning-based mappings between characters (Villegas et al., 2021). However, most methods remain restricted to the same human morphologies (Aberman et al., 2020; Aigerman et al., 2022). While some recent works tackle motion retargeting between heterogeneous morphologies, such as human-to-quadruped (Li et al., 2024b; a), they are often limited to locomotion tasks. Moreover, existing approaches typically assume a fixed notion of retargeting. In practice, the objective of the retargeting may vary: motion retargeting in animation often means joint-space alignment to preserve style, while robotics applications may require task-space alignment to achieve functional goals. These limitations call for a more general and adaptable retargeting framework.\n\nIn this work, we propose MoReFlow, Motion Retargeting via Flow Matching, an unsupervised framework for learning motion retargeting across heterogeneous characters. Specifically, MoReFlow formulates the retargeting problem as constructing a correspondence between the motion spaces of two characters using Flow Matching. The framework consists of two stages: pretraining a motion tokenizer and learning motion correspondence via codebook flow. In the first stage, we train tokenized motion embeddings for each character using a VQ-VAE (Van Den Oord et al., 2017), which yields a compact motion representation together with a motion encoder and decoder. In the second stage, MoReFlow employs guided flow matching to establish correspondences between the latent spaces of different characters. At inference, the source character’s motion is encoded into a latent vector, translated into the target character’s latent vector through the learned flow, and then reconstructed by the target character’s decoder. Our unsupervised framework employs conditional coupling to handle large unpaired datasets, where it guides the flow to minimize geodesic distances in the feature space between samples.\n\nWe conduct extensive experiments covering humanoid and non-humanoid robots to validate our framework. Our main results show that MoReFlow can retarget diverse dynamic human motions—including locomotion, sports, and gestural actions—to new characters such as the smaller Booster T1 humanoid and the Spot quadruped with a manipulator (Figure 1). The retargeted motions not only preserve the style and semantics of the source but also adapt naturally to the target morphology. Furthermore, our framework enables condition-dependent control, producing multiple valid outcomes from the same source motion depending on whether the alignment is defined in local or world coordinates in real time. Compared to the baseline methods, MoReFlow achieves better motion quality while offering a higher degree of controllability in cross-morphology retargeting. Finally, our ablation studies analyze the effects of various factors, such as different embedding designs and motion dataset volume.\n\nIn summary, our contributions are as follows:\n\nWe introduce MoReFlow, a novel unsupervised framework for cross-morphology motion retargeting that leverages flow matching in tokenized motion spaces. It enables retargeting without paired data and supports reversible and modular mappings between characters.\n\nWe show that the proposed framework can generate different motion retargeting results under various conditions, such as local style alignment or world-frame task alignment, which offer users interactive, fine-grained control.\n\nWe validate the effectiveness of the proposed approach through extensive experiments. The results demonstrate that MoReFlow achieves controllable, high-quality motion retargeting that preserves semantic intent and outperforms baseline methods in both motion fidelity and controllability.\n\n1. We introduce MoReFlow, a novel unsupervised framework for cross-morphology motion retargeting that leverages flow matching in tokenized motion spaces. It enables retargeting without paired data and supports reversible and modular mappings between characters.\n\n2. We show that the proposed framework can generate different motion retargeting results under various conditions, such as local style alignment or world-frame task alignment, which offer users interactive, fine-grained control.\n\n3. We validate the effectiveness of the proposed approach through extensive experiments. The results demonstrate that MoReFlow achieves controllable, high-quality motion retargeting that preserves semantic intent and outperforms baseline methods in both motion fidelity and controllability.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有运动重定向方法通常依赖于手工约束或配对数据，限制了其适用性。  \n2. 需要一种通用且灵活的运动重定向框架，以适应不同角色的运动空间和任务需求。  \n\n【提出了什么创新的方法】  \n提出了MoReFlow，一个无监督的运动重定向框架，通过流匹配学习不同角色的运动嵌入空间之间的对应关系。该方法包括两个阶段：首先使用VQ-VAE训练每个角色的标记运动嵌入，生成紧凑的潜在表示；然后通过条件耦合的流匹配对不同角色的潜在空间进行对齐。实验结果表明，MoReFlow能够在多样化的角色和任务中生成高质量的运动，提供了更好的可控性、泛化能力和运动真实感。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Integrator Forwading Design for Unicycles with Constant and Actuated Velocity in Polar Coordinates",
            "authors": "Miroslav Krstic,Velimir Todorovski,Kwang Hak Kim,Alessandro Astolfi",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO); Optimization and Control (math.OC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25579",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25579",
            "arxiv_html_link": "https://arxiv.org/html/2509.25579v1",
            "abstract": "In a companion paper, we present a modular framework for unicycle stabilization in polar coordinates that provides smooth steering laws through backstepping. Surprisingly, the same problem also allows application of integrator forwarding. In this work, we leverage this feature and construct new smooth steering laws together with control Lyapunov functions (CLFs), expanding the set of CLFs available for inverse optimal control design. In the case of constant forward velocity (Dubins car), backstepping produces finite-time (deadbeat) parking, and we show that integrator forwarding yields the very same class of solutions. This reveals a fundamental connection between backstepping and forwarding in addressing both the unicycle and, the Dubins car parking problems.",
            "introduction": "Transforming the unicycle model from Cartesian to polar coordinates has proven highly effective for control design. In polar coordinates, the state naturally encodes both distance and relative heading to the target, and the singularity at the origin allows one to bypass the Brockett–Ryan-Coron–Rosier conditions [6, 7, 18], which prohibit the design of continuous, as well as discontinuous time-invariant stabilizers for the unicycle kinematics.\n\nThis insight was first exploited by Badreddin and Mansour [5], who designed a linear state-feedback controller in polar coordinates. Astolfi [2] later characterized its region of attraction, showing it covers an entire half-plane (x>0x>0 or x<0x<0 for all headings, except a measure-zero set).\nBuilding on this, Aicardi et al. [1] have developed a passivity-based feedback law with bidirectional forward velocity, achieving global asymptotic stabilization. Their analysis relies on a non-strict Lyapunov function and Barbalat’s lemma, which excludes constructive 𝒦​ℒ\\mathcal{KL} estimates. Han and Wang [9] have refined this approach with a Lyapunov function defined only on an arbitrarily large, but compact set containing the origin. Restrepo et al. [17] have gone further in exploiting the polar coordinates, attaining global exponential stability via a backstepping design with a strict CLF; however, this comes at a cost: it sacrifices modularity, complicates extensions to barrier CLFs, and restricts the unicycle to unidirectional motion, which can produce less efficient parking trajectories.\nOther examples of singular transformations are presented in Astolfi [3, 4]. These allow continuous exponential stabilization in the transformed coordinates, though the transformation excludes certain initial conditions—such as states on the xx-axis or aligned with the target.\nNote that, these transformations ensure stability in the transformed coordinates, while in the Cartesian representation only attractivity is obtained\n\nWith constant forward velocity, the unicycle reduces to the Dubins vehicle [8], a standard model in guidance and pursuit problems.\nMissile guidance is largely dominated by proportional navigation (PN), which ensures zero line-of-sight error but ignores terminal orientation [25, 23]. Linear quadratic (LQ) controllers can enforce orientation and provide optimality guarantees [15, 19, 20, 22], but their reliance on linear or linearized dynamics is often very restrictive.\n\nIn [24] and [13] we apply backstepping in polar coordinates to design steering laws for parking, achieving global asymptotic and half-global finite-time stabilization, respectively. In the present work, we develop control laws for both the unicycle and the Dubins vehicle model via integrator forwarding [14, 21, 12], highlighting their connections with our previous backstepping designs. This approach is enabled either by the modular framework introduced in [24] or by fixing the forward velocity, which exposes a system structure naturally suited to integrator forwarding.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要有效的控制设计方法以解决单轮车和Dubins车的停车问题。  \n2. 现有方法在模块化和效率上存在局限性，尤其是在处理多方向运动时。  \n\n【提出了什么创新的方法】  \n本文提出了一种新的平滑转向控制方法，通过引入积分转发和控制Lyapunov函数（CLFs），扩展了逆最优控制设计中可用的CLFs集合。该方法在极坐标下对单轮车和Dubins车模型进行控制设计，展示了积分转发与反步法之间的基本联系。通过这种方法，作者实现了全局渐近稳定性和有限时间停车，提供了一种更高效的控制策略，克服了现有方法的局限性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Modular Design of Strict Control Lyapunov Functions for Global Stabilization of the Unicycle in Polar Coordinates",
            "authors": "Velimir Todorovski,Kwang Hak Kim,Miroslav Krstic",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO); Optimization and Control (math.OC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25575",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25575",
            "arxiv_html_link": "https://arxiv.org/html/2509.25575v1",
            "abstract": "Since the mid-1990s, it has been known that, unlike in Cartesian form where Brockett’s condition rules out static feedback stabilization, the unicycle is globally asymptotically stabilizable by smooth feedback in polar coordinates. In this note, we introduce a modular framework for designing smooth feedback laws that achieve global asymptotic stabilization in polar coordinates. These laws are bidirectional, enabling efficient parking maneuvers, and are paired with families of strict control Lyapunov functions (CLFs) constructed in a modular fashion. The resulting CLFs guarantee global asymptotic stability with explicit convergence rates and include barrier variants that yield “almost global” stabilization, excluding only zero-measure subsets of the rotation manifolds. The strictness of the CLFs is further leveraged in our companion paper, where we develop inverse-optimal redesigns with meaningful cost functions and infinite gain margins.",
            "introduction": "The unicycle, though globally controllable, cannot be asymptotically stabilized by any continuous or discontinuous time-invariant feedback, as shown by Brockett [5] and extended by Ryan, as well as Coron and Rosier [16, 7]. The Brockett–Ryan-Coron–Rosier conditions highlight fundamental topological obstructions to stabilizing nonholonomic systems.\n\nThis challenge has motivated diverse strategies to bypass the Brockett–Ryan-Coron–Rosier obstruction. One line of work introduces time-varying feedback, achieving global asymptotic stabilization of the unicycle [17, 6, 12, 8], but such controllers are sensitive to delays, prone to synchronization issues, and often produce transient oscillations.\nAnother line leverages hysteresis-based hybrid feedback with robustness refinements [10, 13, 14], though these often cause zig-zagging motions, demand high actuation effort, and are difficult to implement in practice.\n\nAn alternative approach relies on coordinate transformations that introduce singularities in the transformed coordinates.\nA notable example is Astolfi’s σ\\sigma-processes [3, 4], which achieve continuous exponential stabilization in the transformed coordinates, though the transformation excludes certain initial conditions—such as states on the xx-axis or aligned with the target.\nPerhaps, a more physically meaningful transformation is to polar coordinates [2], which naturally encodes distance and relative heading to the target. Using this representation, Aicardi et al.[1] design a passivity-based feedback law with bidirectional forward velocity that achieves global asymptotic stabilization of the unicycle. Stability is established using a non-strict Lyapunov function and Barbalat’s lemma, precluding constructive 𝒦​ℒ\\mathcal{KL}- estimates. Building on this work, Han and Wang[9] introduce a strict Lyapunov function valid on large compact sets, while Restrepo et al. [15] achieve global exponential stability through a backstepping design with a strict CLF. However, their approach sacrifices modularity—complicating extensions to barrier CLFs—and restricts the unicycle to unidirectional motion, potentially leading to less efficient parking trajectories.\n\nIn this work, we develop multitudes of feedback laws in polar coordinates for the parking problem of unicycle vehicles with bidirectional forward velocity. The design follows a modular structure: the forward velocity is selected to decouple the distance state from the angular dynamics, which are subsequently regulated by steering through passivity and backstepping techniques. The control laws achieve global asymptotic stabilization, supported by strict CLFs built in a composite fashion that expand the variety of attainable Lyapunov function designs and provide constructive 𝒦​ℒ\\mathcal{KL}-estimates of convergence rates. Within this modular framework, barrier CLFs are developed to confine states to meaningful intervals, preventing angular wind-up and enforcing safety constraints.\nWe note that due to space restrictions, some proofs have been omitted and will be included in the extended journal version of this paper.",
            "llm_summary": "【论文的motivation是什么】  \n1. 解决Brockett条件下的非完整控制系统的稳定性问题。  \n2. 提高无轮车在极坐标下的全局渐近稳定性设计的有效性。  \n3. 设计模块化的反馈控制律以实现高效的停车操作。  \n\n【提出了什么创新的方法】  \n本研究提出了一种模块化框架，用于在极坐标下设计平滑反馈控制律，以实现无轮车的全局渐近稳定。通过选择前进速度来解耦距离状态与角动态，随后通过被动性和反步法进行调节，确保了控制律的有效性。研究中构建的严格控制Lyapunov函数（CLFs）提供了明确的收敛速率估计，并开发了障碍CLFs以维护安全约束。最终，这些方法实现了无轮车在停车问题上的全局渐近稳定性，扩展了可实现的Lyapunov函数设计的多样性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models",
            "authors": "Pranav Saxena,Avigyan Bhattacharya,Ji Zhang,Wenshan Wang",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25528",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25528",
            "arxiv_html_link": "https://arxiv.org/html/2509.25528v1",
            "abstract": "Referential grounding in outdoor driving scenes is challenging due to large scene variability, many visually similar objects, and dynamic elements that complicate resolving natural-language references (e.g., “the black car on the right”). We propose LLM-RG, a hybrid pipeline that combines off-the-shelf vision–language models for fine-grained attribute extraction with large language models for symbolic reasoning. LLM-RG processes an image and a free-form referring expression by using an LLM to extract relevant object types and attributes, detecting candidate regions, generating rich visual descriptors with a VLM, and then combining these descriptors with spatial metadata into natural-language prompts that are input to an LLM for chain-of-thought reasoning to identify the referent’s bounding box. Evaluated on the Talk2Car benchmark, LLM-RG yields substantial gains over both LLM and VLM-based baselines. Additionally, our ablations show that adding 3D spatial cues further improves grounding. Our results demonstrate the complementary strengths of VLMs and LLMs, applied in a zero-shot manner, for robust outdoor referential grounding.",
            "introduction": "Enabling autonomous systems to ground referring expressions to real-world entities in complex settings is a crucial step toward safe and natural interactions with humans. In contrast to indoor settings, which have been the focus of most prior works, outdoor scenes pose distinct challenges due to larger scales, greater object diversity, and more complex, dynamic environments like roads and intersections. Referential expressions in this setting frequently rely on high-level attributes (such as color, orientation, or type) and relative spatial relations (such as “on the right” or “behind the van”), which are harder to resolve than the structured references typically found indoors.\n\nIn recent years, significant progress has been made in grounding referential language within indoor environments. Large-scale 3D datasets such as Matterport3D [1], ScanNet [2], and HM3D [3] have enabled tasks including visual grounding, embodied instruction following, and object-goal navigation. Methods developed on these datasets often combine object detection with geometric or spatial reasoning modules [4, 5, 6], or leverage pretrained language models to link natural language queries to structured scene graphs [7, 8]. More recently, large multimodal models have been explored for zero-shot grounding in 3D indoor spaces [9, 10]. These approaches have shown strong performance in resolving references to small household objects and reasoning about relations such as “next to the chair” or “on top of the table.” However, they remain heavily tuned to the relatively constrained and repetitive structure of indoor scenes, where object categories are limited, contexts are predictable, and the variability of natural language references is narrower.\n\nIn contrast, outdoor referential grounding has received considerably less attention, even though it is essential for applications in autonomous driving, mobile robotics, and delivery systems. Outdoor environments are inherently more complex: they contain a larger and more open-ended vocabulary of objects (e.g., cars, trucks, pedestrians, bicycles, traffic lights), involve greater scene variability (e.g., urban streets, intersections, crosswalks, parking lots), and are subject to dynamic changes such as moving vehicles or occlusions. Furthermore, outdoor language queries tend to be more diverse and ambiguous, often requiring fine-grained disambiguation across multiple similar objects (e.g., “the black car on the right” when several black cars are present) or reasoning over higher-level semantics (e.g., “the car waiting at the stop sign”). Datasets such as Talk2Car [11] address this gap by providing natural language commands linked to visual driving scenes, but methods specifically designed for outdoor referential grounding remain scarce.\n\nTo address these challenges, we propose to leverage recent advances in vision-language models (VLMs) for extracting fine-grained object attributes and large language models (LLMs) for reasoning over natural language queries. We evaluate this approach on the Talk2Car dataset, which provides a realistic and challenging benchmark for outdoor referential grounding in driving scenarios.\n\nOur work, based on SORT3D [10], introduces these three key contributions -\n\nWe present a novel pipeline that combines VLM-based object attribute extraction with LLM-based reasoning for outdoor referential grounding.\n\nWe show that our approach works without any task-specific fine-tuning, making it deployable across unseen datasets and robotic setups.\n\nWe provide an extensive evaluation demonstrating the effectiveness of this hybrid approach and highlight its potential for natural human-vehicle interaction in real-world contexts.",
            "llm_summary": "【论文的motivation是什么】  \n1. Outdoor referential grounding is essential for safe interactions in autonomous systems but is challenging due to scene complexity and variability.  \n2. Existing methods are primarily focused on indoor environments and do not generalize well to outdoor scenarios with diverse objects and dynamic elements.  \n3. There is a lack of effective methods specifically designed for outdoor referential grounding, despite its importance in applications like autonomous driving.  \n\n【提出了什么创新的方法】  \n提出了LLM-RG，一个结合视觉-语言模型（VLM）和大型语言模型（LLM）的混合管道，用于户外场景中的指称基础。该方法首先利用LLM提取相关对象类型和属性，然后检测候选区域，生成丰富的视觉描述符，并将其与空间元数据结合，输入到LLM进行推理以识别目标的边界框。通过在Talk2Car基准上的评估，LLM-RG在性能上显著优于基于LLM和VLM的基线。此外，加入3D空间线索进一步提高了基础效果。结果表明，VLM和LLM的互补优势在户外指称基础任务中表现出色。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity",
            "authors": "Tu-Hoa Pham,Philip Bailey,Daniel Posada,Georgios Georgakis,Jorge Enriquez,Surya Suresh,Marco Dolci,Philip Twu",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "To appear in IEEE Robotics and Automation Letters",
            "pdf_link": "https://arxiv.org/pdf/2509.25520",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25520",
            "arxiv_html_link": "https://arxiv.org/html/2509.25520v1",
            "abstract": "We consider the problem of vision-based 6-DoF object pose estimation in the context of the notional Mars Sample Return campaign, in which a robotic arm would need to localize multiple objects of interest for low-clearance pickup and insertion, under severely constrained hardware.\nWe propose a novel localization algorithm leveraging a custom renderer together with a new template matching metric tailored to the edge domain to achieve robust pose estimation using only low-fidelity, textureless 3D models as inputs.\nExtensive evaluations on synthetic datasets as well as from physical testbeds on Earth and in situ Mars imagery shows that our method consistently beats the state of the art in compute and memory-constrained localization, both in terms of robustness and accuracy,\nin turn enabling new possibilities for cheap and reliable localization on general-purpose hardware.",
            "introduction": "NASA’s Perseverance rover, which landed on Mars in 2021, is collecting rock and atmosphere sample tubes to be returned to Earth as part of a planned NASA-ESA follow-up program, Mars Sample Return [1].\nIn a potential concept of operations (see Fig. 1), a Sample Return Lander would rendezvous with Perseverance on Mars, use a robotic arm to retrieve sample tubes from the rover’s bit carousel (BC) and load them into an orbiting sample (OS) canister to be launched into orbit then returned to Earth.\nThe lander arm would initially extend to a ready pose 3 cm3\\text{\\,}\\mathrm{cm}\nin front of the rover BC.\nIt would then refine its pose knowledge via visual localization using arm-mounted cameras to retrieve a tube from the BC.\nThe arm, now holding a tube, would then extend to another ready pose 3 cm3\\text{\\,}\\mathrm{cm} in front of the OS and\nagain refine its pose knowledge using vision to enable insertion into the OS.\nWhile lander images could be downlinked for processing on Earth,\nsuch localization would take at least 3 Mars days per tube (7474 Earth hours) due to limited communication windows,\nduring which uncertainties could accumulate from thermal expansion and other movement.\nIn order to fit within a 30 min30\\text{\\,}\\mathrm{min} time budget, localization must instead be fully autonomous.\n\nThis is a difficult problem.\nFirst, we must deal with extremely tight requirements due to the cost and criticality of the mission.\nFor example, the relative pose between the end effector and the BC before localization can only be assumed known within\n75 mm75\\text{\\,}\\mathrm{mm} and 5 °5\\text{\\,}\\mathrm{\\SIUnitSymbolDegree} due to kinematics uncertainties, thermal deflections,\nsinkage in the Martian sand as the arm extends, etc.\nStill, from these large input uncertainties,\nvisual localization accuracy is required to be within 0.4 mm0.4\\text{\\,}\\mathrm{mm} and 0.25 °0.25\\text{\\,}\\mathrm{\\SIUnitSymbolDegree}\nin order to guarantee safety of the rover and lander during open-loop tube transfer.\nSecond, such accuracy must be achieved from only monocular inputs,\nallowing for failure of either lander camera.\nThird, localization must run on a flight-qualified, radiation-hardened, single-core 200 MHz200\\text{\\,}\\mathrm{MHz} processor with\nonly 10 MB10\\text{\\,}\\mathrm{MB} of RAM.\nWhile existing methods can tackle some of these challenges, they typically fall short on accuracy and robustness to operational conditions [2, 3] or amount of data and compute required [4, 5].\n\nIn this paper, we propose a novel method that is capable of performing object localization within extremely tight accuracy margins while being robust to noise and lighting, using a single image as input, under severely limited computation and timing budgets.\nWe do so by building upon the state of the art in multiple areas of visual localization (see Section II) and extending it through the following contributions:\n\nA novel render-and-compare algorithm that matches synthetic images from iterative viewpoint hypotheses to a monocular image to localize (see Section III-A);\n\nA custom, extendable renderer focusing on edge features that are more likely to hold between synthetic rendering and reality (see Section III-B);\n\nA novel template matching metric that is explicitly tailored to the edge domain to finish bridging the sim-to-real gap (see Section III-C);\n\nAn extensive dataset comprising 4000 simulated images under the range of experimental conditions we expect to face on Mars, as well as real images collected on Mars and development testbeds on Earth, that we make publicly available111https://doi.org/10.48577/jpl.9IZDE2 (see Section IV).\n\n1. A novel render-and-compare algorithm that matches synthetic images from iterative viewpoint hypotheses to a monocular image to localize (see Section III-A);\n\n2. A custom, extendable renderer focusing on edge features that are more likely to hold between synthetic rendering and reality (see Section III-B);\n\n3. A novel template matching metric that is explicitly tailored to the edge domain to finish bridging the sim-to-real gap (see Section III-C);\n\n4. An extensive dataset comprising 4000 simulated images under the range of experimental conditions we expect to face on Mars, as well as real images collected on Mars and development testbeds on Earth, that we make publicly available111https://doi.org/10.48577/jpl.9IZDE2 (see Section IV).",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要在极端环境下进行视觉基础的6-DoF物体姿态估计。  \n2. 现有方法在准确性和鲁棒性方面未能满足任务要求。  \n3. 受限于计算资源和时间预算，必须实现自主定位。  \n\n【提出了什么创新的方法】  \n提出了一种新颖的定位算法，结合自定义渲染器和针对边缘域的新模板匹配度量，以实现基于低保真度、无纹理3D模型的鲁棒姿态估计。该方法通过迭代视点假设匹配合成图像与单目图像，专注于边缘特征的渲染，显著提高了在计算和内存受限环境下的定位准确性和鲁棒性。通过在合成数据集和实际测试环境中的广泛评估，证明了该方法在定位任务中优于现有技术，开启了在通用硬件上进行低成本、可靠定位的新可能性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "World Model for AI Autonomous Navigation in Mechanical Thrombectomy",
            "authors": "Harry Robertshaw,Han-Ru Wu,Alejandro Granados,Thomas C Booth",
            "subjects": "Machine Learning (cs.LG); Robotics (cs.RO); Image and Video Processing (eess.IV)",
            "comment": "Published in Medical Image Computing and Computer Assisted Intervention - MICCAI 2025, Lecture Notes in Computer Science, vol 15968",
            "pdf_link": "https://arxiv.org/pdf/2509.25518",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25518",
            "arxiv_html_link": "https://arxiv.org/html/2509.25518v1",
            "abstract": "Autonomous navigation for mechanical thrombectomy (MT) remains a critical challenge due to the complexity of vascular anatomy and the need for precise, real-time decision-making. Reinforcement learning (RL)-based approaches have demonstrated potential in automating endovascular navigation, but current methods often struggle with generalization across multiple patient vasculatures and long-horizon tasks. We propose a world model for autonomous endovascular navigation using TD-MPC2, a model-based RL algorithm. We trained a single RL agent across multiple endovascular navigation tasks in ten real patient vasculatures, comparing performance against the state-of-the-art Soft Actor-Critic (SAC) method. Results indicate that TD-MPC2 significantly outperforms SAC in multi-task learning, achieving a 65% mean success rate compared to SAC’s 37% (p<0.001p<0.001), with notable improvements in path ratio. TD-MPC2 exhibited increased procedure times, suggesting a trade-off between success rate and execution speed. These findings highlight the potential of world models for improving autonomous endovascular navigation and lay the foundation for future research in generalizable AI-driven robotic interventions.",
            "introduction": "Stroke is a leading cause of death and disability worldwide, imposing significant strain on healthcare systems and families [1]. Mechanical thrombectomy (MT) has improved ischemic stroke outcomes, reducing mortality and disability compared to medical therapy alone [2, 3]. However, its effectiveness declines with delayed treatment, emphasizing the need for rapid intervention [3, 4]. Despite its efficacy, only 3.1% of stroke admissions in, for example, the UK receive MT, far below the estimated 15% eligibility rate, due to limited access to MT-capable centers and long transfer times [5, 6, 7]. Additionally, operators face radiation exposure, increasing cancer and cataract risks, while protective gear contributes to orthopedic strain [8, 9, 10]. Robotic surgical systems offer a potential solution by improving accessibility and reducing operator dependency [11]. Tele-operated robotic MT could (1) allow specialists in centralized centers to perform procedures remotely, while (2) AI assistance robots may enable less experienced operators (interventional radiologists in peripheral hospitals) to perform MT effectively. Integrating AI into robotic systems could further enhance efficiency and safety, with autonomous surgical robots already demonstrating superior performance over manual techniques in some scenarios [12].\n\nAutonomous endovascular navigation research has primarily focused on the aortic arch [13, 14, 11], with recent work exploring micro-guidewire and micro-catheter navigation in MT’s second phase [15]. These studies use multiple test vasculatures, but examine simple tasks with short episodes, limiting clinical applicability. Additionally, autonomous two-device navigation for phase one of MT has been investigated using a larger episode size but was limited to testing on a single vasculature [16]. These studies use reinforcement learning (RL) algorithms, the majority of which are sensitive to architecture, hyperparameters, and are unable to perform over long time horizons, while often being designed for single-task learning only [17, 18]. For complex or multiple tasks, this limits RL to computationally expensive models or tasks where tuning is prohibitive [19].\n\nTo move towards realizing the benefits of autonomous MT navigation, it is necessary to develop policies capable of performing long navigation tasks across multiple patients vasculatures that can be adapted efficiently to new robots, tasks, and environments [20]. One promising technique may be world models, a learned representation of the environment that simulates its dynamics, enabling a single agent to optimize actions in a virtual setting without relying solely on real-world data [21]. Using this model, large-scale learning from diverse datasets could create AI navigation systems that can understand, predict, and adapt to real-world complexities [22]. Recent work has shown that single configurations of RL algorithms (DreamerV3 and TD-MPC2) with no hyperparameter tuning can outperform specialized methods across diverse benchmark tasks [19, 23]. They can also complete farsighted tasks such as collecting diamonds in Minecraft without human data or curricula and capturing expectations of future events during autonomous driving [19, 22]. Although the translation of these models to real-world applications is limited, they hold the potential for creating agents capable of performing long navigation tasks on a diverse range of patient anatomies. TD-MPC2 has shown significant improvements upon both Soft Actor-Critic (SAC) (a configuration of which is the current state-of-the-art for autonomous endovascular interventions compared to benchmarks [14, 24]), and DreamerV3 when examining multi-task environments with continuous action spaces. It is also able to utilize multiple data types, such as human demonstrator, RL collected, and its own online interaction data (from a single task or across multiple tasks).\n\nThe aim of this study was to propose a framework for a world model for autonomous endovascular interventions, and more specifically, MT. The primary objective was to demonstrate that a singular RL agent could be used to perform multiple endovascular navigation tasks across multiple patient vasculatures. Our contributions are as follows: 1) we implemented a world model capable of performing multiple endovascular navigation tasks across multiple patient vasculatures, for the first time, 2) we proposed a framework for endovascular intervention multi-task learning, 3) we compared our results to the current state-of-the-art RL algorithms for autonomous endovascular interventions, demonstrating performance across the largest dataset examined for autonomous MT.",
            "llm_summary": "【论文的motivation是什么】  \n1. 复杂的血管解剖结构使得机械取栓的自主导航面临重大挑战。  \n2. 现有基于强化学习的方法在多个患者血管和长时间任务中的泛化能力不足。  \n3. 需要开发能够适应新机器人、任务和环境的多任务学习策略。  \n\n【提出了什么创新的方法】  \n提出了一种基于世界模型的自主内血管导航框架，使用TD-MPC2算法训练单一RL代理在多个患者血管中执行多项内血管导航任务。该方法通过与当前最先进的Soft Actor-Critic方法进行比较，显示出显著的性能提升，成功率达到65%，而SAC为37%。此外，TD-MPC2能够处理多种数据类型，包括人类演示数据，展示了在复杂环境中进行长时间导航任务的潜力。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Message passing-based inference in an autoregressive active inference agent",
            "authors": "Wouter M. Kouw,Tim N. Nisslbeck,Wouter L.N. Nuijten",
            "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO); Systems and Control (eess.SY); Machine Learning (stat.ML)",
            "comment": "to be published in the proceedings of the International Workshop on Active Inference 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.25482",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25482",
            "arxiv_html_link": "https://arxiv.org/html/2509.25482v1",
            "abstract": "We present the design of an autoregressive active inference agent in the form of message passing on a factor graph. Expected free energy is derived and distributed across a planning graph. The proposed agent is validated on a robot navigation task, demonstrating exploration and exploitation in a continuous-valued observation space with bounded continuous-valued actions. Compared to a classical optimal controller, the agent modulates action based on predictive uncertainty, arriving later but with a better model of the robot’s dynamics.",
            "introduction": "Active inference is a comprehensive framework that unifies perception, planning, and learning under the free energy principle, offering a promising approach to designing autonomous agents [2, 18]. We present the design of an active inference agent implemented as a message passing procedure on a Forney-style factor graph [10, 8]. The agent is built on an autoregressive model, making continuous-valued observations and inferring bounded continuous-valued actions [7, 15]. We show that leveraging the factor graph approach produces a distributed, efficient and modular implementation [1, 3, 17, 22].\n\nProbabilistic graphical models have long been a unifying framework for the design and analysis of information processing systems, including signal processing, optimal controllers, and artificially intelligent agents [4, 12, 5, 16, 8]. Many famous algorithms can be written as message passing algorithms, including Kalman filtering, model-predictive control, and dynamic programming [12, 16].\nHowever, it can be a challenge to formulate new algorithms due to the requirement of local access to variables and the difficulty of deriving backwards messages. We highlight some of these challenges, and contribute with\n\nthe derivation of expected free energy minimization in a multivariate autoregressive model with continuous-valued observations and bounded continuous-valued actions (Sec. 4.2), and\n\nthe formulation of the planning model as a factor graph with marginal distribution updates based on messages passed along the graph (Figure 3).\n\n1. the derivation of expected free energy minimization in a multivariate autoregressive model with continuous-valued observations and bounded continuous-valued actions (Sec. 4.2), and\n\n2. the formulation of the planning model as a factor graph with marginal distribution updates based on messages passed along the graph (Figure 3).",
            "llm_summary": "【论文的motivation是什么】  \n1. 如何有效地设计自主智能体以统一感知、规划和学习。  \n2. 在复杂的观测空间中实现有效的探索与利用。  \n3. 解决传统控制器在动态建模中的局限性。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于消息传递的自回归主动推理智能体，利用因子图进行设计。通过在因子图上分布期望自由能，智能体能够在连续值观测空间中进行有效的导航。与经典的最优控制器相比，该智能体通过预测不确定性调节动作，尽管到达目标的时间较晚，但能更好地建模机器人的动态特性。实验结果表明，该方法在复杂任务中展现了更优的性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Infrastructure Sensor-enabled Vehicle Data Generation using Multi-Sensor Fusion for Proactive Safety Applications at Work Zone",
            "authors": "Suhala Rabab Saba,Sakib Khan,Minhaj Uddin Ahmad,Jiahe Cao,Mizanur Rahman,Li Zhao,Nathan Huynh,Eren Erman Ozguven",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25452",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25452",
            "arxiv_html_link": "https://arxiv.org/html/2509.25452v1",
            "abstract": "未获取到摘要",
            "introduction": "Infrastructure-based sensing and real-time trajectory generation hold significant promise for improving safety in high-risk roadway segments like work zones, yet practical deployments are hindered by perspective distortion, complex geometry, occlusions, and costs. This study tackles these barriers by (i) integrating roadside camera and LiDAR sensors into a cosimulation environment to develop a scalable, cost-effective vehicle detection and localization framework, and (ii) employing a Kalman Filter-based late fusion strategy to enhance trajectory consistency and accuracy. In simulation, the fusion algorithm reduced longitudinal error by up to 70% compared to individual sensors while preserving lateral accuracy within 1–3 meters. Field validation in an active work zone, using LiDAR, a radar-camera rig, and RTK-GPS as ground truth, demonstrated that the fused trajectories closely match real vehicle paths, even when single-sensor data are intermittent or degraded. These results confirm that KF based sensor fusion can reliably compensate for individual sensor limitations, providing precise and robust vehicle tracking capabilities. Our approach thus offers a practical pathway to deploy infrastructure-enabled multi-sensor systems for proactive safety measures in complex traffic environments.\n\nKeywords: work zone, fusion, lidar, camera, localization, safety",
            "llm_summary": "【论文的motivation是什么】  \n1. 提高高风险路段（如施工区）的安全性。  \n2. 解决现有传感器在复杂环境中的局限性。  \n3. 降低基础设施传感器系统的成本和复杂性。  \n\n【提出了什么创新的方法】  \n本研究提出了一种集成路边摄像头和LiDAR传感器的共模拟环境，开发了一种可扩展、成本效益高的车辆检测和定位框架。采用基于卡尔曼滤波的晚期融合策略，显著提高了轨迹的一致性和准确性。在模拟中，该融合算法将纵向误差减少了70%，同时保持了1-3米的横向准确性。现场验证显示，融合轨迹与真实车辆路径紧密匹配，即使单一传感器数据不稳定或降级。这表明基于卡尔曼滤波的传感器融合能够可靠地弥补单个传感器的局限性，为复杂交通环境中的主动安全措施提供了实际的解决方案。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-01 02:44:22",
            "title": "Sensor optimization for urban wind estimation with cluster-based probabilistic framework",
            "authors": "Yutong Liang,Chang Hou,Guy Y. Cornejo Maceda,Andrea Ianiro,Stefano Discetti,Andrea Meilán-Vila,Didier Sornette,Sandro Claudio Lera,Jialong Chen,Xiaozhou He,Bernd R. Noack",
            "subjects": "Machine Learning (cs.LG); Robotics (cs.RO); Fluid Dynamics (physics.flu-dyn)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.25222",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.25222",
            "arxiv_html_link": "https://arxiv.org/html/2509.25222v1",
            "abstract": "We propose a physics-informed machine-learned framework\nfor sensor-based flow estimation\nfor drone trajectories in complex urban terrain.\nThe input is a rich set of flow simulations\nat many wind conditions.\nThe outputs are velocity and uncertainty estimates\nfor a target domain and subsequent sensor optimization for minimal uncertainty.\nThe framework has three innovations\ncompared to traditional flow estimators.\nFirst, the algorithm scales proportionally to the domain complexity, making it suitable for flows that are too complex\nfor any monolithic reduced-order representation.\nSecond, the framework extrapolates\nbeyond the training data, e.g., smaller and larger wind velocities.\nLast, and perhaps most importantly, the sensor location is a free input,\nsignificantly extending the vast majority of the literature.\nThe key enablers are\n(1) a Reynolds number-based scaling of the flow variables,\n(2) a physics-based domain decomposition,\n(3) a cluster-based flow representation for each subdomain, (4) an information entropy correlating the subdomains,\nand (5) a multi-variate probability function relating sensor input and targeted velocity estimates.\nThis framework is demonstrated using drone flight paths\nthrough a three-building cluster as a simple example. We anticipate adaptations and applications for estimating complete cities and incorporating weather input.",
            "introduction": "Flow estimation in complex fluid systems\nis inherently challenging due to high dimensionality,\nnonlinearity, and limited or noisy sensor signal data Gao et al. (2024).\nAccurate and efficient estimation techniques\nare essential for real-time monitoring, control, and prediction\nin both fundamental and applied fluid dynamics.\nReduced-order modeling (ROM) enables efficient and tractable flow description\nby extracting the dominant flow features Brunton, Noack, and Koumoutsakos (2020),\nthereby offering an efficient data-driven approach for estimating large-scale flow dynamics\nwhile retaining low computational cost and strong physical interpretability.\nAmong various ROM techniques, cluster-based reduced-order models (CROMs) Burkardt, Gunzburger, and Lee (2006a); Kaiser et al. (2014) have gained increasing attention as a data-driven alternative to classical projection-based methods such as proper orthogonal decomposition (POD) Holmes et al. (2012).\n\nThe low computational cost of cluster-based analysis was initially demonstrated in incompressible flow applications Burkardt, Gunzburger, and Lee (2006b).\nRef. Kaiser et al., 2014 formalized the Cluster-based Markov Model (CMM), modeling the temporal evolution of flow fields as a Markov process over discrete clusters.\nRef. Fernex, Noack, and Semaan, 2021; Li et al., 2021 extended this framework by introducing a network-based approach that enables automated construction of reduced-order models from time-resolved data.\nSubsequent advancements have led to variations of cluster-based network models capable of capturing nonlinear dynamics, multi-attractor structures, and multi-frequency behaviors, with a focus on automation and robustness Deng et al. (2022); Hou, Deng, and Noack (2024).\n\nUrban wind field estimation, characterized by complex multiscale flows around buildings Teng et al. (2025), stands to benefit from the advances of CROMs.\nRecent works integrating experiments, simulations, and data-driven models have enhanced both the accuracy and efficiency of urban wind predictions Sousa, García-Sánchez, and Gorlé (2018); Raissi, Yazdani, and Karniadakis (2020); Raissi, Perdikaris, and Karniadakis (2019); Haghighat et al. (2021); Qin et al. (2025).\nAs the CROM-based framework significantly improves physical interpretability while providing robustness and flexibility for handling multiple flow conditions, it can be expected to be well suited for urban wind estimation.\nThis is critical for several tasks, such as trajectory planning of aerial vehicles in urban environments.\n\nAlthough data-driven models have been applied to estimate velocities along drone trajectories,\nthey frequently encounter scalability limitations.\nAs the number of spatial query points increases,\nthe computational cost of inferring flow fields from sensor data rises sharply.\nConsequently, current methods face challenges in\nbalancing accuracy, uncertainty, and model complexity,\nwhich constrains their industrial applicability.\n\nAs shown in Fig. 1,\nwe propose a physics-informed machine-learned framework\nfor sensor-based drone trajectory flow estimation.\nThe key enabler of the framework is a probabilistic model,\nwhich uses sensor signals\nto estimate the velocity of drone trajectories\nbased on the drone position and sensor location.\nThe proposed framework maximizes the accuracy of drone trajectory velocity estimation while mitigating the exponential increase in computational cost associated with growing numbers of sensors and query points.",
            "llm_summary": "【论文的motivation是什么】  \n1. Urban wind field estimation is challenging due to complex multiscale flows around buildings.  \n2. Existing data-driven models face scalability limitations and high computational costs with increasing spatial query points.  \n3. Accurate and efficient estimation techniques are essential for real-time monitoring and control in fluid dynamics.  \n\n【提出了什么创新的方法】  \n提出了一种基于物理知识的机器学习框架，用于传感器驱动的无人机轨迹流量估计。该框架通过五个关键技术（如Reynolds数缩放和基于聚类的流动表示）来优化传感器位置和流速估计，显著提高了在复杂城市环境中的准确性和效率。实验表明，该方法在处理多种风速条件下表现出色，能够有效降低计算成本并提升模型的可扩展性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        }
    ],
    "2025-10-02": [
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Real-Time Trajectory Generation and Hybrid Lyapunov-Based Control for Hopping Robots",
            "authors": "Matthew Woodward",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01138",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01138",
            "arxiv_html_link": "https://arxiv.org/html/2510.01138v1",
            "abstract": "The advent of rotor-based hopping robots has created very capable hopping platforms with high agility and efficiency, and similar controllability, as compared to their purely flying quadrotor counterparts. Advances in robot performance have increased the hopping height to greater than 4 meters and opened up the possibility for more complex aerial trajectories (i.e., behaviors). However, currently hopping robots do not directly control their aerial trajectory or transition to flight, eliminating the efficiency benefits of a hopping system. Here we show a real-time, computationally efficiency, non-linear drag compensated, trajectory generation methodology and accompanying Lyapunov-based controller. The combined system can create and follow complex aerial trajectories from liftoff to touchdown on horizontal and vertical surfaces, while maintaining strict control over the orientation at touchdown. The computational efficiency provides broad applicability across all size scales of hopping robots while maintaining applicability to quadrotors in general.",
            "introduction": "Hopping robots have shown remarkable efficiency as compared to their flying counterparts [1, 2, 3, 4], however both the newer rotor-based and traditional hopping systems [5, 6] operate in the range of 0.6 to 1.6 meters without significant deviation from a predominantly ballistic trajectory. However, as our pervious work on the MultiMo-MHR showed significant increases in hopping performance (>4>4 m), the aerial phase has sufficient time and energy to begin, as with aerial systems, controlling the overall trajectory between liftoff (LO) and touchdown (TD), allowing for greater agility and adaptability in unstructured terrain. However, unlike aerial systems, trajectory generation for hopping robots must strictly control the TD states to ensure proper positioning, orientation, and foot-surface contact to avoid damage.\n\nTo date there exists five untethered continuous hopping robots including: MultiMo-MHR (our robot) [1], PogoDrone [7], Hopcopter [3], Salto/Salto-1P [8, 9, 10, 11, 12, 13, 14, 15], and PogoX [4, 16] and one tethered continuous insect-scale hopping robot [17]; where, the hopping controllers focus on foot placement and orientation at TD. This allows for the subsequent LO state to be controlled facilitating control over the horizontal locomotion path and stability of the hopping cycle. Hopcopter and Salto have both explored hopping from vertical surfaces (i.e., walls). However, the vertical surface hopping controllers typically control orientation only. The Hopcopter transitions from horizontal flight control to an orientation hold controller at 1.8 m from the wall, and the flight controller is reactivated after the wall-hop. Whereas, Salto initiates a wall-hop from a prior ground-hop oriented towards the wall. At LO an orientation hold controller activates to maintain a prescribed wall contact angle, where presumably the foot placement and orientation hold controller would be reactivated; however this is not discussed. In all cases the trajectories are predominantly ballistic however, to accommodate uncertain LO states and desired TD states, interact with both horizontal and vertical surfaces, and avoid obstacles, control over the entire trajectory from LO to TD is necessary to continue advancing the capabilities of rotor-based hopping robots.\n\nThe paper is organized as follows, with Section 2 presenting the dynamic model and the differential flatness derivation. Section 3 develops the real-time hopping trajectory generation methodology, and Section 4 derives the Lyapounv-based controller. Section 5 discusses the trajectory tracking results and Section 6 summarizes the work.",
            "llm_summary": "【论文的motivation是什么】  \n1. 当前跳跃机器人无法直接控制其空中轨迹或过渡到飞行，限制了其效率。  \n2. 现有的跳跃控制器主要关注着陆时的足部放置和方向，缺乏对整个轨迹的控制。  \n3. 需要更复杂的空中轨迹以提高在非结构化地形中的灵活性和适应性。  \n\n【提出了什么创新的方法】  \n本论文提出了一种实时、计算高效的非线性拖曳补偿轨迹生成方法和相应的Lyapunov控制器。该系统能够从起飞到着陆生成和跟踪复杂的空中轨迹，同时严格控制着陆时的姿态。通过这种方法，跳跃机器人能够在不同的表面上实现更高的机动性和适应性，显著提升了其在复杂环境中的表现。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition",
            "authors": "Jiahang Cao,Yize Huang,Hanzhong Guo,Rui Zhang,Mu Nan,Weijian Mai,Jiaxu Wang,Hao Cheng,Jingkai Sun,Gang Han,Wen Zhao,Qiang Zhang,Yijie Guo,Qihao Zheng,Chunfeng Song,Xiao Li,Ping Luo,Andrew F. Luo",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "Project Page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.01068",
            "code": "https://sagecao1125.github.io/GPC-Site/",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01068",
            "arxiv_html_link": "https://arxiv.org/html/2510.01068v1",
            "abstract": "Diffusion-based models for robotic control, including vision-language-action (VLA) and vision-action (VA) policies, have demonstrated significant capabilities. Yet their advancement is constrained by the high cost of acquiring large-scale interaction datasets. This work introduces an alternative paradigm for enhancing policy performance without additional model training. Perhaps surprisingly, we demonstrate that the composed policies can exceed the performance of either parent policy. Our contribution is threefold. First, we establish a theoretical foundation showing that the convex composition of distributional scores from multiple diffusion models can yield a superior one-step functional objective compared to any individual score. A Grönwall-type bound is then used to show that this single-step improvement propagates through entire generation trajectories, leading to systemic performance gains. Second, motivated by these results, we propose General Policy Composition (GPC), a training-free method that enhances performance by combining the distributional scores of multiple pre-trained policies via a convex combination and test-time search. GPC is versatile, allowing for the plug-and-play composition of heterogeneous policies, including VA and VLA models, as well as those based on diffusion or flow-matching, irrespective of their input visual modalities. Third, we provide extensive empirical validation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside real-world robotic evaluations, confirm that GPC consistently improves performance and adaptability across a diverse set of tasks. Further analysis of alternative composition operators and weighting strategies offers insights into the mechanisms underlying the success of GPC. These results establish GPC as a simple yet effective method for improving control performance by leveraging existing policies. Our project page is in https://sagecao1125.github.io/GPC-Site/.",
            "introduction": "Diffusion Policies (DPs) (Chi et al., 2023; Ho et al., 2020; Song et al., 2020a) have emerged as a powerful method for policy parameterization in robot learning, enabling the representation of complex, multi-modal action distributions – a key advantage for policies conditioning on high-dimensional inputs like vision and language in domains from manipulation (Ze et al., 2024b; Zhu et al., 2024; Liu et al., 2024a) to navigation (Sridhar et al., 2024; Zhang et al., 2024a).\nDespite this progress, the advancement of diffusion- and flow-based policies is fundamentally constrained by scaling challenges related to both model capacity and data availability. Performance can plateau due to the intrinsic representational limits of a given model, yet scaling up the model architecture also requires the collection of costly interaction datasets to fully capture the potential performance benefit (Black et al., 2024). Conventional post-training strategies offer limited solutions; supervised fine-tuning requires expensive data collection (Ouyang et al., 2022), while reinforcement learning introduces the complexity of reward engineering and extensive online interaction (Hu et al., 2025).\n\nTo overcome these limitations, this work introduces an alternative paradigm: creating stronger policies by composing existing, pre-trained models. While prior work has explored static model composition (Du & Kaelbling, 2024; Wang et al., 2024c), we find that the optimal weighting is not universal but is instead highly task-dependent, even for a fixed set of parent policies. Drawing inspiration from compositional generative modeling, we first establish a theoretical foundation showing that a convex combination of distributional scores can yield a provably superior objective for policy improvement. This principle underpins our proposed method of General Policy Composition (GPC, Fig. 1). GPC is a training-free framework that, at inference time, combines the distributional scores of multiple pre-trained policies via convex combination and test-time search. This approach flexibly integrates heterogeneous models – spanning diffusion- and flow-based architectures, VA and VLA modalities, and diverse sensory inputs – to form a more capable policy, all without modifying the base models. Crucially, we demonstrate that the resulting composed policy can exceed the performance of any of its individual parent policies.\n\nWe validate GPC through extensive experiments in both simulation and real-world environments, demonstrating consistent outperformance against single-policy baselines. Our analysis extends to alternative composition operators (e.g., logical AND/OR) and various weighting configurations, offering broader insights into why and when composition is effective.\nOur contributions are summarized as follows:\n(i) We establish a theoretical foundation for robot policy composition, proving that the convex combination of distributional scores can yield an improved functional objective and that this advantage propagates to the system level.\n(ii) We propose General Policy Composition (GPC), a flexible, training-free framework that combines pre-trained policies across different modalities and architectures into a more expressive policy.\n(iii) We conduct extensive evaluations in simulation and the real world, demonstrating the consistent performance gains of GPC while analyzing key design choices to guide future research in policy composition.",
            "llm_summary": "【论文的motivation是什么】  \n1. 高成本的交互数据集限制了扩展基于扩散和流的机器人策略的能力。  \n2. 现有的后训练策略在提升性能方面提供的解决方案有限。  \n3. 需要一种新的方法来通过组合现有的预训练模型来创建更强的策略。  \n\n【提出了什么创新的方法】  \n本文提出了一种名为General Policy Composition (GPC)的方法，通过在推理时结合多个预训练策略的分布得分，利用凸组合和测试时搜索来增强策略性能。GPC允许灵活集成不同的模型架构和输入模态，且无需对基础模型进行修改。实验结果表明，GPC在多种任务中均能超越任何单一父策略的性能，展示了其在机器人控制中的有效性和适应性。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "ROSplane 2.0: A Fixed-Wing Autopilot for Research",
            "authors": "Ian Reid,Joseph Ritchie,Jacob Moore,Brandon Sutherland,Gabe Snow,Phillip Tokumaru,Tim McLain",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01041",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01041",
            "arxiv_html_link": "https://arxiv.org/html/2510.01041v1",
            "abstract": "Unmanned aerial vehicle (UAV) research requires the integration of cutting-edge technology into existing autopilot frameworks.\nThis process can be arduous, requiring extensive resources, time, and detailed knowledge of the existing system.\nROSplane is a lean, open-source fixed-wing autonomy stack built by researchers for researchers.\nIt is designed to accelerate research by providing clearly defined interfaces with an easily modifiable framework.\nPowered by ROS 2, ROSplane allows for rapid integration of low or high-level control, path planning, or estimation algorithms.\nA focus on lean, easily understood code and extensive documentation lowers the barrier to entry for researchers.\nRecent developments to ROSplane improve its capacity to accelerate UAV research, including the transition from ROS 1 to ROS 2, enhanced estimation and control algorithms, increased modularity, and an improved aerodynamic modeling pipeline.\nThis aerodynamic modeling pipeline significantly reduces the effort of transitioning from simulation to real-world testing without requiring expensive system identification or computational fluid dynamics tools.\nROSplane’s architecture reduces the effort required to integrate new research tools and methods, expediting hardware experimentation.",
            "introduction": "Unmanned aerial vehicles (UAVs) have gained significant popularity due to applications such as package delivery, photography, surveillance, or advanced air mobility (AAM).\nResearch targeting UAVs often requires ready access to the inner workings of many portions of an autonomy software stack, including estimation, path planning, and high/low-level control.\nThis access is important in all stages of research and development, both in simulation and hardware flight tests.\n\nResearchers often face significant integration problems when it comes to conducting realistic simulations and real-world flight tests.\nResearch software can require extensive rewriting or refactoring to be compatible with realistic simulations or hardware platforms.\nThis increases the difficulty of conducting real-world experiments, which are essential steps in validating and proving out new research algorithms and methods.\n\nROSflight[1] is a lean, open-source111https://github.com/rosflight autopilot designed to mitigate these challenges and reduce the barrier to entry for UAV research.\nIt accomplishes this by allowing the same software that runs in simulation to also control the physical vehicle with no changes.\nSince ROSflight is built on the robot operating system (ROS 2), it offers superior modularity and customizability.\n\nROSplane[2] is an open-source222https://github.com/rosflight/rosplane autonomy stack for fixed-wing UAVs designed to work with ROSflight.\nA lean feature set means ROSplane offers not only basic functionality, but also enables better understanding for quick and seamless integration of external codebases.\nExtensive documentation on the algorithms used in ROSplane is available, making it a valuable resource for educational use and research[3], [2].\n\nWhile ROSplane has received detailed attention in the past[2], recent advances have significantly improved ROSplane’s use in advanced UAV research.\nThese improvements reduce the barriers to practical UAV research by enhancing the usability, modularity, and extensibility of ROSplane, especially for hardware experiments.\nThe contributions of this work are to describe the advancements of ROSplane including\n\nthe transition from ROS 1 to ROS 2,\n\nupdated algorithms and modularity for control and state estimation,\n\nan improved aerodynamic modeling pipeline that significantly reduces the simulated-to-real experiment transition effort.\n\nThe rest of this work is organized as follows.\nSection II discusses other available autopilots and related work.\nSection III describes the system architecture.\nThe improved aerodynamic modeling pipeline is described in Section IV.\nAlgorithm improvements are discussed in Section V.\nA tutorial on using ROSplane is provided in VI.\nHardware test results and comparison to simulation are shown in Section VII, and then concluding remarks are offered.\n\n1. the transition from ROS 1 to ROS 2,\n\n2. updated algorithms and modularity for control and state estimation,\n\n3. an improved aerodynamic modeling pipeline that significantly reduces the simulated-to-real experiment transition effort.",
            "llm_summary": "【论文的motivation是什么】  \n1. 研究人员在进行UAV研究时面临整合现有自动驾驶系统的挑战。  \n2. 现实模拟和真实飞行测试的兼容性问题增加了实验的难度。  \n3. 需要降低UAV研究的进入门槛，以加速研究进程。  \n\n【提出了什么创新的方法】  \n本论文提出了ROSplane 2.0，一个基于ROS 2的开源固定翼自动驾驶系统，旨在为研究人员提供一个可快速集成的框架。其核心方法包括从ROS 1迁移到ROS 2、更新控制和状态估计算法、以及改进的气动建模管道。这些创新显著降低了从模拟到真实实验的过渡难度，提升了模块化和可扩展性。通过这些改进，ROSplane 2.0能够加速UAV研究，提供更高的可用性和更低的技术门槛。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Prometheus: Universal, Open-Source Mocap-Based Teleoperation System with Force Feedback for Dataset Collection in Robot Learning",
            "authors": "S. Satsevich,A. Bazhenov,S. Egorov,A. Erkhov,M. Gromakov,A. Fedoseev,D. Tsetserukou",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01023",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01023",
            "arxiv_html_link": "https://arxiv.org/html/2510.01023v1",
            "abstract": "This paper presents a novel teleoperation system with force feedback, utilizing consumer-grade HTC Vive Trackers 2.0. The system integrates a custom-built controller, a UR3 robotic arm, and a Robotiq gripper equipped with custom-designed fingers to ensure uniform pressure distribution on an embedded force sensor. Real-time compression force data is transmitted to the controller, enabling operators to perceive the gripping force applied to objects. Experimental results demonstrate that the system enhances task success rates and provides a low-cost solution for large-scale imitation learning data collection without compromising affordability.",
            "introduction": "Robotics has undergone significant advancements in recent years, particularly in demonstration-based learning using neural networks with transformer architectures. These methods show remarkable potential for automating both industrial and everyday tasks. However, the performance of such models heavily depends on the quantity and quality of the training data.\n\nA common data collection approach involves teleoperation with motion capture systems, where human motions are transferred to a robot via inverse kinematics. While effective, this method has a critical limitation: the lack of force feedback during object manipulation. Without accurate force perception, both the operator and the neural network may apply excessive gripping forces, leading to object deformation or damage.\n\nTo address this issue, we propose a low-cost, open-source teleoperation system with integrated force feedback (Fig. 1). Our solution leverages 3D-printed and commercially available mechanical components, combined with custom-designed PCBs. The entire system—including all hardware, software, and firmware components— will be made available as open-source resources, including detailed instructions for fabrication, assembly, PCB ordering, and system deployment.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的遥操作系统缺乏在物体操作中的力反馈。  \n2. 训练数据的数量和质量对基于示范的学习模型性能至关重要。  \n\n【提出了什么创新的方法】  \n本研究提出了一种低成本、开源的遥操作系统，集成了力反馈功能，利用HTC Vive Trackers 2.0进行动作捕捉。系统包括自定义控制器、UR3机械臂和Robotiq夹持器，能够实时传输压缩力数据，增强操作者对施加在物体上的抓取力的感知。实验结果表明，该系统提高了任务成功率，为大规模模仿学习数据收集提供了经济实惠的解决方案。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "ROSflight 2.0: Lean ROS 2-Based Autopilot for Unmanned Aerial Vehicles",
            "authors": "Jacob Moore,Phil Tokumaru,Ian Reid,Brandon Sutherland,Joseph Ritchie,Gabe Snow,Tim McLain",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "To be submitted to the 2026 IEEE International Conference on Robotics and Automation in Vienna, Austria",
            "pdf_link": "https://arxiv.org/pdf/2510.00995",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00995",
            "arxiv_html_link": "https://arxiv.org/html/2510.00995v1",
            "abstract": "ROSflight is a lean, open-source autopilot ecosystem for unmanned aerial vehicles (UAVs).\nDesigned by researchers for researchers, it is built to lower the barrier to entry to UAV research and accelerate the transition from simulation to hardware experiments by maintaining a lean (not full-featured), well-documented, and modular codebase.\nThis publication builds on previous treatments and describes significant additions to the architecture that improve the modularity and usability of ROSflight, including the transition from ROS 1 to ROS 2, supported hardware, low-level actuator mixing, and the simulation environment.\nWe believe that these changes improve the usability of ROSflight and enable ROSflight to accelerate research in areas like advanced-air mobility.\nHardware results are provided, showing that ROSflight is able to control a multirotor over a serial connection at 400 Hz while closing all control loops on the companion computer.",
            "introduction": "In recent years, interest in unmanned aerial vehicles (UAVs) has increased significantly.\nTechnological advances have enabled numerous applications of UAVs, including package delivery, photography, search-and-rescue, firefighting, as well as military applications.\n\nAdvanced air mobility (AAM), a category broadly referring to increasing autonomy in urban areas for civilian use, is also currently an area of high interest.\nAAM aircraft often take the form of eVTOL aircraft, and specialized autopilots, algorithms, and hardware are needed to effectively conduct research in this field.\nBecause of this, researchers often need access to the inner workings of an autopilot (e.g., the state estimator or inner loop controller), which makes commercial closed-source autopilots or many open-source autopilots difficult to use.\nAdditionally, simulation and hardware experiments are critical in AAM research to ensure proposed systems and methodologies are safe and function as intended.\n\nROSflight is a lean, open-source autopilot for UAVs built for research.\nBecause it is designed to be lean, ROSflight is not full-featured and does not boast many of the state-of-the-art functions available in other popular open-source autopilots [1, 2].\nInstead, ROSflight offers only the basic functionality needed to support UAV research, prioritizing understandability.\nWhile this places more responsibility on the end user to develop application-specific code, we believe a lean architecture reduces the black-box nature of the autopilot, thus reducing the total effort to implement a user’s application code.\nAdditionally, the ROSflight project has a strong emphasis on clear code and complete documentation, which improves accessibility and lowers the barrier to entry for researchers.\nDocumentation can be found on the project website, rosflight.org.\n\nBuilt on the robot operating system (ROS 2) [3], ROSflight is designed with a modular architecture to fit the needs of varying airframes and applications.\nROSflight is designed to enable true software in the loop (SIL) simulation.\nWhen using ROSflight, the same code that runs the autopilot in simulation also flies the vehicle in hardware, with no changes—significantly enhancing the transition from simulation to hardware experiments.\n\nROSflight has received detailed treatment in [4, 5].\nWe rely on [5] for an excellent description of the overall architecture and design goals of the ROSflight project.\nThe contributions of this work are to describe significant improvements to ROSflight in the release of ROSflight 2.0 including\n\nthe transition from ROS 1 to ROS 2,\n\nimproved modularity and accuracy in the actuator mixing,\n\nnew supported hardware for faster and more reliable operation,\n\na restructured and modular simulation environment to support diverse simulation needs, and\n\nflight test results demonstrating these improvements in hardware.\n\nDue to these advancements, ROSflight 2.0 improves the modularity and usability of the software, enabling ROSflight to accelerate research in areas like advanced air mobility.\n\nThe rest of the paper is organized as follows:\nSection II describes work similar to ROSflight.\nA brief overview of ROSflight is described in Section III\nImprovements to the ROSflight architecture are described in detail in Section IV, and supported hardware and improvements to the simulation environment are discussed in Section V.\nHardware results demonstrating these improvements to ROSflight are described in Section VI and we conclude in Section VII.\n\n1. the transition from ROS 1 to ROS 2,\n\n2. improved modularity and accuracy in the actuator mixing,\n\n3. new supported hardware for faster and more reliable operation,\n\n4. a restructured and modular simulation environment to support diverse simulation needs, and\n\n5. flight test results demonstrating these improvements in hardware.",
            "llm_summary": "【论文的motivation是什么】  \n1. 研究者需要更易用的开源无人机自动驾驶系统以降低研究门槛。  \n2. 现有的商业闭源自动驾驶系统难以满足研究需求，缺乏透明性和可定制性。  \n3. 模拟和硬件实验在先进空中移动性研究中至关重要，需要有效的工具支持。  \n\n【提出了什么创新的方法】  \nROSflight 2.0通过将ROS 1迁移到ROS 2，提升了模块化和准确性，支持新硬件以实现更快更可靠的操作，并重构了模拟环境以满足多样化需求。该系统的设计强调可理解性和文档完整性，降低了研究者的使用门槛。通过这些改进，ROSflight 2.0能够加速在先进空中移动性领域的研究，并在硬件测试中展示了显著的性能提升。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Non-submodular Visual Attention for Robot Navigation",
            "authors": "Reza Vafaee,Kian Behzad,Milad Siami,Luca Carlone,Ali Jadbabaie",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "Accepted to appear in IEEE Transactions on Robotics (T-RO)",
            "pdf_link": "https://arxiv.org/pdf/2510.00942",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00942",
            "arxiv_html_link": "https://arxiv.org/html/2510.00942v1",
            "abstract": "This paper presents a task-oriented computational framework to enhance Visual-Inertial Navigation (VIN) in robots, addressing challenges such as limited time and energy resources. The framework strategically selects visual features using a Mean Squared Error (MSE)-based, non-submodular objective function and a simplified dynamic anticipation model. To address the NP‐hardness of this problem, we introduce four polynomial‐time approximation algorithms: a classic greedy method with constant‐factor guarantees; a low‐rank greedy variant that significantly reduces computational complexity; a randomized greedy sampler that balances efficiency and solution quality; and a linearization‐based selector based on a first‐order Taylor expansion for near‐constant‐time execution. We establish rigorous performance bounds by leveraging submodularity ratios, curvature, and element‐wise curvature analyses. Extensive experiments on both standardized benchmarks and a custom control‐aware platform validate our theoretical results, demonstrating that these methods achieve strong approximation guarantees while enabling real‐time deployment.",
            "introduction": "Achieving efficient navigation in unpredictable environments remains a significant challenge in robotics. Recent advancements in computing devices have opened up new possibilities, propelling substantial progress in this research area [1]. These advancements enable near-real-time resolution of estimation and planning tasks in specific applications. However, navigating robots in rapidly changing environments still faces computational complexities. Despite the availability of high-performance computational units, the increasing demand for agility and autonomy necessitates more efficient onboard processing to ensure timely decision-making and execution in dynamic conditions.\n\nOne of the key challenges in robot navigation is achieving accurate visual odometry while minimizing computational resource usage. Many researchers have focused on the issue of selecting visual features (e.g., [2, 3, 4, 5, 6, 7]). The core idea is that, based on the robot’s current state and its planned future motion (i.e., the task at hand), tracking specific features over a time horizon can provide more valuable information than tracking others. Essentially, some visual features may require greater attention than others.\n\nIn this context, [8] employs a greedy approach to select a subset of pre-identified visual features, thereby streamlining the robot’s pose estimation. In [9], the authors combine simultaneous localization and mapping (SLAM) using unscented Kalman filtering with reinforcement learning to create policies for feature selection. [4] proposes a two-stage methodology for measurement planning, where the initial stage involves selecting a subset of landmarks for observation, followed by determining observation times for each feature. Additionally, in [3], a task-aware approach is investigated to select a subset of features with the goal of minimizing an uncertainty metric.\n\nRelated sparsification problems are explored in other contexts. For example, [10, 11] address optimizing feedback interconnections in linear consensus networks, while [12, 13] focus on selecting sparse sets of sensors or actuators to preserve system observability and controllability. However, these frameworks are not directly applicable to the motion-aware feature selection problem discussed here, as they focus on dyads rather than selecting a limited number of Positive Semidefinite (PSD) matrices.\n\nPrior works such as [5, 6] show that feature selection for robot navigation can be framed as maximizing a non-negative monotone submodular function under a matroid constraint, with the greedy algorithm achieving an optimal approximation of 1−1/e≈0.6321-1/e\\approx 0.632 for cardinality constraints [14]. This process accelerates with lazy evaluations [15] and further optimizes with randomization for linear time complexity [16]. Recently, [17] propose a deterministic non-oblivious local search algorithm with a 1−1/e−ε1-1/e-\\varepsilon (for any ϵ>0\\epsilon>0) guarantee. However, these methods are not applicable to our problem, since MSE, our primary performance metric, is not submodular.\n\nThe most closely related prior work to the present study is conducted by [5] and [6]. In [5], the focus is on visual-inertial navigation, in which the design variable is the selection of features to track over a fixed time horizon. The authors use convex relaxations and a greedy method for feature selection, providing a quantitative assessment of performance guarantees for the quality of the resulting state estimations. They evaluate the effectiveness of the greedy heuristic using the submodularity ratio from [18]. However, calculating the submodularity ratio for the proposed set functions is computationally challenging due to the combinatorial complexity involved in its definition [19]. Additionally, the complexity of a simple semidefinite programming (SDP) convex relaxation scales cubically with the number of detected features, making it infeasible for real-time implementation. Moreover, their measures used to quantify the confidence ellipsoid of the forward predictor are not explicitly related to MSE, which is often the performance metric of interest for estimation problems [20].\n\nA recent study detailed in [21] also tackles the challenge of reducing latency in Visual Odometry (VO)/Visual SLAM (VSLAM) systems by identifying and matching a subset of features deemed most valuable for pose estimation. While their focus lies in optimizing feature matching latency using the Max-logDet metric, our work concentrates on selecting informative features that minimize MSE under motion-aware dynamics, providing theoretical guarantees under non-submodular objectives.\n\nIn [6], the authors propose a randomized sampling algorithm for feature selection instead of using the greedy method or convex relaxations. In their approach, a sampling probability (a number between zero and one) is assigned to each available feature, with these probabilities interpreted as measures of informativeness during the sampling process. This procedure provides a performance guarantee compared to using all visual features, rather than the optimal set, for a range of VIN measures, including MSE. This is achieved by approximating the complete spectrum of the information matrix. However, for some measures, such as the worst-case error, this approach imposes unnecessary computational burden because we only need to approximate the minimum eigenvalue of the information matrix, rather than the full spectrum. Additionally, the proposed approach requires sampling 𝒪​((3​T+3)​log⁡(3​T+3)/ϵ2)\\mathcal{O}((3T+3)\\log(3T+3)/\\epsilon^{2}) features, where TT is the forward time horizon and ϵ∈(0,1)\\epsilon\\in(0,1) is an approximation parameter, to guarantee the proposed performance bounds. We observe that this requirement for the number of sampled features is significant and, in practical scenarios, necessitates sampling all features extracted in the estimation problems.\n\nThe authors of [7] later address the problem of sparse feature selection for localizing a team of agents, where they exchange relative measurements leading to a graphical network. Compared to [6], they improve the probabilistic bound of the randomized feature selection algorithm, although the related problems mentioned in the previous paragraph still exist.\n\nVIN methods are broadly categorized into filtering-based approaches, fixed-lag smoothing, and full smoothing. Filtering methods, such as the extended Kalman filter-based and its variants analyzed in [22], offer real-time performance by sequentially processing data. However, they typically underutilize cross-time correlations, which may limit estimation accuracy in complex scenarios.\n\nFixed-lag smoothers [23, 24] enhance estimation accuracy via sliding-window optimization. Specifically, [23] integrates visual and inertial data in a keyframe-based bundle adjustment framework for visual-inertial SLAM. In contrast, [24] employs a constant-time sliding-window filter using stereo imagery for real-time relative pose estimation during planetary landing. While both methods retain recent poses and marginalize older ones, their objectives and sensing modalities differ.\n\nFull smoothing methods [25, 26] jointly optimize over all past states using nonlinear optimization. The first achieves efficient maximum-a-posteriori estimation through IMU preintegration on manifolds, while the second handles high-dynamic motion without requiring prior state initialization. These methods offer increased accuracy but incur higher computational complexity due to global optimization.\n\nMore recently, ORB-SLAM3 [27] has emerged as a leading keyframe-based SLAM system that integrates visual and inertial data via local bundle adjustment and global pose graph optimization. While it does not implement full smoothing over all states, its flexibility and support for multiple sensor configurations make it a strong practical baseline. This paper focuses on the complementary task of selecting informative visual measurements, which can enhance the front-end performance of VIN systems across all categories.\n\nOur contributions: To address the existing issues, in this work,\n\nusing a similar simplified model for forward simulation of robot dynamics as [5], we formulate the task of feature selection as maximizing a monotone non-submodular objective function directly related to the mean-square state estimation error.\n\nusing a similar simplified model for forward simulation of robot dynamics as [5], we formulate the task of feature selection as maximizing a monotone non-submodular objective function directly related to the mean-square state estimation error.\n\nwe propose constant factor approximation bounds for the greedy algorithm based on recent concepts of submodularity ratio and curvature [28]. We derive bounds on these values according to the spectrum of the information matrices, eliminating the need for combinatorial search. These bounds suggest an easy-to-obtain performance guarantee for the greedy approach.\n\nwe exploit the low-rank structure of the landmark information matrices to significantly reduce the computational cost of matrix inversion in each greedy iteration. This leads to a fast version of the greedy algorithm that maintains the same approximation guarantees as the standard approach, while offering substantial practical speedups.\n\ninspired by [29], we include a randomization step in a simple greedy procedure to increase computational efficiency and practicality for scenarios with numerous detected features or longer prediction horizons. This randomized greedy framework is supported by a performance bound based on the element-wise curvature concept.\n\nwe transform the problem into an efficient modular maximization by using a first-order Taylor series approximation of the VIN performance measure. In this form, the greedy selection method guarantees an optimal solution.\n\nwe validate the proposed selectors in two complementary experiments. First, multiple EuRoC sequences [30] allow direct comparison with prior work. Because EuRoC lacks control inputs for horizon prediction, we also run a full visual–inertial experiment on the QCar platform [31], where stereo images, IMU data, and control commands are all available. This two-pronged evaluation demonstrates both benchmark accuracy and real-world applicability.\n\nUnlike [6], all proposed performance bounds characterize proximity to the optimal visual feature set. We provide some of the proofs in Appendix -B, to keep the main text focused on the discussion of the problem and proposed solutions.\n\nFig. 1 provides a comprehensive overview of the visual attention mechanism investigated in this paper. This mechanism functions as a critical sub-block in the front-end, where it processes inputs such as extracted features from keyframes and motion-related parameters obtained, for example, from an IMU. The primary objective of the visual attention mechanism is to determine which features should be selected for back-end optimization. This selection process is guided by evaluating both the quality of the features and their potential impact in decreasing uncertainties based on robot dynamics. By effectively filtering and prioritizing features, the visual attention mechanism ensures that the back-end optimization process operates with the most relevant and high-quality data, thereby enhancing the overall performance and accuracy of the system.\n\n1. using a similar simplified model for forward simulation of robot dynamics as [5], we formulate the task of feature selection as maximizing a monotone non-submodular objective function directly related to the mean-square state estimation error.\n\n2. we propose constant factor approximation bounds for the greedy algorithm based on recent concepts of submodularity ratio and curvature [28]. We derive bounds on these values according to the spectrum of the information matrices, eliminating the need for combinatorial search. These bounds suggest an easy-to-obtain performance guarantee for the greedy approach.\n\n3. we exploit the low-rank structure of the landmark information matrices to significantly reduce the computational cost of matrix inversion in each greedy iteration. This leads to a fast version of the greedy algorithm that maintains the same approximation guarantees as the standard approach, while offering substantial practical speedups.\n\n4. inspired by [29], we include a randomization step in a simple greedy procedure to increase computational efficiency and practicality for scenarios with numerous detected features or longer prediction horizons. This randomized greedy framework is supported by a performance bound based on the element-wise curvature concept.\n\n5. we transform the problem into an efficient modular maximization by using a first-order Taylor series approximation of the VIN performance measure. In this form, the greedy selection method guarantees an optimal solution.\n\n6. we validate the proposed selectors in two complementary experiments. First, multiple EuRoC sequences [30] allow direct comparison with prior work. Because EuRoC lacks control inputs for horizon prediction, we also run a full visual–inertial experiment on the QCar platform [31], where stereo images, IMU data, and control commands are all available. This two-pronged evaluation demonstrates both benchmark accuracy and real-world applicability.",
            "llm_summary": "【论文的motivation是什么】  \n1. 在动态环境中实现高效导航仍然是机器人领域的重大挑战。  \n2. 现有的视觉特征选择方法在计算复杂性和实时性方面存在局限。  \n3. 需要一种新的方法来优化视觉特征选择，以提高状态估计的准确性。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于非次模目标函数的视觉特征选择框架，旨在最小化均方状态估计误差。通过引入四种多项式时间近似算法，作者解决了特征选择中的NP难题，并提供了常数因子近似保证。利用地标信息矩阵的低秩结构，显著降低了每次贪婪迭代中的矩阵求逆计算成本，从而实现了快速贪婪算法。随机化步骤的引入进一步提高了计算效率。通过在多个EuRoC序列和QCar平台上的实验验证，展示了所提方法在基准准确性和实际应用中的有效性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Product-oriented Product-Process-Resource Asset Network and its Representation in AutomationML for Asset Administration Shell",
            "authors": "Sara Strakosova,Petr Novak,Petr Kadera",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "This work has been submitted to the IEEE for possible publication.",
            "pdf_link": "https://arxiv.org/pdf/2510.00933",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00933",
            "arxiv_html_link": "https://arxiv.org/html/2510.00933v1",
            "abstract": "Current products, especially in the automotive sector, pose complex technical systems having a multi-disciplinary mechatronic nature. Industrial standards supporting system engineering and production typically (i) address the production phase only, but do not cover the complete product life cycle, and (ii) focus on production processes and resources rather than the products themselves. The presented approach is motivated by incorporating impacts of end-of-life phase of the product life cycle into the engineering phase. This paper proposes a modelling approach coming up from the product-process-resource (PPR) modeling paradigm. It combines requirements on (i) respecting the product structure as a basis for the model, and (ii) it incorporates repairing, remanufacturing, or upcycling within cyber-physical production systems. The proposed model called PoPAN should accompany the product during the entire life cycle as a digital shadow encapsulated within the Asset Administration Shell of a product. To facilitate adoption of the proposed paradigm, the paper also proposes serialization of the model in the AutomationML data format. The model is demonstrated on a use-case for disassembling electric vehicle batteries to support their remanufacturing for stationary battery applications.",
            "introduction": "The significant environmental impact of manufacturing industries, underscores the importance of addressing sustainability in the context of recycling and remanufacturing of products [1]. Although the term ”sustainability” is increasingly used in marketing, leading to skepticism and negative perceptions. Distinguishing genuine sustainability efforts from mere marketing strategies is essential. In this paper, we introduce a formalization to model products and production processes, which has a potential to genuinely contribute to sustainability specifically at the end of a product lifecycle.\n\nThis paper introduces the Product-oriented Product–Process–Resource Asset Network (PoPAN) formalization, which builds upon the Product–Process–Resource (PPR) [2] and the Product–Process–Resource Asset Network (PAN) [3] approaches. Current modeling approaches (including the original PAN) are frequently focused on production processes. This perspective is substantiated by numerous industrial standards such as ISA-95 or IEC 62264, Asset Administration Shell, and VDI/VDE 3682. On the contrary, the PoPAN spotlights the product structure as a basis and it assigns production processes/operations to product components.\n\nPoPAN offers a structured description of a product, incorporating its components (represented as products), processes, and resources. This approach enables the design of recycling and remanufacturing processes for products that require disassembly for efficient recycling. Furthermore, PoPAN can function as a digital shadow [4], accompanying the product throughout its entire lifecycle and encapsulating all relevant information. It serves as a record of the product’s evolution, including any modifications or alterations it undergoes during its journey, such as missing components like screws.\nAn additional advantage of employing PoPAN for recycling lies in the data collection aspect. Sustainability initiatives begin at the product design phase, where prioritizing recyclability is paramount. By gathering relevant data from recycling processes, manufacturers can analyze and utilize this information to enhance the recyclability of future product designs. This iterative process leads to more sustainable designs and facilitates easier recycling, contributing to overall environmental sustainability efforts\nanother advantage of a digital twin/PoPAN for recycling is the collection of data from the process.\n\nTo facilitate the adoption of this paradigm, the paper also proposes the serialization of the model in the AutomationML [5] data format. This standardized format enhances interoperability and ease of integration within existing industrial frameworks and systems. An important benefit of AutomationML is the advancing integration with the Asset Administration Shell (AAS) [6]. AAS is a set of standards and recommendations to provide interoperability in industrial systems. AAS is typically used on the resource level (e.g., a motor or a robot). On the contrary, the AAS is used for the product level in the presented approach utilizing PoPAN. Moreover, AAS is used for the entire product life-cycle in this approach, including not only the manufacturing phase, but also operations, maintenance and repairing, as well as decommissioning/recycling. The paper [7] highlights the use of AAS in the context of the product life cycle management.\n\nThis paper addresses the following research questions:\n\nRQ1: How can we adapt the PAN model to primary reflect the product structure rather than the production process with resource structure?\n\nRQ1: How can we adapt the PAN model to primary reflect the product structure rather than the production process with resource structure?\n\nRQ2: Is it possible to combine production and remanufacturing/disassembling processes into a proposed PPR-based description and if yes, how to do so?\n\nRQ3: To improve adoption of the proposed approach by industry and academia, how can be the proposed model represented in the AutomationML data format?\n\nThe proposed modeling approach is demonstrated on a case study on disassembling electric vehicle (EV) batteries for recycling and remanufacturing [8].\n\n1. RQ1: How can we adapt the PAN model to primary reflect the product structure rather than the production process with resource structure?\n\n2. RQ2: Is it possible to combine production and remanufacturing/disassembling processes into a proposed PPR-based description and if yes, how to do so?\n\n3. RQ3: To improve adoption of the proposed approach by industry and academia, how can be the proposed model represented in the AutomationML data format?",
            "llm_summary": "【论文的motivation是什么】  \n1. 当前产品生命周期管理缺乏全面覆盖，尤其是汽车行业的复杂技术系统。  \n2. 现有工业标准主要关注生产过程，而忽视了产品本身及其生命周期的其他阶段。  \n3. 需要将产品的回收和再制造过程纳入工程阶段，以促进可持续发展。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于产品-过程-资源（PPR）建模范式的产品导向产品-过程-资源资产网络（PoPAN）模型。该模型强调产品结构，并将生产过程与产品组件关联。PoPAN作为数字影像，伴随产品整个生命周期，记录其演变过程。通过在AutomationML数据格式中序列化，增强了模型的互操作性和集成性，支持电动汽车电池的拆解与再制造案例，促进了可持续设计和回收过程的数据收集与分析。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "RTFF: Random-to-Target Fabric Flattening Policy using Dual-Arm Manipulator",
            "authors": "Kai Tang,Dipankar Bhattacharya,Hang Xu,Fuyuki Tokuda,Norman C. Tien,Kazuhiro Kosuge",
            "subjects": "Robotics (cs.RO)",
            "comment": "conference",
            "pdf_link": "https://arxiv.org/pdf/2510.00814",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00814",
            "arxiv_html_link": "https://arxiv.org/html/2510.00814v1",
            "abstract": "Robotic fabric manipulation in garment production for sewing, cutting, and ironing requires reliable flattening and alignment, yet remains challenging due to fabric deformability, effectively infinite degrees of freedom, and frequent occlusions from wrinkles, folds, and the manipulator’s End‑Effector (EE) and arm. To address these issues, this paper proposes the first Random‑to‑Target Fabric Flattening (RTFF) policy, which aligns a random wrinkled fabric state to an arbitrary wrinkle‑free target state. The proposed policy adopts a hybrid Imitation Learning–Visual Servoing (IL–VS) framework, where IL learns with explicit fabric models for coarse alignment of the wrinkled fabric toward a wrinkle‑free state near the target, and VS ensures fine alignment to the target. Central to this framework is a template‑based mesh that offers precise target state representation, wrinkle‑aware geometry prediction, and consistent vertex correspondence across RTFF manipulation steps, enabling robust manipulation and seamless IL–VS switching. Leveraging the power of mesh, a novel IL solution for RTFF—Mesh Action Chunking Transformer (MACT)—is then proposed by conditioning the mesh information into a Transformer-based policy. The RTFF policy is validated on a real dual‑arm tele‑operation system, showing zero‑shot alignment to different targets, high accuracy, and strong generalization across fabrics and scales. Project website: https://kaitang98.github.io/RTFF_Policy/",
            "introduction": "Robotic manipulation of fabrics is a promising approach to reduce labor and enhance efficiency in garment production.\nA key task is sewing, where the same fabric must be reloaded in different positions to stitch seams such as the shoulder and sides of a T-shirt, and random wrinkles are inherently introduced during sewing, loading and repositioning. [1]\nTo ensure accurate seam placement, fabric needs to be manipulated from such initial random wrinkled states to a wrinkle‑free, flattened target state, whose pose (orientation and translation) varies across processes.\nThis task, referred to as Random-to-Target Fabric\nFlattening (Fig. 1), typically requires iterative selecting grasping points, flattening, adjusting pose, and aligning until the target state is reached [2]. Efficient RTFF is also equally critical in fabric cutting, screen printing, and ironing, where flatness and precise alignment determine garment quality [3, 1].\n\nWhile RTFF is essential in garment production, most classical alignment methods are designed for rigid objects using Visual Servoing, which reduces error between the observed and target states by calculating the inverse Jacobian that maps error changes to robot motions in real time [4].\nHowever, in RTFF the fabric’s deformability yields complex, non-linear deformations and effectively infinite Degrees of Freedom [5], making the task underactuated with limited manipulator Degrees of Freedom, complicating grasping points selection and inverse Jacobian estimation [6].\n\nRecently, Imitation Learning has become popular for fabric manipulation such as flattening [7] and folding [8]. By learning the observation-action mapping directly from expert demonstrations, IL eliminates the need for explicit models of fabric deformability and dynamics. Thus, IL demonstrates promising potential in RTFF, for example, transforming a wrinkled fabric to a wrinkle-free state, enabling coarse alignment, and providing demonstration-informed grasping points to VS.\nHowever, IL faces two key challenges in RTFF:\ni) most existing IL approaches  [9, 10, 11, 8] generate a sequence of actions (referred to as an action chunk), by directly conditioned on RGB or RGB-D images, which limits the ability of IL policies to specify various targets, identify robust wrinkle features from fabric images and to track alignment errors between the fabric current and target state;\nii) achieving both high-precision manipulation and generalization across diverse wrinkled fabric and target states typically requires large, costly datasets of expert demonstrations [10].\n\nConsequently, a natural direction is a hybrid IL–VS policy that leverages the strengths of both paradigms for RTFF: IL is first used to flatten the initially wrinkled fabric and obtain coarse alignment, followed by VS to achieve fine alignment. The primary challenge in realizing this integration is accurate and efficient fabric state estimation, since both paradigms require a consistent geometric representation of the fabric for precise RTFF problem formulation and fabric manipulation and alignment, which is the key bridge to a unified RTFF policy.\nHowever, fabric state estimation is challenging due to fabric’s effectively infinite Degrees of Freedom. Additionally, self-occlusions from wrinkles and folds and external-occlusions from the manipulator’s End-Effector and arm during RTFF, obscure fabric perception and thereby limit state estimation.\n\nState estimation can be achieved with the template mesh method, where a mesh represents fabric as a grid of vertices connected by edges, and a canonical template mesh serves as an undeformed reference that can be deformed into various fabric states, while preserving vertex correspondences [12].\nThis approach offers several benefits:\n(i) the predicted mesh encodes wrinkle information such as location, depth, and shape, enabling wrinkle-aware reasoning, supporting IL in proposing grasping points, and guiding manipulation toward the target;\n(ii) the target state can be precisely specified using template mesh, enabling alignment error tracking—a prerequisite for applying IL in RTFF—and facilitating natural IL–VS switching by computing alignment error magnitude and fabric flatness;\n(iii) the correspondence-preserving template ensures consistent vertex mapping between simulated and real fabric states, as well as across predicted deformation states at successful time steps, thereby enabling direct sim-to-real transfer and accurate deformation tracking; and\n(iv) the predicted mesh accounts for both self- and external occlusions, which is essential for RTFF policy design.\n\nHence, this work proposes a novel RTFF policy to select grasping points and actions that reduce fabric wrinkles, adjust pose, and progressively align the fabric mesh to the target. The contributions of this paper are as follows:\n\nTo the best of the authors’ knowledge, this work is the first to introduce an Random-to-Target Fabric\nFlattening (RTFF) policy by integrating template mesh‑based state estimation with a hybrid IL-VS framework.\n\nThe template-based mesh representation of fabric allows precise target state definitions, wrinkle shape awareness, correspondence‑preserving inputs for robotic fabric manipulation, and seamless switching between IL and VS.\n\nAs a novel Imitation Learning solution, Mesh Action Chunking Transformer (MACT) incorporates template-based mesh fabric representation into a Transformer-based policy, achieving robust and generalized fabric alignment via a reduced set of demonstrations.\n\nThe proposed RTFF policy is validated via physical experiments on a real dual‑arm tele‑operation system, demonstrating zero‑shot alignment, high accuracy, and strong generalization across fabrics.\n\n1. To the best of the authors’ knowledge, this work is the first to introduce an Random-to-Target Fabric\nFlattening (RTFF) policy by integrating template mesh‑based state estimation with a hybrid IL-VS framework.\n\n2. The template-based mesh representation of fabric allows precise target state definitions, wrinkle shape awareness, correspondence‑preserving inputs for robotic fabric manipulation, and seamless switching between IL and VS.\n\n3. As a novel Imitation Learning solution, Mesh Action Chunking Transformer (MACT) incorporates template-based mesh fabric representation into a Transformer-based policy, achieving robust and generalized fabric alignment via a reduced set of demonstrations.\n\n4. The proposed RTFF policy is validated via physical experiments on a real dual‑arm tele‑operation system, demonstrating zero‑shot alignment, high accuracy, and strong generalization across fabrics.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人在服装生产中的面料操作面临复杂的变形和无限自由度问题。  \n2. 现有的对齐方法主要针对刚性物体，难以有效处理面料的非线性变形。  \n3. 现有的模仿学习方法在处理面料对齐时存在数据集需求大和特征提取不足的问题。  \n\n【提出了什么创新的方法】  \n本文提出了一种随机到目标的面料平整策略（RTFF），结合了模板网格的状态估计与混合的模仿学习-视觉伺服框架。该方法通过使用模板网格表示面料，能够精确定义目标状态并跟踪对齐误差，从而实现有效的面料操作。创新的Mesh Action Chunking Transformer (MACT)方法将模板网格信息融入到基于Transformer的策略中，显著提高了面料对齐的鲁棒性和泛化能力。实验结果表明，该策略在真实的双臂遥操作系统上实现了零-shot对齐，高精度和在不同面料及规模上的强泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions",
            "authors": "Thanh Nguyen Canh,Haolan Zhang,Xiem HoangVan,Nak Young Chong",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00783",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00783",
            "arxiv_html_link": "https://arxiv.org/html/2510.00783v1",
            "abstract": "Semantic Simultaneous Localization and Mapping (SLAM) is a critical area of research within robotics and computer vision, focusing on the simultaneous localization of robotic systems and associating semantic information to construct the most accurate and complete comprehensive model of the surrounding environment. Since the first foundational work in Semantic SLAM appeared more than two decades ago, this field has received increasing attention across various scientific communities. Despite its significance, the field lacks comprehensive surveys encompassing recent advances and persistent challenges. In response, this study provides a thorough examination of the state-of-the-art of Semantic SLAM techniques, with the aim of illuminating current trends and key obstacles. Beginning with an in-depth exploration of the evolution of visual SLAM, this study outlines its strengths and unique characteristics, while also critically assessing previous survey literature. Subsequently, a unified problem formulation and evaluation of the modular solution framework is proposed, which divides the problem into discrete stages, including visual localization, semantic feature extraction, mapping, data association, and loop closure optimization. Moreover, this study investigates alternative methodologies such as deep learning and the utilization of large language models, alongside a review of relevant research about contemporary SLAM datasets. Concluding with a discussion on potential future research directions, this study serves as a comprehensive resource for researchers seeking to navigate the complex landscape of Semantic SLAM.",
            "introduction": "Autonomous robotic systems play a vital role in diverse applications such as search and rescue, exploration, augmented reality, and autonomous navigation. These systems must possess a comprehensive understanding of their environment, which entails creating detailed maps, localizing themselves within these maps, and interpreting semantic information about their surroundings. Semantic SLAM addresses these challenges by integrating traditional SLAM capabilities with semantic perception, thereby enabling robots to construct detailed, high-level representations of their environment. Such semantic maps are essential for executing complex tasks with efficiency and accuracy, while balancing computational and memory constraints.\n\nSemantic SLAM has gained increasing attention in the robotics and computer vision communities over the past decades. This interest stems from the growing demand for autonomous systems capable of operating in dynamic, unstructured, and complex environments. The field has expanded significantly, driven by advances in sensor technologies, computational capabilities, and artificial intelligence (AI). While this growth has broadened the scope of research, it has also introduced challenges related to integrating diverse methodologies and ensuring cross-disciplinary collaboration. This survey seeks to address these gaps by providing a unified perspective on Semantic SLAM, analyzing key advancements, and identifying critical research challenges.\n\nSemantic SLAM is currently at a transformative juncture, propelled by novel developments in spatial perception and AI. Breakthroughs in deep learning have facilitated the extraction of high-level semantic features, enabling robots to recognize objects, infer relationships, and interact more intelligently with their surroundings. These advances include neural network models for beyond-line-of-sight prediction, reasoning over dynamic environments, and processing deformable scenes. By leveraging these technologies, Semantic SLAM has the potential to transcend traditional limitations and offer robust solutions for real-world applications.\n\nHowever, the integration of semantic information introduces significant challenges. These include maintaining accuracy in dynamic settings, achieving real-time processing efficiency, and developing robust data fusion algorithms. Moreover, the field lacks standardized benchmarks and reproducible research practices, which are essential for evaluating and comparing different approaches. This survey aims to highlight these challenges, propose potential research directions, and emphasize the importance of interdisciplinary collaboration to advance the state-of-the-art in Semantic SLAM.\n\nThe concept of Simultaneous Localization and Mapping (SLAM) was first introduced by Smith and Cheeseman in the 1980s [1]. In addition, since the advent of mobile robots in the late 1960s, the goal of enabling them to execute tasks autonomously has remained a central theme in robotics research [2]. Since then, SLAM has become a cornerstone technology in robotics, enabling autonomous systems to navigate and interact with their environments. Inspired by human spatial perception—the ability to localize, map, and adapt to unfamiliar settings—robots equipped with SLAM algorithms can perform similar tasks using various sensors, such as cameras, LiDAR, and inertial measurement units (IMUs). This ability can be enhanced with acquired training and plays a crucial role in human cognition and robot control development.\n\nSLAM technology has found applications across diverse domains, including underwater vehicles (UWVs) [3], unmanned aerial vehicles (UAVs) [4], autonomous driving [5], service robots [6], augmented reality (AR) [7], and virtual reality (VR) [8]. These applications underscore the versatility and importance of SLAM in enabling autonomy.\n\nBased on the type of sensors, traditional SLAM systems can be broadly categorized into LiDAR-based SLAM (L-SLAM) and vision-based SLAM (vSLAM). LiDAR sensors excel at precise distance measurements and high-frequency updates, making them ideal for applications requiring low-drift motion estimation [9, 10]. However, LiDAR systems are costly and computationally intensive. In contrast, vSLAM leverages cameras to capture rich visual information, allowing for feature extraction and environmental understanding at a lower cost [11, 12]. Despite its advantages, vSLAM is sensitive to lighting conditions and struggles in textureless or dynamic environments. In recent days, various SLAM technologies have focused on vSLAM due to its low hardware cost, high accuracy in small scenes, and the ability to obtain rich environmental information. On the other hand, there are still many challenges, especially that come with dynamic object movements and environments lacking textures.\n\nTo overcome these limitations, researchers have explored multi-sensor fusion approaches, combining cameras, IMUs, and GPS to improve localization robustness. Recent advancements in vSLAM have emphasized graph-based methodologies [13], where the front end constructs graphs from sensor data and the back end optimizes these graphs to determine the most probable pose configuration.\n\nWhile traditional vSLAM focuses on geometric map construction and localization, it lacks semantic understanding. The integration of semantic information—enabled by advancements in deep learning—has opened new avenues for vSLAM. By extracting semantic features, researchers have improved pose estimation, map accuracy, and environmental understanding [14, 15, 16]. Semantic vSLAM systems enable robots to perform tasks such as semantic localization and mapping, significantly enhancing their adaptability to complex and dynamic environments. The rise of these systems represents a paradigm shift, bridging the gap between geometric and semantic approaches in SLAM. In tandem with its geometric foundation, SLAM has steadily evolved to incorporate higher-level information about the environment—termed semantic information—giving rise to the field of Semantic SLAM.\n\nEarly SLAM solutions largely centered on geometric cues, such as points, lines, and planar structures, using various sensor modalities like cameras or LiDAR for feature extraction [17, 11]. This approach, often referred to as traditional or geometric SLAM, excels when the environment remains static and well-defined. However, real-world settings inherently involve illumination changes, dynamic obstacles, and complex scenes with sparse textures. These conditions can undermine the robustness and accuracy of purely geometric methods, which typically rely on handcrafted features or geometric primitives. Consequently, the research community has increasingly focused on more holistic solutions that integrate vision sensors (e.g., monocular, stereo, RGB-D cameras) and other sensor data, thereby enhancing SLAM’s adaptability and reliability across diverse and challenging operational domains [2, 18, 17].\n\nIn parallel with these sensor-fusion strategies, breakthroughs in deep learning have led to significant leaps in semantic understanding. By leveraging neural networks for object detection, semantic segmentation, and scene recognition, SLAM systems can move beyond constructing raw geometric maps to encoding rich descriptive information about encountered objects and scene layouts [19, 16]. This synergy—combining geometric estimation with semantic cues—paves the way for systems that can better handle moving objects, reinterpret ambiguous features, and enable robust long-term mapping in dynamic or partially known environments [14, 15]. Notably, Semantic SLAM not only locates and maps entities in the scene but also categorizes and attributes them with meaningful labels, thereby supporting higher-level tasks such as scene understanding, path planning, human–robot interaction, and task-oriented manipulation. In instances where the aim is to enhance localization, mapping, or both, these challenges are known as semantic localization, semantic mapping, and Semantic SLAM, respectively.\n\nSemantic Mapping augments traditional SLAM by embedding high-level contextual and categorical information about the environment into the spatial map, enabling a shift from purely geometric representations to cognitively meaningful scene understanding. This concept was first introduced by Dellaert and Bruemmer [19], highlighting the importance of associating semantic entities—such as object classes, scene layouts, and terrain types—with spatial locations. Unlike conventional occupancy grids or point clouds, semantic maps provide symbolic abstractions that facilitate complex reasoning, task planning, and human-robot interaction [20, 21, 22, 4]. Recent advances in deep learning, particularly in semantic segmentation and instance-level object detection, have significantly improved the granularity and accuracy of semantic labels. This has enabled near-real-time semantic map construction, even in dynamic environments, using data from RGB, RGB-D, or multimodal sensors. Techniques such as Bayesian fusion of per-frame segmentation [15] and learned feature embeddings for instance tracking [23] have further enhanced map coherence and consistency over time.\n\nHowever, despite its potential, semantic mapping introduces several open challenges that hinder scalable deployment in real-world robotics. One major concern is the increased computational complexity associated with maintaining and updating semantic labels, especially under limited hardware and power constraints [2, 17, 24]. Moreover, inaccurate or incomplete semantic predictions can introduce drift and inconsistency into the SLAM pipeline, particularly when these labels are used for loop closure or data association. Handling dynamic objects adds further complexity, as semantic information must be continuously validated and updated to prevent map corruption. Another critical limitation lies in the scarcity of standardized benchmarks and evaluation protocols focused specifically on the quality and consistency of semantic maps. This gap restricts fair comparison and reproducibility across approaches [19]. Additionally, memory-efficient representations, such as OctoMap-based semantic trees [25, 26] or voxel hashing [27, 28], must balance rich semantic encoding with storage constraints, especially for long-term autonomy or large-scale environments.\n\nSemantic Localization aims to enhance pose estimation accuracy and robustness by incorporating high-level semantic information such as object classes, spatial relationships, and scene context, moving beyond low-level geometric features. This paradigm enables more reliable localization in perceptually ambiguous or dynamically changing environments, where traditional SLAM approaches based on handcrafted features often degrade. Semantically-aware localization leverages advances in deep neural networks (DNNs) for object detection, semantic segmentation, and scene classification to extract robust, discriminative features that are invariant to viewpoint and appearance changes. Early works such as SuMa++ [29, 30] integrated semantic segmentation into the SLAM pipeline using a semantic ICP framework, demonstrating improved alignment accuracy in cluttered and unstructured environments. Probabilistic approaches have also been explored, where semantic information augments traditional observation models. For instance, extensions of the Markov localization framework [31, 32] incorporated object-level semantics into the sensor likelihood model to improve robustness against perceptual aliasing.\n\nA significant limitation of these early models lies in their deterministic treatment of semantic observations, which disregards the inherent uncertainty in perception systems. To mitigate this, Akai et al. [33] proposed a Bayesian localization approach that integrates supervised object recognition into a probabilistic graphical model. Their framework models classification uncertainty using a Dirichlet distribution, allowing for a more principled fusion of semantic cues. Subsequently, Akai [34] introduced a probabilistic treatment of depth regression errors, improving pose inference in scenes with noisy depth observations.\n\nRecent efforts [35, 36, 37, 38] have focused on leveraging dense semantic maps to enhance global consistency and robustness under appearance changes. These approaches often encode semantic priors into factor graph-based SLAM systems or utilize metric-semantic fusion for long-term localization. Nevertheless, many of these methods still depend on planar scene assumptions and handcrafted features, which limit their generalization to complex, 3D-structured environments. As highlighted in [39], such limitations can result in projection inconsistencies and scale-dependent feature degradation, underscoring the need for learning-based spatial representations capable of modeling geometric and semantic uncertainty jointly.\n\nSemantic SLAM unifies the principles of semantic mapping and semantic localization into a cohesive framework, enabling intelligent robotic systems. By fusing low-level geometric data with high-level semantic cues, Semantic SLAM systems can achieve more accurate and robust localization based on object-level features, generate high-resolution and context-aware maps, and maintain compact and efficient storage representations. These systems exhibit improved robustness to occlusions, lighting variations, and viewpoint changes, while also facilitating semantic-level interaction with dynamic and unstructured environments [40, 41]. Recent works have demonstrated the utility of Semantic SLAM for active exploration [42], object-centric localization [43], and robust relocalization under appearance and structural changes [44]. In general, Semantic SLAM can be decomposed into four primary sub-problems that define its core functionality:\n\nSemantic extraction and mapping: Leveraging semantic segmentation, object detection, and scene parsing to extract meaningful features and incrementally integrate them into a globally consistent map.\n\nSemantic extraction and mapping: Leveraging semantic segmentation, object detection, and scene parsing to extract meaningful features and incrementally integrate them into a globally consistent map.\n\nSemantic data association: Establishing correspondences between semantic entities across time and viewpoints, often using probabilistic models, learned feature descriptors, or graph-based optimization to maintain label consistency.\n\nSemantic uncertainty and representation: Modeling perceptual uncertainty from semantic predictions (e.g., classification confidence, depth variance) using probabilistic frameworks such as Bayesian filters, Dirichlet distributions, or Gaussian Mixtures to ensure reliable integration.\n\nSemantic cost function: Designing loss functions or objective terms that encode semantic priors and constraints—such as object geometry, label consistency, or topology—for use in optimization-based SLAM back-ends.\n\nFinally, Semantic SLAM extends traditional SLAM by simultaneously estimating the 3D geometry of a scene while attaching semantic labels to observed objects and structures. This unified framework integrates both metric and symbolic understanding of the environment, enabling robots not only to map where things are but also what they are. One of the earliest systems to leverage both spatial and semantic representations was proposed by Galindo et al. [45], which introduced a layered cognitive map architecture to incorporate conceptual knowledge into the mapping process. Contemporary Semantic SLAM approaches typically extend the SLAM state vector to include semantic entities or incorporate semantic constraints into the optimization process. Some works introduce novel object-level representations, such as ellipsoids (quadrics)[46], planar landmarks[47], or mesh models [48], allowing compact and expressive 3D scene encoding. Others focus on semantic data association strategies to maintain map consistency under ambiguity and perceptual noise. Probabilistic models based on maximum likelihood estimation [41, 49], max-mixture frameworks [50], and multi-hypothesis k-assignment methods [51] have been developed to handle uncertain object detection and multi-instance association. Approaches can also be categorized based on how semantics are modeled: explicitly through symbolic labels or object poses, or implicitly via learned feature embeddings or volumetric priors. While explicit [52, 53, 4] representations facilitate interpretability and planning, but they often demand increased storage and suffer from brittleness under perceptual uncertainty. Implicit representations, learned through DNNs [54, 55], offer improved robustness but pose challenges for explainability and optimization within SLAM back-ends.\n\nRecently, 3D Gaussian Splatting has emerged as a promising representation for capturing scene geometry and appearance. By modeling the scene as a collection of anisotropic 3D Gaussians, this approach supports continuous view synthesis and efficient rendering while offering potential as a compact SLAM representation. Its integration into SLAM frameworks remains an active area of research, with early efforts exploring its suitability for semantic reconstruction and localization [56, 57, 58, 59]. A detailed comparison of these Semantic SLAM systems is presented in Table LABEL:tab:vslam, summarizing core characteristics such as sensor modality, representation type, semantic modeling technique, back-end architecture, and suitability for dynamic or large-scale environments.\n\nSeveral recent surveys [60, 61, 62, 63, 64, 65, 17, 66, 67, 68, 69, 70] have investigated the progression of visual SLAM, particularly highlighting its evolution from traditional geometric approaches to those that incorporate deep learning and semantic scene understanding. However, the majority of these studies emphasize the importance and the limitations of classical vSLAM, especially in dynamic environments, and the potential of semantic cues to enhance scene interpretation. Few offer a systematic breakdown of Semantic SLAM as a unified framework, and most do not examine its subcomponents in detail, such as semantic data association or loop closure optimization. Table II summarizes the themes covered by each study and contrasts them with the scope of this survey.\n\nThe earliest among them, by Kostavelis and Gasteratos [60], offers a comprehensive survey on “semantic mapping for mobile robots.” This work analyzes the trends and main aspects of semantic mapping and identifies three core research challenges: (1) defining the minimal criteria for a map to be considered semantic, (2) developing semantic mapping evaluation metrics, and (3) integrating semantic maps for knowledge representation. However, this review predominantly focuses on augmenting traditional 2D or 3D maps (e.g., topological maps) with semantic labels and does not address components like semantic localization, data association, or back-end optimization. More recent works [64, 65, 67] acknowledge the limitations of geometric SLAM and advocate for deep learning as a data-driven paradigm for sensor interpretation and scene understanding. These surveys trace the chronological evolution of SLAM technologies and paradigms but stop short of delving into the architecture or mathematical modeling of Semantic SLAM systems. Other surveys [62, 71, 70] approach semantic mapping as a strategy to improve perceptual awareness in autonomous systems. These papers typically divide the semantic mapping pipeline into three stages: data acquisition, semantic and spatial fusion, and symbolic knowledge representation. However, they generally treat mapping and perception as decoupled modules rather than parts of an integrated SLAM framework.\n\nSualeh et al.[61] introduces the fundamentals of SLAM and provides a high-level overview of Semantic SLAM. Their work is introductory and does not explore a formalized model or detailed submodules. Similarly, Pu et al.[66] highlight the challenges of vSLAM in dynamic environments, reviewing how deep learning improves the front-end (e.g., dynamic object segmentation, feature robustness under varying illumination) and the back-end (e.g., place recognition and loop detection). However, their work lacks a unified mathematical formulation of Semantic SLAM and does not cover semantic data association or map optimization in depth. Wen et al.[63] study the influence of dynamic objects on visual SLAM and leverage Mask R-CNN to segment dynamic regions and initialize camera poses. Their approach combines photometric, geometric, and depth consistency to differentiate between static and dynamic features. Meanwhile, Chen et al.[17] surveys semantic object association methods—both probabilistic and non-probabilistic—and briefly mentions semantic localization in indoor and outdoor environments, but only at a high level. Most recently, Wang et al. [68] categorize Semantic SLAM in dynamic scenes into four key areas: (1) dynamic object exclusion, (2) semantic tracking for localization, (3) semantic mapping under dynamic conditions, and (4) multi-sensor fusion. While this survey emphasizes the challenges of real-world deployment, it does not offer a unified treatment of Semantic SLAM nor a detailed breakdown of how semantics are integrated into specific SLAM components such as data association, loop closure, or cost modeling.\n\nIn summary, existing surveys underscore the significance of integrating semantics into SLAM and outline historical trends and potential applications. However, they fall short of presenting Semantic SLAM as a unified probabilistic framework or exploring the four critical components—semantic extraction and mapping, semantic data association, semantic uncertainty modeling, and semantic-aware cost functions—in a structured manner. Therefore, this paper aims to fill this gap by formulating Semantic SLAM probabilistically and providing an in-depth discussion of its modular architecture and open challenges.\n\nThe remainder of this paper is structured as follows. Section II introduces the Semantic SLAM problem, beginning with a unified probabilistic formulation and followed by the decomposition into key subproblems. Section III discusses semantic feature extraction from raw sensor data, including object detection, segmentation, and scene understanding. Section IV focuses on semantic localization methods that leverage object-level and high-level contextual cues for robust pose estimation. Section V covers semantic mapping techniques, including map fusion strategies and handling dynamic environments. Section VI explores semantic data association and\n\nintegration, emphasizing probabilistic models and multi-instance tracking. Section VII addresses loop closure detection and global optimization incorporating semantic constraints. Section VIII presents learning-based approaches, including Deep Learning and foundation models. Section IX reviews recent advances in continuous and implicit scene representations, such as neural fields and 3D Gaussian splatting. Section X extends the discussion to multi-robot Semantic SLAM, focusing on collaborative semantic mapping and distributed optimization. Section XI outlines open research challenges, including lifelong learning, zero-shot generalization, and standardization of semantic benchmarks. Finally, Section XII concludes the paper with a summary of key insights and future directions for the field.",
            "llm_summary": "大模型总结失败",
            "llm_score": 0,
            "llm_error": "API 状态码异常：403，响应：{\"error\":{\"message\":\"免费API限制模型输入token小于4096，如有更多需求，请访问 https://api.chatanywhere.tech/#/shop 购买付费API。The number of prompt tokens for free accounts is limited to 4096. If you have additional requirements, please visit https://api.chatanywhere.tech/#/shop to purchase a premium key.(当前请求使用的ApiKey: sk-8l9****i4zt)【如果您遇到问题，欢迎加入QQ群咨询：1048463714】\",\"type\":\"chatanywhere_error\",\"param\":null,\"code\":\"403 FORBIDDEN\"}}"
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Tele-rehabilitation with online skill transfer and adaptation in $\\mathbb{R}^3 \\times \\mathit{S}^3$",
            "authors": "Tianle Ni,Xiao Chen,Hamid Sadeghian,Sami Haddadin",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00770",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00770",
            "arxiv_html_link": "https://arxiv.org/html/2510.00770v1",
            "abstract": "This paper proposes a tele-teaching framework for the domain of robot-assisted tele-rehabilitation. The system connects two robotic manipulators on therapist and patient side via bilateral teleoperation, enabling a therapist to remotely demonstrate rehabilitation exercises that are executed by the patient-side robot. A 6-DoF Dynamical Movement Primitives formulation is employed to jointly encode translational and rotational motions in ℝ3×S3\\mathbb{R}^{3}\\times\\mathit{S}^{3} space, ensuring accurate trajectory reproduction. The framework supports smooth transitions between therapist-led guidance and patient passive training, while allowing adaptive adjustment of motion. Experiments with 7-DoF manipulators demonstrate the feasibility of the approach, highlighting its potential for personalized and remotely supervised rehabilitation.",
            "introduction": "The rapid growth of the aging population is accompanied by a sharp rise in the number of stroke survivors and patients with other neurological injuries, such as traumatic brain injury and spinal cord injury. These conditions often leave individuals with long-term motor impairments that limit their ability to perform activities of daily living, highlighting the need for innovative therapeutic interventions to restore lost or impaired motor function. Effective rehabilitation typically requires intensive, repetitive, and prolonged training sessions. However, healthcare systems are increasingly constrained by a shortage of professional therapists, creating a significant imbalance between the rising demand for rehabilitation services and the limited clinical resources available. This mismatch restricts the frequency and intensity of therapy that patients can receive, even though both are critical for functional recovery.\n\nRobot-assisted rehabilitation has emerged as a promising approach to bridge this gap by delivering consistent, repeatable, and quantifiable training while alleviating the burden on therapists [1, 2]. Robots are capable of providing personalized assistance [3] tailored to the patient’s motor intent and performance, thereby playing a crucial role in promoting neuromuscular recovery [4]. Moreover, research shows that rehabilitation outcomes improve when patients are actively engaged in training with minimal external support, as active participation enhances neural plasticity [1]. To encourage this, active rehabilitation strategies have been developed, enabling robotic systems to deliver only the minimum level of support required for successful task execution[1, 5, 6, 7].\n\nDespite these advances, most robotic rehabilitation platforms[8, 9, 10, 11] remain confined to in-clinic use and lack mechanisms for remote monitoring or supervision. This limitation reduces accessibility, particularly for patients who would benefit from continued therapy at home, underscoring the growing importance of tele-rehabilitation solutions. To address this challenge, a tele-rehabilitation system for the upper extremity using a humanoid robot was proposed in [12]. The framework consists of two robotic stations, located at the patient and therapist sides, and employs optimal control and game theory [13], modeling the patient, therapist, and robot as three agents sharing a common cost function. The system is capable of delivering passive, active, and assist-as-needed rehabilitation through dynamic adaptation of the agents’ roles.\n\nA therapist-in-the-loop, robot-assisted mirror rehabilitation approach was introduced in [14], enabling a remote therapist to actively participate in the rehabilitation process. Unlike in clinical settings, where the therapist has direct access to both the patient and the robot, remote supervision imposes significant technical challenges. A transparent robotic interface is required at the therapist’s side to provide high-fidelity interaction with the patient-robot system. Additionally, a robust communication infrastructure is essential to ensure reliable transfer of sensory and audio-visual data. Network issues such as time delays and packet loss can adversely affect both the safety and performance of teleoperation, motivating ongoing research into advanced compensation techniques [15, 16].\n\nIn this paper, we propose a tele-rehabilitation system that enables remote therapeutic guidance and patient training. The system consists of two synchronized robotic manipulators connected through bilateral teleoperation channels, as illustrated in Figure 1. To support online and remote rehabilitation, the teleoperation framework is combined with kinesthetic teaching, allowing a therapist to guide exercises in ℝ3×S3\\mathbb{R}^{3}\\times\\mathit{S}^{3} space in real time. The therapist operates the therapist-side robot—while the patient interacts with it through their own robot—to demonstrate periodic rehabilitation motions. During these demonstrations, the therapist directly guides the patient-side robot through teleoperation, ensuring that the patient follows the prescribed therapeutic motions. The therapist can flexibly adjust motion parameters—including speed, scale, and trajectory shape—based on the patient’s feedback. Once the learning is finished, autonomy is gradually shifted to the patient-side robot so that the impedance controller on this side tracks the reproduced trajectory. The therapist can intervene at any time to introduce new rehabilitation motion profiles in ℝ3×S3\\mathbb{R}^{3}\\times\\mathit{S}^{3} space, which are encoded and adapted online. This design enables seamless transitions between therapist-led guidance and patient passive training.\n\nThe main contributions are as follows:\n\nA tele-teaching framework that enables remote therapeutic guidance and patient training with periodic rehabilitation motions in ℝ3×S3\\mathbb{R}^{3}\\times\\mathit{S}^{3} space.\n\nA dynamic leader-follower bilateral teleoperation control architecture.\n\nA novel autonomy allocation mechanism based on operator intention and skill learning convergence enables intuitive and seamless transitions between therapist-led guidance and patient passive training.\n\n1. A tele-teaching framework that enables remote therapeutic guidance and patient training with periodic rehabilitation motions in ℝ3×S3\\mathbb{R}^{3}\\times\\mathit{S}^{3} space.\n\n2. A dynamic leader-follower bilateral teleoperation control architecture.\n\n3. A novel autonomy allocation mechanism based on operator intention and skill learning convergence enables intuitive and seamless transitions between therapist-led guidance and patient passive training.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要创新的治疗干预以恢复因中风和神经损伤导致的运动功能丧失。  \n2. 由于专业治疗师短缺，患者无法获得足够频率和强度的康复治疗。  \n3. 现有的机器人康复平台缺乏远程监控或监督机制，限制了患者在家中继续治疗的可能性。  \n\n【提出了什么创新的方法】  \n本研究提出了一种远程治疗指导和患者训练的远程教学框架，结合了双向遥操作和运动学习，允许治疗师实时在ℝ3×S3空间中指导康复运动。该系统通过动态领导-跟随的遥操作控制架构，实现了治疗师与患者之间的无缝互动，并根据患者反馈灵活调整运动参数。创新的自主性分配机制使得治疗师主导的指导与患者被动训练之间的过渡变得直观且顺畅。实验结果表明，该方法在个性化和远程监督的康复中具有良好的可行性，能够有效提升患者的参与度和康复效果。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation",
            "authors": "Giovanni Minelli,Giulio Turrisi,Victor Barasuol,Claudio Semini",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "Code and data available atthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.00726",
            "code": "https://github.com/iit-DLSLab/croSTAta",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00726",
            "arxiv_html_link": "https://arxiv.org/html/2510.00726v1",
            "abstract": "Learning robotic manipulation policies through supervised learning from\ndemonstrations remains challenging when policies encounter execution\nvariations not explicitly covered during training. While incorporating\nhistorical context through attention mechanisms can improve robustness,\nstandard approaches process all past states in a sequence without\nexplicitly modeling the temporal structure that demonstrations may include,\nsuch as failure and recovery patterns. We propose a Cross-State\nTransition Attention Transformer\nthat employs a novel State Transition Attention (STA) mechanism to modulate\nstandard attention weights based on learned state evolution patterns,\nenabling policies to better adapt their behavior based on execution history.\nOur approach combines this structured attention with temporal masking\nduring training, where visual information is randomly removed from recent\ntimesteps to encourage temporal reasoning from historical context.\nEvaluation in simulation shows that STA consistently outperforms standard\ncross-attention and temporal modeling approaches like TCN and LSTM networks\nacross all tasks, achieving more than 2× improvement over cross-attention on precision-critical tasks.\nThe source code and data can be accessed at the following link: https://github.com/iit-DLSLab/croSTAta",
            "introduction": "Imitation learning (IL) has emerged as a promising paradigm for training\nrobotic policies by leveraging expert demonstrations rather than learning\na policy from scratch through extensive interaction with the environment\n[1]. The appeal of IL lies in its data efficiency\nand ability to leverage human expertise, making it particularly attractive\nfor complex manipulation tasks [2, 3, 4, 5, 6].\nHowever, a fundamental limitation of IL approaches lies in the inherent\ndependence on the statistical distribution of training data, leading to\nbrittle policies that struggle to handle situations not explicitly observed\nduring training [1, 7, 8].\nThis becomes even more relevant when deploying these models in unstructured and\nreal-world scenarios where environmental conditions, object properties, or\nexecution dynamics may differ from those observed in demonstrations\n[7, 9, 10].\n\nTo address this distributional shift problem, recent work has explored\nusing suboptimal or noisy demonstration data, showing that sufficient\ndiversity can sometimes outperform expert-only training, especially for\nlong-horizon tasks [11, 12]. This has motivated\ndata augmentation approaches and automated demonstration generation systems\n[4, 13, 14, 15, 16, 17, 18]\nas methods to include diversity and failure/recovery trajectories in training data,\nthereby providing explicit examples of mistakes and corrections.\n\nHowever, while enriching demonstrations with failure and recovery examples\nprovides a valuable training signal, simply adding more demonstrations is\nnot a scalable solution. In general, it would be extremely difficult to\ncollect examples for every possible failure scenario. This fundamental\nlimitation highlights the need for approaches that can better leverage\nthe underlying causal dependencies present in the data beyond straightforward\nsequence imitation. These dependencies span multiple aspects: logical\ndependencies where low-level actions depend on high-level plans; spatial\ndependencies where end-effector orientation depends on position; and\ncrucially, temporal dependencies where future actions depend\non past execution history. Alternative approaches based on planning\n[19] and hierarchical [20] policy\narchitectures address some of these challenges with promising results,\nbut the fundamental question of how to extract dependency concepts\nfrom data and effectively model them in a policy remains central to\nachieving robust and adaptive behavior.\n\nThe temporal modeling challenge is particularly relevant because many\nrobotic tasks are inherently non-Markovian: appropriate action selection\noften depends not only on current observations, but also on past observations\nand actions [5, 21, 22].\nFor example, manipulation scenarios where the robotic arm occludes critical\nscene information [23], or multi-stage tasks where early\nexecution steps inform later strategies [3, 24].\nIn these cases, information from past actions and decisions – such as speed,\ntrajectory curvature, or overall strategy – fundamentally determines the\nexecution of future actions. Despite the importance of historical context,\nlearning long-context robotic policies through imitation learning remains\nchallenging due to spurious correlations in extended observation histories\n[25].\n\nCurrent sequence modeling approaches in robotics predominantly treat all\ntemporal elements equally, learning relationships between past and present\nstates primarily through the statistical co-occurrence of elements in\ndemonstrated trajectories [26]. While this approach has\nshown success in various domains, it may not optimally exploit the structured\ntemporal dependencies in rich demonstrations, where specific past states\ndirectly inform corrective actions; thus, more targeted attention mechanisms\ncould better capture these state transition relationships.\n\nWe propose a state transition attention mechanism that shifts\nattention-based temporal processing to focus specifically on how the past\ninforms current action selection. Rather than extracting information from\npast timesteps and learning how to weight attention across the temporal\ndimension, our approach directly learns to act based on state transition\npatterns. This allows policies to leverage historical context by matching\ncurrent situations to learned temporal patterns during action selection.\nWe evaluate our approach against standard attention mechanism for temporal\nmodeling and demonstrate its particular effectiveness in learning from\nrecovery-rich demonstrations. Moreover, through analysis and ablations of\nthe proposed method, we provide insights into how historical information\nis retrieved during execution phases, demonstrating that our structured\nattention mechanisms designed for state transition modeling can significantly\nenhance policy robustness in sequential decision-making.\n\nThe main contributions of the paper are:\n\nState Transition Attention (STA), a novel attention mechanism designed\nto capture state transition patterns in robotic manipulation that, to the best\nof our knowledge, is the first approach to explicitly modulate attention\nweights based on learned state evolution patterns for temporal reasoning\nin manipulation policies;\n\nComprehensive empirical evaluation and ablation studies across four\nmanipulation tasks demonstrating that STA achieves consistent improvements\nover the standard cross-attention approach (up to 2× on precision-critical tasks)\nand outperforms traditional temporal modeling methods including TCN and LSTM\nnetworks. Our analysis highlights the method’s capability to differentiate\nbetween different types of historical context during policy execution.\n\n1. State Transition Attention (STA), a novel attention mechanism designed\nto capture state transition patterns in robotic manipulation that, to the best\nof our knowledge, is the first approach to explicitly modulate attention\nweights based on learned state evolution patterns for temporal reasoning\nin manipulation policies;\n\n2. Comprehensive empirical evaluation and ablation studies across four\nmanipulation tasks demonstrating that STA achieves consistent improvements\nover the standard cross-attention approach (up to 2× on precision-critical tasks)\nand outperforms traditional temporal modeling methods including TCN and LSTM\nnetworks. Our analysis highlights the method’s capability to differentiate\nbetween different types of historical context during policy execution.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的模仿学习方法在执行过程中遇到未覆盖的变异时表现脆弱。  \n2. 需要更好地利用数据中的因果依赖关系，以提高策略的鲁棒性和适应性。  \n3. 传统的序列建模方法未能有效捕捉丰富演示中的结构化时间依赖性。  \n\n【提出了什么创新的方法】  \n提出了一种新的状态转移注意力机制（State Transition Attention, STA），旨在通过学习状态演变模式来调节注意力权重，从而增强机器人操作策略的时间推理能力。该方法结合了结构化注意力和时间掩蔽训练，鼓励模型从历史上下文中进行时间推理。实验结果表明，STA在四个操作任务中均优于标准的交叉注意力和传统的时间建模方法，如TCN和LSTM网络，在精度关键任务上实现了超过2倍的改进。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "MultiPhysio-HRC: Multimodal Physiological Signals Dataset for industrial Human-Robot Collaboration",
            "authors": "Andrea Bussolan,Stefano Baraldo,Oliver Avram,Pablo Urcola,Luis Montesano,Luca Maria Gambardella,Anna Valente",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00703",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00703",
            "arxiv_html_link": "https://arxiv.org/html/2510.00703v1",
            "abstract": "Human-robot collaboration (HRC) is a key focus of Industry 5.0, aiming to enhance worker productivity while ensuring well-being. The ability to perceive human psycho-physical states, such as stress and cognitive load, is crucial for adaptive and human-aware robotics. This paper introduces MultiPhysio-HRC, a multimodal dataset containing physiological, audio, and facial data collected during real-world HRC scenarios. The dataset includes electroencephalography (EEG), electrocardiography (ECG), electrodermal activity (EDA), respiration (RESP), electromyography (EMG), voice recordings, and facial action units. The dataset integrates controlled cognitive tasks, immersive virtual reality experiences, and industrial disassembly activities performed manually and with robotic assistance, to capture a holistic view of the participants’ mental states. Rich ground truth annotations were obtained using validated psychological self-assessment questionnaires. Baseline models were evaluated for stress and cognitive load classification, demonstrating the dataset’s potential for affective computing and human-aware robotics research. MultiPhysio-HRC is publicly available to support research in human-centered automation, workplace well-being, and intelligent robotic systems.",
            "introduction": "In the field of Human-Robot Collaboration (HRC), physiological signals are raising high interest thanks to their potential to capture human states such as stress, cognitive load, and fatigue [1]. In the human-centric view promoted by Industry 5.0, industrial workplaces should aim at striking a balance between worker productivity and well-being [2]. This includes conceiving robotic systems that can not only perform physical tasks in support of human workers but also change their behavior depending on the psycho-physical state of operators, coupled with context information. This approach of deliberative robotics [3] cannot unleash its full potential unless the human psycho-physical state can be perceived by the robot. This idea is the core goal of the Fluently project, which aims to enhance human-robot collaboration by enabling robots to adapt their behavior based on the psycho-physical state of human operators.\n\nTo develop robotic systems capable of adapting to human states, it is essential to build machine learning models that can reliably infer the mental state from physiological and behavioral signals. However, training such models requires datasets that not only include a diverse range of conditions but also reflect real-world industrial settings. Many existing datasets focus on a limited subset of modalities and are rarely collected outside of controlled laboratory conditions, limiting their applicability to HRC scenarios where multiple factors influence human states simultaneously.\n\nIn this paper, we present MultiPhysio-HRC, a dataset containing facial features, audio, and physiological signals - electrocardiogram (ECG), electrodermal activity (EDA), respiration (RESP), electromyography (EMG), and Electroencephalography (EEG). To the best of our knowledge, MultiPhysio-HRC is the first dataset to include this wide combination of data obtained during real-world human-robot collaboration, various psychological tests, and VR-based activities, designed to elicit multiple psychological states. Furthermore, the ground truth labels collected for this dataset enable the analysis of various aspects of the human mental state, including stress levels, cognitive load, and emotional dimensions. The dataset is publicly available at https://automation-robotics-machines.github.io/MultiPhysio-HRC.github.io/.\n\nWe summarize our main contributions as follows:\n\nReal-World HRC Context - To the best of our knowledge, MultiPhysio-HRC is the first publicly available dataset to include real-world industrial-like HRC scenarios comprehensively.\n\nReal-World HRC Context - To the best of our knowledge, MultiPhysio-HRC is the first publicly available dataset to include real-world industrial-like HRC scenarios comprehensively.\n\nComplete Multimodal Data - While existing datasets often include subsets of modalities, MultiPhysio-HRC integrates facial features, audio, and a comprehensive set of physiological signals: EEG, ECG, EDA, RESP, and EMG. This combination allows for a holistic assessment of mental states, addressing cognitive load, stress, and emotional dimensions.\n\nTask Diversification - The dataset comprises tasks specifically designed to elicit various mental states. These include cognitive tests, immersive VR activities, and industrial tasks.\n\nRich Ground Truth Annotations - Ground truth labels were collected through validated psychological questionnaires at multiple stages during the experiment. Combined with multimodal measurements, these labels offer unparalleled granularity for studying human states in HRC contexts.\n\nThe remainder of this paper is organized as follows: Section II presents the related dataset with similar modalities combination; Section III explains the experimental protocol for data collection, describing tasks and data; in Section IV the processing pipelines for filtering and feature extraction are detailed; while Section V presents and discusses the results achieved using traditional models. In the end, Section VI concludes the work by presenting final remarks and future directions.\n\n1. Real-World HRC Context - To the best of our knowledge, MultiPhysio-HRC is the first publicly available dataset to include real-world industrial-like HRC scenarios comprehensively.\n\n2. Complete Multimodal Data - While existing datasets often include subsets of modalities, MultiPhysio-HRC integrates facial features, audio, and a comprehensive set of physiological signals: EEG, ECG, EDA, RESP, and EMG. This combination allows for a holistic assessment of mental states, addressing cognitive load, stress, and emotional dimensions.\n\n3. Task Diversification - The dataset comprises tasks specifically designed to elicit various mental states. These include cognitive tests, immersive VR activities, and industrial tasks.\n\n4. Rich Ground Truth Annotations - Ground truth labels were collected through validated psychological questionnaires at multiple stages during the experiment. Combined with multimodal measurements, these labels offer unparalleled granularity for studying human states in HRC contexts.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要理解人类的心理生理状态以提升人机协作的适应性。  \n2. 现有数据集缺乏多模态和真实工业环境下的应用。  \n3. 需要丰富的标注数据来训练机器学习模型以推断人类心理状态。  \n\n【提出了什么创新的方法】  \n本研究提出了MultiPhysio-HRC数据集，包含在真实工业人机协作场景中收集的多模态生理信号、音频和面部数据。数据集整合了EEG、ECG、EDA、RESP和EMG等信号，涵盖了认知测试、沉浸式虚拟现实活动和工业任务，旨在全面捕捉参与者的心理状态。通过使用经过验证的心理自评问卷收集的丰富标注，数据集为研究人机协作中的人类状态提供了前所未有的细致度。基线模型的评估结果表明，该数据集在情感计算和人性化机器人研究中具有重要潜力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy",
            "authors": "Myungkyu Koo,Daewon Choi,Taeyoung Kim,Kyungmin Lee,Changyeon Kim,Youngyo Seo,Jinwoo Shin",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.00695",
            "code": "https://myungkyukoo.github.io/hamlet/",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00695",
            "arxiv_html_link": "https://arxiv.org/html/2510.00695v1",
            "abstract": "Inherently, robotic manipulation tasks are history-dependent: leveraging past context could be beneficial.\nHowever, most existing Vision-Language-Action models (VLAs) have been designed without considering this aspect, i.e., they rely solely on the current observation, ignoring preceding context.\nIn this paper, we propose HAMLET, a scalable framework to adapt VLAs to attend to the historical context during action prediction.\nSpecifically, we introduce moment tokens that compactly encode perceptual information at each timestep.\nTheir representations are initialized with time-contrastive learning, allowing them to better capture temporally distinctive aspects.\nNext, we employ a lightweight memory module that integrates the moment tokens across past timesteps into memory features, which are then leveraged for action prediction.\nThrough empirical evaluation, we show that HAMLET successfully transforms a state-of-the-art VLA into a history-aware policy, especially demonstrating significant improvements on long-horizon tasks that require historical context.\nIn particular, on top of GR00T N1.5, HAMLET achieves an average success rate of 76.4% on history-dependent real-world tasks, surpassing the baseline performance by 47.2%.\nFurthermore, HAMLET pushes prior art performance from 64.1% to 66.4% on RoboCasa Kitchen (100-demo setup) and from 95.6% to 97.7% on LIBERO, highlighting its effectiveness even under generic robot-manipulation benchmarks.\nProject page is available.",
            "introduction": "Vision-Language-Action models (VLAs; Zitkovich et al. 2023; Kim et al. 2024; Black et al. 2025; Pertsch et al. 2025; Li et al. 2024a; Qu et al. 2025; Bjorck et al. 2025b) have shown their promise in robotic policy learning by leveraging large-scale pre-trained Vision-Language Models (VLMs;  Beyer et al. 2024; Chen et al. 2023; Driess et al. 2023; Karamcheti et al. 2024; Touvron et al. 2023) with diverse robot-specific datasets (Walke et al., 2023; O’Neill et al., 2024; Khazatsky et al., 2024).\nThey typically adopt a single-frame assumption, predicting each action solely from the current observation.\nHowever, such reliance on a current snapshot fundamentally limits their capability, since robotic manipulation tasks are intrinsically history-dependent.\nFor instance, consider a simple scenario of placing an object on a table.\nThe decision to move the arm depends on prior context—specifically, whether the object has already been grasped.\nWhen restricted to the current frame, the policy may struggle to determine the proper next action, particularly if the object is occluded.\n\nDespite being a desirable property, incorporating history-awareness during pre-training is viewed as a costly design choice.\nA major challenge is that leveraging historical context incurs substantial computational overhead.\nFor example, we observe that naïvely appending only four additional past observation frames to the VLA input slows down the forward pass by ∼\\sim35% and increases peak memory consumption by ∼\\sim3.6×\\times (see Multi-frame in Table 4).\nIn particular, the inflated memory footprint further restricts scalability by reducing feasible batch sizes compared to the single-frame setting.\nTogether, these observations raise a key research question: How can we integrate history-awareness into pre-trained VLAs without resorting to costly pre-training from scratch?\n\nTo tackle this problem, we propose HAMLET, a fine-tuning framework for VLAs that introduces History-Aware Memory with LEarned Tokens.\nOur framework consists of two components: (a) moment tokens, which summarize the instantaneous VLM representations at each timestep, and (b) a memory module, which consolidates moment tokens across different timesteps to produce a temporally informed condition for action prediction.\nThe moment tokens are appended to the VLM input at each timestep and initialized with time-contrastive learning (Sermanet et al., 2018), which encourages distinctiveness across timesteps.\nThis initialization enables the moment tokens to emphasize task-relevant dynamics while suppressing redundant information such as static backgrounds (see Figure 4 for details).\nBuilding on this, we incorporate a lightweight memory module that stores and integrates moment token representations across timesteps. This design is motivated by the observation that not all moments are equally informative; treating every timestep with equal importance can introduce redundancy and obscure critical cues (see Moment Concat. in Table 5).\n\nTo validate the effectiveness and generality of HAMLET, we conduct comprehensive experiments across both real-world and simulation environments.\nWe first evaluate HAMLET on the long-horizon, real-world tasks that require reasoning over past trajectories.\nWe show that HAMLET improves performance by 47.2% over the naïvely fine-tuned VLA, which demonstrates the effectiveness of exploiting historical information for real-world robot policy learning.\nWe further examine the generality and applicability of HAMLET across different VLA backbones.\nWhen fine-tuning GR00T N1.5 (Bjorck et al., 2025a) on the RoboCasa (Nasiriany et al., 2024) Kitchen dataset, HAMLET achieves an average success rate of 66.4%, compared to 64.1% for the baseline.\nSimilarly, when applied to CogACT (Li et al., 2024a) on the SimplerEnv-Bridge (Li et al., 2024b) dataset, HAMLET attains 63.5%, substantially improving over the baseline performance of 52.1%.\nThese results highlight that incorporating history-awareness consistently yields benefits across diverse VLA policies, and that HAMLET provides consistent improvements in a flexible, plug-in manner.\n\nContributions. Our contributions are as follows:\n\nMotivated by VLAs’ reliance on the current observation alone, we propose HAMLET, a plug-and-play framework that integrates history-awareness into pre-trained VLAs.\n\nWe introduce moment tokens, initialized with time-contrastive learning, to capture key temporal cues at each timestep. Building on this, we design a lightweight memory module that selectively aggregates these tokens across timesteps to produce history-aware features for action prediction.\n\nWe validate HAMLET across both real-world and simulation benchmarks, achieving substantial gains over state-of-the-art baselines. By alleviating backbone models’ reliance on the current observation, HAMLET delivers consistent improvements, especially with the strongest benefits on long-horizon tasks. Importantly, its backbone-agnostic design allows seamless and efficient integration into diverse VLAs without requiring any additional large-scale pre-training.\n\n1. Motivated by VLAs’ reliance on the current observation alone, we propose HAMLET, a plug-and-play framework that integrates history-awareness into pre-trained VLAs.\n\n2. We introduce moment tokens, initialized with time-contrastive learning, to capture key temporal cues at each timestep. Building on this, we design a lightweight memory module that selectively aggregates these tokens across timesteps to produce history-aware features for action prediction.\n\n3. We validate HAMLET across both real-world and simulation benchmarks, achieving substantial gains over state-of-the-art baselines. By alleviating backbone models’ reliance on the current observation, HAMLET delivers consistent improvements, especially with the strongest benefits on long-horizon tasks. Importantly, its backbone-agnostic design allows seamless and efficient integration into diverse VLAs without requiring any additional large-scale pre-training.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的Vision-Language-Action模型仅依赖当前观察，忽视历史上下文，限制了其在机器人操作任务中的表现。  \n2. 机器人操作任务本质上是历史依赖的，利用过去的上下文信息可以显著改善任务执行效果。  \n\n【提出了什么创新的方法】  \n提出了HAMLET，一个可扩展的框架，通过引入moment tokens和轻量级记忆模块，使得Vision-Language-Action模型能够关注历史上下文。moment tokens在每个时间步紧凑编码感知信息，并通过时间对比学习初始化，以捕捉时间上独特的特征。记忆模块整合过去时间步的moment tokens，生成用于动作预测的历史感知特征。通过实证评估，HAMLET在需要历史上下文的长时间任务上表现出显著的改进，成功将GR00T N1.5的成功率提升至76.4%，超越基线性能47.2%。此外，HAMLET在多个基准测试中均展现了其有效性，证明了历史感知的整合能够在多样化的VLA政策中带来一致的性能提升。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Shared Object Manipulation with a Team of Collaborative Quadrupeds",
            "authors": "Shengzhi Wang,Niels Dehio,Xuanqi Zeng,Xian Yang,Lingwei Zhang,Yun-Hui Liu,K. W. Samuel Au",
            "subjects": "Robotics (cs.RO); Multiagent Systems (cs.MA); Systems and Control (eess.SY)",
            "comment": "submitted to The 2026 American Control Conference",
            "pdf_link": "https://arxiv.org/pdf/2510.00682",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00682",
            "arxiv_html_link": "https://arxiv.org/html/2510.00682v1",
            "abstract": "Utilizing teams of multiple robots is advantageous for handling bulky objects.\nMany related works focus on multi-manipulator systems,\nwhich are limited by workspace constraints.\nIn this paper, we extend a classical hybrid motion-force controller\nto a team of legged manipulator systems,\nenabling collaborative loco-manipulation of rigid objects with a force-closed grasp.\nOur novel approach allows the robots to flexibly coordinate their movements,\nachieving efficient and stable object co-manipulation and transport,\nvalidated through extensive simulations and real-world experiments.",
            "introduction": "Controlling a team of collaborative robots has become a hot research topic,\nas these teams enable the handling of large and bulky objects that would be impossible for a single robot.\nScenarios are often inspired by applications in logistics, where boxes need to be grabbed [1], rotated, transported, tossed [2]\nand/or caught in flight [3, 4].\n\nRobot teams aiming at shared object manipulation and transportation\nmay be composed of fixed-base manipulators, i.e.,\ndual-arm [5, 6, 7]\nand multi-arm systems [8, 9, 10],\nhowever, these systems have a limited workspace.\nTo address this limitation,\nmultiple floating-base robots equipped with manipulators have been combined,\nsuch as mobile manipulators [11, 12],\nhumanoids [13, 14],\ndrones [15]\nand underwater vehicles [16].\n\nTo our best knowledge,\nthere has been little exploration of using a team of legged manipulators for joint object manipulation requiring a force-closed grasp,\nsuch as transporting objects (see Fig. 1).\nWe address this gap with a novel torque-control scheme for collaborative loco-manipulation of rigid, bulky objects with multiple legged manipulators.\nWe formulate a Cartesian impedance for the object and simultaneously resolve the remaining redundancy in the robot team.\nOur simulations and real-world experiments demonstrate efficient and stable joint co-manipulation and transport of an object.\n\nShared control of an object is challenging due to the physical coupling and exchange of contact forces, necessitating both a force-closed grasp and simultaneous motion control.\nWithin this work, we are particularly interested in a hybrid motion-force control concept called\nProjected Inverse Dynamics Control (PIDC),\noriginally introduced in [17]\nfor a single fully actuated stationary robot.\nExtensions to the underactuated case\nwere presented in [20, 19] based on numeric optimization methods.\nIn contrast, [23, 21, 22] found a closed-form solution to that problem,\nwith [23] generalizing [21, 22] by explicitly also controlling contact forces.\nConsidering unilateral contacts, additional friction cone constraints must be considered.\nThese constraints, expressed as inequalities, can be explicitly handled using quadratic programming to minimize power consumption, as demonstrated for manipulators in [18].\n\nIn this paper, we formulate a coherent control scheme that combines\nthe closed-form PIDC approach for underactuated robots [23],\nwith a contact force optimization for fully actuated robots [18].\nWith appropriate substitutions,\nour generic formulation simplifies to [23] or [18], also indicated in table I.\nHence,\nour main contribution is a low-level hybrid motion-force control scheme for torque-controlled underactuated multi-robot systems,\nenabling collaborative loco-manipulation tasks.\nFor the first time, we demonstrate a force-closed grasp with a team of legged manipulators.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有多操纵器系统在工作空间上受到限制。  \n2. 需要有效的控制方案来实现多腿机器人团队的协同操作。  \n3. 缺乏对腿式操纵器团队进行力闭合抓取的研究。  \n\n【提出了什么创新的方法】  \n本文提出了一种低级混合运动-力控制方案，结合了针对欠驱动机器人的闭式PIDC方法和针对完全驱动机器人的接触力优化。该方法允许多个腿式机器人灵活协调运动，实现刚性物体的协同操控和运输。通过广泛的仿真和现实世界实验验证了该方法的有效性和稳定性，首次展示了腿式操纵器团队的力闭合抓取能力。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Enabling High-Frequency Cross-Modality Visual Positioning Service for Accurate Drone Landing",
            "authors": "Haoyang Wang,Xinyu Luo,Wenhua Ding,Jingao Xu,Xuecheng Chen,Ruiyang Duan,Jialong Chen,Haitao Zhang,Yunhao Liu,Xinlei Chen",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00646",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00646",
            "arxiv_html_link": "https://arxiv.org/html/2510.00646v1",
            "abstract": "After years of growth, drone-based delivery is transforming logistics.\nAt its core, real-time 6-DoF drone pose tracking enables precise flight control and accurate drone landing.\nWith the widespread availability of urban 3D maps, the Visual Positioning Service (VPS), a mobile pose estimation system, has been adapted to enhance drone pose tracking during the landing phase, as conventional systems like GPS are unreliable in urban environments due to signal attenuation and multi-path propagation.\nHowever, deploying the current VPS on drones faces limitations in both estimation accuracy and efficiency.\nIn this work, we redesign drone-oriented VPS with the event camera and introduce EV-Pose to enable accurate, high-frequency 6-DoF pose tracking for accurate drone landing.\nEV-Pose introduces a spatio-temporal feature-instructed pose estimation module that extracts a temporal distance field to enable 3D point map matching for pose estimation;\nand a motion-aware hierarchical fusion and optimization scheme to enhance the above estimation in accuracy and efficiency, by utilizing drone motion in the early stage of event filtering and the later stage of pose optimization.\nEvaluation shows that EV-Pose achieves a rotation accuracy of 1.34°\\degree and a translation accuracy of 6.9m​mmm with a tracking latency of 10.08m​sms, outperforming baselines by >>50%, thus enabling accurate drone landings.\n\nDemo: https://ev-pose.github.io/.",
            "introduction": "Drone-based delivery has been revolutionizing logistics by reducing delivery times, enhancing accessibility to congested areas in rush hours, and providing a sustainable alternative to conventional transportation [1, 2, 3, 4].\n\nCentral to success of drone-based delivery is the landing phase, in which accurate real-time six-degree-of-freedom (6-DoF) pose estimation plays a pivotal role by enabling precise navigation, effective obstacle avoidance, and reliable package drop-off [5, 6, 7, 8].\nHowever, conventional positioning systems such as GPS are inherently unreliable for accurate pose estimation during landing in urban areas, primarily due to severe signal attenuation and multi-path propagation effects [9, 10, 11].\nWithout reliable systems to guarantee accurate and timely landings, operational efficiency will be compromised, and risks will be elevated in densely populated or high-traffic commercial zones [12].\n\nRecently, as urban 3D maps have become more widely accessible through platforms such as Google Earth, Mapbox, and Apple’s Flyover, Visual Positioning Services (VPS) are increasingly being integrated into drones to achieve precise 6-DoF pose estimation during landing in urban areas [13, 14, 15].\nVPS uses video frames captured by an RGB camera.\nIt extracts visual feature points from frames and matches them against a pre-built 3D environmental map (typically represented as a 3D point cloud) to accurately determine the drone’s real-time 6-DoF pose [16, 17, 18, 19, 20].\nIt further improves the accuracy of these pose estimations by integrating data from onboard IMU sensors using Visual-Inertial Odometry (VIO) technology [15, 21].\n\nWhile VPS shows great promise for accurate drone pose estimation during landing, our field studies conducted in collaboration with a leading drone delivery company uncovers two major limitations when applying current VPS to drones:\n\n(i)(i) Motion blur negatively impacts the drone pose estimation accuracy.\nDrone delivery systems need to rapidly and precisely adjust their pose during the landing process to ensure accurate touchdown at the designated location.\nHowever, during flight, the drone’s motion causes video frames to be highly susceptible to severe motion blur, impairing feature point detection and pose estimation algorithms, ultimately resulting in increased errors in estimating drone pose [22].\n\n(i​i)(ii) The sampling rate mismatch between sensor modalities delays drone pose estimation.\nFor precise flight control, drone pose estimation must operate in real-time (e.g.,>>100 Hz) to provide immediate feedback to the flight controller [23, 24].\nThe onboard IMU typically operates at <<200 Hz, whereas the camera captures frames at <<30 Hz.\nAccordingly, the camera’s limited sampling rate constrains VIO pose updates to <<30 Hz, forcing the controller to rely on IMU over extended periods, leading to accumulated errors and potential loss of control [25, 6].\n\nEven worse, in practice, since urban maps are typically stored as 3D point clouds, the significant computational overhead required to match 2D images with 3D digital maps often limits drone pose estimation algorithms to operating at <<1 Hz [15, 13].\nAll these issues necessitate a comprehensive rethinking of the visual positioning system designed for drone landing in urban areas, spanning from hardware components to algorithmic designs.\n\nThis paper explores the feasibility of redesigning drone-oriented VPS with event cameras.\nEvent cameras are bio-inspired sensors that capture pixel-level intensity changes with a high dynamic range [26, 27].\nCompared to low frame-rate RGB cameras, they offer m​sms-level resolution and latency and thus can effectively capture scene dynamics without blurring [28, 29].\nAs shown in Fig.1, dual advantage of absence of motion blurring and high temporal resolution makes event cameras a better alternative for addressing aforementioned limitations with current VPS systems.\n\nNevertheless, simply replacing RGB cameras with event cameras without adapting the current software stack and vision algorithms in VPS systems would not work due to the unique hardware characteristics of event cameras. We summarize two major challenges below.\n\n∙\\bullet C1: Extraction of repeatable vision features from the event stream for cross-modality vision feature matching.\nEvent cameras detect illumination changes at pixel-level granularity, enabling easy identification of object edges within a scene, as illustrated in Fig.2(b).\nHowever, unlike RGB images, where visual features typically remain consistent regardless of the camera’s viewpoint, event cameras produce significantly different event patterns depending on the drone’s direction of flight relative to the observed object.\nConsequently, existing feature extraction algorithms, even those tailored to event cameras, face difficulties in consistently extracting repeatable features, as illustrated by Fig.2(c) and Fig.2(d).\nThis often results in numerous noisy points, negatively affecting the accuracy of matching visual features with the 3D point map.\n\n∙\\bullet C2: Joint optimization of efficiency and accuracy in map matching-based pose estimation.\nEvent cameras are highly sensitive to environmental changes, with even slight movements triggering numerous pixels and generating hundreds of events.\nAs the drone moves, rapid scene changes lead to event bursts, generating thousands of event reports in a short time.\nUtilizing all the reported events for feature matching would introduce significant computational overhead, delaying drone pose estimations.\nMoreover, the inherent lack of semantic content and scale ambiguity in event cameras causes incorrect matching when encountering complex structures (e.g., repetitive textures), leading to an accuracy decline.\n\nTo conquer these challenges, we introduce EV-Pose, an Event camera-enhanced Visual Positioning Service, which is a novel, accurate, and high-frequency event-based drone 6-DoF Pose tracking system (Fig.1).\nEV-Pose is designed to function in urban canyon environments, where satellite-based systems lose accuracy, rendering them nearly useless.\nWith EV-Pose, drones can achieve 6-DoF pose tracking even in challenging conditions, ensuring efficient flight.\n\nTo address C1, we design a spatio-temporal feature instructed pose estimation (STPE) module (§IV).\nLeveraging temporal relationships among events in an event stream, STPE first introduces a separate polarity time surface and extracts a temporal distance field as a feature representation.\nThis feature enables cross-modality matching between 2D events and a 3D point map, aiding in global drone pose estimation, which can be treated as exteroceptive measurements.\n\nTo address C2, we propose a motion-aware hierarchical fusion and optimization (MHFO) scheme (§V) to enhance pose estimation in STPE.\nEvent camera maintains temporal consistency with IMU, which provides drone motion information.\nIncorporating motion information and structural data from a 3D point map, MHFO first predicts event polarity and performs fine-grained event filtering, fusing event camera with IMU at early stage of raw data processing to improve estimation efficiency.\nThen, MHFO treats motion as proprioceptive measurements and integrates them with exteroceptive data using a factor graph, further fusing event camera and IMU at later stage of pose optimization to improve accuracy.\n\nWe implement EV-Pose and integrate it into ArduPilot, a widely used open-source drone flight controller, and conduct 20+ hours of field studies.\nOur experiments also cover public datasets with challenging scene sequences.\nThe system is benchmarked against three SOTA pose tracking systems using translation error, rotation error, and tracking latency metrics.\nEvaluation results show that EV-Pose achieves an average rotation error of 1.34°\\degree and a translation error of 6.9m​mmm, with a tracking latency of 10.08m​sms, outperforming baselines by >> 50%.\nWe also conduct extensive experiments in field studies to demonstrate robustness of EV-Pose, including various dynamic scenarios and illumination conditions.\n\nSummary.\nThe contributions of this work are as follows:\n(1) We propose EV-Pose, as far as we are aware, the first system to redesign drone-oriented VPS with event cameras, enabling precise, high-frequency drone 6-DoF pose tracking for accurate drone flight control and landing.\n(2) We design a STPE module that leverages temporal relationships among events to extract a temporal distance field used in matching with 3D map for pose estimation, and a MHFO scheme to enhance STPE for accurate and efficient estimation by hierarchically utilizing drone motion in early stage of event filtering and later stage of pose optimization.\n(3) We fully implement EV-Pose and evaluate its performance through extensive field studies and experiments. Comparisons with SOTA systems demonstrate the significant advantages and application potential of EV-Pose.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的GPS系统在城市环境中不可靠，影响无人机的精确着陆。  \n2. 当前的视觉定位服务（VPS）在无人机着陆阶段面临准确性和效率的限制。  \n\n【提出了什么创新的方法】  \n本文提出了EV-Pose，一个基于事件相机的视觉定位服务，旨在实现高频率的无人机6-DoF姿态追踪。EV-Pose通过设计时空特征指导的姿态估计模块（STPE）和运动感知的层次融合与优化方案（MHFO），有效解决了运动模糊和传感器采样率不匹配的问题。实验结果表明，EV-Pose在旋转误差和位移误差上分别达到了1.34°和6.9m，跟踪延迟为10.08ms，性能超越现有基准系统50%以上，确保无人机在复杂城市环境中的高效飞行和准确着陆。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Trajectory Based Observer Design: A Framework for Lightweight Sensor Fusion",
            "authors": "Federico Oliva,Tom Shaked,Daniele Carnevale,Amir Degani",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00630",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00630",
            "arxiv_html_link": "https://arxiv.org/html/2510.00630v1",
            "abstract": "Efficient observer design and accurate sensor fusion are key in state estimation. This work proposes an optimization-based methodology, termed Trajectory Based Optimization Design (TBOD), allowing the user to easily design observers for general nonlinear systems and multi-sensor setups. Starting from parametrized observer dynamics, the proposed method considers a finite set of pre-recorded measurement trajectories from the nominal plant and exploits them to tune the observer parameters through numerical optimization. This research hinges on the classic observer’s theory and Moving Horizon Estimators methodology. Optimization is exploited to ease the observer’s design, providing the user with a lightweight, general-purpose sensor fusion methodology. TBOD’s main characteristics are the capability to handle general sensors efficiently and in a modular way and, most importantly, its straightforward tuning procedure. The TBOD’s performance is tested on a terrestrial rover localization problem, combining IMU and ranging sensors provided by Ultra Wide Band antennas, and validated through a motion-capture system. Comparison with an Extended Kalman Filter is also provided, matching its position estimation accuracy and significantly improving in the orientation.",
            "introduction": "Observation Theory studies how to design dynamic systems fully reconstructing the state 𝝃\\bm{\\xi} of a generic plant ℳ\\mathcal{M}, starting from a limited set of measured signals 𝒚\\bm{y}. Fundamental results solving the so-called Observation problem were obtained in the 60s, with the Kalman Filter (KF) and Luenberger observer formulations [1, 2, 3].\n\nThe Observation Problem has been constructively solved for linear dynamics, while only for particular classes of systems in the nonlinear context [4]. Specifically, KF is the statistically optimal solution for linear systems with Gaussian measurement and process noise [5]. The KF extension through linearization, i.e., the Extended Kalman Filter (EKF), provides an effective tool for general nonlinear input-affine systems [6, 7]. The EKF’s properties have been exploited through several methods, such as output injection [8, 9, 10], linearization [11, 12, 13, 14, 15, 16, 17, 18, 19], and differential observability [20]. High-gain observers [21, 22, 23, 24, 25] are another relevant class of observers, characterized by an easy tuning process controlled by a single parameter. Observation Theory also started exploiting the growing computational power, introducing optimization in novel design methodologies, like Full Information Estimators (FIE) [26] and Moving Horizon Estimators (MHE) [27, 28, 29]. For both of them, stability proofs are available [30, 31, 32, 33]. Even though some works move towards real-time solutions [34, 35, 36], the computational burden still represents the main bottleneck.\n\nThis work proposes an optimization-based methodology, termed Trajectory Based Optimization Design (TBOD), allowing the user to quickly design observers for general nonlinear systems and multi-sensor setups. Starting from a general and parametrized observer structure, a set of trajectories of the nominal plant is considered and exploited to tune the observer through numerical optimization. The main strengths of TBOD are:\n\nlightweight: the entire computational burden is focused on the offline tuning procedure.\n\nlightweight: the entire computational burden is focused on the offline tuning procedure.\n\nspecificity: the TBOD method allows fine-tuning observers for specific trajectories occurring in some applications (e.g., satellites orbiting, area patrolling).\n\nmodularity: the TBOD method can be easily adapted and scaled up for different sensor setups.\n\nThe basics of TBOD have been introduced in [37], addressing a simple position estimate problem cast as a linear observer design on simulated data. The work also compares the TBOD to a KF and a Linear Matrix Inequality (LMI) design method. The current work generalizes the TBOD for generic nonlinear systems (i.e., a joint position-orientation observer in this case) and provides a deeper analysis with more structured theoretical stability guarantees. Furthermore, the accuracy of the proposed method is validated through experimental data, positioning the TBOD as a valid general-purpose sensor fusion approach.\n\nIn this work, the effectiveness of TBOD is shown in a real case study. Specifically, the TBOD method is exploited to design a position and orientation observer on a rover provided with an IMU sensor and ranging measurements from Ultra Wide Band (UWB) antennas[38, 39, 40]. Such a scenario was selected due to the rise of interest in navigation for patrolling and surveillance systems, which strongly rely on the agent’s localization. For the observer structure, results from the classic Luenberger observer [3] and hybrid systems [41] are exploited, as they well describe intermittent measurements [42, 43, 44]. Indeed, EKF represents the standard solution for localization problems [45, 46, 47, 48, 49, 50, 51, 19], and it is therefore considered the benchmark for the TBOD performance evaluation, together with another widespread algorithm, i.e., the Particle Filter (PF) [15].\n\nThis work unfolds as follows: a general recap of the Observation Problem is presented in Sec.˜2. The description of the TBOD methodology is provided in Sec.˜3. Sec.˜4 is devoted to showing the proposed approach’s performance in the above localization problem. Results are described in Sec.˜5, while conclusions are drawn in Sec.˜6.\n\n1. lightweight: the entire computational burden is focused on the offline tuning procedure.\n\n2. specificity: the TBOD method allows fine-tuning observers for specific trajectories occurring in some applications (e.g., satellites orbiting, area patrolling).\n\n3. modularity: the TBOD method can be easily adapted and scaled up for different sensor setups.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的观测器设计方法在处理非线性系统时存在局限性。  \n2. 计算负担仍然是实时解决方案的主要瓶颈。  \n3. 需要一种轻量级且通用的传感器融合方法以提高定位精度。  \n\n【提出了什么创新的方法】  \n提出了一种基于优化的方法，称为Trajectory Based Optimization Design (TBOD)，用于快速设计非线性系统和多传感器设置的观测器。该方法通过利用预录制的测量轨迹进行参数调优，显著简化了观测器设计过程。TBOD的优势在于其轻量级、特定性和模块化，能够高效处理不同传感器组合。实验结果表明，TBOD在定位精度上与扩展卡尔曼滤波器相匹配，并在方向估计上显著提高。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "What Did I Learn? Operational Competence Assessment for AI-Based Trajectory Planners",
            "authors": "Michiel Braat,Maren Buermann,Marijke van Weperen,Jan-Pieter Paardekooper",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Accepted for publication in proceedings of the 2025 IEEE International Automated Vehicle Validation Conference",
            "pdf_link": "https://arxiv.org/pdf/2510.00619",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00619",
            "arxiv_html_link": "https://arxiv.org/html/2510.00619v1",
            "abstract": "Automated driving functions increasingly rely on machine learning for tasks like perception and trajectory planning, requiring large, relevant datasets. The performance of these algorithms depends on how closely the training data matches the task. To ensure reliable functioning, it is crucial to know what is included in the dataset to assess the trained model’s operational risk.\nWe aim to enhance the safe use of machine learning in automated driving by developing a method to recognize situations that an automated vehicle has not been sufficiently trained on. This method also improves explainability by describing the dataset at a human-understandable level.\nWe propose modeling driving data as knowledge graphs, representing driving scenes with entities and their relationships. These graphs are queried for specific sub-scene configurations to check their occurrence in the dataset. We estimate a vehicle’s competence in a driving scene by considering the coverage and complexity of sub-scene configurations in the training set. Higher complexity scenes require greater coverage for high competence.\nWe apply this method to the NuPlan dataset, modeling it with knowledge graphs and analyzing the coverage of specific driving scenes. This approach helps monitor the competence of machine learning models trained on the dataset, which is essential for trustworthy AI to be deployed in automated driving.",
            "introduction": "Automated driving (AD) functions are increasingly becoming integral to the modern transportation system. At the same time, these AD functions are incorporating more and more machine learning algorithms, such as deep neural networks (DNNs), for tasks ranging from perception to trajectory planning. These algorithms are often black box algorithms, making it challenging to understand their internal workings and predict their behavior in all contexts.\n\nTo achieve public acceptance of AD functions that include machine learning algorithms, it is important that they are trustworthy. A system is trustworthy when stakeholders are justified in putting trust in the system [1]. For example, when a passenger trusts the automated lane keeping system functionality of their vehicle, this trust should be justified. Whether a stakeholder is justified to put trust in a system depends on factors such as whether the system is predictable, robust, aware of its own capabilities, and can explain its own actions or reasoning [1, 2, 3]. However, trustworthiness does not mean that a trustworthy system always knows what to do, but rather that the trust placed in the system should align with the capabilities of the system [2, 3]. A trustworthy system should be able to identify when it is and when it is not competent. It must be possible to determine whether the system’s correct functionality can be guaranteed in the specific context it is operating in.\n\nThe performance of machine learning algorithms depends heavily on how close a task is to what the algorithm was exposed to during training [4]. Therefore, for trustworthy and reliable functioning of these algorithms, it is key to know what is included in the dataset and what is not, such that we know what can be expected from the trained model and determine its operational risk. Ensuring the safe deployment of machine learning functions involves recognizing when a vehicle encounters a situation that it has not been adequately trained for. This recognition is essential for preventing potential failures and enhancing the overall trustworthiness of AD systems.\n\nIt is crucial for machine learning algorithms that the training data is of good quality and diverse. Therefore, increasing explainability and insight on what is actually in the driving data that a DNN is trained is essential. This shows the need of being able to describe the driving data at a human-understandable level, and making it easier to see exactly what a model has been trained and tested on.\n\nWe aim to accommodate the safe use of machine learning functions in AD by developing a competence measure that can estimate if an automated vehicle is entering a situation that it has not been sufficiently trained for, and should not be relied upon. This competence measure combines exposure to similar situations during training, or in other words the coverage of the current scene in the training dataset, and the complexity of the situation. This approach assumes that more complex scenes take longer to learn compared to simpler ones, necessitating greater coverage in such instances.\n\nTo make possible to estimate the competence, an accurate description of the situation the vehicle is in should be present. We propose to model driving data as knowledge graphs (KGs), depicting driving scenes at given time instances. Ultimately, by describing the data that is in the training set on a symbolic level in the KGs, and using it to calculate the complexity and coverage for scenes in the dataset, it is possible to create more insight in what is in the driving data.\n\nIn this paper, we demonstrate how we construct this competence measure based on learning experience through the following steps. Section˜III details how the KGs are constructed, after which Section˜IV explains how the competence metric is calculated based on the graphs. In Section˜V, we illustrate that we can compute the coverage and complexity of the scenes in the NuPlan Dataset [5] to create more insight into what is in the dataset, and in Section˜VI, we show that the competence metric for scenes correlates with the performance of a trained trajectory planner on the dataset.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要评估自动驾驶中机器学习算法的操作风险。  \n2. 现有算法缺乏对训练数据的透明度和可解释性。  \n3. 复杂场景的学习需求更高的训练数据覆盖率。  \n\n【提出了什么创新的方法】  \n提出了一种基于知识图的驾驶数据建模方法，通过查询特定子场景配置来评估自动驾驶车辆的能力。该方法结合了训练数据的覆盖率和复杂性，能够识别车辆是否进入未充分训练的场景。通过在NuPlan数据集上的应用，展示了该方法在增强模型可解释性和监测机器学习模型能力方面的有效性。最终，能力度量与训练的轨迹规划器在数据集上的表现相关联，提升了自动驾驶系统的信任度。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Hybrid Training for Vision-Language-Action Models",
            "authors": "Pietro Mazzaglia,Cansu Sancaktar,Markus Peschl,Daniel Dijkman",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00600",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00600",
            "arxiv_html_link": "https://arxiv.org/html/2510.00600v1",
            "abstract": "Using Large Language Models to produce intermediate thoughts, a.k.a. Chain-of-thought (CoT), before providing an answer has been a successful recipe for solving complex language tasks. In robotics, similar embodied CoT strategies, generating thoughts before actions, have also been shown to lead to improved performance when using Vision-Language-Action models (VLAs).\nAs these techniques increase the length of the model’s generated outputs to include the thoughts, the inference time is negatively affected. Delaying an agent’s actions in real-world executions, as in robotic manipulation settings, strongly affects the usability of a method, as tasks require long sequences of actions.\nHowever, is the generation of long chains-of-thought a strong prerequisite for achieving performance improvements? In this work, we explore the idea of Hybrid Training (HyT), a framework that enables VLAs to learn from thoughts and benefit from the associated performance gains, while enabling the possibility to leave out CoT generation during inference. Furthermore, by learning to conditionally predict a diverse set of outputs, HyT supports flexibility at inference time, enabling the model to either predict actions directly, generate thoughts or follow instructions. We evaluate the proposed method in a series of simulated benchmarks and real-world experiments.",
            "introduction": "Despite recent advances in robotics, truly generalist robot policies have long been elusive. Thanks to the joint efforts of collecting large-scale robot data (O’Neill et al., 2024) and making large Vision Language Models (VLM) open-source (Steiner et al., 2024; Tong et al., 2024), we have entered a new era in robotics foundation models. By fine-tuning VLMs on robotic datasets containing actions, we obtain so-called Vision-Language-Action models (VLAs) (Kim et al., 2024; Brohan et al., 2023b; a): large policy models that are trained end-to-end to take language instructions and raw camera images as inputs, and output low-level robotic actions.\n\nVLAs possess several advantages over previous work, such as multimodal prompting of the agent and the availability of knowledge from the base pre-trained VLM. However, generalization to out-of-distribution (OOD) settings, e.g., task configurations not available in the robotics fine-tuning dataset, remains challenging. Indeed, the knowledge of the agent is vast about general concepts, but remains limited in the robotics settings, where the data distribution is often narrow.\n\nIn order to further unleash the capabilities of VLAs with little data, recent works have trained models to predict intermediate outputs, representing the agent’s intentions, before predicting actions (Zawalski et al., 2024; Zhao et al., 2025). One notable example is Embodied CoT (ECoT) (Zawalski et al., 2024), where the VLA learns to output useful information about the given task in language form (Wei et al., 2023), before generating the actions to execute. This not only has shown to improve performance, but it also allows humans to more easily interpret the agent’s intentions and potentially intervene on them, i.e. correcting the agent’s intentions, before action generation. However, due to the intermediate reasoning outputs generated before actions, the action inference frequency of these models can be significantly lower.\n\nThe human cognition process from observation to action\nis hypothesized to leverage the interaction of two systems (Kahneman, 2011). The fast and intuitive System I handles most daily tasks, taking control in contexts that our brain judges as unchallenging.\nThe slow and deliberate System II is activated when decisions require additional computation, such as comparing options or processing complex information. The tendency of the brain is to delegate as many decisions as possible to System I, to save energy and time. However, in order to do so, humans need to improve their capabilities to deal with complex decisions. This is done by developing a skilled intuition (Kahneman & Klein, 2009; Simon, 1992) that allows leveraging previously learned cues to solve familiar tasks, effortlessly.\n\nIn this work, we explore the hypothesis that VLA models can similarly develop skilled intuition, when trained with the right objective. Learning from CoT reasoning traces, a model can further internalize knowledge about environments and tasks. Then, at test-time, the model more eagerly recognizes patterns and can leverage such knowledge to generate actions, even in the absence of intermediate thoughts generation. With this hypothesis in mind, we develop a Hybrid Training (HyT) framework, which allows the agent to learn from a combination of CoT and actions data.\n\nHybrid training presents a more flexible learning objective, which encompasses both ECoT and standard VLAs. During training, we implement the HyT objective using a Monte Carlo estimate, consisting of sampling a variety of conditional inputs and outputs with different probabilities. The model learns to predict a set of ouputs, modelling a multitude of conditional action probabilities, which mainly depends on a newly introduced modality variable. During test-time, it’s the modality variable that allows us to influence the model’s generation. By default, the modality variable conditions the model to directly predict actions. This allows VLAs trained with HyT to maintain the same inference time as standard VLAs, while benefitting from training on reasoning traces.\n\nFurthermore, the modality variable can be used for manipulating the VLA into operating in different inference modes. The ”act” mode, as mentioned, resembles standard VLA’s inference and allows to generate actions directly. In addition, we show a ”think” mode, where the VLAs generates intermediate thoughts as in ECoT, and a ”follow” mode, where the VLA follows a set of provided intentions, e.g. by a human or an oracle, similarly to the lower level policies in hierarchical systems (Shi et al., 2025; Hafner et al., 2022).\n\nIn addition to investigating and validating our proposed HyT framework, on a set of simulated benchmarks (ClevrSkills (Haresh et al., 2024), LIBERO (Liu et al., 2023)) and real-world tasks (on a UFactory xArm 6), we aim to address a fundamental question regarding VLA models:",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的Vision-Language-Action模型在推理时生成长链思维会影响动作执行的实时性。  \n2. 如何在不牺牲性能的情况下，减少推理过程中的延迟。  \n3. 需要探索如何通过混合训练提升模型在复杂任务中的泛化能力。  \n\n【提出了什么创新的方法】  \n本文提出了一种混合训练（Hybrid Training, HyT）框架，使得Vision-Language-Action模型能够在训练过程中学习中间思维，同时在推理时灵活选择是否生成思维。HyT通过引入模态变量，允许模型在不同推理模式下操作，包括直接生成动作、生成中间思维或遵循给定意图。该方法在模拟基准和真实世界任务中进行了评估，显示出在保持推理时间的同时，模型在复杂任务中的表现得到了显著提升。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "GRITS: A Spillage-Aware Guided Diffusion Policy for Robot Food Scooping Tasks",
            "authors": "Yen-Ling Tai,Yi-Ru Yang,Kuan-Ting Yu,Yu-Wei Chao,Yi-Ting Chen",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00573",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00573",
            "arxiv_html_link": "https://arxiv.org/html/2510.00573v1",
            "abstract": "Robotic food scooping is a critical manipulation skill for food preparation and service robots.\nHowever, existing robot learning algorithms, especially learn-from-demonstration methods, still struggle to handle diverse and dynamic food states, which often results in spillage and reduced reliability.\nIn this work, we introduce GRITS: A Spillage-Aware Guided Diffusion Policy for RobotIc Food Scoop TaskS.\nThis framework leverages guided diffusion policy to minimize food spillage during scooping and to ensure reliable transfer of food items from the initial to the target location.\nSpecifically, we design a spillage predictor that estimates the probability of spillage given current observation and action rollout.\nThe predictor is trained on a simulated dataset with food spillage scenarios, constructed from four primitive shapes (spheres, cubes, cones, and cylinders) with varied physical properties such as mass, friction, and particle size.\nAt inference time, the predictor serves as a differentiable guidance signal, steering the diffusion sampling process toward safer trajectories while preserving task success.\nWe validate GRITS on a real-world robotic food scooping platform. GRITS is trained on six food categories and evaluated on ten unseen categories with different shapes and quantities.\nGRITS achieves an 82% task success rate and a 4% spillage rate, reducing spillage by over 40% compared to baselines without guidance, thereby demonstrating its effectiveness.",
            "introduction": "Robotic food scooping is a critical manipulation skill with broad applications in food preparation [1, 2, 3, 4] and assistive feeding [5, 6, 7, 8, 9, 10, 11, 12, 13, 14].\nThese technological advances can boost efficiency, ease labor shortages, and improve quality of life for care recipients and caregivers.\nRecently, learning from demonstrations (LfD) has attracted great attention\nfor robot food scooping [5, 10, 11, 14].\nThese methods have shown great promise in learning policies for manipulating dynamic and deformable food items.\nHowever, these methods are limited by the coverage of available demonstrations because collecting data across diverse food types and conditions is costly and labor-intensive.\nThis constraint prevents methods from generalizing to unseen scenarios and adapting to evolving deployment requirements.\n\nAmong emerging approaches, diffusion policies stand out for their strong capability and generalization across diverse robotic manipulation tasks, including food manipulation [15, 16].\nThese models are appealing as they demonstrate strong generalization and robustness with relatively few expert demonstrations.\nWhile these models can favorably imitate demonstrations, they fail to account for unseen food states such as a bowl full of food, as shown in Fig. LABEL:fig:teaser, and that often causes unintended spillage.\nRecently, guided diffusion policies have emerged.\nThey enable test-time guidance that refines motion trajectories based on the intended objectives, while ensuring task success without retraining.\nFor example, the community has applied test-time guidance to\nsteer diffusion models toward specified task outcomes [17], avoid undesirable states such as collisions [18], and ensure stability and smoothness of robot trajectory [19].\nThis leads our central question: Can guided diffusion policy be leveraged to adjust trajectories in real-world food scooping, ensuring task success while avoiding risky situations such as spillage?\n\nIn this work, we introduce GRITS: A Spillage-Aware Guided Diffusion Policy for RobotIc Food Scoop TaskS.\nGRITS exploits diffusion policy [15] and proposes a novel guidance mechanism to minimize food spillage during scooping and to ensure reliable transfer of food items from the initial to the target location.\nSpecifically, we design a spillage predictor that estimates the probability of spillage given a current observation and an action rollout.\nHowever, training such a predictor in the real world is impractical, as data collection would require tremendous effort to create spillage scenarios and manage cleanup.\nTo overcome this challenge, we utilize Isaac Lab [20], a high-fidelity simulator, to construct a simulated food scooping dataset.\nWe design four primitive shapes (spheres, cubes, cones, and cylinders) and assign them diverse physical properties such as mass, friction, and particle size, which are randomized within specified ranges during dataset collection. This design synthesizes a broad set of controllable food-like items and spillage scenarios.\nWe choose point clouds as the predictor’s input representation to mitigate the sim-to-real gap.\nThe trained spillage predictor is integrated into the denoising process of a guided diffusion policy to continuously evaluate spillage risk.\nThis contrasts with typical classifier-based diffusion models [21], which rely on static labels during image generation.\nTo the best of our knowledge, this is the first work to apply guided diffusion policies to robotic food scooping, a task that requires dynamic rollout adjustment.\n\nIn our experiments, we demonstrate the effectiveness of GRITS in successfully scooping a wide range of food items while significantly reducing spillage.\nWe compare GRITS against several strong baselines, including rule-based methods, learning-based approaches without spillage guidance [14, 15], and variations of guided diffusion policies.\nExperiments are conduct on a real-world robotic platform with diverse food types varying in shape, particle size, and quantity, with six granular food categories for training and ten unseen categories for testing.\nThe results show that GRITS generalizes well to novel scenarios, achieving an 82% success rate and the lowest spillage rate of 4%, which represents a reduction of more than 40% compared to standard diffusion policy baselines.\n\nIn summary, our main contributions are as follows:\n\nWe present GRITS, a novel spillage-aware guided diffusion policy that refines trajectories at the test time through continuous spillage-risk estimation for robotic food scooping.\n\nWe introduce a simulated data collection pipeline for scooping diverse food shapes with varying physical properties.\n\nWe demonstrate that GRITS effectively scoops diverse and unseen food items and significantly reduces spillage compared to strong baselines in real-world experiments.\n\n1. We present GRITS, a novel spillage-aware guided diffusion policy that refines trajectories at the test time through continuous spillage-risk estimation for robotic food scooping.\n\n2. We introduce a simulated data collection pipeline for scooping diverse food shapes with varying physical properties.\n\n3. We demonstrate that GRITS effectively scoops diverse and unseen food items and significantly reduces spillage compared to strong baselines in real-world experiments.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的学习算法在处理动态食物状态时容易导致溢出，降低了可靠性。  \n2. 收集多样化食物类型的演示数据成本高且劳动密集，限制了方法的泛化能力。  \n3. 需要一种方法来确保在真实世界中的食物舀取任务中成功完成，同时避免溢出等风险情况。  \n\n【提出了什么创新的方法】  \n本研究提出了GRITS，一个针对机器人食物舀取任务的溢出感知引导扩散策略。该框架利用引导扩散策略，通过设计一个溢出预测器来估计当前观察和动作展开下的溢出概率，从而在舀取过程中最小化食物溢出并确保食物从初始位置到目标位置的可靠转移。通过在高保真模拟器中构建多样化的食物舀取数据集，GRITS在真实世界的机器人平台上验证，取得了82%的任务成功率和4%的溢出率，相比于没有引导的基线方法，溢出减少了超过40%。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Two stage GNSS outlier detection for factor graph optimization based GNSS-RTK/INS/odometer fusion",
            "authors": "Baoshan Song,Penggao Yan,Xiao Xia,Yihan Zhong,Weisong Wen,Li-Ta Hsu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00524",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00524",
            "arxiv_html_link": "https://arxiv.org/html/2510.00524v1",
            "abstract": "Reliable GNSS positioning in complex environments remains a critical challenge due to non-line-of-sight (NLOS) propagation, multipath effects, and frequent signal blockages. These effects can easily introduce large outliers into the raw pseudo-range measurements, which significantly degrade the performance of global navigation satellite system (GNSS) real-time kinematic (RTK) positioning and limit the effectiveness of tightly coupled GNSS-based integrated navigation system. To address this issue, we propose a two-stage outlier detection method and apply the method in a tightly coupled GNSS-RTK, inertial navigation system (INS), and odometer integration based on factor graph optimization (FGO).\nIn the first stage, Doppler measurements are employed to detect pseudo-range outliers in a GNSS-only manner, since Doppler is less sensitive to multipath and NLOS effects compared with pseudo-range, making it a more stable reference for detecting sudden inconsistencies. In the second stage, pre-integrated inertial measurement units (IMU) and odometer constraints are used to generate predicted double-difference pseudo-range measurements, which enable a more refined identification and rejection of remaining outliers. By combining these two complementary stages, the system achieves improved robustness against both gross pseudo-range errors and degraded satellite measuring quality.\nTo ensure real-time feasibility, the high-frequency IMU and odometer measurements are processed through a pre-integration scheme, while a marginalization-based sliding window maintains computational efficiency. The proposed method was validated using several real-world datasets collected in GNSS-challenging conditions. The experimental results demonstrate that the two-stage detection framework significantly reduces the impact of pseudo-range outliers, and leads to improved positioning accuracy and consistency compared with representative baseline approaches. In the deep urban canyon test, the outlier mitigation method has limits the RMSE of GNSS-RTK/INS/odometer fusion from 0.52 m to 0.30 m, with 42.3% improvement.",
            "introduction": "Localization is a fundamental requirement for autonomous ground vehicles, such as robotic delivery or patrolling platforms [1]. GNSS provides continuous and globally available positioning, navigation, and timing (PNT) services, making it the backbone of outdoor navigation [2]. In particular, GNSS real-time kinematic (RTK), based on double differenced (DD) carrier-phase measurements, can achieve centimeter-level positioning accuracy after resolving carrier-phase ambiguities, which is sufficient for high-precision applications such as automated surveying, precision agriculture, and autonomous vehicle navigation [3].\nHowever, double differenced GNSS measurements are still vulnerable to gross errors caused by multipath, signal blockage, or environmental reflections, especially in urban canyons, dense forests, tunnels, or other complex environments [4]. These gross measurement errors, commonly referred to as outliers, not only degrade standalone GNSS positioning performance but also propagate into any navigation system that relies on GNSS, including loosely or tightly coupled integrated navigation with an IMU or an odometer [5]. In such systems, even occasional GNSS outliers can introduce significant biases in velocity, heading, or position estimates, potentially causing divergence or long-term drift.\n\nA GNSS receiver typically takes pseudo-range, carrier-phase, and Doppler measurements as the primary inputs [6]. Each of these observables may suffer from abnormal deviations due to multipath effects, atmospheric disturbances, satellite hardware faults, or receiver-related issues [7, 8]. Pseudo-range measurements are easily affected by the reflected signal which could lead to large ranging bias. Together, carrier-phase measurements are sensitive to abrupt discontinuities, and such abnormal jumps are commonly referred to as cycle slips, which represent a special form of outlier [9]. Accurately identifying and handling these faulty measurements is crucial for ensuring the reliability of downstream PNT applications. The research on GNSS outlier detection and mitigation has last for decades. From the perspective of input sources, existing outlier detection and mitigation methods can be broadly divided into GNSS-only and sensor-aided strategies.\n\nGNSS-only outlier mitigation is general but limited to geometric information statistics.\nIn this work, we first focus on the outlier mitigation for the generated measurements after signal processing. In this phase, GNSS-only approaches typically include pre-processing and robust state estimation. The pre-processing method relies on the physical models, aiming to flag outliers before the data are passed into the estimator. The fundamental idea is to recognize part of the measurements using consensus check on single or multiple kinds of GNSS measurements.\nThe effectiveness of this step strongly depends on the information source used to form the prediction. Traditional GNSS-only pre-processing methods exploit internal redundancy of the measurements, such as pseudo-range checks [10], carrier-phase combination tests [11], or Doppler cross-validation [12]. These methods are attractive because they do not require external sensors, but they often suffer from degraded performance in challenging environments with limited satellite visibility or highly correlated errors. More recently, advanced estimation methods and machine learning–based techniques have been introduced to enrich the prediction model. For instance, [13] utilizes an autoregressive integrated moving average (ARIMA) model together with a multilayer perceptron (MLP) to generate pseudo-GNSS time series, thereby enabling the identification of outliers that deviate from learned patterns.\nThe robust estimation often exploits residual analysis or statistical consistency checks tied to a particular estimation framework. For example, [14] employs a particle filter to detect pseudo-range and Doppler outliers by analyzing deviations in the measurement innovation sequence. Such methods can be accurate in some applications, but their performance is tightly related to the assumed system model and estimation algorithm, making them less flexible across different application scenarios.\n\nSensor-aided outlier mitigation is feasible but relies on accurate reference.\nIn addition to GNSS-only approaches, sensor-aided methods, leveraging information from motion sensors or perception maps to assist outlier detection, have open a new window for outlier detection and mitigation in GNSS challenging environments. For example, a 3D map is first used in [15] to detect the NLOS measurements. However, the 3D map could cost much time and money to generate, making it to difficult to maintain. [16] employs the innovation sequence from a tightly coupled GNSS/IMU filter to mitigate pseudo-range outliers, where the IMU serves as a short-term reference. [17] proposes a robust iterated cubature Kalman filter to minimize the impact of GNSS outliers under NLOS and multipath conditions in GNSS/INS fusion. While this strategy can be effective, it also introduces new challenges: IMU measurements are subject to intrinsic errors such as bias, scale factor, and noise, which vary between devices and can drift over time. If these parameters are not properly calibrated or estimated online, the predicted GNSS measurement will be inaccurate, which in turn may lead to false alarms or missed detections.\nAs a common sensor on the ground vehicle, an odometer could provide stable velocity measurements during operation in environments. Similar to IMU, an odometer could provide high-frequency measurements to resume the vehicle motion in two dimensions [5]. Nevertheless, it is still not employed to enhance GNSS measurement quality checking.\n\nThe objective of this work is to address these limitations observed in both GNSS-only and sensor-aided strategies, particularly in terms of detection accuracy and robustness under real-world conditions. To this end, we propose a two-stage GNSS outlier mitigation framework. In the first stage, raw GNSS measurements undergo consistency checks and redundancy tests to capture obvious anomalies in a GNSS-only fashion. In the second stage, pre-integrated IMU and odometer measurements are incorporated into a joint GNSS quality-checking module, where they serve as physically consistent references over short time intervals. By combining the complementary strengths of GNSS redundancy and inertial pre-integration, the proposed method is capable of detecting both abrupt and subtle outliers more reliably, even in challenging environments such as urban canyons or tunnels. We also apply this GNSS outlier detection and mitigation method to a FGO-based GNSS-RTK/INS/odometer fusion system, demonstrating that our hybrid design offers a pathway toward more dependable GNSS-based navigation solutions in autonomous ground vehicles.\nThe contributions of this work can be summarized as follows:\n\nTwo-stage GNSS outlier detection framework: A practical two-stage approach is proposed to enhance GNSS measurement reliability. The first stage employs Doppler measurements to detect pseudo-range outliers using GNSS-only information, while the second stage refines the detection using predicted pseudo-range from pre-integrated IMU and odometer measurements.\n\nApplication in a tightly coupled GNSS-RTK/INS/odometer system: The outlier detection is embedded in a factor graph optimization framework that jointly estimates navigation states and IMU-odometer parameters. This enables the system to maintain high positioning accuracy and consistency in challenging environments where GNSS signals may be degraded.\n\nExperimental validation and real-world applicability: Extensive simulations and real-world experiments demonstrate the effectiveness of the proposed method under various GNSS-challenging scenarios. The framework is robust to limited satellite geometry and measurement outliers, and it provides a foundation for practical deployment in autonomous ground vehicles.",
            "llm_summary": "【论文的motivation是什么】  \n1. GNSS定位在复杂环境中面临挑战，容易受到大范围伪距测量的影响。  \n2. 现有的GNSS仅基于方法在恶劣环境中的性能不足，无法有效识别和处理异常值。  \n3. 传感器辅助方法依赖于准确的参考，存在设备误差和漂移的问题。  \n\n【提出了什么创新的方法】  \n提出了一种两阶段的GNSS异常值检测框架。第一阶段通过多普勒测量进行伪距异常值的检测，利用GNSS单一信息进行初步筛选。第二阶段结合预积分的IMU和里程计测量，生成预测的伪距，进一步精细化异常值的识别与剔除。该方法在GNSS信号受损的复杂环境中表现出更高的鲁棒性，显著降低了伪距异常值的影响，提升了定位精度和一致性，实验结果显示在深度城市峡谷测试中，RMSE从0.52米降低到0.30米，改善幅度达42.3%。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "From Human Hands to Robot Arms: Manipulation Skills Transfer via Trajectory Alignment",
            "authors": "Han Zhou,Jinjin Cao,Liyuan Ma,Xueji Fang,Guo-jun Qi",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00491",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00491",
            "arxiv_html_link": "https://arxiv.org/html/2510.00491v1",
            "abstract": "Learning diverse manipulation skills for real-world robots is severely bottlenecked by the reliance on costly and hard-to-scale teleoperated demonstrations.\nWhile human videos offer a scalable alternative, effectively transferring manipulation knowledge is fundamentally hindered by the significant morphological gap between human and robotic embodiments.\nTo address this challenge and facilitate skill transfer from human to robot, we introduce Traj2Action,a novel framework that bridges this embodiment gap by using the 3D trajectory of the operational endpoint as a unified intermediate representation, and then transfers the manipulation knowledge embedded in this trajectory to the robot’s actions.\nOur policy first learns to generate a coarse trajectory, which forms an high-level motion plan by leveraging both human and robot data.\nThis plan then conditions the synthesis of precise, robot-specific actions (e.g., orientation and gripper state) within a co-denoising framework.\nExtensive real-world experiments on a Franka robot demonstrate that Traj2Action boosts the performance by up to 27% and 22.25% over π0\\pi_{0} baseline on short- and long-horizon real-world tasks, and achieves significant gains as human data scales in robot policy learning.\nOur project website, featuring code and video demonstrations, is available at https://anonymous.4open.science/w/Traj2Action-4A45/.",
            "introduction": "Enabling robots to master a diverse array of manipulation skills in the real world presents a formidable challenge, primarily due to the data-hungry nature of modern policy learning (Kim et al., 2024; Black et al., ; Team et al., 2024).\nThe prevailing paradigm of imitation learning relies on large-scale datasets of expert demonstrations (O’Neill et al., 2024; Khazatsky et al., 2024), which are typically gathered through time-consuming teleoperation (Wu et al., 2023; Fu et al., 2024; Zhao et al., 2023; Orbik, 2021).\nThis process not only requires significant investment in specialized hardware but also demands extensive operator training, rendering it a critical bottleneck for scaling up robotic capabilities.\n\nWhile human videos offer a cost-effective and abundant alternative data source, their direct application is fundamentally hindered by the significant morphological gap between embodiments, as they lack the robot-specific action labels required for direct mimicry.\nPrior works have attempted to learn a shared visual representation across both human and robot videos (Wang et al., 2023), formulating reward functions (Ma et al., 2022; Shao et al., 2021; Ma et al., 2023), or using human demonstrations as high-level context for robot action prediction (Zhu et al., 2025; Shah et al., 2025).\nHowever, these indirect approaches often fail to leverage the rich, explicit motion signals embedded within human actions.\nMore direct lines of research attempt to define a unified action space or to transfer motion skills in a way that can be effectively utilized by robots.\nYet, these strategies often introduce their own limitations.\nSome methods adopt a two-stage pipeline Bharadhwaj et al. (2024); Bi et al. (2025); Luo et al. (2025), where models are first trained on human data and then adapted with robot data, but this sequential procedure can constrain the final quality of robot policy.\nOther joint-training approaches (Yang et al., 2025; Qiu et al., 2025) are typically confined to kinematically similar embodiments like humanoid robots, while methods (Ren et al., 2025; Kareer et al., 2025) that directly align mismatched poses (e.g., human hand to parallel gripper) struggle to establish a physically meaningful correspondence.\nEven though some works (Bharadhwaj et al., 2024) adopt simplified representations like rigid transformations to mitigate the embodiment gap, fundamental structural differences mean that the physical interpretation of rotations remains misaligned.\nFurthermore, other methods (Park et al., 2025; Liu et al., 2025) impose heavy constraints, either requiring strictly paired human-robot demonstrations or relying on complex motion retargeting pipelines, which restricts their applicability and negates the cost-benefit of using human data.\n\nTo address these limitations, we propose Traj2Action, a framework that transfers motion knowledge from human to robot learning process by leveraging 3D trajectories as a robust intermediate representation.\nOur method follows a coarse-to-fine action prediction paradigm for robot, where coarse trajectory planning derived from both human and robot demonstrations guides fine-grained robot action learning.\nSpecifically, we introduce a unified trajectory representation of human hand and robot end-effector 3D positions, which reduces embodiment discrepancies and allows abundant, cost-effective human data to improve coarse trajectory planning.\nWe then utilize a co-denoising training scheme over trajectories and actions, which enables the policy to focus on generating robot actions with precise orientation and gripper states under the guidance of the coarse trajectory plan.\nTo further bridge the observational gap, we designed a hand wrist-mounted camera to supply ego-view for human data collection, which mimics the robot’s ego-centric view observation and thus enhances cross-embodiment training effectiveness and consistency.\n\nBy leveraging human demonstrations, our method surpass a traditional VLA baseline on short- and long-horizon real-world tasks by a large margin.\nBesides, as human data scales, our method shows a significant performance gain in robot manipulation tasks.\nFurthermore, we show that our method allows for replacing a significant amount of expensive robot data with low-cost human demonstrations while achieving comparable final policy performance, validating the effectiveness of using low-cost human demonstration data for robot learning.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人学习多样化操作技能受限于昂贵且难以扩展的遥控演示。  \n2. 人类视频作为替代数据源，但由于人类与机器人之间的形态差异，知识转移面临挑战。  \n3. 现有方法未能有效利用人类动作中的丰富运动信号。  \n\n【提出了什么创新的方法】  \n提出了Traj2Action框架，通过使用3D轨迹作为统一的中间表示，解决了人类与机器人之间的形态差异。该方法首先学习生成粗略轨迹，形成高层次运动计划，然后在共去噪框架下合成精确的机器人特定动作。通过大量真实世界实验，Traj2Action在短期和长期任务中分别提升了27%和22.25%的性能，并随着人类数据的增加，机器人政策学习显著提高。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation",
            "authors": "Run Su,Hao Fu,Shuai Zhou,Yingao Fu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00466",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00466",
            "arxiv_html_link": "https://arxiv.org/html/2510.00466v1",
            "abstract": "Offline reinforcement learning (RL) has emerged as a promising framework for addressing social robot navigation challenges. However, inherent uncertainties in pedestrian behavior and limited environmental interaction during training often lead to suboptimal exploration and distributional shifts between offline pre-training and online deployment. To overcome these limitations, this paper proposes a novel offline-to-online fine-tuning RL algorithm for social robot navigation by integrating Return-to-Go (RTG) prediction into a causal transformer architecture. Our algorithm features a spatio-temporal fusion model designed to precisely estimate RTG values in real-time by jointly encoding temporal pedestrian motion patterns and spatial crowd dynamics. This RTG prediction framework mitigates distribution shift by aligning offline policy training with online environmental interactions. Furthermore, a hybrid offline-online experience sampling mechanism is built to stabilize policy updates during fine-tuning, ensuring balanced integration of pre-trained knowledge and real-time adaptation. Extensive experiments in simulated social navigation environments demonstrate that our method achieves a higher success rate and lower collision rate compared to state-of-the-art baselines. These results underscore the efficacy of our algorithm in enhancing navigation policy robustness and adaptability. This work paves the way for more reliable and adaptive robotic navigation systems in real-world applications.",
            "introduction": "With significant advancements in robotics and artificial intelligence, autonomous mobile robot navigation has garnered considerable attention. A primary challenge is developing a system that enables robots to move from a starting point to a desired target while effectively avoiding obstacles, especially in human-shared environments such as smart manufacturing, warehouses, and autonomous driving. This concept is referred to as socially-aware robot navigation. However, the complexity of pedestrian movement poses numerous challenges in designing effective social robot navigation algorithms.\n\nSocially aware robot navigation can be achieved through human-robot interaction, leveraging the inherent advantage of learning through trial and error. Recently, significant efforts [1, 2, 3] have been made in the field of socially aware robot navigation by incorporating deep learning techniques, such as Long Short-Term Memory (LSTM) and attention mechanisms. These algorithms frame socially aware robot navigation as a Markov Decision Process (MDP), which is subsequently solved using value-based deep reinforcement learning (DRL). Current robot navigation algorithms primarily focus on training policies in an online mode, learning navigation policies from raw sensory inputs, such as laser scans [4], images [5], or agent-level state representations [6].\n\nCurrent theses online reinforcement learning methods for social robot navigation necessitate frequent robot-pedestrian interactions within crowded settings. They rely on the iterative collection of extensive exploratory data to refine navigation policies. However, this training paradigm suffers from low sample efficiency, as it demands substantial volumes of interactive data to learn effective policies. Furthermore, the suboptimal policies characteristic of initial training phases can lead to unsafe exploration, presenting potential collision risks for both the robot and pedestrians.\n\nIn contrast, offline reinforcement learning, when applied to social navigation, leverages pre-existing datasets to optimize the navigation policy without requiring online interaction. This methodology significantly improves safety throughout the training process by eliminating risky exploratory actions. Nevertheless, the absence of online exploration and limited environmental interaction can impede the learning of a truly optimal navigation strategy.\n\nTo address the aforementioned challenges, the offline-to-online fine-tuning approach demonstrates significant potential. During its offline training, Return-to-Go (RTG) values are derived directly from empirical trajectory data by computing cumulative returns observed in the dataset. In contrast, during its online interaction, the use of fixed exploration RTG may exhibit discrepancies relative to dynamically generated returns in the real-world crowd scenario. Such misalignment can induce distribution shift problem, leading to aggressive or unsafe decision behavior. This challenge is further compounded by uncertainties of pedestrian behavior. To mitigate this issue, we propose a OTOFRL (offline-to-online fine-tuning RL) algorithm. In particular, our algorithm trains a Return-to-Go prediction (RTGP) model for sequence modeling in the causal transformer, aiming to eliminate the distribution shift problem during the online fine-tuning phase caused by the complexity of pedestrian movements and the fixed RTG in the online setting. The key contributions of this study are summarized as follows:\n\nTo address the distribution shift issue during online fine-tuning, the OTOFRL algorithm is proposed through the establishment of an RTGP model based on a spatio-temporal fusion transformer and integrating sequence modeling with a causal transformer. By capturing the dynamic behavioral patterns of pedestrians in both temporal and spatial dimensions, the model can accurately predict long-term cumulative returns. This long-term return prediction enables the model to gain a more comprehensive understanding of environmental dynamics, thereby enhancing its adaptability to new data in human-robot interaction environments.\n\nTo avoid the potential deviation issue, arising from synchronous updates between the robot navigation policy and the RTGP, this paper builds a hybrid offline-online sampling mechanism by incorporating a dual timescale update to manage the updates of these two components, so as to effectively reduces prediction variance and enhances the stability of policy adaptation.\n\nTo ensure a seamless transition from offline pre-training to online fine-tuning, we propose a hybrid offline-online sampling method that combines hybrid offline-online experience replay with a prioritized sampling mechanism.\n\nSocially Aware Robot Navigation refers to the movement of robots in spaces shared with pedestrians, where pedestrian behavior is often unpredictable and non-cooperative. Traditional reactive methods, such as optimal reciprocal collision avoidance (ORCA)[7] and reciprocal velocity obstacle (RVO)[8], specify interaction rules for a single step based on the current geometric configuration between robots and pedestrians. However, they fail to capture pedestrian behavior, leading to potentially unsafe movements. While trajectory-based methods can mitigate this issue, they inevitably encounter the ”freezing” problem[9] in dense crowds.\n\nTo address this issue, Chen et al. proposed a collision avoidance with DRL (CADRL) algorithm. To handle pedestrian behavior randomness, they extended it to Socially-Aware CADRL by introducing social norms [10]. However, these approaches require assumptions about specific motion models for neighboring agents over short time scales. To eliminate this need, Everett et al. used LSTM to extend CADRL, enabling it to accommodate varying pedestrian numbers. Additionally, self-attention mechanism has been employed to enhance DRL-based social navigation performance for improved crowd-robot interaction.\n\nHowever, due to the limitations of online training, all these methods inevitably require frequent interactions with the environment to collect the data necessary for training the robot. Consequently, safety issue arises from collisions between navigating robots and pedestrians during exploration. Additionally, low sampling efficiency during pedestrian-robot interactions poses a significant challenge.\n\nRecent advancements in RL have introduced a novel perspective that frames the offline RL problem as a context-conditioned sequence modeling task [11], aligning RL with a supervised learning paradigm [12]. This approach shifts the focus from explicitly learning Q-functions or policy gradients to predicting action sequences conditioned on task specifications. For instance, Chen et al. [13] trained transformers as model-free, context-conditioned policies, while Janner et al. employed transformers for both policy and dynamics modeling, demonstrating that beam search could significantly enhance model-free performance. However, these studies primarily operate within the offline RL paradigm, analogous to fixed dataset training in natural language processing. Despite the promise of such methods, the prevailing paradigm in RL remains offline pre-training followed by online fine-tuning. Nair et al. [14] highlighted that applying offline or off-policy RL methods in this context often results in suboptimal performance, or even performance degradation, due to the accumulation of off-policy errors [15] and the excessive conservatism required in offline RL to mitigate overestimation in out-of-distribution states.\n\nTo address these challenges, various algorithms have been proposed. For example, Nair et al. developed an approach effective for both offline and online training regimes, while Kostrikov et al. [16] introduced an expected implicit Q-learning algorithm that leverages behavior cloning to extract policies, thereby avoiding out-of-distribution actions and achieving robust online fine-tuning performance. Lee et al. [17] tackled the offline-to-online transition by balancing replay strategies and employing Q-function ensembles to preserve conservativeness during offline training. On the basis of the offline Decision Transformer (DT) [18], Zheng et al. [19] enhanced the performance of the online fine-tuning phase by introducing an exploration mechanism and historical experience mixing in Online Decision Transformer (ODT). It is evident that excessive sampling of low-return experiences, such as those involving collisions between the robot and pedestrians, is detrimental to online fine-tuning. During the online phase, the hybrid offline-online sampling method is adopted to mitigate the issue of over-sampling low-reward experiences, thereby enhancing the model’s ability to address challenges associated with online fine-tuning. By strategically focusing on high-reward and informative experiences, such as successfully navigating through dense crowds in a socially compliant manner, the sampling mechanism ensures more efficient learning and improved policy adaptation in complex social navigation tasks.\n\n1. To address the distribution shift issue during online fine-tuning, the OTOFRL algorithm is proposed through the establishment of an RTGP model based on a spatio-temporal fusion transformer and integrating sequence modeling with a causal transformer. By capturing the dynamic behavioral patterns of pedestrians in both temporal and spatial dimensions, the model can accurately predict long-term cumulative returns. This long-term return prediction enables the model to gain a more comprehensive understanding of environmental dynamics, thereby enhancing its adaptability to new data in human-robot interaction environments.\n\n2. To avoid the potential deviation issue, arising from synchronous updates between the robot navigation policy and the RTGP, this paper builds a hybrid offline-online sampling mechanism by incorporating a dual timescale update to manage the updates of these two components, so as to effectively reduces prediction variance and enhances the stability of policy adaptation.\n\n3. To ensure a seamless transition from offline pre-training to online fine-tuning, we propose a hybrid offline-online sampling method that combines hybrid offline-online experience replay with a prioritized sampling mechanism.",
            "llm_summary": "【论文的motivation是什么】  \n1. 社会机器人导航面临行人行为不确定性和有限环境交互导致的次优探索问题。  \n2. 现有的在线强化学习方法需要频繁的机器人-行人交互，导致低样本效率和潜在的安全风险。  \n3. 离线强化学习虽然提高了安全性，但缺乏在线探索会妨碍学习最优导航策略。  \n\n【提出了什么创新的方法】  \n本文提出了一种离线到在线微调的强化学习算法（OTOFRL），通过在因果变换器架构中集成回报预测（RTG）来解决社会机器人导航中的分布偏移问题。该算法采用时空融合模型，实时精确估计RTG值，捕捉行人的动态行为模式，从而增强了模型对环境动态的理解和适应能力。此外，构建了混合离线-在线经验采样机制，以稳定策略更新，确保预训练知识与实时适应的平衡。实验结果表明，该方法在模拟社交导航环境中实现了更高的成功率和更低的碰撞率，展示了其在增强导航策略鲁棒性和适应性方面的有效性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation",
            "authors": "Yiyuan Pan,Yunzhe Xu,Zhe Liu,Hesheng Wang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00441",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00441",
            "arxiv_html_link": "https://arxiv.org/html/2510.00441v1",
            "abstract": "Visual navigation is a fundamental problem in embodied AI, yet practical deployments demand long-horizon planning capabilities to address multi-objective tasks. A major bottleneck is data scarcity: policies learned from limited data often overfit and fail to generalize OOD. Existing neural network-based agents typically increase architectural complexity that paradoxically become counterproductive in the small-sample regime. This paper introduce NeuRO, a integrated learning-to-optimize framework that tightly couples perception networks with downstream task-level robust optimization. Specifically, NeuRO addresses core difficulties in this integration: (i) it transforms noisy visual predictions under data scarcity into convex uncertainty sets using Partially Input Convex Neural Networks (PICNNs) with conformal calibration, which directly parameterize the optimization constraints; and (ii) it reformulates planning under partial observability as a robust optimization problem, enabling uncertainty-aware policies that transfer across environments. Extensive experiments on both unordered and sequential multi-object navigation tasks demonstrate that NeuRO establishes SoTA performance, particularly in generalization to unseen environments. Our work thus presents a significant advancement for developing robust, generalizable autonomous agents.",
            "introduction": "Visual navigation has emerged as a cornerstone problem in robotics, where agents must reason over complex 3D environments and pursue possible multiple goals under uncertainty. Among benchmark tasks, Multi-Object Navigation (MultiON) task [23] evaluates an agent’s ability to locate multiple objects within 3D environments, requiring sophisticated scheduling strategies across multiple goals. This task presents significant challenges due to its multi-goal nature and severe data scarcity, which often causes agents to overfit to training environments and generalize poorly to unseen scenarios. While existing neural network (NN)-based approaches attempt to enhance performance by stacking complex network modules, these methods tend to exacerbate overfitting rather than improve generalization in low-data regimes such as search-rescue missions [19, 24].\n\nA potential solution is instead to couple learning with explicit optimization models to form task-aware training frameworks [9]: First, optimization models can explicitly articulate hard task constraints (e.g., collision avoidance in navigation) without additional parameters to capture shared dynamics across related tasks, thus enhancing generalization; Secondly, their inherent structure facilitates the simultaneous consideration and scheduling of multiple objectives. Therefore, it’s a natural idea to wed networks with optimization in visual navigation. However, two non-trivial challenges are raised (see Fig.˜1): First, the reliability of optimization outputs depends critically on the accuracy of parameters predicted by the network and the data scarcity can lead to prediction errors, destabilizing the entire pipeline; Secondly, formulating an optimization problem often requires global information, which is unattainable in partially observable environments typical of embodied navigation. In essence, a naïve integration might instead degrade system stability and overall performance.\n\nTo this end, we propose NeuRO, a robust framework for training visual navigation agents end-to-end with downstream optimization tasks. Specifically, NeuRO incorporates two key technical innovations: (i) For the unreliable network predictions, we employ Partially Input Convex Neural Networks (PICNNs) to distill complex, non-convex visual information into high-dimensional convex embeddings. These embeddings are then transformed by a proposed calibration method into a tractable, convex uncertainty set, which is subsequently passed to the downstream optimization problem; (ii) For the issue of partial observability, we formulate the visual navigation (formally modeled as a POMDP) as a pursuit-evasion game, casting it as a robust optimization (RO) problem. Such formulation inherently manages parametric uncertainty arising from partial observations and demonstrates generality across diverse navigation tasks. We evaluate NeuRO on our proposed unordered MultiON and traditional sequential MultiON tasks, demonstrating superior performance and generalization over state-of-the-art (SoTA) network-based approaches. In summary, our contributions are threefold:\n\nWe propose NeuRO, a novel hybrid framework that synergistically integrates deep neural networks with downstream optimization tasks for end-to-end training, significantly improved generalization in data-scarce regimes.\n\nWe introduce a methodology to bridge unreliable navigation prediction and safe trajectory optimization. By leveraging PICNN-based conformal calibration and casting POMDP planning as robust optimization, NeuRO effectively handles partial observability and parametric uncertainty.\n\nExtensive empirical validation demonstrating that NeuRO establishes superior performance on challenging MultiON benchmarks, significantly outperforming existing methods, particularly in generalization to unseen environments.\n\n1. We propose NeuRO, a novel hybrid framework that synergistically integrates deep neural networks with downstream optimization tasks for end-to-end training, significantly improved generalization in data-scarce regimes.\n\n2. We introduce a methodology to bridge unreliable navigation prediction and safe trajectory optimization. By leveraging PICNN-based conformal calibration and casting POMDP planning as robust optimization, NeuRO effectively handles partial observability and parametric uncertainty.\n\n3. Extensive empirical validation demonstrating that NeuRO establishes superior performance on challenging MultiON benchmarks, significantly outperforming existing methods, particularly in generalization to unseen environments.",
            "llm_summary": "【论文的motivation是什么】  \n1. 数据稀缺导致的过拟合问题，影响视觉导航的泛化能力。  \n2. 现有神经网络方法在小样本情况下复杂性增加，反而降低了性能。  \n3. 需要将学习与优化模型结合，以形成任务感知的训练框架。  \n\n【提出了什么创新的方法】  \n本文提出了NeuRO，一个将深度神经网络与下游优化任务紧密结合的混合框架，旨在实现端到端训练并显著提高数据稀缺情况下的泛化能力。NeuRO通过使用部分输入凸神经网络（PICNNs）将不可靠的视觉预测转化为可处理的凸不确定性集，并将视觉导航问题建模为一个鲁棒优化问题，从而有效管理部分可观测性和参数不确定性。通过在无序和顺序多目标导航任务上的广泛实验，NeuRO展示了优于现有方法的出色性能，尤其是在对未见环境的泛化能力方面。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators",
            "authors": "Hengtao Li,Pengxiang Ding,Runze Suo,Yihao Wang,Zirui Ge,Dongyuan Zang,Kexian Yu,Mingyang Sun,Hongyin Zhang,Donglin Wang,Weihua Su",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00406",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00406",
            "arxiv_html_link": "https://arxiv.org/html/2510.00406v1",
            "abstract": "Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to our webpage.",
            "introduction": "Vision-Language-Action (VLA) models have recently achieved remarkable progress by building upon large, pre-trained vision-language models (VLMs) (Li et al., 2025b; Karamcheti et al., 2024; Driess et al., 2023). Leveraging the powerful perceptual generalization of VLMs allows these models to operate under diverse visual conditions. However, most existing VLAs (Brohan et al., 2022; Zitkovich et al., 2023; Black et al., 2024; Bjorck et al., 2025; Kim et al., 2024) are trained purely via imitation learning. This approach is prone to error accumulation under distribution shift, where small deviations from expert demonstrations gradually drive the policy toward unfamiliar states and weaken its robustness (Ross & Bagnell, 2010; De Haan et al., 2019; Foster et al., 2024).\n\nIn contrast, reinforcement learning (RL) offers a promising avenue to overcome these limitations by explicitly optimizing beyond demonstrated behaviors and encouraging exploration (Liu et al., 2025). Recent studies have increasingly incorporated RL into VLA training, demonstrating its critical role in enhancing generalization and long-horizon task performance through offline RL approaches (Zhang et al., 2025c; 2024), direct real-world RL (Xu et al., 2024; Guo et al., 2025), and simulation-based RL (Lu et al., 2025; Tan et al., 2025; Liu et al., 2025).\n\nYet, standard RL pipelines for VLA face steep challenges. Simulation-based RL (Chen & Li, 2025; Chen et al., 2025b; Shu et al., 2025) often requires millions of interactions and suffers from a pronounced sim-to-real gap. Real-world training (Xu et al., 2024; Mark et al., 2024; Guo et al., 2025; Chen et al., 2025a), on the other hand, is prohibitively costly and can raise safety concerns. Offline RL also remains limited: without interaction with the environment, models are vulnerable to distribution shift and cannot learn from the consequences of their own actions (Tan et al., 2025).\n\nTo address these challenges, we propose VLA-RFT, a reinforcement fine-tuning framework that leverages a world model as a high-fidelity simulator for policy optimization. At its core, VLA-RFT employs a controllable world simulator that, once trained on a dataset of robot interactions, can predict future visual observations conditioned on an action sequence. Unlike conventional simulation environments restricted to handcrafted scenarios, this simulator is entirely data-driven, capturing the diversity of real-world interactions while avoiding the prohibitive cost and safety risks of training directly in the physical world. For a given task, policy-proposed actions are rolled out within this simulator to generate predicted visual trajectories. These synthetic trajectories then enable the design of a dense, task-grounded reward by comparing them against the visual trajectory from goal-achieving reference trajectory. These rewards are then used to optimize the policy via Generalized Reinforcement Policy Optimization (GRPO), enabling stable and efficient reinforcement fine-tuning.\n\nThis design provides a continuous, action-aligned learning signal that substantially reduces the sample complexity of reinforcement fine-tuning. Empirically, we show that with as few as 400 fine-tuning steps, VLA-RFT not only outperforms strong supervised fine-tuning baselines (Wang et al., 2025) in both overall performance and compositional generalization, but also achieves markedly higher efficiency than simulator-based RL algorithms that demand orders of magnitude more interactions. Furthermore, in perturbed or adversarial scenarios, VLA-RFT exhibits superior action robustness, sustaining stable task execution even under unexpected environmental variations. Taken together, this combination of efficiency, generalization, and robustness underscores the practical advantages of our framework for scalable VLA training.\n\nFinally, we hope that our method, experiments, and analysis will motivate future research to harness world models as a general and efficient post-training paradigm for VLAs, thereby substantially enhancing their practicality and accelerating their real-world deployment.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的Vision-Language-Action (VLA)模型依赖模仿学习，导致错误累积和鲁棒性差。  \n2. 强化学习虽然能缓解这些问题，但通常需要昂贵的真实世界交互或面临sim-to-real差距。  \n\n【提出了什么创新的方法】  \nVLA-RFT是一种强化微调框架，利用数据驱动的世界模型作为可控模拟器。该模拟器基于真实交互数据训练，能够预测基于动作的未来视觉观察。通过在模拟器中进行策略回放，生成与目标参考轨迹比较的密集奖励，从而优化策略。VLA-RFT在少于400个微调步骤内超越了强监督基线，并在扰动条件下展现出强大的鲁棒性，稳定执行任务。该方法显著降低了样本需求，提高了效率和泛化能力，展示了世界模型在VLA训练中的实用优势。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Physics-Informed Neural Controlled Differential Equations for Scalable Long Horizon Multi-Agent Motion Forecasting",
            "authors": "Shounak Sural,Charles Kekeh,Wenliang Liu,Federico Pecora,Mouhacine Benosman",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00401",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00401",
            "arxiv_html_link": "https://arxiv.org/html/2510.00401v1",
            "abstract": "Long-horizon motion forecasting for multiple autonomous robots is challenging due to non-linear agent interactions, compounding prediction errors, and continuous-time evolution of dynamics. Learned dynamics of such a system can be useful in various applications such as travel time prediction, prediction-guided planning and generative simulation. In this work, we aim to develop an efficient trajectory forecasting model conditioned on multi-agent goals. Motivated by the recent success of physics-guided deep learning for partially known dynamical systems, we develop a model based on neural Controlled Differential Equations (CDEs) for long-horizon motion forecasting. Unlike discrete-time methods such as RNNs and transformers, neural CDEs operate in continuous time, allowing us to combine physics-informed constraints and biases to jointly model multi-robot dynamics. Our approach, named PINCoDE (Physics-Informed Neural Controlled Differential Equations), learns differential equation parameters that can be used to predict the trajectories of a multi-agent system starting from an initial condition. PINCoDE is conditioned on future goals and enforces physics constraints for robot motion over extended periods of time. We adopt a strategy that scales our model from 10 robots to 100 robots without the need for additional model parameters, while producing predictions with an average ADE below 0.5 m for a 1-minute horizon. Furthermore, progressive training with curriculum learning for our PINCoDE model results in a 2.7×2.7\\times reduction of forecasted pose error over 4 minute horizons compared to analytical models.",
            "introduction": "Spatio-temporal dynamics modeling for multiple interacting autonomous mobile robots (AMRs) can be a challenging task [1, 2, 3, 4]. Non-linear coupled dynamics of such robots, hidden agents and irregular sampling times across multiple robots often add to the complexity. Such dynamics modeling can benefit from methods that learn the continuous-time evolution of the joint state of the multi-agent system. Modern robot motion forecasting methods primarily rely on transformers or GNNs to model robot interactions and then predict robot poses in discrete-time with auto-regressive models [3, 2, 4]. However, many such auto-regressive methods tend to predict divergent trajectories with errors compounding over time steps, especially in scenarios such as sharp turns, sudden events and cases with missing or irregularly-timestamped data [5].\n\nFor a large fleet of robots operating in a shared warehouse environment, communication overheads to a central database can restrict pose recording frequencies to maximum values of 1 Hz or lower. At this frequency, there can be significant movement and robot interactions within a one second time window. Furthermore, pose data is likely to be estimated at different time instants across robots within the time window, fundamentally causing a mismatch when rounded off to the nearest second. With many interacting robots moving in close quarters, these truncation errors can add up over time. To deal with such challenges, Neural Differential Equations (NDEs) [6, 7] are a promising solution. These neural networks can model the dynamics of the system more explicitly in the form of continuous-time differential equations that learn the smooth temporal evolution of robot trajectories. Additionally, neural ODEs can naturally incorporate physics constraints directly into motion forecasts and handle complex situations like the one shown in Figure 1.\n\nPhysics-Informed Neural Networks (PINNs) [8] work well for modeling systems where the dynamics evolve in continuous-time based on physical laws that are difficult to precisely model based on known equations. Multi-agent motion forecasting for autonomous robots is one such application where continuous-time NDEs and PINNs can potentially benefit the modeling of robot dynamics. In this work, we explore a variant of NDEs called Neural Controlled Differential Equations (NCDEs) [9] that we extend to explicitly model how control dynamics laws guide the evolution of robot states. Furthermore, we adopt a curriculum learning strategy to progressively train our continuous-time model over longer horizons while maintaining stability.\n\nOur primary contributions can be summarized as follows:\n\nWe develop a Physics-Informed Neural Controlled Differential Equations approach (PINCoDE) that is capable of accurate motion forecasting over 1-4 minute horizons for multiple robots, conditioned on goals.\n\nWe introduce physics-informed constraints into the network training and show that it results in significantly improved performance over discrete-time baselines.\n\nA simple but effective strategy for scaling our method is devised to adapt from our primary experimental space of 10 robots to a larger space of 100 robots operating in the same environment.\n\nA curriculum learning strategy that progressively trains on longer horizons with the PINCoDE model results in a 2.7×2.7\\times improvement over analytical models for motion prediction across a 4 minute horizon.\n\n1. We develop a Physics-Informed Neural Controlled Differential Equations approach (PINCoDE) that is capable of accurate motion forecasting over 1-4 minute horizons for multiple robots, conditioned on goals.\n\n2. We introduce physics-informed constraints into the network training and show that it results in significantly improved performance over discrete-time baselines.\n\n3. A simple but effective strategy for scaling our method is devised to adapt from our primary experimental space of 10 robots to a larger space of 100 robots operating in the same environment.\n\n4. A curriculum learning strategy that progressively trains on longer horizons with the PINCoDE model results in a 2.7×2.7\\times improvement over analytical models for motion prediction across a 4 minute horizon.",
            "llm_summary": "【论文的motivation是什么】  \n1. 多机器人系统的长时间运动预测面临非线性交互和预测误差累积的挑战。  \n2. 现有的离散时间方法在处理复杂动态时表现不佳，尤其是在突发事件和不规则数据情况下。  \n3. 需要一种能够在连续时间中建模多机器人动态的方法，以提高预测精度。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于物理信息的神经控制微分方程（PINCoDE）的方法，能够在1-4分钟的时间范围内准确预测多个机器人的运动。该方法结合了物理约束，利用连续时间的动态建模，显著提高了预测性能。通过从10个机器人扩展到100个机器人的实验，PINCoDE展示了良好的可扩展性，并在4分钟的预测中实现了2.7倍的误差减少。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts",
            "authors": "Linjin He,Xinda Qi,Dong Chen,Zhaojian Li,Xiaobo Tan",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00358",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00358",
            "arxiv_html_link": "https://arxiv.org/html/2510.00358v1",
            "abstract": "Soft snake robots offer remarkable flexibility and adaptability in complex environments, yet their control remains challenging due to highly nonlinear dynamics. Existing model-based and bio-inspired controllers rely on simplified assumptions that limit performance. Deep reinforcement learning (DRL) has recently emerged as a promising alternative, but online training is often impractical because of costly and potentially damaging real-world interactions. Offline RL provides a safer option by leveraging pre-collected datasets, but it suffers from distribution shift, which degrades generalization to unseen scenarios. To overcome this challenge, we propose DiSA-IQL (Distribution-Shift-Aware Implicit Q-Learning), an extension of IQL that incorporates robustness modulation by penalizing unreliable state–action pairs to mitigate distribution shift. We evaluate DiSA-IQL on goal-reaching tasks across two settings: in-distribution and out-of-distribution evaluation. Simulation results show that DiSA-IQL consistently outperforms baseline models, including Behavior Cloning (BC), Conservative Q-Learning (CQL), and vanilla IQL, achieving higher success rates, smoother trajectories, and improved robustness. The codes are open-sourced to support reproducibility and to facilitate further research in offline RL for soft robot control.",
            "introduction": "Soft robots have attracted wide research attention across various applications, such as fruit harvesting [1], medical surgery [2], and search-and-rescue operations [3]. Among them, soft snake robots have gained particular interest due to their unique locomotion modes, high flexibility, and adaptability to complex and cluttered environments [4, 5]. However, the control of such robots remains challenging due to their highly non-linear dynamics, large state-space dimensions, and limited or sparse feedback [6].\n\nTraditional control methods for soft robots rely on mathematical modeling of the robot’s kinematics and dynamics, often using simplified geometric or reduced-order models (e.g., piecewise constant curvature) to reduce complexity. However, these methods are highly sensitive to modeling errors and can be computationally intensive [7, 8]. Additionally, bio-inspired approaches, such as serpenoid curve–based controllers and gait-based methods derived from real snake locomotion, generate sinusoidal or wave-like patterns to achieve forward or lateral movement [9, 10]. While easier to implement and effective on flat terrain, these methods depend on simplified environmental assumptions and often lack robustness in complex or uncertain environments.\n\nRecently, reinforcement learning (RL) has emerged as a promising alternative for controlling soft robots. Unlike traditional model-based or bio-inspired methods, RL does not require explicit kinematic or dynamic models; instead, it learns control policies directly from interaction with the environment. This allows the robot to adapt to nonlinear dynamics, unmodeled friction, and uncertain terrains, thereby improving robustness and generalization in real-world scenarios [11, 12, 13, 14]. For instance, Graule et al. [11] introduced SoMoGym, a soft robot training library, where multiple RL algorithms were benchmarked on tasks such as planar block pushing, snake locomotion, and in-hand manipulation, demonstrating the potential of RL for effective soft robot control. In our recent study [14], we proposed Back-Stepping Experience Replay (BER), an augmentation strategy that generates reversed transitions to improve exploration efficiency. Applied to a soft snake robot, BER achieved a 100% success rate and a 48% faster average speed compared to state-of-the-art baselines. Despite these achievements, current online RL approaches often require extensive training interactions, making them sample-inefficient and difficult to deploy directly on physical soft robots [15, 16].\n\nOne promising direction is offline RL, which reduces reliance on real-time exploration by leveraging pre-collected datasets [17]. Beyond data efficiency, offline RL improves safety by avoiding excessive hardware wear, facilitates scalable policy training, and supports sim-to-real transfer by integrating simulated and real data [17, 18]. Recent studies have applied goal-conditioned offline RL to deformable objects [19] and combined learned dynamics models with RL in soft robot simulators [20]. However, applications to soft robots, and particularly to soft snake robots, remain largely unexplored. Moreover, offline RL faces challenges such as distribution shift, where the learned policy may exploit out-of-distribution actions not adequately represented in the training dataset, leading to suboptimal or unsafe behaviors [21, 22, 23]. To address these challenges, we propose a distribution shift–aware offline RL framework specifically designed for soft snake robots. The main contributions of this work are as follows:\n\nWe systematically evaluate and benchmark multiple classical offline RL algorithms, Behavior Cloning (BC), Conservative Q-Learning (CQL), and Implicit Q-Learning (IQL), on the locomotion and navigation tasks of a soft snake robot.\n\nWe enhance IQL with an out-of-distribution action suppression mechanism to mitigate distribution shift in soft robot control.\n\nWe validate the proposed algorithm across two different environments with varying levels of complexity and compared to baseline methods. Additionally, we open-source the implementation and training framework to facilitate reproducibility and to support future research in offline RL for soft robot control111Codes and simulator: https://github.com/hlj0908/DiSA-IQL-for-Soft-Robot-Control.\n\n1. We systematically evaluate and benchmark multiple classical offline RL algorithms, Behavior Cloning (BC), Conservative Q-Learning (CQL), and Implicit Q-Learning (IQL), on the locomotion and navigation tasks of a soft snake robot.\n\n2. We enhance IQL with an out-of-distribution action suppression mechanism to mitigate distribution shift in soft robot control.\n\n3. We validate the proposed algorithm across two different environments with varying levels of complexity and compared to baseline methods. Additionally, we open-source the implementation and training framework to facilitate reproducibility and to support future research in offline RL for soft robot control111Codes and simulator: https://github.com/hlj0908/DiSA-IQL-for-Soft-Robot-Control.",
            "llm_summary": "【论文的motivation是什么】  \n1. 软体蛇形机器人控制面临高度非线性动态的挑战。  \n2. 现有的模型基础和生物启发式控制方法在复杂环境中表现不佳。  \n3. 在线强化学习的训练成本高且难以在真实环境中应用。  \n4. 离线强化学习在处理分布转移时存在性能下降的问题。  \n\n【提出了什么创新的方法】  \n提出了DiSA-IQL（Distribution-Shift-Aware Implicit Q-Learning），通过对不可靠的状态-动作对施加惩罚来增强IQL，旨在缓解分布转移对软体机器人控制的影响。该方法在两种不同复杂度的环境中进行评估，结果显示DiSA-IQL在成功率、轨迹平滑性和鲁棒性方面均优于基线模型，包括行为克隆（BC）、保守Q学习（CQL）和普通IQL。通过开源代码，支持了离线强化学习在软体机器人控制领域的进一步研究。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Learning Human Reaching Optimality Principles from Minimal Observation Inverse Reinforcement Learning",
            "authors": "Sarmad Mehrdad,Maxime Sabbah,Vincent Bonnet,Ludovic Righetti",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00329",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00329",
            "arxiv_html_link": "https://arxiv.org/html/2510.00329v1",
            "abstract": "This paper investigates the application of Minimal Observation Inverse Reinforcement Learning (MO‑IRL) to model and predict human arm‑reaching movements with time‑varying cost weights. Using a planar two‑link biomechanical model and high‑resolution motion‑capture data from subjects performing a pointing task, we segment each trajectory into multiple phases and learn phase‑specific combinations of seven candidate cost functions. MO‑IRL iteratively refines cost weights by scaling observed and generated trajectories in the maximum entropy IRL formulation, greatly reducing the number of required demonstrations and convergence time compared to classical IRL approaches. Training on ten trials per posture yields average joint‑angle Root Mean Squared Errors (RMSE) of 6.4 deg and 5.6 deg for six‑ and eight‑segment weight divisions, respectively, versus 10.4 deg using a single static weight. Cross‑validation on remaining trials and, for the first time, inter‑subject validation on an unseen subject’s 20 trials, demonstrates comparable predictive accuracy, around 8 deg RMSE, indicating robust generalization. Learned weights emphasize joint acceleration minimization during movement onset and termination, aligning with smoothness principles observed in biological motion. These results suggest that MO‑IRL can efficiently uncover dynamic, subject‑independent cost structures underlying human motor control, with potential applications for humanoid robots.",
            "introduction": "Understanding the optimal principles underlying simple motions like human arm reaching is crucial for progress in both neuroscience and robotics. In neuroscience, these principles shed light on how the central nervous system plans and executes goal-directed movements under constraints such as muscle redundancy, sensory noise, and biomechanical limitations. Capturing these strategies helps elucidate motor control mechanisms and supports clinical rehabilitation by identifying deviations from optimality in pathological movements.\nIn robotics and human–robot interaction, modeling human reaching as an optimal control problem facilitates the design of bio-inspired controllers and predictive algorithms. This is especially valuable for humanoids, assistive robotics and prosthetics, where human-likeness and intent prediction are critical.\nBiological motion exhibits invariant properties despite the wide range of available motor strategies. Voluntary movements tend to follow consistent patterns, suggesting that the nervous system resolves motor redundancy by adhering to specific organizational principles, the so-called optimal weights.\nHowever, the precise link between cost functions and variables encoded by the central nervous system remains unclear. Moreover, the idea of a single universal cost function may be unrealistic.\nThe central nervous system might flexibly adjust cost weightings based on task demands [1], or even during the same task. For example, individuals may reduce velocity at the end of a reach to aim more accurately, while seeking overall speed. This suggests a balance between objective (task-related) and subjective (body-related) costs, which current models often fail to capture [2].\n\nA widely used framework for exploring the principles underlying motor control is optimal control theory, which makes the hypothesis that biological movements arise from the minimization of specific cost or loss functions. Numerous models based on this theory have been proposed [3, 4], many of which claim to replicate experimental data with reasonable accuracy. However, these models often rely on a single cost function per task, which may not adequately capture the complexity and variability of human motion. As a result, relatively high Root Mean Square Errors (RMSEs) are frequently observed between the predicted and measured trajectories. For instance, Sylla et al. [4] reported an average RMSE of 77deg, with some angles exhibiting errors superior to 1515deg, even for simple reaching movements. Moreover, many studies unfortunately do not report any quantitative comparison or use tailored metrics [3] between their model predictions and experimental data. This raises questions about the relevance and predictive power of such models, especially when considering the sensitivity of their outcomes to variations in the chosen cost function components.\n\nUnfortunately, because of the current limitation of Inverse Optimal Control (IOC) and Inverse Reinforcement Learning (IRL) methods used to retrieve optimal cost function weights from human optimal motion, a single set of parameters for a given task is generally used [5].\nIndeed, adding time-varying weights leads to a significant increase in the number of parameters to be identified which these algorithms struggle to handle. In this paper, we leverage a new\nefficient IRL algorithm to instead study how time-varying weights lead to a more nuanced and accurate description of the movement.\n\nIOC provides a model-based framework for inferring cost function weights that best explain observed human motion, assuming that the motion is optimal for some performance criterion [5]. Despite its conceptual appeal, practical applications of IOC face significant challenges. The standard bi-level formulation, in which cost weights are optimized through repeated solutions of a nested optimal control problem, is computationally expensive, often requiring several days of computation. Additionally, this approach is prone to convergence to local minima, particularly in high-dimensional problems.\nTo address these issues, alternative formulations based on the residuals of the Karush-Kuhn-Tucker (KKT) conditions have been proposed. These methods aim to eliminate the need for repeated trajectory optimization. However, they remain highly sensitive to measurement noise and modeling errors commonly observed in human motion data [6, 7]. More recently, promising hybrid approaches that combine elements of the bi-level and residual-based formulations have been introduced [8], although these have only been validated in simulation.\n\nIn contrast to IOC, IRL adopts a probabilistic framework to infer the underlying cost function. This approach relaxes the number of bi-level-like iterations and is especially appealing for tasks involving uncertainty and variability, such as those performed by humans. IRL defines a probability distribution over all demonstrations and seeks to identify the cost function that maximizes the likelihood of the optimal trajectories. Given this definition, IRL ideally requires all the possible trajectories for the utmost optimal cost function derivation, which is impossible. Hence, IRL’s performance is heavily hinged on the trajectory space approximation accuracy obtained from a finite set of observations. There have been several efforts to circumvent this shortcoming by approximating the trajectory set through more intelligent sampling around the optimal trajectory [9, 10] and trajectory set augmentation [11].\n\nHowever, the sampled trajectories often lie close to the observed ones and may not sufficiently explore the broader trajectory space. Furthermore, in order to improve cost function estimation, IRL must consider all sampled and iteratively generated trajectories in the probability maximization process. This requirement substantially increases the computational cost of IRL. To address these shortcomings, we turn to the newly proposed Minimal Observation Inverse Reinforcement Learning (MO-IRL) [12]. MO-IRL takes an iterative approach for cost function estimation, by approximating the trajectory space through scaling the effectiveness of each observed trajectory depending on its current estimate of optimality. With this added feature, even with a small observation set, MO-IRL empirically provides better iterates that lead to an improved estimation of the weights. This leads to iterative cost function learning with minimal information about the trajectory space, resulting in considerably faster convergence. To our knowledge, MO-IRL was designed and tested only with robotics tasks and fixed weights.\n\nIn this paper, we extend MO-IRL to learn tasks requiring time-varying cost weights and investigate its use in predicting accurate human joint trajectories by learning simultaneously from positions and velocities. The proposed approach is validated with a subset of reference human data from the human motor control community [3]. In particular, we show that the method can learn task weights leading to accurate movement reproduction that also generalize across movements.",
            "llm_summary": "【论文的motivation是什么】  \n1. 理解人类手臂运动的最优原则对神经科学和机器人技术的进展至关重要。  \n2. 现有模型通常依赖单一成本函数，无法充分捕捉人类运动的复杂性和变异性。  \n3. 传统的逆最优控制方法在提取最优成本函数权重时面临计算复杂性和局部最优解的问题。  \n\n【提出了什么创新的方法】  \n本研究提出了一种最小观察逆强化学习（MO-IRL）方法，能够通过最小化观察数据来学习时间变化的成本权重。该方法通过将观察到的轨迹进行缩放，迭代优化成本权重，从而显著减少所需演示数量和收敛时间。实验结果显示，使用MO-IRL方法，关节角度的均方根误差（RMSE）显著低于传统方法，表明其在动态、主体独立的成本结构学习方面的有效性。该方法的成功应用为人形机器人设计提供了新的思路。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "BC-MPPI: A Probabilistic Constraint Layer for Safe Model-Predictive Path-Integral Control",
            "authors": "Odichimnma Ezeji,Michael Ziegltrum,Giulio Turrisi,Tommaso Belvedere,Valerio Modugno",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00272",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00272",
            "arxiv_html_link": "https://arxiv.org/html/2510.00272v1",
            "abstract": "Model Predictive Path Integral (MPPI) control has recently emerged as a fast, gradient-free alternative to model-predictive control in highly non-linear robotic tasks, yet it offers no hard guarantees on constraint satisfaction. We introduce Bayesian-Constraints MPPI (BC-MPPI), a lightweight safety layer that attaches a probabilistic surrogate to every state and input constraint. At each re-planning step the surrogate returns the probability that a candidate trajectory is feasible; this joint probability scales the weight given to a candidate, automatically down-weighting rollouts likely to collide or exceed limits and pushing the sampling distribution toward the safe subset; no hand-tuned penalty\ncosts or explicit sample rejection required. We train the surrogate\nfrom 1,0001{,}000 offline simulations and deploy the controller on a\nquadrotor in MuJoCo with both static and moving obstacles. Across\nK∈[100,1500]K\\!\\in\\![100,1500] rollouts BC-MPPI preserves safety margins while satisfying the prescribed probability of violation. Because the surrogate is a stand-alone, version-controlled artefact and the runtime safety score is a single scalar, the approach integrates naturally with verification-and-validation pipelines for certifiable autonomous systems.",
            "introduction": "Model Predictive Control (MPC) was first developed in the process-control community [15] and has since become a workhorse in robotics, underpinning behaviors as diverse as mobile-robot navigation, manipulation, legged locomotion, and aerial acrobatics [5]. Its appeal lies in the explicit use of a predictive model to optimize a finite-horizon cost while respecting user-defined constraints at every step. Yet this very strength is also a weakness: effective deployment still depends on hand-crafting analytic cost terms and constraint sets that are differentiable, well-conditioned, and sufficiently rich to capture task objectives and safety limits. For highly nonlinear, contact-rich, or perception-driven tasks—where objectives may be implicit in sensor data or learned from experience—defining such functions becomes a substantial engineering burden and can limit MPC’s practicality in advanced robotic applications [21].\n\nStochastic sampling–based Model Predictive Path Integral (MPPI) control offers a compelling alternative. MPPI sidesteps gradient evaluations by estimating the path integral of cost along thousands of Monte-Carlo rollouts, allowing it to tackle highly nonlinear, non-convex dynamics and non-differentiable objectives. It has delivered state-of-the-art performance in aggressive autonomous driving [23, 22], agile quadrotor flight [11, 1], and contact-rich quadruped locomotion [2, 20]. The adoption of GPU-accelerated sampling and differentiable programming frameworks now enables sub-millisecond evaluation of tens of thousands of trajectories, bringing MPPI firmly into the real-time regime [20]. At the same time, techniques such as low pass filtering [6] and learned importance sample priors [4] mitigate the characteristic action noise of MPPI, reducing oscillations and improving closed-loop stability.\n\nHowever, where MPC excels in hard constraint satisfaction MPPI struggles. Its Monte-Carlo nature makes it difficult to guarantee that every sampled trajectory respects state and input limits, and naive penalty costs often lead to brittle tuning and constraint-violation outliers. Embedding a principled constraint-handling layer within MPPI, therefore, remains an open challenge, especially for safety-critical robotic applications that must certify collision-avoidance, torque limits, or contact-stability conditions in real time.\n\nIn this work we close that gap with Bayesian Constraints MPPI (BC-MPPI), a safety layer that learns a probabilistic description of task constraints and folds it directly into the MPPI sampling process. We represent each hard constraint with a Bayesian surrogate - a Bayesian neural network (BNN) - which returns both a mean estimate of constraint satisfaction and an epistemic uncertainty measure [16, 12]. At every control step, these surrogates reshape the MPPI proposal distribution: trajectories that venture into regions with high violation probability or high model uncertainty are exponentially down-weighted. At the same time, those that remain in the safe set are sampled more densely.\nWe validate BC-MPPI in a high-fidelity quadrotor simulator, where the drone must fly point-to-point while respecting static obstacles (fixed walls, ceiling, and floor) and dynamic constraints (moving no-fly zones and time-varying thrust limits). The experiments show that the learned Bayesian constraint layer steers the sampling toward safe rollouts, keeping the violation probability below\n1 without sacrificing trajectory optimality.",
            "llm_summary": "【论文的motivation是什么】  \n1. MPPI控制在满足约束方面缺乏硬性保证，限制了其在安全关键应用中的使用。  \n2. 在高度非线性和接触丰富的任务中，定义可微的成本和约束函数成为工程负担。  \n3. 现有的约束处理方法往往导致调优脆弱和约束违反的异常。  \n\n【提出了什么创新的方法】  \n提出了Bayesian Constraints MPPI (BC-MPPI)，通过为每个状态和输入约束引入一个概率性替代层，解决了MPPI在约束满足方面的不足。该方法使用贝叶斯神经网络（BNN）来学习任务约束的概率描述，并在每个控制步骤中重塑MPPI的提议分布。通过这种方式，BC-MPPI能够自动降低进入高违反概率区域的轨迹的权重，同时增加安全区域内轨迹的采样密度。实验结果表明，BC-MPPI能够在不牺牲轨迹最优性的情况下，将违反概率保持在1以下。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic Tasks",
            "authors": "Yue Meng,Fei Chen,Chuchu Fan",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00225",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00225",
            "arxiv_html_link": "https://arxiv.org/html/2510.00225v1",
            "abstract": "Learning control policies for complex, long-horizon tasks is a central challenge in robotics and autonomous systems. Signal Temporal Logic (STL) offers a powerful and expressive language for specifying such tasks, but its non-Markovian nature and inherent sparse reward make it difficult to be solved via standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus only on limited STL fragments or use STL robustness scores as sparse terminal rewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization, to solve general STL tasks. TGPO decomposes STL into timed subgoals and invariant constraints and provides a hierarchical framework to tackle the problem. The high-level component of TGPO proposes concrete time allocations for these subgoals, and the low-level time-conditioned policy learns to achieve the sequenced subgoals using a dense, stage-wise reward signal. During inference, we sample various time allocations and select the most promising assignment for the policy network to rollout the solution trajectory. To foster efficient policy learning for complex STL with multiple subgoals, we leverage the learned critic to guide the high-level temporal search via Metropolis-Hastings sampling, focusing exploration on temporally feasible solutions. We conduct experiments on five environments, ranging from low-dimensional navigation to manipulation, drone, and quadrupedal locomotion. Under a wide range of STL tasks, TGPO significantly outperforms state-of-the-art baselines (especially for high-dimensional and long-horizon cases), with an average of 31.6% improvement in task success rate compared to the best baseline. The code will be available at https://github.com/mengyuest/TGPO",
            "introduction": "Signal Temporal Logic (STL) is a powerful framework for specifying tasks with temporal and spatial constraints in real-world robotic applications. However, designing controllers to satisfy these specifications is difficult, especially for systems with complex dynamics and a long task horizon. While Reinforcement Learning (RL) excels in handling these dynamical systems, directly deploying RL for STL specifications poses significant challenges. The history-dependent nature of STL breaks the Markovian assumption for the common RL algorithms. Furthermore, the reward based on the STL satisfaction is extremely sparse for long-horizon tasks, making RL struggle to learn effectively.\n\nExisting model-free RL approaches for STL tasks typically leverage state augmentation with reward shaping. τ\\tau-MDP (Aksaray et al., 2016) encodes histories explicitly in the augmented spaces and F-MDP (Venkataraman et al., 2020) designs flags to bookkeep the satisfaction of STL subformulas. However, these techniques only work on limited STL fragments with up to two temporal layers. While model-based RL (Kapoor et al., 2020; He et al., 2024) has fewer restrictions on the STL formulas, learning the system (latent space) dynamics can be challenging, and the estimation error accumulates over long horizons. Additionally, the planning often relies on Monte Carlo Tree Search or sampling action sequences, which may not be tractable for high-dimensional systems.\n\nWe argue that the primary barrier for RL to efficiently solve STL tasks is the difficulty of designing a dense, stage-wise reward function. This challenge stems directly from the unspecified temporal variables governing the “reach”-type tasks in STL formulas, which prevents a direct decomposition of STL into a sequence of executable subgoals. For example, for an STL F[0,160]​A∧F[0,160]​BF_{[0,160]}A\\land F_{[0,160]}B (“Eventually reach AA and eventually reach BB within the time interval [0,160][0,160]”), the time assignments for reaching AA and reaching BB determine the order of visiting these regions. If we can ground the variables into concrete values (e.g., reach AA at 35, and reach BB at 120), the problem can be cast into a sequence of goal-reaching problems, which is much easier to solve by RL.\n\nInspired by this observation, we propose a hierarchical RL framework to solve STL tasks by iteratively conducting Temporal Grounding and Policy Optimization (TGPO). The high-level component assigns values for the time variables to form the sequenced subgoals, and the low-level time-conditioned policy learns to achieve the task guided by the dense, stage-wise rewards derived from these subgoals. To efficiently bind values for multiple time variables, we carry out a high-level temporal search with a critic that predicts STL satisfaction. A Metropolis–Hastings sampling is used to guide exploration toward more “promising” time allocations. During inference, we sample time variable assignments and evaluate them using the critic. The most promising schedule is then executed by the low-level policy to generate the final solution trajectory for the STL specification.\n\nWe conduct extensive experiments over five simulation environments, ranging from 2D linear dynamics to 29D Ant navigation tasks. Compared to other baselines, TGPO* (with Bayesian time variable sampling) achieves the highest overall task success rate. The performance gains are significant, especially in high-dimensional systems and long-horizon tasks. Furthermore, our time-conditioned design offers key benefits: our critic offers interpretability by identifying promising temporal plans, and the policy can generate diverse, multi-modal behaviors to satisfy a single STL specification.\n\nOur main contributions are summarized as follows: (1) Hierarchical RL-STL framework: To the best of our knowledge, we are the first to develop a hierarchical model-free RL algorithm capable of solving general, nested STL tasks over long horizons. (2) Critic-guided Bayesian sampling: We introduce a critic-guided temporal grounding mechanism that, together with STL decomposition, yields subgoals and invariant constraints. This mechanism constructs an augmented MDP with dense, stage-wise rewards and thus overcomes the sparse reward challenges that have hindered existing RL approaches. (3) Interpretability: By explicitly grounding subgoals and invariant constraints in the STL structure using critic-guided Bayesian sampling, our approach offers a more interpretable learning process, where progress can be directly traced to logical task components. (4) Complex dynamics and reproducibility: TGPO demonstrates strong performance over other baselines and fits for complex dynamics, which supports the effectiveness of the design. All the code (the algorithm, the simulations and STL tasks) will be open-sourced to advance STL planning.",
            "llm_summary": "【论文的motivation是什么】  \n1. 复杂长时间任务的控制策略学习是机器人和自主系统中的核心挑战。  \n2. Signal Temporal Logic (STL) 提供了强大的任务规范语言，但其非马尔可夫特性和稀疏奖励使得标准强化学习算法难以解决。  \n3. 现有的强化学习方法仅关注有限的STL片段或使用稀疏终端奖励，导致学习效率低下。  \n\n【提出了什么创新的方法】  \n提出了TGPO（Temporal Grounded Policy Optimization）方法，通过将STL分解为定时子目标和不变约束，构建了一个分层框架来解决一般STL任务。TGPO的高层组件为子目标提供具体的时间分配，而低层的时间条件策略则学习在密集的阶段性奖励信号下实现这些序列子目标。在推理过程中，我们对各种时间分配进行采样，并选择最有前景的分配供策略网络生成解决轨迹。实验结果表明，TGPO在五个环境下显著优于最先进的基线，尤其是在高维和长时间任务中，任务成功率平均提高了31.6%。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "A Systematic Study of Large Language Models for Task and Motion Planning With PDDLStream",
            "authors": "Jorge Mendez-Mendez",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00182",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00182",
            "arxiv_html_link": "https://arxiv.org/html/2510.00182v1",
            "abstract": "Using large language models (LLMs) to solve complex robotics problems requires understanding their planning capabilities. Yet while we know that LLMs can plan on some problems, the extent to which these planning capabilities cover the space of robotics tasks is unclear. One promising direction is to integrate the semantic knowledge of LLMs with the formal reasoning of task and motion planning (TAMP). However, the myriad of choices for how to integrate LLMs within TAMP complicates the design of such systems. We develop 16 algorithms that use Gemini 2.5 Flash to substitute key TAMP components. Our zero-shot experiments across 4 950 problems and three domains reveal that the Gemini-based planners exhibit lower success rates and higher planning times than their engineered counterparts. We show that providing geometric details increases the number of task-planning errors compared to pure PDDL descriptions, and that (faster) non-reasoning LLM variants outperform (slower) reasoning variants in most cases, since the TAMP system can direct the LLM to correct its mistakes.",
            "introduction": "Autonomous robots must handle a breadth of long-horizon problems in unstructured environments (e.g., Fig. 1). Hierarchical planning tackles this goal, with many recent efforts leveraging large pretrained models. The promise of large models—e.g., LLMs, vision-language models (VLMs), and vision-language-action models (VLAs)—is that their pretraining may have equipped them with key robotics capabilities, including building abstractions, finding steps that achieve a goal, and generalizing to unseen scenarios. We empirically study the ability of LLMs to produce plans for geometrically intricate problems expressed in PDDLStream [1].\n\nLarge models are promising candidates to constitute key components of future robotics solutions. Choices include acting as a perception system, converting vision or language into formal domain specifications, producing abstract plans, or producing low-level plans end-to-end. We can categorize these choices into making decisions (a planner) and constructing abstractions (a world model)—of course, the two may overlap. Among decision-making variants, LLM-Modulo approaches that use the LLM as a planner within a verification loop are intuitively appealing. This paper provides the first large-scale empirical answer to the question: Is the LLM-Modulo framework a viable strategy for TAMP?\n\nDecision-making variants face a great generalization challenge. Upon encountering a new problem, they must efficiently produce a correct plan, which may require action sequences that were unseen during pretraining (e.g., using a broomstick to collect an object under a sofa). To study whether current LLMs possess this generalization capability, we thoroughly evaluate 16 LLM-based planners on TAMP problems. Our main focus is on understanding the trade-offs imposed by various choices (e.g., reasoning or direct generation, geometric or task planning, more or less information in the prompt).\n\nOur LLM-based planners implement components of existing TAMP systems as interactions with Gemini 2.5 Flash [2]. The TAMP system determines when to execute task- or motion-level planning, and verifies plan correctness. For each of two base TAMP methods (adaptive [1] and bilevel [3]), we create eight LLM planners (four reasoning and four direct), each of which generates: integrated task plan or continuous parameters, task plan only, continuous parameters only, and independent task plan and continuous parameters. We conducted an extensive evaluation encompassing 4 950 problems across three TAMP domains with a time limit of 300 seconds per task. Our findings provide evidence that LLM-based planners can solve many TAMP problems, but have lower success rates and significantly higher planning times than the base planners. Our evaluation further dissects the causes for failure of the various LLM-based planners.",
            "llm_summary": "【论文的motivation是什么】  \n1. 了解大型语言模型（LLMs）在复杂机器人任务中的规划能力。  \n2. 探索将LLMs与任务和运动规划（TAMP）结合的潜力。  \n3. 评估LLM-Modulo框架在TAMP中的可行性。  \n\n【提出了什么创新的方法】  \n本文开发了16种算法，利用Gemini 2.5 Flash替代TAMP的关键组件。通过对4950个问题的零-shot实验，发现Gemini基础的规划器在成功率和规划时间上均低于传统工程方法。提供几何细节反而增加了任务规划错误，而非推理的LLM变体在大多数情况下优于推理变体。研究表明，LLM能够解决许多TAMP问题，但成功率较低且规划时间显著增加。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "RoboPilot: Generalizable Dynamic Robotic Manipulation with Dual-thinking Modes",
            "authors": "Xinyi Liu,Mohammadreza Fani Sani,Zewei Zhou,Julius Wirbel,Bahram Zarrin,Roberto Galeazzi",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00154",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00154",
            "arxiv_html_link": "https://arxiv.org/html/2510.00154v1",
            "abstract": "Despite rapid progress in autonomous robotics, executing complex or long-horizon tasks remains a fundamental challenge. Most current approaches follow an open-loop paradigm with limited reasoning and no feedback, resulting in poor robustness to environmental changes and severe error accumulation. We present RoboPilot, a dual-thinking closed-loop framework for robotic manipulation that supports adaptive reasoning for complex tasks in real-world dynamic environments. RoboPilot leverages primitive actions for structured task planning and flexible action generation, while introducing feedback to enable replanning from dynamic changes and execution errors. Chain-of-Thought reasoning further enhances high-level task planning and guides low-level action generation. The system dynamically switches between fast and slow thinking to balance efficiency and accuracy. To systematically evaluate the robustness of RoboPilot in diverse robot manipulation scenarios, we introduce RoboPilot-Bench, a benchmark spanning 21 tasks across 10 categories, including infeasible-task recognition and failure recovery. Experiments show that RoboPilot outperforms state-of-the-art baselines by 25.9% in task success rate, and the real-world deployment on an industrial robot further demonstrates its robustness in real-world settings.",
            "introduction": "General-purpose robots, also known as generalist robots, have emerged as a central focus in robotics research due to their potential to autonomously execute diverse tasks in unseen real-world environments [1]. Recent advances in large language models (LLMs) have demonstrated remarkable capabilities in mapping natural language instructions to robot planning and control [2, 3]. By transferring the extensive world knowledge and reasoning capability of the LLM models to the physical world, the integration of LLMs into robotic systems offers a powerful framework to streamline the planning and execution of robotic tasks. Despite this, real-world complex or long-horizon tasks remain challenging for robot manipulation [4, 5], as they require robust execution under dynamic conditions, and powerful and adaptive reasoning capability.\n\nThese requirements manifest as two fundamental challenges: 1) Static planning without closed feedback\nloop for dynamic changes. Previous work focused on static planning, where a single plan is generated at the beginning of the task without subsequent replanning [6, 7, 8]. This paradigm often leads to task failure due to accumulated errors or suboptimal plans, particularly in complex or long-horizon scenarios, and it cannot adapt to unexpected situations, such as execution failures or changes in object positions. 2) Lack of strong and adaptive reasoning capabilities to handle various complex and long-horizon tasks. Robots need to decompose problems into stepwise solutions and solve them with strong reasoning capability, yet early studies relied on single-pass action generation within LLMs [9, 10, 11], which limits adaptability and impedes replanning in dynamic environments.\n\nIn this work, we introduce RoboPilot, a dual-thinking closed-loop robotics system that solves real-world manipulation tasks in dynamic environments. In contrast to prior approaches that rely on elaborate prompt engineering with manipulation examples [6, 7, 12], our system uses action primitives as abstracted API functions with formal structures, breaking down complex manipulation tasks into high-level task planning and low-level action parameter generation.\nRather than statically generating actions [8, 13, 14], RoboPilot continuously monitors task progress, integrates environment feedback, and leverages historical messages to recover from dynamic changes and execution errors. Furthermore, to enhance reasoning over complex or long-horizon tasks, we incorporate Chain-of-Thought (CoT) reasoning to support complex computation and explicitly decouple high-level task planning from low-level action generation as separate steps. To avoid unnecessary reasoning in simple scenarios, we introduce an LLM-based thinking mode selector that chooses between a fast-thinking mode and a CoT-enhanced slow-thinking mode based on the task complexity.\n\nThe lack of benchmarks that evaluate robotic systems in dynamic and long-horizon tasks remains a critical challenge. Existing manipulation benchmarks prioritize object diversity [15] or task generalization [16], but fail to test for manipulation robustness in dynamic situations. To address this gap, we introduce a manipulation benchmark consisting of 10 groups for a total of 21 tasks, including long-horizon planning, infeasible objectives, and deliberately designed failure cases. Our experimental results, both in simulation and with the real-world robot, demonstrate that RoboPilot achieves substantial improvements over state-of-the-art (SOTA) baselines, exhibiting strong robustness in dynamic environments. In summary, our main contributions are:\n\nWe propose RoboPilot, a dual-thinking closed-loop system for dynamic manipulation, enabling replanning with adaptive dual-thinking modes for dynamic environments, particularly in complex or long-horizon tasks.\n\nWe propose RoboPilot, a dual-thinking closed-loop system for dynamic manipulation, enabling replanning with adaptive dual-thinking modes for dynamic environments, particularly in complex or long-horizon tasks.\n\nWe adopt primitive actions to structure the task planning and action generation, and introduce the feedback and replanning module to facilitate recovery from dynamic changes and errors. Chain-of-Thought reasoning is introduced in slow-thinking to enhance task planning and guide low-level action generation. RoboPilot can further dynamically switch between dual-thinking modes, balancing efficiency and accuracy.\n\nWe present RoboPilot-Bench, a comprehensive benchmark for robotics manipulation, covering infeasible task recognition and failure recovery for robustness testing.\n\nRoboPilot achieves an overall 25.9% improvement in success rate over state-of-the-art methods, and real-world deployment further demonstrates its robustness.\n\n1. We propose RoboPilot, a dual-thinking closed-loop system for dynamic manipulation, enabling replanning with adaptive dual-thinking modes for dynamic environments, particularly in complex or long-horizon tasks.\n\n2. We adopt primitive actions to structure the task planning and action generation, and introduce the feedback and replanning module to facilitate recovery from dynamic changes and errors. Chain-of-Thought reasoning is introduced in slow-thinking to enhance task planning and guide low-level action generation. RoboPilot can further dynamically switch between dual-thinking modes, balancing efficiency and accuracy.\n\n3. We present RoboPilot-Bench, a comprehensive benchmark for robotics manipulation, covering infeasible task recognition and failure recovery for robustness testing.\n\n4. RoboPilot achieves an overall 25.9% improvement in success rate over state-of-the-art methods, and real-world deployment further demonstrates its robustness.",
            "llm_summary": "【论文的motivation是什么】  \n1. 复杂或长时间任务的执行仍然是自主机器人面临的基本挑战。  \n2. 现有方法缺乏闭环反馈，导致对环境变化的适应性差。  \n3. 机器人需要强大且适应性强的推理能力来处理复杂任务。  \n\n【提出了什么创新的方法】  \n本文提出了RoboPilot，一个双思维闭环系统，专为动态环境中的机器人操作设计。该系统通过使用原始动作结构化任务规划和动作生成，结合反馈和重规划模块，能够从动态变化和执行错误中恢复。引入的Chain-of-Thought推理增强了高层次任务规划，并指导低层次动作生成。RoboPilot能够在快速思维模式和慢速思维模式之间动态切换，以平衡效率和准确性。此外，RoboPilot-Bench基准测试涵盖了不可能任务识别和失败恢复，系统评估其在动态环境中的鲁棒性。实验结果表明，RoboPilot在任务成功率上比现有最先进方法提高了25.9%。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Strategic Fusion of Vision Language Models: Shapley-Credited Context-Aware Dawid-Skene for Multi-Label Tasks in Autonomous Driving",
            "authors": "Yuxiang Feng,Keyang Zhang,Hassane Ouchouid,Ashwil Kaniamparambil,Ioannis Souflas,Panagiotis Angeloudis",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01126",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01126",
            "arxiv_html_link": "https://arxiv.org/html/2510.01126v1",
            "abstract": "Large vision–language models (VLMs) are increasingly used in autonomous-vehicle (AV) stacks, but hallucinations limit their reliability in safety-critical pipelines. We present Shapley-credited Context-Aware Dawid–Skene with Agreement, a game-theoretic fusion method for multi-label understanding of ego-view dashcam video. It learns per-model, per-label, context-conditioned reliabilities from labelled history and, at inference, converts each model’s report into an agreement-guardrailed log-likelihood ratio that is combined with a contextual prior and a public reputation state updated using Shapley-based team credit. The result is calibrated, thresholded posteriors that (i) amplify agreement among reliable models, (ii) preserve uniquely correct single-model signals, and (iii) adapt to drift. To specialise general VLMs, we curate 1,000 real-world dashcam clips with structured annotations (scene description, manoeuvre recommendation, rationale) using an automatic pipeline that fuses HDD ground-truth, vehicle kinematics, and YOLOv11 + BoT-SORT tracking, guided by a three-step chain-of-thought prompt; three heterogeneous VLMs are then fine-tuned with LoRA. We evaluate with Hamming distance, Micro-/Macro-F1, and average per-video latency. Empirically, the proposed method achieves a 23% reduction in Hamming distance, 55% improvement in Macro-F1, and 47% improvement in Micro-F1 when comparing with the best single model, demonstrating VLM fusion as a calibrated, interpretable, and robust decision-support mechanism in AV pipelines.",
            "introduction": "Following GPT-4’s release in 2023, large language models (LLMs) have shown strong performance in text processing and are now widely used as general-purpose assistants across professional and technical domains. Extending these capabilities, large vision–language models (VLMs) such as LLaVA [1] integrate visual understanding with language, enabling tasks such as image captioning and visual reasoning. These properties are particularly relevant to autonomous vehicles (AVs), where multimodal comprehension, human–machine interaction, and explainability are crucial, and have driven integration of LLMs/VLMs into both modular and end-to-end AV architectures [2].\n\nIn modular pipelines, VLMs enhance scene understanding and contextual reasoning by capturing nuanced interactions, recognising rare or anomalous events, and inferring latent intentions often missed by conventional perception [3, 4]. Pre-training on large multimodal data supports perception and high-level decision-making, allowing external knowledge, common-sense reasoning, and natural-language instructions to inform planning and improve safety, adaptability, and efficiency [5, 6]. For trajectory prediction, VLMs contribute semantic awareness and temporal coherence that aid forecasting of other agents’ motions [7, 8]. In end-to-end systems, VLMs have also been used as core components that map raw sensor inputs and textual prompts directly to driving commands, enabling interpretable, goal-directed behaviour across diverse conditions [9, 10].\n\nDespite rapid progress, VLM reliability in safety-critical AV applications remains a subject of debate, primarily due to hallucinations. Modern VLMs can misinterpret complex driving scenes or infer actions from incomplete or ambiguous inputs, producing confident but incorrect outputs. While techniques such as reinforcement learning from human feedback [11] and causal reasoning [12] have been explored to mitigate hallucinations, studies that directly target VLM-specific hallucinations in the AV context are still limited. As a result, VLMs are typically relegated to auxiliary roles rather than integrated into safety-critical decision pipelines, and their readiness for real-world deployment remains an open research challenge.\n\nTo address this challenge, we introduce a game-theoretic fusion method, Shapley-credited Context-Aware Dawid–Skene with Agreement, for multi-label understanding of ego-view dashcam video. Three heterogeneous VLMs each propose a subset of labels from a fixed ontology; from historical labelled rounds we learn per-model, per-label, context-conditioned reliabilities via kernel-weighted Beta-Bernoulli pooling on embedding neighbours, yielding true/false-positive rates as functions of context. During inference, each report is converted to a log-likelihood ratio (LLR), adjusted using pairwise error correlations to guard against double-counting, and weighted by a public reputation state that evolves with Shapley-based team credit. Combined with a contextual prior (from top-K nearest labelled contexts), these signals produce calibrated posterior probabilities per label. This design (i) amplifies agreement among reliable models, (ii) preserves uniquely correct labels proposed by a trusted model, and (iii) adapts over time as performance drifts. Framed as a repeated forecasting game with proper-scoring incentives, the proposed method is interpretable, data-efficient, and yields precise, thresholded probabilities for decision-making.\n\nThe main contributions of this study are summarised as follows:\n\nGame-theoretic fusion for AV decision-making. We propose a repeated forecasting framework that operates on ego-view dashcam videos and aggregates three heterogeneous VLMs via context-aware reliabilities (kernel–weighted Beta–Bernoulli on CLIP-embedding neighbours), agreement-weighted log-likelihood ratios with a correlation guardrail, and a contextual prior; calibrated probabilities drive predictions, while Shapley-based team credit and reputation dynamics mitigate hallucinations and adapt trust over time.\n\nAutomatic, temporally coherent annotation pipeline. We build an annotation system that fuses HDD ground-truth with vehicle kinematics and object-level cues from YOLOv11 + BoT-SORT, orchestrated by a three-step LLaMA-3.2 Chain-of-Thought (scenario description, manoeuvre recommendation, and rationale). This produces structured, time-consistent scene interpretations; 1,000 training clips are generated and manually reviewed for quality.\n\nTask-specialised VLMs and end-to-end evaluation. Three general-purpose VLMs are adapted to driving via LoRA fine-tuning on the curated data, yielding models that emit manoeuvre recommendations with explicit kinematic cues and interpretable CoT rationales. We report accuracy (Hamming distance, Micro-/Macro-F1) and system performance (average latency) to characterise both effectiveness and efficiency.",
            "llm_summary": "【论文的motivation是什么】  \n1. VLMs在安全关键的自动驾驶应用中的可靠性受到质疑，主要由于幻觉问题。  \n2. 现有技术对VLM特定幻觉的直接研究仍然有限，导致其在决策管道中的应用受到限制。  \n\n【提出了什么创新的方法】  \n提出了一种游戏理论融合方法——Shapley-credited Context-Aware Dawid–Skene with Agreement，专注于多标签理解的自我视角行车记录仪视频。该方法通过历史标注学习每个模型的上下文条件可靠性，并在推理时将每个模型的报告转换为经过协议保护的对数似然比，结合上下文先验和基于Shapley的团队信用更新的公共声誉状态。结果是经过校准的后验概率，能够放大可靠模型之间的共识，保留唯一正确的单模型信号，并适应性能漂移。实验表明，该方法在Hamming距离上减少了23%，在Macro-F1和Micro-F1上分别提高了55%和47%，展示了VLM融合在自动驾驶管道中的可校准、可解释和稳健的决策支持机制。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Predictive Control Barrier Functions for Discrete-Time Linear Systems with Unmodeled Delays",
            "authors": "Juan Augusto Paredes Salazar,James Usevitch,Ankit Goel",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO); Optimization and Control (math.OC)",
            "comment": "submitted to ACC 2026",
            "pdf_link": "https://arxiv.org/pdf/2510.01059",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01059",
            "arxiv_html_link": "https://arxiv.org/html/2510.01059v1",
            "abstract": "This paper introduces a predictive control barrier function (PCBF) framework for enforcing state constraints in discrete-time systems with unknown relative degree, which can be caused by input delays or unmodeled input dynamics.\nExisting discrete-time CBF formulations typically require the construction of auxiliary barrier functions when the relative degree is greater than one, which complicates implementation and may yield conservative safe sets.\nThe proposed PCBF framework addresses this challenge by extending the prediction horizon to construct a CBF for an associated system with relative degree one.\nAs a result, the superlevel set of the PCBF coincides with the safe set, simplifying constraint enforcement and eliminating the need for auxiliary functions.\nThe effectiveness of the proposed method is demonstrated on a discrete-time double integrator with input delay and a bicopter system with position constraints.",
            "introduction": "Control barrier functions (CBFs) have emerged as a powerful framework for enforcing safety in control systems by guaranteeing the forward invariance of a prescribed safe set.\nCBFs are grounded in Nagumo’s forward invariance theorem, which characterizes the conditions under which trajectories of an ordinary differential equation remain inside a given set [1].\nBy ensuring that Nagumo’s condition holds, CBFs enforce state constraints and yield a safety filter that modifies the nominal control input to maintain constraint satisfaction.\nSeveral variants of CBFs have been proposed in the literature, including zeroing CBFs, higher-order CBFs, and related formulations\n[2, 3, 4].\nThese methods provide a systematic framework for constraint satisfaction, often through the solution of constrained optimization problems.\n\nHowever, most CBF formulations are developed in continuous time, which necessitates discretization for practical implementation in digital control systems.\nSince safety constraints are only evaluated at discrete sampling instants, constraint violations may occur between updates.\nMoreover, discrete enforcement of CBF conditions can lead to aggressive corrective actions when a constraint is about to be violated. These limitations have motivated several extensions, including sampled-data CBFs\n[5, 6, 7],\nrobust formulations that account for inter-sample effects\n[8, 9, 10, 11],\nand event-triggered or self-triggered update mechanisms\n[12, 13, 14].\nTo directly address discrete dynamics, several recent works have focused on discrete-time CBF (DT-CBF) formulations [15, 16, 17].\nGiven that model predictive control (MPC) is inherently implemented in discrete time, DT-CBFs have been widely employed to enhance constraint enforcement in MPC frameworks [18, 19, 20, 21, 22, 23, 24].\n\nA central challenge in designing safety filters and ensuring the forward invariance of a desired safe set lies in the relative degree of the constraint function with respect to the control input.\nWhen the relative degree is greater than one, it is necessary to construct a sequence of auxiliary barrier functions equal in number to the relative degree, with forward invariance guaranteed only for the intersection of the superlevel sets of these auxiliary functions.\nHowever, the resulting safe set is often challenging to compute explicitly and may be overly conservative or impractical for control design.\n\nThis paper is focused on the problem of designing safety filters for discrete-time systems with unknown relative degree arising from input delays [25, 26] or unmodeled input dynamics [27, 28, 11].\nThe main contribution is the development of a predictive CBF (PCBF) framework.\nBy extending the prediction horizon, the proposed approach constructs a CBF for an associated system with relative degree one, thereby eliminating the need to design auxiliary barrier functions that are typically required in the high relative-degree case.\nFurthermore, reducing the relative degree to one ensures that the superlevel set of the control barrier function directly defines the safe set.\n\nThe contents of this paper are as follows.\nSection II briefly reviews the sampled-data feedback control problem for a continuous-time dynamic system and introduces the CBF operation in the control loop.\nSection III presents the formulation of a CBF for DT systems with high relative degree.\nSection IV presents the formulation of the PCBF framework, which extends the formulation in Section III to the case where the DT system dynamics are linear, allowing for the evaluation of CBF conditions over a prediction horizon.\nSection V presents examples that illustrate the performance of the proposed PCBF algorithm and its effectiveness at enforcing state constraints.\nFinally, the paper concludes with a summary in Section VI.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的离散时间控制障碍函数（CBF）在相对度数大于一时需要辅助函数，导致实现复杂。  \n2. 离散时间CBF的条件评估仅在采样时刻，可能导致约束违反。  \n3. 高相对度情况下的安全集计算困难且可能过于保守。  \n\n【提出了什么创新的方法】  \n提出了一种预测控制障碍函数（PCBF）框架，通过扩展预测视野来构建相对度为一的CBF，从而消除了高相对度情况下对辅助障碍函数的需求。该方法简化了约束执行，确保了控制障碍函数的超水平集直接定义安全集。通过在具有输入延迟的离散时间双积分器和带位置约束的双旋翼系统上的实验，验证了该方法在执行状态约束方面的有效性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "KeySG: Hierarchical Keyframe-Based 3D Scene Graphs",
            "authors": "Abdelrhman Werby,Dennis Rotondi,Fabio Scaparro,Kai O. Arras",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01049",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01049",
            "arxiv_html_link": "https://arxiv.org/html/2510.01049v1",
            "abstract": "In recent years, 3D scene graphs have emerged as a powerful world representation, offering both geometric accuracy and semantic richness.\nCombining 3D scene graphs with large language models enables robots to reason, plan, and navigate in complex human-centered environments.\nHowever, current approaches for constructing 3D scene graphs are semantically limited to a predefined set of relationships, and their serialization in large environments can easily exceed an LLM’s context window.\nWe introduce KeySG, a framework that represents 3D scenes as a hierarchical graph consisting of floors, rooms, objects, and functional elements, where nodes are augmented with multi-modal information extracted from keyframes selected to optimize geometric and visual coverage.\nThe keyframes allow us to efficiently leverage VLM to extract scene information, alleviating the need to explicitly model relationship edges between objects, enabling more general, task-agnostic reasoning and planning. Our approach can process complex and ambiguous queries while mitigating the scalability issues associated with large scene graphs by utilizing a hierarchical retrieval-augmented generation (RAG) pipeline to extract relevant context from the graph.\nEvaluated across four distinct benchmarks –including 3D object segmentation and complex query retrieval– KeySG outperforms prior approaches on most metrics, demonstrating its superior semantic richness and efficiency.",
            "introduction": "A long-standing goal in robotics is to create autonomous agents that can operate effectively in human-centered environments such as homes or offices.\nThese environments are characterized by high object density, semantic richness, and a variety of potential tasks.\nA key challenge for this goal is the development of a 3D world representation that is simultaneously detailed for precise manipulation and abstract enough for high-level reasoning and long-horizon planning.\n\n3D scene graphs (3DSGs) [1, 2, 3, 4, 5] have gained significant attention as a powerful representation to address the limitations of purely geometric maps. By modeling the world as a graph where nodes represent entities and edges represent their relationships, 3DSGs impose a structure on raw perception, explicitly linking geometry to semantics.\n\nHowever, current 3D scene graph approaches have two main limitations: first, they are restricted to a predefined set of geometric or semantic relationships, reducing the diversity of tasks and queries they can support.\nFor instance, a 3DSG [6] with edges representing spatial relationships between objects and places would excel in locating objects in large buildings. In contrast, a 3DSG [7] encoding functional relationships would be well suited for tasks that require understanding how functional elements control their objects (e.g, “turn off the oven,\" where knowing the relationship between the knob and the oven is necessary). A 3D scene graph designed with a predefined set of relationships for a specific task is inherently suboptimal for others.\n\nSecond, scalability is a major bottleneck. 3D scene graphs are often paired with large language models (LLMs), serving as a persistent world model that the LLM uses for planning and high-level reasoning. However, providing a complete, detailed scene graph of a large-scale environment, such as a multi-story office building, directly to an LLM can exceed the context window limits of even the most advanced models.\nEven if the graph fits within the context window, LLMs suffer from attentional biases and a “lost in the middle\" problem  [8], where performance degrades as the model is distracted by the vast amount of task-irrelevant information present in the prompt. This makes it challenging for the LLM to identify the crucial entities and environmental states necessary for robust reasoning and planning.\n\nTo this end, we present the Hierarchical KeyFrame-Based 3D Scene Graphs (KeySG), a novel framework that resolves the semantic and scalability dilemma by augmenting the 3D scene graph with multi-modal contextual information, implicitly capturing the geometry, semantics, affordances, and states of objects. Our key idea is to sample keyframes that ensure comprehensive visual coverage of each room in the environment. A Vision-Language Model (VLM) then generates a detailed description for each keyframe. To address scalability, these descriptions are recursively summarized into concise textual overviews for rooms and, subsequently, entire floors. This hierarchy is queried using a multi-modal retrieval-augmented generation (RAG) pipeline, which ensures that only the most relevant information is retrieved and provided to the LLM planner, enabling efficient and accurate reasoning.\n\nIn summary, we make the following contributions:\n\nWe introduce KeyFrame-Based 3DSGs (KeySG), the first 3D Scene Graph framework to model environments across five hierarchical levels of abstraction: buildings, floors, rooms, objects, and functional elements.\n\nWe propose a new pipeline to augment 3DSGs with multi-modal context from keyframes, featuring hierarchical scene summarization, and a RAG-based retrieval mechanism that efficiently provides task-relevant context to LLMs.\n\nWe conduct a comprehensive evaluation of KeySG across diverse benchmarks, including open-vocabulary 3D segmentation (Replica [9]), functional element segmentation (FunGraph3D [10]), and 3D object grounding from natural language queries (Habitat Matterport [11], Nr3D [12]).\n\n1. We introduce KeyFrame-Based 3DSGs (KeySG), the first 3D Scene Graph framework to model environments across five hierarchical levels of abstraction: buildings, floors, rooms, objects, and functional elements.\n\n2. We propose a new pipeline to augment 3DSGs with multi-modal context from keyframes, featuring hierarchical scene summarization, and a RAG-based retrieval mechanism that efficiently provides task-relevant context to LLMs.\n\n3. We conduct a comprehensive evaluation of KeySG across diverse benchmarks, including open-vocabulary 3D segmentation (Replica [9]), functional element segmentation (FunGraph3D [10]), and 3D object grounding from natural language queries (Habitat Matterport [11], Nr3D [12]).",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的3D场景图在语义上受限于预定义的关系，无法支持多样化的任务和查询。  \n2. 3D场景图在大规模环境中的可扩展性存在瓶颈，容易超出大型语言模型的上下文窗口。  \n\n【提出了什么创新的方法】  \n提出了KeySG框架，通过层次化的关键帧图来表示3D场景，包含建筑、楼层、房间、物体和功能元素五个层次。该方法利用关键帧提取的多模态信息，避免显式建模对象之间的关系边，支持更广泛的任务和查询。通过递归总结和RAG机制，KeySG有效地从图中提取相关上下文，提升了推理和规划的效率。评估结果显示，KeySG在多个基准测试中超越了之前的方法，展现了更优的语义丰富性和效率。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Conflict-Based Search as a Protocol: A Multi-Agent Motion Planning Protocol for Heterogeneous Agents, Solvers, and Independent Tasks",
            "authors": "Rishi Veerapaneni,Alvin Tang,Haodong He,Sophia Zhao,Viraj Shah,Yidai Cen,Ziteng Ji,Gabriel Olin,Jon Arrizabalaga,Yorai Shaoul,Jiaoyang Li,Maxim Likhachev",
            "subjects": "Multiagent Systems (cs.MA); Robotics (cs.RO)",
            "comment": "Project webpage:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.00425",
            "code": "https://rishi-v.github.io/CBS-Protocol/",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00425",
            "arxiv_html_link": "https://arxiv.org/html/2510.00425v1",
            "abstract": "Imagine the future construction site, hospital, office, or even sophisticated household with dozens of robots bought from different manufacturers. How can we enable these different systems to effectively move in a shared environment, given that each robot may have its own independent motion planning system?\nThis work shows how we can get efficient collision-free movements between algorithmically heterogeneous agents by using Conflict-Based Search (Sharon et al. 2015) as a protocol.\nAt its core, the CBS Protocol requires one specific single-agent motion planning API; finding a collision-free path that satisfies certain space-time constraints. Given such an API, CBS uses a central planner to find collision-free paths - independent of how the API is implemented.\nWe show how this protocol enables multi-agent motion planning for a heterogeneous team of agents completing independent tasks with a variety of single-agent planners including: Heuristic Search (e.g., A*), Sampling Based Search (e.g., RRT), Optimization (e.g., Direct Collocation), Diffusion, and Reinforcement Learning.",
            "introduction": "The rapid progress of robotics technology is lowering robots’ costs while enhancing capabilities, leading to a future in which multi-agent robotic systems will be ubiquitous.\nSuch systems may operate across a wide range of domains, including construction, manufacturing, office environments, and even sophisticated household settings. Importantly, these multi-agent teams are unlikely to be homogeneous; rather, they will consist of robots produced by different manufacturers, each performing independent tasks. This raises a fundamental question: how can heterogeneous robots, each potentially equipped with its own proprietary motion planning system, coordinate their movements in a shared environment?\n\nIn particular, such robots may:\n\nExhibit different embodiments and kinodynamic constraints,\n\nPursue distinct and independent tasks, and\n\nBe developed by different companies, each with proprietary motion planning solvers.\n\nWhile this may seem far-fetched, it is already beginning to appear in practice. Current automated warehouses and manufacturing facilities often integrate diverse platforms such as forklifts, trucks, and mobile carts, which must all operate within the same workspace. However, it remains unclear how to enable such heterogeneous robots to navigate and coordinate effectively in a common environment.\n\nTo that end, our main contribution is showing how we can use Conflict-Based Search as a protocol to enable efficient collision-free movement.\nConflict-Based Search (CBS) [2] is a foundational method from Multi-Agent Path Finding [3] that finds collision-free multi-agent paths.\nAt its core, we view CBS as defining a protocol that requires one specific single-agent motion planning API; finding a path that avoids space-time locations. Given such an API, CBS uses a central planner to resolve collision between agents - independent of how the API is implemented. Thus, “CBS as a protocol” enables collision-free multi-agent motion planning using different single-agent planners. We demonstrate the effectiveness of this protocol by showing how we can effectively plan for a heterogeneous team with a variety of single-agent planners including: Heuristic Search (e.g., A*), Sampling Based Search (e.g., RRT), Optimization (e.g., Direct Collocation), Diffusion, and Reinforcement Learning. Additionally, the CBS Protocol enables each agent to complete different independent tasks (e.g., coverage, surveillance) instead of just start-goal tasks.\n\nTaking a step back, to our knowledge the problem of Multi-Agent Motion Planning (MAMP) with algorithmically heterogeneous solvers has not been explored by the MAMP community (discussed more in Section II-B). Thus, our first contribution is to bring attention to this problem which we term Algorithmically Heterogeneous MAMP (AH-MAMP).\n\nSecond, the CBS Protocol bridges a gap between single-agent motion planning, multi-agent motion planning, and Multi-Agent Path Finding (MAPF). In particular, our CBS Protocol solution takes existing technology (i.e., existing single-agent solvers and existing MAPF techniques) and shows how they can be put together in a novel way.\nThus, our paper is a proof-of-concept paper whose primary contributions are shaped by the reader’s context:\n\n(1) Readers focused on single-agent motion planning but not multi-agent motion planning; For these readers, the key contribution of this paper is showing how we can use their single-agent motion planners in multi-agent systems through the CBS Protocol via defining one single API. The hope is that the protocol more easily enables single-agent motion planning researchers to incorporate their planners in multi-agent systems.\n\n(2) Readers working on real-world MAMP but are unfamiliar with CBS, or only familiar with CBS in its classical (gridworld) context; For these readers, the primary contribution is showing how the CBS Protocol extends beyond gridworld and can incorporate a wide range of diverse motion planners (e.g., optimization solvers) for realistic kinodynamic motion planning.\n\n(3) Readers familiar with CBS / MAPF and its extensions past non-gridworld; For these readers, the main contribution is the protocol formulation using heterogeneous solvers at once as well as some intricacies of the single-agent planners in the CBS context (e.g., using an RL policy).",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要协调不同制造商的异构机器人在共享环境中有效移动。  \n2. 现有的多代理运动规划方法未能解决算法异构的多代理运动规划问题。  \n3. 机器人在执行独立任务时可能面临碰撞问题。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于冲突搜索（CBS）协议的方法，允许异构机器人在共享环境中进行有效的无碰撞运动规划。该方法通过定义一个特定的单代理运动规划API，使得中央规划器能够独立于API的实现，解决代理之间的碰撞问题。通过这种方式，CBS协议能够支持多种单代理规划器（如启发式搜索、采样基搜索、优化等），并允许每个代理完成不同的独立任务。实验结果表明，该协议有效地实现了异构代理的协调运动规划，展示了其在实际应用中的潜力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations",
            "authors": "Jiayi Liu,Jiaming Zhou,Ke Ye,Kun-Yu Lin,Allan Wang,Junwei Liang",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00405",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00405",
            "arxiv_html_link": "https://arxiv.org/html/2510.00405v1",
            "abstract": "Reliable trajectory prediction from an ego-centric perspective is crucial for robotic navigation in human-centric environments. However, existing methods typically assume idealized observation histories, failing to account for the perceptual artifacts inherent in first-person vision, such as occlusions, ID switches, and tracking drift.\nThis discrepancy between training assumptions and deployment reality severely limits model robustness.\nTo bridge this gap, we introduce EgoTraj-Bench, the first real-world benchmark that grounds noisy, first-person visual histories in clean, bird’s-eye-view future trajectories, enabling robust learning under realistic perceptual constraints.\nBuilding on this benchmark, we propose BiFlow, a dual-stream flow matching model that concurrently denoises historical observations and forecasts future motion by leveraging a shared latent representation.\nTo better model agent intent, BiFlow incorporates our EgoAnchor mechanism, which conditions the prediction decoder on distilled historical features via feature modulation.\nExtensive experiments show that BiFlow achieves state-of-the-art performance, reducing minADE and minFDE by 10–15% on average and demonstrating superior robustness. We anticipate that our benchmark and model will provide a critical foundation for developing trajectory forecasting systems truly resilient to the challenges of real-world, ego-centric perception.",
            "introduction": "Pedestrian trajectory prediction[1, 2], aiming to estimate the multimodal future paths of agents in dynamic environments, serves as a foundation for safe, socially compliant motion planning in autonomous systems such as mobile robots, intelligent prosthetics, and service vehicles[3, 4, 5, 6, 7, 8]. Although extensively studied, most existing methods are developed and evaluated under idealized bird’s-eye view (BEV) settings with globally consistent observations and flawless agent tracking[2]. However, these conditions rarely hold in real-world deployment. Autonomous agents, such as mobile robots, typically perceive the environment through front-facing cameras, where observations are inherently incomplete and noisy as illustrated in Fig. 1: pedestrians may be occluded, enter or exit the field of view (FOV), or suffer from physical sensing errors such as perspective distortion. These imperfections substantially violate the idealized historical assumptions in BEV settings. Therefore, trajectory prediction under ego-centric (first-person view, FPV) noisy observations is essential for enabling robust deployment in real-world scenarios.\n\nSome prior work [9, 6, 10] predicts trajectories from ego-centric input, typically predicting future positions in image space, e.g., bounding boxes or keypoints. Although operating from an ego-centric view, these methods lack spatial reasoning in real-world space and assume idealized tracking results in image space, leaving unresolved the modeling of fine-grained interaction in real-world trajectory prediction. In contrast, methods[11, 12] that predict trajectories in global metric spaces (e.g., world coordinates) enable precise spatial reasoning about proximity, collision risk, and social norms, and we therefore focus on the latter paradigm.\n\nIn addition, some studies [13, 14] simulate ego-centric conditions by rendering BEV data into synthetic views using simulators. While this approximates the visual input of moving entities, the rule-based agent motion and simplified rendering in the simulator fail to capture the intricate and subtle motion patterns and visual nuances present in authentic scenes. Moreover, the utilized BEV data[15, 16] are collected in open and uncluttered environments such as streets with few static obstacles, resulting in overly clean inputs that do not reflect the perception challenges of dense, interactive environments. These limitations highlight the critical need for a real-world benchmark for robust trajectory prediction with ego-centric noisy observations.\n\nTo this end, we introduce EgoTraj-Bench, the first real-world benchmark for trajectory prediction under ego-centric noise. Built upon the TBD dataset[17], EgoTraj-Bench first derives historical trajectories with noise from real ego-view videos, capturing deployment-realistic imperfections such as occlusions, mis-tracked IDs, FOV truncations, and perspective distortions. Furthermore, the observed ego-centric trajectory with noise is projected into world coordinates and paired with the corresponding clean, human-verified future trajectory from the BEV view, ensuring metric-consistent supervision while preserving the realism of ego-centric input conditions. This practice can transfer the disturbance from the ego-view noise to the widely used BEV-based trajectory prediction framework, thereby providing a fairer and trustworthy platform for systematically evaluating existing BEV-based trajectory prediction methods. The benchmarking results show that state-of-the-art BEV-based models suffer significant performance degradation when their input of historical observations is disturbed by the ego-view noises, underscoring the need for new frameworks for robust trajectory prediction under real-world ego-view perturbation.\n\nTo address this problem, we propose BiFlow, a novel noise-resistant dual-stream flow matching model as an example solution for our benchmark. BiFlow jointly recovers the observed noisy historical trajectories and predicts future trajectories. By jointly learning latent features across the two tasks, the model implicitly leverages denoised historical semantics to guide future trajectory predictions, improving robustness while maintaining parameter efficiency. In addition, we introduce EgoAnchor, a mechanism to distill compact, ego-centric tokens from agent- and scene-level histories. These intent-aware representations, extracted via attention mechanism during history reconstruction, are injected into the decoder via feature-wise affine modulation, providing a robust intent prior to stabilize prediction under partial or corrupted input.\n\nThe main contributions of our work are: 1) We introduce EgoTraj-Bench, the first real-world benchmark for trajectory prediction under deployment-realistic conditions, enabling rigorous evaluation of models under authentic ego-centric noisy perturbations; 2) We propose a novel dual-stream flow matching framework with a distillation mechanism, which jointly recovers noisy historical observations and predicts future trajectories, aiming to leverage clean historical semantics to facilitate and stabilize future forecasting; 3) Our experiments demonstrate the significant impact of ego-view noise on existing models and the robustness of our proposed approach, which outperforms baselines by over 10% in minADE and 13% in minFDE averaged over datasets, highlighting the importance of noise-aware modeling and providing valuable insights for future research in ego-view realistic trajectory prediction.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有方法假设理想化观察历史，未考虑第一人称视觉中的感知伪影。  \n2. 机器人在以人为中心的环境中进行导航时，可靠的轨迹预测至关重要。  \n3. 需要一个真实世界的基准来评估在真实感知约束下的轨迹预测模型。  \n\n【提出了什么创新的方法】  \n本文提出了EgoTraj-Bench，这是第一个针对在真实世界中存在噪声的第一人称视角下的轨迹预测的基准。基于此基准，提出了BiFlow，一个双流流匹配模型，能够同时去噪历史观察和预测未来运动。BiFlow通过共享潜在表示来提高模型的鲁棒性，并引入EgoAnchor机制，以增强对代理意图的建模。实验结果表明，BiFlow在minADE和minFDE上平均减少了10-15%，显示出其在真实世界中对噪声的抵抗能力。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "A Hierarchical Agentic Framework for Autonomous Drone-Based Visual Inspection",
            "authors": "Ethan Herron,Xian Yeow Lee,Gregory Sin,Teresa Gonzalez Diaz,Ahmed Farahat,Chetan Gupta",
            "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00259",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00259",
            "arxiv_html_link": "https://arxiv.org/html/2510.00259v1",
            "abstract": "Autonomous inspection systems are essential for ensuring the performance and longevity of industrial assets.\nRecently, agentic frameworks have demonstrated significant potential for automating inspection workflows but have been limited to digital tasks.\nTheir application to physical assets in real-world environments, however, remains underexplored.\nIn this work, our contributions are two-fold: first, we propose a hierarchical agentic framework for autonomous drone control, and second, a reasoning methodology for individual function executions which we refer to as ReActEval.\nOur framework focuses on visual inspection tasks in indoor industrial settings, such as interpreting industrial readouts or inspecting equipment.\nIt employs a multi-agent system comprising a head agent and multiple worker agents, each controlling a single drone.\nThe head agent performs high-level planning and evaluates outcomes, while the worker agents implement our ReActEval methodology to reason over and execute low-level actions.\nOperating entirely in the natural language space, ReActEval follows a plan, reason, act, evaluate cycle, enabling drones to handle tasks ranging from simple navigation (e.g., flying forward 10 meters and land) to complex high-level tasks (e.g., locating and reading a pressure gauge).\nThe evaluation phase serves as a feedback and/or replanning stage, ensuring the actions executed align with user objectives while preventing undesirable outcomes.\nWe evaluate the framework in a simulated environment with two worker agents, assessing performance qualitatively and quantitatively based on task completion across varying levels of task complexity and agentic workflow efficiency.\nBy leveraging natural language processing for agent communication, our approach offers a novel, flexible, and user-accessible alternative to traditional drone-based solutions, enabling a more autonomous problem-solving approach to industrial inspection tasks without requiring extensive user intervention.",
            "introduction": "Industrial inspection systems face rising demand for autonomous solutions capable of reducing safety risks and operational costs while maintaining coverage of critical infrastructure. Manual inspection in hazardous environments, such as chemical plants and power facilities, introduces safety concerns and limits the frequency and thoroughness of assessments due to human constraints( U.S. Department of Labor and Administration [2025]).\n\nCurrent drone-based inspection systems rely heavily on skilled operators and manual intervention( Rodríguez et al. [2024]). They use pre-programmed flight paths that lack adaptability for dynamic industrial environments. Real-time pilot requirements create decision-making bottlenecks, while multi-drone coordination presents significant cognitive load challenges. These limitations prevent current systems from scaling effectively across three critical dimensions: task breadth (deployment in diverse industrial settings), task complexity, and the number of concurrently deployed drones.\n\nAgentic frameworks, systems composed of multiple coordinated Large Language Models (LLMs) that operate together to achieve predefined tasks, have shown remarkable success in digital domains, particularly in software development with agentic Integrated Development Environments (IDEs) and coding agents( Slasky [2025], Wu et al. [2023]) as well as in supporting researchers in performing scientific research( Gridach et al. [2025], Huang et al. [2025]). These systems use natural language interfaces and multi-agent coordination to achieve human-level reasoning in complex problem-solving scenarios, suggesting significant potential for physical applications.\n\nHowever, applying multi-agent systems to physical asset inspection reveals critical research gaps. This work addresses two fundamental challenges. First, how should multiple agents, in our case drones, be managed within the system? Second, how should each drone’s agent handle task execution effectively?\n\nTo address multi-agent management, we introduce a hierarchical agentic framework in which a single head agent generates high-level plans for individual worker agents, which are LLMs controlling their respective drones. This architecture centralizes communication between frontline workers such as technicians, operators, inspectors and the broader inspection system, reducing latency and eliminating the need for complex inter-agent coordination.\n\nInitial experimentation with the hierarchical framework revealed poor performance in drone-level task execution, including incorrect movements and incomplete task completion. These findings motivated the development of our second contribution: the ReActEval framework. Although the effectiveness of various reasoning methods for LLM-driven vehicle control remains poorly understood, the foundational ReAct framework by Yao et al. [2022] provides an established approach that combines reasoning and acting through interleaved generation of reasoning traces and task-specific actions. Building on this established framework, we propose ReActEval, which extends ReAct by adding a critical third step—evaluate—after each action execution to enable structured self-correction in physical tasks, addressing the unique challenges of real-world drone control.\n\nWe evaluate our ReActEval method within the hierarchical multi-agent framework for autonomous drone visual inspection. Through systematic comparison against ReAct and a simplified Act method across different model capabilities and task complexities, we provide the first comprehensive analysis of how reasoning method selection affects performance in physical agentic systems. Our contributions are: (1) a hierarchical agentic framework for drone-based industrial vision tasks, such as inspection, monitoring, tracking and identification, (2) the novel ReActEval framework for enhanced planning and evaluation in physical agentic systems, and (3) a systematic comparison of reasoning approaches across different model capabilities that reveals important performance tradeoffs.\n\nOur findings reveal counterintuitive relationships between reasoning methods and model capability. We demonstrate that more complex reasoning methods are not universally superior, but rather their effectiveness depends critically on the underlying model’s capability and task complexity. This challenges the assumption that sophisticated reasoning frameworks always improve performance. These insights offer guidance in developing effective autonomous inspection systems and lay the groundwork for future research on agentic frameworks for physical tasks.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有无人机检查系统依赖人工操作，缺乏适应动态工业环境的能力。  \n2. 多代理系统在物理资产检查中的应用尚未充分探索，存在关键研究空白。  \n3. 需要减少人工干预，提高工业检查的自动化程度和效率。  \n\n【提出了什么创新的方法】  \n本研究提出了一个层次化的代理框架，结合ReActEval方法，旨在提升无人机在工业环境中的视觉检查能力。该框架通过一个主代理进行高层规划，多个工作代理执行低层任务，采用自然语言处理进行代理间的沟通。ReActEval方法在每次行动后进行评估，确保执行的行动与用户目标一致，从而实现更高效的任务完成。通过在模拟环境中的评估，展示了该框架在不同任务复杂性和代理工作效率方面的优越性，提供了一种灵活且用户友好的工业检查解决方案。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Robust Attitude Control of Nonlinear Multi-Rotor Dynamics with LFT Models and $\\mathcal{H}_\\infty$ Performance",
            "authors": "Tanay Kumar,Raktim Bhattacharya",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO); Optimization and Control (math.OC)",
            "comment": "submitted to ACC 2026",
            "pdf_link": "https://arxiv.org/pdf/2510.00208",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00208",
            "arxiv_html_link": "https://arxiv.org/html/2510.00208v1",
            "abstract": "Attitude stabilization of unmanned aerial vehicles in uncertain environments presents significant challenges due to nonlinear dynamics, parameter variations, and sensor limitations. This paper presents a comparative study of ℋ∞\\mathcal{H}_{\\infty} and classical PID controllers for multi-rotor attitude regulation in the presence of wind disturbances and gyroscope noise. The flight dynamics are modeled using a linear parameter-varying (LPV) framework, where nonlinearities and parameter variations are systematically represented as structured uncertainties within a linear fractional transformation formulation. A robust controller based on ℋ∞\\mathcal{H}_{\\infty} formulation is designed using only gyroscope measurements to ensure guaranteed performance bounds. Nonlinear simulation results demonstrate the effectiveness of the robust controllers compared to classical PID control, showing significant improvement in attitude regulation under severe wind disturbances.",
            "introduction": "Unmanned Aerial Vehicles (UAVs) have seen rapid adoption across diverse sectors, driving increased demands for robust control systems. Complex missions involving urban navigation, multi-agent coordination, and beyond-visual-line-of-sight operations present challenges that conventional control approaches struggle to address. These applications face significant disturbances, including atmospheric turbulence, wind gradients, mass distribution variations, sensor imprecision, and model uncertainties. Robust control methodologies are therefore essential to maintain stability and performance under such perturbations. Recent statistical analyses confirm this need, showing increased incidents related to control system failures in UAV operations [1].\n\nLinear controllers remain predominant in practical implementations due to their analytical tractability and computational efficiency. Proportional-Integral-Derivative (PID) control structures, particularly in cascaded configurations, are extensively deployed in industry [2]; however, their single-input-single-output (SISO) formulation inherently neglects multi-axis coupling effects and relies on heuristic tuning procedures, rendering them susceptible to parametric uncertainties and exogenous disturbances [3, 4]. Gain scheduling partially compensates for nonlinearities by adapting controller parameters across the operational envelope, yet this approach provides only incremental improvements in robustness while necessitating extensive empirical calibration [5, 6].\n\nLinear Quadratic Regulators (LQR) provide a mathematically optimal control framework that minimizes a quadratic cost function, balancing state regulation performance against control effort. Despite their theoretical optimality, LQR implementations suffer from two critical limitations: they require full state feedback, necessitating state estimation in practical applications, and they lack inherent robustness guarantees against parametric uncertainties. When system dynamics deviate from the nominal model – a common occurrence in aerial vehicles due to aerodynamic effects and mass distribution changes – LQR performance deteriorates significantly. Additionally, the state estimation typically relies on Kalman filtering, which achieves optimality only under Gaussian noise assumptions. Furthermore, LQR controllers commonly exhibit longer response times than their PID counterparts, despite their theoretical optimality properties [7].\n\nNonlinear control methodologies have been proposed to address the limitations above. Backstepping techniques, founded on recursive Lyapunov stability theory, offer enhanced robustness to matched uncertainties. However, their implementation yields controllers of elevated structural complexity with substantial computational overhead and parameter sensitivity, thus imposing practical constraints on real-time applications [6]. Feedback linearization approaches transform nonlinear dynamics into equivalent linear systems via nonlinear state transformations and control laws. Yet, these methods exhibit pronounced sensitivity to modeling errors and measurement noise, consequently compromising robustness margins [6]. Adaptive control frameworks facilitate online parameter estimation and controller reconfiguration to mitigate parametric uncertainties.\nNevertheless, several challenges persist, including parameter convergence rates under time-varying conditions, susceptibility to measurement noise, and analytical complexity in establishing uniform stability guarantees. While these nonlinear methodologies demonstrate theoretical superiority over linear control architectures under nominal conditions, they frequently encounter implementation barriers on resource-constrained UAV platforms and lack rigorous performance guarantees under structured uncertainties and bounded disturbances.\n\nThese limitations have motivated the development of robust control frameworks, particularly ℋ∞\\mathcal{H}_{\\infty} control [8], which provides rigorous stability guarantees under bounded parameter variations by representing system uncertainties as norm-bounded operators and minimizing worst-case induced norms without requiring precise disturbance characterization. These methodologies have gained significant traction across aerospace applications, with recent literature demonstrating their effectiveness for tailsitter UAVs under severe turbulence [9], helicopter control with certified performance bounds [10], and resource-constrained implementations that substantially outperform traditional PID architectures despite computational limitations [11].\n\nA key advantage of robust control frameworks lies in their compatibility with Linear Parameter-Varying (LPV) modeling, which enables exact representation of nonlinear dynamics without local linearization or approximation [12]. The rotational dynamics of multi-rotors contain trigonometric and rational expressions that can be modeled precisely using static nonlinear operators, which are then treated as structured, bounded uncertainty blocks within the Linear Fractional Transformation (LFT) formalism. This representation maintains the full fidelity of the nonlinear dynamics while enabling systematic controller synthesis via ℋ∞\\mathcal{H}_{\\infty} technique.\n\nThe LPV-LFT approach effectively bridges the gap between high-fidelity dynamic modeling and tractable robust control design by encapsulating nonlinearities as parameter-dependent terms around a linear time-invariant core. This framework embeds parameter variations directly into the synthesis process rather than treating them as afterthoughts, offering formal robustness guarantees against structured uncertainties. The resulting controllers maintain stability and performance across the entire operating envelope without requiring gain scheduling or extensive empirical tuning, making them particularly suitable for autonomous systems operating in uncertain environments.\n\nThe effectiveness of LFT-based modeling has been demonstrated across aerospace applications, including NASA’s stability margin assessments [13] and ESA’s spacecraft attitude control [14]. LPV approaches extend beyond gain scheduling by embedding parameter variations into control design, offering formal robustness guarantees [15, 16]. While synthesis methods typically use linear matrix inequalities [17, 18], challenges remain in representing nonlinear dependencies, often requiring probabilistic solutions [19, 20]. Nevertheless, LPV-LFT approaches have achieved success in applications from reconfigurable flight control to polynomially parameterized stability analysis [21, 22, 23, 24], motivating this work’s robust control framework for aerospace systems.\n\nThis paper addresses significant challenges in robust multi-rotor stabilization by advancing the application of structured uncertainty frameworks to nonlinear multi-rotor flight dynamics. The contributions of this work are threefold:\n\nWe establish a systematic framework for representing multi-rotor nonlinear dynamics within the LFT formalism, enabling rigorous treatment of trigonometric nonlinearities and parameter-dependent terms as structured uncertainties.\n\nWe establish a systematic framework for representing multi-rotor nonlinear dynamics within the LFT formalism, enabling rigorous treatment of trigonometric nonlinearities and parameter-dependent terms as structured uncertainties.\n\nWe develop and validate robust control synthesis techniques that utilize ℋ∞\\mathcal{H}_{\\infty} methodology for attitude stabilization under significant external disturbances, utilizing only gyroscope measurements with explicit characterization of state-dependent sensor noise.\n\nWe provide a comprehensive comparative analysis between classical PID and robust control approaches, quantifying performance improvements regarding disturbance rejection capabilities and control effort optimization under wind turbulence conditions representative of practical operating environments.\n\nThis work advances the application of structured uncertainty frameworks in multi-rotor control by integrating LPV-LFT modeling with robust synthesis techniques. Unlike approaches that rely on linearization approximations, our methodology precisely captures system nonlinearities while incorporating realistic sensor limitations. The proposed framework addresses fundamental challenges in existing multi-rotor controllers by providing formal robustness guarantees against parametric uncertainties and exogenous disturbances, thereby bridging theoretical control design with practical implementation constraints.",
            "llm_summary": "【论文的motivation是什么】  \n1. 复杂的无人机任务面临非线性动态和环境不确定性带来的控制挑战。  \n2. 传统控制方法（如PID和LQR）在应对多轴耦合和参数不确定性时表现不足。  \n3. 需要开发具有鲁棒性的控制框架，以确保在各种干扰下的稳定性和性能。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于线性分数变换（LFT）和ℋ∞控制的系统框架，用于多旋翼无人机的姿态控制。该方法通过将非线性动态建模为结构性不确定性，确保在外部干扰下的鲁棒性。通过仅使用陀螺仪测量，本文展示了与经典PID控制相比，鲁棒控制器在风扰动下的显著性能提升，提供了更好的干扰拒绝能力和控制努力优化。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI",
            "authors": "Diego Ortiz Barbosa,Mohit Agrawal,Yash Malegaonkar,Luis Burbano,Axel Andersson,György Dán,Henrik Sandberg,Alvaro A. Cardenas",
            "subjects": "Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00167",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00167",
            "arxiv_html_link": "https://arxiv.org/html/2510.00167v1",
            "abstract": "未获取到摘要",
            "introduction": "Autonomous drones hold enormous promise for society, with potential applications in disaster response, infrastructure inspection, environmental monitoring, and transportation. Their ability to operate in dynamic environments and carry out missions without constant human supervision could transform how critical services are delivered. However, to fulfill this promise, drones must be trustworthy. In particular, they must be prepared to respond when alarms or unexpected events occur. These alarms may signal cyber attacks such as GPS spoofing, electromagnetic interference (EMI) signal injection Jang et al. (2023), device failures such as sensor degradation, or adverse conditions such as sudden weather changes or unexpected obstacles. Although rare, such events can have severe consequences, and autonomous agents must be able to assess the situation and determine an appropriate course of action in real time.\n\nPrevious work recognizes the need for automatic recovery when alarms or unexpected events occur, but existing approaches are tailored to specific failure models and lack general, context-aware reasoning Cardenas (2025); Barbosa et al. (2025). (A) Recovery to pre-specified targets. A popular recovery method formalizes how to steer a system to a pre-defined safe set after an alert Zhang et al. (2020, 2024). These approaches assume that the target (safe) set is known in advance and valid throughout recovery; they do not revise that target if the environment becomes unsafe at runtime. (B) Mitigation based on experience. Complementary lines address particular attack or failure classes by learning from previous successful traces Dash et al. (2021); however, if the failure condition requires a completely new maneuver or destination not seen before, these approaches may struggle. In general, previous efforts are scenario-bound: they depend on pre-enumerated fault/attack models, static safe-target definitions, or fixed fallback behaviors, rather than dynamically interpreting complex scenes and choosing actions with common-sense reasoning in open-ended, evolving environments.\n\nIn this work, we introduce a new pipeline that leverages large visual-language models (LVLMs) to support real-time decision making when sudden events occur. Our approach focuses on the case of sudden landing maneuvers, where a drone must quickly assess its surroundings and select a safe course of action. The pipeline integrates traditional control modules with LVLM-based reasoning: perception modules identify candidate surfaces, the LVLM evaluates their suitability using common-sense reasoning, and a movement planner executes the maneuver. To evaluate this approach, we build a benchmark on top of open source platforms, leveraging the Unreal Engine to propose realistic and dynamic-looking environments, with the Cosys-AirSim simulator, modeling the sensors and control stack of drones.\n\nThis benchmark provides diverse, realistic urban scenarios with dynamic obstacles and configurable conditions, enabling systematic testing of recovery pipelines. Our contributions are threefold:\n\nWe propose an LVLM-driven pipeline for sudden landing decisions that combines perception, reasoning, and control;\n\nWe develop a reproducible benchmark for evaluating such pipelines in realistic urban simulations. Our benchmark is openly accessible at https://github.com/RollingBeatle/Airsim-closeloop\n\nWe demonstrate through experiments that embodied AI enables adaptive recovery behaviors that were previously infeasible.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有无人机在应对突发事件时缺乏通用的、上下文感知的推理能力。  \n2. 现有恢复方法依赖于预先定义的安全目标，无法动态调整应对策略。  \n3. 需要一种新的方法来支持无人机在复杂环境中进行实时决策。  \n\n【提出了什么创新的方法】  \n提出了一种新的管道，利用大型视觉语言模型（LVLMs）来支持无人机在突发事件发生时的实时决策，特别是在突然着陆的情况下。该方法将传统控制模块与基于LVLM的推理结合起来：感知模块识别候选表面，LVLM使用常识推理评估其适用性，运动规划器执行相应的操作。通过在开放源代码平台上构建基准，利用虚幻引擎创建逼真动态环境，结合Cosys-AirSim模拟器，评估该方法的有效性。实验表明，具身智能能够实现以前无法实现的自适应恢复行为。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-02 02:21:58",
            "title": "Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving",
            "authors": "Sheng Yang,Tong Zhan,Guancheng Chen,Yanfeng Lu,Jian Wang",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.00060",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.00060",
            "arxiv_html_link": "https://arxiv.org/html/2510.00060v1",
            "abstract": "In this work, we reconceptualize autonomous driving as a generalized language and formulate the trajectory planning task as next waypoint prediction. We introduce Max-V1 111In tribute to the renowned Dutch racing driver Max Verstappen., a novel framework for one-stage end-to-end autonomous driving. Our framework presents a single-pass generation paradigm that aligns with the inherent sequentiality of driving. This approach leverages the generative capacity of the VLM (Vision-Language Model) to enable end-to-end trajectory prediction directly from front-view camera input. The efficacy of this method is underpinned by a principled supervision strategy derived from statistical modeling. This provides a well-defined learning objective, which makes the framework highly amenable to master complex driving policies through imitation learning from large-scale expert demonstrations. Empirically, our method achieves the state-of-the-art performance on the nuScenes dataset, delivers an overall improvement of over 30%30\\% compared to prior baselines. Furthermore, it exhibits superior generalization performance on cross-domain datasets acquired from diverse vehicles, demonstrating notable potential for cross-vehicle robustness and adaptability. Due to these empirical strengths, this work introduces a model enabling fundamental driving behaviors, laying the foundation for the development of more capable self-driving agents. Code will be available upon publication.",
            "introduction": "Human driving is an inherently sequential decision-making process, in which each action is conditioned on a real-time understanding of the surrounding scene. This dynamic interplay of perception and action exhibits strong similarities to natural language generation, which also involves producing a highly correlated sequence of outputs. Viewing the driving task from this perspective allows us to frame a Vision-Language Model (VLM) as a powerful policy network. In this context, the model’s objective shifts from predicting the next word to generating the next driving action, transforming the planning problem into a tractable, autoregressive sequence modeling task. This conceptual leap opens the door for leveraging the vast pre-trained knowledge and sophisticated reasoning capabilities of VLMs to tackle the complexities of autonomous driving.\n\nThe end-to-end approach has emerged as a dominant paradigm in autonomous driving, as it facilitates global optimization of the entire system and mitigates error accumulation. Within this paradigm, current research has diverged into two primary approaches. The first centers on developing bespoke architectures, trained exclusively on large-scale, domain-specific driving datasets. The second focuses on adapting large, pre-trained VLMs, aiming to leverage their vast world knowledge and general reasoning capabilities for the driving task.\n\nThe first approach, exemplified by methods like UniAD (Hu et al., 2023), typically employs carefully designed bespoke sequential architectures centered around Bird’s-Eye View (BEV) representations. This approach is predicated on the assumption that a model, when meticulously trained on vast amounts of real-world driving data, can learn robust policies for practical deployment, with BEV serving as an efficient intermediate representation. However, this paradigm faces a notable dual challenge. On the one hand, its strong dependence on pattern recognition within high-quality curated datasets limits its generalization capabilities when encountering long-tail scenarios. On the other hand, the BEV representation itself introduces fragility: its generation from camera imagery is an ill-posed problem prone to information loss, and the scarcity of large-scale, accurately annotated BEV datasets remains a critical unavoidable bottleneck.\n\nThe second approach, in contrast, more flexibly and effectively leverages mature VLM-related frameworks like those in (Jiang et al., 2024; Xing et al., 2024; Qiao et al., 2025) as high-level reasoning engines. By adopting Q&\\&A format, these systems can deeply tap into and utilize the rich pre-trained knowledge of VLMs to further enhance their contextual awareness of driving scenarios. However, their generalist nature leads to a mismatch in autonomous driving task alignment: model architectures and objective functions optimized for discrete text processing are not naturally suited for the continuous, fine-grained control essential to real-world trajectory planning.\n\nThis analysis of current end-to-end approaches reveals two parallel schools of thought, each with inherent limitations. Specialized models are optimized for large-scale domain-specific datasets yet limited by their data-driven nature and fragile intermediate representations. The other focuses on VLM-related frameworks, which offer strong reasoning but face challenges with computational inefficiency and an inherent unsuitability for the continuous control problem. Developing more integrated architectures to bridge these gaps thus offers a promising evolutionary path and serves as the primary motivation for our work.\n\nIn this work, we present Max-V1, an end-to-end autonomous driving trajectory planner built on a pure VLM. Our approach enables a pre-trained VLM to acquire driving-related capabilities through fine-tuning solely on driving-specific behaviors, allowing the model to focus on the task. To achieve this, Max-V1 models driving as a sequential decision process similar to natural language and eliminates the traditional BEV feature space, instead processing raw sensor input directly from an ego-centric, first-person perspective. By operating within this pure VLM-driven, end-to-end architecture, our paradigm combines both high performance and structural simplicity with the potential for robust cross-domain generalization. This approach avoids error accumulation from BEV construction, harnesses pre-trained knowledge, reduces dependency on costly BEV-specific annotations, and aligns more closely with the nature of driving. Specifically, we formulate our contributions as follows.\n\nWe statistically model driving behavior as a sequential decision process and frame the planning task as next waypoint prediction, for which we demonstrate the validity of our supervision signal design. This formulation lays a principled foundation for our single-pass design and aligns with the nature of driving. We then leverage the pre-trained VLM as both a domain-specific knowledge repository and a powerful policy network to address this task via fine-tuning.\n\nWe statistically model driving behavior as a sequential decision process and frame the planning task as next waypoint prediction, for which we demonstrate the validity of our supervision signal design. This formulation lays a principled foundation for our single-pass design and aligns with the nature of driving. We then leverage the pre-trained VLM as both a domain-specific knowledge repository and a powerful policy network to address this task via fine-tuning.\n\nWithout any external information during training, our method achieves the state-of-the-art performance on the nuScenes dataset, delivers an overall improvement of over 30%30\\% compared to prior baselines. In particular, our model demonstrates strong zero-shot generalization, exhibiting competent driving behavior in distinct scenarios. As these datasets were collected using entirely different vehicles, this performance indicates a strong potential for robust cross-vehicle deployment. In addition, we briefly explore first-person perspective LiDAR-image fusion, identifying a trade-off that leans more toward short-term objectives.\n\nOur framework provides a task-specific adaptation framework for VLMs to replace the conventional multi-stage driving pipeline. This unified architecture provides a structurally simplified foundation, making it a scalable foundation for the development of more capable self-driving agents through reinforcement learning.\n\n1. We statistically model driving behavior as a sequential decision process and frame the planning task as next waypoint prediction, for which we demonstrate the validity of our supervision signal design. This formulation lays a principled foundation for our single-pass design and aligns with the nature of driving. We then leverage the pre-trained VLM as both a domain-specific knowledge repository and a powerful policy network to address this task via fine-tuning.\n\n2. Without any external information during training, our method achieves the state-of-the-art performance on the nuScenes dataset, delivers an overall improvement of over 30%30\\% compared to prior baselines. In particular, our model demonstrates strong zero-shot generalization, exhibiting competent driving behavior in distinct scenarios. As these datasets were collected using entirely different vehicles, this performance indicates a strong potential for robust cross-vehicle deployment. In addition, we briefly explore first-person perspective LiDAR-image fusion, identifying a trade-off that leans more toward short-term objectives.\n\n3. Our framework provides a task-specific adaptation framework for VLMs to replace the conventional multi-stage driving pipeline. This unified architecture provides a structurally simplified foundation, making it a scalable foundation for the development of more capable self-driving agents through reinforcement learning.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的自主驾驶模型在处理长尾场景时的泛化能力有限。  \n2. 传统的BEV表示方法容易导致信息损失，且缺乏大规模准确标注的数据集。  \n3. 当前的VLM框架在连续控制任务中不够适用，存在效率低下的问题。  \n\n【提出了什么创新的方法】  \n本研究提出了Max-V1，一个基于纯VLM的端到端自主驾驶轨迹规划器。该方法将驾驶建模为类似自然语言的顺序决策过程，直接处理来自第一人称视角的原始传感器输入，避免了传统BEV特征空间的使用。通过对预训练VLM的微调，Max-V1能够有效地掌握驾驶相关能力，并在nuScenes数据集上实现了超过30%的性能提升，展现出强大的零-shot泛化能力，适应不同车辆的驾驶场景。该框架为VLM提供了任务特定的适应性，简化了传统多阶段驾驶管道，为更强大的自驾代理的发展奠定了基础。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        }
    ],
    "2025-10-03": [
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation",
            "authors": "Wenye Yu,Jun Lv,Zixi Ying,Yang Jin,Chuan Wen,Cewu Lu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02298",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02298",
            "arxiv_html_link": "https://arxiv.org/html/2510.02298v1",
            "abstract": "Imitation learning has shown promise in learning from large-scale real-world datasets.\nHowever, pretrained policies usually perform poorly without sufficient in-domain data.\nBesides, human-collected demonstrations entail substantial labour and tend to encompass mixed-quality data and redundant information.\nAs a workaround, human-in-the-loop systems gather domain-specific data for policy post-training, and exploit closed-loop policy feedback to offer informative guidance, but usually require full-time human surveillance during policy rollout.\nIn this work, we devise ARMADA, a multi-robot deployment and adaptation system with human-in-the-loop shared control, featuring an autonomous online failure detection method named FLOAT.\nThanks to FLOAT, ARMADA enables paralleled policy rollout and requests human intervention only when necessary, significantly reducing reliance on human supervision.\nHence, ARMADA enables efficient acquisition of in-domain data, and leads to more scalable deployment and faster adaptation to new scenarios.\nWe evaluate the performance of ARMADA on four real-world tasks.\nFLOAT achieves nearly 95% accuracy on average, surpassing prior state-of-the-art failure detection approaches by over 20%.\nBesides, ARMADA manifests more than 4×\\times increase in success rate and greater than 2×\\times reduction in human intervention rate over multiple rounds of policy rollout and post-training, compared to previous human-in-the-loop learning methods.",
            "introduction": "Recent years have witnessed the burgeoning of data-driven approaches in the field of robotic manipulation [8, 54, 20].\nIn pursuit of policies for real-world deployment, joint efforts have been made to build large-scale datasets for policy learning.\nSome prior works gather human-collected data from multiple sources and integrate them into large-scale heterogeneous datasets [34, 19, 11].\nAnother line of work introduces unified data collection systems, including embodiment-agnostic handheld devices [9, 39, 49] and low-cost exoskeletons [12, 5, 13].\nThrough pretraining on these datasets, we obtain policies that have rudimentary ability of performing specific tasks in real-world scenarios.\n\nHowever, pretrained policies usually lack robustness during deployment due to the deficiency of in-domain data.\nBesides, as indicated in previous works on data curation [16, 6, 52], human-collected demonstrations often contain segments with mixed quality and redundant information, which impedes robots from gaining superior performance.\nTo this end, prior works explore human-in-the-loop systems, where human operators and learned policies control the robot in a shared manner.\nThese systems enable interactive collection of domain-specific data, which are utilized for policy post-training and help adapt the policies to the given scenario.\nMoreover, by observing robot behaviour during online rollouts, human operators are able to offer informative guidance with closed-loop policy feedback.\nSome works allow human operators to intervene in policy rollout when the robot fails to accomplish the given tasks, and take advantage of human correction data for policy post-training [23, 25, 43, 18, 46].\nOthers embrace the idea of shared autonomy and develop joint control between human and robot by a time-varying ratio [28, 51].\n\nNevertheless, most of these human-in-the-loop systems require full-time surveillance from human operators so that they can spot task failures in time and help robots recover.\nThis confines the current systems to a one-to-one control setting where each human operator attends to a single robot.\nIn order to further enhance the scalability of human-in-the-loop systems, we hold that there are two desiderata:\n1) A real-time failure detection module that monitors policy rollout can help alert human operators of possible task failures and thus alleviate reliance on full-time human supervision.\n2) A multi-robot shared control system scales up policy deployment and post-training for faster adaptation to new scenarios.\n\nTo this end, we propose a scalable real-world robot system featuring autonomous online failure detection and multi-robot shared control, dubbed ARMADA (Autonomous Real-world Multi-robot system with human Assistance for Deployment and Adaptation).\nConcretely, we devise an online failure detection method for visuomotor imitation learning algorithms based on policy embedding.\nGiven a pretrained visuomotor policy and expert demonstrations that constitute the training data, we perform online trajectory matching between current policy rollouts and expert trajectories and compute the “distance” between the matched trajectories in terms of their policy embeddings using Optimal Transport (OT) [14, 35, 10], which we refer to as the FLOAT index (FaiLure detection based on OptimAl Transport).\nUtilizing the FLOAT indices of all successful rollout trajectories, we then define a universal FLOAT threshold, which serves as a real-time and plug-in-and-play approach to online failure detection.\nMore importantly, we implement a multi-robot shared control system that allows for autonomous policy rollouts and timely human intervention when our failure detection model raises warning, significantly improving the efficiency of human-in-the-loop systems.\nOur system can be easily adapted to various imitation learning policies, human intervention patterns (teleoperation, exoskeleton, etc.), and embodiments.\nThe entire implementation of ARMADA will be open-sourced.\n\nWe carry out comprehensive experiments to verify the effectiveness of our novel failure detection approach and multi-robot system.\nAcross four real-world tasks, FLOAT lifts the average accuracy to nearly 95%, which is an improvement of over 20% compared to state-of-the-art approaches.\nBesides, over several rounds of rollout and fine-tuning, ARMADA achieves more than 4×\\times increase in success rate and greater than double reduction in human intervention rate compared to previous human-in-the-loop shared control systems.\n\nIn a nutshell, our contributions are three-fold:\n\nWe devise FLOAT, a plug-in-and-play online failure detection system for visuomotor imitation learning methods that achieves nearly 95% accuracy in real world.\n\nWe devise FLOAT, a plug-in-and-play online failure detection system for visuomotor imitation learning methods that achieves nearly 95% accuracy in real world.\n\nWe implement ARMADA, a multi-robot system with human-in-the-loop shared control that enables paralleled and autonomous robot rollouts, empowering scalable real-world deployment and adaptation.\n\nARMADA, over multiple rounds of post-training, leads to a more than four-fold increase in success rate and a greater than two-fold decrease in human intervention ratio compared to previous human-in-the-loop learning approaches that require full-time human supervision.",
            "llm_summary": "【论文的motivation是什么】  \n1. 预训练策略在缺乏领域内数据时表现不佳。  \n2. 人工收集的演示数据质量参差不齐，影响机器人性能。  \n3. 现有的人机协作系统需要全时人类监督，限制了可扩展性。  \n\n【提出了什么创新的方法】  \n提出了ARMADA，一个具有人机协作的多机器人系统，结合了FLOAT在线故障检测方法。FLOAT通过在线轨迹匹配和最优传输计算故障指数，实时监控策略的执行，显著减少对人类监督的依赖。ARMADA实现了并行策略的自主执行，提升了成功率超过4倍，同时将人类干预率减少超过2倍，展示了其在真实场景中的有效性和可扩展性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning",
            "authors": "Tianchong Jiang,Jingtian Ji,Xiangshan Tan,Jiading Fang,Anand Bhattad,Vitor Guizilini,Matthew R. Walter",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "Code and project materials are available atthis http URL",
            "pdf_link": "https://arxiv.org/pdf/2510.02268",
            "code": "http://ripl.github.io/know_your_camera",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02268",
            "arxiv_html_link": "https://arxiv.org/html/2510.02268v1",
            "abstract": "We study view-invariant imitation learning by explicitly conditioning policies on camera extrinsics.\nUsing Plücker embeddings of per-pixel rays, we show that conditioning on extrinsics significantly improves generalization across viewpoints for standard behavior cloning policies, including ACT, Diffusion Policy, and SmolVLA.\nTo evaluate policy robustness under realistic viewpoint shifts, we introduce six manipulation tasks in RoboSuite and ManiSkill that pair “fixed” and “randomized” scene variants, decoupling background cues from camera pose.\nOur analysis reveals that policies without extrinsics often infer camera pose using visual cues from static backgrounds in fixed scenes. This shortcut collapses when workspace geometry or camera placement shifts.\nConditioning on extrinsics restores performance and yields robust RGB-only control without depth. We release the tasks, demonstrations, and code to facilitate reproducibility and future research.\nCode and project materials are available at ripl.github.io/know_your_camera.",
            "introduction": "Significant recent attention has been devoted to imitation learning as a means of enabling robots to perform diverse and complex manipulation tasks [1, 2, 3, 4].\nThese policies usually directly operate on RGB images and are trained with fixed third-person viewpoints.\nHowever, when deployed in the real world, it is often not possible to position the cameras to exactly match their training pose due to environmental constraints. Consequently, many such models fail when testing on different viewpoints.\nThe problem is exacerbated when the robot’s embodiment changes, which inherently involves diverse camera configurations and viewpoints, making viewpoint invariance essential for cross-embodiment transfer [5].\n\nConcurrently, accurate camera geometry is increasingly accessible. Such information can be obtained directly from the datasets [6] used for training, or estimated through classic methods such as hand-eye calibration [7], visual-SLAM and structure-from-motion methods [8, 9, 10, 11, 12], or modern learning-based methods that handle sparse or dynamic views [13, 14]. Neglecting this readily available geometric information is a significant missed opportunity for improving robot learning.\n\nTo achieve viewpoint invariance, a policy must be aware of the camera’s geometry relative to the robot. This raises a fundamental question: should the end-to-end policy implicitly infer camera geometries during training, or should the robot policy model explicitly condition on camera geometries when they are readily available?\n\nWhile the architectural simplicity of end-to-end learning is appealing, especially in data-rich domains like language modeling, robotics is notoriously data-scarce. Expecting a single model to simultaneously estimate camera pose while also learning complex manipulation policies poses a serious trade-off between learnability and data efficiency. Therefore, we investigate the benefits of explicit camera conditioning. Assuming access to camera geometry, we study its effect on the robustness of robot policies under viewpoint variations.\n\nMore specifically, our contributions are the following:\n\nA Novel Camera Conditioning Method. We propose an approach to effectively condition common imitation learning policies on explicit camera geometries with per-pixel Plücker embeddings.\n\nA Novel Camera Conditioning Method. We propose an approach to effectively condition common imitation learning policies on explicit camera geometries with per-pixel Plücker embeddings.\n\nComprehensive Empirical Analysis. We show the benefits of explicit camera conditioning for policy generalization to view variations, and identify key factors affecting performance, including action space, random cropping, early vs. late conditioning, the effect of the pretrained visual encoder, and the scaling law with respect to the number of camera views.\n\nNew Benchmarks for Viewpoint Generalization. We introduce benchmark tasks—three in RoboSuite and three in ManiSkill—targeting challenging viewpoint generalization, and the corresponding demonstrations.\n\n1. A Novel Camera Conditioning Method. We propose an approach to effectively condition common imitation learning policies on explicit camera geometries with per-pixel Plücker embeddings.\n\n2. Comprehensive Empirical Analysis. We show the benefits of explicit camera conditioning for policy generalization to view variations, and identify key factors affecting performance, including action space, random cropping, early vs. late conditioning, the effect of the pretrained visual encoder, and the scaling law with respect to the number of camera views.\n\n3. New Benchmarks for Viewpoint Generalization. We introduce benchmark tasks—three in RoboSuite and three in ManiSkill—targeting challenging viewpoint generalization, and the corresponding demonstrations.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有模仿学习政策在不同视角下的泛化能力不足。  \n2. 忽视相机几何信息导致政策在真实环境中的表现不佳。  \n3. 机器人在不同环境和机体配置下需要具备视角不变性。  \n\n【提出了什么创新的方法】  \n本研究提出了一种新颖的相机条件化方法，通过每像素的Plücker嵌入有效地将常见的模仿学习政策与显式相机几何信息相结合。我们通过六个操控任务的实证分析，展示了显式相机条件化在视角变化下的政策泛化能力，并识别了影响性能的关键因素，如动作空间、随机裁剪、相机视图数量等。实验结果表明，条件化后的政策在RGB图像控制下表现出更强的鲁棒性，能够有效应对视角变化。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking",
            "authors": "Joao Pedro Araujo,Yanjie Ze,Pei Xu,Jiajun Wu,C. Karen Liu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02252",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02252",
            "arxiv_html_link": "https://arxiv.org/html/2510.02252v1",
            "abstract": "Humanoid motion tracking policies are central to building teleoperation pipelines and hierarchical controllers, yet they face a fundamental challenge: the embodiment gap between humans and humanoid robots. Current approaches address this gap by retargeting human motion data to humanoid embodiments and then training reinforcement learning (RL) policies to imitate these reference trajectories. However, artifacts introduced during retargeting, such as foot sliding, self-penetration, and physically infeasible motion are often left in the reference trajectories for the RL policy to correct. While prior work has demonstrated motion tracking abilities, they often require extensive reward engineering and domain randomization to succeed. In this paper, we systematically evaluate how retargeting quality affects policy performance when excessive reward tuning is suppressed. To address issues that we identify with existing retargeting methods, we propose a new retargeting method, General Motion Retargeting (GMR). We evaluate GMR alongside two open-source retargeters, PHC and ProtoMotions, as well as with a high-quality closed-source dataset from Unitree. Using BeyondMimic for policy training, we isolate retargeting effects without reward tuning. Our experiments on a diverse subset of the LAFAN1 dataset reveal that while most motions can be tracked, artifacts in retargeted data significantly reduce policy robustness, particularly for dynamic or long sequences. GMR consistently outperforms existing open-source methods in both tracking performance and faithfulness to the source motion, achieving perceptual fidelity and policy success rates close to the closed-source baseline. Website: jaraujo98.github.io/retargeting_matters. Code: github.com/YanjieZe/GMR.",
            "introduction": "Developing humanoid policies that truly generalize to real-world environments requires learning from data that captures physical interaction with the world. Given the morphological similarities between humans and humanoids, recent work [1, 2, 3, 4, 5, 6, 7, 8] has leveraged 3D human motion data (sourced from motion capture [9] or human motion recovery from video [7, 8, 10]) as demonstrations to train humanoids to perform whole-body movements requiring human-like balance and agility. These humanoid motion tracking policies are a fundamental tool for building teleoperation pipelines or hierarchical control systems. However, significant differences still exist between humans and humanoids in terms of bone length, joint range of motion, kinematic structure, body shape, mass distribution, and actuation mechanisms. This embodiment gap is the first major hurdle that must be overcome for 3D human motion data to be fully useful for humanoid learning.\n\nThe standard approach for overcoming this embodiment gap is to use kinematic retargeting from the source human motion to the target humanoid embodiment. Given the retargeted data, a popular practice in current robotics research is to use a reinforcement learning (RL) based approach to learn a policy capable of achieving a desired task through reference motion imitation. In most cases (for an exception see [11]), this policy is then deployed zero-shot into the real world. This practice either overlooks glaring artifacts introduced by the retargeting process (such as foot sliding, ground penetration, and physically impossible motion due to self-penetration), instead forcing the RL policy to imitate physically infeasible motions while maintaining physical constraints, or discards the poorly retargeted data [7]. Prior work [12, 13, 14, 11] has shown that while training policies on retargeted data with severe artifacts in simulation is possible, transferring them to the real world demands extensive trial-and-error, reward shaping, and parameter tuning. Considering this practice, our hypothesis is that with enough engineering in the reward function and domain randomization, the artifacts caused by retargeting can be mostly mitigated or removed. However, without these engineering efforts, the quality of retargeting results plays a significant role.\n\nIn this paper, we conduct rigorous experiments and analysis to validate our hypothesis. We compare three methods for human to humanoid motion retargeting applied to motion tracking tasks that do not involve interaction with an object or a complex scene: PHC, widely used by recent humanoid motion tracking works [2, 11]; ProtoMotions, used for retargeting challenging dynamic tasks [8]; and GMR, a general motion retargeting method proposed by us to address the problems we find in the other two methods. Specifically, we find that the way in which PHC and ProtoMotions handle human to robot scaling introduces several motion artifacts that negatively impact performance. GMR addresses this by using a simple but flexible non-uniform local scaling procedure, followed by a two-stage optimization to solve for the robot motion that tracks the scaled reference. In addition, we compare the three retargeting methods with a high-quality retargeted motion dataset generated by a closed-source method not available to the public (Unitree).\n\nTo isolate the impact of retargeting methods, we use BeyondMimic [15] for training and evaluation of RL policies that track given reference motions. BeyondMimic does not depend on reward tuning and is developed independently from the retargeting methods we use, making it a fair method to evaluate them.\nWe train single-trajectory policies for a diverse subset of the LAFAN1 dataset, excluding motions with contacts other than feet. Our final dataset consists of 21 sequences with lengths ranging from 5 seconds to 2 minutes.\n\nFor evaluation, we compare success rates measured using a very strict success definition that requires the policy to complete the reference motion in its entirety for the rollout to be considered successful. We evaluate each policy a significant number of times under conditions that take into account observation noise (both from noisy sensors and from state estimation algorithms), errors in model parameter estimation, and network latency in the controller.\nWe also conduct a user study to evaluate the perceptual faithfulness of the retargeted motions to the source human motion. This additionally tells us if the policies will be learning the motions we intend them to learn, or if they are learning a variant (which might be easier or harder to track).\n\nOur end-to-end experiments demonstrate that the choice of retargeting method critically impacts humanoid performance. While policies trained on motions retargeted with different methods are generally capable of tracking a wide range of motions, including both simple and highly dynamic ones (consistent with results presented in prior work), there are a few exceptions where artifacts introduced during retargeting make it more difficult (and in some cases impossible) for the policy to learn effectively. These cases highlight that without the extensive reward engineering found in prior works, retargeting artifacts do pose challenges for certain motions and reduce policy performance. We found that foot penetration, self-intersection, and abrupt velocity spikes are all critical artifacts that should be avoided during retargeting. Additionally, we note that the initial frame of the reference motion can greatly impact whether the policy is able to start tracking it or if it fails immediately (regardless of retargeting method used), something that was observed in prior work [7]. Finally, our results show that GMR comes very close to the Unitree retarget dataset in both faithfulness score and policy success rate, while also having higher success rates and lower motion tracking errors than the other methods. In summary, the contribution of this work includes:\n\nA new general motion retargeting method, GMR, that addresses issues found in other retargeters and produces high-quality retargeted motion from a wide range of human motion.\n\nA comprehensive study on the impact of retargeted reference motion quality on the performance of humanoid motion tracking policies.\n\n1. A new general motion retargeting method, GMR, that addresses issues found in other retargeters and produces high-quality retargeted motion from a wide range of human motion.\n\n2. A comprehensive study on the impact of retargeted reference motion quality on the performance of humanoid motion tracking policies.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的人形机器人运动跟踪策略面临人类与人形机器人之间的体现差距。  \n2. 现有的运动重定向方法在重定向过程中引入的伪影影响了策略的性能。  \n3. 过度的奖励调节和领域随机化对现有方法的成功至关重要。  \n\n【提出了什么创新的方法】  \n提出了一种新的重定向方法，称为General Motion Retargeting (GMR)，旨在解决现有重定向方法中的问题。GMR使用简单灵活的非均匀局部缩放程序，结合两阶段优化，生成高质量的人类运动重定向数据。通过与其他开源重定向方法（PHC和ProtoMotions）以及高质量闭源数据集（Unitree）进行比较，GMR在跟踪性能和对源运动的忠实度方面表现优异。实验结果显示，GMR在策略成功率和运动跟踪误差方面均优于其他方法，接近闭源基准的表现。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Performance-Guided Refinement for Visual Aerial Navigation using Editable Gaussian Splatting in FalconGym 2.0",
            "authors": "Yan Miao,Ege Yuceel,Georgios Fainekos,Bardh Hoxha,Hideki Okamoto,Sayan Mitra",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02248",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02248",
            "arxiv_html_link": "https://arxiv.org/html/2510.02248v1",
            "abstract": "Visual policy design is crucial for aerial navigation.\nHowever, state-of-the-art visual policies often overfit to a single track and their performance degrades when track geometry changes.\nWe develop FalconGym 2.0, a photorealistic simulation framework built on Gaussian Splatting (GSplat) with an Edit API that programmatically generates diverse static and dynamic tracks in milliseconds.\nLeveraging FalconGym 2.0’s editability, we propose a Performance-Guided Refinement (PGR) algorithm, which concentrates visual policy’s training on challenging tracks while iteratively improving its performance.\nAcross two case studies (fixed-wing UAVs and quadrotors) with distinct dynamics and environments, we show that a single visual policy trained with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in generalization and robustness: it generalizes to three unseen tracks with 100% success without per-track retraining and maintains higher success rates under gate-pose perturbations.\nFinally, we demonstrate that the visual policy trained with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30 trials spanning two three-gate tracks and a moving-gate track.",
            "introduction": "Visual aerial navigation is critical for applications such as mapping, search-and-rescue, environmental monitoring and racing.\nRecent progress in photorealistic simulation environments has fueled zero-shot sim-to-real success for visual aerial navigation.\nNotably,\nSOUS VIDE [1] used Gaussian Splatting (GSplat) [2] to reconstruct an indoor lab and achieved zero-shot sim-to-real navigation;\nFalconGym [3] used NeRF [4] to build digital twins of three racing tracks and demonstrated zero-shot sim-to-real transfer of quadrotor gate crossing via imitation learning;\nGeles et al. [5] reported strong sim-to-real performance with a vision-only asynchronous actor-critic on three quadrotor racing tracks.\nWhile effective, these benchmarks face generalization limits:\nall three achieve high performance on their training tracks but do not generalize to unseen tracks (as we also confirm in Section IV), restricting broader applicability.\nGRaD-Nav [6] improves robustness to different gate positions and background distractors by using one policy, but still relies on constructing multiple GSplat tracks first and training in those.\n\nIn this paper, we focus on developing a single visual policy that can traverse a family of tracks, where each track consists of an ordered sequence of tight racing gates, as shown in Figure 1 and Figure 4.\nTo enable such cross-track generalization from limited real data, we develop FalconGym 2.0.\nFrom just minutes of real-world videos, FalconGym 2.0 can generate arbitrarily many synthetic yet photorealistic tracks for training without additional real-world data collection.\nThis is achieved by our Edit API, as shown in Figure 3, that allows programmatic editing of gates in the GSplat to create diverse tracks in milliseconds.\nBeyond static edits, our Edit API also supports 4D time-varying simulation with moving gates, enabling dynamic tracks in Section IV.\nAlthough this paper focuses on aerial navigation and gate-editing, the same Edit API could readily extend to broader robotics settings (e.g., obstacle placement for ground robots).\n\nFalconGym 2.0’s editable capability to generate arbitrarily many photorealistic tracks unlocks a range of visual policy training strategy from active learning to curriculum learning  [7].\nWe develop a Performance-Guided Refinement (PGR) algorithm that: (i) identifies challenging tracks where the visual policy underperforms; (ii) synthesizes similar yet slightly different challenging tracks with the Edit API to augment the training dataset; and (iii) iteratively refines the visual policy via imitation learning from a state-based expert.\n\nBeyond the Edit API and the PGR algorithm it enables, FalconGym 2.0 improves over FalconGym [3] via:\n(i) replacing NeRF rendering with fast GSplat to accelerate training;\n(ii) eliminating expensive motion capture with an accessible ArUco marker for world-frame simulation reconstruction.\n\nThrough two case studies (fixed-wing UAVs and quadrotors) with different dynamics, environments and gate geometries, we show that our visual policy trained with PGR can generalize to three unseen tracks in FalconGym 2.0 with 100% success.\nMoreover, we outperform state-of-the-art visual baselines [3, 5] in both generalization and robustness: our single visual policy operates across three unseen tracks, where baselines require separate per-track policies, and we maintain higher performance under gate-pose perturbations.\nFinally, the visual policy trained with PGR in FalconGym 2.0 can zero-shot transfer to a quadrotor hardware, achieving 98.6% (69 / 70 gates) success rate in 30 hardware trials spanning two 3-gate tracks and a moving-gate track, as shown in Figure 6.\n\nIn summary, our contributions are:\n(1) FalconGym 2.0: an editable photorealistic simulation framework based on GSplat that supports fast world-frame modification of environment configurations.\n(2) Performance-Guided Refinement (PGR): an algorithm that leverages FalconGym 2.0’s editability to expose the visual policy’s training to challenging tracks and iteratively improve performance.\n(3) Zero-Shot Sim-to-real: Our visual policy trained with PGR in FalconGym 2.0 can be zero-shot transferred to a real quadrotor hardware to traverse three unseen tracks.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有视觉策略在单一轨道上表现良好，但在轨道几何变化时性能下降。  \n2. 需要一种方法来提高视觉策略的跨轨道泛化能力，以适应不同的环境和动态。  \n\n【提出了什么创新的方法】  \n本文提出了FalconGym 2.0，一个基于Gaussian Splatting的可编辑仿真框架，能够快速生成多样的静态和动态轨道。通过引入Performance-Guided Refinement (PGR)算法，重点训练视觉策略在具有挑战性的轨道上，并通过模仿学习迭代提升其性能。实验结果表明，使用PGR训练的视觉策略在三个未见轨道上实现了100%的成功率，并在硬件上实现了98.6%的成功率，展示了其优越的泛化和鲁棒性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis",
            "authors": "Jialin Gao,Donghao Zhou,Mingjian Liang,Lihao Liu,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02178",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02178",
            "arxiv_html_link": "https://arxiv.org/html/2510.02178v1",
            "abstract": "3D indoor layout synthesis is crucial for creating virtual environments.\nTraditional methods struggle with generalization due to fixed datasets.\nWhile recent LLM and VLM-based approaches offer improved semantic richness, they often lack robust and flexible refinement, resulting in suboptimal layouts. We develop DisCo-Layout, a novel framework that disentangles and coordinates physical and semantic refinement.\nFor independent refinement, our Semantic Refinement Tool (SRT) corrects abstract object relationships, while the Physical Refinement Tool (PRT) resolves concrete spatial issues via a grid-matching algorithm.\nFor collaborative refinement, a multi-agent framework intelligently orchestrates these tools, featuring a planner for placement rules, a designer for initial layouts, and an evaluator for assessment.\nExperiments demonstrate DisCo-Layout’s state-of-the-art performance, generating realistic, coherent, and generalizable 3D indoor layouts.\nOur code will be publicly available.",
            "introduction": "3D indoor layout synthesis is driving advancements in various fields, including virtual reality (Patil et al., 2024), video games (Hu et al., 2024), and embodied AI (Krantz et al., 2020; Nasiriany et al., 2024; Kolve et al., 2017). This process involves creating a full 3D environment based on a natural language prompt and a library of 3D assets.\nThe goal is to produce layouts that are not only physically plausible but also semantically aligned with the given instructions. The capacity to convert abstract, high-level commands into realistic, interactive 3D environments is becoming increasingly vital as downstream applications demand more automation and user-specific customization.\n\nTraditional methods (Paschalidou et al., 2021; Tang et al., 2024; Yang et al., 2024a), which learn from predefined layout datasets (Fu et al., 2021), are consequently confined to in-domain 3D assets and fixed placement schemes. This limitation hinders their ability to generalize to unseen objects, novel layouts, or open-ended user instructions, reducing utility for real-world simulations and interactive environments.\n\nThe rise of large language models (LLMs) and vision-language models (VLMs) has unlocked exciting new avenues in 3D layout synthesis. By tapping into their understanding of spatial and object relationships, LLM-based approaches (Feng et al., 2023; Yang et al., 2024b; Sun et al., 2025a; Ling et al., 2025) create varied and semantically rich layouts directly from natural language prompts, even when working with open-domain 3D assets. For example, these models are able to deduce that chairs should face a table or bookshelves should align with walls.\n\nHowever, despite this progress, several significant limitations persist:\n(i) Lack of systematic laxyout refinement: Some methods (Feng et al., 2023; Yang et al., 2024b) directly reason object placements using a single LLM or VLM without any refinement. As a result of these models’ inherent randomness and tendency to “hallucinate,” the generated layouts are inconsistent, physically implausible, or semantically nonsensical (Figure 1(a)).\n(ii) Inflexible refinement strategies: Other approaches attempt to refine layouts but employ intertwined mechanisms for both physical and semantic aspects (Figure 1(b)). For instance, LayoutVLM (Sun et al., 2025a) uses a unified differentiable optimization, which makes it challenging to balance competing objectives effectively. Similarly, TreeSearchGen (Deng et al., 2025) relies on VLMs to jointly assess physical and semantic criteria, often leading to interference between goals and causing suboptimal outcomes.\n\nTo tackle these limitations, we present DisCo-Layout, a novel framework that Disentangles and then Coordinates semantic and physical refinement through a VLM-based multi-agent system for 3D indoor Layout synthesis.\nThis design enables the independent and collaborative refinement process, resulting in realistic and coherent 3D indoor layouts, as illustrated in Figure 1(c).\nSpecifically, to disentangle these two refinement capabilities, DisCo-Layout first incorporates two specialized tools: the Semantic Refinement Tool (SRT), which corrects high-level abstract object relationships (e.g., chairs facing tables) via feedback-driven adjustment, and the Physical Refinement Tool (PRT), which addresses low-level concrete spatial issues (e.g., asset collisions) using a grid-matching algorithm.\nThis separation ensures modularity and prevents conflicts between semantic and physical criteria.\nFurthermore, DisCo-Layout employs a multi-agent system to coordinate these tools with three specialized VLM-based agents: a planner, which derives high-level placement rules; a designer, which predicts initial 2D coordinates; and an evaluator, which assesses the layout for further refinement. This dedicated collaboration dynamically orchestrates the refinement process.\n\nExtensive experiments demonstrate that DisCo-Layout achieves state-of-the-art performance, generating realistic, coherent, and physically plausible 3D indoor layouts. It also demonstrates robust generalization capabilities across diverse assets and natural language instructions, establishing a new baseline for 3D indoor layout synthesis.\n\nOur main contributions are as follows:\n\nWe present DisCo-Layout, a novel approach to 3D indoor layout synthesis that disentangles and coordinates physical and semantic refinement through specialized tools and a multi-agent system, producing realistic, coherent, and physically plausible layouts.\n\nWe formulate two distinct refinement mechanisms: the Semantic Refinement Tool (SRT), designed to address abstract object relationships, and the Physical Refinement Tool (PRT), engineered for precise spatial adjustments.\n\nWe develop a VLM-based multi-agent framework comprising a planner for deriving high-level layout rules, a designer for generating initial layouts, and an evaluator for assessing layout quality and guiding refinement.\n\nThrough extensive experiments, we demonstrate that DisCo-Layout achieves superior semantic accuracy and physical plausibility in 3D indoor layout synthesis compared to current state-of-the-art methods.\n\n1. We present DisCo-Layout, a novel approach to 3D indoor layout synthesis that disentangles and coordinates physical and semantic refinement through specialized tools and a multi-agent system, producing realistic, coherent, and physically plausible layouts.\n\n2. We formulate two distinct refinement mechanisms: the Semantic Refinement Tool (SRT), designed to address abstract object relationships, and the Physical Refinement Tool (PRT), engineered for precise spatial adjustments.\n\n3. We develop a VLM-based multi-agent framework comprising a planner for deriving high-level layout rules, a designer for generating initial layouts, and an evaluator for assessing layout quality and guiding refinement.\n\n4. Through extensive experiments, we demonstrate that DisCo-Layout achieves superior semantic accuracy and physical plausibility in 3D indoor layout synthesis compared to current state-of-the-art methods.",
            "llm_summary": "【论文的motivation是什么】  \n1. 3D indoor layout synthesis is essential for creating realistic virtual environments.  \n2. Traditional methods struggle with generalization due to fixed datasets and lack robust refinement.  \n3. Recent LLM and VLM approaches often produce suboptimal layouts due to intertwined refinement strategies.  \n\n【提出了什么创新的方法】  \nDisCo-Layout introduces a framework that disentangles and coordinates semantic and physical refinement for 3D indoor layout synthesis. It employs two specialized tools: the Semantic Refinement Tool (SRT) for correcting abstract object relationships and the Physical Refinement Tool (PRT) for addressing spatial issues. A multi-agent system orchestrates these tools, featuring a planner, designer, and evaluator to enhance the refinement process. The framework demonstrates state-of-the-art performance, generating realistic, coherent, and generalizable 3D indoor layouts, thus establishing a new baseline in the field.  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Product Digital Twin Supporting End-of-life Phase of Electric Vehicle Batteries Utilizing Product-Process-Resource Asset Network",
            "authors": "Sara Strakosova,Petr Novak,Petr Kadera",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "This work has been submitted to the IEEE for possible publication.",
            "pdf_link": "https://arxiv.org/pdf/2510.02167",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02167",
            "arxiv_html_link": "https://arxiv.org/html/2510.02167v1",
            "abstract": "In the context of the circular economy, products in their end-of-life phase should be either remanufactured or recycled. Both of these processes are crucial for sustainability and environmental conservation. However, manufacturers often do not support these processes enough by not sharing relevant data. This paper proposes use of a digital twin technology, which is capable to help optimizing the disassembly processes to reduce ecological impact and enhance sustainability. The proposed approach is demonstrated through a disassembly use-case of the product digital twin of an electric vehicle battery. By utilizing product digital twins, challenges associated with the disassembly of electric vehicle batteries can be solved flexibly and efficiently for various battery types. As a backbone for the product digital twin representation, the paper uses the paradigm of product-process-resource asset networks (PAN). Such networks enable to model relevant relationships across products, production resources, manufacturing processes, and specific production operations that have to be done in the manufacturing phase of a product. This paper introduces a Bi-Flow Product-Process-Resource Asset Network (Bi-PAN) representation, which extends the PAN paradigm to cover not only the manufacturing, but also the remanufacturing/recycling phase.",
            "introduction": "The circular economy is an economic system that aims to keep resources in use for as long as possible, through a closed-loop approach of reducing, reusing, remanufacturing, and recycling materials and products. It contrasts with the traditional linear economy, where resources are extracted, transformed into products, used, and disposed of as waste [1].\n\nRecycling electric vehicle (EV) batteries is crucial for the circular economy because it can help to reduce waste and conserve resources. EV batteries contain valuable materials such as lithium, cobalt, nickel or others depending on the specific battery chemistry, which can be recovered and reused. By recycling these materials, the need for mining new resources is reduced, which can have positive environmental and social impacts [2]. Conventional methods for EV battery recycling are based on crushing or shredding the entire battery or battery modules, separating the chemical elements and their reusing for new batteries. On the contrary, another approach is based on reusing battery modules in a satisfactory state-of-health for stationary applications in households, industrial factories, energy sources, or energy storages [3]. This is the case that motivates the presented research. To be able to get battery modules in a good state, a support for dismantling the entire EV battery is needed.\nMoreover, recycling or reusing EV batteries can also provide a solution for managing their end-of-life.\nAs the number of EVs on the road is continually increasing, the number of batteries reaching their end-of-life will also increase. If these batteries are not properly managed, they can pose environmental and safety risks.\n\nProduction processes can be efficiently supported by a technology of digital twins, which pose digital replicas for physical systems. In the domain of EV batteries, integration with digital twins is already a researched matter. Articles [4] and [5] provide extensive information on digital twin for EV batteries in several possible applications and usages as well as highlighting the advantages of using digital twins for remanufacturing and recycling EV batteries.\n\nThis approach highlights the use of digital twins in the end-of-life phase of EV batteries utilizing the so-called Product Digital Twin (PDT). PDT is enhancing the sustainability and efficiency of product lifecycle management processes. PDT brings to manufacturers and remanufacturers a comprehensive tool for planning and optimizing products.\n\nIn this paper, the concept of a Bi-Flow Product-Process-Resource Asset Network (Bi-PAN) is introduced. Bi-PAN is designed to support both assembly and disassembly workflows within a single framework. Built on the foundation of the Product-Process-Resource Asset Network (PAN) [6], the extended framework preserves the representation of products, processes, and resources accompanied by their links, while incorporating also additional features to support reverse process flows. Thus, Bi-PAN supports the end-of-life phase of a product seamlessly together with production data. By integrating disassembly capabilities into the PAN, it serves to efficiently facilitate the design of remanufacturing and recycling processes in addition to the assembly process.\n\nThe Bi-PAN and Product Digital Twin together offer a holistic approach to product lifecycle optimization, facilitating the transition towards more sustainable and circular manufacturing practices.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统线性经济模式导致资源浪费，缺乏有效的循环利用机制。  \n2. 电动汽车电池的回收和再利用面临管理和技术挑战。  \n3. 制造商在支持电池回收和再制造过程中缺乏数据共享。  \n\n【提出了什么创新的方法】  \n本文提出了一种双流产品-过程-资源资产网络（Bi-PAN），旨在支持电动汽车电池的组装和拆卸工作流程。通过整合数字双胞胎技术，Bi-PAN能够优化电池的拆解过程，从而提高资源的回收效率和可持续性。该方法展示了如何在电池的生命周期管理中实现更高效的再制造和回收流程，促进了向循环经济的转型。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "SCANS: A Soft Gripper with Curvature and Spectroscopy Sensors for In-Hand Material Differentiation",
            "authors": "Nathaniel Hanson,Austin Allison,Charles DiMarzio,Taşkın Padır,Kristen L. Dorsey",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted to IEEE Robotics & Automation Letters Special Issue on Interdisciplinarity and Widening Horizons in Soft Robotics",
            "pdf_link": "https://arxiv.org/pdf/2510.02164",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02164",
            "arxiv_html_link": "https://arxiv.org/html/2510.02164v1",
            "abstract": "We introduce the soft curvature and spectroscopy (SCANS) system: a versatile, electronics-free, fluidically actuated soft manipulator capable of assessing the spectral properties of objects either in hand or through pre-touch caging. This platform offers a wider spectral sensing capability than previous soft robotic counterparts. We perform a material analysis to explore optimal soft substrates for spectral sensing, and evaluate both pre-touch and in-hand performance. Experiments demonstrate explainable, statistical separation across diverse object classes and sizes (metal, wood, plastic, organic, paper, foam), with large spectral angle differences between items. Through linear discriminant analysis, we show that sensitivity in the near-infrared wavelengths is critical to distinguishing visually similar objects. These capabilities advance the potential of optics as a multi-functional sensory modality for soft robots. The complete parts list, assembly guidelines, and processing code for the SCANS gripper are accessible at: https://parses-lab.github.io/scans/.",
            "introduction": "Soft robot grippers are increasingly used in commercial, healthcare, and industrial applications for material handling. While commercial and research-grade sensors have improved force distribution and proprioception, they still lack the ability to distinguish subtle differences in material properties—texture, moisture content, or internal structure—between visually similar objects. Moreover, soft grippers often lack sensing modalities that allow them to perceive object properties before contact. This absence of pre-touch sensing limits their ability to anticipate how best to grasp, manipulate, or sort objects, especially when visual information is ambiguous or occluded. Access to material composition before or during grasping would allow soft grippers to more accurately classify objects and infer functional properties such as ripeness or fragility, thereby enhancing manipulation, planning, and execution.\n\nSpectral analysis of material composition, such as in the near-infrared (NIR) and short-wave infrared spectrum at wavelengths from 800−2500800-2500 nm [1], is a widely used set of techniques in laboratory environments. Spectrometers measure the intensity of the reflected light spectrum created by light-surface interactions.\nWhen compared to the illumination source, the patterns of peaks or troughs in the spectrum reveal the presence of particular chemical bonds and therefore the composition of the material (e.g., water or sugar content) [2].\n\nIn traditional robotics, material recognition has been explored using visual and non-visual methods [3], yet existing techniques rely on rigid sensors that are not compatible with soft grippers.\nSensors fabricated from compliant materials [4] largely focus on measuring mechanical forces,\nbut distinguishing chemically or visually similar materials remains a challenge that must be solved to translate such approaches to practical applications.\nThese gaps underscore a fundamental need in robotics: mechanically robust and compliant sensors that perceive material properties.\n\nOur present work builds upon recent advances in soft optical sensing and robotics-centric spectroscopy to introduce the soft curvature and spectroscopy (SCANS) gripper. SCANS leverages optical fibers to estimate finger curvature and perform in-hand spectral analysis without the need for embedded electronics or interconnects within the actuated portion of the finger—an area where components are often susceptible to mechanical failure. As such, the gripper acquires high-fidelity material signatures in a fully flexible and mechanically robust platform. This co-location allows for adaptive, safer interaction with fragile objects, and maintains high-fidelity spectral sensing without separate rigid components or electronic interconnects as in prior rigid systems [5, 6]. Figure 1 illustrates the SCANS gripper and shows the recorded spectral reflectance from an apple before contact and during grasp.\n\nWith the SCANS gripper, we address the major challenge of designing a spectral sensing finger with sufficient optical transmission across wavelengths from visible light (400 nm) to short-wave infrared (1700 nm).\nOur key contributions are:\n\nInfrared spectral characterization of common soft robotic materials to inform optical sensing design.\n\nInfrared spectral characterization of common soft robotic materials to inform optical sensing design.\n\nA soft sensing architecture for finger curvature estimation and spectral signature acquisition.\n\nAn extensible soft gripper design enabling pre-grasp object identification via integrated spectroscopy.\n\n1. Infrared spectral characterization of common soft robotic materials to inform optical sensing design.\n\n2. A soft sensing architecture for finger curvature estimation and spectral signature acquisition.\n\n3. An extensible soft gripper design enabling pre-grasp object identification via integrated spectroscopy.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有软抓手缺乏区分视觉相似物体的材料属性能力。  \n2. 软抓手缺少在接触前感知物体属性的传感能力。  \n3. 需要机械稳健且柔性传感器以感知材料特性。  \n\n【提出了什么创新的方法】  \n本文提出了SCANS系统，这是一种软性抓手，结合了曲率和光谱传感器，能够在抓取前和抓取过程中对物体进行光谱分析。SCANS利用光纤来估计手指的曲率，并在没有嵌入电子元件的情况下进行高保真材料特征采集。这种设计使得抓手在与脆弱物体互动时更加安全，同时保持高效的光谱感知能力。实验结果表明，SCANS能够有效区分不同材料，提升了软机器人在复杂环境中的操作能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Stand Up, NAO! Increasing the Reliability of Stand-Up Motions Through Error Compensation in Position Control",
            "authors": "Philip Reichenberg,Tim Laue",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02129",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02129",
            "arxiv_html_link": "https://arxiv.org/html/2510.02129v1",
            "abstract": "Stand-up motions are an indispensable part of humanoid robot soccer. A robot incapable of standing up by itself is removed from the game for some time.\nIn this paper, we present our stand-up motions for the NAO robot. Our approach dates back to 2019 and has been evaluated and slightly expanded over the past six years. We claim that the main reason for failed stand-up attempts are large errors in the executed joint positions.\nBy addressing such problems by either executing special motions to free up stuck limbs such as the arms, or by compensating large errors with other joints, we significantly increased the overall success rate of our stand-up routine. The motions presented in this paper are also used by several other teams in the Standard Platform League, which thereby achieve similar success rates, as shown in an analysis of videos from multiple tournaments.",
            "introduction": "In this paper, we present our approach for reliable stand-up motions on the NAO robot, functioning on a large spectrum of different aspects of wear and tear. The NAO robot is used by all teams in the RoboCup Standard Platform League (SPL). Here, the rules state that the robots need to successfully stand up by themselves, otherwise they become removed from the pitch for a given time. Therefore, having a working motion for such situations is crucial, especially since falling over – for different reasons – is very common as described in [18].\nThe approach presented in this paper has been used by the team B-Human since 2019 [17] and did only undergo minor changes since then. During this period, it was also adopted by a number of other teams, which base their developments on different B-Human code releases [10, 1, 12, 16, 19, 23], which provides a significant basis for its evaluation.\n\nThe importance of research on this topic increased at RoboCup 2016. Here, for the first time, a subset of games was played on 8 mm8\\text{\\,}\\mathrm{mm} artificial turf instead of flat felt carpet. Since 2017, all SPL games are played on such a surface. This lead to a significant decline of the success rate of stand-up motions, as shown in Footnote˜1. Even after two years, the analyzed teams still showed significant problems regarding their stand-up motions. Previously, the robots primarily failed their routines when something was broken or a mismatch in the kinematics occurred, like a stuck arm after a goal keeper jump. With the more challenging carpet, which offers a slightly less stable base for standing, the robots fell over more often while standing up.\n\nFurthermore, over the years, many SPL teams operate a growing number of older robots, which are subject to wear and tear, making them particularly susceptible to problems when getting up. This is an important aspect, which is addressed by the approach presented in this paper.\n\nThe remainder of this work is organized as follows: Section˜2 discusses related work. This is followed by an analysis of the causes for failed stand-up attempts in Section˜3. Our approaches for compensation, balancing as well as further improvements are described in section 4 and section 5. Subsequently, an overview of the architecture is given in section 6. Finally, section 7 presents an evaluation that covers several teams at multiple tournaments.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人在RoboCup比赛中需要独立站立，失败会导致被移除。  \n2. 由于地面条件变化和机器人老化，现有的站立动作成功率下降。  \n3. 现有的站立动作对关节位置误差敏感，导致失败。  \n\n【提出了什么创新的方法】  \n本文提出了一种改进的NAO机器人站立动作，通过特殊动作释放被卡住的肢体或通过其他关节补偿大误差来提高成功率。该方法经过六年的评估和扩展，已被多个团队采用，显著提升了其站立成功率。通过分析多个比赛的视频，验证了这些改进措施的有效性，确保了机器人在复杂环境中的可靠性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions",
            "authors": "Yunhan Lin,Wenqi Wu,Zhijie Zhang,Huasong Min",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02104",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02104",
            "arxiv_html_link": "https://arxiv.org/html/2510.02104v1",
            "abstract": "The existing language-driven grasping methods struggle to fully handle ambiguous instructions containing implicit intents. To tackle this challenge, we propose LangGrasp, a novel language-interactive robotic grasping framework. The framework integrates fine-tuned large language models (LLMs) to leverage their robust commonsense understanding and environmental perception capabilities, thereby deducing implicit intents from linguistic instructions and clarifying task requirements along with target manipulation objects. Furthermore, our designed point cloud localization module, guided by 2D part segmentation, enables partial point cloud localization in scenes, thereby extending grasping operations from coarse-grained object-level to fine-grained part-level manipulation. Experimental results show that the LangGrasp framework accurately resolves implicit intents in ambiguous instructions, identifying critical operations and target information that are unstated yet essential for task completion. Additionally, it dynamically selects optimal grasping poses by integrating environmental information. This enables high-precision grasping from object-level to part-level manipulation, significantly enhancing the adaptability and task execution efficiency of robots in unstructured environments. More information and code are available here: https://github.com/wu467/LangGrasp.",
            "introduction": "With the increasing deployment of robots in daily environments, natural language based human-robot interaction and manipulation have significantly enhanced efficiency and task executability due to their intuitiveness and flexibility. However, accurately interpreting ambiguous or context dependent linguistic instructions in dynamic, unstructured environments, particularly those containing implicit intents, remains a critical challenge. Furthermore, existing language-guided grasping frameworks are limited to predicting object-level grasping poses, neglecting the functional distinctions between object parts. For instance, in tasks such as cutting fruit with a knife, successful execution requires not only recognition of the object’s global geometry but also a functional-structural analysis of its parts to identify the handle as the optimal grasping region.\n\nThis necessitates that robotic systems possess both efficient natural language understanding capabilities for parsing complex linguistic constructs and fundamental visual perception for fusing semantic-geometric cues, enabling commonsense-driven intent disambiguation. LLMs, leveraging their vast world knowledge, demonstrate significant potential in resolving linguistic ambiguity, contextual reasoning, and functional analysis. Integrating LLMs into human-robot interaction frameworks to enhance the comprehension of complex instructions and task planning efficiency has emerged as a prominent research topic.\n\nPrevious studies [1],[2] treat natural language as high-level semantic cues, combining visual information with pretrained visual grounding models to localize target object and determine grasping region. Such approaches rely on explicit semantic cues and unambiguous instruction structures, which results in their failure to handle linguistically ambiguous or polysemous expressions. Recent work [3] has integrated LLMs into this paradigm, utilizing their advanced natural language understanding and reasoning capabilities to tackle challenges in extracting target objects and task parameters from vague or inference-dependent instructions. However, due to the inherent uncertainty and redundancy in LLM outputs, even with optimized prompt engineering, are mains essential to extract actionable information, thus increasing operational complexity within the pipeline.\n\nTo address these challenges, we propose LangGrasp, a novel language interactive grasping framework, as illustrated in Figure 1. The framework leverages fine-tuning to preserve the LLM’s vast world knowledge while significantly enhancing its reasoning and decision-making capabilities in open-ended interaction scenarios. During the interaction phase, the perception and inference module generates operation sequences in JSON format based on the environmental context and multi-turn dialogue history. Unlike unstructured text responses, structured output enables direct extraction of target objects and task specifications without additional parsing steps. At the operational level, we have designed a point cloud localization module to meet the precision requirements in open environments. By leveraging a pretrained 2D part segmentation model, our point cloud localization module achieves localization for target objects or their parts. When manipulating a composite object with multiple parts, the framework dynamically selects the optimal manipulation region by synthesizing geometric shapes, part attributes, and task requirements. Finally, the grasp pose detection module predicts the grasp pose for the localized point cloud.\n\nAs shown in Figure 1, LLM is used as the core of human-robot interaction in LangGrasp, with three instructions demonstrated. The first is a simple instruction: the robot is asked to bring a banana to the user, and it can select any grasping pose to pick up the banana. The second and third instructions generate part-level grasp poses, supporting long-horizon task execution by combining linguistic instructions with commonsense reasoning. Specifically, for the instruction “Hand me the hammer” the robot grasps the hammer’s head while orienting the handle toward the user. This commonsense-based strategy ensures the stability of the grasp and optimizes ergonomic comfort during the handover process. In the third case, when executing carpentry instructions that require hammer strikes, the robot selects the handle of the hammer to ensure the correct striking posture. These diverse strategies enhance adaptability to complex instructions, while also improving operational reliability and precision.\n\nThe main contributions of this paper are as follows:\n\n(1) We propose LangGrasp, a novel language interactive grasping framework that leverages the commonsense reasoning and environmental perception capabilities of LLMs to interpret natural language instructions of varying complexity, enabling fine-grained analysis of target objects. Through the designed perception and inference module, point cloud localization module, and grasp pose detection module, LangGrasp achieves precise grasping region selection, advancing the grasping targets from the object-level to the part-level.\n\n(2) We construct a fine-tuning dataset for LLMs specifically for robotic grasping tasks, enhancing the parsability and interpretability of LLM outputs when processing different complexities of linguistic instructions.\n\n(3) Two platforms, a desktop experimental scene and a cabinet experimental scene, are designed to validate the efficiency of the proposed method. The experimental results demonstrate the advantages of proposed modules and the effectiveness of the LangGrasp framework.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有语言驱动的抓取方法难以处理含有隐含意图的模糊指令。  \n2. 机器人在动态、非结构化环境中准确理解复杂语言指令仍然是一个关键挑战。  \n3. 现有方法主要关注对象级抓取，忽视了对象部件之间的功能区别。  \n\n【提出了什么创新的方法】  \nLangGrasp是一个新颖的语言交互式抓取框架，集成了微调的大型语言模型（LLMs），以解析模糊指令并明确任务要求。该框架通过感知和推理模块生成操作序列，利用点云定位模块实现目标对象或部件的精确定位，并通过抓取姿态检测模块预测抓取姿态。实验结果表明，LangGrasp能够准确解析隐含意图，动态选择最佳抓取姿势，从而显著提高机器人在非结构化环境中的适应性和任务执行效率。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction",
            "authors": "Lingxiang Hu,Naima Ait Oufroukh,Fabien Bonardi,Raymond Ghandour",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02080",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02080",
            "arxiv_html_link": "https://arxiv.org/html/2510.02080v1",
            "abstract": "The application of monocular dense Simultaneous Localization and Mapping (SLAM) is often hindered by high latency, large GPU memory consumption, and reliance on camera calibration. To relax this constraint, we propose EC3R-SLAM, a novel calibration-free monocular dense SLAM framework that jointly achieves high localization and mapping accuracy, low latency, and low GPU memory consumption. This enables the framework to achieve efficiency through the coupling of a tracking module, which maintains a sparse map of feature points, and a mapping module based on a feed-forward 3D reconstruction model that simultaneously estimates camera intrinsics. In addition, both local and global loop closures are incorporated to ensure mid-term and long-term data association, enforcing multi-view consistency and thereby enhancing the overall accuracy and robustness of the system. Experiments across multiple benchmarks show that EC3R-SLAM achieves competitive performance compared to state-of-the-art methods, while being faster and more memory-efficient. Moreover, it runs effectively even on resource-constrained platforms such as laptops and Jetson Orin NX, highlighting its potential for real-world robotics applications. We will make our source code publicly available. Project page: https://h0xg.github.io/ec3r/.",
            "introduction": "Monocular dense SLAM has found wide applications in autonomous driving, robotic navigation, and AR/VR [1, 2].\nExisting approaches exploit neural priors [3, 2, 4, 5, 6] to recover dense geometry from monocular input.\nAlthough effective, these methods often suffer from high GPU memory usage [3, 2, 7], significant latency [5, 1, 8, 7], and complex optimization pipelines [4, 1], which limit their suitability for real-time deployment.\nMore recently, a new class of 3D reconstruction models [9, 10, 11, 12, 13] has demonstrated the ability to recover dense geometry directly from uncalibrated RGB frames. Among them, VGGT [12] and Fast3R [13] are able to reconstruct thousands of images within seconds,\nbut their high GPU memory requirements render them unsuitable for use on consumer-grade hardware [14, 2].\n\nTo overcome these limitations, we propose EC3R-SLAM, an Efficient and Consistent 3D Reconstruction framework for calibration-free monocular dense SLAM. Our framework employs lightweight feature-based tracking to select keyframes, while only a small subset of frames (five in our implementation) is forwarded to VGGT for feed-forward reconstruction, generating local submaps that are subsequently fused into a global map. This strategy preserves the efficiency of feed-forward models while markedly reducing computational and memory demands: our pipeline operates with less than 10 GB of GPU memory and runs at more than 30 FPS. By comparison, the concurrent work VGGT–SLAM [2] feeds 32 frames at once,\nwhereas VGGT-Long [14] feeds more than 60 frames at once into VGGT, usually pushing memory usage beyond 20GB, which constrains practical deployment on resource-constrained robotic platforms. However, fusing multiple small submaps can introduce severe inconsistencies, a limitation also observed in VGGT–SLAM and MASt3R–SLAM [3] (see Fig. 1a). To mitigate this, we incorporate both local and global loop closure modules that establish mid-term and long-term associations, thereby enforcing multi-view consistency and alleviating misalignment.\nAs a result, EC3R-SLAM achieves accurate, real-time, and resource-efficient dense mapping and camera pose estimation, as illustrated in Fig. 1b.\n\nIn summary, our main contributions are:\n\nWe present, for the first time, an innovative method that combines lightweight feature point-based tracking with feed-forward 3D reconstruction model-based mapping, enabling an efficient pipeline for localization and mapping.\n\nWe propose a novel data association strategy that integrates local loop and global loop closure, and injects loop information into pose graph optimization, thereby enhancing multi-view consistency and significantly improving the overall accuracy of the system.\n\nBy unifying these components, we propose a new uncalibrated monocular dense SLAM framework that delivers competitive results across multiple benchmarks, while maintaining low GPU memory usage and real-time performance.",
            "llm_summary": "【论文的motivation是什么】  \n1. Monocular dense SLAM在实时部署中面临高延迟和大GPU内存消耗的问题。  \n2. 现有方法通常依赖于复杂的优化流程和相机标定，限制了其适用性。  \n3. 需要一种高效且一致的SLAM框架以适应资源受限的机器人平台。  \n\n【提出了什么创新的方法】  \n提出了EC3R-SLAM，一个高效且一致的单目稠密SLAM框架，采用轻量级特征点跟踪与前馈3D重建模型相结合。该方法通过选择关键帧并仅转发少量帧进行重建，显著降低了计算和内存需求，确保了实时性能。通过引入局部和全局回环闭合模块，增强了多视图一致性，最终实现了准确、实时且资源高效的稠密映射和相机位姿估计。实验结果表明，EC3R-SLAM在多个基准测试中表现出竞争力，同时保持低GPU内存使用。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Reducing Discomfort in Driving Simulators: Motion Cueing for Motion Sickness Mitigation",
            "authors": "Varun Kotian,Vishrut Jain,Andrea Michelle Rios Lazcano,Daan Marinus Pool,Riender Happee,Barys Shyrokau",
            "subjects": "Robotics (cs.RO); Human-Computer Interaction (cs.HC); Systems and Control (eess.SY); Optimization and Control (math.OC); Neurons and Cognition (q-bio.NC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01986",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01986",
            "arxiv_html_link": "https://arxiv.org/html/2510.01986v1",
            "abstract": "Driving simulators are increasingly used in research and development. However, simulators often cause motion sickness due to downscaled motion and unscaled veridical visuals. In this paper, a motion cueing algorithm is proposed that reduces motion sickness as predicted by the subjective vertical conflict (SVC) model using model predictive control (MPC).\nBoth sensory conflict and specific force errors are penalised in the cost function, allowing the algorithm to jointly optimise fidelity and comfort.",
            "introduction": "Since the inception of driving simulator technology, a key challenge has been to bring the driving simulator experience closer to the real vehicle experience.\nHowever, the realistic (unscaled) visual motion is still presented with scaled or even without any physical motion. This causes a mismatch between expected and perceived motion, eliciting motion sickness [1].\nMotion sickness is a syndrome that arises as a consequence of a wide range of self-motion and orientation cues. It is characterised by symptoms of sweating, headache, dizziness, stomach awareness, where these symptoms usually grow in severity until nausea, retching and ultimately vomiting occurs [2].\nThe mechanisms behind the development and evolution of motion sickness have been studied extensively, relying heavily on models that predict sensory conflicts based on well-known mathematical models of vestibular and visual sensory integration [3, 4, 5, 6, 7].\n\nIn driving simulators, motion sickness typically occurs due to the intricate interplay of synthetic parallel visual, vestibular, and proprioceptive cues.\nA recent meta-analysis on simulator sickness [8] reviewed 41 studies and reported modest sickness levels across different simulator configurations. It found that improved visual fidelity significantly reduced sickness in motion-base simulators, but not in fixed-base systems. Mechanical motion presence had a minor, non-significant effect on sickness.\nActive driving reduced sickness compared to passive driving, although these comparisons were across different studies.\nNotably, active driving rarely induces sickness in real vehicles, highlighting a gap between simulation and reality.\nIn a comparative study [9], three motion cueing strategies were assessed against a fixed-base setup. While the motion-cueing strategies improved perceived realism, they were also associated with increased sickness incidence, highlighting a trade-off between immersion and physiological comfort.\n\nIn this paper, we aim to develop a motion cueing algorithm (MCA) that can recreate specific forces from a real vehicle drive in a driving simulator, while explicitly minimizing the level of motion sickness.\nTo the best of the authors’ knowledge, no prior work has established an MCA explicitly aimed at reproducing or reducing motion sickness in a simulator environment.\nFor this, we need an MCA that can take into account both the simulator dynamics and the time evolution of motion sickness.\nNew approaches to develop MCAs such as [10, 11, 12] based on model predictive control (MPC) are well suited as they solve a constrained optimal control problem and can take advantage of models of simulator dynamics and motion sickness development in their optimization.\nTraditionally, simple metrics of motion sickness severity are optimised, such as the motion sickness dose value (MSDV) in [13, 14, 15, 16] for vehicle trajectory planning.\nWhile effective in optimising the magnitude of horizontal motion in real vehicles, such methods will not capture the effects of the frequency of motion, platform rotation, and conflicting visual and mechanical motion.\nTo capture these effects, we selected the 6-DoF subjective vertical conflict (SVC) model from [4]. Comparing a range of models, this model best matched motion sickness data collected in several laboratory experiments and real-world driving studies [7]. In [17], we extended this SVC model with a personalised motion sickness accumulation model. We calibrated the accumulation parameters such that the model predicts sickness rated with the well-established MISC scale. Moreover, a population model was added to predict the sickness variation within a population.\n\nThis paper presents a novel MPC-based MCA that mitigates motion sickness, using these motion sickness models to find a balance between motion sickness reduction and specific force recreation.\nWe validate the MCAs in human-in-the-loop experiments, also including a reference adaptive washout MCA and a no motion condition.\n\nThe contributions of the work are outlined below:\n\nIncorporated a six-degree-of-freedom subjective vertical conflict motion sickness model, for the first time, directly into a motion cueing algorithm as part of its cost function.\n\nIncorporated a six-degree-of-freedom subjective vertical conflict motion sickness model, for the first time, directly into a motion cueing algorithm as part of its cost function.\n\nFormulated a multi-objective optimisation framework that jointly optimizes for motion sickness reduction and reproduction of reference vehicle motion in the driving simulator.\n\nDemonstrated that inclusion of the motion sickness model allows for an adjustable trade-off between motion fidelity and predicted motion sickness, enabling the algorithm to prioritise either motion fidelity or participant comfort based on application-specific requirements.\n\nConducted a human-in-the-loop experiment confirming that the proposed algorithm effectively mitigates motion sickness, while having minimal impact on perceived motion fidelity.\n\n1. Incorporated a six-degree-of-freedom subjective vertical conflict motion sickness model, for the first time, directly into a motion cueing algorithm as part of its cost function.\n\n2. Formulated a multi-objective optimisation framework that jointly optimizes for motion sickness reduction and reproduction of reference vehicle motion in the driving simulator.\n\n3. Demonstrated that inclusion of the motion sickness model allows for an adjustable trade-off between motion fidelity and predicted motion sickness, enabling the algorithm to prioritise either motion fidelity or participant comfort based on application-specific requirements.\n\n4. Conducted a human-in-the-loop experiment confirming that the proposed algorithm effectively mitigates motion sickness, while having minimal impact on perceived motion fidelity.",
            "llm_summary": "【论文的motivation是什么】  \n1. 驾驶模拟器使用日益增加，但常导致运动病。  \n2. 现有运动提示算法未能有效减轻运动病。  \n3. 需要一种新方法来平衡运动真实感与舒适度。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于模型预测控制（MPC）的运动提示算法（MCA），首次将六自由度主观垂直冲突运动病模型直接纳入算法的成本函数中。该算法通过多目标优化框架，联合优化运动病减少与真实车辆运动再现，能够根据应用需求调整运动真实感与运动病预测之间的权衡。通过人机实验验证，该算法有效缓解了运动病，同时对感知运动真实感的影响最小。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robot",
            "authors": "Yue Wang",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01984",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01984",
            "arxiv_html_link": "https://arxiv.org/html/2510.01984v1",
            "abstract": "We present SPARC, a compact, open-source 3-DoF sagittal-plane spine module that combines revolute (pitch) and prismatic (axial) motion with programmable task-space impedance for quadruped robots. The system integrates three torque-controlled actuators, a custom 1 kHz control board, and a protected power unit in a 1.26 kg package, enabling closed-loop stiffness and damping shaping along x, z, and θ\\theta. We develop an RNEA-based computed-acceleration controller with smooth Stribeck friction compensation to render spring–damper behavior without explicit inertia shaping. Bench experiments validate the approach. Quasi-static push–pull tests show linear force–displacement characteristics with commanded horizontal stiffness spanning 300–700 N/m and ≤1.5%\\leq 1.5\\% relative error (R2≥0.992R^{2}\\geq 0.992, narrow 95% CIs). Dynamic displace-and-release trials confirm mass–spring–damper responses over multiple damping settings, with small, interpretable phase deviations due to configuration-dependent inertia and low-speed friction effects. A task-space PD controller produces roughly linear stiffness but with greater variability and coupling sensitivity. SPARC provides a portable platform for systematic studies of spine compliance in legged locomotion and will be released with complete hardware and firmware resources.",
            "introduction": "Quadruped mammals use flexible spines to lengthen stride, absorb landing impacts, and generate thrust, achieving agility and efficiency [1] beyond rigid torsos [2]. Most robotic quadrupeds, however, employ rigid trunks to simplify structure and control [3, 4], which constrains energy shaping and workspace modulation at high speed. We target this gap with a compact spine that can bend in the sagittal plane (pitch) and extend axially under active control, providing continuous, tunable compliance.\n\nPrior efforts cover three main directions. Passive spines use elastic elements but are hard to adjust to task demands [5]. Actuated, position-controlled spines behave effectively rigid when holding posture and offer only modest energy benefits [6]. Spring-loaded mechanisms can create axial motion but must be pre-compressed and release open loop, making them single-shot rather than continuously controllable [7]. In contrast, biological spines (e.g., dogs) exhibit loosely coupled vertebrae that allow simultaneous bending and changes in inter-vertebral spacing, i.e., coupled revolute and prismatic motions that support impact attenuation and thrust generation [8]. Compliance is therefore essential for efficient locomotion [9]. What is missing is a compact, replicable spine unit that operates in closed loop, simultaneously realizes revolute and prismatic motions, and provides tunable stiffness and damping for systematic studies on legged platforms.\n\nMotivated by this need, we present SPARC777Our design is open-sourced and replication and adaptation are welcomed. Github repostory: https://github.com/YueWang996/xxx (Spine with Prismatic And Revolute Compliance), a 3-DoF sagittal-plane spine module with torque-controlled actuation and programmable impedance (as shown in Figure 1). SPARC behaves like a software-defined spring–damper along both the linear and rotational axes and mounts between fore and hind body segments to enable controlled, repeatable experiments on spine motion in quadruped locomotion. The contributions of this paper are as follows:\n\nWe present SPARC, an open-sourced compact 3-DoF sagittal-plane spine that provides prismatic and revolute motion simultaneously, offering a portable platform to study spine effects in quadruped locomotion.\n\nWe develop an RNEA-based model with Cartesian impedance control and friction compensation to realize tunable linear and torsional stiffness and damping.\n\nWe implement the system and validate it through static and dynamic experiments that quantify the accuracy and repeatability of the commanded stiffness and damping.\n\nThe remainder of this paper is organized as follows. Section II reviews spine mechanisms and control approaches and highlights gaps in prismatic motion. Section III details SPARC hardware and control. Section IV presents experiments and analysis. Section V concludes with limitations and future work.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的四足机器人通常使用刚性躯干，限制了能量塑形和高速度下的工作空间调节。  \n2. 需要一个紧凑、可复制的脊柱单元，能够在闭环中同时实现转动和伸缩运动，并提供可调的刚度和阻尼。  \n\n【提出了什么创新的方法】  \n提出了SPARC，一个开源的3自由度矢状面脊柱模块，结合了可控的转动和伸缩运动，能够在四足机器人中实现可编程的阻抗控制。该系统通过三种扭矩控制的执行器和自定义控制板，能够在静态和动态实验中验证其准确性和重复性。SPARC的设计使得研究四足机器人脊柱运动的影响成为可能，提供了一个便携的平台，促进了对脊柱柔性在四足运动中的系统性研究。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "TACOS: Task Agnostic COordinator of a multi-drone System",
            "authors": "Alessandro Nazzari,Roberto Rubinacci,Marco Lovera",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
            "comment": "accepted as poster at 2025 IEEE International Symposium on Multi-Robot & Multi-Agent Systems",
            "pdf_link": "https://arxiv.org/pdf/2510.01869",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01869",
            "arxiv_html_link": "https://arxiv.org/html/2510.01869v1",
            "abstract": "When a single pilot is responsible for managing a multi-drone system, the task demands varying levels of autonomy, from direct control of individual UAVs, to group-level coordination, to fully autonomous swarm behaviors for accomplishing high-level tasks.\nEnabling such flexible interaction requires a framework that supports multiple modes of shared autonomy. As language models continue to improve in reasoning and planning, they provide a natural foundation for such systems, reducing pilot workload by enabling high-level task delegation through intuitive, language-based interfaces.\nIn this paper we present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified framework that enables high-level natural language control of multi-UAV systems through Large Language Models. TACOS integrates three key capabilities into a single architecture: a one-to-many natural language interface for intuitive user interaction, an intelligent coordinator for translating user intent into structured task plans, and an autonomous agent that executes plans interacting with the real-world. TACOS allows a LLM to interact with a library of executable APIs, bridging semantic reasoning with real-time multi-robot coordination. We demonstrate the system in real-world multi-drone system and conduct an ablation study to assess the contribution of each module.",
            "introduction": "Coordinating multiple Unmanned Aerial Vehicles has become a core robotics challenge, with applications ranging from surveillance and mapping to disaster response and delivery.\nAssigning a dedicated pilot to each UAV does not scale with swarm size: it is expensive, inefficient, difficult to coordinate, and prone to human error. As a result, multi-UAV coordination systems must fulfill two critical requirements: provide a high-level intuitive interface for the user, and manage the swarm’s behavior autonomously, including supporting user situational awareness.\n\nPrevious approaches have explored a range of interaction modalities. Some systems use fusion modules that combine voice and gesture recognition to interpret commands [1], while others rely on tablet-based interfaces for manual control [2]. However, recent advances in LLMs have opened new directions in autonomous systems, going beyond basic natural language interfaces for human-robot interaction.\n\nRecent works have explored the use of LLMs to directly generate robot commands from natural language input. Other approaches have gone further, using LLMs to produce executable code in real time for planning and control tasks [3]. A particularly compelling direction is the ReAct framework [4], which allows language models to interact with the environment enabling reasoning and action in a closed feedback loop. Building on these ideas, several studies have investigated LLM-driven control in both single-robot [5] and multi-agent [6, 7, 8, 9] scenarios, demonstrating promising results. These developments naturally connect with long-standing challenges in multi-UAV systems, where researchers have developed efficient algorithms for distributed task allocation [10, 11] , trajectory generation [12] , and formation control [13, 14]. Many of these algorithms can be exposed as callable APIs, making them well suited for integration with LLM-based agents under the ReAct framework.\n\nIn this work, we bridge this gap by applying the ReAct paradigm to swarm coordination. We use a language model to interpret high-level user instructions and interact with a library of low-level swarm actions via structured API calls. Specifically we present TACOS, an LLM-powered coordinator for multi-drone system. Beyond enabling an intuitive one-to-many user interface, a relevant contribution in itself, our framework allows the swarm to benefit from the semantic and commonsense reasoning capabilities of large language models. This paves the way to more flexible and resilient swarm mission execution in unpredictable settings.\n\nTACOS is designed to support the following core capabilities:\n\nOne-to-many natural language interface. Users can issue direct, UAV-specific instructions such as “Alfa, take off”, “Move Alfa toward the north” or “Swap Alfa and Bravo’s positions”;\n\nIntelligent coordinator. TACOS supports high-level swarm command like “Split the swarm into two groups, send one group north, and have the other surround the target”;\n\nTask manager. The system generates structured execution plans to fulfill complex tasks and continuously monitors the swarm’s state to ensure successful execution.\n\nTo the best of our knowledge, this is the first demonstration of a language model interfaced with a real-world multi-drone system for one-to-many interaction and closed-loop task execution.\n\nThe remainder of the paper is organized as follows: Section II introduces the problem setup. Section III presents the proposed framework, TACOS. Section IV reports both simulation results and real-world experiments using quadrotor platforms.\n\n1. One-to-many natural language interface. Users can issue direct, UAV-specific instructions such as “Alfa, take off”, “Move Alfa toward the north” or “Swap Alfa and Bravo’s positions”;\n\n2. Intelligent coordinator. TACOS supports high-level swarm command like “Split the swarm into two groups, send one group north, and have the other surround the target”;\n\n3. Task manager. The system generates structured execution plans to fulfill complex tasks and continuously monitors the swarm’s state to ensure successful execution.",
            "llm_summary": "【论文的motivation是什么】  \n1. 多无人机系统的协调是核心机器人挑战，现有方法无法有效扩展。  \n2. 需要高水平的直观接口和自主管理群体行为以减少人类错误。  \n\n【提出了什么创新的方法】  \n本文提出了TACOS，一个基于大型语言模型的多无人机系统协调器。TACOS集成了三大核心能力：一是用户可以通过自然语言发出具体指令，二是智能协调器将用户意图转化为结构化任务计划，三是自主代理执行计划并与现实世界互动。通过这种方式，TACOS实现了高效的多无人机协调，支持灵活的任务执行，提升了用户的操作体验和系统的自主性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for Mobile Robotics",
            "authors": "Diram Tabaa,Gianni Di Caro",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01848",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01848",
            "arxiv_html_link": "https://arxiv.org/html/2510.01848v1",
            "abstract": "Simulating greenhouse environments is critical for developing and evaluating robotic systems for agriculture, yet existing approaches rely on simplistic or synthetic assets that limit simulation-to-real transfer. Recent advances in radiance field methods, such as Gaussian splatting, enable photorealistic reconstruction but have so far been restricted to individual plants or controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a framework and dataset for generating photorealistic greenhouse assets directly from inexpensive RGB images. The resulting assets are integrated into a ROS-based simulation with support for camera and LiDAR rendering, enabling tasks such as localization with fiducial markers. We provide a dataset of 82 cucumber plants across multiple row configurations and demonstrate its utility for robotics evaluation. GreenhouseSplat represents the first step toward greenhouse-scale radiance-field simulation and offers a foundation for future research in agricultural robotics.",
            "introduction": "Autonomous Mobile Robots have recently witnessed broader adoption in the agricultural sector as a means of automating the labor-intensive task of monitoring large areas of land for pests, diseases, and yield prediction [31]. This adoption has been enabled in part by the combination of aerial field imagery and robotic localization through global positioning systems (GPS). However, this adoption has been more limited in greenhouse environments, due to the restricted applicability of GPS and the difficulty of acquiring aerial images, which constrains robotic solutions to mostly ground vehicles only.\n\nIn response to these challenges, greenhouse robotics has emerged as an active research area in recent years. Contemporary approaches often draw inspiration from indoor mobile robotics, adapting methods such as SLAM [29] to greenhouse environments. Nonetheless, these approaches transfer poorly, as greenhouses are complex, highly occluded, and geometrically non-convex compared to the structured indoor settings where mobile robots are typically deployed. This necessitates domain-specific adaptation and highlights the need for greenhouse simulation environments to evaluate robotic algorithms prior to deployment.\n\nAgricultural and botanical simulation systems have existed since the advent of efficient raster graphics software [11, 24], with early theoretical models such as L-systems [16] enabling the procedural generation of synthetic plants. More recent efforts rely on manually designed 3D assets to construct simulation environments. However, both procedurally and manually generated assets exhibit limited variability and, more critically, lack realism when integrated into simulation pipelines, thereby hindering sim-to-real transfer.\n\nRecent advances in radiance field methods, particularly 3D Gaussian splatting [10] and its derivatives [8, 6], have enabled 3D reconstruction suitable for real-time photorealistic rendering. While these advances have begun to see applications in agriculture [32], their scope has so far been limited to individual plants or even individual fruits. To date, no greenhouse-scale simulations have attempted to integrate such methods.\n\nIn this work, we address the absence of a greenhouse simulation framework by introducing a novel pipeline for generating photorealistic virtual greenhouse environments from inexpensive RGB imagery. We demonstrate the utility of this pipeline by incorporating the resulting environments into a Robot Operating System [17] (ROS)-based simulation and conducting evaluations for localization. Lastly, we provide a dataset of multiple greenhouse configurations created using our pipeline to further enable greenhouse-based robotics research.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有温室模拟方法过于简单，限制了模拟到现实的转移。  \n2. 温室环境复杂且高度遮挡，现有的室内移动机器人方法难以适用。  \n3. 需要更真实的模拟环境来评估农业机器人算法。  \n\n【提出了什么创新的方法】  \n本研究提出了GreenhouseSplat框架，利用低成本RGB图像生成逼真的温室资产，并将其集成到基于ROS的模拟中。该方法采用3D高斯喷溅技术，实现了温室规模的光场模拟，支持相机和LiDAR渲染。通过提供82种黄瓜植物的多种行配置数据集，展示了该框架在机器人评估中的实用性，为未来农业机器人研究奠定了基础。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots",
            "authors": "Wanyue Li,Ji Ma,Minghao Lu,Peng Lu",
            "subjects": "Robotics (cs.RO)",
            "comment": "conference paper",
            "pdf_link": "https://arxiv.org/pdf/2510.01843",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01843",
            "arxiv_html_link": "https://arxiv.org/html/2510.01843v1",
            "abstract": "Humanoid robot soccer presents several challenges, particularly in maintaining system stability during aggressive kicking motions while achieving precise ball trajectory control.\nCurrent solutions, whether traditional position-based control methods or reinforcement learning (RL) approaches, exhibit significant limitations.\nModel predictive control (MPC) is a prevalent approach for ordinary quadruped and biped robots. While MPC has demonstrated advantages in legged robots, existing studies often oversimplify the leg swing progress, relying merely on simple trajectory interpolation methods. This severely constrains the foot’s environmental interaction capability, hindering tasks such as ball kicking.\nThis study innovatively adapts the spatial-temporal trajectory planning method, which has been successful in drone applications, to bipedal robotic systems.\nThe proposed approach autonomously generates foot trajectories that satisfy constraints on target kicking position, velocity, and acceleration while simultaneously optimizing swing phase duration. Experimental results demonstrate that the optimized trajectories closely mimic human kicking behavior, featuring a backswing motion. Simulation and hardware experiments confirm the algorithm’s efficiency, with trajectory planning times under 1 ms, and its reliability, achieving nearly 100 % task completion accuracy when the soccer goal is within the range of -90° to 90°.",
            "introduction": "Football is one of the most popular sports worldwide, capturing the interest of billions. Given their human-like form, the application of humanoid robots in football naturally attracts significant interest. Since the inaugural RoboCup in 1997 [1], the competition has served as a driving force for research in robot soccer, leading to advancements in areas such as visual perception and stable motion control.\nHowever, despite significant progress in robotic football, achieving precise passing and shooting remains a major challenge that requires further investigation.\n\nMost previous approaches have focused primarily on executing kicking motions rather than precisely controlling the placement of the shot. For instance, humanoid robots in RoboCup[4],[5],[6] mainly rely on predefined motions, while [16] employed parameterized multi-phase actions in simulation to extend shooting range.\nIn recent years, some studies have begun to emphasize precise passing, mainly through reinforcement learning (RL). For example, [3] demonstrated that a quadruped robot could use RL to perform single-leg passing tasks from a static position, [19] showcased its capability for remote-controlled dribbling. Similarly, [17], [2] developed a highly realistic physical system that closely matches its simulation counterpart, enabling small-scale humanoid robots to autonomously dribble, kick, and shoot in 1v1 soccer scenarios with the aid of motion capture systems.\nHowever, RL-based methods are often tailored to specific robots and environments, limiting their ability to generalize to real-world scenarios. Furthermore, RL policies typically operate as a “black box,” making their decision-making process challenging to interpret and control. As a result, there remains a lack of humanoid robot systems capable of autonomously planning and executing precise shots based on human-specified targets. Our work aims to\nbridge this gap.\n\nModel Predictive Control (MPC) has become increasingly popular in legged robot control, achieving remarkable success in legged robot locomotion.\nConvex MPC based on a single rigid body (SRB) model, the MIT Cheetah 3 can reach speeds up to 3.7 m/s [26] [20].\nPerceptive Locomotion through Nonlinear MPC [21] enables quadrupedal robots to traverse complex terrains stably. Contact-Implicit MPC [22] eliminates the need for predefined contact sequences, allowing quadrupedal robots to achieve stable bipedal standing.\nThese advancements have also influenced bipedal locomotion.\n[23][24] adapted MPC strategies for bipedal robots with line feet and explored MPC modeling for bipedal locomotion. CDM-MPC considers that the centroidal dynamics model can realize dynamic jumping[15].\nHowever, swing leg control has often been overlooked, and a simple Bézier curve is typically used. It primarily ensures foot placement at the designed touchdown time, treating it as a terminal constraint rather than tracking the entire trajectory [25].\nThis highlights the underexplored potential of swing leg control, warranting further investigation.\n\nDuring human kicking motions, movement is naturally adjusted based on the target distance: for short-range passes, a slight leg swing suffices, whereas long-range passes or shots require a more pronounced backswing. The backswing serves to extend both the acceleration distance and duration, allowing the foot to reach a higher velocity at the moment of impact and achieve the desired motion state at the target location. Fundamentally, this process involves the simultaneous optimization of trajectory shape and timing parameters, a concept known as spatial-temporal trajectory planning [14].\n\nThis approach has been widely applied in drone trajectory optimization. [8] and [7] demonstrated coordinated outdoor flight, obstacle avoidance, and inter-robot collision avoidance in multi-drone systems. [9] ensured continuous, collision-free motion for drones navigating complex environments with non-convex geometries. In real-world tests, SUPER achieved autonomous flights at speeds exceeding 20 meters per second [13].\n\nInspired by human kicking mechanics and leveraging spatial-temporal trajectory planning [10], this study introduces the Spatial-Temporal Optimization of Foot Trajectory (STOFT) Planner, a novel approach designed to optimize the kicking trajectory of humanoid robots.\n\nThe main contributions of this paper include:\n\nA comprehensive system architecture for dynamic and precise shooting, integrating vision-based soccer detection and localization, visual aiming, robot state estimation, MPC-based balance control, spatial-temporal foot trajectory planning, and adaptive gait generation for uncertain swing phases\n\nProposed the STOFT Planner, which performs spatial-temporal optimization to execute precise kicking tasks while adhering to dynamic constraints and avoiding self-collision. This approach generates natural backswing motions, closely mimicking human kicking behavior.\n\nDeveloped the humanoid robot PEARL for real-world testing. Experimental results show that the system can perform precise shooting football, with the planner completing a single computation in under 1 ms.\n\nThe rest of this paper is arranged as follows. Section II presents the system pipeline. Section III introduces the STOFT planner. In Section IV, experimental results verify the reliability of the proposed method. Finally, Section V concludes this paper.\n\n1. A comprehensive system architecture for dynamic and precise shooting, integrating vision-based soccer detection and localization, visual aiming, robot state estimation, MPC-based balance control, spatial-temporal foot trajectory planning, and adaptive gait generation for uncertain swing phases\n\n2. Proposed the STOFT Planner, which performs spatial-temporal optimization to execute precise kicking tasks while adhering to dynamic constraints and avoiding self-collision. This approach generates natural backswing motions, closely mimicking human kicking behavior.\n\n3. Developed the humanoid robot PEARL for real-world testing. Experimental results show that the system can perform precise shooting football, with the planner completing a single computation in under 1 ms.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的机器人足球解决方案在精确控制射门方面存在显著局限性。  \n2. 传统的控制方法和强化学习方法难以适应真实世界场景，缺乏可解释性。  \n3. 现有的模型预测控制（MPC）方法未充分考虑摆腿控制的潜力。  \n\n【提出了什么创新的方法】  \n本研究提出了一种新的空间-时间优化脚轨迹规划器（STOFT Planner），旨在优化类人机器人踢球轨迹。该方法通过空间-时间优化，生成符合动态约束的自然摆腿动作，模拟人类踢球行为。实验结果表明，该系统在真实环境中能够实现精确射门，轨迹规划时间低于1毫秒，任务完成准确率接近100%。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework",
            "authors": "Hongze Wang,Boyang Sun,Jiaxu Xing,Fan Yang,Marco Hutter,Dhruv Shah,Davide Scaramuzza,Marc Pollefeys",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01830",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01830",
            "arxiv_html_link": "https://arxiv.org/html/2510.01830v1",
            "abstract": "Object-Goal Navigation (ObjectNav) is a critical component toward deploying mobile robots in everyday, uncontrolled environments such as homes, schools, and workplaces.\nIn this context, a robot must locate target objects in previously unseen environments using only its onboard perception.\nSuccess requires the integration of semantic understanding, spatial reasoning, and long-horizon planning, which is a combination that remains extremely challenging.\nWhile reinforcement learning (RL) has become the dominant paradigm, progress has spanned a wide range of design choices, yet the field still lacks a unifying analysis to determine which components truly drive performance.\nIn this work, we conduct a large-scale empirical study of modular RL-based ObjectNav systems, decomposing them into three key components: perception, policy, and test-time enhancement.\nThrough extensive controlled experiments, we isolate the contribution of each and uncover clear trends: perception quality and test-time strategies are decisive drivers of performance, whereas policy improvements with current methods yield only marginal gains.\nBuilding on these insights, we propose practical design guidelines and demonstrate an enhanced modular system that surpasses State-of-the-Art (SotA) methods by 6.6% on SPL and by a 2.7% success rate.\nWe also introduce a human baseline under identical conditions, where experts achieve an average 98% success, underscoring the gap between RL agents and human-level navigation.\nOur study not only sets the SotA performance but also provides principled guidance for future ObjectNav development and evaluation.",
            "introduction": "Recent advances in computer vision and deep learning have inspired growing interest in interdisciplinary applications that bridge perception, reasoning, and control, especially in robotics.\nAmong these, vision-based navigation has emerged as a foundational capability for autonomous mobile agents.\nA key benchmark in this domain is Object-Goal Navigation (ObjectNav), where a robot must navigate to an instance of a specified object category in an unseen environment, relying solely on its onboard sensors.\nThis task is both practically important and technically challenging: it requires semantic understanding, spatial reasoning, and long-horizon planning.\nAmong many approaches, Reinforcement Learning (RL) has become a dominant paradigm for ObjectNav, offering a structured framework to learn directly through trial-and-error and showing steady progress across various benchmarks.\n\nWhile end-to-end RL policies are common, modular RL approaches have shown greater robustness and improved generalization.\nBy decomposing the system into interpretable and tunable components, such as perception, mapping, policy, and action execution, these methods align with the multi-faceted nature of ObjectNav and often achieve better sim-to-real transfer navi_in_the_real.\nHowever, this modular design also increases overall system complexity, and the standalone contribution of each component has not been systematically studied in recent literature.\nAs a result, current bottlenecks remain unclear, and the lack of a unified design guide makes it difficult to fairly evaluate and advance modular RL systems.\n\nIn this work, we disentangle the components of modular RL-based ObjectNav and ask a fundamental question: Which design choices truly matter for RL-based ObjectNav?\nWe establish a principled decomposition of modular ObjectNav systems into three essential parts—perception, policy, and test-time enhancement.\nFor each component, we categorize representative design choices from the literature and, through extensive experiments, isolate the individual contribution of each.\n\nOur key findings are as follows:\n(a) The capability of the perception module has a substantial impact on overall navigation performance.\n(b) Test-time enhancement techniques are often overlooked; however, they play a surprisingly significant role in boosting performance.\n(c) With well-designed perception and test-time modules, further improvements must come from the policy module; however, we observe that with current learning approaches, such gains are limited or marginal.\n\nBased on these findings, we offer practical recommendations for selecting and combining system components, along with insights into the reasoning behind them.\nFollowing these guidelines, we design an enhanced modular system that achieves state-of-the-art performance with 47.5% SPL (success weighted by path length) and 85.3% success rate, surpassing the best prior methods by 6.6% in SPL and 2.7% in success rate.\nIn addition, we introduce a human baseline comparison, where expert participants operate under the same training setup, test environments, and observation modalities as the agent, achieving an average of 98.0% success rate and 53.3% SPL.\nThis contrast reveals a clear gap between current RL-based systems and human-level navigation, emphasizing the need for new algorithms that can enhance both performance and robustness in ObjectNav.\n\nInstead of emphasizing our empirical gains, our primary goal is to highlight broader implications for the vision-based navigation community from the design of future algorithms to the establishment of fair evaluation protocols and the deployment of ObjectNav systems. We will publicly release our code and evaluation tools to the community.",
            "llm_summary": "【论文的motivation是什么】  \n1. Object-Goal Navigation (ObjectNav) is crucial for deploying mobile robots in everyday environments.  \n2. The integration of semantic understanding, spatial reasoning, and long-horizon planning remains challenging.  \n3. There is a lack of unified analysis to determine which components drive performance in RL-based ObjectNav systems.  \n\n【提出了什么创新的方法】  \n本研究通过对模块化RL-based ObjectNav系统进行大规模实证研究，分解为感知、策略和测试时增强三个关键组件。通过控制实验，发现感知质量和测试时策略对性能的影响显著，而当前方法在策略改进上的收益有限。基于这些发现，提出了实用设计指南，并设计了一个增强的模块化系统，成功率和SPL分别提高了2.7%和6.6%。此外，研究引入了人类基线，显示出当前RL系统与人类导航能力之间的明显差距，强调了需要新算法以提升ObjectNav的性能和鲁棒性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving",
            "authors": "Haibo Hu,Lianming Huang,Xinyu Wang,Yufei Cui,Nan Guan,Chun Jason Xue",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01795",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01795",
            "arxiv_html_link": "https://arxiv.org/html/2510.01795v1",
            "abstract": "Vision-Language Models (VLMs) are increasingly applied in autonomous driving for unified perception and reasoning, but high inference latency hinders real-time deployment. Early-exit reduces latency by terminating inference at intermediate layers, yet its task-dependent nature limits generalization across diverse scenarios. We observe that this limitation aligns with autonomous driving: navigation systems can anticipate upcoming contexts (e.g., intersections, traffic lights), indicating which tasks will be required. We propose Nav-EE, a navigation-guided early-exit framework that precomputes task-specific exit layers offline and dynamically applies them online based on navigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE achieves accuracy comparable to full inference while reducing latency by up to 63.9%. Real-vehicle integration with Autoware Universe further demonstrates reduced inference latency (600 ms to 300 ms), supporting faster decision-making in complex scenarios. These results suggest that coupling navigation foresight with early-exit offers a viable path toward efficient deployment of large models in autonomous systems. Code and data are available at our anonymous repository: https://anonymous.4open.science/r/Nav-EE-BBC4",
            "introduction": "The integration of Vision-Language Models (VLMs) into autonomous driving has emerged as a transformative trend, unifying visual perception with high-level reasoning. Recent advances demonstrate that VLMs can enhance perception and decision-making by capturing contextual cues often missed by conventional models [1, 2, 3]. For example, Li Auto’s DriveVLM-Dual system provides semantic understanding to assist smaller perception models [4], while research prototypes such as Talk2BEV [1] and Text-to-Drive [2] highlight the potential to bridge vision and planning. Large-scale models like DeepSeek-VL2 [5] and LLaVA [6] further illustrate the strong multimodal reasoning capabilities of modern VLMs. Despite these advances, deploying VLMs in real time remains difficult due to high inference latency and redundant computation, which directly limit responsiveness in safety-critical driving scenarios [7, 8, 32, 33].\n\nAmong sparsification techniques, early exit (EE) is particularly attractive, as it halts inference once predictions stabilize [9, 10, 11]. As shown in Fig. 2, correct recognition often emerges in early layers, while later computation adds little value. EE therefore offers strong potential for reducing latency in driving systems, where avoiding unnecessary inference directly improves response time and efficiency.\n\nHowever, EE strategies often lack generalization[34]. Prior work shows that in-domain or task-specific EE can deliver substantial computation savings, whereas out-of-domain or general-purpose benchmarks (e.g., GLUE) see limited benefits [9, 11, 12]. As illustrated in Fig. 1, confidence dynamics vary widely across domains, restricting aggressive exits in general-purpose settings. In contrast, for narrow tasks such as text classification or domain-focused applications, EE can skip up to half of the layers with negligible accuracy loss [9, 12]. This disparity suggests that EE is highly effective in specialized domains but unreliable when applied broadly.\n\nInterestingly, this limitation aligns naturally with autonomous driving. Navigation systems can anticipate upcoming contexts—such as intersections, traffic lights, or pedestrian zones—thus providing strong priors about the perception or decision-making tasks that will soon be required. As illustrated in Fig. 3, these priors enable task-specific EE decisions, allowing VLMs to exit earlier without compromising accuracy.\n\nIn this work, we extend EE from unimodal or NLP models to large VLMs for autonomous driving. We exploit the determinism of navigation priors to adopt an aggressive statistical strategy for selecting exit layers, and design a dynamically switchable EE configuration that adapts to driving contexts in real time. We evaluate Nav-EE on large-scale datasets (Waymo [14], CODA [15], and Bosch) and validate it on a real vehicle integrated with Autoware.Universe. By monitoring HD-map ROS 2 topics to trigger task switches, Nav-EE achieves context-aware early exiting in practice, reducing latency while maintaining or improving accuracy.\n\nIn summary, the challenge is that conventional EE strategies cannot generalize across the diverse and dynamic tasks in autonomous driving. Our contribution is to adapt EE to large VLMs and leverage deterministic navigation priors as a strength: they decompose driving into predictable sub-tasks, enabling reliable, task-aware acceleration in practice.\n\nThe main contributions of this work are summarized as follows:\n\nWe are the first to adapt early-exit strategies to large VLMs for autonomous driving, transforming them from task-specific techniques into a practical, navigation-aware acceleration mechanism.\n\nWe are the first to adapt early-exit strategies to large VLMs for autonomous driving, transforming them from task-specific techniques into a practical, navigation-aware acceleration mechanism.\n\nWe conduct extensive experiments across diverse datasets and models: the Waymo dataset for common perception tasks, the CODA dataset for animal detection and corner-case recognition, and the Bosch dataset for traffic-light decision making. Results consistently show that Nav-EE reduces inference latency and resource consumption while simultaneously improving accuracy.\n\nWe further validate Nav-EE in real-world deployment by integrating it into an autonomous vehicle running Autoware.Universe. The real-vehicle experiments confirm that navigation-guided early exit can operate reliably under practical driving conditions, achieving both efficiency gains and accuracy improvements.\n\n1. We are the first to adapt early-exit strategies to large VLMs for autonomous driving, transforming them from task-specific techniques into a practical, navigation-aware acceleration mechanism.\n\n2. We conduct extensive experiments across diverse datasets and models: the Waymo dataset for common perception tasks, the CODA dataset for animal detection and corner-case recognition, and the Bosch dataset for traffic-light decision making. Results consistently show that Nav-EE reduces inference latency and resource consumption while simultaneously improving accuracy.\n\n3. We further validate Nav-EE in real-world deployment by integrating it into an autonomous vehicle running Autoware.Universe. The real-vehicle experiments confirm that navigation-guided early exit can operate reliably under practical driving conditions, achieving both efficiency gains and accuracy improvements.",
            "llm_summary": "【论文的motivation是什么】  \n1. 高推理延迟限制了视觉-语言模型在自动驾驶中的实时部署。  \n2. 现有的早期退出策略缺乏跨任务的泛化能力，限制了其在复杂场景中的应用。  \n3. 导航系统能够预测即将到来的上下文，为任务特定的早期退出提供强有力的先验信息。  \n\n【提出了什么创新的方法】  \n提出了Nav-EE，一个基于导航指导的早期退出框架，通过离线预计算任务特定的退出层，并根据导航先验动态应用。该方法通过在大型数据集（如Waymo、CODA和Bosch）上的实验，显示出与完全推理相当的准确性，同时将延迟减少了高达63.9%。在与Autoware Universe集成的真实车辆中，Nav-EE将推理延迟从600毫秒减少到300毫秒，支持在复杂场景中更快的决策。结果表明，将导航前瞻与早期退出相结合，为大型模型在自动驾驶系统中的高效部署提供了一条可行的路径。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "An Anytime, Scalable and Complete Algorithm for Embedding a Manufacturing Procedure in a Smart Factory",
            "authors": "Christopher Leet,Aidan Sciortino,Sven Koenig",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01770",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01770",
            "arxiv_html_link": "https://arxiv.org/html/2510.01770v1",
            "abstract": "Modern automated factories increasingly run manufacturing procedures using a matrix of programmable machines, such as 3D printers, interconnected by a programmable transport system, such as a fleet of tabletop robots. To embed a manufacturing procedure into a smart factory, an operator must: (a) assign each of its processes to a machine and (b) specify how agents should transport parts between machines. The problem of embedding a manufacturing process into a smart factory is termed the Smart Factory Embedding (SFE) problem. State-of-the-art SFE solvers can only scale to factories containing a couple dozen machines. Modern smart factories, however, may contain hundreds of machines. We fill this hole by introducing the first highly scalable solution to the SFE, TS-ACES, the Traffic System based Anytime Cyclic Embedding Solver. We show that TS-ACES is complete and can scale to SFE instances based on real industrial scenarios with more than a hundred machines.",
            "introduction": "Flexible manufacturing is a key objective of the modern manufacturing industry [1]. A smart factory is flexible if it can be easily reconfigured to produce different products. Flexible manufacturing systems can reduce the cost of producing new products, lower the time required to fulfill orders, and allow products to be customized. To perform flexible manufacturing, a smart factory needs two components:\n\nFlexible Machines. Flexible machines are general purpose machines such as CNC machines which can be used to perform a range of manufacturing processes. Flexible machines can be easily reprogrammed with a new process when a smart factory’s manufacturing procedure changes.\n\nFlexible Transport System. Flexible transport systems make it easy to adjust the materials that the machines in a smart factory are supplied with. Most flexible transport systems transport materials with a team of agents [2]. These agents are generally autonomous mobile vehicles [2]. However, recent mag-lev based systems such as BOSCH’s ctrlX Flow6D{}^{\\text{6D}} [3].\n\nTo embed a manufacturing procedure into a smart factory, the smart factory’s operator needs to:\n\nassign each process in the manufacturing procedure to one or more machines in the smart factory.\n\nconstruct a transport plan that specifies how the smart factory’s agents should carry parts between machines.\n\nMost existing systems for coordinating agents in a smart factory assume processes have already been assigned to the smart factory’s machines [4, 5]. Assigning processes to\nmachines and paths to agents separately, however, limits the throughput that these solvers can achieve.\n\nOne recent approach to the SFE problem, ACES [6], jointly optimizes its two components. ACES models a smart factory as a grid of cells. Time is discretized. ACES uses a Mixed Integer Linear Program (MILP) to jointly assign processes to machines and find a transport plan to its agents. ACES generates a transport plan which loops after a certain number of timesteps, allowing it to be run continuously.\n\nUnfortunately, ACES scales poorly. ACES’s MILP has a binary variable which indicates if a given cell contains an agent carrying a given component on given timestep for every possible (cell, component, timestep) combination. A SFE instance may have hundreds of cells and tens of components. A transport plan may have tens of timesteps. As a result, when ACES is used to solve a large SFE instance, it may generate a MILP with 100,000s of these variables. A MILP with 100,000s of variables is often difficult to solve. Thus, to date, there is no solver which jointly optimizes both components of the SFE problem that works at scale.\n\nWe address this hole by proposing the Traffic System based Cyclic Embedding Solver, TS-ACES. TS-ACES is based on the following observation: most smart factories coordinate agents using a traffic system, a network of roads. TS-ACES aggregates timesteps into epochs. It uses a MILP to construct a traffic system based embedding, an embedding which moves agents through a traffic system at the rate of one road per epoch. A traffic system often has several times fewer roads than cells. A traffic system based embedding often has several times fewer epochs than timesteps. TS-ACES’s MILP thus often has dozens of times fewer variables than ACES’s MILP, making it much easier to solve.\n\nThe throughput and runtime of TS-ACES’s MILP are dependent on its hyperparameters, the number of epochs and the length of an epoch in the embedding that it generates. We introduce a novel, principled search algorithm to find good values for these hyperparameters. TS-ACES uses its traffic system based embedding to construct a transport plan generator. This generator can generate timestep by timestep instructions for the smart factory’s agents in real time, allowing it to run its manufacturing procedure indefinitely.\n\nWe analyze TS-ACES and show that it is complete. Our evaluations show that TS-ACES can scale to SFE instances based on real scenarios with more than 100 machines.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的SFE求解器无法扩展到包含数百台机器的现代智能工厂。  \n2. 现有方法在同时优化过程分配和运输计划时，计算复杂度过高，难以解决大规模问题。  \n\n【提出了什么创新的方法】  \n提出了一种新的算法TS-ACES，基于交通系统的周期嵌入求解器。该方法通过将时间步聚合为周期，并使用混合整数线性规划（MILP）构建交通系统嵌入，从而显著减少变量数量，提升求解效率。TS-ACES能够实时生成运输计划，支持智能工厂的持续运行，且在实际工业场景中可扩展至超过100台机器的SFE实例。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Dual-Mode Magnetic Continuum Robot for Targeted Drug Delivery",
            "authors": "Wendu Zhang,Heng Wang,Shuangyi Wang,Yuanrui Huang",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "under review of ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2510.01761",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01761",
            "arxiv_html_link": "https://arxiv.org/html/2510.01761v1",
            "abstract": "Magnetic continuum robots (MCRs) enable minimally invasive navigation through tortuous anatomical channels, yet axially magnetized designs have largely been limited to bending-only motion. To expand deformation capabilities, this paper presents a simple assembly that embeds permanent magnets radially within the catheter wall, allowing a single externally steered permanent magnet to independently induce either bending or torsion. A physics-based formulation together with finite-element analysis establishes the actuation principles, and benchtop experiments validate decoupled mode control under practical fields. Building on this, a dual-layer blockage mechanism consisting of outer grooves and inner plates leverages torsional shear to achieve on-demand drug release. Finally, an in-phantom intervention experiment demonstrates end-to-end operation: lumen following by bending for target approach, followed by twist-activated release at the site. The resulting compact, cable-free platform combines versatile deformation with precise payload delivery, indicating strong potential for next-generation, site-specific therapies.",
            "introduction": "Minimally invasive interventional techniques enable surgical instruments to reach deep-seated lesions and deliver therapeutic agents locally at the target site, thereby enhancing treatment efficacy and minimizing systemic side effects. Within this context, precise and controllable drug delivery has become a central goal in the advancement of medical robotic systems.\n\nTo date, research on drug delivery in medical robotics has predominantly focused on micro/nanorobotic platforms. However, conventional nanocarriers are typically passive, lacking on-board actuation or navigation capabilities. As a result, they rely heavily on systemic circulation and increased dosing to reach therapeutic levels at the target site—raising the risk of off-target exposure and associated toxicity [1,2]. Recent progress in nanotechnology and microfabrication has led to the development of medical micro/nanorobots capable of active locomotion (often under external magnetic, acoustic, optical, or electric fields), targeted navigation, and programmable, on-demand cargo release. These features allow for spatiotemporally controlled delivery tailored to specific clinical requirements. Nonetheless, translating such systems to in vivo applications remains challenging. Their miniature and independent nature necessitates sufficient thrust generation in complex physiological environments, reliable tracking and control, and strict compliance with biocompatibility, biodegradability, and system-level coordination—including collective swarm behaviors [3–7].\n\nIn contrast, magnetic continuum robots (MCRs) offer a more robust and scalable approach to locomotion and navigation by leveraging externally applied magnetic fields, thereby eliminating the need for on-board propulsion systems. Due to their inherent flexibility and ability to navigate through narrow and tortuous anatomical pathways, MCRs have emerged as a promising platform for minimally invasive interventions. By embedding magnetic materials along the robot body and steering them using controlled external magnetic fields, MCRs achieve wireless actuation within the human body [8–11]. This approach circumvents the complexity of in vivo actuators found in tendon-driven [12,13], hydraulic [14,15], or pneumatic systems [16,17], and has garnered increasing attention in recent years.\n\nDespite their potential, studies on drug delivery mechanisms using MCRs remain limited. Recent research has primarily focused on enhancing bending capabilities—typically by optimizing the spatial distribution of magnetic moments to improve bending angles and reachability for deep-seated sites [18–26]. However, this emphasis on bending has left other deformation modes underexplored. In particular, the lack of systematic investigation into torsional actuation has hindered the multifunctionality of MCRs and constrained their broader applicability in complex clinical scenarios.\n\nTo address this gap, this study proposes a dual-deformation-mode MCR that simultaneously supports both locomotion and drug delivery functions. Mode switching is achieved solely by reorienting an external magnetic field using a six-degree-of-freedom (6-DoF) robotic mechanism to manipulate an external permanent magnet (EPM) [27–31].\n\nThe main contributions of this study are as follows:\n\nDual-Mode Magnetic Actuation for Continuum Robot:\nWe present a simple yet effective design and actuation strategy that enables a single continuum robot to achieve both bending and torsion through magnetic control.\n\nTwist-Triggered Drug Release Mechanism:\nWe introduce a compact, dual-layer blockage structure comprising outer grooves and inner plates. This mechanism utilizes torsional shear strain to open or close the drug outlet, enabling on-demand, site-specific drug release without requiring additional delivery lines or valve components.\n\nThis paper is organized as follows: Section II presents the actuation principles and control strategies for both bending and torsion, as well as the proposed drug-release mechanism. Section III details the methods used to evaluate deformation performance and presents corresponding results, including navigation and targeted drug delivery experiments conducted in a PDMS-based phantom. Section IV provides further analysis and discussion of the experimental outcomes. Finally, Section V concludes the paper and outlines future research directions.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的药物输送系统缺乏主动导航能力，依赖于系统循环，增加了毒性风险。  \n2. 现有的磁性连续机器人（MCRs）主要集中于弯曲运动，缺乏对扭转运动的系统研究。  \n3. 需要一种新型的机器人能够同时实现运动和药物输送功能，以满足复杂临床场景的需求。  \n\n【提出了什么创新的方法】  \n本文提出了一种双模式磁性连续机器人（MCR），通过外部磁场的重新定向实现弯曲和扭转的双重运动。该机器人采用简单有效的设计，嵌入永久磁铁以实现独立的运动控制。同时，提出了一种扭转触发的药物释放机制，利用扭转剪切应变控制药物出口的开闭，实现按需、特定部位的药物释放。实验结果表明，该平台具有灵活的变形能力和精确的药物输送潜力，展示了在下一代靶向治疗中的强大应用前景。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Contrastive Representation Regularization for Vision-Language-Action Models",
            "authors": "Taeyoung Kim,Jimin Lee,Myungkyu Koo,Dongyoung Kim,Kyungmin Lee,Changyeon Kim,Younggyo Seo,Jinwoo Shin",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01711",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01711",
            "arxiv_html_link": "https://arxiv.org/html/2510.01711v1",
            "abstract": "Vision-Language-Action (VLA) models have shown its capabilities in robot manipulation by leveraging rich representations from pre-trained Vision-Language Models (VLMs).\nHowever, their representations arguably remain suboptimal, lacking sensitivity to robotic signals such as control actions and proprioceptive states.\nTo address the issue, we introduce Robot State-aware Contrastive Loss (RS-CL), a simple and effective representation regularization for VLA models, designed to bridge the gap between VLM representations and robotic signals.\nIn particular, RS-CL aligns the representations more closely with the robot’s proprioceptive states, by using relative distances between the states as soft supervision.\nComplementing the original action prediction objective, RS-CL effectively enhances control-relevant representation learning, while being lightweight and fully compatible with standard VLA training pipeline.\nOur empirical results demonstrate that RS-CL substantially improves the manipulation performance of state-of-the-art VLA models;\nit pushes the prior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen, through more accurate positioning during grasping and placing,\nand boosts success rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.",
            "introduction": "Vision-Language-Action (VLA; Zitkovich et al. 2023) models have emerged as a powerful framework for robot manipulation, leveraging pre-trained Vision-Language Models (VLM; Liu et al. 2023b) to provide rich visual and semantic grounding for control policies.\nAmong the state-of-the-art VLA models, the common design is to employ a generative action decoder conditioned on VLM-derived representations (Black et al., 2025b; Bjorck et al., 2025).\nThese decoders are trained with an action prediction loss, supervised by the ground-truth sequence of actions.\n\nPrior studies have shown that fine-tuning the VLM alongside training the action decoder is essential to the action prediction performance of VLA models.\nThis is because VLM representations are typically trained on large-scale visual instruction datasets, but have not been explicitly exposed to robotic modalities, such as low-level control actions and proprioceptive information.\nConsequently, training VLA models conditioned on frozen VLM representations leads to suboptimal performance, as the VLM lacks the capability to capture robotic signals (Driess et al., 2025).\n\nMany recent works have proposed different approaches to train the VLM backbone in VLA models to tackle this issue.\nA widely adopted strategy is to directly update the VLM via gradients from the action prediction objective (Black et al., 2025b; Bjorck et al., 2025).\nBeyond this, several works introduce auxiliary objectives, such as jointly training the VLM backbone with curated instruction datasets (Yang et al., 2025), or blocking gradients from the action decoder instead learning to generate intermediate subtasks and discretized actions (Driess et al., 2025).\nAnother line of work further trains the VLM on embodied reasoning or spatial grounding tasks using robotics datasets (Ji et al., 2025; Luo et al., 2025; Azzolini et al., 2025; GEAR, 2025), or autoregressively predicts discretized actions (Kim et al., 2025; Black et al., 2025a) before fine-tuning them for continuous action prediction.\nWhile these approaches help bridge the gap between general-purpose VLM representations and the demands of action prediction, they often require additional training stages or carefully curated datasets.\n\nIn contrast, we aim to directly refine VLM representations to better serve action generation, while remaining efficient and seamlessly compatible with the existing VLA training pipelines.\nIn particular, we focus on contrastive learning, as it provides a principled way to refine representations by defining similar and dissimilar pairs, effectively structuring the embedding space.\nThe specific choice of pair construction determines what the embeddings should capture, ranging from semantic relations between modalities (Radford et al., 2021) to temporal dynamics and policy-relevant representations (Sermanet et al., 2018; Nair et al., 2022; Ma et al., 2023).\nInspired by this perspective, we introduce a contrastive objective that explicitly guides the representations to capture robotic signals, in particular the robot’s proprioceptive states.\nBy jointly optimizing the VLM representation with the standard action prediction loss, we forge representations that are not only semantically rich but also deeply grounded in the robot’s physical state, leading to accurate action prediction.\n\nContribution. \nIn this paper, we introduce a novel self-supervised regularization objective for VLA models, termed Robot State-aware Contrastive Loss (RS-CL), a loss that explicitly shapes VLM representations toward capturing robotic signals.\nDifferent from the conventional contrastive loss, RS-CL assigns pairwise weights based on the distances between robot proprioceptive states, guiding the representations to better reflect robot control-relevant structure.\nIn addition, we propose an representation-level augmentation for VLA models, called view cutoff.\nThis augmentation constructs alternative embeddings by masking out the feature corresponding to a randomly selected observation view.\nBy operating at the representation-level and minimizing the forwarding process through the pre-trained VLM, RS-CL remains lightweight and fully compatible with existing training pipeline.\n\nWe extensively evaluate the effectiveness of RS-CL under manipulation benchmarks such as RoboCasa-Kitchen (Nasiriany et al., 2024) and LIBERO (Liu et al., 2023a).\nFor instance, RS-CL pushes the prior art VLA model from 48.2% to 53.0% (+4.8%), 63.9% to 67.2% (+3.3%), and 65.7% to 69.7% (+4.0%) on RoboCasa-Kitchen, with 30, 100, and 300 demonstrations, respectively.\nWe emphasize that RS-CL gives larger improvement of 30.3% to 41.5% (+11.2%) on pick-and-place tasks, which requires precise positioning during grasping and placing.\nFinally, we show that RS-CL is applicable to real-robot hardware experiments, showing improvement from 45.0% to 58.3% (+13.3%) on challenging manipulation tasks.\n\nIn summary, our contributions are as follows:\n\nWe introduce Robot State-aware Contrastive Loss (RS-CL), a novel objective for VLA models that explicitly aligns VLM representations with proprioceptive states.\n\nWe design RS-CL to operate directly at the representation alongside the original action prediction objective. Therefore RS-CL remains lightweight and compatible with the existing training pipeline.\n\nWe validate RS-CL across diverse training scenarios on manipulation benchmarks and real-world experiments, showing consistent improvements over the state-of-the-art VLA models.\n\n1. We introduce Robot State-aware Contrastive Loss (RS-CL), a novel objective for VLA models that explicitly aligns VLM representations with proprioceptive states.\n\n2. We design RS-CL to operate directly at the representation alongside the original action prediction objective. Therefore RS-CL remains lightweight and compatible with the existing training pipeline.\n\n3. We validate RS-CL across diverse training scenarios on manipulation benchmarks and real-world experiments, showing consistent improvements over the state-of-the-art VLA models.",
            "llm_summary": "【论文的motivation是什么】  \n1. VLA模型在机器人操作中表现出色，但其表示对机器人信号的敏感性不足。  \n2. 现有方法在训练VLM时需要额外的训练阶段或精心策划的数据集。  \n3. 需要直接优化VLM表示以更好地服务于动作生成，同时保持高效性。  \n\n【提出了什么创新的方法】  \n本文提出了一种新的自监督正则化目标，称为Robot State-aware Contrastive Loss (RS-CL)，旨在明确引导VLM表示捕捉机器人信号。RS-CL通过基于机器人本体状态之间的距离分配成对权重，优化VLM表示，使其更好地反映控制相关的结构。此外，提出了一种表示级增强方法view cutoff，通过遮蔽随机选择的观察视图的特征，构建替代嵌入。RS-CL在多个操作基准和真实机器人实验中验证了其有效性，显著提高了VLA模型的性能，尤其在精确抓取和放置任务中，成功率从45.0%提升至58.3%。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via Multi-Simulator Dynamics Randomization",
            "authors": "Zixing Lei,Zibo Zhou,Sheng Yin,Yueru Chen,Qingyao Xu,Weixin Li,Yunhong Wang,Bowei Tang,Wei Jing,Siheng Chen",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01708",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01708",
            "arxiv_html_link": "https://arxiv.org/html/2510.01708v1",
            "abstract": "Humanoid whole-body control (WBC) policies trained in simulation often suffer from the sim-to-real gap, which fundamentally arises from simulator inductive bias—the inherent assumptions and limitations of any single simulator. These biases lead to nontrivial discrepancies both across simulators and between simulation and the real world. To mitigate the effect of simulator inductive bias, the key idea is to train policies jointly across multiple simulators, encouraging the learned controller to capture dynamics that generalize beyond any single simulator’s assumptions. We thus introduce PolySim, a WBC training platform that integrates multiple heterogeneous simulators. PolySim can launch parallel environments from different engines simultaneously within a single training run, thereby realizing dynamics-level domain randomization. Theoretically, we show that PolySim yields a tighter upper bound on simulator inductive bias than single-simulator training. In the experiments, PolySim substantially reduces motion-tracking error in sim-to-sim evaluations; for example, on MuJoCo, it improves execution success by 52.8% over an IsaacSim baseline. PolySim further enables zero-shot deployment on a real Unitree G1 without additional fine-tuning, showing effective transfer from simulation to the real world. We will release the PolySim code upon acceptance of this work.",
            "introduction": "Thanks to rapid progress in reinforcement learning (RL) and high-fidelity simulation, humanoid whole-body control (WBC)[1] has advanced markedly in recent years. Humanoid control is inherently high-dimensional and contact-rich, making the collection of sufficiently diverse real-world interaction data expensive and challenging. Consequently, simulator-based training provides a practical and scalable alternative. Simulation-based trajectory sampling circumvents the safety and efficiency limitations of real-world exploration, enabling the generation of massive rollouts.\n\nDespite their utility, simulators provide only approximate models of real-world physics, each encoding its own inherent assumptions. Here we define simulator inductive bias as the inherent modeling assumptions of any simulator that determine its transition dynamics. As shown in Fig. 2, every simulator occupies a distinct position in the space of dynamics, while the real world lies elsewhere, with its own intrinsic bias. Policies trained in a particular simulator inevitably inherit its bias, hindering generalization across simulators and to the real world, and causing a significant sim-to-real gap.\n\nTo mitigate such a gap, the most common practice is domain randomization[2, 3], which perturbs observations, actions, latent states, or a limited set of physical parameters around a single simulator. This can partially improve generalization by exposing the policy to trajectories slightly beyond the nominal simulator dynamics. Nevertheless, even extensive randomization is structurally limited: the transition model remains that of the chosen simulator, determined by its modeling choices, contact solver, time integrator, and actuator models. Consequently, the randomized rollouts still occupy a narrow neighborhood of that simulator’s dynamics as shown in Fig. 2, causing the the sim-to-real gap still substantial.\n\nTo address this limitation, our key idea is to train policies jointly across multiple simulators, encouraging the learned controller to capture dynamics that generalize beyond any single simulator’s assumptions. Therefore, we propose PolySim, a WBC training platform that integrates multiple heterogeneous embodied simulators. PolySim enables domain randomization at the level of simulator dynamics, substantially mitigating the effect of any single simulator’s inductive bias. Compared with prior work[4, 5], PolySim provides three novel features: (i) training–simulation isolation via a client–server architecture that decouples RL training from simulator runtimes and supports flexible distributed execution; (ii) a unified simulator router that performs API translation and resource scheduling, enabling a single training loop to launch parallel environments backed by different simulators; and (iii) GPU pass-through communication that exchanges trajectories and diagnostics directly between training and simulation processes with negligible additional latency. Taken together, these features enable parallel, efficient sampling of diverse trajectories across heterogeneous simulator dynamics during training as shown in Fig LABEL:fig:big, thereby improving generalization performance.\n\nTo validate the effectiveness of PolySim, we analyze its performance both theoretically and empirically. Theoretically, our analysis shows that the proposed PolySim, which performs dynamics-level randomization, admits a tighter upper bound on simulator inductive bias than approaches that only randomize at the parameter level. Empirically, our extensive experiments show that i) the proposed PolySim significantly reduces motion-tracking error compared to single-simulator baselines in sim-to-sim evaluations. For example, integrating IsaacSim, IsaacGym, and Genesis increases the motion-tracking execution success rate by 52.8% over an IsaacSim-only baseline in MuJoCo; and ii) PolySim enables zero-shot deployment on the real Unitree G1 without additional fine-tuning, reflecting its effectiveness in both simulation and real-world deployment.",
            "llm_summary": "【论文的motivation是什么】  \n1. 训练在单一模拟器上的策略会继承其固有的模拟器偏差，导致在不同模拟器和真实世界之间的泛化能力不足。  \n2. 现有的领域随机化方法在一定程度上改善了泛化能力，但仍然受限于单一模拟器的过于狭窄的动态范围。  \n\n【提出了什么创新的方法】  \n提出了PolySim，一个集成多个异构模拟器的WBC训练平台，允许在单一训练过程中并行运行来自不同引擎的环境，从而实现动态级别的领域随机化。通过这种方法，PolySim显著降低了模拟到模拟的运动跟踪误差，并在MuJoCo中相比于IsaacSim基线提高了52.8%的执行成功率。此外，PolySim还实现了在真实Unitree G1上的零-shot部署，无需额外微调，展示了其在模拟和现实世界部署中的有效性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation",
            "authors": "Yifei Simon Shao,Yuchen Zheng,Sunan Sun,Pratik Chaudhari,Vijay Kumar,Nadia Figueroa",
            "subjects": "Robotics (cs.RO)",
            "comment": "CoRL 2025 Learning Effective Abstractions for Planning (LEAP) Workshop Best Paper Award (this https URL)",
            "pdf_link": "https://arxiv.org/pdf/2510.01661",
            "code": "https://sites.google.com/view/symskill",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01661",
            "arxiv_html_link": "https://arxiv.org/html/2510.01661v1",
            "abstract": "Multi-step manipulation in dynamic environments remains challenging. Two major families of methods fail in distinct ways: (i) imitation learning (IL) is reactive but lacks compositional generalization, as monolithic policies do not decide which skill to reuse when scenes change; (ii) classical task-and-motion planning (TAMP) offers compositionality but has prohibitive planning latency, preventing real-time failure recovery.\nWe introduce SymSkill, a unified learning framework that combines the benefits of IL and TAMP, allowing compositional generalization and failure recovery in real-time. Offline, SymSkill jointly learns predicates, operators, and skills directly from unlabeled and unsegmented demonstrations. At execution time, upon specifying a conjunction of one or more learned predicates, SymSkill uses a symbolic planner to compose and reorder learned skills to achieve the symbolic goals, while performing recovery at both the motion and symbolic levels in real time. Coupled with a compliant controller, SymSkill enables safe and uninterrupted execution under human and environmental disturbances.\nIn RoboCasa simulation, SymSkill can execute 12 single-step tasks with 85% success rate. Without additional data, it composes these skills into multi-step plans requiring up to 6 skill recompositions, recovering robustly from execution failures. On a real Franka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented and unlabeled play data, is capable of performing multiple tasks simply by goal specifications.\nThe source code and additional analysis can be found on https://sites.google.com/view/symskill.",
            "introduction": "Enabling robots to perform complex, long‑horizon manipulation in the real world remains challenging. Recent imitation‑learning (IL) approaches [1, 2] excel at reproducing skills given large, high‑quality datasets, but tend to learn monolithic policies rather than reusable skills and predicates that compose into multi‑step plans. Historically, Task and Motion Planning (TAMP) bridges this gap by decomposing problems into symbolic planning over predicates/operators and continuous motion generation [3]. However, two factors limit TAMP scalability in practice. 1) Symbols and skills are often hand‑engineered and tuned per environment, which is labor‑intensive. 2) TAMP takes tens to hundreds of seconds to solve a large problem in a realistic contact-rich simulation environments[4], making it infeasible to plan in dynamic environments with moving objects, or achieve real-time failure recovery at the symbolic or motion level.\n\nSymbol and Skill Co-Invention methods, such as [5], combine the benefits of IL and TAMP by learning reusable symbols and skills from robot demonstrations and planning symbolically to decide which skill to execute at runtime. As shown in [6, 5], there is a delicate trade-off between inventing long-horizon operators that are too general for useful planning at inference time and inventing operators that are too granular, risking skills learned from insufficient data performing poorly. As a result, both works above use a propose and down-select hill-climbing optimization for selecting predicates. However, even when predicates are invented in relative frames, the learning process can take minutes to hours as the number of objects and demonstrations increases, and may still fail to discover semantically meaningful predicates. To address the aforementioned challenge, we take a different approach by sidestepping expensive optimization altogether. Our key insight is that interactions with objects follow only a handful of common patterns: 1) robots typically approach each object in a limited set of ways, and 2) moving object come to rest in one of a few meaningful poses relative to a stationary object.\nInspired by recent works that use Vision-Language Models (VLMs) to identify task-relevant objects, we employ a VLM in a lightweight role: identifying the relevant stationary object in each demonstration.\nThis allows us to transform trajectories into the stationary object’s frame for predicate and skill learning,\nwithout relying on VLMs for policy generation or reasoning online.\n\nTo this end, we propose SymSkill, a unified framework that learns predicates, operators and goal-oriented skills in an unsupervised manner with unsegmented robot demonstrations data — requiring as few as 5 demonstrations per task. At the symbolic level, SymSkill identifies which object each trajectory segment moves toward using VLM and automatically defines predicates as relative pose classifiers. At the motion level, we adopt a dynamical system (DS)-based approach to learn stable motion policies from minimal demonstration data in near real-time.\nAt execution time, given a symbolic goal, specified with the learned predicates, SymSkill uses a symbolic planner to compose skills into long-sequence plans that generalize across number of objects. Due to the fast planning speed, SymSkill supports real-time error recovery at both the symbolic and skill levels.\nCoupled with a compliant passive DS controller, SymSkill ensures the execution is always stable, safe, and uninterrupted by replanning. In RoboCasa simulation, SymSkill learns 24 reusable skills from 12 short-horizon tasks and achieves a 85% success rate. Without additional data, it composes these skills to perform multi-step composite tasks with success. We also validate the approach on real-world robots, performing tasks by learning from 5 minutes of play data.\n\nContributions 1) a framework for joint discovery and learning of symbols and goal-oriented DS skills from unlabeled and unsegmented demonstrations of short and long-horizon tasks, 2) online execution and failure recovery with reactive planning at the task and motion level, and 3) an open-source implementation for out-of-the-box robot-learning in RoboCasa [7] with original demonstrations.",
            "llm_summary": "【论文的motivation是什么】  \n1. 复杂长时间操作在动态环境中的挑战性。  \n2. 现有模仿学习和任务-运动规划方法的局限性。  \n3. 需要实时失败恢复和组合泛化能力。  \n\n【提出了什么创新的方法】  \nSymSkill是一个统一的学习框架，结合了模仿学习和任务-运动规划的优点。它通过无标签和未分段的演示数据，联合学习谓词、操作符和技能。在执行时，SymSkill利用符号规划器根据指定的符号目标组合和重新排序学习到的技能，同时在运动和符号层面实时进行故障恢复。该方法在RoboCasa仿真中实现了85%的成功率，并在真实的Franka机器人上展示了从5分钟的未分段演示中学习执行多任务的能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Statistical Uncertainty Learning for Robust Visual-Inertial State Estimation",
            "authors": "Seungwon Choi,Donggyu Park,Seo-Yeon Hwang,Tae-Wan Kim",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01648",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01648",
            "arxiv_html_link": "https://arxiv.org/html/2510.01648v1",
            "abstract": "A fundamental challenge in robust visual-inertial odometry (VIO) is to dynamically assess the reliability of sensor measurements. This assessment is crucial for properly weighting the contribution of each measurement to the state estimate. Conventional methods often simplify this by assuming a static, uniform uncertainty for all measurements. This heuristic, however, may be limited in its ability to capture the dynamic error characteristics inherent in real-world data. To improve this limitation, we present a statistical framework that learns measurement reliability assessment online, directly from sensor data and optimization results. Our approach leverages multi-view geometric consistency as a form of self-supervision. This enables the system to infer landmark uncertainty and adaptively weight visual measurements during optimization. We evaluated our method on the public EuRoC dataset, demonstrating improvements in tracking accuracy with average reductions of approximately 24% in translation error and 42% in rotation error compared to baseline methods with fixed uncertainty parameters. The resulting framework operates in real time while showing enhanced accuracy and robustness. To facilitate reproducibility and encourage further research, the source code will be made publicly available.",
            "introduction": "State estimation is a fundamental research topic in robotics, given its indispensable role in the navigation of autonomous systems[1]. Among the various sensor modalities, the fusion of visual and inertial measurements has become a popular approach for state estimation [2, 3], primarily due to the complementary nature of cameras and Inertial Measurement Units (IMUs). While cameras provide rich geometric information about the environment, IMUs offer high-frequency motion measurements that are robust to visual degradation. This combination allows for robust tracking in a wide range of environments, from well-lit indoor spaces to challenging outdoor scenarios [4].\n\nThe problem is typically formulated as a non-linear least squares (NLS) optimization within a sliding window framework [5], where camera poses, 3D landmarks, and sensor biases are jointly estimated. Modern visual-inertial odometry (VIO) systems often employ factor graph optimization [6] to fuse heterogeneous sensor measurements, with each observation weighted by its assumed uncertainty. However, one potential limitation in many approaches is the reliance on static noise models, which may not fully capture the dynamic nature of measurement uncertainty [7].\n\nConsider the inherent variability in visual measurements: a feature tracked in well-textured, well-lit regions exhibits different error characteristics compared to one tracked near occlusion boundaries or in poorly illuminated areas. Similarly, the quality of stereo depth estimates can vary with baseline geometry and scene depth [8]. Traditional VIO frameworks often address this heterogeneity through uniform information matrices. This treatment of all visual observations as equally reliable is a simplifying assumption that may not always account for the rich statistical structure inherent in the measurements.\n\nThe effects of this limitation can become particularly pronounced in challenging scenarios. When a system encounters rapid motion, varying lighting conditions, or textureless environments, a mismatch between the assumed and actual measurement uncertainties can contribute to less accurate estimates, convergence issues, or tracking failures [9]. As autonomous systems operate in increasingly diverse environments, the ability to adapt measurement uncertainty models online could be crucial for maintaining consistent performance.\n\nTo explore this area, this work proposes a statistical uncertainty learning approach for visual-inertial odometry. Our framework is designed as an alternative to static noise models, aiming to statistically learn and propagate measurement uncertainties throughout the estimation pipeline. By leveraging multi-view geometric consistency and covariance analysis, we investigate whether a system can better estimate the error characteristics of its sensor measurements. Figure 1 illustrates our approach to modeling measurement variability through adaptive covariance representations.\n\nOur system employs a dual-pipeline architecture that integrates statistical uncertainty learning with real-time VIO estimation. The upper pipeline handles real-time pose tracking via Perspective-n-Point (PnP) optimization, while the lower pipeline performs map refinement through sliding window bundle adjustment integrated with our statistical uncertainty learning module. The learned uncertainty estimates are propagated back to the tracking pipeline, establishing a feedback mechanism that enhances system performance. Figure 2 provides an overview of this dual-pipeline architecture.\n\nSpecifically, the key contributions of our proposed framework include:\n\nStatistical multi-view uncertainty learning: We propose a method to estimate 3D landmark uncertainty by analyzing the distribution of triangulated positions across multiple viewpoints. This approach aims to provide a data-driven measure of geometric consistency and measurement reliability.\n\nStatistical multi-view uncertainty learning: We propose a method to estimate 3D landmark uncertainty by analyzing the distribution of triangulated positions across multiple viewpoints. This approach aims to provide a data-driven measure of geometric consistency and measurement reliability.\n\nAdaptive statistical weighting: We investigate transforming learned 3D world uncertainties to 2D pixel-space information matrices through uncertainty propagation. This enables observation-specific weighting that may better reflect the statistical reliability of each measurement.\n\nOpen-source statistical VIO framework: We present a complete, open-source implementation of our approach, which integrates with existing optimization frameworks while maintaining computational efficiency suitable for real-time applications.\n\nThrough evaluation on the EuRoC dataset [10], we demonstrate that our statistical learning approach achieves improved accuracy and robustness compared to baseline methods using static noise models across various conditions. The proposed method represents a step toward more adaptive and statistically-grounded visual-inertial state estimation.\n\n1. Statistical multi-view uncertainty learning: We propose a method to estimate 3D landmark uncertainty by analyzing the distribution of triangulated positions across multiple viewpoints. This approach aims to provide a data-driven measure of geometric consistency and measurement reliability.\n\n2. Adaptive statistical weighting: We investigate transforming learned 3D world uncertainties to 2D pixel-space information matrices through uncertainty propagation. This enables observation-specific weighting that may better reflect the statistical reliability of each measurement.\n\n3. Open-source statistical VIO framework: We present a complete, open-source implementation of our approach, which integrates with existing optimization frameworks while maintaining computational efficiency suitable for real-time applications.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的视觉惯性测量方法依赖静态噪声模型，无法捕捉动态测量不确定性。  \n2. 在复杂环境中，测量的不确定性变化可能导致估计不准确和跟踪失败。  \n3. 需要一种在线学习测量可靠性的方法，以提高状态估计的准确性和鲁棒性。  \n\n【提出了什么创新的方法】  \n本研究提出了一种统计不确定性学习框架，用于视觉惯性测量（VIO）。该方法通过多视角几何一致性进行自我监督，在线学习传感器数据的测量可靠性。系统采用双管道架构，上管道处理实时姿态跟踪，下管道进行地图优化与不确定性学习模块集成。通过评估EuRoC数据集，结果显示该方法在跟踪精度上有显著提升，平均减少了约24%的平移误差和42%的旋转误差，且实现了实时操作。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models",
            "authors": "Zijun Lin,Jiafei Duan,Haoquan Fang,Dieter Fox,Ranjay Krishna,Cheston Tan,Bihan Wen",
            "subjects": "Robotics (cs.RO)",
            "comment": "Project Page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.01642",
            "code": "https://jimntu.github.io/FailSafe/",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01642",
            "arxiv_html_link": "https://arxiv.org/html/2510.01642v1",
            "abstract": "Recent advances in robotic manipulation have integrated low-level robotic control into Vision-Language Models (VLMs), extending them into Vision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve strong performance in downstream robotic applications, supported by large-scale crowd-sourced robot training data, they still inevitably encounter failures during execution. Enabling robots to reason about and recover from unpredictable and abrupt failures remains a critical challenge. Existing robotic manipulation datasets, collected in either simulation or the real world, primarily provide only ground-truth trajectories, leaving robots unable to recover once failures occur. Moreover, the few datasets that address failure detection typically offer only textual explanations, which are difficult to utilize directly in VLA models. To address this gap, we introduce FailSafe, a novel failure generation and recovery system that automatically produces diverse failure cases paired with executable recovery actions. FailSafe can be seamlessly applied to any manipulation task in any simulator, enabling scalable creation of failure–action data. To demonstrate its effectiveness, we fine-tune LLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results show that FailSafe-VLM successfully helps robotic arm detect and recover from potential failures, improving the performance of three state-of-the-art VLA models (πo\\pi_{o}-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several tasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different spatial configurations, camera viewpoints, and robotic embodiments. We plan to release the FailSafe code to the community. Project Page: https://jimntu.github.io/FailSafe/",
            "introduction": "Vision-Language-Action (VLA) model has made remarkable progress recently in open-world robot manipulation task [1, 2, 3, 4, 5, 6, 7]. Given the image observation and the language instruction, a VLA model can directly output executable robot actions to perform diverse tasks. The cornerstone of this progress lies in the increasingly large, high-quality robot datasets collected through community efforts [8, 9, 10, 11, 12]. These datasets, either rolled out in simulation or teleoperated in real-world, typically consist of clean, ground-truth trajectories. However, relying solely on such correct data may not be sufficient, as robots inevitably make mistakes and may encounter situations not represented in the collected trajectories. Therefore, enabling robots to detect and recover from mistakes is crucial for achieving more robust and explainable downstream robotic applications.\n\nLearning and recovering from failure is a fundamental aspect of human intelligence [13, 14, 15, 16], yet remains challenging to integrate into VLA models. Existing failure reflection methods for robotic manipulation, such as OLAF [17] and YAY [18], require an external observer to continuously monitor robot operation and intervene whenever a potential failure is detected. This reliance on human supervision is impractical for real-world scenarios where robots are expected to operate fully autonomously.\n\nMeanwhile, recent pipelines focusing on robotic failures generation, such as AHA [19] and RoboFAC [20], are able to create large-scale failure trajectories dataset with detailed textual descriptions of task understanding and failure analysis. VLMs fine-tuned on these datasets have shown promising results in detecting potential failures automatically during task execution without human supervision. However, these pipelines fall short in failure correction, either overlooking this aspect entirely [19] or offering only textual feedback that robots cannot directly execute [20, 21]. For example, recovery instructions like “the gripper should move left to align with the center of the cube” are inherently ambiguous about magnitudes, scales and endpoints, thereby limiting their effectiveness in improving VLA control. This raises a key question: Is it possible to design an automatic pipeline that generates both high-level failure reasoning and low-level corrective actions at scale, in a way that directly benefits VLA models?\n\nTo address this gap, we propose FailSafe, an automatic failure generation and recovery pipeline designed to adapt seamlessly across diverse tasks, simulators, and embodiments. As illustrated in Figure 1, the dataset produced by FailSafe comprises two primary components: possible Failure scenarios and their corresponding executable recovery actions. FailSafe introduces diverse failure types including translation, rotation and no-ops failure with randomly sampled perturbation magnitudes. These failures are injected at arbitrary steps of ground-truth rollouts in simulation, intentionally causing the original task to fail. This design closely mimics how VLA-controlled robots can make mistakes unexpectedly during real deployment.\n\nFurthermore, unlike prior work that frames failure reasoning merely as textual explanations [22, 19, 20], FailSafe collects executable recovery actions that can be directly applied by robots in real time. To ensure accuracy and robustness, each recovery action is validated through a rigorous sanity check, confirming that it effectively resolves the failure scenario and eventually leads to successful task completion. By incorporating multi-view images and task instructions, this pipeline produces a customized, accurate, and explainable FailSafe dataset for robotic manipulation that would be prohibitively difficult to obtain through real-world data collection.\n\nExperimental results further highlight the effectiveness of FailSafe in VLA deployments. We fine-tune LLaVa-OV-7B [23] to obtain FailSafe-VLM, which demonstrates strong failure reasoning capabilities that generalize across diverse spatial configurations, camera viewpoints, and robotic embodiments. FailSafe-VLM significantly outperforms state-of-the-art VLMs, such as GPT-4o [24] and Gemini-2.5-flash [25], in detecting failures and generating accurate recovery actions on unseen failure trajectories. Furthermore, when serving as an external assistant to VLA models, FailSafe-VLM identifies and corrects potential failures in real time during robot execution, leading to an average performance improvement of up to 22.6% on ManiSkill tasks [26] compared to three VLA baselines without FailSafe-VLM. Finally, FailSafe-VLM also proves effective when evaluated on a previously unseen robot arm, demonstrating its ability to generalize across different embodiments.\n\nOverall, we make the following contributions.\n\nWe are the first to propose FailSafe, a scalable framework that can be seamlessly built on top of any tasks in simulators to generate both failure reasoning explanations and accurate recovery actions that can be directly executed by robots.\n\nWe are the first to propose FailSafe, a scalable framework that can be seamlessly built on top of any tasks in simulators to generate both failure reasoning explanations and accurate recovery actions that can be directly executed by robots.\n\nWe show that FailSafe dataset enables existing VLMs to reason about failures and significantly improves the performance of VLA models across diverse viewpoints, spatial configurations, and robotic embodiments.\n\nWe will open-source FailSafe to support the community in exploring failure reasoning for building more intelligent and explainable embodied AI systems\n\n1. We are the first to propose FailSafe, a scalable framework that can be seamlessly built on top of any tasks in simulators to generate both failure reasoning explanations and accurate recovery actions that can be directly executed by robots.\n\n2. We show that FailSafe dataset enables existing VLMs to reason about failures and significantly improves the performance of VLA models across diverse viewpoints, spatial configurations, and robotic embodiments.\n\n3. We will open-source FailSafe to support the community in exploring failure reasoning for building more intelligent and explainable embodied AI systems",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的VLA模型在执行过程中不可避免地会遇到失败，缺乏有效的故障检测和恢复机制。  \n2. 现有的数据集主要提供真实轨迹，无法帮助机器人在失败后恢复。  \n3. 现有的故障检测方法依赖于文本反馈，无法直接应用于机器人控制。  \n\n【提出了什么创新的方法】  \n提出了FailSafe，一个自动化的故障生成和恢复系统，能够生成多样的故障案例及其可执行的恢复动作。该系统可以无缝应用于任何模拟器中的操作任务，生成的故障-动作数据集使得VLA模型能够在执行过程中有效检测和恢复潜在的故障。通过对LLaVa-OV-7B的微调，FailSafe-VLM在多个任务上实现了平均22.6%的性能提升，并在不同的空间配置、相机视角和机器人形态上展现了良好的泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations",
            "authors": "Qiyuan Zeng,Chengmeng Li,Jude St. John,Zhongyi Zhou,Junjie Wen,Guorui Feng,Yichen Zhu,Yi Xu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "technique report. The website is available atthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.01607",
            "code": "https://activeumi.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01607",
            "arxiv_html_link": "https://arxiv.org/html/2510.01607v1",
            "abstract": "We present ActiveUMI, a framework for a data collection system that transfers in-the-wild human demonstrations to robots capable of complex bimanual manipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized controllers that mirror the robot’s end-effectors, bridging human-robot kinematics via precise pose alignment. To ensure mobility and data quality, we introduce several key techniques, including immersive 3D model rendering, a self-contained wearable computer, and efficient calibration methods. ActiveUMI’s defining feature is its capture of active, egocentric perception. By recording an operator’s deliberate head movements via a head-mounted display, our system learns the crucial link between visual attention and manipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies trained exclusively on ActiveUMI data achieve an average success rate of 70% on in-distribution tasks and demonstrate strong generalization, retaining a 56% success rate when tested on novel objects and in new environments. Our results demonstrate that portable data collection systems, when coupled with learned active perception, provide an effective and scalable pathway toward creating generalizable and highly capable real-world robot policies.",
            "introduction": "Robot foundation models promise generalist policies but are currently constrained by the scale and alignment of available robot data relative to web‑scale corpora. A central challenge is therefore scaling data collection while preserving embodiment fidelity. Prevailing sources—in‑lab teleoperation, human videos, and simulation—each have limitations: teleoperation is costly to scale; human videos [2, 34, 16, 46, 20] incur a cross‑embodiment gap (human to robot); and simulation suffers a sim‑to‑real gap (physics to hardware [30]).\n\nA promising middle ground is sensorized hand‑held interfaces (e.g., grippers, dexterous‑hand devices) that capture action‑aligned trajectories. Yet most current interfaces overlook active, egocentric perception: humans move their heads to manage occlusion and gather context, while existing rigs rely primarily on wrist‑mounted cameras. Even with a wide field-of-view, an end‑effector–centric view underserves long‑horizon tasks and fine manipulation and misaligns with platforms that use head‑mounted cameras. These observations motivate data‑collection and policy‑learning pipelines that couple head‑ego sensing with wrist‑eye control, enabling viewpoint selection as part of the task and improving transfer to real robots.\n\nTo this end, we propose ActiveUMI, a universal manipulation interface with active perception for in-the-wild robot policy learning. Our approach is built on two core principles for scalable data collection: (i) the system must tightly align the robot’s embodiment with natural human movement, and (ii) it must enable active perception to expose the right sensory information at the right time. Our system addresses these needs with a specially designed, portable VR teleoperation kit. We developed a hardware architecture that allows the target robot’s own custom grippers to be mounted directly onto the VR controllers, mirroring the end-effectors precisely. The entire system is self-contained in a backpack, and we implement several calibration techniques to ensure consistent, high-quality data collection in diverse real-world environments. To enable active perception, we map the operator’s head movements to a movable robotics arm with a head-mounted camera. This allows the learned policy to control its own viewpoint, actively seeking out information to solve complex, long-horizon, or visually occluded tasks that are challenging for systems with only static or wrist-mounted cameras.\n\nWe evaluate ActiveUMI on six challenging, real‑robot bimanual tasks that combine precise hand–object interactions with long‑horizon manipulation using only the egocentric head camera and wrist proprioception available to the robot platform. By training policies trained purely on ActiveUMI demonstrations, they attain an average 70% success rate on all tasks. Relative to non-active perception counterparts (i.e., policies trained from wrist‑centric views or static third‑person cameras), ActiveUMI improves average success by 44% and 38%, respectively. Furthermore, when evaluated with novel objects and scenes, learned policies retain 56% of the average success rate, indicating a meaningful generalization from in-wild data.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有数据收集方法在规模和对齐方面存在限制，难以实现高效的机器人政策学习。  \n2. 传统的传感器接口忽视了主动的自我感知，无法有效捕捉长时间任务中的关键信息。  \n3. 需要一种新的方法来结合人类的自然运动与机器人操作，以提高数据收集的质量和有效性。  \n\n【提出了什么创新的方法】  \nActiveUMI是一个结合主动感知的通用操作接口，旨在通过人类演示学习机器人政策。该系统使用便携式VR遥操作设备，精确对齐人类和机器人末端执行器的运动，并通过头戴式显示器捕捉操作员的头部运动，从而实现主动感知。通过这种方式，ActiveUMI能够在复杂的双手操作任务中有效收集数据，并在六个挑战性任务中实现了70%的平均成功率。与传统方法相比，ActiveUMI在成功率上提高了44%和38%，并在新物体和环境中保持了56%的成功率，展示了其在真实世界中的广泛适用性和强大的泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "MiniBEE: A New Form Factor for Compact Bimanual Dexterity",
            "authors": "Sharfin Islam,Zewen Chen,Zhanpeng He,Swapneel Bhatt,Andres Permuy,Brock Taylor,James Vickery,Pedro Piacenza,Cheng Zhang,Matei Ciocarlie",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01603",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01603",
            "arxiv_html_link": "https://arxiv.org/html/2510.01603v1",
            "abstract": "Bimanual robot manipulators can achieve impressive dexterity, but typically rely on two full six- or seven-degree-of-freedom arms so that paired grippers can coordinate effectively. This traditional framework increases system complexity while only exploiting a fraction of the overall workspace for dexterous interaction. We introduce the MiniBEE (Miniature Bimanual End-effector), a compact system in which two reduced-mobility arms (3+ DOF each) are coupled into a kinematic chain that preserves full relative positioning between grippers. To guide our design, we formulate a kinematic dexterity metric that enlarges the dexterous workspace while keeping the mechanism lightweight and wearable. The resulting system supports two complementary modes: (i) wearable kinesthetic data collection with self-tracked gripper poses, and (ii) deployment on a standard robot arm, extending dexterity across its entire workspace. We present kinematic analysis and design optimization methods for maximizing dexterous range, and demonstrate an end-to-end pipeline in which wearable demonstrations train imitation learning policies that perform robust, real-world bimanual manipulation.",
            "introduction": "In recent years, bimanual robotic manipulators have shown remarkable dexterity. The combination of imitation learning from human demonstrations and two well-articulated kinematic chains has enabled such systems to use simple parallel grippers to autonomously perform highly dexterous tasks [1, 2, 3, 4, 5, 6, 7], with robustness to initial conditions or perturbations encountered during execution [8, 9, 10].\n\nTo achieve these results, current systems typically rely on the combination of two 6- or 7-degree-of-freedom (DOF) robotic arms. The dexterous workspace of the overall system, defined here as the space where the two grippers have complete positioning ability w.r.t. each other, thus lies at the intersection of the two arms’ individual workspaces. As a result, this dexterous workspace is relatively small, and only covers a fraction of each arm’s overall individual workspace. Moreover, this dexterous workspace is fixed relative to the base of the entire system, meaning any object must be brought into this portion of the system’s workspace to be manipulated. Overall, this framework for bimanual manipulation is capable of dexterity, but has limitations in terms of footprint and overall portability.\n\nIn this paper, we propose a new approach to bimanual dexterity that aims to address the challenges described above for bimanual systems while preserving many of their benefits. Our fundamental insight is as follows. If each of the arms of a bimanual system has reduced mobility (three+ DOF), the kinematic chain connecting the two grippers still has sufficient articulation (6+ DOF) to allow arbitrary positioning of the grippers w.r.t. each other, and, with careful kinematic design, we can ensure that the dexterous workspace of the overall system is large enough for bimanual tasks. We dub our system the MiniBEE, for Miniature Bimanual End-effector.\n\nThanks to the reduced articulation of each individual arm, the overall MiniBEE remains compact and lightweight. In turn, this provides two key benefits:\n\nThe MiniBEE can be worn directly by an operator, allowing for easy and highly mobile kinesthetic data collection. When this data is used to train autonomous policies, the encoders in the kinematic chain provide full information on the relative pose of the two grippers, removing the need for external tracking or SLAM while the MiniBEE is manipulating an object held in or transferred between its grippers.\n\nThe MiniBEE can also be mounted on a commodity robot arm, for example when a trained policy is deployed. This means that the entire reachable area of the robot arm instantly becomes available for the MiniBEE, and the dexterous workspace (i.e. the area where the two grippers have complete positioning authority w.r.t. each other) is scaled by the overall reach of the robot arm.\n\nThe MiniBEE functions both as wearable mechanism for easy data collection with self-tracking dexterity, and as a bimanual end-effector allowing a single traditional robot arm to become dexterous over the entirety of its workspace. Overall, our main contributions are as follows.\n\nTo the best of our knowledge, this is the first example of reduced mobility arms enabling full relative mobility for bimanual tasks in a system that is compact enough for wearable collection of kinesthetic demonstrations.\n\nWe introduce a kinematic analysis tool that allows a designer to maximize the dexterous workspace while limiting the total number of DOF, and use these to compare multiple possible design variants.\n\nWe demonstrate the end-to-end pipeline consisting of wearable kinesthetic data collection followed by training standard imitation learning policies which leverage the proprioceptive component of the demonstrations to achieve robust, dexterous bimanual manipulation with large reach in the real world.\n\n1. The MiniBEE can be worn directly by an operator, allowing for easy and highly mobile kinesthetic data collection. When this data is used to train autonomous policies, the encoders in the kinematic chain provide full information on the relative pose of the two grippers, removing the need for external tracking or SLAM while the MiniBEE is manipulating an object held in or transferred between its grippers.\n\n2. The MiniBEE can also be mounted on a commodity robot arm, for example when a trained policy is deployed. This means that the entire reachable area of the robot arm instantly becomes available for the MiniBEE, and the dexterous workspace (i.e. the area where the two grippers have complete positioning authority w.r.t. each other) is scaled by the overall reach of the robot arm.\n\n1. To the best of our knowledge, this is the first example of reduced mobility arms enabling full relative mobility for bimanual tasks in a system that is compact enough for wearable collection of kinesthetic demonstrations.\n\n2. We introduce a kinematic analysis tool that allows a designer to maximize the dexterous workspace while limiting the total number of DOF, and use these to compare multiple possible design variants.\n\n3. We demonstrate the end-to-end pipeline consisting of wearable kinesthetic data collection followed by training standard imitation learning policies which leverage the proprioceptive component of the demonstrations to achieve robust, dexterous bimanual manipulation with large reach in the real world.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的双手机器人操控系统复杂，且只利用了部分工作空间。  \n2. 现有系统通常依赖于全自由度的机械臂，导致体积庞大且不便于移动。  \n3. 需要一种新方法来提高双手操控的灵活性和可穿戴性。  \n\n【提出了什么创新的方法】  \n本文提出了MiniBEE（Miniature Bimanual End-effector），一种紧凑的双手操控系统，采用每个手臂3+自由度的设计，通过运动学链连接两个抓手，保持其相对位置的灵活性。该系统支持可穿戴的运动数据收集和在标准机器人臂上的部署，扩大了操作范围。通过引入运动学分析工具，优化了灵活工作空间，并展示了一个端到端的流程，利用可穿戴演示训练模仿学习策略，实现了在真实环境中稳健的双手操控。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Real-time Multi-Plane Segmentation Based on GPU Accelerated High-Resolution 3D Voxel Mapping for Legged Robot Locomotion",
            "authors": "Shun Niijima,Ryoichi Tsuzaki,Noriaki Takasugi,Masaya Kinoshita",
            "subjects": "Robotics (cs.RO)",
            "comment": "This work has been submitted to the IEEE for possible publication. Copyright may be transfered without notice, after which this version may no longer be accessible",
            "pdf_link": "https://arxiv.org/pdf/2510.01592",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01592",
            "arxiv_html_link": "https://arxiv.org/html/2510.01592v1",
            "abstract": "This paper proposes a real-time multi-plane segmentation method based on GPU-accelerated high-resolution 3D voxel mapping for legged robot locomotion. Existing online planar mapping approaches struggle to balance accuracy and computational efficiency: direct depth image segmentation from specific sensors suffers from poor temporal integration, height map-based methods cannot represent complex 3D structures like overhangs, and voxel-based plane segmentation remains unexplored for real-time applications. To address these limitations, we develop a novel framework that integrates vertex-based connected component labeling with random sample consensus based plane detection and convex hull, leveraging GPU parallel computing to rapidly extract planar regions from point clouds accumulated in high-resolution 3D voxel maps. Experimental results demonstrate that the proposed method achieves fast and accurate 3D multi-plane segmentation at over 30 Hz update rate even at a resolution of 0.01 m, enabling the detected planes to be utilized in real time for locomotion tasks. Furthermore, we validate the effectiveness of our approach through experiments in both simulated environments and physical legged robot platforms, confirming robust locomotion performance when considering 3D planar structures.",
            "introduction": "Legged robots enable traversal of challenging 3D structures by utilizing discrete footholds, facilitating locomotion in environments inaccessible to wheeled platforms, as illustrated in Fig. 1(a). This versatility makes them promising for diverse applications such as exploration, surveillance, and autonomous inspection [1, 2, 3, 4].\n\nTo achieve safe and efficient locomotion, legged robots require accurate recognition of traversable foothold regions and rapid detection of stable planar surfaces[5, 6, 7]. For this purpose, height maps have been widely utilized [8, 9, 10], representing environments as 2.5-dimensional structures with a single height value for each (x,y)(x,y) coordinate. While computationally efficient, height maps fundamentally fail to represent multiple planes at identical coordinates, making it challenging to model multi-layered surfaces and overhanging structures. This limitation results in collision risks and locomotion failures when traversing open-tread stairs or locomotion beneath structures such as tables, as illustrated in Fig.1(b)(c).\n\nTo address these challenges, several studies have explored extensions to 3D voxel mapping for locomotion[11, 12]. However, existing frameworks struggle to balance computational speed and accuracy due to the increased processing time caused by the management of 3D voxels and the substantial increase in the number of point clouds. This computational overhead severely constrains the high-speed locomotion capability of legged robots. Although some approaches attempt to ensure real-time performance by lowering the resolution, such measures inevitably sacrifice detection accuracy and environmental adaptability. Therefore, existing frameworks still fail to achieve real-time, high-precision 3D multi-plane detection.\n\nThis research addresses this challenge by proposing a real-time multi-plane segmentation framework based on GPU-accelerated high-resolution 3D voxel mapping. The proposed framework combines a 3D voxel mapping module with a GPU-accelerated multi-plane segmentation using connected component labeling (CCL) clustering and cluster based parallelized plane boundary estimation to enable rapid extraction of planar regions and their boundaries from large-scale point clouds. The framework achieves real-time multi-plane segmentation in the robot’s vicinity while maintaining an exceptionally high resolution of 0.01 m within the 3D voxel representation.\n\nThe primary contributions of this research are as follows:\n\nProposal of a GPU-accelerated 3D multi-plane segmentation utilizing CCL clustering and cluster based parallelized plane boundary estimation method.\n\nImplementation of a comprehensive framework combining 3D voxel mapping with multi-plane segmentation to enable environmental perception for legged robot locomotion.\n\nComprehensive experimental validation demonstrating the effectiveness and successful deployment of the proposed method on legged robot platforms for safe 3D locomotion tasks.\n\n1. Proposal of a GPU-accelerated 3D multi-plane segmentation utilizing CCL clustering and cluster based parallelized plane boundary estimation method.\n\n2. Implementation of a comprehensive framework combining 3D voxel mapping with multi-plane segmentation to enable environmental perception for legged robot locomotion.\n\n3. Comprehensive experimental validation demonstrating the effectiveness and successful deployment of the proposed method on legged robot platforms for safe 3D locomotion tasks.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的在线平面映射方法在准确性和计算效率之间难以平衡。  \n2. 深度图像分割受限于传感器的时间集成能力，无法有效处理复杂的3D结构。  \n3. 现有的体素基础平面分割方法尚未探索实时应用。  \n\n【提出了什么创新的方法】  \n提出了一种基于GPU加速的高分辨率3D体素映射的实时多平面分割框架。该框架结合了基于顶点的连通组件标记与随机采样一致性平面检测，利用GPU并行计算快速提取点云中的平面区域。实验结果表明，该方法在0.01米的高分辨率下以超过30 Hz的更新率实现了快速准确的3D多平面分割，能够实时用于机器人运动任务，并在模拟环境和实际的四足机器人平台上验证了其有效性，确保了在复杂3D结构下的稳健运动性能。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments",
            "authors": "Wei Han Chen,Yuchen Liu,Alexiy Buynitsky,Ahmed H. Qureshi",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01519",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01519",
            "arxiv_html_link": "https://arxiv.org/html/2510.01519v1",
            "abstract": "Robot navigation in large, complex, and unknown indoor environments is a challenging problem. The existing approaches, such as traditional sampling-based methods, struggle with resolution control and scalability, while imitation learning-based methods require a large amount of demonstration data. Active Neural Time Fields (ANTFields) have recently emerged as a promising solution by using local observations to learn cost-to-go functions without relying on demonstrations. Despite their potential, these methods are hampered by challenges such as spectral bias and catastrophic forgetting, which diminish their effectiveness in complex scenarios. To address these issues, our approach decomposes the planning problem into a hierarchical structure. At the high level, a sparse graph captures the environment’s global connectivity, while at the low level, a planner based on neural fields navigates local obstacles by solving the Eikonal PDE. This physics-informed strategy overcomes common pitfalls like spectral bias and neural field fitting difficulties, resulting in a smooth and precise representation of the cost landscape. We validate our framework in large-scale environments, demonstrating its enhanced adaptability and precision compared to previous methods, and highlighting its potential for online exploration, mapping, and real-world navigation. https://sites.google.com/view/mntfields/home",
            "introduction": "Navigating large, unknown environments presents significant challenges in robotics, where both mapping and planning are crucial yet difficult tasks. Traditional mapping approaches often generate occupancy or signed distance field (SDF) maps that require additional processing, such as grid search or optimization, to extract navigable paths [1, 2, 3, 4, 5]. This extra step not only increases computational overhead but also complicates the transition from a raw map to an actionable navigation plan.\n\nAlternatively, some methods directly build probabilistic roadmaps (PRMs) from sensor data [6, 7]. While these techniques can yield efficient paths in simpler settings, they tend to become unwieldy in expansive, complex environments. The sheer number of nodes required to accurately represent intricate spaces often leads to memory-intensive computations and difficulties in maintaining and controlling the roadmap structure over large scales.\n\nMore recently, methods like Neural Time Fields (NTFields) [8] have been introduced to infer cost-to-go functions directly by solving the Eikonal equation. These approaches aim to bypass the intermediate mapping step by providing an implicit representation of the navigation cost. However, NTFields encounter significant hurdles when scaled up to large scenes. Their reliance on neural network architectures introduces issues such as spectral bias, catastrophic forgetting, and poor convergence. Moreover, the inherent scaling challenge of solving a partial differential equation (PDE) further complicates their application in diverse, cluttered environments.\n\nInspired by the hierarchical planning strategies we use in everyday navigation—such as how mapping applications outline broad routes while vehicles make local maneuvers—we propose a modular, hierarchical approach called Modular-NTFields (mNTFields) to address navigation challenges.\n\nAt a high level, our method constructs an online sparse navigation graph from local observations, capturing the connectivity between different subparts of a large environment. This high-level strategy is informed by our low-level method and yields a compact and efficient representation, allowing for rapid global planning without the burden of excessive detail. On a local level, we integrate NTFields to develop detailed cost-to-go maps for each subpart based on local observations. By solving the Eikonal equation locally, our approach effectively manages obstacles and complex geometries while alleviating the scalability issues that often affect traditional NTFields. Additionally, we enhance local planning with Temporal Difference Metric Learning (TDM) [9] to further improve convergence to an accurate Eikonal PDE solution.\n\nWe validate mNTFields across several challenging scenarios, demonstrating that it outperforms existing methods—particularly in large, complex, unknown indoor environments where standard approaches often struggle. Our experiments show that mNTFields achieves significantly faster navigation with higher success rates. We also showcase its practical deployment on a quadruped robot navigating through multiple rooms and narrow passages. This work highlights the potential of hierarchical planning frameworks for advancing robust and scalable robot navigation in unknown environments.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人在复杂未知环境中的导航面临显著挑战。  \n2. 现有方法在解决复杂场景时存在谱偏差和灾难性遗忘问题。  \n3. 传统映射和规划方法在处理大规模环境时效率低下。  \n\n【提出了什么创新的方法】  \n本文提出了一种模块化的层次化规划方法（mNTFields），通过将规划问题分解为高层稀疏导航图和低层基于神经场的局部规划，克服了传统方法的局限性。高层结构捕捉环境的全局连通性，而低层方法通过求解Eikonal PDE来管理局部障碍。该方法有效提高了在复杂环境中的适应性和精度，实验表明mNTFields在大规模未知室内环境中实现了更快的导航和更高的成功率，展示了其在实际部署中的潜力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Pose Estimation of a Thruster-Driven Bioinspired Multi-Link Robot",
            "authors": "Nicholas B. Andrews,Yanhao Yang,Sofya Akhetova,Kristi A. Morgansen,Ross L. Hatton",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01485",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01485",
            "arxiv_html_link": "https://arxiv.org/html/2510.01485v1",
            "abstract": "This work demonstrates pose (position and shape) estimation for a free-floating, bioinspired multi-link robot with unactuated joints, link-mounted thrusters for control, and a single gyroscope per link, resulting in an underactuated, minimally sensed platform. Through a proof-of-concept hardware experiment and offline Kalman filter analysis, we show that the robot’s pose can be reliably estimated. State estimation is performed using an unscented Kalman filter augmented with Gaussian process residual learning to compensate for non-zero-mean, non-Gaussian noise. We further show that a filter trained on a multi-gait dataset (forward, backward, left, right, and turning) performs comparably to one trained on a larger forward-gait-only dataset when both are evaluated on the same forward-gait test trajectory. These results reveal overlap in the gait input space, which can be exploited to reduce training data requirements while enhancing the filter’s generalizability across multiple gaits.",
            "introduction": "The performance of dynamical systems such as underwater robots, autonomous vehicles, and aircraft autopilots critically depends on accurate knowledge of the system state to ensure robustness against disturbances and maintain safety guarantees. At the same time, size, weight, and power constraints limit the number and type of sensors and actuators that can be incorporated into many systems, leading to systems that are both underactuated and minimally sensed.\n\nIn this paper, we consider the specific challenge of\nstate estimation for chains of rigid bodies equipped with inertial measurement units (IMUs), but lacking sensors for interlink joint angles or external measurements from a motion capture system. These system constraints are motivated by our work on next-generation underwater autonomous vehicles composed of self-contained units that can mechanically connect and disconnect to form configurable chains.\n\nPrior work with multi-link and snake robots has used Kalman filters to fuse information from multiple sensors into state and pose estimates [1, 2, 3, 4]. Two key differences between the systems considered in these works and our target systems are that the previously considered systems were equipped with joint angle encoders, whereas our systems are not, and that the systems in the prior work were directly controlled via torque or position control around their joints, whereas our systems have passive joints and are driven by a set of thrusters arrayed along the body [5]. Other work has been aimed at estimating the shape of a multi-link system using IMUs without joint encoders [6, 7, 8]. However, that work has been directed at uncontrolled fixed-base chains over short time horizons, rather than the mobile chains we are targeting, and so has not considered overall body pose estimation.\n\nOur experimental platform is the LandSalp, a planar chain of links equipped with force-controlled omniwheels as illustrated in Fig. 1. This drive system allows us to physically simulate the effects of fluid drag and thrust on a chain immersed in fluid without incurring the logistical complications of actually operating a system in water. The dynamics of the LandSalp provide enough physical noise to be an “interesting” estimation problem, and this system serves as a proving ground for testing controllers and observers before deploying them on a waterborne system such as our FloatSalp [9, 10].\n\nIn previous work, we have used external motion capture systems to perform system identification on the LandSalp and achieve closed-loop control [5]. In this paper, we focus on state estimation and closing the sensor feedback loop without the use of an external motion capture sensor. The primary contribution of this work is the development and offline demonstration of a sequential state estimator for LandSalp’s pose. The estimator accounts for non-Gaussian, non-zero-mean process and measurement noise and demonstrates stable tracking using only gyroscope measurements. This work represents a key milestone toward bridging the gap between controllability and observability in underactuated, minimally sensed, salp-inspired multi-link systems.\n\nThe body of the paper is organized as follows: Section II introduces the experimental robotic platform, and Section III develops the corresponding mathematical model. Section IV details the design of the sequential state estimator, while Section V presents and discusses the offline experimental results. Finally, Section VI concludes the paper and outlines directions for future work.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要在传感器和执行器受限的情况下进行状态估计。  \n2. 现有的多链机器人研究依赖于关节角编码器，而本研究关注无编码器的系统。  \n3. 目标是实现对多链接体的姿态估计，以提高控制和可观察性。  \n\n【提出了什么创新的方法】  \n本研究提出了一种基于无味噪声和非高斯过程的序列状态估计器，专门用于LandSalp机器人的姿态估计。通过使用单个陀螺仪的测量，该方法能够在没有外部运动捕捉系统的情况下实现稳定跟踪。实验结果表明，利用多步态数据集训练的滤波器在多种运动模式下表现良好，显示出其在减少训练数据需求和增强滤波器泛化能力方面的潜力。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs",
            "authors": "Mohamad Al Mdfaa,Svetlana Lukina,Timur Akhtyamov,Arthur Nigmatzyanov,Dmitrii Nalberskii,Sergey Zagoruyko,Gonzalo Ferrer",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "This work has been submitted to the IEEE for possible publication",
            "pdf_link": "https://arxiv.org/pdf/2510.01483",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01483",
            "arxiv_html_link": "https://arxiv.org/html/2510.01483v1",
            "abstract": "Vision-language models (VLMs) have shown potential for robot navigation but encounter fundamental limitations: they lack persistent scene memory, offer limited spatial reasoning, and do not scale effectively with video duration for real-time application.\nWe present VL-KnG, a Visual Scene Understanding system that tackles these challenges using spatiotemporal knowledge graph construction and computationally efficient query processing for navigation goal identification.\nOur approach processes video sequences in chunks utilizing modern VLMs, creates persistent knowledge graphs that maintain object identity over time, and enables explainable spatial reasoning through queryable graph structures.\nWe also introduce WalkieKnowledge, a new benchmark with about 200200 manually annotated questions across 88 diverse trajectories spanning approximately 100100 minutes of video data, enabling fair comparison between structured approaches and general-purpose VLMs.\nReal-world deployment on a differential drive robot demonstrates practical applicability, with our method achieving 77.27%77.27\\% success rate and 76.92%76.92\\% answer accuracy, matching Gemini 2.5 Pro performance while providing explainable reasoning supported by the knowledge graph, computational efficiency for real-time deployment across different tasks, such as localization, navigation and planning. Code and dataset will be released after acceptance.",
            "introduction": "Robot navigation in unstructured environments requires a sophisticated understanding of spatial relationships and temporal object dynamics to enable natural language-guided goal-directed behavior.\nRecent advances in vision-language models [1, 2] have opened new capabilities for robot navigation, yet existing approaches face significant challenges in maintaining persistent scene understanding and enabling efficient real-time deployment.\nCurrent methods either rely on sequential processing that loses temporal consistency [3, 4, 5] or employ direct VLM inference that lacks structured reasoning capabilities [6, 7].\nWe introduce VL-KnG (Vision-Language Knowledge Graph), a novel approach that addresses these limitations through spatiotemporal knowledge graph construction and efficient query processing for visual scene understanding.\nOur key insight is that persistent, structured representations provide complementary advantages to direct VLM inference, particularly in explainability [8], computational efficiency, and adaptability across different tasks.\nVL-KnG processes video sequences in chunks using modern vision-language models [6, 7, 9, 10, 11, 12], constructing a spatiotemporal knowledge graph that maintains object identity across time while capturing relationships between entities.\nThe system employs a GraphRAG-based query processing pipeline [13] that enables efficient subgraph retrieval and reasoning, providing both accurate goal localization and explainable decision-making [8] for navigation applications.\n\nFor objective evaluation of the proposed method and the baselines, we introduce a new benchmark, WalkieKnowledge, aiming to close the gap in evaluation of the related methods. Our WalkieKnowledge benchmark enables four unique query types that encompass a range of real-world navigation situations.\nObject search queries help robots identify particular objects within their environment.\nScene description queries reveal attribute details about objects and environments.\nAction place queries identify locations suitable for the execution of particular actions.\nSpatial relationship queries indicate the relative positioning of objects for navigation planning.\nOur evaluation benchmark offers comprehensive assessment via different query types, allowing different aspects of the approaches to be evaluated.\nOur contributions include:\n\nA semantic-based object association mechanism that maintains unique object identity across temporal sequences.\n\nA comprehensive object descriptor system that captures rich semantic information including color, material, size, affordances, and spatial relationships for enhanced scene understanding.\n\nA spatiotemporal knowledge graph system that enables persistent scene representation and queryable spatial reasoning for navigation applications.\n\nWalkieKnowledge, a new evaluation benchmark with manually annotated trajectories enabling fair comparison between structured approaches and general-purpose VLMs.\n\nReal-world validation demonstrating practical applicability for navigation goal identification.\n\n1. A semantic-based object association mechanism that maintains unique object identity across temporal sequences.\n\n2. A comprehensive object descriptor system that captures rich semantic information including color, material, size, affordances, and spatial relationships for enhanced scene understanding.\n\n3. A spatiotemporal knowledge graph system that enables persistent scene representation and queryable spatial reasoning for navigation applications.\n\n4. WalkieKnowledge, a new evaluation benchmark with manually annotated trajectories enabling fair comparison between structured approaches and general-purpose VLMs.\n\n5. Real-world validation demonstrating practical applicability for navigation goal identification.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有视觉语言模型在机器人导航中缺乏持久场景记忆。  \n2. 现有方法在处理视频时缺乏有效的空间推理能力。  \n3. 需要实现实时应用的高效计算能力。  \n\n【提出了什么创新的方法】  \n提出了VL-KnG系统，通过构建时空知识图和高效的查询处理，解决了机器人导航中的场景理解问题。该方法将视频序列分块处理，创建持久的知识图，维护对象的时间一致性，并通过可查询的图结构实现可解释的空间推理。通过在差分驱动机器人上的实际部署，验证了该方法的实用性，成功率达到77.27%，回答准确率为76.92%，与Gemini 2.5 Pro的性能相当，同时提供了知识图支持的可解释推理和实时部署的计算效率。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Touching the tumor boundary: A pilot study on ultrasound based virtual fixtures for breast-conserving surgery",
            "authors": "Laura Connolly,Tamas Ungi,Adnan Munawar,Anton Deguet,Chris Yeung,Russell H. Taylor,Parvin Mousavi,Gabor Fichtinger Keyvan Hashtrudi-Zaad",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01452",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01452",
            "arxiv_html_link": "https://arxiv.org/html/2510.01452v1",
            "abstract": "This is an un-refereed author version of an article.\nThe final refereed version is available here:\nhttps://link.springer.com/article/10.1007/s11548-025-03342-z",
            "introduction": "Approximately 7 in 10 patients diagnosed with early-stage breast cancer will undergo breast-conserving surgery (BCS) combined with radiation therapy to treat their disease [1]. During BCS, the surgeon will resect the tumor and a small portion of the surrounding healthy tissue in every direction. To determine if the resection was adequate, a pathologist will analyze the excised specimen and look at the distance between the edge of the specimen and the tumor. If this margin is considered positive (< 1 or 2 mm), it is implied that the tumor was transected during surgery and residual tumor tissue may be left behind [2]. In these cases, the patient must undergo immediate reoperation, which can lead to additional complications, impaired cosmesis, increased healthcare costs, and psychological distress [3]. Currently, between 10-40% of patients who undergo BCS experience positive margins [4][5]. This high failure rate is associated with intraoperative challenges such as unclear tumor boundaries and high tissue mobility. Tumor localization is achieved through palpation, radiological methods such as ultrasound, or wire localization and sketches [6]. However, each of these methods suffer from various shortcomings as breast tumors can be non-palpable, accessing a radiologist during a surgery is costly and impractical, and interpreting a localization wire sketch is difficult.\n\nTo address these limitations, a navigation system called LumpNav has been previously introduced which uses electromagnetic (EM) navigation and ultrasound to help localize the tumor boundaries in real-time [7, 8]. LumpNav provides surgeons with a visual display that shows the position of their resection tool relative to the tumor during surgery and is currently undergoing clinical trials. Although the LumpNav system has demonstrated its efficacy in reducing excised tissue volume and avoiding positive margins, it remains dependent on the visual interpretation of navigation data which can be challenging and mentally demanding for the surgeon. In other anatomical areas, this challenge has been overcome with virtual fixture (VF) guidance, sometimes referred to as active constraints.\n\nFixtures or constraints are typically used to enforce a desired behaviour through haptic feedback. In the context of robot-assisted surgery or medical procedures, this desired behaviour can include avoiding delicate or critical anatomy [9], performing a task more efficiently [10], trainee skill development [11], and preventing undesired forces [12]. Forbidden region VFs in particular can be used to prevent the operator from breaching an undesired region [13]. In the past 20 years, there have been several advancements in robot-assisted surgery and medical interventions, and virtual fixtures have remained a core component of emerging research and commercial medical robotic systems. For example, the Mako robot (Stryker, USA) uses a forbidden region VF to limit resection depths and preserve soft tissue during bone sawing for total knee arthroplasty [14]. Similarly, the ROSA One robot (Zimmer Biomet, USA) uses forbidden region VFs for trajectory assistance in neurosurgical applications [15]. The use of robotics in breast surgery, in general, is also being actively investigated at various clinical centers for mastectomy because of its potential positive impact on minimally-invasive surgery [16, 17]. Despite these commercial successes and the increasing interest in robotic-assistance from breast surgeons, the application of VF guidance in BCS remains under explored. The potential benefit of incorporating VF guidance to prevent tumor breach in BCS warrants careful consideration. However, changing the standard of care so significantly, such that the surgeon will cooperatively manipulate their resection tool with a robot, requires thorough evaluation. This is essential to ensure that such changes do not disrupt established surgical practices and outcomes. We hypothesize that while the integration of VF guidance could be advantageous in BCS, its implementation must be thoughtfully considered to balance added value and practicality.\n\nTherefore, in this work, we demonstrate a forbidden region VF guidance system for BCS that relies on ultrasound and EM tracking. While assessing the potential benefits of this system, we also explore the impact on surgical outcome, the acceptance among users, and the potential indirect effects on surgical workflow. To investigate the utility of this guidance system, we conduct a study where users perform a simulated BCS procedure with and without VF guidance. We demonstrate that haptic feedback reduces positive margins and alleviates mental load; at the same time it may increase resection time and margin width. The insights gained from this pilot study will inform the necessary design adjustments, training protocols, and operational parameters for future implementations of haptic guidance in BCS. Our contributions include: 1) the development of an open-source VF guidance system for soft-tissue tumor resections, 2) a new technique for imposing haptic constraints based on real-time EM tracking and ultrasound, and 3) a comprehensive analysis of user interaction and with cooperative robotic guidance in this surgical scenario.",
            "llm_summary": "【论文的motivation是什么】  \n1. 高达40%的乳腺保留手术患者经历阳性切缘，导致需重新手术。  \n2. 现有的肿瘤定位方法存在局限性，影响手术效果。  \n3. 需要有效的虚拟夹具指导以改善手术精度和减少心理负担。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于超声和电磁跟踪的禁区虚拟夹具（VF）指导系统，旨在改善乳腺保留手术（BCS）的效果。通过模拟手术，研究表明该系统能够通过触觉反馈减少阳性切缘的发生，同时减轻外科医生的心理负担。尽管使用VF指导可能导致切除时间和切缘宽度的增加，但其潜在的益处和用户接受度为未来的手术实践提供了重要的设计和培训参考。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Differentiable Skill Optimisation for Powder Manipulation in Laboratory Automation",
            "authors": "Minglun Wei,Xintong Yang,Yu-Kun Lai,Ze Ji",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01438",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01438",
            "arxiv_html_link": "https://arxiv.org/html/2510.01438v1",
            "abstract": "Robotic automation is accelerating scientific discovery by reducing manual effort in laboratory workflows. However, precise manipulation of powders remains challenging, particularly in tasks such as transport that demand accuracy and stability. We propose a trajectory optimisation framework for powder transport in laboratory settings, which integrates differentiable physics simulation for accurate modelling of granular dynamics, low-dimensional skill-space parameterisation to reduce optimisation complexity, and a curriculum-based strategy that progressively refines task competence over long horizons. This formulation enables end-to-end optimisation of contact-rich robot trajectories while maintaining stability and convergence efficiency. Experimental results demonstrate that the proposed method achieves superior task success rates and stability compared to the reinforcement learning baseline.",
            "introduction": "Robotic automation is accelerating scientific discovery by streamlining workflows in areas such as photocatalysis [1] and synthetic chemistry [2]. These systems reduce manual workload and allow scientists to focus on higher-level reasoning and design [3]. However, while macroscale processes have seen rapid automation, challenges persist at the microscale, particularly in the precise handling of powders. Powders play a critical role in pharmaceuticals and materials science, where precise transport is essential for reproducibility and system stability. Despite some efforts in weighing [4, 3] and grinding [5, 6], powder transport is often treated as secondary, and its reliable optimisation remains underexplored.\n\nThis research gap is not incidental. The dynamic behavior of powders during motion and interaction is highly nonlinear and sensitive to environmental variability, making it difficult to address with conventional learning or control techniques.\nRecent advances in differentiable physics and high-performance parallel computation [7, 8] present a promising direction for addressing this challenge. By optimising within high-fidelity, real-world-consistent differentiable simulation environments, it becomes possible to obtain precise and reliable powder transport trajectories in a safe manner. While prior studies have applied such methods to the manipulation of materials like elastoplastic solids [9, 10, 11] and fluids [12, 13], no existing work has systematically addressed the problem of powder transport in laboratory settings using skill or trajectory optimisation.\n\nTo address this gap, we propose a trajectory optimisation framework for powder transport in a laboratory setting. Specifically, we build a differentiable physics simulator using Taichi to model powder dynamics and define differentiable skill parameters mapped to control inputs. To tackle the challenges of long-horizon manipulation, we further adopt a curriculum optimisation strategy that first focuses on scooping-related parameters before optimising the full parameter set. These parameters are optimised via gradient backpropagation through a task-specific loss. Comparative experiments with standard reinforcement learning (RL) methods demonstrate the effectiveness and efficiency of our approach.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人自动化在科学发现中加速了实验室工作流程的效率。  \n2. 粉末的精确操控在药物和材料科学中至关重要，但仍面临挑战。  \n3. 传统学习或控制技术难以应对粉末动态行为的非线性和环境变异性。  \n\n【提出了什么创新的方法】  \n提出了一种用于实验室粉末运输的轨迹优化框架，结合了可微分物理模拟、低维技能空间参数化和基于课程的策略。该方法通过高保真、与现实一致的可微分模拟环境进行优化，能够获得精确且可靠的粉末运输轨迹。实验结果表明，该方法在任务成功率和稳定性方面优于强化学习基线。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation",
            "authors": "Anukriti Singh,Kasra Torshizi,Khuzema Habib,Kelin Yu,Ruohan Gao,Pratap Tokekar",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01433",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01433",
            "arxiv_html_link": "https://arxiv.org/html/2510.01433v1",
            "abstract": "Vision-based robot learning often relies on dense image or point-cloud inputs, which are computationally heavy and entangle irrelevant background features. Existing keypoint-based approaches can focus on manipulation-centric features and be lightweight, but either depend on manual heuristics or task-coupled selection, limiting scalability and semantic understanding. To address this, we propose Afford2Act, an affordance-guided framework that distills a minimal set of semantic 2D keypoints from a text prompt and a single image. Afford2Act follows a three-stage pipeline: affordance filtering, category-level keypoint construction, and transformer-based policy learning with embedded gating to reason about the most relevant keypoints, yielding a compact 38-dimensional state policy that can be trained in 15 minutes, which performs well in real-time without proprioception or dense representations. Across diverse real-world manipulation tasks, Afford2Act consistently improves data efficiency, achieving an 82% success rate on unseen objects, novel categories, backgrounds, and distractors. Visualizations available at: https://afford2act.github.io/",
            "introduction": "Learning effective robot manipulation policies hinges on overcoming a fundamental representation challenge: how to extract the manipulation-related features which are both compact and expressive? Dense visual inputs such as images [1], pointclouds [2], or semantic dense features [3, 4] provide rich details, but burden a policy with extraneous information. In contrast, 2D keypoint [5] offers a sparse state representation that highlights only the essential features of objects’ motions while still being representative enough to learn a good policy. This sparsity directly enables a lightweight policy with far fewer but important features than dense representation and support effective and real-time inference with less computational demand. However, the real challenge is in discovering these keypoints automatically and reliably: picking too few or irrelevant points can omit critical information, while relying on human insight for what points to use does not scale. In this paper, we present a method for identifying task-relevant object points that are both minimal (compact) and sufficient (expressive) for effective robot manipulation.\n\nThe problem of identifying keypoints has received a lot of attention recently. Some previous work utilizes manual selection [6] and\nprompts large Vision-Language Models (VLMs) to mark affordances on images [7, 3]. While these methods can provide useful cues, they depend on human guidance or external knowledge, making them labor-intensive and domain-specific. At the other end of the spectrum, some approaches forgo sparsity altogether in favor of dense feature grid-based representations. For instance, all previous work [8, 9, 10, 11] learn from dense grid of points in the scene or the object. While dense features are expressive, they are not compact and incur heavy computational demands. Furthermore, dense features may also entangle irrelevant dynamics (such as background features), which limits scalability and focus.\n\nRecently, there has been work on automatically selecting a subset of keypoints from a dense grid [12, 13, 14] in an end-to-end manner. In particular, ATK [12] jointly optimizes a masking model for selecting a compact keypoint set and a task-specific control policy, yielding strong performance on challenging visuomotor tasks. However, there are two key limitations in ATK that prohibit it from generalizing: (i) Joint training couples the masking model to the task, so extending to a new task requires retraining both components (e.g., moving from mug picking from left to right); (ii) Existing evaluations emphasize robustness to positional variation and visual perturbations, whereas other generalization regimes, duplicate objects and cross-category transfer (e.g., from mugs to teapots), have received limited attention. In summary, prior works either require significant prior knowledge (manual keypoints), incur high computational overhead (dense grids), or struggle with stability and semantic generalization. This state of affairs motivates a core question for robot learning: How can a robot learn to identify the most relevant interaction points, the task hotspots, directly from visual inputs?\n\nTo address this challenge, we propose Afford2Act: Affordance-Guided Keypoints to Act, which uses text-prompted affordance cues to automatically select task-relevant keypoints, with no manual labels. This pipeline contains three stages: (i) Affordance Filtering (e.g., “hold” →\\rightarrow mug handle), (ii) Keypoint Pool Construction from a small set of masked DINO anchors, and (iii) Keypoint-Based Policy Learning, where a transformer-based encoder followed by a gating mechanism learns the importance of each keypoint. Because mask support and geometry vary, the number of selected keypoints per object also varies; thus, correspondence is defined as a flexible set of affordance-region points. As visualized in Fig. 1, the policy typically attends to a small effective subset (Fig. 11) of keypoints. With a 38-dimensional input, policies remain robust under open-vocabulary prompts and scene changes across six real-world tasks.\nEvaluating across six real-world tasks, Afford2Act consistently outperforms keypoint-selection baselines and RGB/RGB-D policies, and stays robust and generalizable to synonym prompts, lighting, distractors, and clutter (Fig. 1).\n\nContributions. We present:\n\nA unified Affordance-Guided Keypoint Distillation framework that extracts sparse, semantically meaningful 2D keypoints directly from text prompts.\n\nAn efficient, attention-based framework that trains a compact keypoint-guided policy in ∼\\sim15 minutes.\n\nExtensive real-robot experiments in six tasks, demonstrating superior 82% success in unseen instances and robustness to open-vocabulary prompts (Fig. 1).",
            "llm_summary": "【论文的motivation是什么】  \n1. 如何从视觉输入中自动识别与操作相关的关键点以提高机器人学习的有效性。  \n2. 现有方法依赖人工选择或密集特征，导致计算负担重且缺乏可扩展性。  \n3. 需要一种既紧凑又具表现力的特征表示，以支持实时推理和有效的策略学习。  \n\n【提出了什么创新的方法】  \n提出了Afford2Act，一个基于可供性引导的框架，通过文本提示和单幅图像自动选择任务相关的2D关键点。该方法包含三个阶段：可供性过滤、类别级关键点构建和基于关键点的策略学习。通过嵌入式门控机制，模型能够有效识别最相关的关键点，最终生成一个38维的状态策略，能够在15分钟内训练完成。实验表明，Afford2Act在六个真实世界任务中表现出色，成功率达到82%，并在面对新物体、类别和背景时保持鲁棒性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "How Well do Diffusion Policies Learn Kinematic Constraint Manifolds?",
            "authors": "Lexi Foland,Thomas Cohn,Adam Wei,Nicholas Pfaff,Boyuan Chen,Russ Tedrake",
            "subjects": "Robotics (cs.RO)",
            "comment": "Under review. . Additional results available atthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.01404",
            "code": "https://diffusion-learns-kinematic.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01404",
            "arxiv_html_link": "https://arxiv.org/html/2510.01404v1",
            "abstract": "Diffusion policies have shown impressive results in robot imitation learning, even for tasks that require satisfaction of kinematic equality constraints.\nHowever, task performance alone is not a reliable indicator of the policy’s ability to precisely learn constraints in the training data.\nTo investigate, we analyze how well diffusion policies discover these manifolds with a case study on a bimanual pick-and-place task that encourages fulfillment of a kinematic constraint for success.\nWe study how three factors affect trained policies: dataset size, dataset quality, and manifold curvature.\nOur experiments show diffusion policies learn a coarse approximation of the constraint manifold with learning affected negatively by decreases in both dataset size and quality. On the other hand, the curvature of the constraint manifold showed inconclusive correlations with both constraint satisfaction and task success. A hardware evaluation verifies the applicability of our results in the real world.\nProject website with additional results and visuals: https://diffusion-learns-kinematic.github.io/.",
            "introduction": "Advances in generative modeling have laid the foundation for several breakthroughs in robot imitation learning. For example, diffusion and flow-based policies have shown that dexterous and even kinematically constrained manipulation tasks can be learned from just a few hours of robot data [chi2023diffusion, black2025pi05, trilbmteam2025carefulexaminationlargebehavior].\nA central idea in generative modeling is the manifold hypothesis, which posits that high-dimensional real-world datasets are concentrated along lower-dimensional manifolds in the higher-dimensional space [bengio2013representationlearning, fefferman2013testingmanifoldhypothesis].\nIf one accepts this hypothesis, then diffusion is approximately equivalent to Euclidean projections onto the data manifold [permenter2024interpretingimprovingdiffusionmodels]. By extension, Diffusion Policy [chi2023diffusion] can be interpreted as a method for sampling from the manifold of robot trajectories.\n\nThe data manifold in robotics is challenging to characterize; however, robot trajectories often exhibit well-understood structures. For instance, robot motion frequently operates along a submanifold of configuration space defined by kinematic constraints. While these constraints are implicitly present in datasets, they are rarely encoded explicitly during training. Thus, there is no guarantee that the trained policies will respect these constraints, even when they may be critical for task completion. Furthermore, policies are often deployed in conjunction with constraint-enforcing or compliant low-level controllers that mitigate constraint violations in the policy’s action predictions. Thus, there is value in quantifying the extent to which diffusion policies adhere to the constraint manifold in robot data (independent of the lower-level control stack), as well as carefully studying the factors that affect the quality of this adherence.\n\nIn this paper, we study how well diffusion policies learn the kinematic constraint manifolds in robot data. We consider a bimanual pick-and-place problem where a robot must move objects from a table into a shelf while avoiding collisions with the environment. Maintaining a constant relative pose between the grippers is essential to avoid dropping the object during transport. This requirement is a nonlinear kinematic constraint in the robot’s configuration space. We collect data for this task using a specialized teleoperation setup that allows the teleoperator to lock the relative transform between the end-effectors during object transport. This ensures all training data satisfies the constraint and also provides a more intuitive and forgiving user experience. Fig. 1 contains images of the simulation and hardware setups as well as a broad overview of our method.\n\nWhile there are a variety of factors that affect policy performance, we isolate and study the policy’s adherence to kinematic constraints, independent of confounding factors such as choice of low-level controller, gripper compliance, and contact forces.\nMore concretely, we study the effect of\n\nData amount: by training policies with varying numbers of demonstrations.\n\nData quality: by artificially introducing constraint violations into the training data. Although the demonstrations still succeed at the task, the more the policy learns that it can violate the constraints, the more likely it is to drop the box.\n\nConstraint difficulty: by measuring the curvature of the kinematic constraint manifold.\n\nWe coarsely evaluate policies on an outcome basis (success, partial success, partial failure, and full failure).\nImportantly, we also measure the policy’s ability to learn the constraints by evaluating the action predictions.\nConsidering the predictions instead of the measured positions isolates the policy’s constraint satisfaction from confounding factors, such as the low-level controller and contact forces. Lastly, we consider correlations between constraint manifold curvature, task success, and constraint satisfaction. We conclude by conducting experiments on hardware to verify that our methods and experiments translate to the real world.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人模仿学习中的扩散策略在满足运动学约束任务上表现出色。  \n2. 任务性能并不能可靠地指示策略对训练数据中约束的学习能力。  \n3. 需要量化扩散策略在机器人数据中遵循约束流形的程度。  \n\n【提出了什么创新的方法】  \n本研究通过分析扩散策略在双手抓取和放置任务中学习运动学约束流形的能力，探讨了数据集大小、质量和流形曲率对策略性能的影响。我们收集了满足约束的训练数据，并评估了策略的约束满足能力。实验结果表明，扩散策略能够学习约束流形的粗略近似，但数据集的大小和质量对学习效果有负面影响。流形曲率与约束满足和任务成功之间的相关性不明确。通过硬件评估验证了结果在现实世界中的适用性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Beyond Collision Cones: Dynamic Obstacle Avoidance for Nonholonomic Robots via Dynamic Parabolic Control Barrier Functions",
            "authors": "Hun Kuk Park,Taekyung Kim,Dimitra Panagou",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "The first two authors contributed equally to this work. Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.01402",
            "code": "https://www.taekyung.me/dpcbf",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01402",
            "arxiv_html_link": "https://arxiv.org/html/2510.01402v1",
            "abstract": "Control Barrier Functions (CBFs) are a powerful tool for ensuring the safety of autonomous systems, yet applying them to nonholonomic robots in cluttered, dynamic environments remains an open challenge. State-of-the-art methods often rely on collision-cone or velocity-obstacle constraints which, by only considering the angle of the relative velocity, are inherently conservative and can render the CBF-based quadratic program infeasible, particularly in dense scenarios. To address this issue, we propose a Dynamic Parabolic Control Barrier Function (DPCBF) that defines the safe set using a parabolic boundary. The parabola’s vertex and curvature dynamically adapt based on both the distance to an obstacle and the magnitude of the relative velocity, creating a less restrictive safety constraint. We prove that the proposed DPCBF is valid for a kinematic bicycle model subject to input constraints. Extensive comparative simulations demonstrate that our DPCBF-based controller significantly enhances navigation success rates and QP feasibility compared to baseline methods. Our approach successfully navigates through dense environments with up to 100 dynamic obstacles, scenarios where collision cone-based methods fail due to infeasibility. [Project Page]111Project page: https://www.taekyung.me/dpcbf [Code] [Video]",
            "introduction": "Ensuring safety is a fundamental challenge for autonomous systems, particularly nonholonomic robots and autonomous vehicles operating in dynamic and cluttered environments. Control Barrier Functions (CBFs) have emerged as a powerful tool for enforcing safety constraints in real-time, formulated within a Quadratic Program (QP) [1] or with Model Predictive Control (MPC) [2]. Their effectiveness has led to widespread adoption in applications from robotic navigation [3] to multi-agent coordination [4].\n\nCollision avoidance can be encoded through a distance-based CBF, which defines the safe set based on the Euclidean distance to an obstacle. To incorporate the relative velocity between the robot and the obstacle, one can employ a High-Order CBF (HOCBF) [5]. However, it requires all control inputs to appear in the CBF condition, which makes it difficult to be applied to systems with inputs of different relative degrees [6].\n\nRecent work addresses dynamic obstacles within the CBF framework by leveraging velocity-obstacle (VO) constraints [7], also referred to as collision cones in other literature [8]. These methods define the unsafe set as a collision cone in the relative-velocity space and constrain the relative velocity to lie outside a fixed cone [9, 10]. This approach has been successfully applied to various systems, including the kinematic bicycle model, by showing that the constraint has relative degree one with respect to all control inputs. Despite their advantages for dynamic obstacle avoidance, cone-based and VO-based methods exhibit fundamental conservatism. Because the safety constraint depends only on the heading angle of the relative velocity, the robot is prohibited from moving toward the obstacle, regardless of their distance or relative speed. This rigidity can induce immediate QP infeasibility when the initial relative velocity lies within a collision cone, or in dense environments where the union of multiple cones removes all feasible control inputs, even when sufficient collision-free space exists (see Fig. 1a).\n\nThis paper introduces a Dynamic Parabolic Control Barrier Function (DPCBF) that explicitly incorporates both clearance and the magnitude of the relative velocity. Instead of a fixed cone, we define a state-dependent parabolic safety boundary whose curvature and vertex adapt with distance and relative velocity (see Fig. 1b). This design allows a less conservative safety constraints, improving the CBF-based controller’s feasibility in cluttered, dynamic environments. The main contributions of this work are:\n\nWe propose a DPCBF for nonholonomic robots in dynamic obstacle avoidance tasks, which dynamically shapes the safety boundary to provide less conservative safety margins by adapting to distance and relative velocity.\n\nWe propose a DPCBF for nonholonomic robots in dynamic obstacle avoidance tasks, which dynamically shapes the safety boundary to provide less conservative safety margins by adapting to distance and relative velocity.\n\nWe prove that DPCBF is valid for the kinematic bicycle model under input constraints.\n\nWe show extensive simulation results in dense, dynamic environments, demonstrating higher feasibility and success rates, and lower control intervention, compared to state-of-the-art CBF methods.\n\n1. We propose a DPCBF for nonholonomic robots in dynamic obstacle avoidance tasks, which dynamically shapes the safety boundary to provide less conservative safety margins by adapting to distance and relative velocity.\n\n2. We prove that DPCBF is valid for the kinematic bicycle model under input constraints.\n\n3. We show extensive simulation results in dense, dynamic environments, demonstrating higher feasibility and success rates, and lower control intervention, compared to state-of-the-art CBF methods.",
            "llm_summary": "【论文的motivation是什么】  \n1. 非holonomic机器人在动态环境中的安全性保障仍然是一个开放挑战。  \n2. 现有的碰撞锥和速度障碍方法过于保守，导致在密集场景中控制问题不可行。  \n\n【提出了什么创新的方法】  \n本文提出了一种动态抛物线控制障碍函数（DPCBF），通过动态调整安全边界的顶点和曲率，基于障碍物的距离和相对速度来定义安全集，从而提供更少的保守性安全约束。DPCBF在运动学自行车模型下被证明是有效的。通过大量比较仿真，DPCBF控制器在导航成功率和QP可行性方面显著优于基线方法，能够成功穿越高达100个动态障碍物的密集环境，而碰撞锥方法在此类场景中常常失效。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge",
            "authors": "Michal Werner,David Čapek,Tomáš Musil,Ondřej Franěk,Tomáš Báča,Martin Saska",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01348",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01348",
            "arxiv_html_link": "https://arxiv.org/html/2510.01348v1",
            "abstract": "Reliable long-range flight of unmanned aerial vehicles (UAVs) in GNSS-denied environments is challenging: integrating odometry leads to drift, loop closures are unavailable in previously unseen areas and embedded platforms provide limited computational power.\nWe present a fully onboard UAV system developed for the SPRIN-D Funke Fully Autonomous Flight Challenge, which required 9 km long-range waypoint navigation below 25 m AGL (Above Ground Level) without GNSS or prior dense mapping.\nThe system integrates perception, mapping, planning, and control with a lightweight drift-correction method that matches LiDAR-derived local heightmaps to a prior geodata heightmap via gradient-template matching and fuses the evidence with odometry in a clustered particle filter.\nDeployed during the competition, the system executed kilometer-scale flights across urban, forest, and open-field terrain and reduced drift substantially relative to raw odometry, while running in real time on CPU-only hardware.\nWe describe the system architecture, the localization pipeline, and the competition evaluation, and we report practical insights from field deployment that inform the design of GNSS-denied UAV autonomy.\nSUPLEMENTARY MATERIALS:\nhttps://gnssdenied.github.io/",
            "introduction": "Unmanned Aerial Vehicles are increasingly deployed for infrastructure inspection, logistics, and search-and-rescue. Many of these missions require long-range, low-altitude flight in areas where Global Navigation Satellite Systems are unreliable, unavailable, or denied.\nWithout GNSS, UAVs typically rely on visual–inertial or LiDAR odometry to navigate a previously unseen environment.\nThese methods are consistent locally, but accumulate unbounded drift in previously unvisited areas, where loop closures cannot be used.\nWhen flying over kilometer-scale trajectories, this leads to position errors too large for waypoint-based navigation.\nThe core problem addressed in our paper is therefore how to manage the drift using available geodata on a low-altitude resource-constrained UAV.\n\nExisting methods address parts of the problem – deep-learning geolocalization achieves high accuracy but is too computationally heavy to run in real-time on embedded platforms, while odometry-only systems based on  Visual-Inertial Odometry (VIO) or  Lidar-Inertial Odometry (LIO) are efficient but accumulate excessive drift.\nAt high altitudes, UAVs can use satellite data to correct their positions, but this becomes unfeasible close to the ground or under trees.\nFully onboard systems that scale to kilometer ranges in diverse unseen environments remain rare, since outdoor geodata-based drift correction that is both lightweight and robust has not been solved.\nAddressing these challenges requires not only an individual algorithm but the integration of perception, localization, planning, and control into a reliable onboard system.\n\nWe study this problem in the context of the SPRIN-D Funke Fully Autonomous Flight Challenge, which provided a stringent test of GNSS-denied autonomy.\nIn response, we developed a UAV system that integrates onboard mapping, planning, and mission execution with a lightweight geodata-based localization method for drift correction.\nOur method’s key idea is to exploit heightmap gradients as a compact and robust structural signature—simple enough for real-time onboard use, yet distinctive enough to correct drift over kilometer-scale trajectories.\nThe system was deployed during the competition, where it successfully executed kilometer-scale flights in mixed environments and achieved substantial drift reduction compared to raw odometry.\n\nThe SPRIN-D Challenge required an autonomous UAV with total mass below 25 kg25\\text{\\,}\\mathrm{kg} to fly a prescribed 9 km9\\text{\\,}\\mathrm{km} long sequence of waypoints, each waypoint marked by a red flag on a 1 m1\\text{\\,}\\mathrm{m} pole, without relying on any GNSS-based localization. The waypoint positions were provided on a printed map with an uncertainty of approximately 20 m20\\text{\\,}\\mathrm{m}. Throughout the mission, the UAV had to remain below 25 m25\\text{\\,}\\mathrm{m} Above Ground Level (AGL) and autonomously avoid obstacles such as trees, buildings, or water curtains sprayed from a fire truck.\n\nThe competition environment combined urban areas, forests, and open fields. Although the area was announced beforehand, reconnaissance flights or custom dense mapping were explicitly prohibited, precluding the use of pre-recorded maps or ground-truth data. Together, these requirements defined not only a localization problem but a full autonomy challenge, demanding a UAV system capable of reliable long-range navigation and mission execution using prior data of the deployment area, and onboard sensing with constrained computation.\n\nLocalization without GNSS is a widely studied problem in robotics, but remains challenging for long-range UAV missions.\nOdometry methods such as VIO and LIO provide local motion estimates, yet accumulate drift that is usually corrected through loop closures.\nIn real-world missions, e.g. search and rescue, UAVs operate in previously unseen environments where revisits are rare and loop closures cannot be relied upon.\nWe tested state-of-the-art visual  Simultaneous Localization And Mapping (SLAM) systems such as RTAB-Map [1] and ORB-SLAM3 [2] in a high-fidelity simulator [3], and found that both performance degraded significantly in a long-range scenario (beyond  1 km1\\text{\\,}\\mathrm{km}), as their memory and compute demands grow with the size of the environment.\nMoreover, RTAB-Map was unable to maintain quality odometry in faster flight speeds (beyond 2 m s−12\\text{\\,}\\mathrm{m}\\text{\\,}{\\mathrm{s}}^{-1}), while ORB-SLAM3 suffered from tracking loss in textureless areas.\nThis confirmed that conventional SLAM cannot serve as the basis for our system and motivates approaches that combine odometry with external geodata to maintain bounded error.\n\nSeveral large-scale autonomy efforts, most notably the DARPA Subterranean Challenge [4], demonstrated advanced GNSS-denied navigation capabilities.\nSubT is the closest system-level reference, but its setting was fundamentally different: multi-robot teams operating primarily in underground environments, often supported by larger platforms and heterogeneous sensing.\nIn contrast, the SPRIN-D Challenge focused on UAVs to operate autonomously in large outdoor environments at altitudes constrained below 25 m25\\text{\\,}\\mathrm{m}, without any prior mapping and without the support of other robots.\nFurthermore, the use of commercial UAV platforms was prohibited, requiring the development of a custom platform.\n\nThe absence of loop closures during the mission naturally leads to the problem of geolocalization within prior geodata.\nExisting geolocalization methods can be broadly divided into camera-based, LiDAR-based, and semantic approaches.\nCamera-based methods often align aerial or UAV imagery with satellite maps.\nHigh-altitude matching (such as [5, 6]) works reasonably well, but at low altitudes (25 m25\\text{\\,}\\mathrm{m}) the viewpoint differs drastically, making roofs, facades, and vegetation inconsistent with satellite imagery.\nA range of works [7, 8, 9, 10, 11, 12] consider low-altitude or ground-based localization.\n[7]\ncombine a 3D lidar with camera data and train an end-to-end matching model for localizing a grounded agent with a forward-facing camera.\n[8, 9] train Siamese networks to match ground and satellite image embeddings.\nThe authors of [10] train a cross-view-matching network for satellite-ground localization and combine the matches with odometry using a particle filter [11].\nThe method [12] use panoramic ground imagery warped to bird’s-eye view.\nWhile effective in their respective domains and on certain datasets, these methods assume ground agents or structured viewpoints and are not directly applicable to small UAVs without onboard GPU support deployed in cluttered outdoor mixed environments.\n\nLiDAR-based methods use structural cues to improve robustness. Examples include learned place recognition fused with odometry [13], tree segmentation for forested areas matched with prior aerial scan of the area [14], or heightmap matching [15]. However, these are typically restricted to small scales or specific environments, and are not easily transferable to urban–forest missions. Semantic approaches aim to mitigate appearance variability by extracting higher-level categories. Roads [16], sematic maps [17], or canopy structures [14] have been exploited as robust cues, often fused with odometry inside a particle filter. These methods offer seasonal robustness, but are limited in scope and domain.\n\nOur work follows the general paradigm of combining a similarity estimation front-end with odometry in a particle filter [11, 18], but adapts it to the unique constraints of the task we address: scalability for kilometers-scale flights, GNSS-denied operation, mixed urban–forest domains, and the requirement for reliable out-of-the-box functionality on test day. Unlike previous methods, we designed a new similarity estimation approach based on tall-object detection, which bridges LiDAR-based and semantic cues.\nTall objects provide a stable and distinctive cue visible at low altitudes in both urban and forest environments, making them particularly suitable for the challenge setting.\nThis choice proved to be computationally efficient, robust across diverse large environments, and suitable for real-world deployment without extensive parameter tuning.\n\nWe present a complete UAV system capable of kilometer-scale GNSS-denied flight below 25 m25\\text{\\,}\\mathrm{m} AGL, operating fully onboard without a dedicated GPU acceleration.\nMoreover, we introduce a novel drift-correction method based on template matching of LiDAR-derived heightmap gradients, fused with odometry in a clustered particle filter.\nFinally, we validated the system in urban, forest, and open-field environments during the SPRIN-D Challenge, where the organizers fixed the conditions of the challenge which led to an objective evaluation of all teams’ solutions.\nAs the only team, we demonstrated kilometer-scale flights in adverse conditions with substantial drift reduction relative to raw odometry, and we report practical lessons learned to guide future UAV design in GNSS-denied settings.",
            "llm_summary": "【论文的motivation是什么】  \n1. 在GNSS-denied环境中，UAV的可靠长距离飞行面临挑战。  \n2. 现有的视觉-惯性或LiDAR里程计方法在未访问区域会累积漂移。  \n3. 需要一种轻量级的、基于地理数据的漂移修正方法以支持低空飞行。  \n\n【提出了什么创新的方法】  \n本文提出了一种完全自主的UAV系统，集成了感知、映射、规划和控制，采用基于LiDAR生成的高度图梯度的漂移修正方法。该方法通过梯度模板匹配将局部高度图与先前的地理数据高度图进行匹配，并在聚类粒子滤波器中融合证据。系统在SPRIN-D挑战中成功执行了公里级飞行，显著减少了相对于原始里程计的漂移，且在CPU-only硬件上实时运行。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Cooperative Guidance for Aerial Defense in Multiagent Systems",
            "authors": "Shivam Bajpai,Abhinav Sinha,Shashi Ranjan Kumar",
            "subjects": "Systems and Control (eess.SY); Multiagent Systems (cs.MA); Robotics (cs.RO); Dynamical Systems (math.DS)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02087",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02087",
            "arxiv_html_link": "https://arxiv.org/html/2510.02087v1",
            "abstract": "This paper addresses a critical aerial defense challenge in contested airspace, involving three autonomous aerial vehicles– a hostile drone (the pursuer), a high-value drone (the evader), and a protective drone (the defender). We present a cooperative guidance framework for the evader-defender team that guarantees interception of the pursuer before it can capture the evader, even under highly dynamic and uncertain engagement conditions. Unlike traditional heuristic, optimal control, or differential game-based methods, we approach the problem within a time-constrained guidance framework, leveraging true proportional navigation based approach that ensures robust and guaranteed solutions to the aerial defense problem. The proposed strategy is computationally lightweight, scalable to a large number of agent configurations, and does not require knowledge of the pursuer’s strategy or control laws. From arbitrary initial geometries, our method guarantees that key engagement errors are driven to zero within a fixed time, leading to a successful mission. Extensive simulations across diverse and adversarial scenarios confirm the effectiveness of the proposed strategy and its relevance for real-time autonomous defense in contested airspace environments.",
            "introduction": "In modern contested airspace, the rapid increase in the presence of fast, agile, and networked unmanned aerial vehicles (UAVs) has intensified the need for reliable aerial defense strategies. Small, inexpensive UAVs can be deployed in large numbers to overwhelm defenses, execute coordinated attacks, or serve as reconnaissance assets to enable subsequent strikes. In such environments, traditional two-body point-defense systems may be inadequate, as engagements often occur at short ranges with limited reaction time. In contrast, cooperative multiagent defense, where defenders actively protect high-value UAVs (evaders) from hostile pursuers, offers a critical layer of protection. This results in a three-body engagement necessitating greater autonomy and strategic decision-making within stricter constraints on engagement duration.\n\nResearch on the kinematics of three-body engagements can be traced to [1], where a closed-form solution was obtained for constant-bearing collision courses, and to [2], which determined the intercept point in the evader-centered reference frame. Subsequent studies have adopted optimal control formulations for three-agent engagements with specific performance objectives, such as minimizing energy or interception cost [3, 4, 5, 6, 7]. For example, the work in [3] integrated a differential game approach into cooperative optimal guidance to maximize pursuer–evader separation, while that in [4] addressed cooperative pursuit–evasion strategies for defender–evader teams with arbitrary-order linearized agent dynamics, assuming the pursuer’s guidance law was known. A multiple-model adaptive estimator enabling cooperative information sharing to predict the pursuer’s likely linear strategy was proposed in [5]. Three-layer cooperation with explicit defender–evader communication was examined in [6], whereas results in [7] provided algebraic capture conditions under which a pursuer could bypass the defender. While these approaches benefit from analytical tractability, their reliance on linearized dynamics can reduce robustness under large initial heading errors or highly nonlinear kinematics.\n\nNonlinear guidance strategies address these shortcomings by avoiding small-error assumptions and explicitly incorporating turn-rate constraints. Representative works include [8, 9, 10, 11, 12]. In [8], a sliding-mode control–based terminal intercept guidance and autopilot design was developed for a defender to shield the evader from an incoming pursuer. A related sliding-mode cooperative defense law was presented in [9], while the work in [10] extended this to multi-defender engagements to enable simultaneous interception of the pursuer before it reaches the evader. In [13], the authors proposed a hybrid cooperative guidance law for defenders that combined inertial delay control, prescribed performance control, and sliding mode control. This fusion offered greater flexibility and performance but relied on an intermediate switching mechanism to transition between the individual guidance schemes.\n\nIn this paper, we develop a cooperative guidance strategy for the evader and the defender that is inherently robust to arbitrary and potentially aggressive maneuvers of the pursuer, eliminating the need to model or predict its specific guidance law. Moreover, our approach includes different levels of cooperation between the evader and the defender based on whether the defender can access the evader’s guidance law.\n\nThe proposed approach is formulated entirely within a nonlinear engagement framework, removing restrictive assumptions on small heading errors or linearized dynamics and thereby broadening applicability to diverse operational conditions. The evader’s and the defender’s control laws are designed to make the relevant error variables vanish within a fixed time, which helps ensure that the defender is able to protect the evader starting from any feasible and arbitrary engagement geometry.\n\nA key novelty is that the proposed guidance strategy for the defender is developed on the true proportional navigation principle. This allows rapid course adjustments and reliable interception even in engagements with fast-changing relative motion. Unlike other variants of the popular proportional navigation guidance, which approximate the guidance command based solely on the line-of-sight rate, the current method is based on computing acceleration directly perpendicular to the instantaneous line-of-sight, utilizing both radial and tangential control components. This grants the defender significantly enhanced agility, precise trajectory shaping, and improved interception capability against maneuvering threats.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现代 contested airspace 中对可靠的 aerial defense 策略的需求增加。  \n2. 传统的两体点防御系统在短距离交战中可能不足以应对快速、灵活的无人机。  \n3. 需要更高的自主性和战略决策能力来保护高价值无人机（evader）。  \n\n【提出了什么创新的方法】  \n本文提出了一种合作引导策略，旨在确保防御者能够在动态和不确定的环境中有效拦截追击者。该方法基于真实的比例导航原则，允许防御者在快速变化的相对运动中进行快速航向调整和可靠拦截。通过非线性交战框架，消除了对小误差或线性化动力学的限制，使得该方法适用于多种操作条件。实验结果表明，该策略在各种对抗场景中表现出色，能够确保防御者在任意初始几何配置下有效保护evader。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale",
            "authors": "Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01665",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01665",
            "arxiv_html_link": "https://arxiv.org/html/2510.01665v1",
            "abstract": "Non-rigid structure-from-motion (NRSfM), a promising technique for addressing the mapping challenges in monocular visual deformable simultaneous localization and mapping (SLAM), has attracted growing attention. We introduce a novel method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing isometric deformations as a subset. Our approach performs point-wise reconstruction using 2D selected image warps optimized through a graph-based framework. Unlike existing methods that rely on strict assumptions, such as locally planar surfaces or locally linear deformations, and fail to recover the conformal scale, our method eliminates these constraints and accurately computes the local conformal scale. Additionally, our framework decouples constraints on depth and conformal scale, which are inseparable in other approaches, enabling more precise depth estimation. To address the sensitivity of the formulated problem, we employ a parallel separable iterative optimization strategy. Furthermore, a self-supervised learning framework, utilizing an encoder-decoder network, is incorporated to generate dense 3D point clouds with texture. Simulation and experimental results using both synthetic and real datasets demonstrate that our method surpasses existing approaches in terms of reconstruction accuracy and robustness. The code for the proposed method will be made publicly available on the project website: https://sites.google.com/view/con-nrsfm.",
            "introduction": "Non-rigid structure-from-motion (NRSfM) addresses the challenge of reconstructing the 3D shapes of deforming objects from multiple calibrated monocular images. It is a fundamental problem in 3D computer vision, with applications ranging from entertainment [1] to modern surgery [2]. The NRSfM algorithm reconstructs 3D shapes in local camera coordinates, inherently intertwining camera motion with object deformations. This concept closely aligns with deformable visual SLAM, which aims to localize a robot and map its environment, even in dynamically deforming scenarios. NRSfM has the potential to play a pivotal role in overcoming the mapping challenges associated with deformable visual SLAM [2]. By integrating tools for robot pose estimation, NRSfM can enhance mapping consistency through information fusion, paving the way for significant advancements in deformable visual SLAM research [3, 4].\n\nNaturally, the NRSfM problem is unsolvable without introducing constraints or assumptions on deformations. Common approaches to deformation modeling include statistical constraints, such as low-rank shapes [5] or trajectory basis [6], and physical constraints, such as isometry (preserving local geodesics) [7], inextensibility [8], and local rigidity [10]. Among these, physical methods generally outperform statistical ones in reconstructing highly deformable objects.\n\nRecent advancements have introduced local formulations of physical constraints, including isometry (distance-preserving)[11], conformality (angle-preserving)[12], and smoothness [13]. These methods assume surfaces to be locally planar (LP) and deformations to be locally linear (LL), expressing physical constraints in terms of local depth derivatives as unknowns. This approach offers several advantages: it is often more accurate, computationally efficient, and robust to missing data. However, there are two major limitations: 1) indirect depth computation: all constraints are derived on local normals (expressed with depth derivatives) and the depth is only obtained by interpolating them. Such a formulation leads to weaker constraints on the surface geometry which affects their performance. 2) LP and LL assumptions: While LP and LL assumptions are valid for continuous surfaces, these methods often operate on sparsely sampled points (100–200 points) on the surface. This sparse sampling compromises the accuracy of local derivative computations, leading to degraded performance, especially in scenarios involving strong deformations, such as surface bending. These limitations emphasize the need for further refinement to improve accuracy and robustness in handling complex deformations.\n\nIn this paper, we extend the existing local formulations of isometry/conformality to overcome these limitations and develop a novel NRSfM method tailored for conformal deformations. The main contributions of this paper are:\n\nRotational invariance under conformal deformation: We establish that connections and moving frames across surfaces preserve distinct invariance properties under different types of deformations. Critically, we prove that connections under conformal deformation preserve rotational invariance, enabling the decoupling of conformal scale and depth estimation. This introduces a novel and strict theoretical constraint on the geometry of deformable surfaces. As a result, we derive physical constraints in terms of conformal scale, depth, and normals (expressed as depth derivatives).\n\nRotational invariance under conformal deformation: We establish that connections and moving frames across surfaces preserve distinct invariance properties under different types of deformations. Critically, we prove that connections under conformal deformation preserve rotational invariance, enabling the decoupling of conformal scale and depth estimation. This introduces a novel and strict theoretical constraint on the geometry of deformable surfaces. As a result, we derive physical constraints in terms of conformal scale, depth, and normals (expressed as depth derivatives).\n\nRelaxation of LL and LP assumptions: Unlike prior approaches, we do not impose LL or LP assumptions. Instead, we define physical constraints that hold up to the second-order derivatives of local depth, introducing additional variables to better align the formulation with real-world scenarios. To solve this, we propose a parallel, separable, iterative optimization algorithm that independently recovers depth and normals (depth derivatives). The algorithm is robust to initialization and offers acceptable computational complexity.\n\n1. Rotational invariance under conformal deformation: We establish that connections and moving frames across surfaces preserve distinct invariance properties under different types of deformations. Critically, we prove that connections under conformal deformation preserve rotational invariance, enabling the decoupling of conformal scale and depth estimation. This introduces a novel and strict theoretical constraint on the geometry of deformable surfaces. As a result, we derive physical constraints in terms of conformal scale, depth, and normals (expressed as depth derivatives).\n\n2. Relaxation of LL and LP assumptions: Unlike prior approaches, we do not impose LL or LP assumptions. Instead, we define physical constraints that hold up to the second-order derivatives of local depth, introducing additional variables to better align the formulation with real-world scenarios. To solve this, we propose a parallel, separable, iterative optimization algorithm that independently recovers depth and normals (depth derivatives). The algorithm is robust to initialization and offers acceptable computational complexity.",
            "llm_summary": "【论文的motivation是什么】  \n1. NRSfM aims to reconstruct 3D shapes of deforming objects from monocular images but faces challenges in accuracy and robustness.  \n2. Existing methods rely on strict assumptions that limit their applicability in real-world scenarios with complex deformations.  \n3. There is a need for improved depth estimation and conformal scale recovery in NRSfM to enhance mapping consistency in deformable visual SLAM.  \n\n【提出了什么创新的方法】  \n本研究提出了一种名为Con-NRSfM的新方法，针对具有可恢复共形尺度的非刚性结构从运动问题。该方法通过基于图的框架优化选择的2D图像变形进行逐点重建，消除了对局部平面表面和局部线性变形的严格假设。通过引入旋转不变性和放宽对深度的约束，提出了一种并行可分离的迭代优化算法，能够独立恢复深度和法线。实验结果表明，该方法在重建精度和鲁棒性方面超越了现有方法，能够有效处理复杂变形场景。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
            "authors": "Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.01623",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01623",
            "arxiv_html_link": "https://arxiv.org/html/2510.01623v1",
            "abstract": "Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work.\nCode: https://github.com/GigaAI-research/VLA-R1.\nWebsite: https://gigaai-research.github.io/VLA-R1.",
            "introduction": "Vision–Language–Action (VLA) models unify perception, language, and action. They first learn open-vocabulary semantics and cross-modal alignment from internet-scale image–text pretraining. These semantics are then grounded into the action space through multi-task manipulation data. This enables analogical transfer to unseen objects and compositional generalization to novel commands. Compared with modular pipelines [6, 41] or state-driven policies [9], VLAs show stronger cross-task and cross-scene generalization [35, 17, 18, 26, 16, 37]. Representative works include VoxPoser [14] for zero-shot trajectory planning, and ManipLVM-R1 [35] and RoboBrain [17] for integrating affordance perception and pose estimation. Meanwhile, Reinforcement Learning from Verifiable Rewards (RLVR) enhances reasoning and generalization in vision–language models. Vision-R1 [15] matches larger models through cold-start data and progressive training; LMM-R1 [30] employs a two-stage regimen from textual reasoning to multimodal tasks; and VLM-R1 [33] applies R1-style reinforcement to visual grounding, boosting open-vocabulary detection.\n\nHowever, existing VLA models present two significant challenges. First, they often lack step-by-step reasoning: models tend to emit final actions directly without explicit inference over affordance constraints, geometric relations, or container selection. This limitation leads to instruction-disambiguation failures under color similarity, duplicate instances, or multiple candidate receptacles.\nSecond, post-training rarely provides systematic reinforcement of reasoning. Current method relies on supervised fine-tuning (SFT) with little reward optimization targeted at reasoning quality and execution efficacy. Even when Reinforcement Learning (RL) is used, reward design is typically single-objective and struggles to jointly optimize region alignment and trajectory consistency, degrading performance on out-of-distribution data and in the real world.\n\nTo address these challenges, we propose VLA-R1, a post-training-enhanced VLA model capable of step-by-step reasoning. Unlike prior approaches, VLA-R1 simultaneously emphasizes data-level Chain-of-Thought (CoT) supervision and optimization-level reward alignment, bridging the gap between reasoning and execution. This enables the model to not only provide answers but also explain them, making it robust to challenges like color similarity, repeated instances, and multiple receptacle choices during reasoning.\n\nTo further enhance the model’s reasoning capabilities, we introduce an RLVR-based post-training strategy at the optimization layer. Specifically, we employ Group Relative Policy Optimization (GRPO) [32] with three verifiable rewards: an affordance reward based on Generalized Intersection over Union (GIoU) [31] to provide informative gradients for non-overlapping predicted and ground truth affordance regions, speeding up learning; a distance-based reward using the improved Fréchet distance to ensure reasonable trajectory curvature and segment length; and an output-format reward to enforce well-formed reasoning and action specifications. These optimizations enable VLA-R1 to generate accurate affordance regions and well-formed execution trajectories, enhancing decision-making.\n\nMoreover, many existing datasets, although large in scale, fail to fully support complex reasoning tasks due to the lack of detailed explanations and reasoning processes in their annotations. To address this, we develop the VLA-CoT data engine, which generates the high-quality VLA-CoT-13K dataset, making reasoning steps explicit. The engine aligns CoT with affordance and trajectory annotations, encouraging the model to learn task-consistent reasoning and enabling it to acquire basic reasoning capabilities during the SFT phase.\n\nFinally, we conduct comprehensive evaluations of VLA-R1 across in-domain, out-of-domain, simulation, and real-robot settings. Empirically, VLA-R1 achieves an IoU of 36.51 on the in-domain affordance benchmark, a 17.78% improvement over the baseline; on the in-domain trajectory benchmark it attains an Average distance of 91.74 (lower is better), reducing the baseline by 17.25%. It also delivers state-of-the-art (SOTA) performance in the out-of-domain setting. On physical hardware, VLA-R1 reaches 62.5% success for affordance perception and 75% for trajectory execution. These results demonstrate the method’s effectiveness under controlled conditions and its robustness and practicality across domains and real-world scenarios.\n\nContributions in our paper can be summarized in the following three folds:\n\nWe propose VLA-R1, a VLA foundation model that VLA foundation model that introduces an RLVR optimization scheme with carefully designed rewards (region alignment, trajectory consistency, and output formatting), augmented by GRPO, to systematically strengthen reasoning and execution robustness while reducing reliance on manual annotation.\n\nWe propose VLA-R1, a VLA foundation model that VLA foundation model that introduces an RLVR optimization scheme with carefully designed rewards (region alignment, trajectory consistency, and output formatting), augmented by GRPO, to systematically strengthen reasoning and execution robustness while reducing reliance on manual annotation.\n\nWe introduce the VLA-CoT data engine, which produces high-quality VLA-CoT-13K aligned with affordance and trajectory labels and incorporates verifiable rewards, explicitly remedying the lack of step-wise reasoning in existing VLA models.\n\nWe comprehensively evaluate VLA-R1 on in-domain and out-of-domain datasets, in simulation, and on real-robot platforms, empirically verifying its effectiveness and cross-domain generalization.\n\n1. We propose VLA-R1, a VLA foundation model that VLA foundation model that introduces an RLVR optimization scheme with carefully designed rewards (region alignment, trajectory consistency, and output formatting), augmented by GRPO, to systematically strengthen reasoning and execution robustness while reducing reliance on manual annotation.\n\n2. We introduce the VLA-CoT data engine, which produces high-quality VLA-CoT-13K aligned with affordance and trajectory labels and incorporates verifiable rewards, explicitly remedying the lack of step-wise reasoning in existing VLA models.\n\n3. We comprehensively evaluate VLA-R1 on in-domain and out-of-domain datasets, in simulation, and on real-robot platforms, empirically verifying its effectiveness and cross-domain generalization.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的VLA模型缺乏逐步推理，直接输出最终动作，未考虑环境约束和几何关系。  \n2. 后训练阶段缺乏系统性推理强化，主要依赖于弱奖励设计的监督微调。  \n\n【提出了什么创新的方法】  \n提出了VLA-R1模型，结合可验证奖励的强化学习（RLVR）与群体相对策略优化（GRPO），系统优化推理与执行。设计了基于RLVR的后训练策略，利用区域对齐、轨迹一致性和输出格式化的可验证奖励，增强推理的稳健性和执行的准确性。此外，开发了VLA-CoT-13K数据集，提供与环境和轨迹注释明确对齐的链式推理监督。通过在多个领域和真实机器人平台的广泛评估，VLA-R1在泛化能力和现实世界表现上优于先前的VLA方法。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Predictive Preference Learning from Human Interventions",
            "authors": "Haoyuan Cai,Zhenghao Peng,Bolei Zhou",
            "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "NeurIPS 2025 Spotlight. Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.01545",
            "code": "https://metadriverse.github.io/ppl",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01545",
            "arxiv_html_link": "https://arxiv.org/html/2510.01545v1",
            "abstract": "Learning from human involvement aims to incorporate the human subject to monitor and correct agent behavior errors.\nAlthough most interactive imitation learning methods focus on correcting the agent’s action at the current state, they do not adjust its actions in future states, which may be potentially more hazardous. To address this, we introduce Predictive Preference Learning from Human Interventions (PPL), which leverages the implicit preference signals contained in human interventions to inform predictions of future rollouts.\nThe key idea of PPL is to bootstrap each human intervention into LL future time steps, called the preference horizon, with the assumption that the agent follows the same action and the human makes the same intervention in the preference horizon.\nBy applying preference optimization on these future states, expert corrections are propagated into the safety-critical regions where the agent is expected to explore, significantly improving learning efficiency and reducing human demonstrations needed. We evaluate our approach with experiments on both autonomous driving and robotic manipulation benchmarks and demonstrate its efficiency and generality. Our theoretical analysis further shows that selecting an appropriate preference horizon LL balances coverage of risky states with label correctness, thereby bounding the algorithmic optimality gap. Demo and code are available at: https://metadriverse.github.io/ppl.",
            "introduction": "Effectively leveraging human demonstrations to teach and align autonomous agents remains a central challenge in both Reinforcement Learning (RL) (xue2023guarded, ) and Imitation Learning (IL) (li2021efficient, ). In the literature of RL and more recent RL from Human Feedback (RLHF), the agent explores the environment through trial and error or under human feedback guidance, and the learning process hinges on a carefully crafted reward function that reflects human preferences.\nHowever, RL algorithms often require a large number of environment interactions to learn stable policies, and their exploration can lead agents to dangerous or task-irrelevant states (saunders2018trial, ; peng2021safe, ). In contrast, IL methods train agents to emulate human behavior using offline demonstrations from experts. Nevertheless, IL agents are susceptible to distributional shift because the offline dataset may lack corrective samples in safety-critical or out-of-distribution states (ross2010efficient, ; ravichandar2020recent, ; chernova2022robot, ; zare2024survey, ).\n\nInteractive Imitation Learning (IIL) (cai2025robot, ; reddy2018shared, ; kelly2019hg, ; spencer2020learning, ; peng2024learning, ; seraj2024interactive, ; liu2022robot, ; liu2024multi, ) incorporates human participants to intervene in the training process and provide online demonstrations. Such methods have improved alignment and learning efficiency in a wide variety of tasks, including robot manipulation (fang2019survey, ; ganapathi2021learning, ), autonomous driving (peng2021safe, ; peng2024learning, ), and even the strategy game StarCraft II (samvelyan2019starcraft, ). One line of research on confidence-based IIL designs various task-specific criteria to request human help, including uncertainty estimation (menda2019ensembledagger, ) and confidence in completing the task (chernova2009interactive, ; Saeidi2018International, ). In contrast, an increasing body of work focuses on learning from active human involvement, where human subjects actively intervene and provide demonstrations during training when the agent makes mistakes (kelly2019hg, ; spencer2020learning, ; mandlekar2020human, ; li2021efficient, ; peng2024learning, ). Compared to confidence-based IIL, active human involvement can ensure training safety (peng2021safe, ) and does not require carefully designing human intervention criteria for each task (hoque2021thriftydagger, ). However, these methods require the human expert to monitor the entire training process, predict the agent’s future trajectories, and intervene immediately in safety-critical states (peng2024learning, ), imposing a significant cognitive burden on the human participant. In addition, these methods often correct the agent’s behavior only at the current intervened state, penalizing undesired actions step by step. For instance, in HG-DAgger (kelly2019hg, ), the agent is optimized to mimic human actions solely at the states where interventions occur. In practice, it is intuitive that the agent may repeat similar mistakes in the consecutive future steps t+1,⋯,t+Lt+1,\\cdots,t+L following an error at step tt. As a result, the expert must repeatedly provide corrective demonstrations in these regions, compromising training efficiency (li2021efficient, ).\n\nIn this work, we propose a novel Interactive Imitation Learning algorithm, Predictive Preference Learning from Human Interventions (PPL), to learn from active human involvement. As shown in Fig. 1, our approach has two key designs: First, we employ an efficient rollout-based trajectory prediction model to forecast the agent’s future states. These predicted rollouts are visualized in real time for the user, helping human supervisors proactively determine when an intervention is necessary. Second, our algorithm leverages preference learning on the predicted future trajectories to further improve the sample efficiency and reduce the expert demonstrations needed. Such designs bring three strengths: (1) They mitigate the distributional shift problem in IIL and improve training efficiency. By incorporating anticipated future states into the training process, our method constructs a richer dataset, especially in safety-critical situations. This expanded dataset offers more information than expert corrective demonstrations in human-intervened states only.\n(2) The preference learning reduces the agent’s visits to dangerous states, thus suppressing human interventions in safety-critical situations.\n(3) By visualizing the agent’s predicted future trajectories in the user interface, we significantly reduce the cognitive burden on the human supervisor to constantly anticipate the agent’s behavior.\n\nOur contributions can be summarized as follows:\n\nWe introduce a novel Interactive Imitation Learning (IIL) algorithm that leverages trajectory prediction to inform human intervention and employs preference learning to deter the agent from returning to dangerous states.\n\nWe evaluate our algorithm on the MetaDrive (li2021metadrive, ) and Robosuite (zhu2020robosuite, ) benchmarks, using both neural experts and real human participants, showing that PPL requires fewer expert monitoring efforts and demonstrations to achieve near-optimal policies.\n\nWe present a theoretical analysis that derives an upper bound on the performance gap of our approach. This bound highlights that the efficacy of our method lies in reducing distributional shifts while preserving the quality of preference data.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的交互模仿学习方法仅在当前状态下纠正代理行为，未能调整未来状态的动作。  \n2. 人类干预的监控和预测对代理的安全性和学习效率至关重要。  \n\n【提出了什么创新的方法】  \n提出了一种名为预测偏好学习（PPL）的方法，通过利用人类干预中的隐含偏好信号来预测未来的状态。PPL的核心在于将每次人类干预扩展到未来的多个时间步，称为偏好视野，并在这些未来状态上应用偏好优化，从而将专家的纠正传播到安全关键区域。该方法显著提高了学习效率，减少了所需的人类演示次数。实验结果表明，PPL在自主驾驶和机器人操作基准测试中表现出色，能够在减少人类监督负担的同时实现接近最优的策略。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "Information Seeking for Robust Decision Making under Partial Observability",
            "authors": "Djengo Cyun-Jyun Fang,Tsung-Wei Ke",
            "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Robotics (cs.RO)",
            "comment": "The project page is available atthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.01531",
            "code": "https://infoseekerllm.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01531",
            "arxiv_html_link": "https://arxiv.org/html/2510.01531v1",
            "abstract": "Explicit information seeking is essential to human problem-solving in practical environments characterized by incomplete information and noisy dynamics. When the true environmental state is not directly observable, humans seek information to update their internal dynamics and inform future decision-making. Although existing Large Language Model (LLM) planning agents have addressed observational uncertainty, they often overlook discrepancies between their internal dynamics and the actual environment. We introduce Information Seeking Decision Planner (InfoSeeker), an LLM decision-making framework that integrates task-oriented planning with information seeking to align internal dynamics and make optimal decisions under uncertainty in both agent observations and environmental dynamics. InfoSeeker prompts an LLM to actively gather information by planning actions to validate its understanding, detect environmental changes, or test hypotheses before generating or revising task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark suite featuring partially observable environments with incomplete observations and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%74\\% absolute performance gain over prior methods without sacrificing sample efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms baselines on established benchmarks such as robotic manipulation and web navigation. These findings underscore the importance of tightly integrating planning and information seeking for robust behavior in partially observable environments. Project page: this https URL.",
            "introduction": "Real-world decision-making tasks are often partially observable, where observations and environmental dynamics may be noisy or uncertain. For example, in software engineering, a function may produce unexpected results due to incorrect usage or faulty implementation; In robotics, erroneous action control can result from wear and tear or inaccurate tuning of controllers. To correct failures, it is critical for agents to figure out the true underlying cause.\n\nHumans exhibit strong problem-solving capabilities in dynamic and uncertain environments, enabled by two core abilities. Task-oriented planning selects action sequences to achieve a goal, while information seeking (Gopnik, 2012; Kidd & Hayden, 2015; Case & Given, 2016) proactively gathers information to align internal beliefs, inferred from internal dynamics, with the external world. Information seeking is especially crucial under partial observability, where the true environment state is hidden and decisions rely on imperfect beliefs. For example, to reach the target in Fig. 1a (blue block), we initially plan under the assumption that the robot arm follows the commanded (x, y) positions accurately. If the outcome deviates from expectations, we collect new evidence (e.g., moving to various locations and measuring the resulting positions) to update our beliefs about the environmental dynamics. Together, task-oriented planning and information seeking enable humans to uncover hidden causes and make robust decisions under uncertainty.\n\nLarge Language Models (LLMs) have emerged as versatile zero-shot agents for autonomous decision-making (Hu et al., 2023; Wang et al., 2023a; Hong et al., 2024). In interactive environments, they generate action sequences and revise strategies in response to feedback (Yao et al., 2023; Wang et al., 2023b). This closed-loop planning paradigm has shown promise across diverse domains such as robotics (Sun et al., 2023; Wang et al., 2024) and scientific discovery (Jansen et al., 2024). Recent studies have examined LLMs under partial observability, emphasizing either the recovery of hidden knowledge (Ke et al., 2024; Krishnamurthy et al., 2024; Pan et al., 2025; Piriyakulkij et al., 2024) or identification of missing information in user instructions (Huang et al., 2024; Sun et al., 2024). However, these approaches overlook a critical challenge: mismatches between the agent’s internal dynamics and the actual environment. Without informative observations to realign them, the agent develops inaccurate beliefs of latent states, leading to systematically flawed plans.\n\nWe propose Information Seeking Decision Planner (InfoSeeker), a framework that integrates information seeking directly into the decision-making loop. Our key insight is that robust decision making requires explicitly planned information seeking actions to reconcile the agent’s internal dynamics with the external environment. As illustrated in Fig. 1a (blue block) and b, InfoSeeker prompts the LLM to actively gather information before proposing or revising a plan. Specifically, the LLM conducts targeted diagnostic trials that validate its understanding and detect shifts in environmental dynamics. In contrast, prior interactive planning approaches (Fig. 1a and c, yellow block) rely solely on reactive adaptation without explicit information gathering. By seeking evidence first, InfoSeeker uncovers the root causes of failure and adjusts its plans accordingly.\n\nTo evaluate our method’s robustness, we introduce a suite of text-based simulation benchmarks that test LLM agents under partial observability and noisy dynamics. To our knowledge, this is the first benchmark that directly evaluates agents’ planning capabilities under noisy environmental dynamics. Prior benchmarks (Fan et al., 2022; Shridhar et al., 2020; Wang et al., 2022) focus solely on observation uncertainty, where action outcomes are largely predictable. In contrast, our benchmarks incorporate uncertain dynamics, where actions may yield unexpected results due to unmodeled factors. For example, as illustrated in Fig. 1a, robot’s final position can deviate from the commanded target due to miscalibrated controllers. This setting better reflects real-world scenarios, requiring agents to handle both incomplete observations and unpredictable dynamics that violate their assumptions. Although our benchmark is currently hand-crafted, it highlights the need for more rigorous evaluations of planning under uncertainty in both observations and dynamics.\n\nInfoSeeker achieves an absolute performance gain of 74%74\\% over prior methods on our challenging benchmark. Active information seeking improves information acquisition without sacrificing sample efficiency, enabling the model to generate better task-oriented plans and achieve success faster. Moreover, InfoSeeker generalizes across different LLMs and outperforms existing approaches on established benchmarks, including LLM3 (Wang et al., 2024) and TravelPlanner (Xie et al., 2024), demonstrating both versatility and robustness.\n\nOur key contributions are: (1) An LLM-based planning framework that explicitly integrates information seeking to handle uncertainty in both dynamics and observations; (2) A novel benchmark suite for evaluating planning in partially observable environments with uncertainty in both observations and dynamics; (3) A formal connection between LLM-based planning and Partially Observable Markov Decision Processes (POMDPs).",
            "llm_summary": "【论文的motivation是什么】  \n1. 现实世界决策任务通常是部分可观察的，观察和环境动态可能是噪声或不确定的。  \n2. 现有的LLM规划代理在处理观察不确定性时，往往忽视了内部动态与实际环境之间的差异。  \n3. 需要一种方法来整合信息寻求与决策制定，以提高在不确定环境中的鲁棒性。  \n\n【提出了什么创新的方法】  \n提出了信息寻求决策规划器（InfoSeeker），该框架将信息寻求直接集成到决策循环中。InfoSeeker通过规划行动来主动收集信息，从而验证其理解、检测环境变化或测试假设，然后再生成或修订任务导向的计划。实验表明，InfoSeeker在新基准测试中相较于先前方法实现了74%的绝对性能提升，并且在样本效率上没有牺牲。此外，InfoSeeker在不同的LLM上具有良好的泛化能力，并在现有基准上超越了基线，展示了其多样性和鲁棒性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "A Robust Neural Control Design for Multi-drone Slung Payload Manipulation with Control Contraction Metrics",
            "authors": "Xinyuan Liang,Longhao Qian,Yi Lok Lo,Hugh H.T. Liu",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO)",
            "comment": "Submit to the 2026 American Control Conference (ACC)",
            "pdf_link": "https://arxiv.org/pdf/2510.01489",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01489",
            "arxiv_html_link": "https://arxiv.org/html/2510.01489v1",
            "abstract": "This paper presents a robust neural control design for a three-drone slung payload transportation system to track a reference path under external disturbances. The control contraction metric (CCM) is used to generate a neural exponentially converging baseline controller while complying with control input saturation constraints. We also incorporate the uncertainty and disturbance estimator (UDE) technique to dynamically compensate for persistent disturbances. The proposed framework yields a modularized design, allowing the controller and estimator to perform their individual tasks and achieve a zero trajectory tracking error if the disturbances meet certain assumptions. The stability and robustness of the complete system, incorporating both the CCM controller and the UDE compensator, are presented. Simulations are conducted to demonstrate the capability of the proposed control design to follow complicated trajectories under external disturbances.",
            "introduction": "Modern developments in cable-suspended payload transportation using multirotors present various challenges related to system performance, stability, and safety. Ref.[1] proposed an uncertainty and disturbance estimator (UDE)-based technique for such a slung payload task using a single-drone design. However, compared to a single-agent slung payload system, a multi-drone design offers a more scalable solution with better range, higher payload capacity, additional redundancy, and provides improved localization accuracy thanks to increased sensor data [2]. Various improvements have been made for the proposed multi-drone payload scheme [3, 4, 5, 6, 7, 8, 9, 10].\n\nIt is difficult to prove the stability of the multi-drone slung load system despite the successful simulation results due to its high-dimensional coupling characteristics [2], [5], and underactuated dynamics [3]. To address this problem, Qian and Liu [11] designed a two-loop control and tracking scheme that includes an outer loop robust controller for trajectory tracking and an inner loop attitude tracker on each drone, which follows the attitude commands from the outer loop controller. Later, they proved that the overall system was Lyapunov stable [12]. They also improved the design by adding a UDE to the outer loop. Both experiments and simulations of path-following tasks with disturbances were conducted to showcase the real-world implementation capabilities. Cai et al. [5] also used a similar hierarchical controller design and achieved Lyapunov stability, with simulations showing position convergence and attitude stabilization. Directly proving stability is also possible with multiple assumptions; Lee [3] successfully demonstrated stability using the designed geometric controller and simplified dynamics, with simulations demonstrating the ability of this controller to stabilize with bounded tracking error. Furthermore, Gao et al. [13] recently proved the stability of a neuro-geometric controller for a centralized 3-drone transportation system.\n\nThe complexity of the multi-drone slung payload system makes controller design challenging from a traditional control Lyapunov function (CLF) approach. Around 1998, the concept of control contraction metric (CCM) for trajectory tracking problems was proposed in [14]. Multiple studies since then have yielded a new control method using CCM on nonlinear systems [15]. The rapid development of deep learning has forged a new approach to find such a contraction metric and controller through a neural network [16]. Many advancements focus on realizing robustness has been addressed using such CCM controller design [17, 18, 19, 20, 21, 22]. Detailed descriptions of neural CCM (N-CCM) can be found in [23, 24]. However, only simplified low-dimensional cases were tested in [16], while high-dimensional nonlinear systems may fail, such as our multi-drone payload system. On the other hand, many safety considerations were addressed in [25], but control saturation remains a challenge.\n\nIn this paper, we propose a robust non-linear control scheme using N-CCM for a three-drone point-mass-slung payload system. The dynamic model is derived using Kane’s method. A CCM-based controller is constructed as in [16], with a control saturation to satisfy the control constraint. The contributions and novelty of the paper are listed as follows.\n\nAn exponentially converging controller for the multi-quadrotor slung-load system is obtained by using N-CCM. Compared with previous work [11] on slung-load control, our strategy naturally inherits bounded control output to satisfy control saturation constraints while guaranteeing the stability of the system.\n\nAn UDE derived from the results in [11] to compensate for persistent external disturbances. We show that the UDE compensator provides a bounded and converging disturbance estimation error.\n\nThe proposed controller scheme is fully modularized. By combining the classic UDE and attitude tracker adopted from [11] and [26] with the CCM-based baseline controller, we show that the complete closed-loop system is stable and robust.",
            "llm_summary": "【论文的motivation是什么】  \n1. 多无人机悬挂负载系统的稳定性和控制设计面临挑战。  \n2. 现有控制方法在高维非线性系统中表现不佳。  \n3. 控制饱和问题仍然是实现稳定控制的难点。  \n\n【提出了什么创新的方法】  \n本论文提出了一种基于神经控制收缩度量（N-CCM）的非线性控制方案，专门针对三架无人机的悬挂负载系统。通过Kane方法推导动态模型，构建了一个满足控制饱和约束的CCM控制器。该控制器能够实现指数收敛，并保证系统的稳定性。引入的外部干扰估计器（UDE）有效补偿持续干扰，确保了估计误差的有界收敛。整个控制方案模块化设计，结合了经典的UDE和姿态跟踪器，展示了闭环系统的稳定性和鲁棒性。最终，通过仿真验证了该控制设计在复杂轨迹跟踪中的能力。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-03 02:21:30",
            "title": "A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab",
            "authors": "Isaac Peterson,Christopher Allred,Jacob Morrey,Mario Harper",
            "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "codethis https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.01264",
            "code": "https://github.com/DIRECTLab/IsaacLab-HARL",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.01264",
            "arxiv_html_link": "https://arxiv.org/html/2510.01264v1",
            "abstract": "Multi-Agent Reinforcement Learning (MARL) is central to robotic systems cooperating in dynamic environments. While prior work has focused on these collaborative settings, adversarial interactions are equally critical for real-world applications such as pursuit-evasion, security, and competitive manipulation. In this work, we extend the IsaacLab framework to support scalable training of adversarial policies in high-fidelity physics simulations. We introduce a suite of adversarial MARL environments featuring heterogeneous agents with asymmetric goals and capabilities. Our platform integrates a competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal Policy Optimization (HAPPO), enabling efficient training and evaluation under adversarial dynamics. Experiments across several benchmark scenarios demonstrate the framework’s ability to model and train robust policies for morphologically diverse multi-agent competition while maintaining high throughput and simulation realism. Code and benchmarks are available at: https://directlab.github.io/IsaacLab-HARL/.",
            "introduction": "Reinforcement learning (RL) has emerged as a leading paradigm for training robots to acquire complex skills [1, 2]. From locomotion in humanoids [3] and quadrupeds [4] to advanced behaviors such as parkour [5, 6], RL has repeatedly demonstrated its effectiveness in solving high-dimensional control problems.\n\nBuilding on these advances, multi-agent reinforcement learning (MARL) has shown strong potential for coordination and control in robotics [7], and the advent of GPU-parallelized simulators such as IsaacLab [8] and MuJoCo [9] has made large-scale training feasible. However, as mentioned in [10], several important challenges remain, including heterogeneous teaming and physics-based training, which were recently explored in cooperative settings by [11].\n\nWe present heterogeneous adversarial learning in high-fidelity contexts as another gap in adversarial multi-agent reinforcement learning. We define heterogeneous teaming as situations where teams can have different numbers of agents, with differences in morphologies, observations, and actions. Many real-world robotics applications involve competition rather than pure cooperation. Scenarios such as pursuit–evasion, security, and competitive manipulation all require potentially different agents to anticipate and counter the strategies of opponents. Unlike simplified grid worlds or abstract games, these domains demand contact-rich dynamics where accurate physics simulation is critical. Moreover, heterogeneous robots—such as legged and wheeled platforms competing together—introduce additional complexity, as physical differences can lead to specialized strategies.\n\nAdversarial learning in this setting raises several challenges. First, competitive training can be unstable, as both agents are evolving over time. Second, heterogeneous teams require team-specific critics under centralized training and decentralized execution, as standard parameter-sharing approaches are insufficient. Third, reward design in physics-based tasks must balance dense shaping with sparse success signals to prevent unintended strategies.\n\nIn this paper, we address these challenges by extending the IsaacLab simulator with Heterogeneous Agent Reinforcement Learning (HARL) algorithms to support scalable multi-team adversarial training. We present HARL-Adversarial (HARL-A), a framework for the implementation of multi-agent adversarial reinforcement learning with benchmark environments, allowing others to easily develop and test their own scenarios. We introduce benchmark environments (shown in Figure 1) that highlight the unique challenges of adversarial play across morphologies, and demonstrate the effectiveness of our framework through large-scale experiments.\n\nThe main contributions of this work are:\n\nModification of both HARL and IsaacLab to enable multi-agent heterogeneous adversarial learning at scale.\n\nImplementation of functional environments and trained policies for heterogeneous adversarial learning, facilitating accelerated research in this domain.\n\nIntroduction of new benchmarks for testing MARL algorithms in high-fidelity adversarial settings, enabling the development of more robust algorithms under competitive dynamics.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的多智能体强化学习（MARL）研究主要集中在合作环境中，缺乏对对抗性互动的关注。  \n2. 现实世界应用中，如追捕-逃避和竞争操控，强调了对抗性学习的重要性。  \n3. 需要解决异构团队在对抗性训练中的不稳定性和复杂性。  \n\n【提出了什么创新的方法】  \n本文扩展了IsaacLab框架，提出了HARL-Adversarial (HARL-A)方法，支持可扩展的多智能体对抗性强化学习。该方法结合异构智能体的特点，设计了多种基准环境，以便于研究人员开发和测试自己的场景。通过大规模实验，验证了该框架在高保真物理模拟下训练强健策略的能力，推动了对抗性学习领域的研究进展。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        }
    ],
    "2025-10-04": [],
    "2025-10-05": [],
    "2025-10-06": [
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Simulation to Rules: A Dual-VLM Framework for Formal Visual Planning",
            "authors": "Yilun Hao,Yongchao Chen,Chuchu Fan,Yang Zhang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Symbolic Computation (cs.SC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.03182",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.03182",
            "arxiv_html_link": "https://arxiv.org/html/2510.03182v1",
            "abstract": "Vision Language Models (VLMs) show strong potential for visual planning but struggle with precise spatial and long-horizon reasoning. In contrast, Planning Domain Definition Language (PDDL) planners excel at long-horizon formal planning, but cannot interpret visual inputs. Recent works combine these complementary advantages by enabling VLMs to turn visual planning problems into PDDL files for formal planning. However, while VLMs can generate PDDL problem files satisfactorily, they struggle to accurately generate the PDDL domain files, which describe all the planning rules. As a result, prior methods rely on human experts to predefine domain files or on constant environment access for refinement.\nWe propose VLMFP, a Dual-VLM-guided framework that can autonomously generate both PDDL problem and domain files for formal visual planning. VLMFP introduces two VLMs to ensure reliable PDDL file generation: A SimVLM that simulates action consequences based on input rule descriptions, and a GenVLM that generates and iteratively refines PDDL files by comparing the PDDL and SimVLM execution results.\nVLMFP unleashes multiple levels of generalizability: The same generated PDDL domain file works for all the different instances under the same problem, and VLMs generalize to different problems with varied appearances and rules.\nWe evaluate VLMFP with 6 grid-world domains and test its generalization to unseen instances, appearance, and game rules. On average, SimVLM accurately describes 95.5%, 82.6% of scenarios, simulates 85.5%, 87.8% of action sequence, and judges 82.4%, 85.6% goal reaching for seen and unseen appearances, respectively. With the guidance of SimVLM, VLMFP can generate PDDL files to reach 70.0%, 54.1% valid plans for unseen instances in seen and unseen appearances, respectively. Project page: https://sites.google.com/view/vlmfp.",
            "introduction": "Although Large Language Models (LLMs) have shown strong performance in solving text-based planning problems (Wei et al., 2022; Yao et al., 2022; Raman et al., 2022; Yao et al., 2024), many real-world planning tasks, such as robot assembly, drone navigation, and autonomous driving, are inherently visual, making the reliance on carefully engineered text inputs impractical and limiting. This gap motivates the shift toward VLM-based planning, where visual inputs provide a more direct and intuitive basis for reasoning.\nHowever, current VLMs lack precise spatial understanding and long-horizon reasoning, which constrains their ability to address complex, multi-step planning problems that involve intricate spatial relationships among multiple objects (Wu et al., 2024).\n\nOn the other hand, Planning Domain Definition Language (PDDL) (McDermott, 2000) is a formal language designed to describe planning problems and domains in a structured, machine-interpretable way. PDDL has enabled numerous automated planners to derive long-horizon solutions. However, although PDDL-based planners excel at reasoning over structured domains, they depend on correctly structured PDDL domain and problem files and cannot directly interpret visual inputs. Constructing accurate PDDL definitions is non-trivial and requires expert knowledge, which is often inaccessible to non-expert users, limiting the broader adoption of PDDL planners in real-world scenarios.\n\nRecent works have explored combining the advantages of language models and PDDL planners through various approaches. (Liu et al., 2023; Xie et al., 2023) employ LLMs as translators to convert natural language scenario descriptions into PDDL problem files. (Mahdavi et al., 2024) leverages environment interactions to enable file generation for both problem and domain. However, these methods require either textual scenario descriptions, environment access, or pre-defined PDDL files, which cannot be directly achieved through visual inputs. More recently, vision-language models (VLMs) have been used to extract scenario information from visual inputs and generate corresponding PDDL problem files(Shirai et al., 2024; Dang et al., 2025). However, since current VLMs lack the ability to accurately generate the domain PDDL, these approaches also assume access to ground truth domain PDDL files, without which PDDL planners cannot produce any results.\n\nIn this paper, we address the challenge of visual long-horizon planning by introducing a novel framework, VLM-Guided Formal Planning (VLMFP, illustrated in Fig. 1), a Dual-VLM-guided framework that autonomously generates both problem and domain PDDL files for visual planning.\nVLMFP integrates two specialized VLMs: a fine-tuned SimVLM that perceives the scenario from visual inputs and simulates action outcomes, and a large GenVLM that generates and iteratively refines PDDL files by aligning their execution with SimVLM’s simulations.\nGenerating both problem and domain PDDL from visual inputs requires object recognition, spatial understanding, reasoning, and PDDL knowledge. We fine-tune a small VLM as SimVLM to strengthen its spatial reasoning, while using a large model as GenVLM for general reasoning and extensive PDDL knowledge.\n\nImportantly, VLMFP achieves multiple levels of generalizability. A generated domain PDDL can be reused across all instances of the same domain, while problem PDDL files can be adapted efficiently for new instances as in-context examples. The framework also transfers well to unseen appearances and even altered environment rules. We evaluate VLMFP on six grid world domains, showing that SimVLM reliably describes scenarios, simulates actions, and determines goal achievement, while GenVLM, guided by SimVLM feedback, generates valid PDDL files that enable planners to solve both seen and unseen problems.\n\nIn summary, our key contributions are:\n\nWe construct a large-scale dataset of 430k action sequence simulations with reasoning and feedback across 6 grid-world domains of different map sizes, appearances, and game rules. We fine-tune Qwen2-VL-7B with the dataset, and our finetuned model demonstrates strong generalization to unseen instances, appearances, and game rules.\n\nWe propose VLMFP, a Dual-VLM-guided framework that autonomously generates PDDL domain and problem files for visual planning, which, to our knowledge, is the first framework to leverage visual inputs to generate both PDDL files without human feedback or direct environment access.\n\nBy combining SimVLM (for perception and action simulation) with GenVLM (for symbolic reasoning and file refinement), VLMFP achieves robust, reusable domain generation and efficient problem instantiation. VLMFP notably achieves 70.0% and 54.1% success rates with GPT-4o as the GenVLM, outperforming the best baseline CodePDDLGPT-4o{}_{\\textsc{GPT-4o}} by 39.3% and 21.8% for unseen instances in seen and unseen appearances, respectively.\n\n1. We construct a large-scale dataset of 430k action sequence simulations with reasoning and feedback across 6 grid-world domains of different map sizes, appearances, and game rules. We fine-tune Qwen2-VL-7B with the dataset, and our finetuned model demonstrates strong generalization to unseen instances, appearances, and game rules.\n\n2. We propose VLMFP, a Dual-VLM-guided framework that autonomously generates PDDL domain and problem files for visual planning, which, to our knowledge, is the first framework to leverage visual inputs to generate both PDDL files without human feedback or direct environment access.\n\n3. By combining SimVLM (for perception and action simulation) with GenVLM (for symbolic reasoning and file refinement), VLMFP achieves robust, reusable domain generation and efficient problem instantiation. VLMFP notably achieves 70.0% and 54.1% success rates with GPT-4o as the GenVLM, outperforming the best baseline CodePDDLGPT-4o{}_{\\textsc{GPT-4o}} by 39.3% and 21.8% for unseen instances in seen and unseen appearances, respectively.",
            "llm_summary": "【论文的motivation是什么】  \n1. VLMs struggle with precise spatial understanding and long-horizon reasoning in visual planning tasks.  \n2. PDDL planners excel at long-horizon planning but cannot interpret visual inputs, limiting their applicability.  \n3. Existing methods require human-defined PDDL files or constant environment access, which is impractical.  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. Recent works combine LLMs and PDDL planners but still depend on textual descriptions or predefined files.  \n2. VLMs have been used to generate PDDL problem files but fail to generate accurate domain files, necessitating ground truth access.  \n\n【提出了什么创新的方法】  \nVLMFP is introduced as a Dual-VLM-guided framework that autonomously generates both PDDL problem and domain files from visual inputs. It employs two specialized VLMs: SimVLM for simulating action outcomes and GenVLM for generating and refining PDDL files. This approach allows for robust generalization across different instances and environments without human intervention. The framework demonstrates high accuracy in scenario description and action simulation, achieving significant success rates in generating valid plans for unseen instances.  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Optimal Smooth Coverage Trajectory Planning for Quadrotors in Cluttered Environment",
            "authors": "Duanjiao Li,Yun Chen,Ying Zhang,Junwen Yao,Dongyue Huang,Jianguo Zhang,Ning Ding",
            "subjects": "Robotics (cs.RO)",
            "comment": "This paper has been accepted for publication in the 44th Chinese Control Conference, 2025. Please cite the paper using appropriate formats",
            "pdf_link": "https://arxiv.org/pdf/2510.03169",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.03169",
            "arxiv_html_link": "https://arxiv.org/html/2510.03169v1",
            "abstract": "For typical applications of UAVs in power grid scenarios, we construct the problem as planning UAV trajectories for coverage in cluttered environments. In this paper, we propose an optimal smooth coverage trajectory planning algorithm. The algorithm consists of two stages. In the front-end, a Genetic Algorithm (GA) is employed to solve the Traveling Salesman Problem (TSP) for Points of Interest (POIs), generating an initial sequence of optimized visiting points. In the back-end, the sequence is further optimized by considering trajectory smoothness, time consumption, and obstacle avoidance. This is formulated as a nonlinear least squares problem and solved to produce a smooth coverage trajectory that satisfies these constraints. Numerical simulations validate the effectiveness of the proposed algorithm, ensuring UAVs can smoothly cover all POIs in cluttered environments.",
            "introduction": "In recent years, with the rapid development of manufacturing industries, unmanned systems have found widespread applications across various fields. Among them, quadrotors have been increasingly utilized in industrial applications such as aerial photography and surveying [1]. As electricity consumption continues to rise, the frequency of power grid maintenance has also increased. Given the high risks and costs associated with manual inspections, the importance of utilizing unmanned systems for autonomous power grid inspections has become increasingly evident [2], as shown in Fig 1. Substations, as critical components of the power grid system, play an essential role in ensuring seamless inspection across modules within the same facility or between different facilities. The units scheduled for inspection can be abstracted as a series of access points, with drones acting as agents tasked with visiting these points. Consequently, the key focus of this paper is to explore how quadrotors can efficiently cover all inspection points while minimizing energy costs.\n\nIn cluttered environments with obstacles, a broad range of motion planning strategies has been extensively studied. Most focus on optimality guarantees and static obstacle avoidance capabilities, such as Expansive Space Trees (EST) [3], Probabilistic Roadmap Methods (PRM) [4], Rapidly exploring Random Trees (RRT) [5], and optimal RRT (RRT*) [6]. Additionally, as noted in [7], approaches like multi-RRT* Fixed Node (RRT*FN) and genetic algorithm (GA) have been explored for UAV motion planning in cluttered environments. Yet, these methods fail to adequately address the trajectory generation problem for feasible UAV paths. Moreover, these methods lack real-time applicability and are less suitable for dynamic environments due to high computational costs.\nAdditionally, some works, such as [10], consider not only the shortest path as the sole criterion when addressing the Traveling Salesman Problem (TSP), problem but also incorporate the UAV’s dynamic model into the planning process. However, this approach does not take obstacle avoidance into account.\n\nFor the trajectory generation of UAVs, several studies have been conducted. For instance, [8] generates trajectories with jerk limits, while [9] employs optimal smoothing B-splines and validates the idea through physical experiments. However, these works do not focus on the critical aspect of efficiently accessing predefined trajectory points.\n\nIn this paper, we address the defined problem: how to generate smooth and feasible trajectories for UAVs to cover all inspection targets under limited resources. We propose a two-scale solution.\nAt the first scale, without considering obstacles, all target points are formulated as a TSP, which is an NP-hard problem, solved by a GA.\nAt the second scale, based on the visitation sequence obtained from the first scale, we use an optimal B-spline to generate trajectories. During this process, obstacles represented by Euclidean Distance Transforms (EDT) are incorporated as constraints to ensure the generated trajectory is feasible for UAV following.\n\nThe remaining of the paper is organized as follows: problem\nstatement is presented in Section II. The proposed algorithms are highlighted in Section III. The numerical simulations are conducted in Section IV. Eventually, some conclusions are remarked in the last section.",
            "llm_summary": "【论文的motivation是什么】  \n1. 在复杂环境中，如何有效规划无人机的轨迹以覆盖所有检查点。  \n2. 现有方法在动态环境中的实时应用能力不足，无法满足实际需求。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 许多运动规划策略关注静态障碍物的规避，但未能有效解决轨迹生成问题。  \n2. 现有的TSP解决方案未考虑障碍物规避，导致路径规划的局限性。  \n3. 现有研究缺乏对预定义轨迹点的高效访问的关注。  \n\n【提出了什么创新的方法】  \n本文提出了一种两阶段的解决方案。第一阶段，使用遗传算法（GA）解决无人机访问所有目标点的旅行推销员问题（TSP），生成初步的访问顺序。第二阶段，基于第一阶段的结果，利用优化的B样条生成平滑的轨迹，同时考虑障碍物的约束。通过数值仿真验证了该算法的有效性，确保无人机能够在复杂环境中平滑覆盖所有检查点。最终，提出的方法在轨迹平滑性、时间消耗和障碍物规避方面均表现出色。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning",
            "authors": "Tianyu Xu,Jiawei Chen,Jiazhao Zhang,Wenyao Zhang,Zekun Qi,Minghan Li,Zhizheng Zhang,He Wang",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.03142",
            "code": "https://pku-epic.github.io/MM-Nav-Web/",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.03142",
            "arxiv_html_link": "https://arxiv.org/html/2510.03142v1",
            "abstract": "Visual navigation policy is widely regarded as a promising direction, as it mimics humans by using egocentric visual observations for navigation. However, optical information of visual observations is difficult to be explicitly modeled like LiDAR point clouds or depth maps, which subsequently requires intelligent models and large-scale data.\nTo this end, we propose to leverage the intelligence of the Vision-Language-Action (VLA) model to learn diverse navigation capabilities from synthetic expert data in a teacher-student manner. Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360° observation) based on pretrained large language models and visual foundation models.\nFor large-scale navigation data, we collect expert data from three reinforcement learning (RL) experts trained with privileged depth information in three challenging tailor-made environments for different navigation capabilities: ‘reaching’, ‘squeezing’, and ‘avoiding’.\nWe iteratively train our VLA model using data collected online from RL experts, where the training ratio is dynamically balanced based on performance on individual capabilities.\nThrough extensive experiments in synthetic environments, we demonstrate that our model achieves strong generalization capability. Moreover, we find that our student VLA model outperforms the RL teachers, demonstrating the synergistic effect of integrating multiple capabilities. Extensive real-world experiments further confirm the effectiveness of our method.",
            "introduction": "Visual navigation has garnered considerable attention in the robotics field [1, 2, 3, 4, 5, 6, 7, 8, 9], as it requires robots to reach target locations based on visual inputs. Visual observations provide detailed and rich environmental information for navigation while remaining cost-effective. However, interpreting such informative visual data and planning appropriate navigation actions remains challenging [3, 2], demanding highly intelligent models and large-scale navigation datasets.\n\nTo this end, existing methods [1, 7, 9, 2, 3] address this challenge through learned policies that implicitly interpret visual inputs and predict subsequent actions. Advanced approaches have made initial progress by training on real-world passive navigation or touring videos to emulate general navigation capabilities. However, these methods remain constrained by limited observational perspectives (e.g., only front-view) and the relatively spacious environments common in navigation datasets [10, 11], hindering their applicability in more challenging scenarios.\n\nIn this context, visual navigation faces a conflict: real-world navigation data are mainly collected from single-camera setups and lack extremely challenging or hazardous scenarios (e.g., avoid damaging robots); whereas synthetic navigation data allow customizable camera configurations and can generate data reflecting challenging navigation capabilities, yet they suffer from the sim-to-real gap  [10, 12, 13] due to non-photorealistic imagery.\n\nTo resolve this conflict, we propose MM-Nav, a multi-view VLA model that captures 360∘ observations around the robot and learns navigation capabilities from multiple synthetic RL experts. This mechanism combines the best of both worlds: the generality of VLAs and the diverse navigation data available in synthetic environments. Specifically, we construct the VLA model with four horizontally distributed camera views to cover the surrounding environment and employ an action head to predict velocity commands for robot control. To maintain inference efficiency, we carefully design the tokenization of historical and current observations, enabling the VLA model to achieve an inference speed of approximately 7 Hz, which is comparable to existing visual navigation methods [7, 2].\n\nTo train MM-Nav for strong navigation capabilities, a large amount of navigation data is required. To this end, we construct synthetic environments and train individual reinforcement learning (RL) experts with three distinct navigation capabilities: reaching, squeezing, and avoiding. Leveraging data from these RL experts, we pre-train our model on their successful demonstrations. Then, we adopt an online teacher-student training strategy (in a DAgger manner [14]) to iteratively fine-tune our model with data from different navigation capabilities. Here, we employ a capability-balanced data aggregation strategy that enables goal-oriented training, which we find leads to faster training convergence.\n\nWe conducted extensive experiments in both synthetic and real-world environments. The results show that our method exhibits strong navigation performance across environments with different capabilities, even outperforming specifically trained RL experts. This demonstrates that our approach achieves superior navigation performance by leveraging the synergy among diverse capabilities.\nFurthermore, extensive real-world experiments confirm the robustness of our method in robust zero-shot sim-to-real transfer in challenging environments.",
            "llm_summary": "【论文的motivation是什么】  \n1. 视觉导航需要智能模型来处理复杂的视觉输入和导航动作。  \n2. 现有方法在处理多视角和挑战性场景时存在局限性。  \n3. 需要有效利用合成数据来弥补真实数据的不足。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有方法通过学习策略来解释视觉输入，但大多只限于单一视角。  \n2. 真实世界数据缺乏挑战性场景，而合成数据存在sim-to-real差距。  \n\n【提出了什么创新的方法】  \n我们提出了MM-Nav，一个多视角VLA模型，通过360°观察来学习多种导航能力。该模型结合了来自三个强化学习专家的合成数据，通过教师-学生训练策略进行迭代训练。我们设计了一个能力平衡的数据聚合策略，促进了目标导向的训练，显著提高了模型的收敛速度。实验结果表明，MM-Nav在不同环境中表现出色，甚至超越了专门训练的RL专家，展示了多种能力整合的协同效应。此外，广泛的真实世界实验验证了该方法在挑战性环境中的强大零-shot sim-to-real转移能力。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Whisker-based Tactile Flight for Tiny Drones",
            "authors": "Chaoxiang Ye,Guido de Croon,Salua Hamaza",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.03119",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.03119",
            "arxiv_html_link": "https://arxiv.org/html/2510.03119v1",
            "abstract": "Tiny flying robots hold great potential for search-and-rescue, safety inspections, and environmental monitoring, but their small size limits conventional sensing—especially with poor-lighting, smoke, dust or reflective obstacles. Inspired by nature, we propose a lightweight, 3.2-gram, whisker-based tactile sensing apparatus for tiny drones, enabling them to navigate and explore through gentle physical interaction. Just as rats and moles use whiskers to perceive surroundings, our system equips drones with tactile perception in flight, allowing obstacle sensing even in pitch-dark conditions. The apparatus uses barometer-based whisker sensors to detect obstacle locations while minimising destabilisation. To address sensor noise and drift, we develop a tactile depth estimation method achieving sub-6 mm accuracy. This enables drones to navigate, contour obstacles, and explore confined spaces solely through touch—even in total darkness along both soft and rigid surfaces. Running fully onboard a 192-KB RAM microcontroller, the system supports autonomous tactile flight and is validated in both simulation and real-world tests. Our bio-inspired approach redefines vision-free navigation, opening new possibilities for micro aerial vehicles in extreme environments.",
            "introduction": "未获取到引言",
            "llm_summary": "【论文的motivation是什么】  \n1. 小型无人机在复杂环境中的导航能力有限。  \n2. 现有传感器在低光照或障碍物反射情况下表现不佳。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统视觉传感器在复杂环境中难以有效工作，尤其是在低光照条件下。  \n2. 现有的触觉传感技术尚未应用于飞行机器人，缺乏相应的导航解决方案。  \n\n【提出了什么创新的方法】  \n本研究提出了一种轻量级的触觉传感器，模仿自然界中的触须，赋予小型无人机在飞行中通过触觉感知周围环境的能力。该系统使用基于气压的触须传感器来检测障碍物位置，并通过开发的触觉深度估计方法实现了亚6毫米的精度。该方法支持无人机在完全黑暗中沿软硬表面进行自主导航和探索。经过模拟和实地测试验证，该系统展示了在极端环境中无视力导航的新可能性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Embracing Evolution: A Call for Body-Control Co-Design in Embodied Humanoid Robot",
            "authors": "Guiliang Liu,Bo Yue,Yi Jin Kim,Kui Jia",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.03081",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.03081",
            "arxiv_html_link": "https://arxiv.org/html/2510.03081v1",
            "abstract": "Humanoid robots, as general-purpose physical agents, must integrate both intelligent control and adaptive morphology to operate effectively in diverse real-world environments. While recent research has focused primarily on optimizing control policies for fixed robot structures, this position paper argues for “evolving both control strategies and humanoid robots’ physical structure under a co-design mechanism”.\nInspired by biological evolution, this approach enables robots to iteratively adapt both their form and behavior to optimize performance within task-specific and resource-constrained contexts.\nDespite its promise, co-design in humanoid robotics remains a relatively underexplored domain, raising fundamental questions about its feasibility and necessity in achieving true embodied intelligence.\nTo address these challenges, we propose practical co-design methodologies grounded in strategic exploration, Sim2Real transfer, and meta-policy learning.\nWe further argue for the essential role of co-design by analyzing it from methodological, application-driven, and community-oriented perspectives.\nStriving to guide and inspire future studies, we present open research questions, spanning from short-term innovations to long-term goals.\nThis work positions co-design as a cornerstone for developing the next generation of intelligent and adaptable humanoid agents.",
            "introduction": "As an emerging research area, Embodied AI posits that intelligence stems from an agent’s ability to actively explore, interact with, and learn from its environment in a continuous and dynamic manner. Within this learning paradigm, recent studies have developed various robot control models based on deep neural network backbones, enabling scalability across diverse tasks and environments Zhao2023aloha ; Lin2024VILA ; liu2025rdtb ; Ghosh2024Octo .\n\nIn studies of embodied robot agents, their skills are closely tied to the physical form. For example, robot arms, grippers, and dexterous hands are commonly employed for manipulation tasks such as grasping, placing, and assembling objects luo2023fmb ; luo2024precise . Similarly, wheeled robots, bipedal robots, and quadrupedal robots are designed for locomotion tasks, including walking, climbing, and navigation xu2024human .\nTo develop general-purpose robots, recent studies have focused on humanoid robots. Equipped with dual arms, legged body, and advanced sensors, humanoid robots are well-suited for a wide range of mobile locomotion tasks, enabling them to seamlessly handle everyday tasks fu2024humanplus ; Cheng2024Exbody ; he2024omnih2o ; He2024HOVER .\n\nIn recent years, the development of humanoid robots has primarily centered on control policy design, typically built upon predefined physical structures.\nThese robotic designs are often the result of manual engineering and domain-specific heuristics, which are rarely subject to optimization within the embodied humanoid system.\nHowever, embodied intelligence is not solely determined by control performance, but is also fundamentally grounded in agents’ physical structure gupta2021embodied .\nFor instance, in natural systems, organisms evolve their body morphology to adapt to changing environmental conditions. Similarly, embodied agents should incorporate evolutionary mechanisms to adapt to task requirements and environmental dynamics.\n\nAn effective method of realizing such evolutionary mechanisms is the robotic co-design problem, which seeks to jointly optimize both the control policy and the morphological design of robotic systems Carlone2019CoDesign .\nWhile prior studies have explored co-design in quadruped robots Belmonte2022RL4Leggy ; Bjelonic2023CoDesignQuad , soft robots Wang2023PreCo , bi-piedal robots Cheng2024SERL ; Ghansah2023HumanoidCoDesign  and modular robots Zhao2020RoboGrammar ; Whitman2020RLDesign  (see Table 1), its extension to more advanced humanoid robots and connection to embodied intelligence remains largely unexplored.\nIt remains unclear how to efficiently discover the optimal design of a generalist humanoid robot capable of performing a variety of tasks. More importantly, the necessity of addressing such co-design problems in the development of embodied humanoid robots has yet to be fully established.\n\nThis article provides a principled formulation of the humanoid co-design problem, emphasizing that evolving physical structure is both feasible and essential for realizing embodied intelligence in humanoid robots.\nSpecifically, we formulate the humanoid co-design problem as a bi-level optimization. Such an optimizer can be integrated into the reasoning–acting architecture of an advanced controlling model, enabling an embodied humanoid robot to exhibit dexterity, mobility, perception, and intelligence.\n\nBeyond the proposed formulation, we investigate an alternative perspective for realizing embodied humanoid robots based on predefined and manually specified designs.\nWe analyze why such paradigms prevail in recent humanoid robotics research and examine the potential challenges of adopting co-design, particularly regarding algorithmic complexity, physical evaluation, and design scalability.\nTo address these challenges, we introduce advancements in learning-based solvers, such as strategic robot structure exploration, the Sim2Real learning paradigm, and meta control policy, highlighting the feasibility of evolving humanoid robot architectures.\n\nTo understand the necessity of humanoid robot co-design, we investigate its unique advantages in facilitating robot morphology optimization, real-world task adaptation, and cross-disciplinary collaboration, examined from the perspectives of methodology, application, and community.\nTo realize these key advantages, we identify open questions within the humanoid robot co-design problem, highlighting those that may be tractable with current methodologies in the short term, as well as those that may depend on long-term advances in emerging research domains.",
            "llm_summary": "【论文的motivation是什么】  \n1. 当前的类人机器人研究主要集中在固定结构的控制策略优化。  \n2. 需要探索如何同时优化控制策略和物理结构以实现真正的具身智能。  \n3. 现有的类人机器人设计缺乏有效的共同设计机制。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 先前研究主要集中在四足机器人、软体机器人和模块化机器人上的共同设计。  \n2. 类人机器人领域的共同设计仍然是一个相对未被充分探索的领域。  \n3. 现有方法未能有效解决类人机器人在多任务和环境适应性方面的设计问题。  \n\n【提出了什么创新的方法】  \n提出了一种基于生物进化的共同设计方法，强调控制策略和物理结构的协同优化。具体方法包括：  \n- 采用双层优化框架来解决类人机器人共同设计问题。  \n- 引入战略性结构探索、Sim2Real转移和元策略学习等先进的学习方法。  \n- 分析共同设计在方法论、应用和社区层面的重要性。  \n通过这些方法，研究表明共同设计不仅是实现类人机器人具身智能的可行途径，而且能够优化机器人形态、提高任务适应性并促进跨学科合作。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Long-Term Human Motion Prediction Using Spatio-Temporal Maps of Dynamics",
            "authors": "Yufei Zhu,Andrey Rudenko,Tomasz P. Kucner,Achim J. Lilienthal,Martin Magnusson",
            "subjects": "Robotics (cs.RO)",
            "comment": "IEEE Robotics and Automation Letters",
            "pdf_link": "https://arxiv.org/pdf/2510.03031",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.03031",
            "arxiv_html_link": "https://arxiv.org/html/2510.03031v1",
            "abstract": "Long-term human motion prediction (LHMP) is important for the safe and efficient operation of autonomous robots and vehicles in environments shared with humans. Accurate predictions are important for applications including motion planning, tracking, human-robot interaction, and safety monitoring. In this paper, we exploit Maps of Dynamics (MoDs), which encode spatial or spatio-temporal motion patterns as environment features, to achieve LHMP for horizons of up to 60 seconds. We propose an MoD-informed LHMP framework that supports various types of MoDs and includes a ranking method to output the most likely predicted trajectory, improving practical utility in robotics. Further, a time-conditioned MoD is introduced to capture motion patterns that vary across different times of day. We evaluate MoD-LHMP instantiated with three types of MoDs. Experiments on two real-world datasets show that MoD-informed method outperforms learning-based ones, with up to 50% improvement in average displacement error, and the time-conditioned variant achieves the highest accuracy overall. Project code is available at https://github.com/test-bai-cpu/LHMP-with-MoDs.git",
            "introduction": "Ensuring safe and efficient operation of robots in complex and dynamic environments, particularly in the presence of humans, is critical for deploying robotic systems to assist with a wide range of real-world tasks [1, 2]. A key element in achieving this goal is long-term human motion prediction, i.e., anticipating the trajectories of individuals over extended periods. Accurate long-term prediction of future trajectories is a fundamental requirement for various applications, including optimized motion planning, refined tracking, advanced automated driving, improved human-robot interaction, and enhanced intelligent safety monitoring and surveillance.\n\nHuman motion is complex, influenced by various factors. These include not only an individual’s intrinsic intent and dynamics but also external influences such as social conventions and environmental cues [3]. Predicting trajectories over extended periods, such as 20 seconds or more, requires careful consideration of the impact of large-scale environments on human behavior. While short-term predictions can often rely on current state and immediate interactions, long-term predictions demand more comprehensive modeling to capture how the environment influences and guides human movement.\n\nAn effective approach to address long-term human motion prediction (LHMP) is through the use of maps of dynamics (MoDs), which encode spatial or spatio-temporal motion patterns as a feature of the environment. Prior work, CLiFF-LHMP [4], exploits the CLiFF-map representation [5], which is a specific type of MoD that stores a multi-modal, continuous joint distribution of speed and orientation for each discrete map location, to predict human trajectories over long-term horizons. In this work, we extend CLiFF-LHMP to a general MoD-informed LHMP framework, named MoD-LHMP, which can be applied with various types of MoDs. By using MoDs, motion prediction can utilize previously observed motion patterns. We also introduce a ranking method that enables the prediction of the most likely trajectory output, enhancing its practical utility for robotic applications.\n\nWe instantiate MoD-LHMP with multiple types of MoDs, including: (1) the original CLiFF-map;\n(2) a Time-Conditioned CLiFF-map, which we introduce in this work;\nand (3) STeF-map [6], a spatio-temporal MoD designed to capture periodic motion patterns. Given that human movement in the same environment varies over time, in this work, we address the problem of capturing spatio-temporal motion patterns and use them for LHMP. We present the Time-Conditioned CLiFF-map, which adapts to varying motion patterns at different times of day.\nThe methods are also compared with Trajectron++ [7], LSTM-based human motion prediction methods [8],\na diffusion-based model [9] and a transformer-based model [10]\n.\nThe evaluation uses two real-world datasets: the ATC [11] and the Edinburgh dataset [12],\nboth capturing open indoor environments.\n Both datasets span multiple days and exhibit variations in human motion patterns throughout the day. Through this comparative study, we aim to evaluate the performance of spatio-temporal MoDs in the LHMP task.\n\nIn summary, we make the following contributions:\n\nWe extend CLiFF-LHMP by introducing a ranking method, enabling most-likely output predicted trajectory, improving its practical applicability in robotics.\n\nWe demonstrate how the framework can be instantiated with a range of different map representations, thus providing a general MoD-LHMP framework.\n\nWe introduce Time-Conditioned CLiFF-map, a temporal variant of CLiFF-map, to improve prediction accuracy.\n\nWe analyze how different instantiations of MoD-LHMP perform on two real-world datasets, compared with learning-based baselines.\n\n1. We extend CLiFF-LHMP by introducing a ranking method, enabling most-likely output predicted trajectory, improving its practical applicability in robotics.\n\n2. We demonstrate how the framework can be instantiated with a range of different map representations, thus providing a general MoD-LHMP framework.\n\n3. We introduce Time-Conditioned CLiFF-map, a temporal variant of CLiFF-map, to improve prediction accuracy.\n\n4. We analyze how different instantiations of MoD-LHMP perform on two real-world datasets, compared with learning-based baselines.",
            "llm_summary": "【论文的motivation是什么】  \n1. Long-term human motion prediction (LHMP) is essential for safe and efficient robot operation in human-shared environments.  \n2. Accurate trajectory predictions are crucial for applications like motion planning and human-robot interaction.  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. Prior work like CLiFF-LHMP utilized specific MoDs for trajectory prediction but lacked generalizability across different MoD types.  \n2. Existing methods often fail to account for temporal variations in human motion patterns over time.  \n\n【提出了什么创新的方法】  \n我们提出了一种MoD-informed LHMP框架，支持多种MoD类型，并引入了排名方法以输出最可能的预测轨迹。通过引入时间条件的CLiFF-map，我们能够捕捉不同时间段的运动模式。实验结果表明，MoD-informed方法在两个真实数据集上表现优于基于学习的方法，平均位移误差提高了50%，时间条件变体在整体准确性上达到了最高水平。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "HumanoidExo: Scalable Whole-Body Humanoid Manipulation via Wearable Exoskeleton",
            "authors": "Rui Zhong,Yizhe Sun,Junjie Wen,Jinming Li,Chuang Cheng,Wei Dai,Zhiwen Zeng,Huimin Lu,Yichen Zhu,Yi Xu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.03022",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.03022",
            "arxiv_html_link": "https://arxiv.org/html/2510.03022v1",
            "abstract": "A significant bottleneck in humanoid policy learning is the acquisition of large-scale, diverse datasets, as collecting reliable real-world data remains both difficult and cost-prohibitive. To address this limitation, we introduce HumanoidExo, a novel system that transfers human motion to whole-body humanoid data. HumanoidExo offers a high-efficiency solution that minimizes the embodiment gap between the human demonstrator and the robot, thereby tackling the scarcity of whole-body humanoid data. By facilitating the collection of more voluminous and diverse datasets, our approach significantly enhances the performance of humanoid robots in dynamic, real-world scenarios. We evaluated our method across three challenging real-world tasks: table-top manipulation, manipulation integrated with stand-squat motions, and whole-body manipulation. Our results empirically demonstrate that HumanoidExo is a crucial addition to real-robot data, as it enables the humanoid policy to generalize to novel environments, learn complex whole-body control from only five real-robot demonstrations, and even acquire new skills (i.e., walking) solely from HumanoidExo data.",
            "introduction": "Humanoid policy learning is a rapidly advancing field, now spanning locomotion, manipulation, and language-conditioned tasking. This progress is largely propelled by foundational model initiatives for general-purpose humanoids, such as Nvidia’s GR00T[27] and Figure AI. To mitigate the high cost of real-robot demonstrations, researchers have introduced several data-efficient pipelines. These include sim-to-real transfers, learning from web-scale human videos (e.g., EgoMimic[19], Vid2Robot[16]), and the development of diverse teleoperation systems for more effective data collection.\n\nDespite these efforts, scaling humanoid data collection remains a significant challenge for two primary reasons. First, simulation and human video data both suffer from severe embodiment gaps. Simulated robot dynamics inevitably mismatch their real-world counterparts, while the morphological and kinematic differences between humans and robots make direct video-to-policy transfer notoriously difficult. Second, direct teleoperation is difficult to scale. This approach typically requires a one-to-one, human-to-robot setup, which is expensive and resource-intensive. Furthermore, the process is physically and mentally demanding, leading to operator fatigue that limits the duration of collection sessions and requires highly skilled personnel. This reliance on expert operators and the inherent risk of damaging costly hardware during live sessions create a major bottleneck for generating large-scale datasets.\n\nIn this work, we introduce HumanoidExo, an integrated system that advances both data collection and policy learning for humanoid robotics. We utilize a custom-designed, lightweight, and flexible wearable exoskeleton[47] to capture human motion without impeding the operator’s natural movements. This design enables the comfortable performance of diverse daily tasks, while our system records and translates the operator’s actions into structured data for robot learning. To capture comprehensive, whole-body motion, a back-mounted LiDAR sensor tracks the operator’s torso, providing a 6D pose to record base movements such as walking, squatting, and bending. By fusing data from the exoskeleton and LiDAR, our system generates kinematically feasible, whole-body trajectories ready for large-scale policy learning.\n\nTo leverage this data, we present HumanoidExo-VLA (HE-VLA in short), a refined, vision-language-action model for whole-body humanoid policy learning from exoskeleton data. This hybrid approach uses imitation learning as a foundation and incorporates reinforcement learning to ensure the robot maintains balance and stability during movement and manipulation. The synergy between our hardware and software enables stable, efficient policy learning from exoskeleton data, leading to policies that are directly deployable on a physical humanoid robot.\n\nTo validate the effectiveness of our method, we conduct a comprehensive study across three challenging real-world tasks. These tasks include table-top manipulation, dexterous manipulation involving stand-squat motions, and whole-body manipulation that requires walking to a table. Our experimental results highlight the critical role of HumanoidExo data in enhancing policy performance: 1) Generalization: It enables the learned policy to generalize effectively to novel scenes and environments. 2) Data Efficiency: It allows an end-to-end model to learn complex tasks with as few as five real-robot demonstrations. 3) Skill Acquisition: It empowers the humanoid robot to acquire entirely new skills (e.g., walking) using only data from the exoskeleton, without any real-robot demonstrations. We believe that HumanoidExo represents a significant step toward achieving scalable whole-body humanoid policy learning.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的人形机器人政策学习面临数据收集困难，尤其是大规模多样化数据的获取。  \n2. 现有方法在缩小人类示范与机器人之间的体现差距方面存在不足。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有研究通过模拟和人类视频数据进行政策学习，但存在严重的体现差距。  \n2. 直接的遥操作方法难以扩展，且需要高技能操作员，导致数据收集效率低下。  \n\n【提出了什么创新的方法】  \n我们提出了HumanoidExo，一个集成系统，通过可穿戴外骨骼捕捉人类运动，生成结构化数据以供机器人学习。该系统结合了外骨骼和LiDAR数据，生成可用于大规模政策学习的全身运动轨迹。我们还提出了HumanoidExo-VLA（HE-VLA），结合模仿学习和强化学习，确保机器人在运动和操作过程中保持平衡和稳定。通过在三个具有挑战性的真实任务中进行验证，我们的方法显著提高了政策的泛化能力和数据效率，使得机器人能够从仅五个真实示范中学习复杂任务，并从外骨骼数据中获得新技能（如行走）。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "3D-CovDiffusion: 3D-Aware Diffusion Policy for Coverage Path Planning",
            "authors": "Chenyuan Chen,Haoran Ding,Ran Ding,Tianyu Liu,Zewen He,Anqing Duan,Dezhen Song,Xiaodan Liang,Yoshihiko Nakamura",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.03011",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.03011",
            "arxiv_html_link": "https://arxiv.org/html/2510.03011v1",
            "abstract": "Diffusion models, as a class of deep generative models, have recently emerged as powerful tools for robot skills by enabling stable training with reliable convergence. In this paper, we present an end-to-end framework for generating long, smooth trajectories that explicitly target high surface coverage across various industrial tasks, including polishing, robotic painting, and spray coating. The conventional methods are always fundamentally constrained by their predefined functional forms, which limit the shapes of the trajectories they can represent and make it difficult to handle complex and diverse tasks. Moreover, their generalization is poor, often requiring manual redesign or extensive parameter tuning when applied to new scenarios. These limitations highlight the need for more expressive generative models, making diffusion-based approaches a compelling choice for trajectory generation. By iteratively denoising trajectories with carefully learned noise schedules and conditioning mechanisms, diffusion models not only ensure smooth and consistent motion but also flexibly adapt to the task context. In experiments, our method improves trajectory continuity, maintains high coverage, and generalizes to unseen shapes, paving the way for unified end-to-end trajectory learning across industrial surface-processing tasks without category-specific models. On average, our approach\nimproves Point-wise Chamfer Distance by 98.2% and smoothness\nby 97.0%, while increasing surface coverage by 61% compared\nto prior methods. The link to our code can be found here.",
            "introduction": "Imitation learning [1] has emerged as a powerful paradigm in robotics, enabling agents to acquire complex skills directly from expert demonstrations rather than relying on costly manual programming [2, 3, 4]. This approach is particularly valuable for industrial domains, where tasks such as painting, coating, or surface finishing require long-horizon trajectories that are smooth, adaptive, and robust across diverse geometries. Traditional methods, however, are often constrained by pre-defined motion primitives or category-specific designs, which limit flexibility and generalization. The core challenge lies in managing the inherent complexity of free-form 3D inputs together with the high-dimensional outputs needed to specify complete robot programs. Robotic spray painting exemplifies this setting, as the robot must generate multiple trajectories to cover a surface, with each trajectory forming a distinct spatial path.\n\nDespite recent progress, existing learning-based solutions for industrial spray painting still face notable limitations. Many approaches rely on segment-wise trajectory prediction followed by heuristic concatenation, which often leads to locally inflexible paths[2] and suboptimal execution [5]—particularly when dealing with geometrically complex objects. Moreover, generalization remains constrained: separate training across object categories is often required [2], and even with joint training, performance on novel or highly heterogeneous shapes is limited. This reveals a fundamental gap: existing imitation-learning frameworks lack a unified, end-to-end solution that generates smooth, spatially coherent trajectories across diverse object categories.\n\nTo address these limitations, we propose a diffusion-based approach that enhances trajectory generation across diverse categories. Diffusion models [6, 7] have recently shown promise for imitation learning [8]: the iterative denoising process, applied over entire trajectories, implicitly preserves temporal continuity while capturing the multimodal distribution of expert behaviors. Our key insight is to directly generate trajectories end-to-end, conditioned on point clouds, enabling a single diffusion policy to generalize across categories (e.g., cuboids, windows, shelves, containers) without category-specific training. This reduces manual engineering effort and improves scalability and robustness in industrial settings. To further improve performance, we used the extended dataset introduced in MaskPlanner [5], which represents a follow-up work from the PaintNet group and provides a richer scene and more unified preprocessing.\n\nIn summary, we leverage diffusion models to directly produce smooth, spatially coherent paths conditioned on geometry and task constraints. In contrast to segment-wise prediction and heuristic stitching from the current method, our method performs end-to-end trajectory generation, improving continuity, generalization, and scalability. Our main contributions are:\n\nWe propose an end-to-end diffusion framework, augmented with a geometry-conditioned encoder, that produces smooth and spatially coherent trajectories by generating ordered segments that can be directly concatenated without heuristic sorting, in contrast to conventional piecewise approaches.\n\nWe introduce 3D point cloud inputs as the conditioning signal for diffusion policies, providing an expressive yet simple representation that exploits surface geometry and enables well-aligned trajectories.\n\nWe demonstrate that a single policy generates coherent 6‑DoF action sequences within our evaluation domain without additional retraining, exhibiting robust in‑domain generalization.\n\n1. We propose an end-to-end diffusion framework, augmented with a geometry-conditioned encoder, that produces smooth and spatially coherent trajectories by generating ordered segments that can be directly concatenated without heuristic sorting, in contrast to conventional piecewise approaches.\n\n2. We introduce 3D point cloud inputs as the conditioning signal for diffusion policies, providing an expressive yet simple representation that exploits surface geometry and enables well-aligned trajectories.\n\n3. We demonstrate that a single policy generates coherent 6‑DoF action sequences within our evaluation domain without additional retraining, exhibiting robust in‑domain generalization.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的模仿学习方法在生成复杂的工业任务轨迹时缺乏灵活性和通用性。  \n2. 传统方法受限于预定义的运动原语，难以处理复杂几何形状。  \n3. 需要一种统一的端到端解决方案，以生成平滑且空间一致的轨迹。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有学习方法往往依赖于分段轨迹预测，导致路径局部不灵活。  \n2. 许多方法在处理新形状时表现不佳，缺乏跨类别的通用性。  \n3. 现有模仿学习框架未能提供统一的端到端解决方案。  \n\n【提出了什么创新的方法】  \n我们提出了一种基于扩散模型的端到端框架，通过几何条件编码器生成平滑且空间一致的轨迹。该方法利用3D点云作为条件信号，能够生成有序段落，避免了传统方法中的启发式拼接问题。通过迭代去噪过程，我们的模型能够捕捉专家行为的多模态分布，实现了在不同类别上的通用性。实验结果显示，该方法在轨迹连续性、覆盖率和对未见形状的泛化能力上均有显著提升，平均提高了点对点Chamfer距离98.2%和光滑度97.0%，同时增加了61%的表面覆盖率。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Real-Time Nonlinear Model Predictive Control of Heavy-Duty Skid-Steered Mobile Platform for Trajectory Tracking Tasks",
            "authors": "Alvaro Paz,Pauli Mustalahti,Mohammad Dastranj,Jouni Mattila",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02976",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02976",
            "arxiv_html_link": "https://arxiv.org/html/2510.02976v1",
            "abstract": "This paper presents a framework for real-time optimal controlling of a heavy-duty skid-steered mobile platform for trajectory tracking. The importance of accurate real-time performance of the controller lies in safety considerations of situations where the dynamic system under control is affected by uncertainties and disturbances, and the controller should compensate for such phenomena in order to provide stable performance. A multiple-shooting nonlinear model-predictive control framework is proposed in this paper. This framework benefits from suitable algorithm along with readings from various sensors for genuine real-time performance with extremely high accuracy. The controller is then tested for tracking different trajectories where it demonstrates highly desirable performance in terms of both speed and accuracy. This controller shows remarkable improvement when compared to existing nonlinear model-predictive controllers in the literature that were implemented on skid-steered mobile platforms.",
            "introduction": "Mobile robots are a common type of dynamic systems that facilitate tasks in various fields, and their control has been an active research topic in recent decades [1]. While various types of mobile robots exist, the focus of this paper is on controlling a four-wheel skid-steered mobile platform. Use of four-wheels provides proper stability without the excess design complexity compared to using more wheels [2], while the skid steering mechanism allows for better mobility on uneven terrain, simpler mechanical design and handling larger payloads [3]. These benefits, while making this steering mechanism extremely popular for mobile robots, are significantly important for heavy-duty applications.\n\nOptimization-based control approaches, especially model-predictive control (MPC), have been an attractive topic for research in recent years as they allow for achieving proper tracking performance with optimal control effort, increasing efficiency. There is a trade-off concerning the choice between linear or nonlinear MPC approaches. Linear MPC provides computationally light solutions while its performance is limited by the nonlinear nature of many physical systems. On the other hand, nonlinear MPC (NMPC) provides reliable performance over a wider set of applications, but at the cost of being computationally heavy, challenging for real-time implementations. Therefore, it is crucial to choose optimization algorithms for NMPC with the aim of real-time implementation [4].\n\nThe multiple-shooting algorithm introduced by [5] allows for creating an NMPC framework to provide reliable robust optimal control performance, while yielding real-time implementation [6]. The real-time operation of the controller is of significant importance in the presence of effective uncertainties, including tire-ground interactions in the operation of heavy-duty mobile platforms. If the controller performance is much slower than the actual dynamics affecting the system’s motion, the controller may not compensate for these effects and therefore, instability may occur.\n\nThe efficient control with NMPC frameworks has demonstrated the high capabilities of NMPC for navigation tasks while including multiple sensors information and a wide variety of constraints and boundaries for legged robots [7, 8], while the control of skid-steered mobile robots, using various control approaches, has been the topic of many pieces of research, it has been in recent years that NMPC was utilized for controlling this robot type. The applications of NMPC in trajectory tracking of skid-steered mobile robots has been reported in works like [9] where, by combining estimation features with NMPC, they achieved competitive real-time computations for S-shaped trajectories in a conventional-size platform. Furthermore, [10] successfully included static obstacle avoidance in the NMPC for a small skid-steered platform with GPS localization. Also, for a better low-level controlling, [11] considered the actuator’s dynamics in a heavy-duty robot demonstrating its efficiency when tracking circular trajectories. More complex trajectories on flat terrain, e.g. Lemniscate, are reported by [12] for a small mobile manipulator where its robust NMPC is endowed with passivity features.\n\nDespite the proper implementation of these controllers in practical problems and the acceptable results, there is much room for improvement in terms of accuracy and speed. The accuracy is most important in scenarios such as confined or restricted work spaces, and the importance of computational speed is extremely noticeable in cases of fast dynamics. One remarkable risk regarding slow computation is the mismatch of estimations and the fast dynamics, which can cause imbalance in the system. Also, for the computational speed of the controller to be considered real-time, it must be fast enough to perform the required operations between the two consecutive sampling steps of the required sensors of rather high frequency. High accuracy and real-time performance of the controller reduces the risks of damage to the personnel, the system, and/or the working environment.\n\nThis work is targeted to heavy-duty skid-steered platforms which their inherent features, e.g. bulky, heavy and slow motion, must be considered when designing a proper NMPC controller. We assume that our platform performs slow motion, which is reasonable for a 6,000 [k​gkg] total weight robot with dimensions 3.7×\\times2.3 [mm]; and considering that it is equipped with a heavy-duty manipulator on top of it, thus fast motions can easily compromise its dynamic equilibrium resulting in overturning events.\n\nThen, we present a novel NMPC framework capable of tracking trajectories with high accuracy and real-time computation. Our NMPC is endowed with multiple features aiming at these two goals. We combine a multiple-shooting approach, that is fed with visual SLAM information, with wheel sensors’ information for better accuracy. Additionally, we consider a three-fold approach with smooth-and-continuous step functions in the robot’s dynamics for dealing with dead zones, which their effect is more considerable in heavy-duty robots due to the large inertia and friction. We also include a robust low-level controller for accurate tracking of the wheels’ velocity.\n\nIn terms of fast computation, we achieve high performance by fulfilling the criteria of a proper real-time implementation. First, a combination of warm-start solutions, high-rate sensors sampling (1 kHz), and bounded maximum number of iterations in the solver result in a strategy to generate an optimal solution before the next sensor measurement arrives, i.e. execution time lower than the fastest sensor rate. Also, we implement algorithmic routines for managing the sensor’s buffers. With this, we avoid unexpected data saturation and computational burden. These strategies endow our algorithm with deterministic computation time, which is another criterion for real-time implementations.\n\nTo demonstrate the fast computation of optimized solutions, we perform a test with millions of online samples and prove that 98 [%\\%] of our NMPC samples are executed around 1 [m​sms], see Fig. 5, which overcomes the times reported by [9]. Additionally, our time horizon is 3 times longer, i.e. NN = 30. We perform experiments with 3 trajectories, for which errors are reported in Table I. These are similar trajectories to the ones tested in [9, 10, 11, 12] while they reported position errors around 1-5 [mm] we obtained a maximum error of 6.2 [c​mcm] demonstrating the high accuracy of our NMPC. Our maximum velocity error is 5.0 [m​m/smm/s]. The limitation in our work is that, even though the accuracy and computational speed reported in these works are improved in this research, the results of the previous works were obtained from tests in higher velocities while our platform only admits slow velocities.\n\nThe structure of the paper for following parts is as follows. In Section II, the methodology used in this research is explained in parts. First, the dynamic system on which the controller is implemented is described. Then, the details of how the proposed solution of this research tackles the control objective are explained in detail. Section III comprises of the details on how the NMPC works for the problem at hand, while the experimental results of implementing the proposed controller is presented in Section IV. Finally, the analysis on how the proposed solution has handled the considered problem of real-time optimal trajectory tracking for a heavy-duty skid-steered mobile platform is concluded in Section V.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要提高重型滑移转向移动平台的轨迹跟踪精度。  \n2. 实现实时控制以应对动态系统中的不确定性和干扰。  \n3. 现有非线性模型预测控制（NMPC）方法在实时性和准确性方面存在不足。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有的NMPC方法在复杂环境下的实时性能不足。  \n2. 之前的研究未能充分考虑重型平台的动态特性和控制精度。  \n3. 现有控制方法在高速度下的表现良好，但在低速情况下的控制效果仍需改进。  \n\n【提出了什么创新的方法】  \n本研究提出了一种新型的NMPC框架，结合了多重发射算法和视觉SLAM信息，以提高轨迹跟踪的准确性和实时计算能力。该方法通过高频传感器采样和优化算法的结合，实现了在每次传感器测量之间快速生成最优解。实验结果显示，该NMPC在三条轨迹测试中表现出优异的性能，最大位置误差为6.2cm，最大速度误差为5.0mm/s，显著优于现有方法。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "AI-Enhanced Kinematic Modeling of Flexible Manipulators Using Multi-IMU Sensor Fusion",
            "authors": "Amir Hossein Barjini,Jouni Mattila",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02975",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02975",
            "arxiv_html_link": "https://arxiv.org/html/2510.02975v1",
            "abstract": "This paper presents a novel framework for estimating the position and orientation of flexible manipulators undergoing vertical motion using multiple inertial measurement units (IMUs), optimized and calibrated with ground truth data. The flexible links are modeled as a series of rigid segments, with joint angles estimated from accelerometer and gyroscope measurements acquired by cost-effective IMUs. A complementary filter is employed to fuse the measurements, with its parameters optimized through particle swarm optimization (PSO) to mitigate noise and delay. To further improve estimation accuracy, residual errors in position and orientation are compensated using radial basis function neural networks (RBFNN). Experimental results validate the effectiveness of the proposed intelligent multi-IMU kinematic estimation method, achieving root mean square errors (RMSE) of 0.00021 m, 0.00041 m, and 0.00024 rad for yy, zz, and θ\\theta, respectively.",
            "introduction": "Flexible manipulators, characterized by lightweight construction and low energy consumption, have attracted considerable attention in diverse domains such as aerospace [1], industrial automation, and medical robotics [2]. However, their inherent elasticity and the resulting vibrations introduce strong nonlinearities into the kinematic behavior [3], which is further influenced by factors such as payload, motion speed, and boundary conditions. Therefore, the development of a robust and industry-compatible framework for accurate position and orientation estimation of flexible manipulators remains an essential challenge.\n\nNumerous approaches have been investigated in the literature for the dynamic modeling of flexible manipulators [4], ranging from infinite-dimensional formulations based on partial differential equations (PDEs), such as those derived from Hamilton’s principle [5, 6], to finite-dimensional methods including the finite element method (FEM) [7]. The kinematics of the system, i.e., the position and orientation of each segment, are subsequently obtained by solving the governing dynamic equations, which remains a non-trivial task. For instance, non-homogeneous boundary conditions induced by gravitational effects complicate the derivation of accurate mode shapes [8]. Furthermore, solving these equations can be computationally intensive and may not consistently produce results that align with experimental observations [9]. In addition, many control strategies rely on measurements of the flexible link’s end-effector position or elastic deflections [10]. Hence, the accurate estimation of the position and orientation of flexible manipulators remains a critical challenge, motivating further research in this domain.\n\nVarious methods have been proposed for obtaining the states of a flexible link, which generally fall into two categories: model-based state observers and sensor-based state measurements. Model-based observers estimate system states from limited measurements; however, their accuracy strongly depends on the fidelity of the underlying model, and parameter uncertainties can lead to significant estimation errors in practical applications [11]. Moreover, even observer-based approaches require measurement inputs to function effectively [12].\nAlternatively, sensor-based state measurement techniques for flexible manipulators primarily employ strain gauges [13] or inertial measurement units (IMUs) [14]. Strain gauges, however, often suffer from reduced reliability under extreme environmental conditions and may degrade during long-term dynamic operation. IMUs, while widely adopted, are susceptible to integration drift, and their accelerometers and gyroscopes are noise-sensitive, requiring advanced filtering and sensor fusion methods to achieve accurate state estimation [15, 16].\n\nIn the operation of flexible manipulators, elastic deformations and vibrations strongly affect the accuracy of end-effector motion control. This challenge is further amplified in heavy-duty flexible manipulators, where gravitational forces play a significant role [17]. Moreover, precise end-effector state information is essential for the implementation of PDE-based control strategies in flexible manipulators [10].\nA major concern in kinematic estimation lies in both the cost and reliability of sensing technologies under diverse operating conditions. For instance, while laser trackers can provide highly accurate measurements, their high cost limits widespread industrial adoption. Similarly, although cost-efficient, strain gauges often suffer from reduced reliability in harsh environments (e.g., temperature extremes, high humidity, mechanical shock, and dust).\nTherefore, this research is motivated by the need to develop a methodology for the kinematic estimation of flexible manipulators that is accurate, reliable, and cost-effective, while remaining suitable for industrial deployment.\n\nAs highlighted in the literature, multi-IMU systems have been employed for end-effector kinematic estimation; however, the effect of filter parameters on estimation noise and temporal delay remains underexplored, and systematic parameter optimization is still an open research problem [18, 19]. Furthermore, although recent studies have applied IMUs for kinematic estimation in flexible manipulators, time-varying errors between the estimated and actual end-effector positions persist [20, 9], highlighting the need for improved refinement and correction strategies.\n\nThe primary contributions of this research are summarized as follows:\n\nDevelopment of an AI-augmented IMU-based kinematic estimation framework applicable to rigid, flexible, and soft robots, enabling accurate end-effector position and orientation estimation in vertical motions.\n\nOptimization of complementary filter parameters using Particle Swarm Optimization (PSO) to minimize sensor noise and delay effects in flexible links.\n\nCompensation of residual position and orientation estimation errors in flexible links through a Radial Basis Function Neural Network (RBFNN), trained using laser tracker data as ground truth.\n\nThe remainder of this paper is organized as follows. Section II presents the IMU-based kinematic modeling approach. Section III covers the complementary filtering method for sensor fusion, while Section IV describes its PSO-based parameter optimization. Section V introduces an RBFNN-based model for residual error correction using ground-truth data. Experimental results are provided in Section VI, and conclusions and future directions are given in Section VII.\n\n1. Development of an AI-augmented IMU-based kinematic estimation framework applicable to rigid, flexible, and soft robots, enabling accurate end-effector position and orientation estimation in vertical motions.\n\n2. Optimization of complementary filter parameters using Particle Swarm Optimization (PSO) to minimize sensor noise and delay effects in flexible links.\n\n3. Compensation of residual position and orientation estimation errors in flexible links through a Radial Basis Function Neural Network (RBFNN), trained using laser tracker data as ground truth.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要开发一种准确、可靠且成本效益高的柔性操纵器运动学估计方法。  \n2. 现有的多IMU系统在估计噪声和时间延迟方面的参数优化仍未得到充分研究。  \n3. 现有方法在柔性操纵器的状态估计中存在时间变化的误差。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 许多方法已被提出用于柔性操纵器的动态建模，但计算复杂性和模型准确性仍然是挑战。  \n2. 模型基础的状态观察者依赖于模型的准确性，且参数不确定性可能导致显著的估计误差。  \n3. 传感器基础的状态测量技术（如IMU）对噪声敏感，需要先进的过滤和传感器融合方法。  \n\n【提出了什么创新的方法】  \n1. 开发了一种AI增强的IMU基础运动学估计框架，适用于刚性、柔性和软体机器人，能够在垂直运动中实现准确的末端执行器位置和方向估计。  \n2. 使用粒子群优化（PSO）优化互补滤波器参数，以最小化柔性链中的传感器噪声和延迟影响。  \n3. 通过使用激光跟踪器数据作为真实值训练的径向基函数神经网络（RBFNN）来补偿柔性链中的残余位置和方向估计误差。  \n实验结果验证了所提出的智能多IMU运动学估计方法的有效性，分别在yy、zz和θ方向上实现了0.00021 m、0.00041 m和0.00024 rad的均方根误差（RMSE）。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "YawSitter: Modeling and Controlling a Tail-Sitter UAV with Enhanced Yaw Control",
            "authors": "Amir Habel,Fawad Mehboob,Jeffrin Sam,Clement Fortin,Dzmitry Tsetserukou",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02968",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02968",
            "arxiv_html_link": "https://arxiv.org/html/2510.02968v1",
            "abstract": "Achieving precise lateral motion modeling and decoupled control in hover remains a significant challenge for tail-sitter Unmanned Aerial Vehicles (UAVs), primarily due to complex aerodynamic couplings and the absence of well-defined lateral dynamics. This paper presents a novel modeling and control strategy that enhances yaw authority and lateral motion by introducing a sideslip force model derived from differential propeller slipstream effects acting on the fuselage under differential thrust. The resulting lateral force along the body yy-axis enables yaw-based lateral position control without inducing roll coupling.",
            "introduction": "Tail-sitter Vertical Take-off and Landing (VTOL) fixed-wing Unmanned Aerial Vehicles (UAVs) combine the endurance of fixed-wing aircraft with vertical takeoff and landing capabilities, eliminating the need for a runway. Their simple design consists of a flying wing with large control surfaces and thrusters, but no vertical tail. This design enables transitions from vertical hover to level flight, where lift shifts from thrust to the fixed-wing, making them ideal for applications like surveillance, inspection, and payload delivery in confined or remote areas. Tail-sitters are categorized into mono-thrust transitioning (MTT), differential thrust transitioning (DTT), and collective thrust transitioning (CTT) types, as detailed in [1]. This study focuses on the CTT type, which uses coupled thrust and control surface actions.\n\nTail-sitter UAVs exhibit nonlinear and time-varying dynamics, particularly during hover-to-level transitions, posing significant modeling and control challenges [9]. The exact 6-DOF rigid body dynamics and aerodynamic effects are difficult to formulate, especially in the lateral direction. A rigorous dynamical model is found in [3], with a detailed analysis of aerodynamic effects presented in [2] using a component breakdown approach.\n\nSimilarly, in [17], the dynamics are rigorously defined, and the problem of lateral motion control is mentioned but not fully addressed. Some studies are also focused on using data-driven approaches addressing nonlinear dynamics in post stall regions [4], but lateral motion control is missing. Moreover, the dynamical model are often further simplified while implementing control [3].\n\nEffective control of these underactuated systems is critical. Nonlinear quaternion-based cascaded PID controllers, comprising position, attitude, and thrust components, are common, as seen in [5, 7, 3]. Hierarchical PID with mode-switching [8] and smooth attitude control [6] address transition challenges. However, poor yaw authority and absent y-direction forces limit sideways motion and yaw control. Recent studies propose solutions like L1 neural network-based adaptive control [13], optimal transitioning frameworks [9], Dynamic Front and Back Transition Corridors [14], successive linearization MPC [15], unified cascaded PID frameworks [16], and incremental nonlinear dynamic inversion [17]. Moreover, in [18], Lyapunov-based control is used for 2D hovering to address uncertainties like gusts.\n\nDespite these advances, inadequate yaw authority remains a limitation. This study presents a dual-thruster, no-stabilizer CTT tail-sitter UAV, enhancing yaw authority in both dynamics and control while retaining a cascaded PID framework, addressing the prior shortcomings.",
            "llm_summary": "【论文的motivation是什么】  \n1. 提高尾坐式无人机的偏航控制能力。  \n2. 解决尾坐式无人机在悬停和水平飞行转换中的建模与控制挑战。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 前人工作主要集中在非线性动态建模和控制，但对侧向运动控制的研究不够全面。  \n2. 虽然已有多种控制方法被提出，但偏航控制能力仍然不足，限制了侧向运动的实现。  \n\n【提出了什么创新的方法】  \n本研究提出了一种新颖的建模和控制策略，通过引入基于差分推力的侧滑力模型，增强了尾坐式无人机的偏航控制能力。该方法结合了双推力器设计和级联PID控制框架，解决了以往研究中的不足。通过该方法，尾坐式无人机在动态和控制方面的偏航能力得到了显著提升，能够更有效地进行悬停和水平飞行的转换。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Single-Rod Brachiation Robot: Mechatronic Control Design and Validation of Prejump Phases",
            "authors": "Juraj Lieskovský,Hijiri Akahane,Aoto Osawa,Jaroslav Bušek,Ikuo Mizuuchi,Tomáš Vyhlídal",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "Accepted 27 July 2025, Available online 16 Sept 2025, Version of Record 28 Sept 2025",
            "pdf_link": "https://arxiv.org/pdf/2510.02946",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02946",
            "arxiv_html_link": "https://arxiv.org/html/2510.02946v1",
            "abstract": "A complete mechatronic design of a minimal configuration brachiation robot is presented. The robot consists of a single rigid rod with gripper mechanisms attached to both ends. The grippers are used to hang the robot on a horizontal bar on which it swings or rotates. The motion is imposed by repositioning the robot’s center of mass, which is performed using a crank-slide mechanism. Based on a non-linear model, an optimal control strategy is proposed, for repositioning the center of mass in a bang-bang manner. Consequently, utilizing the concept of input-output linearization, a continuous control strategy is proposed that takes into account the limited torque of the crank-slide mechanism and its geometry. An increased attention is paid to energy accumulation towards the subsequent jump stage of the brachiation. These two strategies are validated and compared in simulations. The continuous control strategy is then also implemented within a low-cost STM32-based control system, and both the swing and rotation stages of the brachiation motion are experimentally validated.",
            "introduction": "Brachiation is a form of motion used by primates to move from one branch to another. Research into mimicking it with robots has previously been conducted mostly using multi-link mechanisms. The work by Fukuda et al. [1], where a six-link model of a brachiation robot was proposed and analyzed. In [2] a two-link brachiating robot was developed, its motion being realized using heuristic control. A control system for this two-link robot was proposed in [3] and torque time series minimizing the energy consumption of a two-link brachiation robot moving over a flexible cable was calculated in [4].\n\nIn this paper, we focus on analysis, control optimization, and experimental validation of a minimal configuration of a brachiation robot. This extends the preliminary work presented by a part of the team in a conference paper [5], where a novel single-rod robot that uses an aerial phase in its motion was proposed and constructed. By periodically repositioning its center of mass while swinging or later rotating around a bar, the aim is to evoke sufficient energy to jump from one horizontal bar (branch) to another. The desired cycle of motion can be separated into four distinct phases; see Fig. 1. In the first two phases, energy is accumulated in the system, first during a swinging motion and secondly during rotation. The third phase is dedicated to preparation for the fourth phase, which is initiated by the robot releasing the bar, during which the robot spans the distance to the next bar. The phase ends by grasping the other horizontal bar and restarting the cycle. The general objective is to achieve locomotion of the robot between the bars. If the distances of bars are not equal, a different amount of energy must be determined and accumulated during the pre-jump phases for each distance. Note also that the distance between two neighboring bars must be greater than the length of the robot.\n\nCompared to conventional brachiation robots with two-link arms with elbow joints, the advantage of the proposed single-rod robot is that it does not have the inherent chaotic dynamics of a serial multi-link pendulum. In terms of practical applications, it can be used as a means for moving through space on a pre-installed ladder-like structure. In the case of inspections of high-voltage lines, a single-rod brachiation robot can maintain its position without expending energy as long as it is holding a wire and is relatively robust to wind and external disturbances. Although the robot can only be used where there is a structure that can be grasped, it could be used for inspection work, maintenance work, and photography in spaces with overhead structures.\n\nThe preliminary work [5] was focused on the first phase of the motion. Repositioning of the center of mass was performed by a feedforward policy, i.e. without any feedback from the angular position of the rod. Denoting the robot swing frequency as Ω\\Omega, repositioning of the center of mass is to be performed with the frequency 2​Ω2\\Omega. This proposed feedforward control is suitable for small swing angles, where the frequency Ω\\Omega is (almost) constant. However, considering the physical pendulum-like nature of the robot, the oscillation frequency slightly reduces with growing amplitude of the swing. Thus, in the long run, this ideal to true frequency mismatch is likely to lead to loss of synchronization of the center-of-mass repositioning during the swing stage. Similarly, the synchronization loss can also be caused by disturbances acting on the robot, e.g. the effect of wind. In order to move the research closer to the applications, it was necessary to turn the feedforward control policy into a feedback policy, which can handle this synchronization imperfection through feedback from the swing angle measurement. The first attempts in this direction were presented in subsequent conference publication of the authors’ team [6]. Considering a simplified model of a rod with moving mass (free of the actuator dynamics), we proposed the feedback control of the first two phases of motion. It was done by utilizing the results on an analogous problem, a pendulum’s swing and revolution. Such a swinging problem occurs e.g. in modeling children on a swing, [7, 8]. The oscillations of a pendulum with a periodically varying length were studied in [9] with the key objective of determining the existence of periodic solutions. The stability of such periodic solutions was studied in [10, 11, 12]. The time-invariant control law to pump appropriate energy into the variable-length pendulum for achieving the desired swing motion was developed in [13, 14]. A nonlinear feedback strategy to control the periodic motion of the pendulum has also been proposed in [15] with a consideration of energy harvesting from the rotational motion.\n\nAn increased attention has also been paid to a related problem of using the pendulum length adjustment to dampen the pendulum swing. The open-loop solution derived in [16] through energy analysis was turned by a part of the team into a practical closed-loop solution by introducing nonlinear time-delay feedback in [17]. The theoretical results are followed by comprehensive laboratory validation. In subsequent works, [18], [19] and [20] the Lyapunov method was applied to derive the nonlinear control rule to damp the pendulum swing.\nIn [21], the efficiency of amplitude suppression of an oscillating pendulum by a controllable moving mass was studied by simulations for several suppression rules. The problem was further studied, and its results were experimentally validated in [22]. Let us also point to an analogous problem by the author’s team studied in [23], where the pendulum length is kept fixed, and its angular motion is damped by an up-and-down motion of the pivot.\n\nIn [20], in addition to the design and analysis of Lyapunov-based control rules, a numerical study was performed to determine the optimal solution. For the damping of the pendulum, it leads to a bang-bang length variation, where the pendulum is stepwise prolonged when passing the (equilibrium) zero-angle position and stepwise shortened at the turning-angle positions. Although this damping problem is inverse to the first phase of the motion of the brachiation robot considered here, the results can be applied well if the direction of motion of the center of mass is reversed. This idea was applied and elaborated further in [6] for the first stage of the brachiation motion, and was also adapted for the second stage of motion.\n\nBeyond the preliminary results presented in the conference papers [5] and [6], and beyond the state of the art, the contribution of this paper is as follows:\n\nCompared to [5], the construction of the brachiation robot is adjusted to allow model-based validation of the swing and rotation stages of the brachiation motion.\nIn addition, the control system and measurement hardware are redesigned to allow experimental validation of feedback control in the swing and rotation stages.\n\nThe feedback control policies proposed conceptually in [6] for a mathematical model of a rod with moving mass are adapted for the experimental setup of the brachiation robot.\n\nFor the objective of the control design, a precise mathematical model of the robot is derived and parameterized, including the submodel of the crank-slide mechanism used to move the center of mass (not considered in [6]).\n\nNext to the validation of the control design concepts in the mathematical model, for the first time, the experimental validation of the swing and rotation stages of the brachiation by a single-rod robot is performed.\n\nThe remainder of the paper is structured as follows. In Section II, utilizing the results of [6], we define the optimal motion of the center of mass and the two targeted phases of single-rod robot’s brachiation.\nIn Section III we present the experimental setup for its validation, including its construction and hardware adjustments compared to [5]. In Section IV a precise nonlinear model of the setup is derived and parameterized. The ideal limit case and practically applicable continuous control policies are proposed in Section V using the robot’s model. In Section VI a thorough case study validation is performed. It includes a simulation-based validation of both control policies, followed by an experimental validation of the continuous control policy. The main results and research prospects are provided in the concluding Section VII.\n\n1. Compared to [5], the construction of the brachiation robot is adjusted to allow model-based validation of the swing and rotation stages of the brachiation motion.\nIn addition, the control system and measurement hardware are redesigned to allow experimental validation of feedback control in the swing and rotation stages.\n\n2. The feedback control policies proposed conceptually in [6] for a mathematical model of a rod with moving mass are adapted for the experimental setup of the brachiation robot.\n\n3. For the objective of the control design, a precise mathematical model of the robot is derived and parameterized, including the submodel of the crank-slide mechanism used to move the center of mass (not considered in [6]).\n\n4. Next to the validation of the control design concepts in the mathematical model, for the first time, the experimental validation of the swing and rotation stages of the brachiation by a single-rod robot is performed.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的多链接摆动机器人在控制和稳定性方面存在挑战。  \n2. 需要一种新型的、简化的机器人设计以提高控制精度和能量效率。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 之前的研究主要集中在多链接机器人，未充分探索单杆设计的潜力。  \n2. 现有控制策略多为开环控制，缺乏反馈机制，导致在动态环境中表现不佳。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于非线性模型的反馈控制策略，旨在优化单杆机器人的摆动和旋转阶段。通过重新设计控制系统和测量硬件，首次实现了对摆动和旋转阶段的实验验证。此外，构建了精确的数学模型，包括用于移动重心的曲柄滑块机制的子模型。实验结果表明，所提出的反馈控制策略显著提高了机器人的运动稳定性和能量效率，能够有效应对外部扰动并保持同步。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Metrics vs Surveys: Can Quantitative Measures Replace Human Surveys in Social Robot Navigation? A Correlation Analysis",
            "authors": "Stefano Trepella,Mauro Martini,Noé Pérez-Higueras,Andrea Ostuni,Fernando Caballero,Luis Merino,Marcello Chiaberge",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02941",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02941",
            "arxiv_html_link": "https://arxiv.org/html/2510.02941v1",
            "abstract": "Social, also called human-aware, navigation is a key challenge for the integration of mobile robots into human environments. The evaluation of such systems is complex, as factors such as comfort, safety, and legibility must be considered. Human-centered assessments, typically conducted through surveys, provide reliable insights but are costly, resource-intensive, and difficult to reproduce or compare across systems. Alternatively, numerical social navigation metrics are easy to compute and facilitate comparisons, yet the community lacks consensus on a standard set of metrics.",
            "introduction": "Human-aware robot navigation is a key research area for integrating mobile robots into human environments [1, 2]. Beyond the classical challenges of path planning and obstacle avoidance, human-aware navigation must address qualitative aspects of social interaction, such as comfort, predictability, and personal space, which are difficult to capture with mathematical models [3, 4].\n\nPrecisely, these qualitative aspects make the evaluation of human-aware navigation complex. In addition to traditional performance metrics (e.g. time, distance), several quantitative metrics for social navigation have been proposed [5, 6]. These metrics are easy to compute in controlled experiments and enable efficient comparisons between approaches. However, adopting a wide set of contrastive measures is often misleading in clearly understanding the quality of an experiment. Furthermore, there is no agreement on a set of metrics that reliably capture social aspects such as comfort or legibility. Such aspects are typically assessed through participant surveys, which, while informative, are costly, time-consuming, and challenging to scale.\n\nThis work considers the question of whether it is possible to infer the qualitative aspects from quantitative metrics that can be measured in experiments. We approach the problem by analyzing the correlations of quantitative evaluations and qualitative human judgment from surveys in experiments with real robots and human participants. Our goal is to identify quantitative metrics that align with human perceptions, paving the way for simpler, standardized quantitative evaluations that reduce the reliance on surveys. A schematic of the proposed framework is illustrated in Fig. 1. The correlation analysis is performed by combining K-means clustering and statistical approaches to identify the sets of quantitative metrics strongly related to human evaluations. An overall scores comparison is finally carried out to demonstrate that the set of metrics identified through correlation can obtain human-like evaluation trends for social navigation.\n\nThe contributions of this work can therefore be summarized as follows:\n\nA novel framework to perform a joint analysis of surveys and quantitative metrics, offering insights on the correlations between them through clustering and statistical tests.\n\nA novel framework to perform a joint analysis of surveys and quantitative metrics, offering insights on the correlations between them through clustering and statistical tests.\n\nThe identification of a subset of quantitative metrics that results most relevant to obtain human-like evaluation trends for social navigation experiments.\n\nA dataset of robot and people trajectories in eight different social scenarios collected through real world experiments with an accurate ground truth and survey evaluation, that can be used for development and benchmarking of future research.\n\nThe correlation analysis framework, the dataset, and the survey questionnaire are available in the repository111https://github.com/PIC4SeR/Social-Nav-Metrics-Matching.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的人类评估方法（如调查）在社会机器人导航中成本高、资源密集且难以比较。  \n2. 社会导航的定量度量缺乏共识，难以准确捕捉人类感知的定性方面。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统的性能指标（如时间、距离）未能有效捕捉社会交互的定性特征。  \n2. 现有的定量度量虽然易于计算，但缺乏标准化，导致对实验质量的理解不清晰。  \n\n【提出了什么创新的方法】  \n提出了一种新的框架，通过聚类和统计测试对调查和定量度量进行联合分析，识别与人类评估高度相关的定量指标。该方法结合K均值聚类和统计方法，能够有效识别与人类感知一致的定量指标，从而简化社会导航的评估过程。最终的结果表明，所识别的定量指标集能够获得与人类评估趋势相似的结果，推动了社会导航领域的研究。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Point Cloud-Based Control Barrier Functions for Model Predictive Control in Safety-Critical Navigation of Autonomous Mobile Robots",
            "authors": "Faduo Liang,Yunfeng Yang,Shi-Lu Dai",
            "subjects": "Robotics (cs.RO)",
            "comment": "accepted to IROS2025",
            "pdf_link": "https://arxiv.org/pdf/2510.02885",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02885",
            "arxiv_html_link": "https://arxiv.org/html/2510.02885v1",
            "abstract": "In this work, we propose a novel motion planning algorithm to facilitate safety-critical navigation for autonomous mobile robots. The proposed algorithm integrates a real-time dynamic obstacle tracking and mapping system that categorizes point clouds into dynamic and static components. For dynamic point clouds, the Kalman filter is employed to estimate and predict their motion states. Based on these predictions, we extrapolate the future states of dynamic point clouds, which are subsequently merged with static point clouds to construct the forward-time-domain (FTD) map. By combining control barrier functions (CBFs) with nonlinear model predictive control, the proposed algorithm enables the robot to effectively avoid both static and dynamic obstacles. The CBF constraints are formulated based on risk points identified through collision detection between the predicted future states and the FTD map. Experimental results from both simulated and real-world scenarios demonstrate the efficacy of the proposed algorithm in complex environments. In simulation experiments, the proposed algorithm is compared with two baseline approaches, showing superior performance in terms of safety and robustness in obstacle avoidance. The source code is released for the reference of the robotics community.",
            "introduction": "Research on safety-critical optimal planning and control for autonomous mobile robots has been actively pursued over the past decades. Significant progress has been made in collision avoidance methods for both static and dynamic environments, with related algorithms becoming increasingly mature. However, reliable and efficient collision avoidance in environments containing both static (e.g., tables, shelves) and dynamic (e.g., pedestrians) obstacles remains a significant challenge.\n\nA primary challenge in mobile robot navigation lies in accurately characterizing obstacles. Existing works [1, 2, 3, 4] typically model obstacles as ellipsoids through clustering and generate optimal trajectories under obstacle avoidance constraints. However, such approaches can lead to a certain degree of conservativeness [5]. Firstly, density-based clustering methods like DBSCAN [6] are sensitive to variations in local data density and require careful tuning of parameters (e.g., eps and minPts), limiting their adaptability to high-dimensional spaces [4]. Secondly, representing obstacles as ellipsoids reduces the available state space for robots [7]. This approach overestimates obstacle volumes by including irrelevant regions, such as hollow spaces, particularly for irregularly shaped obstacles, which restricts robot movement. Such over-approximation significantly impacts the feasibility and optimality of motion planning in complex environments. Recent studies [5, 8, 9] have proposed several important methods to characterize obstacles using point cloud data, but these methods still struggle to handle complex and dynamic environments effectively.\n\nJointly solving motion planning and control under perception constraints also presents substantial difficulties. Nonlinear model predictive control (NMPC) addresses these challenges by integrating planning and control into a unified constraint optimization framework [2]. However, despite its theoretical appeal, most approaches primarily use NMPC as an optimal controller, underutilizing its predictive capabilities and limiting their effectiveness in real-world applications. Recently, control barrier functions (CBFs) have emerged as a widely adopted method for enforcing safety constraints in obstacle avoidance. In [8], surface geometry was employed to locally define and synthesize a quadratic CBF over point cloud data. A MPC-DCBF framework was presented in [2] to address dynamic obstacle avoidance. However, these existing CBF-based methods still face challenges in simultaneously handling both dynamic and static obstacles.\n\nTo address these challenges, we propose a novel framework that integrates NMPC with point cloud-based CBFs to simultaneously handle both static and dynamic obstacles under real-time perception constraints. The contributions of our work are summarized as follows:\n\nWe develop a point cloud-based CBF-NMPC algorithm that integrates planning and control, enabling robots to generate safe and collision-free trajectories in unknown and dynamic environments.\n\nWe develop a point cloud-based CBF-NMPC algorithm that integrates planning and control, enabling robots to generate safe and collision-free trajectories in unknown and dynamic environments.\n\nWe propose a heuristic method for identifying high-risk collision points using the forward-time-domain (FTD) map and the predictive capabilities of NMPC. This method effectively characterizes potential collisions and formulates CBFs for enhanced safety.\n\nWe conduct experiments in both Gazebo simulations and real-world scenarios. The results demonstrate the effectiveness and robustness of our safety-critical obstacle avoidance algorithm in unstructured environments with static and dynamic obstacles.",
            "llm_summary": "【论文的motivation是什么】  \n1. 可靠高效的碰撞避免在静态和动态障碍物共存的环境中仍然是一个重大挑战。  \n2. 现有方法在处理复杂和动态环境时效果不佳，限制了移动机器人的应用。  \n3. 需要一种新的框架来集成规划和控制，以应对实时感知约束下的障碍物。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有方法通常将障碍物建模为椭球体，导致过于保守的碰撞避免策略。  \n2. 现有的动态障碍物检测方法在复杂环境中表现不佳，难以有效处理动态障碍物。  \n3. CBF-based方法在同时处理静态和动态障碍物时仍面临挑战，缺乏对未来状态的预测能力。  \n\n【提出了什么创新的方法】  \n我们提出了一种新颖的点云基础CBF-NMPC算法，该算法将规划与控制集成，能够在未知和动态环境中生成安全且无碰撞的轨迹。该方法通过前向时间域（FTD）地图识别高风险碰撞点，并利用NMPC的预测能力制定CBF，从而增强安全性。实验结果表明，该算法在复杂环境中的安全性和鲁棒性优于现有方法，能够有效应对静态和动态障碍物的挑战。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Novel UWB Synthetic Aperture Radar Imaging for Mobile Robot Mapping",
            "authors": "Charith Premachandra,U-Xuan Tan",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted and presented at the 15th International Conference on Indoor Positioning and Indoor Navigation (IPIN) 2025, seethis https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.02874",
            "code": "https://ipin-conference.org/2025/",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02874",
            "arxiv_html_link": "https://arxiv.org/html/2510.02874v1",
            "abstract": "Traditional exteroceptive sensors in mobile robots, such as LiDARs and cameras often struggle to perceive the environment in poor visibility conditions. Recently, radar technologies, such as ultra-wideband (UWB) have emerged as potential alternatives due to their ability to see through adverse environmental conditions (e.g. dust, smoke and rain). However, due to the small apertures with low directivity, the UWB radars cannot reconstruct a detailed image of its field of view (FOV) using a single scan. Hence, a virtual large aperture is synthesized by moving the radar along a mobile robot path. The resulting synthetic aperture radar (SAR) image is a high-definition representation of the surrounding environment. Hence, this paper proposes a pipeline for mobile robots to incorporate UWB radar-based SAR imaging to map an unknown environment. Finally, we evaluated the performance of classical feature detectors: SIFT, SURF, BRISK, AKAZE and ORB to identify loop closures using UWB SAR images. The experiments were conducted emulating adverse environmental conditions. The results demonstrate the viability and effectiveness of UWB SAR imaging for high-resolution environmental mapping and loop closure detection toward more robust and reliable robotic perception systems.",
            "introduction": "Radar frequency bands are preferred over visible (e.g. camera) and near-visible bands (e.g. LiDAR) due to their ability to penetrate through adverse environmental conditions, such as smoke, dust and rain [1]. In this context, ultra-wideband (UWB) radar exhibits excellent penetration properties attributed to its high frequency components. When it comes to obtain indoor close-range measurements, UWB radar demonstrates high Signal-to-Noise Ratio (SNR) and low power consumption compared to other radar technologies (e.g. continuous wave radar) [2].\n\nRecently, UWB radars have been incorporated in mobile robotics for mapping in challenging environments [3, 4]. Those systems have been proposed to replace conventional LiDAR and Camera-based systems. The raw UWB radar observations provide the reflected waveform from the surroundings as a timeseries. The amplitudes (i.e. reflection intensities) of the raw waveform indicate the size or material of the objects in the radar’s Field of View (FOV). Generally, the small aperture in UWB radar modules result in a large FOV thus affecting the spatial resolution and directionality. Hence, existing UWB radar-based maps consist of either extracted features (e.g. points and lines) using several observations, or used the entire waveform as a visual template corresponding to the locations in the environment [5, 6, 7]. Conversely, LiDAR and camera modules provide a feature-rich observation through a single scan. Those information are often integrated with grid maps for a better representation of the environment.\n\nHowever, the aperture of the UWB radar can be artificially expanded by moving the radar sensor along a predefined linear [8] or circular [9] fixed path. The radar observations are collected relative to the known poses to generate a high-resolution representation of the environment called: Synthetic Aperture Radar (SAR) image. Each pixel intensity represents the occupancy of objects in the surroundings. The features of objects with large radar cross-sections (RCS) appear brighter and vice versa.\nThere are several algorithms to generate SAR images using UWB radar observations, such as optical algorithm [10], range migration algorithm [11] and back-projection algorithm. When it comes to SAR imaging along a free path, back-projection algorithm is preferred over the others due to its flexibility in accounting for both the position and orientation of the radar system [8, 9]. Hence, back-projection is the backbone of SAR imaging in this study.\n\nMeanwhile, feature extraction and matching using feature detectors have been widely utilized in the context of vision-based applications. Feature detection has been one of the fundamental components in most of the visual SLAM algorithms, especially for loop closure (e.g. VINS Mono [12], ORB-SLAM [13]). Thus, several studies have analysed strengths and limitations of these feature detectors, and have suggested most versatile detector in their respective application domains [14, 15]. However, in contrast to SAR images, RGB camera outputs are rich in distinctive features, which facilitates more reliable feature detection. Hence, this paper evaluates the effectiveness of conventional feature detectors when applied to SAR images and examines whether their performance is consistent with that observed in RGB images.\n\nThe contributions of this paper are as follows:\n\nWe propose a complete pipeline to generate UWB SAR images using state-of-the-art UWB radar modules to perform environmental mapping;\n\nWe discuss the feasibility of using feature extraction and description algorithms: SIFT, SURF, BRISK, AKAZE and ORB on SAR images to perform loop closures;\n\nWe publicly share our experiment datasets and code within the ROS2 framework to support future research.111https://github.com/CPrem95/uwb_sar\n\n1. We propose a complete pipeline to generate UWB SAR images using state-of-the-art UWB radar modules to perform environmental mapping;\n\n2. We discuss the feasibility of using feature extraction and description algorithms: SIFT, SURF, BRISK, AKAZE and ORB on SAR images to perform loop closures;\n\n3. We publicly share our experiment datasets and code within the ROS2 framework to support future research.111https://github.com/CPrem95/uwb_sar",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的传感器在恶劣环境下难以有效感知周围环境。  \n2. UWB雷达在复杂环境中具有潜在的优势，但其空间分辨率受限。  \n3. 需要一种新的方法来生成高分辨率的环境映射。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统的LiDAR和相机在恶劣天气条件下表现不佳。  \n2. 现有的UWB雷达系统通常依赖于多次观测来提取特征，导致效率低下。  \n3. 现有的SAR成像方法尚未充分探索UWB雷达在移动机器人中的应用。  \n\n【提出了什么创新的方法】  \n本研究提出了一种完整的流程，通过移动UWB雷达生成合成孔径雷达（SAR）图像，以进行环境映射。该流程包括雷达观测的获取、信号预处理和图像重建，利用后投影算法生成高分辨率的SAR图像。此外，评估了多种特征检测算法（如SIFT、SURF、BRISK等）在SAR图像中的表现，以实现闭环检测。实验结果表明，UWB SAR成像在高分辨率环境映射和闭环检测方面具有良好的可行性和有效性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Action Deviation-Aware Inference for Low-Latency Wireless Robots",
            "authors": "Jeyoung Park,Yeonsub Lim,Seungeun Oh,Jihong Park,Jinho Choi,Seong-Lyun Kim",
            "subjects": "Robotics (cs.RO); Distributed, Parallel, and Cluster Computing (cs.DC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02851",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02851",
            "arxiv_html_link": "https://arxiv.org/html/2510.02851v1",
            "abstract": "To support latency-sensitive AI applications ranging from autonomous driving to industrial robot manipulation, 6G envisions distributed ML, connecting distributed computational resources in edge and cloud over hyper-reliable low-latency communication (HRLLC). In this setting, speculative decoding can facilitate collaborative inference of models distributively deployed: an on-device draft model locally generates drafts and a remote server-based target model verifies and corrects them, resulting lower latency. However, unlike autoregressive text generation, behavior cloning policies, typically used for embodied AI applications like robot manipulation and autonomous driving, cannot parallelize verification and correction for multiple drafts as each action depends on observation which needs to be updated by a previous action. To this end, we propose Action Deviation-Aware Hybrid Inference, wherein the draft model estimates an action’s need for verification and correction by the target model and selectively skips communication and computation for server operations. Action deviation shows a strong correlation with action’s rejection probability by the target model, enabling selective skipping. We derive the path deviation threshold that balances the transmission rate and the inference performance, and we empirically show that action deviation-aware hybrid inference reduces uplink transmission and server operation by 40%, while lowering end-to-end latency by 33.32% relative to hybrid inference without skipping and achieving task success rate up to 97.03% of that of target model only inference.",
            "introduction": "As Artificial Intelligence (AI) and the Internet of Things (IoT) proliferate, cellular infrastructure will play a critical role in supporting latency-sensitive applications, ranging from distributed systems as cooperative autonomous driving to local tasks such as on-device intelligence. However, these applications are not effectively supported by 5G due to its inability to reliably maintain low latency and integrate distributed computing resources [1]. To overcome the practical constraints of 5G in supporting on-device intelligence, the 6G infrastructure envisions enabling latency as low as a hundred microseconds, data rates of up to 1 Terabit per second, and highly scalable connectivity to support hundreds of thousands of devices. By distributing computation across devices and servers connected with networks, AI applications can meet latency deadlines that centralized clouds often cannot, while computation-intensive tasks can be offloaded to servers [2].\n\nWith hyper-reliable low-latency communication (HRLLC) that 6G aims to support [3], AI applications can be distributively deployed across mobile devices, edge servers, and cloud centers; device-server collaborative inference for large language models (LLMs) and transformers can be facilitated with speculative decoding [4] to accelerate inference and reduce communication and computational overhead [5]. Speculative decoding reduces inference latency by having more computationally efficient lightweight models—draft models—continuously generate draft tokens, while a more capable larger model—target model—verifies and corrects them in parallel through speculative sampling. This approach is advantageous as the draft and target models can each be deployed to an edge device and a server respectively, thus efficiently distributing computational resources, and it also prevents excessive communication as the target model only requires the draft model’s outputs, unlike split learning necessitating the transmission of high-dimensional hidden states [5].\n\nWhile foundation models, notably LLMs and Vision Language Models (VLMs) with billions of parameters, are anticipated to benefit from the distributed architecture and the 6G’s vision of HRLLC [6], collaborative inference can also be adopted for control policies in robotics, used for mapping actions from states and observations. For instance, behavior cloning frameworks with transformer architectures such as PerAct [7] generate probability distributions over tokens that represent actions for a given observation, to which speculative sampling can be applied [4]. Behavior cloning, which is a form of imitation learning [8], refers to learning robotic skills from expert demonstrations as a supervised learning so that a learned policy can output actions such as joint torques and end-effector poses from observations including camera images and sensor readings [9]. While behavior cloning’s use cases range from autonomous driving [10] to robot manipulation [11] and Unmanned Aerial Vehicle (UAV) control [12], policies are almost always required to meet strict end-to-end latency of less than a second [5]. Those for time sensitive platforms such as industrial manipulators and autonomous vehicles often have requirements of no more than tens of milliseconds in terms of latency [5]. Considering that 6G envisions distributed computing and intelligence and that robotic policies implemented with behavior cloning are inherently on-device, applying collaborative inference with edge devices and servers interconnected by 6G wireless networks would enable meeting both stringent latency and reliability requirements for such applications.\n\nHowever, while promising, a key challenge to this approach is that speculative decoding cannot directly be applied to the transformers for manipulation tasks. It can straightforwardly accelerate inference for text generation with a target model reviewing the multiple draft tokens in parallel, but this cannot be the case for robotic policies. Unlike language models, which are autoregressive and can reuse their outputs as inputs, action-generation models cannot generate multiple drafts simultaneously because each subsequent action depends on updated observations from executing the previous one.\n\nBut what if the on-device draft model can predict whether a certain draft token needs to be verified and corrected by a target model or not? That way, before transmitting, it can predict if the action is likely to be corrected by the target model and transmit only when the draft action requires verification to minimize incurring unnecessary communication and computational overhead. Interestingly, this has already been done in the domain of natural language processing. Uncertainty-Aware Hybrid Language Model (U-HLM) [13, 14, 15] uses uncertainty—an on-device small language model’s (SLM) self-assessed confidence in its outputs with a strong linear relationship with the rejection probability at the server; U-HLM skips uplink transmission for draft tokens with low uncertainty, thus lowering inference latency from repetitive computation and transmission without compromising the accuracy.\n\nTo this end, we present Action Deviation-Aware Hybrid Inference (ADAHI), an architecture to facilitate collaborative inference for action generation with selective speculative sampling. Specifically, given that the draft model must be able to independently evaluate the necessity of speculative sampling at server, we devise Action Deviation for estimating the rejection probability for the generated action. Motivated by mean reversion in finance, we compute action deviation by measuring the divergence of the current action from the exponential moving average of the past ones, and we show that it has a strong correlation with the rejection probability at the server. We then apply selective speculative sampling for actions generated by Vector-Quantized Behavior Transformer (VQ-BeT) [16]—a transformer-based architecture for behavior cloning that generates residual embedding vectors for a given observation and reconstruct their sum into a continous action. As illustrate in Figure 1, a portion of the sampled embedding vectors with high action deviation is transmitted for speculative sampling at server, incurring minimal computational and communication overhead. Through experiments over different use cases ranging from robotic manipulations to swarm control, we demonstrate that our method preserves a target model’s performance and efficiency in action generation, while significantly reducing latency invoked by communication and computation.\n\nAddressing the absence of framework for collaborative inference in robotics, we envision action generation with distributed computational resources interconnected with low latency wireless networks as a key enabler of embodied on-device intelligence. Our contributions in this work are as follow:\n\nTo the best of our knowledge, we are first to apply speculative sampling for robotic motion control, introducing the ADAHI framework that facilitates collaborative inference under latency and accuracy constraints.\n\nWe devise and propose action deviation, a new metric for measuring the rejection probability of draft tokens generated by the on-device VQ-BeT draft model by the server-side VQ-BeT target model.\n\nWe implement a physical testbed with wireless communication between an edge device that deploys a draft model and a remote server hosting a target model.\n\n1. To the best of our knowledge, we are first to apply speculative sampling for robotic motion control, introducing the ADAHI framework that facilitates collaborative inference under latency and accuracy constraints.\n\n2. We devise and propose action deviation, a new metric for measuring the rejection probability of draft tokens generated by the on-device VQ-BeT draft model by the server-side VQ-BeT target model.\n\n3. We implement a physical testbed with wireless communication between an edge device that deploys a draft model and a remote server hosting a target model.",
            "llm_summary": "【论文的motivation是什么】  \n1. 核心问题1：如何在低延迟无线网络中实现机器人控制的协同推理。  \n2. 核心问题2：现有的行为克隆策略无法有效并行化多个草稿的验证与修正。  \n3. 核心问题3：如何减少通信和计算开销以满足严格的延迟要求。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有的5G网络无法支持低延迟和分布式计算资源的有效整合。  \n2. 现有的行为克隆方法未能实现对动作生成的并行验证，限制了其在实时应用中的有效性。  \n3. 尽管有一些自然语言处理领域的研究采用了不确定性来跳过低置信度的传输，但在机器人控制中尚未得到应用。  \n\n【提出了什么创新的方法】  \n我们提出了“动作偏差感知混合推理”（ADAHI）框架，允许草稿模型预测动作是否需要目标模型的验证，从而选择性地跳过不必要的通信和计算。通过计算当前动作与过去动作的偏差，我们能够有效地估计被拒绝的概率。该方法在多个机器人操作任务中表现出显著的性能提升，减少了40%的上行传输和33.32%的端到端延迟，同时在任务成功率上达到了目标模型推理的97.03%。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Assist-as-needed Control for FES in Foot Drop Management",
            "authors": "Andreas Christou,Elliot Lister,Georgia Andreopoulou,Don Mahad,Sethu Vijayakumar",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02808",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02808",
            "arxiv_html_link": "https://arxiv.org/html/2510.02808v1",
            "abstract": "Foot drop is commonly managed using Functional Electrical Stimulation (FES), typically delivered via open-loop controllers with fixed stimulation intensities. While users may manually adjust the intensity through external controls, this approach risks overstimulation, leading to muscle fatigue and discomfort, or understimulation, which compromises dorsiflexion and increases fall risk. In this study, we propose a novel closed-loop FES controller that dynamically adjusts the stimulation intensity based on real-time toe clearance, providing “assistance as needed”. We evaluate this system by inducing foot drop in healthy participants and comparing the effects of the closed-loop controller with a traditional open-loop controller across various walking conditions, including different speeds and surface inclinations. Kinematic data reveal that our closed-loop controller maintains adequate toe clearance without significantly affecting the joint angles of the hips, the knees, and the ankles, and while using significantly lower stimulation intensities compared to the open-loop controller. These findings suggest that the proposed method not only matches the effectiveness of existing systems but also offers the potential for reduced muscle fatigue and improved long-term user comfort and adherence.",
            "introduction": "Foot drop is a common symptom after neurological damage that can limit the mobility of patients and negatively impact their quality of life. Due to compromised spinal cord pathways, and/or muscle function, foot drop can manifest itself in a wide range of conditions such as stroke, multiple sclerosis, cerebral palsy and traumatic injury [1]. As a result, there is an increased risk of falling that can limit the confidence of walking independently and can significantly reduce gait speed.\n\nFunctional electrical stimulation (FES) has shown great promise in preventing foot drop and facilitating independent ambulation in people experiencing foot drop [2]. Using non-invasive electrodes and low-intensity electrical impulses, FES can externally activate the patient’s inert muscles to support gait and prevent foot drop. This offers an affordable and lightweight solution to manage foot drop that can have a significant positive impact in people’s daily living.\n\nHowever, the way FES is currently being used lacks personalisation and relies on the user’s conscious input in order to adjust the stimulation intensity. In most cases, commercial devices can differentiate between the stance and the swing during gait, either using pressure sensors or inertial measurement units, and use this information to turn FES ON or OFF at the intensity selected by the therapist or the user [1]. As a result, higher or lower than needed stimulation may be provided, which may either unnecessarily fatigue the muscles or put the user at risk of falling, respectively.\n\nThis has led to the investigation of alternative control systems that can modulate the intensity of stimulation. Proportional control has been studied in order to regulate stimulation intensity based on electromyography (EMG) [3, 4] or the error between desired and actual movement [5, 6], where the desired movement is defined based on the average ankle joint kinematics of healthy controls. Other EMG-informed controllers have also been tested where pre-defined stimulation patterns were used for the different gait phases, based on the EMG activity observed during the gait of healthy controls [7]. Iterative learning control [8, 9] and repetitive control [10], which refine stimulation based on previous gait cycles and previous samples, respectively, have also been studied, but the performance of these methods is highly dependent on the periodicity of gait. Model predictive control (MPC) and data-driven controllers have also been explored [11, 12] but these methods can be computationally expensive and may suffer from model inaccuracies or limited generalisability, respectively.\n\nWhat has not been studied extensively is how FES can be reliably used in daily life where the user may frequently change their gait speed or may walk on terrain of variable inclination. It is evident that human biomechanics can significantly change depending on these conditions [13, 14] and it is prudent that our FES devices take this into consideration to ensure comfort and safety.\n\nIn this paper we present a novel method for controlling FES using toe clearance as an input in order to provide assistance as needed. We propose a closed-loop controller that adjusts the stimulation intensity in real time to ensure adequate toe clearance during gait at different speeds and slope. It is hypothesised that our proposed controller will more effectively adjust the intensity of stimulation to guarantee safety and minimise stimulation compared to an open-loop control system where the stimulation profile is kept constant. Our contributions include: (1) the development of a novel closed-loop FES controller, (2) the use of toe clearance as an input which is an underutilised yet more direct and informative predictor of gait safety and stability than metrics like ankle orientation, as it directly reflects the risk of stumbling, (3) the controller’s validation on healthy participants, and (4) its comparison to commercial open-loop FES control (Figure 1). We also provide valuable kinematic data obtained from healthy controls walking at different speeds and slopes, recorded using 3D motion capture technology, and analysed using musculoskeletal modelling.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的功能电刺激（FES）控制系统缺乏个性化，依赖用户手动调整刺激强度。  \n2. 传统的开环控制方法可能导致过度或不足刺激，增加肌肉疲劳或跌倒风险。  \n3. 需要一种能够实时调整刺激强度的控制系统，以适应用户在不同步态和地形条件下的需求。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有研究主要集中在基于EMG或运动误差的比例控制，但未充分考虑用户日常生活中的变化。  \n2. 迭代学习控制和模型预测控制方法的性能依赖于步态的周期性，缺乏灵活性。  \n3. 现有方法未能有效利用用户的步态变化，特别是在不同速度和地形条件下的适应性。  \n\n【提出了什么创新的方法】  \n我们提出了一种新颖的闭环FES控制器，利用实时的脚趾间隙作为输入，动态调整刺激强度以提供“按需辅助”。该控制器能够在不同速度和坡度下确保足够的脚趾间隙，减少肌肉疲劳并提高用户的舒适度和依从性。通过在健康参与者身上的实验，我们的闭环控制器在保持足够脚趾间隙的同时，使用的刺激强度显著低于传统开环控制器。实验结果表明，该方法在安全性和有效性上优于现有系统，具有良好的应用前景。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving",
            "authors": "Yifan Liao,Zhen Sun,Xiaoyun Qiu,Zixiao Zhao,Wenbing Tang,Xinlei He,Xinhu Zheng,Tianwei Zhang,Xinyi Huang,Xingshuo Han",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02803",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02803",
            "arxiv_html_link": "https://arxiv.org/html/2510.02803v1",
            "abstract": "Visual Language Models (VLMs), with powerful multimodal reasoning capabilities, are gradually integrated into autonomous driving by several automobile manufacturers to enhance planning capability in challenging environments.\nHowever, the trajectory planning capability of VLMs in work zones, which often include irregular layouts, temporary traffic control, and dynamically changing geometric structures, is still unexplored.\nTo bridge this gap, we conduct the first systematic study of VLMs for work zone trajectory planning, revealing that mainstream VLMs fail to generate correct trajectories in 68.0% of cases.\nTo better understand these failures, we first identify candidate patterns via subgraph mining and clustering analysis, and then confirm the validity of 8 common failure patterns through human verification.\nBuilding on these findings, we propose 𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive}, a trajectory planning framework that integrates VLMs with Retrieval-Augmented Generation (RAG). Specifically,\n𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive} leverages VLMs to convert prior failure cases into constraint rules and executable trajectory planning code, while RAG retrieves similar patterns in new scenarios to guide trajectory generation.\nExperimental results on the ROADWork dataset show that REACT-Drive yields a reduction of around 3×3\\times in average displacement error relative to VLM baselines under evaluation with Qwen2.5-VL.\nIn addition, 𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive} yields the lowest inference time (0.580.58s) compared with other methods such as fine-tuning (17.9017.90s).\nWe further conduct experiments using a real vehicle in 15 work zone scenarios in the physical world, demonstrating the strong practicality of 𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive}.",
            "introduction": "Autonomous driving has progressed rapidly, with both academia and industry devoting extensive efforts to vehicles that can operate in urban and highway environments with minimal or no human supervision.\nOwing to the strong performance of deep learning in pattern recognition and large-scale data processing, significant progress has been achieved in safety DBLP:journals/csur/JahanSNA19 , trajectory planning leon2021review , and visual perception DBLP:journals/spm/ChenCCTRK20 , with increasing applications in real-world scenarios ApolloGo_official ; MBUSA_DrivePilot_manual .\nAlthough autonomous driving has demonstrated increasingly reliable performance in structured environments such as urban roads and highways, recent incidents highlight its pronounced limitations in complex and dynamic settings such as work zones.\nWork zone crashes remain a significant safety concern in roadway transportation, with nearly 100,000 incidents occurring annually in the United States and more than 40,000 individuals injured each year penndot2019ads .\nIn 2017, a Tesla Model S Autopilot failed to recognize road signs and crossed a temporary barrier Tesla_Autopilot_Dallas2017 .\nIn 2023, a Cruise robotaxi entered an active construction site Cruise_FireTruckCollision2023 .\nIn 2025, a Xiaomi SU7 failed to decelerate when approaching a highway construction site under intelligent driving assistance, resulting in three fatalities Xiaomi_SU7FatalAccident2025 .\nA common feature of these accidents is that they all occurred when autonomous vehicles encountered work zones.\nWork zones are characterized by long-tail and highly dynamic conditions, such as irregular layouts, which pose significant challenges for autonomous driving systems.\n\nVision Language Models (VLMs) combine strong visual perception and language understanding capabilities.\nWith their advantage in zero-shot transfer, they have been shown to address complex road planning problems in autonomous driving zhou2024vision .\nAccording to recent reports, several automotive companies, including Li Auto Inc. LiAuto2024DeliveryUpdate  and Geely Geely2025NewsRelease , have begun integrating VLMs into their autonomous driving systems.\nHowever, our study shows that current VLMs are inadequate for trajectory planning in work zone scenarios.\nFor instance, mainstream VLMs like Qwen2.5-VL DBLP:journals/corr/abs-2502-13923  achieve an FDE of only 285.90285.90 on the ROADWork dataset ROADWork , in contrast to 106.38106.38 on the normal commonsense driving cases in NuScenes nuscene .\n\nTo better understand the abnormal behaviors, we filter out those abnormal scenarios (images) where all VLMs fail to provide the correct path.\nWe then follow a three-step analysis framework to discover the main causes of VLM failures.\nWe first construct scene graphs from different abnormal scenarios.\nBased on these graphs, we conduct abnormal subgraph mining and candidate merging, then apply clustering and inflection-point analysis to identify 1010 representative patterns.\nFinally, we manually summarize and verify these patterns, which reveal 88 main patterns of VLM failures in work zone scenarios.\nBased on the summarized abnormal patterns, we propose a two-stage framework: Retrieval-Enhanced And Constraint-verified Trajectory for Driving (𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive}).\nIn the offline stage, failure cases are converted into constraint rules expressions and executable trajectory mitigation code.\nA self-verification mechanism is applied to ensure their usability, resulting in a searchable failure case mitigation code database.\nIn the online stage, we use a retrieval-augmented generation (RAG) pipeline: the current scenario is encoded as a query to retrieve cases matching the failure patterns, and the associated historical mitigation code is then executed.\nThis process guides the autonomous driving system to generate trajectories that comply with safety requirements and traffic rules.\nExperimental results demonstrate that 𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive} improves trajectory planning performance in work zones in both effectiveness and efficiency.\nFor example, when evaluated with Qwen2.5-VL-72B as the backbone model, 𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive} achieves an around 3×3\\times reduction in average displacement error compared with VLM baselines.\nFurthermore, we perform physical evaluation with data collected from 1515 real-world work zones, which shows that 𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive} consistently achieves lower error.\nWe envision that 𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive} can shed light on future research in improving work zone driving ability via VLMs.\n\nOverall, we make the following contributions:\n\nWe perform the first evaluation of VLM-based driving systems on the trajectory planning task in work zones, revealing that current VLMs struggle significantly with this task.\n\nThrough scene graph mining and human verification, we conduct abnormal pattern analysis on failure cases and summarize 88 typical failure patterns of VLM-based driving systems.\nWe then propose a two-stage framework 𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive} that leverages constraint rules and RAG to reuse mitigation code for safe trajectory planning in new scenarios.\n\nEvaluations show that 𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive} significantly reduces trajectory prediction errors for around 3×3\\times and planning overhead to 0.58​s0.58s. In addition, we conduct physical experiments in real-world work zone scenarios using data collected from autonomous driving vehicles, which further validate the effectiveness of 𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive}.\n\n1. We perform the first evaluation of VLM-based driving systems on the trajectory planning task in work zones, revealing that current VLMs struggle significantly with this task.\n\n2. Through scene graph mining and human verification, we conduct abnormal pattern analysis on failure cases and summarize 88 typical failure patterns of VLM-based driving systems.\nWe then propose a two-stage framework 𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive} that leverages constraint rules and RAG to reuse mitigation code for safe trajectory planning in new scenarios.\n\n3. Evaluations show that 𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive} significantly reduces trajectory prediction errors for around 3×3\\times and planning overhead to 0.58​s0.58s. In addition, we conduct physical experiments in real-world work zone scenarios using data collected from autonomous driving vehicles, which further validate the effectiveness of 𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive}.",
            "llm_summary": "【论文的motivation是什么】  \n1. 当前的视觉语言模型（VLMs）在复杂的工作区场景下进行轨迹规划的能力不足。  \n2. 工作区的动态和不规则布局对自动驾驶系统构成重大挑战，导致安全隐患。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有研究未系统评估VLM在工作区轨迹规划中的表现。  \n2. 现有方法未能识别和分析VLM在工作区场景中的失败模式。  \n\n【提出了什么创新的方法】  \n提出了一种名为𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾\\mathsf{REACT\\text{-}Drive}的轨迹规划框架，该框架结合了VLM和检索增强生成（RAG）。该方法通过将先前失败案例转化为约束规则和可执行的轨迹规划代码，利用RAG在新场景中检索相似模式以指导轨迹生成。实验结果表明，𝖱𝖤𝖠𝖢𝖳​-​𝖣𝗋𝗂𝗏𝖾显著降低了轨迹预测误差，减少了约3倍的平均位移误差，并且在15个真实工作区场景中的物理实验验证了其强大的实用性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Flow with the Force Field: Learning 3D Compliant Flow Matching Policies from Force and Demonstration-Guided Simulation Data",
            "authors": "Tianyu Li,Yihan Li,Zizhe Zhang,Nadia Figueroa",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02738",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02738",
            "arxiv_html_link": "https://arxiv.org/html/2510.02738v1",
            "abstract": "While visuomotor policy has made advancements in recent years, contact-rich tasks still remain a challenge. Robotic manipulation tasks that require continuous contact demand explicit handling of compliance and force. However, most visuomotor policies ignore compliance, overlooking the importance of physical interaction with the real world, often leading to excessive contact forces or fragile behavior under uncertainty. Introducing force information into vision-based imitation learning could help improve awareness of contacts, but could also require a lot of data to perform well. One remedy for data scarcity is to generate data in simulation, yet computationally taxing processes are required to generate data good enough not to suffer from the Sim2Real gap. In this work, we introduce a framework for generating force-informed data in simulation, instantiated by a single human demonstration, and show how coupling with a compliant policy improves the performance of a visuomotor policy learned from synthetic data. We validate our approach on real-robot tasks, including non-prehensile block flipping and a bi-manual object moving, where the learned policy exhibits reliable contact maintenance and adaptation to novel conditions. Project Website: flow-with-the-force-field.github.io.",
            "introduction": "Contact-rich robotic manipulation tasks demand a delicate balance between precise motion control and compliant force regulation. Mechanical compliance is crucial for successful contact interactions. Visuomotor policies, which are control policy representations that map raw visual observations to motor actions, have emerged as the leading robot learning paradigm for manipulation tasks due to their ease of specification and multi-modal capabilities. Yet, state-of-the-art approaches often ignore compliance and focus only on positional accuracy [1, 2, 3]. Recent advances have highlighted that incorporating compliance or force feedback can drastically improve performance in tasks like object flipping or wiping, where fixed-stiffness controllers fail to handle varying contact conditions [4, 5]. Some learn variable stiffness profiles from human demonstrations via diffusion models [4, 6], while others use reinforcement learning or trajectory optimization to tune compliance [7, 8].\n\nWhile these methods achieve strong performance, they typically demand substantial human effort. For instance, ACP [4] requires hundreds of real demonstrations, and RL or trajectory optimization methods need carefully designed reward functions and extensive data or training. Consequently, current compliance policies lack scalability due to intensive physical data collection or intricate reward engineering. In contrast, our pipeline significantly reduces these burdens by generating force-informed simulation data from a single demonstration, potentially automating compliant policy learning for continuous, contact-rich tasks.\n\nIn this work, we address the challenge of learning vision-force adaptive compliance policies from simulation-generated data instantiated by a single human demonstration. The single demonstration is collected in a simulation environment via teleoperation [9], removing the need for a real robot setup.\nIn particular, we propose a lightweight, yet effective, data generation strategy that produces diverse behaviors from a single demonstration in a simulator via force-informed trajectory modulation [10, 11, 4, 5] and Laplacian editing [12, 13, 14]. These synthetic trajectories are used to train an imitation policy that conditions on 3D pointcloud observations, similar to [15, 16], end effector pose, and force measurements. The policy outputs task-specific passive impedance parameters that can be executed by a low-level compliance controller on real robots. To learn this complex and multi-modal sensorimotor mapping, we leverage the flow-matching approach [17, 18, 3], allowing high-frequency inference. We introduce using point clouds and force input for the flow matching policy.\n\nWhile a policy that outputs good trajectories is important, the rollout controller is equally critical for ensuring robustness and safety in the real world, especially under the Sim2Real gap. Thus, we encode the policy rollout poses into a state‑velocity field, which, during execution, is coupled with a Passive Impedance Controller [19] that dampens deviations from the desired velocity. Unlike position‑based controllers that rigidly follow waypoints and often generate abrupt contact forces, this velocity‑based approach reduces energy injection with softer contact, as with state-dependent dynamical system (DS) policies [20]. Combined visuo-force as an input and the compliant vector as the output, this framework reduces the risk of damage due to misalignment while maintaining good performance.\n\nTo summarize, we propose a framework for learning adaptive compliant vision‑based policies from simulation data, consisting of three major components, including i) generating force‑informed sim data, ii) policy learning with flow matching, and iii) safe rollout on real hardware with state-velocity fields. Our contributions are fourfold:\n\nPropose a lightweight, yet effective, force‑informed data generation strategy in simulation with virtual targets and Laplacian editing for vision-based imitation learning.\n\nWe design an Adaptive Compliant Flow Matching policy that uses point cloud and force as inputs and outputs pose actions and impedance parameters.\n\nWe demonstrate zero-shot transfer of our point cloud-based policy to real Franka robots on tasks like box flipping and bimanual grasping, without any real-world demonstrations or sim2real transfer algorithm.\n\nWe design a scheme for generating a vector field from policy pose rollouts, enabling passive impedance controller to carry out the compliant policy on real robots with better performance and lower energy injection.\n\n1. Propose a lightweight, yet effective, force‑informed data generation strategy in simulation with virtual targets and Laplacian editing for vision-based imitation learning.\n\n2. We design an Adaptive Compliant Flow Matching policy that uses point cloud and force as inputs and outputs pose actions and impedance parameters.\n\n3. We demonstrate zero-shot transfer of our point cloud-based policy to real Franka robots on tasks like box flipping and bimanual grasping, without any real-world demonstrations or sim2real transfer algorithm.\n\n4. We design a scheme for generating a vector field from policy pose rollouts, enabling passive impedance controller to carry out the compliant policy on real robots with better performance and lower energy injection.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的visuomotor策略在处理接触丰富的任务时表现不佳，尤其是缺乏对合规性和力的处理。  \n2. 生成高质量的模拟数据以克服Sim2Real差距的过程计算量大且复杂。  \n3. 现有方法通常需要大量的人类演示，缺乏可扩展性。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 一些方法通过人类演示学习可变刚度，但需要大量真实数据。  \n2. 现有的强化学习和轨迹优化方法依赖复杂的奖励设计和大量训练数据。  \n3. 许多方法仍然依赖真实或遥控数据，缺乏对合规性和接触调节的有效处理。  \n\n【提出了什么创新的方法】  \n我们提出了一种框架，通过单一的人类演示生成力信息丰富的模拟数据，结合合规策略来改善从合成数据学习的visuomotor策略。该方法包括三个主要组件：i) 在模拟中生成力信息数据，ii) 使用流匹配进行策略学习，iii) 在真实硬件上安全执行。我们的方法通过轻量级的轨迹修改技术生成多样化行为，利用点云和力作为输入，输出任务特定的被动阻抗参数。我们的策略在真实的Franka机器人上实现了零-shot转移，成功完成了如箱子翻转和双手抓取等任务，展示了在接触维护和适应新条件方面的可靠性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation - Technical Report for IROS 2025 RoboSense Challenge Track 4",
            "authors": "Lingfeng Zhang,Erjia Xiao,Yuchen Zhang,Haoxiang Fu,Ruibin Hu,Yanbiao Ma,Wenbo Ding,Long Chen,Hangjun Ye,Xiaoshuai Hao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02728",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02728",
            "arxiv_html_link": "https://arxiv.org/html/2510.02728v1",
            "abstract": "Cross-modal drone navigation remains a challenging task in robotics, requiring efficient retrieval of relevant images from large-scale databases based on natural language descriptions. The RoboSense 2025 Track 4 challenge addresses this challenge, focusing on robust, natural language-guided cross-view image retrieval across multiple platforms (drones, satellites, and ground cameras).\nCurrent baseline methods, while effective for initial retrieval, often struggle to achieve fine-grained semantic matching between text queries and visual content, especially in complex aerial scenes.\nTo address this challenge, we propose a two-stage retrieval refinement method: Caption-Guided Retrieval System (CGRS) that enhances the baseline coarse ranking through intelligent reranking. Our method first leverages a baseline model to obtain an initial coarse ranking of the top 20 most relevant images for each query. We then use Vision-Language-Model (VLM) to generate detailed captions for these candidate images, capturing rich semantic descriptions of their visual content. These generated captions are then used in a multimodal similarity computation framework to perform fine-grained reranking of the original text query, effectively building a semantic bridge between the visual content and natural language descriptions. Our approach significantly improves upon the baseline, achieving a consistent 5% improvement across all key metrics (Recall@1, Recall@5, and Recall@10). Our approach win TOP-2 in the challenge, demonstrating the practical value of our semantic refinement strategy in real-world robotic navigation scenarios.\n\n\n Welcome to 2025 RoboSense Challenge!",
            "introduction": "Cross-modal drone navigation represents a key advancement in autonomous robotics, enabling drones to understand and execute navigation instructions derived from natural language descriptions [5, 43, 17, 18, 16]. This technology has profound implications for applications ranging from disaster management and search and rescue operations to autonomous surveillance and environmental monitoring. Unlike traditional GPS navigation systems, natural language-guided drone navigation requires a deep understanding of spatial relationships, visual semantics, and cross-modal matching between textual descriptions and aerial imagery [9, 44, 41].\nThe fundamental challenge in this field lies in bridging the semantic gap between human-language descriptions and visual representations captured from aerial perspectives. Drone-viewed imagery exhibits unique characteristics, including significant viewpoint variations, scale differences, and complex spatial arrangements that differ significantly from ground-based imagery. These factors render traditional image retrieval methods inadequate for real-world drone navigation applications, where accuracy and reliability are crucial [37, 5].\n\nExisting cross-modal retrieval methods for drone navigation primarily rely on end-to-end learning frameworks that directly map textual queries to visual features. The GeoText-1652 [5] baseline method proposed by Chu et al. represents the current state of the art in this area. It employs a multimodal framework that combines an image encoder (Swin Transformer), a text encoder (BERT) [7], and a cross-modal attention mechanism with spatially aware learning via hybrid spatial matching. Despite these advances, this baseline method still faces inherent limitations in terms of semantic precision and retrieval accuracy. Direct mapping from textual descriptions to visual embeddings often fails to capture subtle semantic relationships, especially when multiple visually similar objects are present in aerial scenes.\n\nTo overcome these limitations, we propose CGRS (Caption-Guided Retrieval System), a novel two-stage retrieval refinement framework that improves semantic precision through intelligent reranking.\nSpecifically, CGRS operates through a carefully designed two-stage pipeline. In the first stage, we leverage the proven effectiveness of the GeoText-1652 baseline to perform an initial coarse retrieval, obtaining the top 20 most relevant candidate images for each text query. This coarse ranking leverages the baseline’s strengths in spatially-aware feature learning and cross-modal alignment, while providing a manageable subset of candidate images for further refinement.\nThe second stage represents our key innovation: we employ a Vision-Language-Model to automatically generate detailed captions for the top 20 candidate images. These generated captions serve as a rich semantic bridge, capturing fine-grained visual details, spatial relationships, and contextual information that may not be fully captured in the original visual embeddings.\nSubsequently, we perform semantic reranking of the candidate images by computing semantic similarity between the original text query and the captions generated by the VLM. This caption-guided refinement process leverages the descriptive power of natural language to capture subtle visual differences and semantic nuances that direct visual-text matching might miss, resulting in more accurate matching.\n\nOur experimental results on the RoboSense 2025 Track 4 challenge dataset [5] demonstrate the effectiveness of this approach, achieving a consistent 5% improvement across all key metrics (Recall@1, Recall@5, and Recall@10) compared to the baseline method. These improvements ultimately helped us secure a top-2 finish in the competition, validating the practical value of our semantic refinement strategy in real-world cross-modal drone navigation applications. The main contributions are as follows:\n\nWe propose a Caption-Guided Retrieval System (CGRS), a two-stage coarse-to-fine retrieval framework that leverages visual language models to bridge the semantic gap between natural language queries and aerial imagery.\n\nWe introduce a novel caption-guided reranking mechanism that transforms cross-modal matching into a text-to-text similarity computation, enabling finer-grained semantic alignment in complex aerial scenes.\n\nCompared to baseline methods, we achieve a consistent 5% improvement on all key metrics (Recall@1, Recall@5, and Recall@10) and win TOP-2 in Cross-Modal Drone Navigation Track of the IROS 2025 RoboSense Challenge.\n\n1. We propose a Caption-Guided Retrieval System (CGRS), a two-stage coarse-to-fine retrieval framework that leverages visual language models to bridge the semantic gap between natural language queries and aerial imagery.\n\n2. We introduce a novel caption-guided reranking mechanism that transforms cross-modal matching into a text-to-text similarity computation, enabling finer-grained semantic alignment in complex aerial scenes.\n\n3. Compared to baseline methods, we achieve a consistent 5% improvement on all key metrics (Recall@1, Recall@5, and Recall@10) and win TOP-2 in Cross-Modal Drone Navigation Track of the IROS 2025 RoboSense Challenge.",
            "llm_summary": "【论文的motivation是什么】  \n1. 跨模态无人机导航需要有效地从大规模数据库中检索与自然语言描述相关的图像。  \n2. 现有方法在复杂的空中场景中难以实现文本查询与视觉内容之间的细粒度语义匹配。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有的跨模态检索方法主要依赖于端到端学习框架，直接将文本查询映射到视觉特征。  \n2. 当前的基线方法GeoText-1652虽然有效，但在语义精度和检索准确性方面仍存在局限性，无法捕捉细微的语义关系。  \n\n【提出了什么创新的方法】  \n我们提出了一个名为Caption-Guided Retrieval System (CGRS)的两阶段检索精炼框架。该方法首先利用基线模型进行初步粗略检索，获取与每个文本查询最相关的20张候选图像。接着，使用视觉语言模型（VLM）为这些候选图像生成详细的描述性标题，捕捉其视觉内容的丰富语义。通过计算原始文本查询与生成标题之间的语义相似度，我们进行语义重排序，从而实现更精确的匹配。实验结果表明，该方法在所有关键指标（Recall@1, Recall@5, Recall@10）上均提高了5%，并在挑战中获得了第二名，验证了我们语义精炼策略在现实世界无人机导航应用中的实际价值。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "A $1000\\times$ Faster LLM-enhanced Algorithm For Path Planning in Large-scale Grid Maps",
            "authors": "Junlin Zeng,Xin Zhang,Xiang Zhao,Yan Pan",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02716",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02716",
            "arxiv_html_link": "https://arxiv.org/html/2510.02716v1",
            "abstract": "Path planning in grid maps, arising from various applications, has garnered significant attention. Existing methods, such as A*, Dijkstra, and their variants, work well for small-scale maps but fail to address large-scale ones due to high search time and memory consumption. Recently, Large Language Models (LLMs) have shown remarkable performance in path planning but still suffer from spatial illusion and poor planning performance. Among all the works, LLM-A* (Meng et al. 2024) leverages LLM to generate a series of waypoints and then uses A* to plan the paths between the neighboring waypoints. In this way, the complete path is constructed. However, LLM-A* still suffers from high computational time for large-scale maps. To fill this gap, we conducted a deep investigation into LLM-A* and found its bottleneck, resulting in limited performance. Accordingly, we design an innovative LLM-enhanced algorithm, abbr. as iLLM-A*. iLLM-A* includes 3 carefully designed mechanisms, including the optimization of A*, an incremental learning method for LLM to generate high-quality waypoints, and the selection of the appropriate waypoints for A* for path planning. Finally, a comprehensive evaluation on various grid maps shows that, compared with LLM-A*, iLLM-A* 1) achieves more than 1000×1000\\times speedup on average, and up to 2349.5×2349.5\\times speedup in the extreme case, 2) saves up to 58.6%58.6\\% of the memory cost, 3) achieves both obviously shorter path length and lower path length standard deviation.",
            "introduction": "Path planning in a grid map determines a collision-free path from a start location to a goal location, adhering to specific criteria such as minimizing distance, time, or energy (Liu et al. 2023a). This is a fundamental problem in a wide range of real-world applications, such as robot navigation (Carvalho and Aguiar 2025), automated vehicle parking (Jiang, Zhang, and Wang 2023), and player role planning in game or emulated training environments (Panov, Yakovlev, and Suvorov 2018).\n\nExisting algorithms such as A*, Dijkstra, and their variants are capable of finding the optimal path with the time complexity of O​(N4​l​o​g​N)O(N^{4}logN) (Carlson et al. 2023), where NN is the edge length (grid number) of a 2D square grid map and N2N^{2} is the total grid number of the map. Such algorithms work well for small-scale maps. However, in more scenarios, the need for path planning for large-scale grid maps boosts (Sun et al. 2024). An example is that, with the enhancement of robots’ capacities, their working space dramatically expands (Tang, Mao, and Ma 2025). Another example is that with the proliferation of high-resolution computer games, the game maps are increasingly complex\n(Lee and Lawrence 2013; Kirilenko et al. 2025). In such large-scale grid maps, the existing algorithms encounter a significant computation cost increase in both time and memory (Ou et al. 2022). In recent years, Large Language Models (LLMs) have achieved a remarkable milestone in addressing various planning tasks, inspired by their notable reasoning and planning capacities in complex contexts. Specifically, several works have utilized LLMs for path planning (Fan et al. 2025), which, however, suffer from spatial illusion (Aghzal, Plaku, and Yao 2024; Xie et al. 2024; Kwon, Di Palo, and Johns 2024) and result in unstable and limited planning performance.\n\nTo achieve robust path planning, a state-of-the-art (SOTA) work (Meng et al. 2024) proposed LLM-A*, which combined the global insight of LLM with the robust planning capacity of A*. The basic idea of LLM-A* is that the LLM first generates a series of waypoints between the start and goal locations, then A* iteratively plans the paths between the neighbor waypoints, and finally, the whole path is constructed by connecting all the waypoints in sequence. In this way, A* does not need to explore the entire map when planning a path between each neighboring waypoints. Therefore, the overall computational and memory cost is reduced. However, when being applied for large-scale grid maps (with N≥200N\\geq 200), LLM-A* suffers from some critical limitations. First, the implementation of A* in LLM-A*, such as the grid cost query and collision detection, is inefficient, resulting in a long search time. Second, the global OPEN and CLOSED lists enconter high memory cost. Third, LLMs may stochastically generate some inappropriate waypoints. LLM-A* naively utilizes all waypoints for path planning, while some waypoints may be redundant and could be reduced for a better path.\n\nMotivated by LLM-A*, this work proposes an innovative LLM-enhanced path planning algorithm, abbr. as iLLM-A*. Specifically, iLLM-A* consists of three remarkable mechanisms.\n1) Optimization of A*: To reduce the search time of A*, we first use a hash table to replace the linear CLOSED list of A* to store the explored grids for fast grid query, then update the evaluation values of a small portion (instead of all of the unexplored grids) of the OPEN list, and finally use an efficient two-stage collision detection to replace the precise collision detection to reduce the detection cost. 2) Waypoint Generating by Incremental Learning based LLM: We implement an incremental learning-based prompt, in which the Few-shot prompt is dynamically enriched, to guide the LLM to generate higher quality waypoints.\n3) Appropriate Waypoint Selection: To address redundant LLM-generated waypoints, we employ an experience-driven method to select the appropriate subset of waypoints from the LLM-generated waypoints for path planning.\nFinally, a comprehensive evaluation on various grid maps shows that, compared to LLM-A *, iLLM-A* 1) achieves more than 1000×1000\\times speedup on average and up to 2349.5×2349.5\\times speedup in extreme cases, 2) saves up to 58.6%58.6\\% of the memory cost, 3) achieves both obviously shorter path length and lower path length standard deviation.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有路径规划算法在大规模网格地图中计算时间和内存消耗过高。  \n2. LLM-A*在大规模地图中表现不佳，存在空间幻觉和规划性能不稳定的问题。  \n3. 需要一种高效的路径规划算法来处理复杂的环境和大规模地图。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有算法如A*和Dijkstra适用于小规模地图，但在大规模地图中效率低下。  \n2. LLM-A*结合了LLM的全局视角和A*的规划能力，但在大规模地图中仍面临高计算成本和不稳定性。  \n\n【提出了什么创新的方法】  \niLLM-A*算法提出了三种机制：  \n1. 优化A*算法，通过哈希表替代线性CLOSED列表，加速网格查询，并采用高效的两阶段碰撞检测。  \n2. 基于增量学习的LLM生成高质量的路径点，动态丰富Few-shot提示。  \n3. 通过经验驱动的方法选择合适的路径点，减少冗余。  \n最终，iLLM-A*在各种网格地图上的评估显示，与LLM-A*相比，平均速度提升超过1000倍，极端情况下可达2349.5倍，内存消耗减少58.6%，路径长度明显缩短且标准差降低。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "A Trajectory Generator for High-Density Traffic and Diverse Agent-Interaction Scenarios",
            "authors": "Ruining Yang,Yi Xu,Yixiao Chen,Yun Fu,Lili Su",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02627",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02627",
            "arxiv_html_link": "https://arxiv.org/html/2510.02627v1",
            "abstract": "Accurate trajectory prediction is fundamental to autonomous driving, as it underpins safe motion planning and collision avoidance in complex environments.\nHowever, existing benchmark datasets suffer from a pronounced long-tail distribution problem,\nwith most samples drawn from low-density scenarios and simple straight-driving behaviors.\nThis underrepresentation of high-density scenarios and safety critical maneuvers such as lane changes, overtaking and turning is an obstacle to model generalization and leads to overly optimistic evaluations.\nTo address these challenges, we propose a novel trajectory generation framework that simultaneously enhances scenarios density and enriches behavioral diversity.\nSpecifically, our approach converts continuous road environments into a structured grid representation that supports fine-grained path planning, explicit conflict detection, and multi-agent coordination.\nBuilt upon this representation, we introduce behavior-aware generation mechanisms that combine rule-based decision triggers with Frenet-based trajectory smoothing and dynamic feasibility constraints.\nThis design allows us to synthesize realistic high-density scenarios and rare behaviors with complex interactions that are often missing in real data.\nExtensive experiments on the large-scale Argoverse 1 and Argoverse 2 datasets demonstrate that our method significantly improves both agent density and behavior diversity, while preserving motion realism and scenario-level safety. Our synthetic data also benefits downstream trajectory prediction models and enhances performance in challenging high-density scenarios.",
            "introduction": "Accurate trajectory prediction is fundamental to ensuring that autonomous vehicle can operate smoothly and safely in complex and uncertain environments [1, 2].\nRecent advances in artificial intelligence, both in model architectures such as transformers [3, 4] and in hardware acceleration (e.g. modern GPUs), have substantially improved the speed and accuracy of prediction models.\nHowever, unlike conventional machine learning tasks, trajectory prediction faces unique challenges: the data are inherently multimodal and costly to annotate [5, 6], making dataset curation difficult and expensive.\nThis data bottleneck is becoming an obstacle to progress.\nIn particular, many benchmark datasets [7, 8, 9] suffer from long-tailed distributions in two key aspects:\n\nAgent density imbalance:\nThe number of traffic participants in a scenario can range from just a few to over 80 [10]. Yet the distribution is skewed, most scenarios involve low to moderate density, while high-density scenarios are severely underrepresented.\n\nLimited interaction diversity:\nThe majority of trajectories correspond to straight driving, where agent interactions are minimal. In contrast, safety-critical maneuvers such as lane changes, overtaking, and sharp turns, where complex interactions arise, are relatively scarce.\n\nThese long-tailed issues hinder the generalization of the prediction models in critical scenarios where accurate behavior is most needed.\nThey also lead to overly optimistic evaluations that models trained and tested on low-density or simple-interaction scenarios may appear competent but fail in rare yet safety-critical situations.\nFor example, when a vehicle suddenly makes an unexpected right lane change with little clearance, a prediction model must correctly anticipate the maneuver to avoid collision.\nMoreover, such challenges are particularly relevant for advanced perception models with attention mechanisms [11, 12, 13].\nHigh agent density increases the computational complexity of attention, while diverse interactions directly affect attention allocation and the ability to capture dependencies.\nAs a result, benchmarks dominated by easy cases may overstate a model’s reliability, obscuring its weaknesses in rare but high-risk scenarios.\n\nIn this paper, inspired by the success of data generation in computer vision [14, 15], we tackle the long-tailed challenges in trajectory prediction through data generation.\nPrior approaches on trajectory data generation can be broadly grouped into two categories: simulation-based and learning-based.\nSimulation-based methods [16, 17] offer flexibility in generating arbitrary trajectories under physical constraints.\nHowever, they require extensive manual effort for operation and post-filtering (e.g., removing collisions and off-road cases), and the resulting traffic scenarios often lack realism with respect to road geometry and traffic dynamics.\nLearning-based methods [18, 19, 20] typically rely on interpolation or perturbation of existing trajectories.\nWhile they can partially mitigate density imbalance by adding vehicles or extending fragmented tracks [18], the resulting interactions remain limited.\nThis is not surprising, because from an information-theory perspective, most learning-based methods recycle existing patterns without introducing fundamentally new information.\n\nTo overcome these limitations, we introduce HiD2, a trajectory generation framework designed for generating High-Density and Diverse scenarios.\nHiD2 synthesizes scenarios within real-world maps and produces diverse driving behaviors such as lane changes, overtaking, and sharp turns behaviors that are underrepresented yet safety-critical.\nOur contributions are summarized as follows:\n\nWe propose HiD2, which converts continuous road environments into a structured grid representation, enabling fine-grained trajectory generation that adheres to traffic rules and avoids collisions.\nOn top of this, we design dedicated behavior-generation mechanisms that integrate rule-based triggers, Frenet-based smoothing, and dynamic feasibility constraints, ensuring that trajectories are both geometrically smooth and physically realistic.\n\nThrough comprehensive evaluations, we demonstrate that HiD2 preserves agent-level realism and scenario-level safety, while systematically enriching the diversity of high-density and rare-behavior cases, thereby filling critical gaps in existing datasets.\n\nWe show that augmenting training sets with HiD2 data consistently improves the robustness of state-of-the-art trajectory prediction models, especially in high-density and interaction-heavy scenarios, compared to training on the original datasets alone.\n\n1. Agent density imbalance:\nThe number of traffic participants in a scenario can range from just a few to over 80 [10]. Yet the distribution is skewed, most scenarios involve low to moderate density, while high-density scenarios are severely underrepresented.\n\n2. Limited interaction diversity:\nThe majority of trajectories correspond to straight driving, where agent interactions are minimal. In contrast, safety-critical maneuvers such as lane changes, overtaking, and sharp turns, where complex interactions arise, are relatively scarce.\n\n1. We propose HiD2, which converts continuous road environments into a structured grid representation, enabling fine-grained trajectory generation that adheres to traffic rules and avoids collisions.\nOn top of this, we design dedicated behavior-generation mechanisms that integrate rule-based triggers, Frenet-based smoothing, and dynamic feasibility constraints, ensuring that trajectories are both geometrically smooth and physically realistic.\n\n2. Through comprehensive evaluations, we demonstrate that HiD2 preserves agent-level realism and scenario-level safety, while systematically enriching the diversity of high-density and rare-behavior cases, thereby filling critical gaps in existing datasets.\n\n3. We show that augmenting training sets with HiD2 data consistently improves the robustness of state-of-the-art trajectory prediction models, especially in high-density and interaction-heavy scenarios, compared to training on the original datasets alone.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有数据集在高密度场景和复杂交互行为方面严重不足。  \n2. 传统的轨迹生成方法无法有效解决长尾分布问题，导致模型泛化能力不足。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有方法主要依赖于模拟或学习生成轨迹，但往往缺乏现实感和多样性。  \n2. 许多学习方法仅对现有轨迹进行插值或扰动，未能引入新信息，无法有效生成稀有行为。  \n\n【提出了什么创新的方法】  \n我们提出了HiD2，一个轨迹生成框架，通过将连续道路环境转换为结构化网格表示，支持细粒度的轨迹生成，确保遵循交通规则并避免碰撞。该方法结合了基于规则的决策触发器、Frenet平滑和动态可行性约束，生成多样化的驾驶行为，如变道、超车和急转弯。通过全面评估，HiD2在增强高密度和稀有行为场景的多样性方面表现出色，同时保持了运动的现实性和场景安全性。我们还发现，使用HiD2生成的数据增强了现有轨迹预测模型的鲁棒性，尤其是在高密度和交互复杂的场景中。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Multi-robot Rigid Formation Navigation via Synchronous Motion and Discrete-time Communication-Control Optimization",
            "authors": "Qun Yang,Soung Chang Liew",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02624",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02624",
            "arxiv_html_link": "https://arxiv.org/html/2510.02624v1",
            "abstract": "Rigid-formation navigation of multiple robots is essential for applications such as cooperative transportation. This process involves a team of collaborative robots maintaining a predefined geometric configuration, such as a square, while in motion. For untethered collaborative motion, inter-robot communication must be conducted through a wireless network. Notably, few existing works offer a comprehensive solution for multi-robot formation navigation executable on microprocessor platforms via wireless networks, particularly for formations that must traverse complex curvilinear paths. To address this gap, we introduce a novel ”hold-and-hit” communication-control framework designed to work seamlessly with the widely-used Robotic Operating System (ROS) platform. The hold-and-hit framework synchronizes robot movements in a manner that is robust against wireless network delays and packet loss. It operates over discrete-time communication-control cycles, making it suitable for implementation on contemporary microprocessors. Complementary to hold-and-hit, we propose an intra-cycle optimization approach that enables rigid formations to closely follow desired curvilinear paths, even under nonholonomic constraints. Nonholonomic constraints, inherent to most vehicular robots, limit their ability to freely track arbitrary paths between two points. The combination of hold-and-hit and intra-cycle optimization ensures precise and reliable navigation even in challenging scenarios. Simulations in a virtual environment demonstrate the superiority of our method in maintaining a four-robot square formation along an S-shaped path, outperforming two existing approaches. Furthermore, real-world experiments validate the effectiveness of our framework: the robots maintained an inter-distance error within ±0.069​m\\pm 0.069m and an inter-angular orientation error within ±​19.15∘\\textpm 19.15^{\\circ} while navigating along an S-shaped path at a fixed linear velocity of 0.1​m/s0.1m/s. They consistently operated over 312312 cycles without divergence. Finally, the proposed hold-and-hit framework and optimized nonholonomic motion paradigms are generalizable and extendable to a wide range of multi-robot collaboration problems beyond those studied here.",
            "introduction": "The study of Multi-Robot Systems (MRS) has grown significantly in recent years, encompassing various applications such as surveillance, exploration, and cooperative transportation [1]. Within this domain, rigid formation control [2] is particularly critical for cooperative transportation tasks. One widely studied approach to rigid formation control is the leader-follower strategy, where follower robots maintain a rigid formation by tracking the movements of a designated leader.\n\nIn most existing leader-follower schemes, the follower adjusts its trajectory based solely on its own observations, without receiving direct information from the leader [3]. This observation-only approach has a significant drawback: the follower may remain unaware of the leader’s immediate intentions.\n\nConsider the example illustrated in Fig. 1a. Two robots, a leader and a follower, are initially motionless and holding a panel between them. Both robots share the same orientation, with distance LL separating them. Now, suppose the leader decides to move forward with a velocity of vlv_{l}. Without prior knowledge of the leader’s intention, the follower remains stationary until the next control cycle, during which it observes a translational deviation of vl​T{v}_{l}T from the leader, where TT is the control cycle duration. This lag in the follower’s response, particularly if vl​T{v}_{l}T is significant, can disrupt the rigid formation, potentially causing the panel to fall, as depicted in Fig. 1b.\n\nMost MRS prioritize low cost and flexibility over achieving a fast control loop. For instance, the TurtleBot2, a widely-used platform, operates with a recommended control cycle of T=100​m​sT=100ms[4]. However, the ability to achieve rapid synchronized acceleration (i.e., a large vl{v}_{l} in the example above) within a group of robots is highly desirable for agile rigid-formation navigation, which will be challenging to the observation-only approach when the control cycle TT is large.\n\nInstead of the observation-only approach, to ensure synchronized movement between the leader and the follower, the leader can communicate its intentions to the follower before making significant changes in motion. This proactive communication prevents the follower from relying solely on observation to react, which can introduce delays and disrupt the formation. Enabling such communication requires the use of a wireless network.\nAn even more direct approach to ensuring synchronized leader-follower motion is for the leader to compute the control inputs for the follower and transmit them directly. In this setup, the leader uses an onboard sensor, such as LiDAR, to perceive and measure the relative poses (positions and angular orientations) between itself and the follower. It then combines this information with its own intended motion to calculate the control input for the follower.\n\nFurthermore, for a more complex rigid formation involving multiple followers—such as one leader and three followers arranged in a rectangle—relying on each follower to independently measure its relative pose with the leader and compute its own control input can result in inconsistent motions among the followers. This issue arises because each follower attempts to correct errors in its relative pose without accounting for the adjustments being made by other followers, leading to a lack of coordination.\nTo maintain the integrity of the rigid formation, a centralized approach is more effective. Here, the leader computes and distributes the control inputs for all followers based on their relative poses and its own intentions. This approach departs from the traditional leader-follower paradigm and is referred to in this paper as a master-slave scheme. In this scheme, the robot responsible for computing the control inputs acts as the master, while the other robots are the slaves.\n\nA critical component of the master-slave scheme is the use of a wireless network for communication. However, the reliability of the wireless network can significantly impact the system’s performance. Delays or loss of control inputs transmitted from the master to the slaves can compromise the integrity of the rigid formation.\n\nFor instance, in mobile robots such as the TurtleBot, control inputs specifying linear and angular velocities are converted into motor commands that adjust the wheel velocities. These control inputs are typically applied using a zero-order hold (ZOH) method, where a robot’s velocities remain constant until the next control cycle. However, in the presence of long delays or lost control commands, a follower may fail to update its velocities across multiple consecutive control cycles. This can lead to accumulated pose errors, disrupting the formation.\n\nAdditionally, in the case of multiple followers, varying reception times of the latest control inputs among the followers create further challenges, as they can result in unsynchronized actions across the robots. Ensuring synchronized motion in such scenarios requires mechanisms to mitigate the effects of communication delays and packet loss.\n\nTo address the synchronization challenge, we put forth a communication-control framework called “hold-and-hit”. This framework introduces additional elastic delays to align the delays experienced by all robots–specifically, the elastic delays of robots with large communication delays are small, and vice versa, so that the effective delays experienced by all robots are equal. The design offers two key advantages: (i) it mitigates the effects of varying network delays across different robots, and (ii) it allows for the retransmission of control commands multiple times within a predefined delay-budget window to handle packet loss.\nFor instance, in Wi-Fi networks, a lost packet can be retransmitted multiple times until it is successfully received. This mechanism effectively converts packet losses into variable delays, which the hold-and-hit framework is designed to accommodate, ensuring synchronized motion across the robots.\n\nFig. 2 illustrates the hold-and-hit mechanism. At the start of each cycle, at time instances t^=0,1,2​…\\hat{t}=0,1,2..., the master measures pose errors, computes the control inputs, and transmits them to the slaves. The objective is for the slaves to update their velocities synchronously at time t^+d\\hat{t}+d, where d<Td<T, rather than immediately at t^\\hat{t}.\n\nA control input is considered successfully conveyed to a slave only if it is received before t^+d\\hat{t}+d. If the delay is less than dd —say d′d^{\\prime}, the slave introduces an additional elastic delay of d−d′d-d^{\\prime} to ensure the total delay equals dd. In other words, the slave ”holds” its action until time t^+d\\hat{t}+d, at which point it applies the newly received control input. This mechanism is named hold-and-hit, as the slave holds its action until the designated time and then ”hits” by executing the control input.\n\nThe hold-and-hit mechanism ensures that all slaves experience the same delay dd before applying their control inputs, enabling synchronized execution and maintaining the integrity of the rigid formation.\n\nDuring the interval [t^,t^+d)[\\hat{t},\\hat{t}+d), a control input may be lost and retransmitted multiple times by the lower-layer wireless network protocol. As long as the slave receives a copy before t^+d\\hat{t}+d, the process proceeds without any issues. However, if no copy of the control input is received by t^+d\\hat{t}+d, an inter-pose error may accumulate.\n\nThis is not a fatal problem in our approach because in the next cycle, at t^+1\\hat{t}+1, the master will measure the accumulated error and compute a new control input to correct it, ensuring the system remains on track despite the missed input.\n\nAnother significant challenge in rigid formation control for nonholonomic mobile robots is navigating curvilinear paths. Most robots with nonholonomic constraints [5] lack the freedom to follow arbitrary paths from point A to point B. Severe variations in path curvature can cause rapid accumulation of inter-pose errors, and these nonholonomic constraints can further limit the robots’ ability to correct errors in a timely manner.\n\nSome existing studies have addressed this issue by employing control strategies based on continuous-time kinematics [6, 7], where control inputs change continuously over time. However, contemporary robots running on platforms like ROS typically rely on discrete-time kinematics with a timer-triggered control loop, where control inputs are updated only once per control cycle. Applying continuous-time kinematics to such systems introduces inaccuracies, resulting in error accumulation and, in extreme cases, system divergence (as discussed in Section IV).\n\nTo address this, an exact discrete-time control framework is required to enable precise navigation of nonholonomic robots in rigid formations along curvilinear paths, ensuring both accuracy and system stability.\n\nWe put forth a discrete-time control law, referred to as Discrete-time Error Minimization (DEM), which is fully compatible with the hold-and-hit framework. DEM is designed to minimize inter-pose errors in rigid formations while accounting for discrete-time and nonholonomic motion constraints.\n\nAs illustrated in Fig. 2, the master computes error-correction control inputs for the slaves at instants t^=0,1,2​…\\hat{t}=0,1,2... and transmits them via the wireless network. For each of these instants, the slaves then execute these control inputs during the time interval [t^+d,t^+1)[\\hat{t}+d,\\hat{t}+1). Specifically, at instant t^\\hat{t}, the master calculates the control inputs for the slaves to be executed over the interval [t^+d,t^+1)[\\hat{t}+d,\\hat{t}+1) to minimize the errors at instant t^+1\\hat{t}+1, while respecting the nonholonomic constraints of the robots.\n\nTo the best of our knowledge, this is the first comprehensive discrete-time framework that ensures rigid formation of nonholonomic robots navigating curvilinear paths via a wireless network.\n\nTo validate our approach, we conduct extensive simulations and experimental evaluations, benchmarking our results against two conventional leader-follower methods. The results demonstrate that the hold-and-hit framework, combined with DEM, effectively maintains rigid formation without divergence, even over extended iterations, outperforming the baseline methods.\n\nIn summary, our contributions are threefold:\n\nHold-and-Hit Framework: We propose a communication-control strategy called hold-and-hit to ensure synchronized motion of collaborative robots, even in the presence of lossy communication.\n\nHold-and-Hit Framework: We propose a communication-control strategy called hold-and-hit to ensure synchronized motion of collaborative robots, even in the presence of lossy communication.\n\nDiscrete-Time Control Mechanism (DEM): We develop a discrete-time control mechanism, Discrete-time Error Minimization (DEM), which is compatible with the hold-and-hit framework and accounts for nonholonomic constraints, enabling precise curvilinear path navigation in rigid formations.\n\nValidation through Simulations and Experiments: We validate the effectiveness of hold-and-hit and DEM through extensive simulations and real-world experiments, demonstrating their superiority over existing methods.",
            "llm_summary": "【论文的motivation是什么】  \n1. 多机器人系统在协作运输等应用中需要保持刚性队形。  \n2. 现有方法在复杂曲线路径上的多机器人导航存在不足，尤其是在无线网络环境下。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统的领导-跟随策略未能有效应对通信延迟和数据丢失的问题。  \n2. 现有的控制策略多基于连续时间运动，无法适应离散时间控制的需求。  \n\n【提出了什么创新的方法】  \n我们提出了“hold-and-hit”通信控制框架，确保在无线网络延迟和数据丢失情况下的机器人同步运动。该框架通过引入弹性延迟来协调不同机器人的动作，同时结合离散时间误差最小化（DEM）控制机制，确保非完整约束下的精确导航。通过虚拟环境和实际实验验证，我们的方法在维持四机器人方阵沿S形路径移动时表现优越，误差控制在±0.069m和±19.15°以内，且在312个周期内稳定运行，显示出其广泛的适用性和可扩展性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Reachable Predictive Control: A Novel Control Algorithm for Nonlinear Systems with Unknown Dynamics and its Practical Applications",
            "authors": "Taha Shafa,Yiming Meng,Melkior Ornik",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02623",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02623",
            "arxiv_html_link": "https://arxiv.org/html/2510.02623v1",
            "abstract": "This paper proposes an algorithm capable of driving a system to follow a piecewise linear trajectory without prior knowledge of the system dynamics. Motivated by a critical failure scenario in which a system can experience an abrupt change in its dynamics, we demonstrate that it is possible to follow a set of waypoints comprised of states analytically proven to be reachable despite not knowing the system dynamics. The proposed algorithm first applies small perturbations to locally learn the system dynamics around the current state, then computes the set of states that are provably reachable using the locally learned dynamics and their corresponding maximum growth-rate bounds, and finally synthesizes a control action that navigates the system to a guaranteed reachable state.",
            "introduction": "Many real-world systems must operate in environments where their exact dynamics are unknown, yet reliable motion toward safety-critical or mission-critical goals must still be achieved. In such settings, even modest uncertainty in the dynamics can render conventional model-based planning or tracking strategies ineffective. In this paper, we aim to simulate scenarios where the existing system model is ineffective due to an abrupt changes in its dynamics. This could be due to physical failures or environmental disturbances.\n\nWe address this challenge by proposing Reachable Predictive Control (RPC), a resilient framework for safe navigation. RPC proceeds iteratively: (1) update local dynamics through active learning, (2) compute a Guaranteed Reachable Set (GRS) with limited information and assign a feasible waypoint, (3) synthesize a controller to reach that waypoint, and (4) repeat to steer the system toward a provably reachable region. Unlike classical robust safety control, which relies on nominal models perturbed by mismatches or disturbances, RPC assumes only a local Lipschitz bound on the dynamics. This encompasses a broader class of systems and enables rapid response to abrupt environmental disturbances.\n\nWe review some crucial results that are pertinent to the work presented in this paper. In the realm of reachability-based robust safety control, frameworks developed around Control Barrier Functions (CBFs) and Model Predictive Control (MPC) represent the most widely adopted approaches.\n\nCBFs are widely used to enforce safety in dynamical systems, often as filters that minimally adjust control inputs to prevent constraint violations. While elegant in theory, constructing valid CBFs is difficult, though integration with data-driven controllers is attractive. Recent work has explored neural-network-based barrier learning through policy evaluation and Hamilton–Jacobi reachability [1, 2, 3], extending to parametric uncertainties and dynamic environments. These methods, however, face challenges in dynamics assumptions, verification complexity, and interpretability. The approach in [4] eases reliance on precise dynamics and proposes runtime CBF approximations, but without provable safety guarantees.\n\nMPC enforces safety by embedding state and input constraints into a finite-horizon problem solved in receding-horizon form [5, 6]. Robust variants such as tube-based [7] and min-max MPC [8] handle uncertainty when a nominal model is available. Extensions combine MPC with CBFs [9] or reachability-based safety certification [10]. Recent work leverages meta-learning for system identification and Koopman-based models to adapt to nonlinear systems with parametric uncertainties [11, 12, 13]. Despite these advances, MPC still suffers from finite-horizon approximations, the need for long horizons, and heavy computational demands in nonlinear or high-dimensional settings.\n\nLearning-based Safety Control, particularly reinforcement learning (RL), has shown success in generating robust control policies for high-dimensional dynamics, sensory feedback, and complex tasks [14, 15, 16], including agility, robustness, and terrain traversal [17, 18, 19]. However, many RL approaches emphasize performance over safety—whether through motion planner integration or collision penalties [20, 21, 22], and thus inherit model-based drawbacks, restrict mobility, and lack formal safety guarantees. the resulting policies often exhibit degraded safety when deployed in the real world, especially under distribution shifts away from the training environment.\n\nUnlike prior work, the proposed RPC framework uses reachability analysis with provable guarantees, requiring neither a nominal model nor pre-collected data, enabling real-time response to a broader class of uncertainties.\n\nWe formally present the RPC algorithm, an algorithm capable of solving reach-avoid and trajectory-tracking problems without the use of a nominal model. Previous work estimated the GRS using proxy systems [23, 24, 25], later incorporating myopic control [26] and developing synthesis methods [27], but these pipelines could not handle practical, underactuated systems. We generalize results to incorporate underactuated systems, provide a new algorithm capable of optimal path-following under constraints, and demonstrate its operation in an unknown environment via simulation.",
            "llm_summary": "【论文的motivation是什么】  \n1. 许多实际系统在动态未知的环境中必须可靠地导航。  \n2. 传统的基于模型的规划或跟踪策略在动态突变时失效。  \n3. 需要一种新方法来确保在未知动态下的安全导航。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 控制障碍函数（CBFs）和模型预测控制（MPC）是常用的安全控制方法，但构建有效的CBFs困难且缺乏可证明的安全性。  \n2. 现有的MPC方法在处理非线性或高维设置时面临有限时域近似和计算需求的问题。  \n3. 强化学习（RL）方法在性能上取得成功，但往往忽视安全性，导致在真实世界中的安全性下降。  \n\n【提出了什么创新的方法】  \n提出了一种名为可达预测控制（RPC）的框架，能够在没有已知模型的情况下解决轨迹跟踪和避障问题。RPC通过以下步骤迭代进行：  \n1. 通过主动学习更新局部动态。  \n2. 计算在有限信息下的可达集（GRS）并分配可行的航点。  \n3. 合成控制器以到达该航点。  \n4. 重复以上步骤以引导系统到达可证明的可达区域。  \n该方法不依赖于精确的动态模型，能够实时响应更广泛的不确定性，并在仿真中展示了在未知环境中的有效性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "UMI-on-Air: Embodiment-Aware Guidance for Embodiment-Agnostic Visuomotor Policies",
            "authors": "Harsh Gupta,Xiaofeng Guo,Huy Ha,Chuer Pan,Muqing Cao,Dongjae Lee,Sebastian Sherer,Shuran Song,Guanya Shi",
            "subjects": "Robotics (cs.RO)",
            "comment": "Result videos can be found atthis http URL",
            "pdf_link": "https://arxiv.org/pdf/2510.02614",
            "code": "http://umi-on-air.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02614",
            "arxiv_html_link": "https://arxiv.org/html/2510.02614v1",
            "abstract": "We introduce UMI-on-Air, a framework for embodiment-aware deployment of embodiment-agnostic manipulation policies. Our approach leverages diverse, unconstrained human demonstrations collected with a handheld gripper (UMI) to train generalizable visuomotor policies. A central challenge in transferring these policies to constrained robotic embodiments—such as aerial manipulators—is the mismatch in control and robot dynamics, which often leads to out-of-distribution behaviors and poor execution. To address this, we propose Embodiment-Aware Diffusion Policy (EADP), which couples a high-level UMI policy with a low-level embodiment-specific controller at inference time. By integrating gradient feedback from the controller’s tracking cost into the diffusion sampling process, our method steers trajectory generation towards dynamically feasible modes tailored to the deployment embodiment. This enables plug-and-play, embodiment-aware trajectory adaptation at test time. We validate our approach on multiple long-horizon and high-precision aerial manipulation tasks, showing improved success rates, efficiency, and robustness under disturbances compared to unguided diffusion baselines.\nFinally, we demonstrate deployment in previously unseen environments, using UMI demonstrations collected in the wild, highlighting a practical pathway for scaling generalizable manipulation skills across diverse—and even highly constrained—embodiments.\nAll code, data, and checkpoints will be publicly released after acceptance. Result videos can be found at umi-on-air.github.io.",
            "introduction": "There is a growing interest in extending manipulation beyond the lab and into more complex, dynamic settings. Among emerging embodiments, unmanned aerial manipulators (UAMs) hold particular promise. Essentially a manipulator with practically limitless reach, UAMs can access environments that are otherwise unreachable or unsafe—such as performing infrastructure maintenance atop towers or harvesting crops in cluttered orchards. Multiple research works have focused on those practical tasks, including non-destructive testing [3], painting [4], drilling [5], light bulb installation [6], etc. These applications highlight the potential of UAMs as an embodiment, but scalable visuomotor policy learning for UAMs remains limited.\n\nA major bottleneck is data collection. Teleoperation is particularly challenging for UAMs due to expensive and fragile hardware and unintuitive interface. To address this, recent work explores cross-embodiment collection, with the Universal Manipulation Interface (UMI) [1] offering a portable, low-cost way to record demonstrations across environments. By decoupling demonstrations from specific robots, UMI enables training of embodiment-agnostic manipulation policies.\n\nWhile UMI enables embodiment-agnostic manipulation policies, their success hinges on the embodiment’s ability to execute the generated trajectories. Fixed-base arms with precise controllers are highly “UMI-able111We will formally define “UMI-able” in § IV, and Fig. 6 quantifies how “UMI-able” different embodiments are in simulation.”, able to execute UMI policies as if they were the handheld gripper. In contrast, embodiments like UAMs face stringent physical and control constraints such as stability under aerodynamic disturbances [7, 8]. Without accounting for these constraints, UMI policies may yield trajectories that are infeasible, unsafe, or inefficient. Hence, the central challenge is then how to extend UMI beyond highly UMI-able robots to embodiments where control constraints fundamentally shape feasibility.\n\nTo address this challenge, we propose Embodiment-Aware Diffusion Policy (EADP), where the key idea is to enable two-way communication between an embodiment-agnostic high-level manipulation policy and embodiment-specific low-level controllers (Fig. LABEL:fig:teaser). Unlike standard UMI systems [1, 2] that rely on one-way communication by passing policy outputs directly to controllers, EADP lets low-level controllers actively guide the high-level policy through the denoising process, therefore producing end-effector (EE) trajectories that are more feasible for the target embodiment (e.g., a UAM).\n\nConcretely, at each denoising step, the embodiment’s controller evaluates the noisy EE trajectory with a tracking cost, measuring its feasibility under current constraints. By backpropagating this cost to the noisy action trajectory, the policy is guided toward action trajectories that are more feasible. By leveraging the multi-modality of UMI policies (from diverse human data and the diffusion architecture), EADP biases the action generation toward strategies best aligned with the embodiment’s capabilities.\n\nIn summary, the work has three main contributions:\n\nWe propose Embodiment-Aware Diffusion Policy, a framework that integrates embodiment-specific controller feedback into high-level trajectory generation by diffusion policies, enabling plug-and-play embodiment-aware trajectory guidance at test time.\n\nWe introduce a simulation-based benchmark suite222All code, data, and checkpoints will be publicly released after acceptance. Result videos can be found at umi-on-air.github.io, which facilitates systematic investigation of the embodiment gap when using UMI demonstration data on embodiments with varying UMI-abilities.\n\nWe present UMI-on-Air, a system that validates our method on challenging aerial manipulation tasks (Fig. 2), outperforming embodiment-agnostic baselines.\n\nBy closing the gap between embodiment-agnostic policies and embodiment-specific constraints, this work is a step towards making all robots more UMI-able, thus extending scalable, universal manipulation skills to robots and environments previously beyond reach.\n\n1. We propose Embodiment-Aware Diffusion Policy, a framework that integrates embodiment-specific controller feedback into high-level trajectory generation by diffusion policies, enabling plug-and-play embodiment-aware trajectory guidance at test time.\n\n2. We introduce a simulation-based benchmark suite222All code, data, and checkpoints will be publicly released after acceptance. Result videos can be found at umi-on-air.github.io, which facilitates systematic investigation of the embodiment gap when using UMI demonstration data on embodiments with varying UMI-abilities.\n\n3. We present UMI-on-Air, a system that validates our method on challenging aerial manipulation tasks (Fig. 2), outperforming embodiment-agnostic baselines.",
            "llm_summary": "【论文的motivation是什么】  \n1. 如何将人类演示的操作技能有效转移到不同的机器人平台上。  \n2. 解决在复杂环境中，尤其是无人机操作中的控制约束问题。  \n3. 提高对人类演示数据的利用效率，以实现更广泛的机器人操作能力。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有工作利用人类演示数据训练通用操作策略，但缺乏对特定机器人动态的考虑。  \n2. 通过跨平台数据集进行预训练的策略需要大量的特定硬件微调数据，限制了其通用性。  \n3. 尽管有一些研究尝试构建具身意识的模型，但在模仿学习中应用仍然有限。  \n\n【提出了什么创新的方法】  \n我们提出了具身意识扩散策略（EADP），通过将高层次的UMI策略与低层次的特定控制器相结合，实现在推理时的双向通信。该方法在每个去噪步骤中，利用控制器的跟踪成本反馈，引导策略生成更符合目标机器人动态的轨迹。通过这种方式，EADP能够在测试时实现即插即用的轨迹适应。我们在多个高精度的无人机操作任务中验证了该方法，显示出相较于无指导扩散基线的成功率、效率和鲁棒性有显著提升。此外，我们展示了在之前未见环境中的部署能力，强调了在多样化和高度受限的环境中扩展通用操作技能的实际路径。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "SubSense: VR-Haptic and Motor Feedback for Immersive Control in Subsea Telerobotics",
            "authors": "Ruo Chen,David Blow,Adnan Abdullah,Md Jahidul Islam",
            "subjects": "Robotics (cs.RO)",
            "comment": "Presented at the OCEANS 2025 Great Lakes Conference",
            "pdf_link": "https://arxiv.org/pdf/2510.02594",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02594",
            "arxiv_html_link": "https://arxiv.org/html/2510.02594v1",
            "abstract": "This paper investigates the integration of haptic feedback and virtual reality (VR) control interfaces to enhance teleoperation and telemanipulation of underwater ROVs (remotely operated vehicles). Traditional ROV teleoperation relies on low-resolution 2D camera feeds and lacks immersive and sensory feedback, which diminishes situational awareness in complex subsea environments. We propose SubSense – a novel VR-Haptic framework incorporating a non-invasive feedback interface to an otherwise 1-DOF (degree of freedom) manipulator, which is paired with the teleoperator’s glove to provide haptic feedback and grasp status. Additionally, our framework integrates end-to-end software for managing control inputs and displaying immersive camera views through a VR platform. We validate the system through comprehensive experiments and user studies, demonstrating its effectiveness over conventional teleoperation interfaces, particularly for delicate manipulation tasks. Our results highlight the potential of multisensory feedback in immersive virtual environments to significantly improve remote situational awareness and mission performance, offering more intuitive and accessible ROV operations in the field.",
            "introduction": "Remotely Operated Vehicles (ROVs) are indispensable tools in the marine industry, offering a safer and more cost-effective alternative to human divers [1]. Underwater ROVs are versatile platforms supporting a range of missions, from routine imaging and infrastructure inspection to complex tasks such as environmental monitoring [2], maintaining subsea infrastructure [3, 4], performing mine countermeasure and explosive ordinance disposal [5], salvaging, search-and-rescue [6], and deep-water expeditions [7]. With over 79%79\\% of subsea deployments done by ROVs, they play a crucial role in commerce, military, and science – enabling us to explore beyond the limits of human scuba divers [8].\n\nDespite growing industrial demands and recent advancements, underwater ROVs still have inherent limitations, particularly in their immersive control and interaction capabilities. Commercial teleoperation interfaces predominantly rely on 2D visual feeds and joystick controllers, restricting feedback to purely visual cues and relying heavily on their trained skills and intuitions for safe operation [9]. This stands in stark contrast to divers, who benefit from direct multisensory feedback in underwater environments [10]. The absence of tactile and situational sensory information poses significant challenges, particularly in low-light and confined spaces such as underwater caves [11], where operators are at heightened risk of spatial disorientation and collisions [12].\n\nSimilar issues arise in open-water scenarios, where ocean currents can induce disorientation and drift [13]. The lack of haptic feedback further complicates tasks requiring precise manipulation, such as maintenance and repair, increasing the risk of operational errors [14]. This fundamental disconnect between the teleoperator and the ROV’s surroundings is well-documented in recent studies [12, 15], as well as corroborated by experiences in our own field trials; highlighting the challenges of deploying in deep-water environments and performing manipulation tasks [16].\n\nFor any motor action, humans rely on somatosensory and visual feedback to interpret the results of actions within their environment [17, 18]. Compared to ROVs, a diver in the underwater environment can more easily perform delicate manipulation tasks due to being fully engaged with their senses. As shown in Figure 1, when limited to only visual feedback, the difficulty increases due to the lack of supplementary information such as tactile or haptic feedback [19]. Thus, ROV teleoperators lack the sensory information needed to grasp objects when more precision is required [20, 21].\n\nRecent advancements in virtual reality (VR) technologies have opened new avenues for improving human-machine interactions in remote environments [22, 23]. This indicates the potential to significantly enhance ROV teleoperation by enabling immersive experiences for teleoperators through a simulated digital twin [1, 24]. When combined with haptic feedback, VR can simulate a more embodied experience to better understand spatial relations within their environment [25, 26], assisting with tasks that require precise manipulation, e.g., handling sensitive samples.\n\nWhile the integration of sensory feedback has been extensively studied in fields such as industrial and surgical robotics [27, 28], the application of haptic feedback in subsea telemanipulation tasks remains largely underexplored [1]. Furthermore, only a limited number of user studies have examined the impact of subsea ROV teleoperation through VR in physical environments [29, 30]. Among these, most evaluations are conducted in simulation, lacking validation in real-world scenarios [31, 32]. This underscores a significant gap, which we demonstrated in [12, 33] through field experimental trials for ROV-based mapping and inspection tasks inside underwater caves.\n\nIn this paper, we present SubSense, a VR-haptic framework designed to enhance teleoperation by immersing the operator in the ROV environment. This is achieved through the integration of haptic feedback, body movement-based ROV control, and a head-mounted display (HMD) providing a real-time video feed from the ROV’s onboard camera. The system utilizes a BlueROV2 equipped with a Newton Subsea gripper, which inherently supports only open-loop commands (neutral, open, close). To enable closed-loop control, we integrate a non-invasive augmentation for gripper position feedback, allowing the manipulator’s position to be mapped to a user-worn glove while providing vibration feedback upon successful object grasping. Additionally, we integrate the system with a commercial VR platform, enabling seamless ROV control and real-time video streaming within the HMD to enable human-machine-embodied teleoperation.\n\nWe assess human embodiment through a user study involving eight participants, evaluating their performance in transferring delicate objects between predefined locations within a controlled physical testing environment. Specifically, we use a 3-disk Tower of Hanoi (TOH) experimental setup, as shown in Figure 2. Participants operate the ROV in two modes: with (1) traditional first-person view (FPV); and (2) VR-based control with haptic feedback through our SubSense framework. Performance is quantified based on task completion time and the number and severity of object damages. Additionally, participants complete a post-trial survey to subjectively evaluate satisfaction and ease of use for each mode of operation. Our findings indicate that immersive interfaces enhance remote situational awareness and operator embodiment in underwater ROV teleoperation.\n\nWhile preliminary, these results provide a strong foundation for further development and larger-scale evaluations in more complex environments. Ongoing work focuses on improving the user experience through (i) reducing latency by optimizing the communication and rendering performance to maintain a responsive experience; (ii) upgrades to the haptic system to deliver tactile cues that better reflect the physical interaction; (iii) refinements to the control scheme to allow for more intuitive ROV movement; and (iv) incorporation of additional data streams from to the user such as ROV orientation, speed, and other interactive information.",
            "llm_summary": "【论文的motivation是什么】  \n1. 提升ROV的沉浸式控制能力，克服传统2D视觉反馈的局限性。  \n2. 解决在复杂水下环境中缺乏多感官反馈导致的操作困难。  \n3. 增强远程操作的情境意识和任务执行效果。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有的ROV操作主要依赖2D视觉和操纵杆，缺乏有效的触觉反馈。  \n2. 虽然有研究探讨了多模态反馈，但在水下遥操作中的应用仍然不足。  \n3. 现有的用户研究多集中于模拟环境，缺乏真实场景中的验证。  \n\n【提出了什么创新的方法】  \n我们提出了SubSense，一个结合VR和触觉反馈的框架，通过非侵入式反馈接口增强ROV的遥操作体验。该系统集成了用户佩戴的手套与ROV的抓取状态反馈，提供实时的触觉反馈。通过用户研究，我们评估了在传统第一人称视角与VR控制下的操作表现，结果显示使用SubSense框架的参与者在任务完成时间和物体损坏率上表现更佳，表明沉浸式界面显著提升了远程操作的情境意识和操作体验。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Efficient Optimal Path Planning in Dynamic Environments Using Koopman MPC",
            "authors": "Mohammad Abtahi,Navid Mojahed,Shima Nazari",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY); Dynamical Systems (math.DS); Optimization and Control (math.OC)",
            "comment": "This work has been submitted to the ACC2026 conference",
            "pdf_link": "https://arxiv.org/pdf/2510.02584",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02584",
            "arxiv_html_link": "https://arxiv.org/html/2510.02584v1",
            "abstract": "This paper presents a data-driven model predictive control framework for mobile robots navigating in dynamic environments, leveraging Koopman operator theory. Unlike the conventional Koopman-based approaches that focus on the linearization of system dynamics only, our work focuses on finding a global linear representation for the optimal path planning problem that includes both the nonlinear robot dynamics and collision-avoidance constraints. We deploy extended dynamic mode decomposition to identify linear and bilinear Koopman realizations from input–state data. Our open-loop analysis demonstrates that only the bilinear Koopman model can accurately capture nonlinear state–input couplings and quadratic terms essential for collision avoidance, whereas linear realizations fail to do so. We formulate a quadratic program for the robot path planning in the presence of moving obstacles in the lifted space and determine the optimal robot action in an MPC framework. Our approach is capable of finding the safe optimal action 320 times faster than a nonlinear MPC counterpart that solves the path planning problem in the original state space. Our work highlights the potential of bilinear Koopman realizations for linearization of highly nonlinear optimal control problems subject to nonlinear state and input constraints to achieve computational efficiency similar to linear problems.",
            "introduction": "Safe and efficient navigation of autonomous robots in dynamic environments is a long-standing challenge in robotics and control. Robots must continuously plan trajectories that avoid both static and moving obstacles while making progress toward mission objectives. This problem combines nonlinear dynamics with inherently nonconvex collision avoidance constraints [1]. Optimization-based motion planning and control have achieved strong results, yet directly handling these nonlinear constraints within receding horizon frameworks often leads to significant computational burden that limits real-time applicability on embedded platforms [2]. Recent work has explored convex formulations and linear Model Predictive Control (MPC) variants to improve tractability [3], nonetheless, maintaining safety and accuracy in highly dynamic scenes remains difficult. These challenges motivate the development of models and controllers that fully realize the nonlinear structure of the problem to ensure safety while enabling efficient convex optimization at runtime.\n\nNonlinear MPC can directly enforce nonlinear dynamics and safety-critical constraints but suffers from high computational cost for real-time use [4]. Similar trade-offs between accuracy and tractability have been noted [5]. To address this, convex and linearized MPC formulations have been explored, such as successive convexification [6] and convex quadratic programs [7]. Control Barrier Functions (CBF) have also been integrated with MPC to encode obstacle avoidance efficiently, including chance-constrained MPC with CBFs [8], dynamic CBF-MPC for mobile robots [9], and adaptive dynamic CBF-MPC for unmanned Ground vehicles (UGV) in unstructured terrain [10].\n\nData-driven modeling has emerged as an alternative to analytic linearization for motion planning and control. Recently, Koopman operator theory has gained attention as it provides a linear representation of nonlinear dynamics in a lifted observable space, enabling prediction and control with linear tools [11]. Practical approximations include Dynamic Mode Decomposition (DMD) and Extended Dynamic Mode Decomposition (EDMD) with control, which identify finite-dimensional predictors directly from input–state data [12]. These methods have been successfully applied to vehicle dynamics and MPC design [13], driver–automation shared control [14], and robot navigation under uncertainty [15, 16]. More recent works also integrate Koopman models with collision avoidance through control barrier functions [17] or collision-aware planning with learned Koopman dynamics [18]. Deep Koopman structures have also been introduced, where neural networks are used to jointly learn the lifting functions and linear predictors, improving accuracy and scalability for complex vehicle dynamics [19, 20]. While these approaches demonstrate the potential of Koopman models for safety-critical navigation, they often rely on auxiliary safety layers or are integrated within sampling-based planning frameworks. As a result, the collision-avoidance constraints are not represented directly in the lifted linear model, which can limit their efficiency or generalizability in real-time MPC settings.\n\nIn this work, we develop a data-driven framework for motion planning and control of mobile robots that overcomes two major bottlenecks of existing approaches: the nonlinear dynamics of the mobile robot and the nonlinear nature of collision-avoidance constraints. While NMPC can handle both, its computational cost makes real-time operation infeasible in highly interactive environments. Linear Koopman MPC, on the other hand, is tractable but cannot faithfully capture the nonlinear behaviors required for safe navigation. However, bilinear Koopman realizations provide a balanced alternative, preserving linearity in the lifted space while capturing essential input–state couplings that enhance modeling accuracy [21, 22].\n\nOur approach bridges this gap by identifying a bilinear Koopman realization of the unicycle dynamics using bilinear EDMD. The key insight is that the same bilinear lifting also generates the essential nonlinear terms that describe collision-avoidance constraints. By propagating these terms in the lifted space, both the robot dynamics and the collision-avoidance inequalities admit a linear representation. This allows the entire planning and control problem to be formulated as a convex Quadratic Programming Model Predictive Controller (QP-MPC) that maintains the fidelity of NMPC while retaining the computational efficiency of linear methods. We further demonstrate that the identified bilinear model precisely captures the input–state couplings responsible for obstacle interactions, as revealed by the learned high-dimensional matrices.\n\nThe main contributions of this paper are:\n\nEDMD-based bilinear Koopman modeling. A bilinear Koopman realization is identified that captures both unicycle dynamics and the nonlinear terms of collision-avoidance constraints.\n\nValidation of bilinear couplings. Analysis shows that the learned coupling matrices accurately represent key input–state interactions for safe planning.\n\nConvex QP-MPC formulation. The lifted linear structure enables efficient QP-MPC for real-time navigation in dynamic environments.\n\nThe rest of this paper is organized as follows. Section II introduces the unicycle model and collision-avoidance constraints. Section III presents the linear and bilinear Koopman realizations via EDMD, followed by a comparison of their modeling accuracy. Section IV formulates the proposed QP-MPC with lifted dynamics and constraints. Section V reports simulation studies, including a performance comparison with NMPC, showing that the proposed controller achieves comparable accuracy with much lower computation time. Section VI concludes the paper.\n\n1. EDMD-based bilinear Koopman modeling. A bilinear Koopman realization is identified that captures both unicycle dynamics and the nonlinear terms of collision-avoidance constraints.\n\n2. Validation of bilinear couplings. Analysis shows that the learned coupling matrices accurately represent key input–state interactions for safe planning.\n\n3. Convex QP-MPC formulation. The lifted linear structure enables efficient QP-MPC for real-time navigation in dynamic environments.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人在动态环境中安全高效导航的挑战。  \n2. 非线性动态和碰撞避免约束的复杂性导致实时计算负担。  \n3. 现有线性化方法无法有效捕捉非线性行为，影响安全导航。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统的非线性模型预测控制（NMPC）能处理非线性动态，但计算成本高。  \n2. 线性化的MPC方法提高了可行性，但无法准确捕捉非线性行为。  \n3. 现有的Koopman模型通常依赖辅助安全层，未能直接在提升线性模型中表示碰撞避免约束。  \n\n【提出了什么创新的方法】  \n本研究提出了一种数据驱动的运动规划和控制框架，利用双线性Koopman实现来捕捉移动机器人的非线性动态和碰撞避免约束。通过扩展动态模式分解（EDMD），识别出双线性Koopman模型，能够在提升空间中同时表示机器人动态和碰撞避免不等式。这种方法将整个规划和控制问题转化为一个凸二次规划模型预测控制器（QP-MPC），在保持NMPC的精度的同时，显著提高了计算效率。实验结果表明，所提出的控制器在动态环境中的导航性能与NMPC相当，但计算时间减少了320倍。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "A Recipe for Efficient Sim-to-Real Transfer in Manipulation with Online Imitation-Pretrained World Models",
            "authors": "Yilin Wang,Shangzhe Li,Haoyi Niu,Zhiao Huang,Weitong Zhang,Hao Su",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02538",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02538",
            "arxiv_html_link": "https://arxiv.org/html/2510.02538v1",
            "abstract": "We are interested in solving the problem of imitation learning with a limited amount of real-world expert data. Existing offline imitation methods often struggle with poor data coverage and severe performance degradation. We propose a solution that leverages robot simulators to achieve online imitation learning. Our sim-to-real framework is based on world models and combines online imitation pretraining with offline finetuning. By leveraging online interactions, our approach alleviates the data coverage limitations of offline methods, leading to improved robustness and reduced performance degradation during finetuning. It also enhances generalization during domain transfer. Our empirical results demonstrate its effectiveness, improving success rates by at least 31.7% in sim-to-sim transfer and 23.3% in sim-to-real transfer over existing offline imitation learning baselines.",
            "introduction": "Training robotic policies in simulation or from offline datasets and then finetuning them in the real world is a common strategy for real-world robotic control [1, 2, 3, 4, 5, 6]. However, many of these works either require explicitly defined reward functions [2, 1], or depend on large-scale offline expert datasets [5, 6]. In contrast, we study a sim-to-real setting with limited expert demonstrations in both domains and no reward signals available.\n\nThis setting presents significant challenges for offline imitation learning [7], as limited expert coverage leads to overfitting and exacerbates bias accumulation; reinforcement learning approaches [8, 9, 10] are also inapplicable due to the absence of reward signals.\n\nTo address this, we propose a two-phase sim-to-real pipeline: (i) online imitation pretraining in simulation with simulated expert demonstrations and (ii) offline imitation finetuning with limited real-world demonstrations. During the pretraining phase, our method leverages a latent world model for efficient learning and employs the CDRED reward model [11] to generate reward signals by discriminating expert data from online interactions. Empirically, we show that online pretraining improves state-space coverage, reduces degradation after finetuning, and enhances out-of-distribution generalization. We further leverage a practical state estimation pipeline that fuses robot proprioceptive states with object pose estimation, enabling real-world deployment. We evaluate our approach on six sim-to-sim transfer environments with two domain gaps and three sim-to-real transfer tasks. Our method achieves at least 31.7% success rate improvement in sim-to-sim transfer and 23.3% improvement in sim-to-real transfer over state-of-the-art offline imitation learning baselines.\n\nOur main contributions are as follows:\n(i) Proposing an efficient sim-to-real framework for limited-data regimes via online imitation pretraining of world models.\n(ii) Demonstrating, through comparisons with offline imitation learning, that our method achieves superior out-of-distribution (OOD) generalization and is more robust to performance degradation after few-shot finetuning.\n(iii) Providing an empirical analysis that attributes these performance gains to the superior data coverage achieved during the online exploration phase.",
            "llm_summary": "【论文的motivation是什么】  \n1. 解决有限真实专家数据下的模仿学习问题。  \n2. 改善离线模仿学习中数据覆盖不足导致的性能下降。  \n3. 提高在领域转移中的泛化能力。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有的离线模仿学习方法依赖于大量的专家数据，导致数据覆盖不足。  \n2. 强化学习方法在缺乏奖励信号的情况下无法应用。  \n\n【提出了什么创新的方法】  \n提出了一种两阶段的sim-to-real管道：首先在模拟环境中进行在线模仿预训练，利用模拟专家演示；其次在有限的真实世界演示下进行离线模仿微调。该方法利用潜在世界模型进行高效学习，并通过CDRED奖励模型生成奖励信号。通过在线预训练，显著改善了状态空间覆盖，减少了微调后的性能下降，并增强了对分布外数据的泛化能力。实验证明，该方法在六个sim-to-sim转移环境中和三个sim-to-real转移任务中，成功率分别提高了至少31.7%和23.3%。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "U-LAG: Uncertainty-Aware, Lag-Adaptive Goal Retargeting for Robotic Manipulation",
            "authors": "Anamika J H,Anujith Muraleedharan",
            "subjects": "Robotics (cs.RO)",
            "comment": ". Accepted to the IROS 2025 Workshop on Perception and Planning for Mobile Manipulation in Changing Environments",
            "pdf_link": "https://arxiv.org/pdf/2510.02526",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02526",
            "arxiv_html_link": "https://arxiv.org/html/2510.02526v1",
            "abstract": "Robots manipulating in changing environments must act on percepts that are late, noisy, or stale. We present U-LAG, a mid-execution goal-retargeting layer that leaves the low-level controller unchanged while re-aiming task goals (pre-contact, contact, post) as new observations arrive. Unlike motion retargeting or generic visual servoing, U-LAG treats in-flight goal re-aiming as a first-class, pluggable module between perception and control. Our main technical contribution is UAR–PF, an uncertainty-aware retargeter that maintains a distribution over object pose under sensing lag and selects goals that maximize expected progress. We instantiate a reproducible Shift×Lag stress test in PyBullet/PandaGym for pick, push, stacking, and peg insertion, where the object undergoes abrupt in-plane shifts while synthetic perception lag is injected during approach. Across 0–10 cm shifts and 0–400 ms lags, UAR–PF and ICP degrade gracefully relative to a no-retarget baseline, achieving higher success with modest end-effector travel and fewer aborts; simple operational safeguards further improve stability. Contributions: (1) UAR–PF for lag-adaptive, uncertainty-aware goal retargeting; (2) a pluggable retargeting interface; and (3) a reproducible Shift×Lag benchmark with evaluation on pick, push, stacking, and peg insertion.",
            "introduction": "Manipulation pipelines increasingly run with asynchronous perception, so observations can arrive late or intermittently and invalidate precomputed contact goals mid-trajectory. Setpoints that were safe at plan time can be stale when the end effector reaches them, causing missed grasps, glancing pushes, and wasted motion. Classical visual servoing closes the loop directly on image/feature error and is highly effective at high control rates, but it degrades under sensing delay or occlusion [1, 2]. At the other end of the spectrum, task-and-motion planning (TAMP) commits to waypoints and repairs or replans when the world changes [3, 4, 5]; and receding-horizon MPC reacts more quickly, but still recomputes trajectories that can already be out-of-date when perception lags [6, 7]. Learning-based visuomotor policies [8, 9] and grasp-detection modules integrated with servo control [10, 11] absorb small perturbations, and recent trends in large-scale data and diffusion-based policies push reactivity further [12, 13], yet most systems still assume timely observations or fold all adaptation into the low-level controller.\n\nWe advocate a separation of concerns: treat mid-execution goal retargeting as its own module between perception and control. Rather than redesigning controllers or replanning from scratch, a retargeting layer updates pre-contact, contact, and post-contact goals whenever new observations arrive, while an unchanged Cartesian servo executes those goals. This viewpoint is complementary to visual servoing and planning—it revises targets instead of commanding velocities from pixels, and it drops into existing stacks with minimal integration effort.\n\nWe instantiate this idea as U-LAG (Uncertainty-Aware, Lag-Adaptive Goal Retargeting): a lightweight layer that consumes RGB–D observations and outputs refreshed task goals during execution. Within a common interface, we realize multiple retargeters, including deterministic nearest-geometry updates, geometric registration to reconcile earlier and current observations, and a particle-filtered variant that maintains a belief over object pose under delay [14]. An overview is shown in Fig. 2. Lightweight reliability checks draw on uncertainty cues from perception to guard against outliers while preserving the pluggable abstraction[15].\n\nTo probe delay and disturbance systematically, we introduce a Shift×Lag protocol that injects controlled object shifts at approach time and holds actions against lagged goals for prescribed durations before the retargeter updates the setpoints. We evaluate across pick-and-place, pushing, stacking, and peg insertion, spanning non-prehensile and contact-rich regimes represented in prior [8, 16, 17, 18]. Across shifts (0–10 cm) and lags (0–400 ms), U-LAG variants degrade gracefully relative to a no-retarget baseline; registration- and particle-filter–based realizations remain robust in the harder regimes.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人在动态环境中操作时需要应对延迟、噪声或过时的感知数据。  \n2. 现有的控制和规划方法在感知延迟下表现不佳，导致目标失效。  \n3. 需要一种轻量级的中间执行目标重定向模块，以提高操作的鲁棒性和效率。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 经典视觉伺服方法在高控制率下有效，但对感知延迟敏感。  \n2. 任务与运动规划（TAMP）在世界状态变化时进行重新规划，但常常导致当前执行的目标过时。  \n3. 学习型视觉运动策略和抓取检测模块能改善对扰动的鲁棒性，但大多数系统仍假设及时的感知更新。  \n\n【提出了什么创新的方法】  \nU-LAG（不确定性感知、延迟自适应目标重定向）是一个轻量级的中间执行目标重定向层，能够在执行过程中根据新的RGB-D观测更新任务目标。该方法包括不确定性感知的粒子滤波器（UAR–PF），在感知延迟下维护物体姿态的分布，并选择最大化预期进展的目标。通过在PyBullet/PandaGym中实现Shift×Lag基准测试，U-LAG在物体发生突变和感知延迟的情况下，表现出相较于无重定向基线更高的成功率和更少的中止，展示了其在多种操作任务中的有效性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting",
            "authors": "Sung-Yeon Park,Adam Lee,Juanwu Lu,Can Cui,Luyang Jiang,Rohit Gupta,Kyungtae Han,Ahmadreza Moradipari,Ziran Wang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02469",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02469",
            "arxiv_html_link": "https://arxiv.org/html/2510.02469v1",
            "abstract": "Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat’s extensive editing capabilities and adaptability across a wide range of scenarios. Project page: simsplat.",
            "introduction": "Driving simulators have long played a vital role in the development of autonomous driving algorithms, serving as robust virtual testbeds. Traditional platforms such as CARLA [2] and AirSim [16] have been widely adopted due to their scalability enabled by game engines. However, with the rapid progress of generative models and neural scene reconstruction, approaches based on diffusion models [19, 27, 30, 3], Neural Radiance Fields (NeRFs) [23, 25, 5, 21], and Gaussian Splatting [28, 1, 29] have emerged as powerful alternatives. These methods not only reconstruct photo-realistic and geometrically precise driving scenes directly from sensor data but also enable editing scenarios from real-world data, reducing the need for creating long-tail scenarios in virtual environment. More recently, language models have been integrated into this paradigm, allowing users to edit scenes through natural language prompts.\n\nDespite these advances, several limitations remain. First, fine-grained object-level editing is still constrained. To edit a specific object, current methods often rely on an additional 3D object detector or perception model to provide bounding boxes, which are then passed to the generative model as local conditions for removal and inpainting [30, 21]. In addition, because objects are not separately constructed within the scene, per-object editing is rarely supported. As a result, edits to existing objects are typically limited to removal, while adding new objects generally depends on pre-stored virtual assets. This restricts per-object modifications such as changing locations or motions of agents. Second, scenario feasibility is usually validated only for the ego vehicle and a designated target object [27, 21]. In reality, however, any behavioral change affects all surrounding agents, including vehicles and pedestrians. Limiting validation to a single agent often produces unnatural or inconsistent outcomes. Finally, most prior works focus on rigid objects like vehicles, overlooking pedestrians despite the fact that pedestrians are important in safety-critical and corner-case scenarios. Enabling pedestrian-level editing is therefore essential for generating realistic and comprehensive driving simulations.\n\nTo address these challenges, we propose a unified language-controlled driving simulator, SIMSplat, which supports object-level editing and multi-agent path refinement. Our framework starts from integrating motion-aware language embeddings with 3D Gaussians to directly query and manipulate 3D scenes with prompt. In this stage, the motion-aware embeddings align language features with the dynamic movements and spatial locations of agents, extending beyond prior works such as LangSplat and 4DLangSplat [13, 10], which are effective in relatively static or indoor environments but less suited for highly dynamic driving scenes. Based on this object-grounding, an LLM agent supports detailed editing by identifying target objects for modification. Furthermore, the LLM agent generates initial path plans (e.g., turn left, stop at a location, add a following vehicle) through function calling, which are then refined with a multi-agent motion prediction model to ensure global consistency and realism across the scene. Through iterative conversation, users can further adjust the details by changing various asset/motion parameters before obtaining the final rendered simulation.\n\nIn our experiments, SIMSplat demonstrates extensive capabilities for editing and simulating driving scenes, ranging from inserting static objects to sophisticated modifications of dynamic agents. Our motion-aware language-alignment module achieves state-of-the-art performance in road-object querying, surpassing baselines by 61.2% in accuracy. SIMSplat also attains the highest task completion rate among simulators, highlighting its flexible usage and broad range of editing functionalities. Finally, our multi-agent path refinement enables predictive simulation, achieving the lowest collision and failure rates in our experiments.\n\nIn summary, our contributions are threefold.\n\nA motion-aware language-alignment that enables precise 3D object localization in dynamic driving environments.\n\nAn LLM-based object-level editor that supports fine-grained modifications of both vehicles and pedestrians.\n\nA multi-agent path refinement that ensures globally consistent and realistic scenario generation beyond ego-target validation.\n\n1. A motion-aware language-alignment that enables precise 3D object localization in dynamic driving environments.\n\n2. An LLM-based object-level editor that supports fine-grained modifications of both vehicles and pedestrians.\n\n3. A multi-agent path refinement that ensures globally consistent and realistic scenario generation beyond ego-target validation.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的驾驶场景编辑框架在生成逼真场景时效率低下，编辑能力有限。  \n2. 需要支持更细粒度的对象级编辑，特别是在动态驾驶环境中。  \n3. 现有方法对行人编辑支持不足，影响安全关键场景的模拟。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统的驾驶模拟器如CARLA和AirSim提供了可扩展性，但缺乏灵活的编辑能力。  \n2. 现有基于生成模型的方法在对象编辑上依赖于额外的3D检测器，限制了编辑的灵活性。  \n3. 先前的工作主要集中于刚性物体，忽视了行人在复杂场景中的重要性。  \n\n【提出了什么创新的方法】  \nSIMSplat通过集成运动感知的语言嵌入与3D高斯体，提出了一种统一的语言控制驾驶模拟器，支持对象级编辑和多智能体路径优化。该方法允许用户通过自然语言提示直接查询和操控动态场景，解决了以往方法在动态环境中的局限性。通过与大型语言模型（LLM）结合，SIMSplat能够识别目标对象并生成初步路径计划，随后通过多智能体运动预测模型进行优化，确保场景的全局一致性和真实感。实验结果表明，SIMSplat在道路对象查询的准确性上超越基线61.2%，并在任务完成率上表现最佳，展示了其灵活的使用和广泛的编辑功能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "ERUPT: An Open Toolkit for Interfacing with Robot Motion Planners in Extended Reality",
            "authors": "Isaac Ngui,Courtney McBeth,André Santos,Grace He,Katherine J. Mimnaugh,James D. Motes,Luciano Soares,Marco Morales,Nancy M. Amato",
            "subjects": "Robotics (cs.RO); Human-Computer Interaction (cs.HC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02464",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02464",
            "arxiv_html_link": "https://arxiv.org/html/2510.02464v1",
            "abstract": "We propose the Extended Reality Universal Planning Toolkit (ERUPT), an extended reality (XR) system for interactive motion planning. Our system allows users to create and dynamically reconfigure environments while they plan robot paths.\nIn immersive three-dimensional XR environments, users gain a greater spatial understanding. XR also unlocks a broader range of natural interaction capabilities, allowing users to grab and adjust objects in the environment similarly to the real world, rather than using a mouse and keyboard with the scene projected onto a two-dimensional computer screen.\nOur system integrates with MoveIt, a manipulation planning framework, allowing users to send motion planning requests and visualize the resulting robot paths in virtual or augmented reality. We provide a broad range of interaction modalities, allowing users to modify objects in the environment and interact with a virtual robot.\nOur system allows operators to visualize robot motions, ensuring desired behavior as it moves throughout the environment, without risk of collisions within a virtual space, and to then deploy planned paths on physical robots in the real world.",
            "introduction": "Robots are becoming more prevalent in many aspects of life ranging from industrial settings like factories to domestic assistive scenarios. These applications often feature humans reconfiguring environments, requiring robots to adapt their behavior.\nA common way to configure the environment setup and query collision-free robot paths is to use MoveIt, a manipulation planning framework integrated with the Robot Operating System (ROS) [1].\nUsing this approach, environments are typically visualized as projected onto a two-dimensional computer screen, which can limit the operators spatial understanding, requiring them to view the environment from many angles to accurately perceive the robot’s behavior moving throughout the space.\n\nHead mounted display (HMD) technology has made it possible to immersively visualize three-dimensional environments, move around within them, and interact with objects. Extended Reality (XR), which encompasses virtual and augmented reality [2], offers a natural modality for reconfiguring environments and visualizing robot motion. Virtual robots are particularly well suited to exploring potential robot behavior because there is no risk of collision with the environment and nearby human operators. XR also offers a broader range of natural interaction capabilities, allowing users to adjust the position and rotation of environment obstacles by manipulating them in three dimensions, which may be impossible in the real world.\n\nTo leverage the advantages provided by immersive technology to enable rapid environment reconfigurations and (re)programming of robot behavior, we propose the Extended Reality Universal Planning Toolkit (ERUPT). ERUPT is an open-source111Code released for final submission. extended reality application that allows operators to import their own robots, reconfigure environments, plan robot paths, and visualize robot motions. The larger community of robotics researchers and operators can also expand upon our system’s base capabilities to support a broader range of robotics problems and planning frameworks. Our system integrates with ROS2, allowing users to easily extend the system for their applications. We provide an interface to the MoveIt planning framework, allowing operators to update the environment representation and plan and visualize paths generated using standard motion planners.\nOur contributions are:\n\nAn open-source XR system for interactive motion planning, enabling users to interact with the environment and the robot.\n\nImmersive visualization of feedback from the motion planner including output trajectories.\n\nIntegration with ROS2, allowing execution of robot motions planned using a virtual robot on physical robots in the real world.\n\nA set of demonstrations showing the capabilities of our proposed system in simulation and on real robots.\n\nThe rest of the paper is organized as follows.\nSection II provides background information on robotics tools and XR platforms as well as provides an overview of prior work.\nSection III details our system overview describing the capabilities and how users can use our system.\nSection IV showcases our system in action in a variety of different scenarios.\nFinally, in Section V, we analyze the results from our demonstrations and discuss future developments to be implemented with our system.\n\n1. An open-source XR system for interactive motion planning, enabling users to interact with the environment and the robot.\n\n2. Immersive visualization of feedback from the motion planner including output trajectories.\n\n3. Integration with ROS2, allowing execution of robot motions planned using a virtual robot on physical robots in the real world.\n\n4. A set of demonstrations showing the capabilities of our proposed system in simulation and on real robots.",
            "llm_summary": "【论文的motivation是什么】  \n1. 在交互式运动规划中，用户需要更好地理解和配置三维环境。  \n2. 现有的运动规划系统缺乏对环境动态调整的支持。  \n3. 需要一种新的工具来增强用户与机器人及环境的互动能力。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 早期的交互式运动规划系统仅支持二维界面，限制了用户的空间理解。  \n2. 一些系统允许用户选择路径，但不支持环境对象的动态修改。  \n3. 很少有系统支持ROS2，并且几乎没有系统允许用户在不停止系统的情况下创建和修改虚拟对象。  \n\n【提出了什么创新的方法】  \nERUPT是一个开源的扩展现实工具包，允许用户在三维环境中交互式地进行运动规划。该系统集成了ROS2和MoveIt，用户可以动态配置环境、规划机器人路径并可视化运动轨迹。ERUPT提供了多种交互方式，用户可以在虚拟空间中安全地调整环境对象，确保机器人路径的可行性。通过这一系统，用户能够在真实世界中执行虚拟机器人规划的路径，提升了机器人操作的直观性和灵活性。系统的演示展示了其在模拟和真实机器人上的应用能力，证明了其在交互式运动规划中的有效性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Improving Cooperation in Collaborative Embodied AI",
            "authors": "Hima Jacob Leven Suprabha,Laxmi Nag Laxminarayan Nagesh,Ajith Nair,Alvin Reuben Amal Selvaster,Ayan Khan,Raghuram Damarla,Sanju Hannah Samuel,Sreenithi Saravana Perumal,Titouan Puech,Venkataramireddy Marella,Vishal Sonar,Alessandro Suglia,Oliver Lemon",
            "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Robotics (cs.RO)",
            "comment": "In proceedings of UKCI 2025",
            "pdf_link": "https://arxiv.org/pdf/2510.03153",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.03153",
            "arxiv_html_link": "https://arxiv.org/html/2510.03153v1",
            "abstract": "The integration of Large Language Models (LLMs) into multiagent systems has opened new possibilities for collaborative reasoning and cooperation with AI agents.\nThis paper explores different prompting methods and evaluates their effectiveness in enhancing agent\ncollaborative behaviour and decision-making. We enhance CoELA, a framework designed for building Collaborative Embodied Agents that leverage LLMs for multi-agent communication, reasoning, and task coordination in shared virtual spaces. Through systematic experimentation, we examine different LLMs and prompt engineering strategies to identify optimised combinations that maximise collaboration performance. Furthermore, we extend our research by integrating speech capabilities, enabling seamless collaborative voice-based interactions. Our findings highlight the effectiveness of prompt optimisation in enhancing collaborative agent performance; for example, our best combination improved the efficiency of the system running with Gemma3 by 22% compared to the original CoELA system. In addition, the speech integration provides a more engaging user interface for iterative system development and demonstrations.\nAlice and Bob – Voice Chat GUI Video demonstration.",
            "introduction": "As artificial intelligence continues to evolve, there is growing interest in enabling agents not just to act intelligently on their own but to work together – collaboratively solving problems, sharing tasks, and communicating effectively. This vision is central to Collaborative Embodied AI (CEAI), where virtual or physical agents operate in shared environments, coordinating their actions in pursuit of common goals [3, 16, 21, 9, 17].\n\nWith the rise of Large Language Models (LLMs), a new set of possibilities has emerged. LLMs are capable of understanding nuanced instructions, engaging in conversation, and even generating plans – making them ideal candidates for powering collaborative agents. However, integrating LLMs into embodied agents presents unique challenges: how can these models reason about the environment? How should agents communicate effectively? And what types of prompts or memory structures are most useful in team-based scenarios?\n\nThis work builds on the CoELA [16, 21] framework – a modular system for creating multi-agent embodied AI – exploring how different prompting strategies and LLMs affect coordination, decision-making, and task performance.\nSee figure 1 for examples of agents collaborating in CoELA.\n\nBuilding on CoELA, we develop new structured and communication-focused prompts and evaluate them with models such as Llama 3.1 [4], DeepSeek r1 [2], Mistral [5], and Gemma3 [15].\nBy experimenting across multiple configurations, we aim to uncover the best combinations of LLMs and prompts for efficient teamwork. Ultimately, this work contributes to the broader goal of building embodied agents that interact not just intelligently, but collaboratively, much like humans do in real-world tasks.",
            "llm_summary": "【论文的motivation是什么】  \n1. 如何有效地将大型语言模型（LLMs）集成到多智能体系统中以增强合作和决策能力。  \n2. 现有的合作智能体框架在优化协作行为方面存在不足。  \n3. 需要探索不同的提示方法以提升智能体的协作表现。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 以往的研究集中在单个智能体的能力提升，缺乏多智能体协作的深入探讨。  \n2. 现有的框架如CoELA未充分利用LLMs的潜力来优化协作策略。  \n3. 许多研究未考虑语音交互在多智能体合作中的重要性。  \n\n【提出了什么创新的方法】  \n本研究基于CoELA框架，开发了新的结构化和沟通导向的提示方法，并与多种LLM（如Llama 3.1、DeepSeek r1等）进行实验。通过系统的实验，我们识别出最佳的LLM和提示组合，显著提升了智能体的协作性能。例如，最佳组合使得系统运行效率提高了22%。此外，语音集成功能提供了更具吸引力的用户界面，促进了系统的迭代开发和演示。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Mask2IV: Interaction-Centric Video Generation via Mask Trajectories",
            "authors": "Gen Li,Bo Zhao,Jianfei Yang,Laura Sevilla-Lara",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.03135",
            "code": "https://reagan1311.github.io/mask2iv",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.03135",
            "arxiv_html_link": "https://arxiv.org/html/2510.03135v1",
            "abstract": "Generating interaction-centric videos, such as those depicting humans or robots interacting with objects, is crucial for embodied intelligence, as they provide rich and diverse visual priors for robot learning, manipulation policy training, and affordance reasoning.\nHowever, existing methods often struggle to model such complex and dynamic interactions. While recent studies show that masks can serve as effective control signals and enhance generation quality, obtaining dense and precise mask annotations remains a major challenge for real-world use.\nTo overcome this limitation, we introduce Mask2IV, a novel framework specifically designed for interaction-centric video generation. It adopts a decoupled two-stage pipeline that first predicts plausible motion trajectories for both actor and object, then generates a video conditioned on these trajectories.\nThis design eliminates the need for dense mask inputs from users while preserving the flexibility to manipulate the interaction process.\nFurthermore, Mask2IV supports versatile and intuitive control, allowing users to specify the target object of interaction and guide the motion trajectory through action descriptions or spatial position cues.\nTo support systematic training and evaluation, we curate two benchmarks covering diverse action and object categories across both human-object interaction and robotic manipulation scenarios.\nExtensive experiments demonstrate that our method achieves superior visual realism and controllability compared to existing baselines.",
            "introduction": "未获取到引言",
            "llm_summary": "【论文的motivation是什么】  \n1. 生成交互中心视频对于具身智能至关重要，提供丰富的视觉先验。  \n2. 现有方法难以建模复杂和动态的交互。  \n3. 实际应用中获取密集和精确的掩码注释仍然是一个主要挑战。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有方法利用掩码作为控制信号，提高生成质量。  \n2. 但这些方法依赖于用户提供的密集掩码输入，限制了其灵活性和实用性。  \n\n【提出了什么创新的方法】  \nMask2IV是一个新颖的框架，采用解耦的两阶段管道，首先预测演员和物体的合理运动轨迹，然后基于这些轨迹生成视频。该设计消除了对用户密集掩码输入的需求，同时保留了操控交互过程的灵活性。此外，Mask2IV支持多样化和直观的控制，允许用户通过动作描述或空间位置提示来指定交互目标物体和引导运动轨迹。通过系统的训练和评估，我们建立了两个基准，涵盖人类-物体交互和机器人操控场景中的多样动作和物体类别。广泛的实验表明，我们的方法在视觉真实感和可控性方面优于现有基线。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "A Dimension-Decomposed Learning Framework for Online Disturbance Identification in Quadrotor SE(3) Control",
            "authors": "Tianhua Gao",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO); Optimization and Control (math.OC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.03100",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.03100",
            "arxiv_html_link": "https://arxiv.org/html/2510.03100v1",
            "abstract": "Quadrotor stability under complex dynamic disturbances and model uncertainties poses significant challenges. One of them remains the underfitting problem in high-dimensional features, which limits the identification capability of current learning-based methods. To address this, we introduce a new perspective: Dimension-Decomposed Learning (DiD-L), from which we develop the Sliced Adaptive-Neuro Mapping (SANM) approach for geometric control. Specifically, the high-dimensional mapping for identification is axially “sliced” into multiple low-dimensional submappings (“slices”). In this way, the complex high-dimensional problem is decomposed into a set of simple low-dimensional tasks addressed by shallow neural networks and adaptive laws. These neural networks and adaptive laws are updated online via Lyapunov-based adaptation without any pre-training or persistent excitation (PE) condition. To enhance the interpretability of the proposed approach, we prove that the full-state closed-loop system exhibits arbitrarily close to exponential stability despite multi-dimensional time-varying disturbances and model uncertainties. This result is novel as it demonstrates exponential convergence without requiring pre-training for unknown disturbances and specific knowledge of the model.",
            "introduction": "Quadrotor stability remains a critical issue under complex disturbances and model uncertainties.\nCurrent research can be broadly categorized into two primary orientations: conventional adaptive control (e.g., [3]-[8]) and learning-based control (e.g., [9]-[19]). Each category has its own advantages and limitations that need to be addressed.\n\nConventional adaptive methods generally provide better interpretability since they typically rely on explicit system structure or disturbance modeling. However, their performance may be limited when dealing with highly nonlinear disturbances, such as turbulent wind fields. For example, in [4, 5], wind effects are handled using linear drag coefficients and compensated by adaptive laws. In [3], [6, 7], the authors employ the Extended State Observer (ESO) to compensate for disturbances without explicit disturbance modeling, but it remains dependent on assumed structural properties.\n\nIn contrast, learning-based methods leverage neural networks to better approximate complex nonlinear features. These methods have been extensively validated through experiments and demonstrate fast convergence properties [9]-[14]. However, the approximation error of neural networks remains a significant concern, as these methods typically adopt shallow neural networks (SNN), which tend to underfit high-dimensional features during online process. To tackle this, recent studies on data-driven control leverage the representation power of deep neural networks (DNN) for precise offline identification of disturbance and uncertainty features [16]-[18]. These methods exhibit great potential, but issues remain in the weak interpretability of offline training process and generalization capability to unseen environments. Therefore, we attempt to develop a direct enhancement of learning-based methods without relying on data-driven identification.\n\nIn this paper, we present a new branch in learning-based control: Dimension-Decomposed Learning (DiD-L). The key idea is to decompose high-dimensional disturbances and uncertainties into multiple lower-dimensional features, which specifies and simplifies the task of each SNN. Our contributions in this work are summarized as follows:\n\n(1) Proposed the first DiD-L instance: Sliced Adaptive-Neuro Mapping (SANM) with the following advantages:\n\nFull-state Compensation-Some existing studies (e.g., [5, 6, 15, 16, 19]) only addressed force disturbance or moment disturbance, this work presents full-state compensation for multi-dimensional disturbances (both force and moment) and model uncertainties.\n\n𝐒𝐄​(3)\\mathbf{SE}(3) Compatibility-SANM can be deployed onto existing geometric control on 𝐒𝐄​(3)\\mathbf{SE}(3)[21], which does not rely on small-angle assumptions [5] or linearized models.\n\nHighly Customizable-The adaptive law and SNN on each slice can be individually customized based on the dynamic characteristics of different dimensions. Moreover, while this work proposes a 12-slice SANM, the number of slices can be flexibly adjusted according to disturbance rejection requirements during actual deployment.\n\nEfficient Representation-After dimension decomposition, only 5 neurons in a single layer achieve an effective approximation to unseen disturbance in each dimension.\n\nRapid Response-SANM learns disturbance features at the acceleration-level, thereby achieving a transient response.\n\nStrong Generalization-SNNs are updated online via Lyapunov-based adaptation, ensuring bounded weight estimation in unseen environments without persistent excitation (PE) condition.\n\nStrong Interpretability-A rigorous Lyapunov analysis that explicitly considers neural network approximation errors supports the interpretability of SANM.\n\nExponential Convergence- All state errors exponentially converge to an arbitrarily small ball.\n\n(2) Proved the Near-Exponential Stability (NES) of the proposed control system, a new concept defined in Definition 1, which is arbitrarily close to exponential stability. To the best of our knowledge, this result is novel in quadrotor learning-based control against disturbances and uncertainties.\n\n(3) Demonstrated the feasibility and advantages of SANM, through real-time simulation experiments performed in Gazebo, a high-fidelity physics simulator.\n\nThis paper is organized as follows. Section II describes the problem formulation. Section III introduces the design of SANM and controller. Section IV presents results of physics simulation experiments. Finally, Section V concludes the paper and discusses future work. The stability proof is supplemented in Appendix.\n\n1. Full-state Compensation-Some existing studies (e.g., [5, 6, 15, 16, 19]) only addressed force disturbance or moment disturbance, this work presents full-state compensation for multi-dimensional disturbances (both force and moment) and model uncertainties.\n\n2. 𝐒𝐄​(3)\\mathbf{SE}(3) Compatibility-SANM can be deployed onto existing geometric control on 𝐒𝐄​(3)\\mathbf{SE}(3)[21], which does not rely on small-angle assumptions [5] or linearized models.\n\n3. Highly Customizable-The adaptive law and SNN on each slice can be individually customized based on the dynamic characteristics of different dimensions. Moreover, while this work proposes a 12-slice SANM, the number of slices can be flexibly adjusted according to disturbance rejection requirements during actual deployment.\n\n4. Efficient Representation-After dimension decomposition, only 5 neurons in a single layer achieve an effective approximation to unseen disturbance in each dimension.\n\n5. Rapid Response-SANM learns disturbance features at the acceleration-level, thereby achieving a transient response.\n\n6. Strong Generalization-SNNs are updated online via Lyapunov-based adaptation, ensuring bounded weight estimation in unseen environments without persistent excitation (PE) condition.\n\n7. Strong Interpretability-A rigorous Lyapunov analysis that explicitly considers neural network approximation errors supports the interpretability of SANM.\n\n8. Exponential Convergence- All state errors exponentially converge to an arbitrarily small ball.",
            "llm_summary": "【论文的motivation是什么】  \n1. Quadrotor stability under complex disturbances and model uncertainties is a critical challenge.  \n2. Current learning-based methods struggle with underfitting high-dimensional features, limiting their identification capabilities.  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. Conventional adaptive control methods provide better interpretability but struggle with highly nonlinear disturbances.  \n2. Learning-based methods using neural networks show fast convergence but suffer from approximation errors and weak interpretability.  \n3. Recent data-driven control approaches leverage deep neural networks, yet they still depend on offline training and lack generalization to unseen environments.  \n\n【提出了什么创新的方法】  \n本研究提出了Dimension-Decomposed Learning (DiD-L)框架，开发了Sliced Adaptive-Neuro Mapping (SANM)方法。该方法通过将高维特征“切片”成多个低维子映射，简化了任务并提高了学习效率。SANM支持全状态补偿，兼容SE(3)几何控制，具有高度可定制性和强泛化能力。通过实时仿真实验，证明了该方法在未知干扰下的快速响应和近似指数稳定性，展示了其在四旋翼学习控制中的可行性和优势。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "VERNIER: an open-source software pushing marker pose estimation down to the micrometer and nanometer scales",
            "authors": "Patrick Sandoz,Antoine N. André,Guillaume J. Laurent",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02791",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02791",
            "arxiv_html_link": "https://arxiv.org/html/2510.02791v1",
            "abstract": "Pose estimation is still a challenge at the small scales. Few solutions exist to capture the 6 degrees of freedom of an object with nanometric and microradians resolutions over relatively large ranges. Over the years, we have proposed several fiducial marker and pattern designs to achieve reliable performance for various microscopy applications. Centimeter ranges are possible using pattern encoding methods, while nanometer resolutions can be achieved using phase processing of the periodic frames. This paper presents VERNIER, an open source phase processing software designed to provide fast and reliable pose measurement based on pseudo-periodic patterns. Thanks to a phase-based local thresholding algorithm, the software has proven to be particularly robust to noise, defocus and occlusion. The successive steps of the phase processing are presented, as well as the different types of patterns that address different application needs. The implementation procedure is illustrated with synthetic and experimental images. Finally, guidelines are given for selecting the appropriate pattern design and microscope magnification lenses as a function of the desired performance.",
            "introduction": "Microrobotics aims to develop small robots, precision manipulators and automated machines able to handle, assemble and characterize micro and nano objects [1]. These tasks require to monitor the position and the orientation of end-effectors as in usual industrial robotics.\n\nAt human scale, computer vision is widely used to track the movement of people and objects in many applications. Motion capture systems have become indispensable in film production, video game development, drone design and zoology [2]. A lot of fiducial markers have been proposed to track or localize robots in various domains such as construction, logistic, space, and surgery [3].\n\nWhen the size of the object of interest decreases, pose estimation becomes challenging due to the lack of space to integrate sensors and due to the constraint of microscopy imaging. Unlike regular cameras, microscopes suffer from narrow fields of view (FoV), short depths of field, low contrasts and out-of-focus blurs, and usual fiducial markers performs poorly in these conditions.\n\nThere are few alternatives to imaging for measuring the six degrees of freedom (DoF) of a solid with nanometer and microradian resolutions. The most common option is to use six laser interferometers, each of which provides a measurement with sub-nanometer resolution in a single direction [4, 5]. This setup is very cumbersome and requires complex calibration procedures. Furthermore, such approaches do not provide a sufficiently wide angular measurement range. Thus, computer vision remains an essential tool for estimating robot pose.\n\nMany vision-based methods have been proposed to tackle pose estimation at the small scales. In 2021, the Fatikow’s team published a review paper comparing the resolution and the range of state-of-the-art vision-based localization methods [6]. Fig. 1 is extracted from this article and shows that most methods at these scales, rely on template matching. The most precise methods use phase correlation and can achieve sub-nanometer resolutions. However, their measurement ranges are still limited by the microscope’s FoV.\n\nTo overcome this limitation, pseudo-periodic patterns can be used to encode the absolute position over centimetric ranges, while using phase measurement to achieve nanometer resolutions [7]. Based on this principle, we have designed several markers and patterns, along with dedicated processing algorithms, to ensure reliable performance in various microscopy applications. As it can be see in Fig. 1, this approach outperforms all others in terms of range-to-resolution ratio. Recently, we achieved the measurement of six DoF using these patterns with digital holographic microscopy that performs similarly to interferometer-based setups [8].\n\nThis article presents the summary of ten years of research in the form of an open-source C++ library, called VERNIER like the french mathematician Pierre Vernier who was the inventor and eponym of the vernier scale which increases the resolution of measuring devices like calipers. The VERNIER software provides a powerful tool to measure the 2D and 3D poses of objects observed through a microscope lens with unprecedented sub-pixel resolutions and ranges. The library works in combination with periodic markers fixed on the objects of interest, allowing accurate measurements even in noisy and blurred imaging contexts which commonly occurs in the microscopy context. The image processing involves two complementary steps, resulting on the one hand in fine but relative position measurement and, on the other hand, a coarser but absolute localization. The fine position process achieves high sub-pixel resolutions thanks to spectral analysis of the imaged periodic frame of the marker. Complementary coarse measurements are retrieved either from the marker contours or from a binary sequence encrypted within the periodic frame of the pattern.\n\nVERNIER is based on different types of markers, designed to meet different application requirements, and whose design can be easily adapted to specific imaging parameters, mainly magnification, FoV, and expected measurement range and resolution. The next section introduces the different markers and summarizes the main image processing steps required to retrieve the associated position data. Section III presents the software architecture and library functionality, while section IV gives examples of applications using each marker type.\nFinally, guidelines are proposed to enable future users to easily implement this measurement method and meet their specific needs.\nAll sources and data are available on GitHub and can be downloaded for evaluation and testing111https://github.com/vernierlib/vernier.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的姿态估计方法在微观尺度下的精度和范围有限。  \n2. 微观成像条件下，传统的视觉标记在噪声和模糊情况下表现不佳。  \n3. 需要一种新方法来实现高精度的六自由度（DoF）姿态估计。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有方法多依赖模板匹配，限制了测量范围和分辨率。  \n2. 激光干涉仪虽然提供高分辨率，但设置复杂且不够灵活。  \n3. 许多视觉方法在微观尺度下的应用效果不佳，缺乏有效的标记设计。  \n\n【提出了什么创新的方法】  \nVERNIER软件通过伪周期性图案和相位处理算法，提供快速、可靠的姿态测量。该方法包含两个互补步骤：细致的相对位置测量和粗略的绝对定位。细致测量通过傅里叶变换和相位分析实现高分辨率，而粗略测量则利用标记轮廓或嵌入的二进制序列进行。VERNIER能够在噪声和模糊成像条件下实现前所未有的亚像素分辨率和测量范围，适应不同应用需求的标记设计，使其在微观机器人和自动化领域具有广泛应用潜力。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Periodic Event-Triggered Prescribed Time Control of Euler-Lagrange Systems under State and Input Constraints",
            "authors": "Chidre Shravista Kashyap,Karnan A,Pushpak Jagtap,Jishnu Keshavan",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02769",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02769",
            "arxiv_html_link": "https://arxiv.org/html/2510.02769v1",
            "abstract": "This article proposes a periodic event-triggered adaptive barrier control policy for the trajectory tracking problem of perturbed Euler-Lagrangian systems with state, input, and temporal (SIT) constraints. In particular, an approximation-free adaptive-barrier control architecture is designed to ensure prescribed-time convergence of the tracking error to a prescribed bound while rejecting exogenous disturbances. In contrast to existing approaches that necessitate continuous real-time control action, the proposed controller generates event-based updates through periodic evaluation of the triggering condition. Additionally, we derive an upper bound on the monitoring period by analysing the performance degradation of the filtered tracking error to facilitate periodic evaluation of the event-triggered strategy. To this end, a time-varying threshold function is considered in the triggering mechanism to reduce the number of triggers during the transient phase of system behaviour. Notably, the proposed design avoids Zeno behaviour and precludes the need for continuous monitoring of the triggering condition. A simulation and experimental study is undertaken to demonstrate the efficacy of the proposed control scheme.",
            "introduction": "Tracking control of Euler-Lagrange (EL) systems has received significant attention with direct relevance to numerous industrial applications [1, 2, 3, 4]. Importantly, controller synthesis under stringent SIT constraints plays a vital role in various robotic applications. Accordingly, finite time [5] or fixed time control [6] techniques have been developed where tracking error convergence is achieved in a finite/fixed time [7]. However, in both these cases, a user-defined convergence time cannot be specified such that the tracking error converges in the prescribed settling time. In this regard, prescribed time control (PTC) methods [8] have been employed, where the user can specify the exact convergence time, in contrast with finite and fixed time control studies.\n\nMost existing results rely on time-triggered control or sampled-data policies, where the controller transmits signals at every sampling interval, regardless of system performance, leading to redundant use of communication and computational resources [9]. Event-triggered control (ETC) schemes address this issue by updating transmitted signals based on event scheduling [10], thereby reducing communication and computation demands. Despite the benefits of ETC compared to sampled-data policies, ETC schemes are evaluated at every sampling instant, called continuous event-triggered control (CETC) [10], which must ensure a positive minimum inter-event time (MIET) to avoid Zeno behaviour [11]. Although many theoretical frameworks mitigate this issue, CETC implementation on digital platforms is impractical without advanced hardware and sensors [12]. To overcome this, discrete event-triggered control based on discrete-time models was proposed in [13].\n\nNevertheless, exact discrete-time models for nonlinear systems are often infeasible, and approximations often degrade closed-loop performance [11]. To address this limitation, a periodic event-triggered control (PETC) [12] evaluates the triggering condition at intervals defined by the monitoring period, offering a cost-effective alternative to CETC. However, limited studies ensure prescribed-time convergence of EL systems under PETC while accounting for state and input constraints [8]. Therefore, the proposed study develops an approximation-free prescribed-time tracking policy for EL systems, accommodating operating constraints while reducing control communication bandwidth.\n\nThe contributions of the proposed methodology are qualitatively compared with leading studies in the design of PETC for EL systems, as shown in Table 1. The studies in [12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23] designed ETC for EL systems that depend on model parameters, which are often uncertain or unknown in practice. Alternatively, the studies in [24, 25, 26] employ adaptive parametric laws to estimate lumped uncertainties, followed by controller synthesis to achieve better tracking performance. Nevertheless, these studies [12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27] do not address the problem of prescribed time convergence for the EL system. To circumvent this limitation, the studies [28, 29, 30] designed PTC with nominal model parameters and time-varying gains to ensure prescribed time convergence. In contrast, PTC synthesis with parameter approximation techniques using radial basis functions and time-delayed estimation is studied in [31] and [32], respectively. However, the embedding of state constraints in PTC design is not met, unlike studies [33, 34] that leverage prescribed performance functions for guaranteeing state constraint satisfaction. Nonetheless, the studies [28, 29, 30, 31, 32, 33, 34] do not address the problem of event-triggered control of EL systems under SIT constraints.\n\nFurther, concerning MIET, most existing ETC policies [33, 30, 14, 22] primarily focus on preventing Zeno behaviour but fail to determine MIET explicitly. An additional concern pertains to the judicious selection of a suitable monitoring period based on MIET that enables PETC to capture all events the same way as those in CETC; otherwise, it risks missing events triggered by CETC. Furthermore, static-triggering strategies [27, 31, 33] impose constant thresholds on the event-triggering mechanism that offer limited flexibility in reducing communication frequency. Alternatively, self-triggering designs proposed in [17, 20] attempt to overcome this by pre-computing event times using MIET upper bounds; however, they execute control actions at fixed instants without verifying triggering conditions, resulting in suboptimal efficiency.\n\nTo address these limitations, this study proposes a periodic event-triggered adaptive barrier control policy to ensure robust prescribed-time prescribed-bound convergence of tracking error in EL systems under state and input constraints. Firstly, we leverage the time-based generator (TBG) function that prescribes settling time to construct the filtered tracking error. Then, time-varying inequalities on filtered tracking error are utilised to synthesise a state constraint law to avoid state constraint violations and impose input constraints via a saturation function. Additionally, the triggering mechanism employs a time-varying threshold to reduce communication frequency, and conditions on the monitoring period are derived to limit evaluation of the triggering condition, while preserving the desired tracking error performance. Numerical and experimental validation studies are undertaken to demonstrate the efficacy and superior performance of the proposed scheme compared to leading alternative designs. Thus, the major contributions of this article can be summarized below:\n\nAn approximation-free periodic event-based adaptive barrier control policy is proposed to reduce communication frequency for tracking control of EL systems under SIT constraints.\n\nIn contrast with the studies in [12, 31], which fail to address monitoring period selection, this study provides a detailed analysis of its impact within the minimum and maximum inter-event time (IET) bounds. In particular, by appropriately choosing the monitoring period under the proposed scheme, robust local PTPB convergence of the tracking error is ensured with reduced control updates.\n\nWe denote Matrices and vectors by bold letters. Throughout this paper, ℝn\\mathbb{R}^{n} and ℝn×n\\mathbb{R}^{n\\times n} represent the set of all n−n-dimensional real vectors and n×nn\\times n real matrices, respectively. The set of all positive (non-negative) integers and reals is denoted by ℕ+\\mathbb{N}^{+} (ℕ≥0\\mathbb{N}_{\\geq 0}) and ℝ+\\mathbb{R}^{+} (ℝ≥0\\mathbb{R}_{\\geq 0}), respectively. 𝕊+n​(𝔻+n)\\mathbb{S}_{+}^{n}~(\\mathbb{D}_{+}^{n}) denotes the set of all n×nn\\times n positive definite (positive diagonal) matrices. For a matrix XX, the notation X>0X>0 (X<0X<0) symbolizes the positive (negative) definiteness of that matrix. 𝑰n\\bm{I}_{n} denotes the identity of order nn and nn-dimensional vector having all zeros and ones are denoted by 𝟎n\\bm{0}_{n} and 𝟏n\\bm{1}_{n}, respectively. ∥𝒂∥\\lVert\\bm{a}\\rVert denotes the Euclidean norm of a vector 𝒂∈ℝn\\bm{a}\\in\\mathbb{R}^{n} and ∥𝑨∥\\lVert\\bm{A}\\rVert represents induced-2 norm for a matrix 𝑨∈ℝm×n\\bm{A}\\in\\mathbb{R}^{m\\times n}. diag​(…)\\text{diag}(\\ldots) refers for the diagonal matrix. Denote ℕn={1,2,…,n}\\mathbb{N}_{n}=\\{1,2,\\ldots,n\\}. For vectors 𝒂,𝒃∈ℝn\\bm{a},\\bm{b}\\in\\mathbb{R}^{n}, inequality 𝒂⪯𝒃\\bm{a}\\preceq\\bm{b} indicates that ai≤bia_{i}\\leq b_{i}, for all i∈ℕni\\in\\mathbb{N}_{n}. For the scalar xx, the symbols x¯\\underline{x} and x¯\\overline{x}, respectively, denote the lower and upper bounds of the corresponding term.",
            "llm_summary": "大模型总结失败",
            "llm_score": 0,
            "llm_error": "API 状态码异常：403，响应：{\"error\":{\"message\":\"免费API限制模型输入token小于4096，如有更多需求，请访问 https://api.chatanywhere.tech/#/shop 购买付费API。The number of prompt tokens for free accounts is limited to 4096. If you have additional requirements, please visit https://api.chatanywhere.tech/#/shop to purchase a premium key.(当前请求使用的ApiKey: sk-8l9****i4zt)【如果您遇到问题，欢迎加入QQ群咨询：1048463714】\",\"type\":\"chatanywhere_error\",\"param\":null,\"code\":\"403 FORBIDDEN\"}}"
        },
        {
            "crawl_datetime": "2025-10-06 02:25:34",
            "title": "Conceptualizing and Modeling Communication-Based Cyberattacks on Automated Vehicles",
            "authors": "Tianyi Li,Tianyu Liu,Yicheng Yang",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.02364",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.02364",
            "arxiv_html_link": "https://arxiv.org/html/2510.02364v1",
            "abstract": "Adaptive Cruise Control (ACC) is rapidly proliferating across electric vehicles (EVs) and internal combustion engine (ICE) vehicles, enhancing traffic flow while simultaneously expanding the attack surface for communication-based cyberattacks. Because the two powertrains translate control inputs into motion differently, their cyber-resilience remains unquantified.\nTherefore, we formalize six novel message-level attack vectors and implement them in a ring-road simulation that systematically varies the ACC market penetration rates (MPRs) and the spatial pattern of compromised vehicles.\nA three-tier risk taxonomy converts disturbance metrics into actionable defense priorities for practitioners. Across all simulation scenarios, EV platoons exhibit lower velocity standard deviation, reduced spacing oscillations, and faster post-attack recovery compared to ICE counterparts, revealing an inherent stability advantage.\nThese findings clarify how controller-to-powertrain coupling influences vulnerability and offer quantitative guidance for the detection and mitigation of attacks in mixed automated traffic.",
            "introduction": "Advancements in vehicle automation have been widely recognized for their potential to significantly improve transportation systems. Automated Vehicles (AVs) can enhance traffic flow [1], facilitate effective coordination [2], and contribute to greater energy efficiency [3].\nMoreover, AVs demonstrate the potential to significantly enhance roadway safety by reducing human error [4] and to contribute to environmental sustainability through optimized driving behaviors and reduced emissions [5].\n\nHowever, the increasing integration of adaptive cruise control (ACC)-equipped vehicles expands the potential vulnerability exposure, thereby heightening the risk of cyber threats. Malicious attacks targeting AVs can compromise operational safety, potentially resulting in traffic collisions, property damage, and threats to human life [6, 7].\nThe inherent connectivity of AVs significantly increases their exposure to cyber threats. Given their reliance on inter-vehicle coordination and environmental sensing, AVs are especially susceptible to advanced attacks targeting automation functions.\n\nStructurally, AVs consist of three primary subsystems: the driving system, the automotive control system, and the communication system [8], each of which presents distinct cybersecurity vulnerabilities.\nAttacks on the driving system differ significantly from those on traditional vehicles without ACC systems [9].\nModern driving systems increasingly depend on a range of components, including the Global Positioning System (GPS), mobile applications, and sensors. And these sensors are composed of cameras, LiDAR [10], radar, and magnetic encoders [11], all of which are used to receive and interpret external signals, enabling accurate perception and interaction with the surrounding environment.\nSensors’ constant interaction with dynamic environments renders them particularly susceptible to cyberattacks. For instance, GPS spoofing attacks poses a substantial threat to AVs due to their strong reliance on GPS signals for accurate localization and navigation [12]. The deliberate distortion or falsification of GPS data may induce erroneous position estimation, leading to incorrect decision-making and increasing the risk of traffic incidents.\nIn addition, jamming attacks specifically target the LiDAR sensor by emitting interfering signals, which obstruct the sensor’s ability to receive environmental information timely, thereby compromising the vehicle’s situational awareness and operational safety [10].\n\nCyberattacks targeting the automotive control system often focus on critical components including the electronic control unit, inertial measurement unit (IMU) [9], in-vehicle networks, automotive keys, and ACC system [13]. The aforementioned attacks may lead to leakage of personal data, degrade system performance, and impair inter-vehicle coordination. Notably, attacks on car keys can enable unauthorized vehicle ignition without the physical key [14]; Distributed denial of service attacks targeting network interfaces can introduce significant delays in command transmission, disrupting timely vehicle operation [12].\n\nCyberattacks targeting the communication system are often more frequent and pose greater risks due to their role as the essential conduit for real-time data exchange and inter-vehicle coordination [6].\nThe communication system facilitates Vehicle-to-Vehicle (V2V) [9], Vehicle-to-Infrastructure (V2I) [15], and Vehicle-to-Pedestrian (V2P) interactions [16]. However, reliance on wireless communication inherently introduces security vulnerabilities that can be exploited by malicious actors [17].\n\nCyberattacks on V2V communication can disrupt the exchange of critical real-time information and coordination among connected vehicles. To name a few, spam attacks can consume the system bandwidth and degrade communication efficiency [18];\nDenial-of-Service (DoS) attacks can delay communication between vehicles, thereby affecting their normal driving behavior [19];\nPacket Dropping Attack (PDA) selectively discards data during message transmission [20]. Specifically, a malicious node masquerades as a router and drops incoming packets [21], disrupting data flow [22] and compromising network security [23].\nCyberattacks on V2I communication target the data transfer between vehicles and roadside units, enabling attackers to transmit false information [24].\nFor instance, man-in-the-middle attacks intercept and alter communications between vehicles and external entities, compromising message integrity and potentially leading to unsafe or erroneous vehicle behavior [25].\nSybil attacks compromise infrastructure by transmitting fabricated identity data, misleading traffic control systems, and disrupting signal coordination, thereby degrading overall transportation efficiency [24].\nReplay attacks occur when an adversary retransmits recorded traffic signal information. It may cause vehicles to misinterpret current traffic conditions, leading to incorrect decision-making and compromised roadway safety [9].\n\nCyberattacks on V2P communication target the transmission of critical safety information between vehicles and pedestrians, increasing the risk of accidents and compromising pedestrian safety [26].\nFor instance, false data injection attacks can disrupt communication systems by introducing erroneous information, preventing accurate reception of data from pedestrians [27]. Additionally, Sybil and DoS attacks can disrupt and delay information exchange between vehicles and pedestrians, thereby impairing V2P communication.\nTherefore, cyberattacks on the communication system of AVs can burden network resources and may even render functional services unavailable [28, 29].\n\nWith the rapid advancement of network technologies, AVs are increasingly capable of interacting with their surrounding environments. While traditional cyberattacks have been extensively studied, they are gradually becoming outdated as numerous variants emerge from these conventional attack types. Many of these evolving threats remain underexplored despite exhibiting greater intensity and disruptive potential, posing significant risks to the integrity and reliability of AV systems. Furthermore, the impact of cyberattacks varies considerably across different AV platforms.\nTo address these challenges, this study proposes six types of communication-based cyberattacks targeting AVs equipped with ACC, with particular emphasis on their effects on communication systems. We specifically investigate the differential responses between electric vehicles (EVs) and internal combustion engine (ICE) vehicles equipped with ACC when subjected to these attack scenarios. Through a comprehensive analysis of these cyberattacks, we aim to deepen understanding of their underlying mechanisms and consequences, thereby contributing to enhanced driving system safety and reliability.\nThe primary contributions and innovations of this study are presented as follows:\n\nThis study proposes and mathematically formalizes six novel communication-based cyberattacks targeting ACC-equipped vehicles. The analysis divides the driving process into three sequential phases: pre-attack, during-attack, and post-attack. By examining potential attack scenarios from the adversary’s perspective, this work provides critical insights into the underlying attack mechanisms and associated system vulnerabilities.\n\nThis study proposes and mathematically formalizes six novel communication-based cyberattacks targeting ACC-equipped vehicles. The analysis divides the driving process into three sequential phases: pre-attack, during-attack, and post-attack. By examining potential attack scenarios from the adversary’s perspective, this work provides critical insights into the underlying attack mechanisms and associated system vulnerabilities.\n\nWe successfully implement the proposed cyberattacks in a ring-road simulation that systematically varies the ACC market penetration rates (MPRs) and the spatial configurations of compromised vehicles, including non-adjacent and adjacent attacked patterns.\n\nWe conduct a comparative analysis to systematically assess the differential impacts of cyberattacks on EVs and ICE vehicles through comprehensive simulations, utilizing velocity standard deviation and spacing standard deviation as key performance metrics. Based on observed collision potentials, we classify the proposed cyberattacks into three distinct risk categories, providing a framework for risk assessment and mitigation strategies.\n\n1. This study proposes and mathematically formalizes six novel communication-based cyberattacks targeting ACC-equipped vehicles. The analysis divides the driving process into three sequential phases: pre-attack, during-attack, and post-attack. By examining potential attack scenarios from the adversary’s perspective, this work provides critical insights into the underlying attack mechanisms and associated system vulnerabilities.\n\n2. We successfully implement the proposed cyberattacks in a ring-road simulation that systematically varies the ACC market penetration rates (MPRs) and the spatial configurations of compromised vehicles, including non-adjacent and adjacent attacked patterns.\n\n3. We conduct a comparative analysis to systematically assess the differential impacts of cyberattacks on EVs and ICE vehicles through comprehensive simulations, utilizing velocity standard deviation and spacing standard deviation as key performance metrics. Based on observed collision potentials, we classify the proposed cyberattacks into three distinct risk categories, providing a framework for risk assessment and mitigation strategies.",
            "llm_summary": "大模型总结失败",
            "llm_score": 0,
            "llm_error": "API 状态码异常：403，响应：{\"error\":{\"message\":\"免费API限制模型输入token小于4096，如有更多需求，请访问 https://api.chatanywhere.tech/#/shop 购买付费API。The number of prompt tokens for free accounts is limited to 4096. If you have additional requirements, please visit https://api.chatanywhere.tech/#/shop to purchase a premium key.(当前请求使用的ApiKey: sk-8l9****i4zt)【如果您遇到问题，欢迎加入QQ群咨询：1048463714】\",\"type\":\"chatanywhere_error\",\"param\":null,\"code\":\"403 FORBIDDEN\"}}"
        }
    ],
    "2025-10-07": [],
    "2025-10-08": [
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model",
            "authors": "Zefu Lin,Rongxu Cui,Chen Hanning,Xiangyu Wang,Junjia Xu,Xiaojuan Jin,Chen Wenbo,Hui Zhou,Lue Fan,Wenling Li,Zhaoxiang Zhang",
            "subjects": "Robotics (cs.RO)",
            "comment": "Demo Page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.06207",
            "code": "https://anonymous.4open.science/w/Embodied-Coder/",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.06207",
            "arxiv_html_link": "https://arxiv.org/html/2510.06207v1",
            "abstract": "Recent advances in control robot methods, from end-to-end vision-language-action frameworks to modular systems with predefined primitives, have advanced robots’ ability to follow natural language instructions. Nonetheless, many approaches still struggle to scale to diverse environments, as they often rely on large annotated datasets and offer limited interpretability.\nIn this work, we introduce EmbodiedCoder, a training-free framework for open-world mobile robot manipulation that leverages coding models to directly generate executable robot trajectories.\nBy grounding high-level instructions in code, EmbodiedCoder enables flexible object geometry parameterization and manipulation trajectory synthesis without additional data collection or fine-tuning.\nThis coding-based paradigm provides a transparent and generalizable way to connect perception with manipulation.\nExperiments on real mobile robots show that EmbodiedCoder achieves robust performance across diverse long-term tasks and generalizes effectively to novel objects and environments.\nOur results demonstrate an interpretable approach for bridging high-level reasoning and low-level control, moving beyond fixed primitives toward versatile robot intelligence. See the project page at https://anonymous.4open.science/w/Embodied-Coder/.",
            "introduction": "Enabling robots to perform diverse tasks with human-like proficiency in complex, unstructured environments has long been a central goal in robotics [1].\nRecent progress in vision-language-action (VLA) models has brought this ambition closer to reality by enabling end-to-end mapping from sensory inputs and natural language instructions to robot actions. However, their generalization ability remains limited. Even slight changes in the environment, such as variations in object appearance or illumination, can significantly degrade performance. Furthermore, these models typically require massive annotated datasets, making their deployment costly and less scalable.\n\nTo overcome these challenges, hierarchical strategies have been proposed. A common design, such as DovSG [2] and OK-Robot [3], is to employ vision-language models (VLMs) [4, 5] as high-level planners that decompose tasks into subtasks and invoke predefined robotic primitives, such as navigation, grasping, or pick-and-place. This paradigm is theoretically appealing, since it allows robots to leverage the commonsense knowledge encoded in large-scale models while relying on robust control modules for low-level execution. In practice, however, its effectiveness is fundamentally constrained by the repertoire of available manipulation primitives. Many real-world tasks, such as opening doors or drawers, require nuanced interactions that cannot be reduced to a finite set of predefined primitives.\n\nRecent work has attempted to extend beyond this primitive-based architecture by generating executable code for manipulation. Code-as-Policies [6] demonstrated that an LLM can write low-level code to control a robot, but this early attempt was limited to tasks with very simple, specific geometries. RoboCodeX [7] uses a multimodal code generation framework to broaden task generality, yet it relies on learned models to handle physical constraints, which reduces its adaptability to novel scenarios. VoxPoser [8] computes obstacle-aware end-effector trajectories for tasks like drawer opening, but it cannot perform more intricate, contact-rich manipulations. Likewise, Code-as-Monitor [9] generates code to detect and recover from execution failures, but it does not expand the robot’s basic manipulation repertoire beyond the original primitives.\nFor wheeled robots, the task complexity becomes even higher [10]. The robot must be able to retain information about the environment, which allows it to incorporate objects beyond its immediate field of view into the task planning process.\n\nTo address these challenges, we propose EmbodiedCoder, a code-driven framework for open-world mobile robot manipulation.\nUnlike traditional training-intensive approaches, our method leverages the expressive power of coding models to generate executable code that directly encodes manipulation strategies.\nThis design transforms high-level instructions into programmatic representations of geometric parameterization and trajectory synthesis.\nBy grounding the reasoning process in code, the system benefits from both interpretability and flexibility, enabling robots to adapt to novel objects and environments without additional training or fine-tuning.\n\nAt the core of our framework, code serves as the medium that bridges perception and manipulation.\nThe process begins with scene understanding, where VGGT [11] and a vision-language model capture RGB-D observations and ground semantic information into 3D point representations.\nBased on this input, EmbodiedCoder prompts the coding models to generate code for two critical stages.\nFirst, in code-driven geometric parameterization, the system fits point clouds of task-relevant objects to geometry parametric primitives that encode functional affordances, such as approximating a drawer as a cuboid with a pulling axis.\nSecond, in code-driven trajectory synthesis, our method produces programmatic descriptions of feasible motion trajectories that satisfy physical, environmental, and task-specific constraints.\nThe trajectories are first represented as parameterized curves, from which discrete waypoints are sampled and subsequently executed by the robot.\nBy this approach, the system not only achieves robust performance in novel environments but also provides a transparent and generalizable mechanism for linking perception with real-world manipulation.\n\nIn summary, our method not only alleviates the dependency on predefined primitives but also eliminates costly data collection and fine-tuning.\nOur contributions are threefold:\n\nWe introduce a framework that integrates coding models with embodied agents, enabling complex long-term manipulations in real-world environments.\n\nWe propose a novel method for parameterizing objects into functional geometric abstractions, allowing pretrained knowledge to be grounded into executable trajectories for sophisticated.\n\nWe validate EmbodiedCoder on real mobile robots and demonstrate its effectiveness in handling diverse tasks, showing improved generalization and training-free deployment compared to existing approaches.\n\n1. We introduce a framework that integrates coding models with embodied agents, enabling complex long-term manipulations in real-world environments.\n\n2. We propose a novel method for parameterizing objects into functional geometric abstractions, allowing pretrained knowledge to be grounded into executable trajectories for sophisticated.\n\n3. We validate EmbodiedCoder on real mobile robots and demonstrate its effectiveness in handling diverse tasks, showing improved generalization and training-free deployment compared to existing approaches.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人在复杂环境中执行多样任务的能力仍然有限。  \n2. 现有方法依赖大量标注数据，缺乏可解释性和适应性。  \n3. 需要一种能够灵活应对新环境和对象的无训练框架。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有的VLA模型依赖于大量示范数据，难以在环境变化时保持性能。  \n2. 代码生成方法如Code-as-Policies和RoboCodeX在复杂操作中灵活性不足，且依赖于预定义的技能。  \n3. 许多方法无法处理接触丰富的交互，限制了机器人适应新任务的能力。  \n\n【提出了什么创新的方法】  \n我们提出了EmbodiedCoder，一个无训练的框架，通过编码模型直接生成可执行的机器人轨迹。该方法将高层指令转化为程序化的几何参数化和轨迹合成，消除了对预定义技能的依赖。EmbodiedCoder通过场景理解和代码驱动的几何参数化与轨迹合成，提供了一种透明且可扩展的方式，将感知与操作连接起来。实验表明，EmbodiedCoder在多样的长期任务中表现出色，并能有效地推广到新对象和环境，展示了高层推理与低层控制之间的可解释性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "DYMO-Hair: Generalizable Volumetric Dynamics Modeling for Robot Hair Manipulation",
            "authors": "Chengyang Zhao,Uksang Yoo,Arkadeep Narayan Chaudhury,Giljoo Nam,Jonathan Francis,Jeffrey Ichnowski,Jean Oh",
            "subjects": "Robotics (cs.RO)",
            "comment": "Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.06199",
            "code": "https://chengyzhao.github.io/DYMOHair-web/",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.06199",
            "arxiv_html_link": "https://arxiv.org/html/2510.06199v1",
            "abstract": "Hair care is an essential daily activity, yet it remains inaccessible to individuals with limited mobility and challenging for autonomous robot systems due to the fine-grained physical structure and complex dynamics of hair. In this work, we present DYMO-Hair, a model-based robot hair care system. We introduce a novel dynamics learning paradigm that is suited for volumetric quantities such as hair, relying on an action-conditioned latent state editing mechanism, coupled with a compact 3D latent space of diverse hairstyles to improve generalizability. This latent space is pre-trained at scale using a novel hair physics simulator, enabling generalization across previously unseen hairstyles.\nUsing the dynamics model with a Model Predictive Path Integral (MPPI) planner, DYMO-Hair is able to perform visual goal-conditioned hair styling. Experiments in simulation demonstrate that DYMO-Hair’s dynamics model outperforms baselines on capturing local deformation for diverse, unseen hairstyles. DYMO-Hair further outperforms baselines in closed-loop hair styling tasks on unseen hairstyles, with an average of 22% lower final geometric error and 42% higher success rate than the state-of-the-art system.\nReal-world experiments exhibit zero-shot transferability of our system to wigs, achieving consistent success on challenging unseen hairstyles where the state-of-the-art system fails.\nTogether, these results introduce a foundation for model-based robot hair care, advancing toward more generalizable, flexible, and accessible robot hair styling in unconstrained physical environments.\nMore details are available on our project page: https://chengyzhao.github.io/DYMOHair-web/.",
            "introduction": "Hair is central to personal identity and self-esteem [1, 2], yet routine care is difficult for individuals with limited mobility due to reduced coordination, strength, and flexibility [3]. To improve accessibility and autonomy, robot hair care systems have been explored [4, 5, 6, 7], but existing approaches rely on either handcrafted trajectories or rule-based controllers, restricting generalization across diverse hairstyles and goals.\n\nTo address these limitations, we propose DYMO-Hair, a model-based robot hair care system. Our system is capable of generalizable and flexible visual goal-conditioned hair manipulation, across diverse hairstyles and objectives in unconstrained physical environments.\nAt the core of our system is a dynamics model that captures diverse hair deformations across various hairstyles and combing motions.\n\nFor deformable objects like hair, complex structures and unobservable properties make accurate dynamics modeling difficult. While analytical physics-based models exist, they are computationally expensive and impractical for real-time control, motivating the use of learning-based neural dynamics as proxies [8, 9, 10].\nHowever, hair poses unique challenges:\n1) Representation. Low-resolution point clouds cannot capture strand-level geometry.\n2) Structure. Graph-based methods scale poorly as point counts increase for higher resolution.\n3) Supervision. Global metrics miss fine-scale deformations, while point-wise correspondence is impractical for strands.\n4) Data. Hair entanglement makes real-world data collection slow and difficult to reset across styles.\n\nTo address these challenges, we introduce a novel paradigm for generalizable volumetric hair dynamics modeling. We present the first 3D hair-combing dynamics model that leverages large-scale diverse synthetic data for hair dynamics learning and generalizes across various hairstyles. We represent hair as a high-resolution volumetric occupancy grid with a 3D orientation field to capture both hair position and local strand flow, which offers more geometric details and structural information of hair than sparse point clouds.\nIt also allows dense supervision on both occupancy and orientation, providing sufficient local signals for the model to capture fine-grained deformations during learning.\nOur key innovation, inspired by ControlNet [11], is to pre-train a compact 3D latent space for diverse hair states and to introduce a control branch that models dynamics as action-conditioned state editing, enabling significantly improved generalizability through large-scale pre-training.\nTo avoid time-consuming real-world data collection required by the pre-training, we further develop a hair-combing simulator based on Genesis [12]. It leverages a novel formulation of the position-based dynamics (PBD) method for strand-level, contact-rich hair simulation, enabling efficient large-scale generation of visually-realistic and physically-plausible synthetic dynamics data across diverse hairstyles.\nExperiments in simulation demonstrate that our model outperforms baselines on generalizable hair dynamics modeling for local hair deformation across diverse unseen hairstyles.\n\nBuilding on our dynamics model, we introduce DYMO-Hair, a unified, model-based robot hair care system for visual goal-conditioned hair styling. We adopt a Model Predictive Control (MPC) framework, using a Model Predictive Path Integral (MPPI)-based planner to optimize an action trajectory that minimizes the geometric distance between predicted hair states and the objective [13, 14].\nSimulation experiments on diverse unseen hairstyles show that DYMO-Hair achieves superior effectiveness and generalizability for closed-loop hair styling compared to all system baselines, with an average of 22% lower final geometric error and 42% higher absolute success rate than the state-of-the-art system.\nReal-world demonstrations further exhibit zero-shot transferability of DYMO-Hair to physical wigs, achieving consistent success on challenging unseen hairstyles where the state-of-the-art system fails.\n\nTo summarize, our contributions are:\n\nA study of model-based approaches for robot hair manipulation.\n\nDYMO-Hair, a unified, model-based robot system for visual goal-conditioned hair styling, evaluated across diverse hairstyles in simulation and real-world settings.\n\nA 3D generalizable volumetric dynamics model for hair combing.\n\nA hair simulator with a novel PBD method for strand-level, contact-rich hair-combing simulation.\n\n1. A study of model-based approaches for robot hair manipulation.\n\n2. DYMO-Hair, a unified, model-based robot system for visual goal-conditioned hair styling, evaluated across diverse hairstyles in simulation and real-world settings.\n\n3. A 3D generalizable volumetric dynamics model for hair combing.\n\n4. A hair simulator with a novel PBD method for strand-level, contact-rich hair-combing simulation.",
            "llm_summary": "【论文的motivation是什么】  \n1. 提高机器人对头发的操控能力，以改善残疾人士的日常护理。  \n2. 解决现有机器人头发护理方法在多样性和目标灵活性上的局限性。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有方法依赖手工轨迹或规则控制，缺乏对多样化发型的适应能力。  \n2. 物理基础建模计算开销大，实时控制不切实际，且现有学习方法未能有效捕捉头发的细微变形。  \n\n【提出了什么创新的方法】  \n我们提出了DYMO-Hair，一个基于模型的机器人头发护理系统，采用新颖的动态学习范式，利用动作条件的潜在状态编辑机制和紧凑的3D潜在空间来提高对多样化发型的泛化能力。通过大规模预训练的合成数据，我们的系统能够在未见过的发型上实现视觉目标条件的发型设计。实验结果显示，DYMO-Hair在模拟中相较于基线方法在局部变形捕捉上表现更优，且在闭环发型设计任务中，最终几何误差降低了22%，成功率提高了42%。在真实世界实验中，系统实现了对假发的零-shot迁移，成功处理了复杂的未见发型。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "A Preview of HoloOcean 2.0",
            "authors": "Blake Romrell,Abigail Austin,Braden Meyers,Ryan Anderson,Carter Noh,Joshua G. Mangelson",
            "subjects": "Robotics (cs.RO)",
            "comment": "submitted to the ICRA 2025 aq2uasim workshop",
            "pdf_link": "https://arxiv.org/pdf/2510.06160",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.06160",
            "arxiv_html_link": "https://arxiv.org/html/2510.06160v1",
            "abstract": "Marine robotics simulators play a fundamental role in the development of marine robotic systems. With increased focus on the marine robotics field in recent years, there has been significant interest in developing higher fidelity simulation of marine sensors, physics, and visual rendering capabilities to support autonomous marine robot development and validation.\nHoloOcean 2.0, the next major release of HoloOcean, brings state-of-the-art features under a general marine simulator capable of supporting a variety of tasks.\nNew features in HoloOcean 2.0 include migration to Unreal Engine (UE) 5.3, advanced vehicle dynamics using models from Fossen, and support for ROS2 using a custom bridge.\nAdditional features are currently in development, including significantly more efficient ray tracing-based sidescan, forward-looking, and bathymetric sonar implementations; semantic sensors; environment generation tools; volumetric environmental effects; and realistic waves.",
            "introduction": "Marine robotics simulators have supported research and development for autonomous underwater and surface vessels for several decades.\nSimulation is a critical capability that enables development of algorithms for navigation, perception, manipulation, and control, as well as validation of real-world systems and missions.\n\nAs the field of underwater robotics has grown, so has the need for high-fidelity simulations.\nAn increased emphasis on vision-based algorithms in mobile robotics has pushed simulators toward photorealistic graphics renderings.\nComplex missions involving multiple agents or long duration operations require well-modeled dynamics to ensure accurate results.\n\nIn the last several years, marine robotics simulators have seen significant advancements to meet these needs.\nAt least six new simulators have been released in the last six years [1, 2, 3, 4, 5, 6, 7].\nSome simulators such as LRAUV [1] and Stonefish [7] focus on precise vehicle dynamics.\nOthers such as MARUS [2] and UNavSim [3] leverage the high-quality visual rendering available from modern game engines such as Unity [8] and Unreal Engine [9] to enable vision-based algorithms and artificial intelligence.\n\nThe HoloOcean simulator was released in 2022, with the objective of providing high-fidelity visuals and sensor models to enable algorithm development for marine robot navigation, perception, estimation, and localization [10, 11].\nIt emphasized detailed simulation of sonar sensors, including sidescan, imaging, and bathymetric sonars.\nIn the years since its release, HoloOcean has been utilized by researchers at universities and government agencies across the world.\n\nIn this paper, we give a preview of HoloOcean 2.0, a major update to HoloOcean.\nHoloOcean 2.0 brings more state-of-the-art features into a single, high-fidelity and user friendly simulator and introduces novel features not found in other simulators.\n\nThe paper is organized as follows. Section II describes the following new features available in HoloOcean 2.0, including:\n\nmigration to Unreal Engine (UE) 5.3,\n\nimproved vehicle dynamics,\n\nsupport for ROS2, and\n\nintegration of the BlueROV and CoUG-UV vehicles.\n\nSection III provides details on features and improvements currently in development, including:\n\nan improved sonar implementation using ray casting,\n\nsemantic labeling for camera and sonar sensors,\n\nautomatic environment generation,\n\nvolumetric environment effects, and\n\naccurate wave simulation for visuals and dynamics.\n\nSection IV concludes the paper and discusses HoloOcean’s place in the future of underwater robotics simulation.\n\n1. migration to Unreal Engine (UE) 5.3,\n\n2. improved vehicle dynamics,\n\n3. support for ROS2, and\n\n4. integration of the BlueROV and CoUG-UV vehicles.\n\n1. an improved sonar implementation using ray casting,\n\n2. semantic labeling for camera and sonar sensors,\n\n3. automatic environment generation,\n\n4. volumetric environment effects, and\n\n5. accurate wave simulation for visuals and dynamics.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要高保真度的海洋机器人模拟器以支持自主海洋机器人系统的开发。  \n2. 复杂任务和多代理操作要求精确的动态建模以确保准确结果。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有模拟器如LRAUV和UNavSim专注于车辆动态或高质量视觉渲染，但缺乏综合性。  \n2. 许多模拟器未能有效整合最新的游戏引擎技术以提升视觉和物理模拟的真实感。  \n\n【提出了什么创新的方法】  \nHoloOcean 2.0通过迁移到Unreal Engine 5.3，提升了视觉效果和模拟环境的真实感。它实现了高保真的车辆动态模型，支持ROS2集成，并引入了新的车辆模型。新特性包括改进的声纳实现、语义传感器、自动环境生成和真实波浪模拟。通过这些创新，HoloOcean 2.0能够提供更大的模拟场景和更精确的动态响应，促进了海洋机器人算法的开发与验证。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Vision-Guided Targeted Grasping and Vibration for Robotic Pollination in Controlled Environments",
            "authors": "Jaehwan Jeong,Tuan-Anh Vu,Radha Lahoti,Jiawen Wang,Vivek Alumootil,Sangpil Kim,M. Khalid Jawed",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.06146",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.06146",
            "arxiv_html_link": "https://arxiv.org/html/2510.06146v1",
            "abstract": "Robotic pollination offers a promising alternative to manual labor and bumblebee-assisted methods in controlled agriculture, where wind-driven pollination is absent and regulatory restrictions limit the use of commercial pollinators.\nIn this work, we present and validate a vision-guided robotic framework that uses data from an end-effector mounted RGB-D sensor and combines 3D plant reconstruction, targeted grasp planning, and physics-based vibration modeling to enable precise pollination.\nFirst, the plant is reconstructed in 3D and registered to the robot coordinate frame to identify obstacle-free grasp poses along the main stem.\nSecond, a discrete elastic rod model predicts the relationship between actuation parameters and flower dynamics, guiding the selection of optimal pollination strategies.\nFinally, a manipulator with soft grippers grasps the stem and applies controlled vibrations to induce pollen release.\nEnd-to-end experiments demonstrate a 92.5% main-stem grasping success rate, and simulation-guided optimization of vibration parameters further validates the feasibility of our approach, ensuring that the robot can safely and effectively perform pollination without damaging the flower.\nTo our knowledge, this is the first robotic system to jointly integrate vision-based grasping and vibration modeling for automated precision pollination.",
            "introduction": "Controlled environment agriculture (CEA), including greenhouses and indoor farms, is a sustainable solution to food production challenges worsened by climate change, labor shortages, and urbanization. While arable land growth is minimal (under 5%), global food demand is projected to surge 70% by 2050 [1].\nCEA offers efficient resource use (water, fertilizers, pesticides) and protection from environmental variability, but high operating costs—with labor comprising over 25-30% of expenses for several crops [2]—limit its scalability, motivating the adoption of robotic systems to reduce this manual effort.\n\nAs production of major greenhouse crops like the tomato (Solanum lycopersicum)—the second most consumed fresh vegetable in the US—steadily increases, a key challenge in CEA is how to effectively pollinate in the absence of natural wind. The primary biological solution, bumblebees, is often unviable for two main reasons: their use is restricted or heavily regulated in states including California, Oregon, and Washington, and greenhouse lighting conditions can disorient them, reducing pollination efficiency [3]. Consequently, growers rely on manual operation of mechanical vibration tools, such as vibrating wands and blowers, to induce flower vibration and facilitate pollen transfer. However, this manual approach is labor-intensive and costly, with expenses reaching 10,000–25,000 USD per hectare in Australia, highlighting the need for a more efficient, automated solution.\n\nRobotic pollination has emerged as a promising alternative. Ground-based systems such as BrambleBee [4] and commercial solutions by Arugga AI Farming  [5] use air pulses or mechanical contact to pollinate flowers, achieving yields comparable to or exceeding manual methods. However, existing approaches are often proprietary, optimized for different crops, or risk damaging delicate flowers. Prior research also highlights challenges in navigating complex plant geometries, avoiding leaf obstructions, and ensuring precise, safe interaction with stems and flowers [6].\n\nIn this work, we present a novel robotic pollination framework designed to achieve efficient pollination while preventing flower damage. This framework integrates algorithm-based 3D plant skeletonization, collision-free grasp planning, and physics-based vibration analysis.\nOur approach utilizes an end-effector mounted RGB-D sensor to perform generalizable 3D plant skeletonization and perception. This system secures 7-DoF safe grasp points and obstacle-free approach paths along the main stem, allowing the manipulator to safely grasp and induce vibration without damaging delicate flowers or thin stems.\nFurthermore, we incorporate a Discrete Elastic Rod (DER) model to precisely analyze the relationship between stem actuation and flower motion. The results from this simulation guide the selection of optimal pollination parameters, which subsequently complements the experimental validation of the entire framework.\n\nThe main contributions of this work are as follows:\n\nThe first robotic system integrating vision-based grasp planning and physics-based vibration modeling for pollination, validated by a 92.5% grasping success rate.\n\nDevelopment of a novel 3D plant skeletonization technique enabling 7-DoF obstacle-free grasp selection for safe and generalized robotic manipulation.\n\nUtilization of a physics-based Discrete Elastic Rod model, experimentally validated to predict how flower dynamics vary with actuation parameters, thereby enabling a Sim-to-Real optimization framework for identifying optimal pollination strategies.\n\nSection II reviews prior work and highlights the limitations that motivate the framework architecture presented in Section III. Sections III-A and III-B detail the core methodology, introducing the 3D skeletonization algorithm for selecting 7-DoF obstacle-free grasp poses and the elastic rod model used for vibration dynamics analysis. Section IV presents the experimental setup and results for accuracy, grasp success, and vibration transfer. Section V summarizes the findings and discusses future work toward full greenhouse deployment.\n\n1. The first robotic system integrating vision-based grasp planning and physics-based vibration modeling for pollination, validated by a 92.5% grasping success rate.\n\n2. Development of a novel 3D plant skeletonization technique enabling 7-DoF obstacle-free grasp selection for safe and generalized robotic manipulation.\n\n3. Utilization of a physics-based Discrete Elastic Rod model, experimentally validated to predict how flower dynamics vary with actuation parameters, thereby enabling a Sim-to-Real optimization framework for identifying optimal pollination strategies.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的人工授粉方法成本高且劳动密集，限制了温室农业的可扩展性。  \n2. 现有的机械授粉方法存在损坏花朵的风险，缺乏有效的自动化解决方案。  \n3. 需要一种高效、精准且不损伤花朵的机器人授粉系统。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有的机器人授粉系统如BrambleBee和Arugga AI Farming在不同作物上取得了一定成果，但缺乏通用性和可重复性。  \n2. 现有方法多依赖直接接触，可能导致花朵损坏，且在复杂植物几何结构中导航存在挑战。  \n3. 先前的研究未能有效整合物理模型与机器人控制规划，缺乏对植物动态的深入理解。  \n\n【提出了什么创新的方法】  \n本研究提出了一种新颖的机器人授粉框架，结合了3D植物重建、目标抓取规划和基于物理的振动建模。首先，通过RGB-D传感器进行植物的3D重建，识别无障碍的抓取姿态。其次，采用离散弹性杆模型来预测激励参数与花朵动态之间的关系，从而指导最佳授粉策略的选择。最后，使用软抓手抓取植物主干并施加控制振动以诱导花粉释放。实验结果显示，该系统的主干抓取成功率达到92.5%，验证了其在安全有效授粉方面的可行性。\n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Towards Autonomous Tape Handling for Robotic Wound Redressing",
            "authors": "Xiao Liang,Lu Shen,Peihan Zhang,Soofiyan Atar,Florian Richter,Michael Yip",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.06127",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.06127",
            "arxiv_html_link": "https://arxiv.org/html/2510.06127v1",
            "abstract": "Chronic wounds, such as diabetic, pressure, and venous ulcers, affect over 6.5 million patients in the United States alone and generate an annual cost exceeding $25 billion. Despite this burden, chronic wound care remains a routine yet manual process performed exclusively by trained clinicians due to its critical safety demands. We envision a future in which robotics and automation support wound care to lower costs and enhance patient outcomes. This paper introduces an autonomous framework for one of the most fundamental yet challenging subtasks in wound redressing: adhesive tape manipulation. Specifically, we address two critical capabilities: tape initial detachment (TID) and secure tape placement. To handle the complex adhesive dynamics of detachment, we propose a force-feedback imitation learning approach trained from human teleoperation demonstrations. For tape placement, we develop a numerical trajectory optimization method based to ensure smooth adhesion and wrinkle-free application across diverse anatomical surfaces. We validate these methods through extensive experiments, demonstrating reliable performance in both quantitative evaluations and integrated wound redressing pipelines. Our results establish tape manipulation as an essential step toward practical robotic wound care automation.",
            "introduction": "Chronic wounds, including diabetic ulcers, pressure ulcers, and ulcers secondary to venous hypertension, affects more than 6.5 million patients [1] and a yearly cost of more than $25 billion in the United States alone [2, 3].\nThis growing demand for wound care coincides with a critical shortage of professional nursing staff, projected to reach over one million [4].\nThe majority of wound care occurs at a patient’s home\n[5], which is especially crucial because most of the healing (and most complications) happen between clinic visits. However, home-based wound care not only strains the professional workforce due to prolonged travel and scheduling demands, but also places a substantial burden on family members who frequently assume caregiving responsibilities, as these informal caregivers must often sacrifice personal time and well-being to provide essential care, leading to emotional and physical stress [6].\n\nAutomation technologies have been increasingly applied to address this global healthcare challenge from multiple perspectives. For example, immersive display systems have been explored for pain management during wound care [7]. Advances in computer vision and machine learning have enabled automated wound assessment and healing prediction [8]. In robotics, researchers have investigated systems for autonomous wound care, including robotic debridement [9] and automated bandaging [10]. Complementary automation approaches have also targeted wound tracking [11] and three-dimensional reconstruction of wound geometry [12].\n\nMore recently, [13] demonstrates the feasibility of robotic wound care in broader clinical and home settings by introducing an autonomous planning framework for safe wound dressing removal using general-purpose robot manipulators.\nThis approach is a strong start to the challenge problem of wound dressing, but ultimately only a single step of the overall wound redressing task, removing the initial adhesive tape given an initial grasp point on the tape.\nIn practice, wound care involves (1) initiating detachment and grasping of the adhesive tape, (2) removing the secondary dressing, (3) cleaning the wound and reapplying a new primary dressing, and (4) replacing the secondary dressing and securing it with medical tape.\nA common thread across these steps is the essential skill of adhesive tape manipulation. This is a critical capability where robots must avoid pitfalls such as unintended self-adhesion or excessive tension on fragile tissue.\nFor example, even the initial act of detaching tape, which is assumed in [13], illustrates the complexity of this challenge: create a small detachment by scratching or prying at the edge if the tape and carefully enlarging the separation to achieve a precise and stable grasp.\n\nIn this paper, we present the first autonomous approach for medical tape manipulation designed to enable wound redressing, which directly addresses the challenges of initiating detachment (part of step 1) and re-securing dressings with new tape (step 4) — further progressing the field towards fully end-to-end autonomous wound care dressing.\nWe use two separate approaches towards solving tape manipulation: a learning based approach for tape initial detachment (TID) and trajectory optimization using a differentiable simulation for precise tape placement.\nThis hybrid design reflects the nature of the task: while the detachment phase involves complex, hard-to-model adhesive dynamics best addressed with learning, the placement phase requires accurate sensing and careful planning with respect to different body parts, where traditional control offers greater generalization and reliability. Our contributions are summarized as follow:\n\nA force-feedback-enabled teleoperation platform for collecting demonstrations of TID, coupled with a visual imitation learning that leverages semantic segmentation to isolate task-relevant information to improve generalization.\n\nA numerical trajectory optimization method for achieving optimal robot trajectories and force profiles for secure medical tape placement while considering varying surface skin geometries.\n\nExtensive experiments, including quantitative evaluations and a full pipeline demonstration, validating robotic tape manipulation as a key capability for enabling practical wound redressing automation.\n\n1. A force-feedback-enabled teleoperation platform for collecting demonstrations of TID, coupled with a visual imitation learning that leverages semantic segmentation to isolate task-relevant information to improve generalization.\n\n2. A numerical trajectory optimization method for achieving optimal robot trajectories and force profiles for secure medical tape placement while considering varying surface skin geometries.\n\n3. Extensive experiments, including quantitative evaluations and a full pipeline demonstration, validating robotic tape manipulation as a key capability for enabling practical wound redressing automation.",
            "llm_summary": "【论文的motivation是什么】  \n1. Chronic wounds affect millions and incur significant healthcare costs, necessitating automation in wound care.  \n2. Current wound care processes are manual and require trained clinicians, highlighting the need for robotic assistance.  \n3. Effective adhesive tape manipulation is critical for successful wound redressing, yet remains a challenging task for robots.  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. Previous research has explored robotic systems for wound care, but primarily focused on specific tasks like dressing removal.  \n2. Existing methods lack comprehensive solutions for adhesive tape manipulation, particularly in the detachment and placement phases.  \n\n【提出了什么创新的方法】  \n本研究提出了一种混合方法，结合了基于学习的初始脱离（TID）和数值轨迹优化以实现精确的胶带放置。具体流程包括：  \n- 使用力反馈的遥操作平台收集人类演示数据，并通过视觉模仿学习来提高模型的泛化能力。  \n- 采用数值轨迹优化方法，考虑不同皮肤几何形状，实现安全的医疗胶带放置。  \n- 通过广泛的实验验证了这些方法的有效性，展示了机器人胶带操作在实现实际伤口换药自动化中的关键作用。  \n\n本研究的结果表明，胶带操作是实现完全自主的伤口护理的重要一步，能够有效降低医疗成本并提高患者的护理质量。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Multi-Robot Distributed Optimization for Exploration and Mapping of Unknown Environments using Bioinspired Tactile-Sensor",
            "authors": "Roman Ibrahimov,Jannik Matthias Heinen",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.06085",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.06085",
            "arxiv_html_link": "https://arxiv.org/html/2510.06085v1",
            "abstract": "This project proposes a bioinspired multi-robot system using Distributed Optimization for efficient exploration and mapping of unknown environments. Each robot explores its environment and creates a map, which is afterwards put together to form a global 2D map of the environment. Inspired by wall-following behaviors, each robot autonomously explores its neighborhood, based on a tactile sensor, similar to the antenna of a cockroach, mounted on the surface of the robot. Instead of avoiding obstacles, robots log collision points when they touch obstacles. This decentralized control strategy ensures effective task allocation and efficient exploration of unknown terrains, with applications in search-and-rescue, industrial inspection, and environmental monitoring. The approach was validated through experiments using e-puck robots in a simulated 1.5 × 1.5 m environment with three obstacles. The results demonstrated the system’s effectiveness in achieving high coverage, minimizing collisions, and constructing accurate 2D maps [a link for video descroption].",
            "introduction": "Exploration and mapping of unknown environments is a critical task in various fields, including search-and-rescue operations, industrial inspection, and environmental monitoring [1, 2]. Autonomous multi-robot systems have emerged as a promising solution, offering scalability, redundancy, and efficiency over single-robot approaches. Effective exploration, however, requires innovative sensing mechanisms and control strategies to navigate complex, obstacle-laden environments. Bioinspiration has proven to be a valuable approach in robotics, with nature providing elegant and robust solutions to challenging problems [3]. Among these, the tactile sensory capabilities of American cockroaches, particularly their ability to sense and respond to physical contact using antennae, offer a compelling model for robotic navigation in confined and cluttered spaces [4].\n\nThis paper proposes a bioinspired multi-robot system that leverages tactile sensing and distributed optimization for efficient exploration and mapping of unknown environments. Robots equipped with tactile sensors, inspired by the antenna of cockroaches, navigate by logging collision points with obstacles rather than avoiding them. These logged points are used to create local maps, which are afterwards set together to form a global map. This approach was implemented on a group of e-puck robots in the Webots simulation environment, where the robots successfully built a 2D map of the environment, accurately capturing the layout of obstacles.",
            "llm_summary": "【论文的motivation是什么】  \n1. 探索和映射未知环境的需求在多个领域中至关重要。  \n2. 现有的单机器人方法在复杂环境中效率不足。  \n3. 需要创新的传感机制和控制策略来应对障碍物密集的环境。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 以往的研究主要集中在单机器人系统，缺乏多机器人协作的有效策略。  \n2. 现有的生物启发式方法未能充分利用触觉传感器在复杂环境中的潜力。  \n\n【提出了什么创新的方法】  \n本研究提出了一种生物启发的多机器人系统，利用触觉传感器和分布式优化进行未知环境的高效探索和映射。每个机器人通过记录与障碍物的碰撞点来创建局部地图，这些局部地图随后合并为全局地图。该方法在Webots仿真环境中通过e-puck机器人进行了验证，结果显示系统在高覆盖率、最小化碰撞和准确构建2D地图方面表现出色。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Cross-Embodiment Dexterous Hand Articulation Generation via Morphology-Aware Learning",
            "authors": "Heng Zhang,Kevin Yuchen Ma,Mike Zheng Shou,Weisi Lin,Yan Wu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.06068",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.06068",
            "arxiv_html_link": "https://arxiv.org/html/2510.06068v1",
            "abstract": "Dexterous grasping with multi-fingered hands remains challenging due to high-dimensional articulations and the cost of optimization-based pipelines. Existing end-to-end methods require training on large-scale datasets for specific hands, limiting their ability to generalize across different embodiments. We propose an eigengrasp-based, end-to-end framework for cross-embodiment grasp generation. From a hand’s morphology description, we derive a morphology embedding and an eigengrasp set. Conditioned on these, together with the object point cloud and wrist pose, an amplitude predictor regresses articulation coefficients in a low-dimensional space, which are decoded into full joint articulations. Articulation learning is supervised with a Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant motions and injects morphology-specific structure. In simulation on unseen objects across three dexterous hands, our model attains a 91.9% average grasp success rate with <0.4​s<0.4\\,\\text{s} inference per grasp. With few-shot adaptation to an unseen hand, it achieves 85.6% success on unseen objects in simulation, and real-world experiments on this few-shot generalized hand achieve an 87% success rate.\nThe code and additional materials will be made available upon publication on our project website https://connor-zh.github.io/cross_embodimen_dexterous_grasping/.",
            "introduction": "Dexterous grasping with multi-fingered robotic hands is a fundamental capability for versatile manipulation, offering rich contact interactions and adaptability to diverse object geometries with a spectrum of grasp solutions thanks to their inherent kinematic redundancy. However, the high-dimensional kinematics of such hands render grasp planning highly challenging.\n\nMany existing methods are designed for a specific hand [1, 2, 3, 4], requiring large-scale datasets and retraining whenever the embodiment changes. This severely limits scalability, as each new hand design demands dedicated data collection and model training. Recent works have explored cross-embodiment grasp generation, including DRO [5], DexGraspNet [6], DFC [7], UniGrasp [8], and GenDexGrasp [9]. These approaches either (i) directly optimize final hand poses via physics-based energy functions, or (ii) predict intermediate representations such as contact maps or robot–object distance matrices, which are then converted into an inverse-kinematics optimization problem. While effective, such optimization is often computationally expensive, particularly for complex hand morphologies.\n\nEnd-to-end learning of articulations is attractive but challenging, as the dimensionality of hand degrees of freedom (DoFs) grows rapidly with morphological complexity. Santello et al. [10] demonstrate that human grasp postures can be effectively represented in a low-dimensional space, with the first two principal components explaining over 80% of the variance. Extending this outcome to robotics, Ciocarlie and Allen [11] introduce eigengrasps, hand-specific low-dimensional bases that capture coordinated joint patterns. Building on these findings, we hypothesize that a universal low-dimensional representation of grasp articulation could effectively reduce the search space for multifinger grasp planning, facilitate end-to-end training, and support transfer of grasp skills across different robotic hand embodiments.\n\nIn this paper, we propose a framework for cross-embodiment dexterous grasp generation. Our method extracts eigengrasps and morphology embeddings directly from a robotic hand’s Unified Robot Description Format (URDF), explicitly incorporating kinematic and geometric constraints. Given an object point cloud and a target wrist pose, an amplitude predictor estimates articulation coefficients in the eigengrasp space, which are then decoded into full joint configurations. To supervise training, we introduce a Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant motions and implicitly encodes morphology-specific information, in contrast to naïve per-joint regression objectives (e.g., mean squared error) that overemphasize raw joint deviations.\n\nWe demonstrate the effectiveness of the proposed framework through extensive evaluations in both simulation and real-world settings. In simulation on unseen objects across three dexterous hands, our model achieves a 91.9% average grasp success rate with <0.4​s<0.4\\,\\text{s} inference per grasp; with few-shot adaptation to an unseen hand it attains 85.6% success on unseen objects in simulation, and real-world experiments on the same few-shot–generalized hand achieve 87% success.\n\nOur main contributions are:\n\nWe propose an eigengrasp-based framework for cross-embodiment dexterous grasp generation that predicts articulations in an end-to-end manner.\n\nWe design a unified encoding scheme that converts a robot’s URDF into structured morphological tokens, capturing both kinematic constraints and geometric primitives.\n\nWe propose a Kinematic-Aware Articulation Loss (KAL) that injects morphology-specific kinematic information into regression learning for articulation prediction, guiding the model beyond raw joint errors.\n\nWe conduct extensive experiments in simulation and on real hardware, demonstrating effective grasping of novel objects across multiple robotic hands, including both seen hands and an unseen hand via few-shot adaptation.\n\n1. We propose an eigengrasp-based framework for cross-embodiment dexterous grasp generation that predicts articulations in an end-to-end manner.\n\n2. We design a unified encoding scheme that converts a robot’s URDF into structured morphological tokens, capturing both kinematic constraints and geometric primitives.\n\n3. We propose a Kinematic-Aware Articulation Loss (KAL) that injects morphology-specific kinematic information into regression learning for articulation prediction, guiding the model beyond raw joint errors.\n\n4. We conduct extensive experiments in simulation and on real hardware, demonstrating effective grasping of novel objects across multiple robotic hands, including both seen hands and an unseen hand via few-shot adaptation.",
            "llm_summary": "【论文的motivation是什么】  \n1. 多指机器人手的灵巧抓取面临高维度关节问题和优化成本。  \n2. 现有方法对特定手的依赖限制了跨不同形态的泛化能力。  \n3. 需要一种有效的框架来实现跨形态的抓取生成。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有方法通常需要大规模数据集并在每次形态变化时重新训练。  \n2. 许多研究集中于优化最终手姿态或预测中间表示，计算成本高且效率低。  \n3. 尽管有些方法实现了跨形态抓取，但仍然依赖于迭代优化或特定手模块，限制了可扩展性和效率。  \n\n【提出了什么创新的方法】  \n我们提出了一种基于特征抓取的框架，通过手的形态描述生成形态嵌入和特征抓取集。该方法利用对象点云和手腕姿态，预测低维空间中的关节系数，并解码为完整的关节配置。我们引入了一种运动学感知关节损失（KAL），强调与指尖相关的运动，同时注入形态特定结构。通过在三种灵巧手上对未见对象进行模拟，我们的模型实现了91.9%的平均抓取成功率，且每次抓取推理时间小于0.4秒。在对未见手的少量适应中，成功率达到了85.6%，而在真实世界实验中，该模型在同一少量适应的手上实现了87%的成功率。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Coordinate-Consistent Localization via Continuous-Time Calibration and Fusion of UWB and SLAM Observations",
            "authors": "Tien-Dat Nguyen,Thien-Minh Nguyen,Vinh-Hao Nguyen",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05992",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05992",
            "arxiv_html_link": "https://arxiv.org/html/2510.05992v1",
            "abstract": "Onboard simultaneous localization and mapping (SLAM) methods are commonly used to provide accurate localization information for autonomous robots. However, the coordinate origin of SLAM estimate often resets for each run. On the other hand, UWB-based localization with fixed anchors can ensure a consistent coordinate reference across sessions; however, it requires an accurate assignment of the anchor nodes’ coordinates.\nTo this end, we propose a two-stage approach that calibrates and fuses UWB data and SLAM data to achieve coordinate-wise consistent and accurate localization in the same environment.\nIn the first stage, we solve a continuous-time batch optimization problem by using the range and odometry data from one full run, incorporating height priors and anchor-to-anchor distance factors to recover the anchors’ 3D positions. For the subsequent runs in the second stage, a sliding-window optimization scheme fuses the UWB and SLAM data, which facilitates accurate localization in the same coordinate system. Experiments are carried out on the NTU VIRAL dataset with six scenarios of UAV flight, and we show that calibration using data in one run is sufficient to enable accurate localization in the remaining runs. We release our source code to benefit the community at https://github.com/ntdathp/slam-uwb-calibration.",
            "introduction": "Ultra-Wideband (UWB) technology enables highly accurate distance estimation between tags and anchors—often on the order of 10 cm under favorable conditions—by using short-duration, high-bandwidth radio pulses [1]. This capability has made UWB an attractive solution for indoor positioning and navigation in GNSS-denied environments, with successful deployments in domains such as logistics [2], [3], aerial robot swarms [4, 5, 6], and intelligent traffic control [7].\n\nHowever, the accuracy of UWB ranging strongly depends on several factors: precise surveying of anchor positions (since errors directly propagate to the tag), maintaining LoS visibility to at least three anchors, and minimizing multipath or hardware-induced delays that distort time-of-flight estimates [8]. Moreover, large-area coverage requires dense anchor deployment with significant infrastructure support.\n\nCalibrating the anchor positions under these constraints is a critical and challenging task. Conventional approaches rely on external surveying instruments to measure anchor coordinates with sub-centimeter accuracy. While effective, such surveys are labor-intensive, time-consuming, and often impractical in large or dynamic environments. To alleviate this overhead, researchers have proposed self-calibration methods that use only inter-anchor UWB ranging (anchor-to-anchor, A2A) to estimate the network geometry [9]. However, the accuracy of purely UWB-based self-calibration can degrade in settings with severe multipath, poor anchor geometry, or NLoS conditions—situations frequently encountered in real-world deployments.\n\nRecent advances in LiDAR–inertial SLAM enable high-precision trajectory estimation in GNSS-denied environments, with drift of only a few centimeters over long runs [10, 11].\nThis effectively turns the platform into a mobile surveyor: when paired with a UWB tag, it records synchronized trajectory and range data, allowing anchor positions and per-link biases to be jointly estimated via robust nonlinear least squares, without external surveying.\n\nIn this paper, we introduce a portable SLAM–UWB calibration framework for automatic anchor self-calibration (see Fig. 1). During an initial data-collection run, the UWB-tagged platform simultaneously executes a high-precision SLAM algorithm—recording a dense 6-DoF trajectory—and logs two-way UWB ranges to each static anchor. Once completed, the trajectory and range measurements are fed into a robust back-end optimizer, which jointly solves for the 3D positions of all anchors and per-link bias terms via a factor-graph formulation (with optional inter-anchor distance constraints). Finally, the calibrated anchor coordinates serve to geo-reference all subsequent SLAM sessions, directly transforming future trajectories into a consistent global frame defined by the anchor network. In summary, the key contributions of this work include:\n\nA method for jointly estimating 3D anchor positions and range biases from SLAM and UWB data using a robust factor graph with continuous-time interpolation, bias compensation, Cauchy loss, and anchor priors.\n\nA method for jointly estimating 3D anchor positions and range biases from SLAM and UWB data using a robust factor graph with continuous-time interpolation, bias compensation, Cauchy loss, and anchor priors.\n\nA loosely coupled design that supports arbitrary SLAM frontends and enables anchor reuse across multiple runs without re-surveying.\n\nPublic release of datasets, source code, and results to support reproducibility and community use.\n\n1. A method for jointly estimating 3D anchor positions and range biases from SLAM and UWB data using a robust factor graph with continuous-time interpolation, bias compensation, Cauchy loss, and anchor priors.\n\n2. A loosely coupled design that supports arbitrary SLAM frontends and enables anchor reuse across multiple runs without re-surveying.\n\n3. Public release of datasets, source code, and results to support reproducibility and community use.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有SLAM方法在每次运行时坐标原点重置，导致定位不一致。  \n2. UWB定位依赖于固定锚点，需准确分配锚点坐标，且在动态环境中难以实现。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统方法依赖外部测量工具进行锚点坐标的精确测量，效率低下。  \n2. 自校准方法在多路径或不良锚点几何条件下的准确性下降，限制了实际应用。  \n\n【提出了什么创新的方法】  \n本研究提出了一种两阶段的方法，通过连续时间优化和融合UWB与SLAM数据，实现坐标一致的高精度定位。第一阶段通过范围和里程数据解决批优化问题，恢复锚点的3D位置；第二阶段采用滑动窗口优化方案融合UWB和SLAM数据，确保在同一坐标系统中进行准确定位。实验结果表明，仅通过一次数据收集的校准即可在后续运行中实现准确定位，极大提高了定位的可靠性和效率。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "AI-Enabled Capabilities to Facilitate Next-Generation Rover Surface Operations",
            "authors": "Cristina Luna,Robert Field,Steven Kay",
            "subjects": "Robotics (cs.RO)",
            "comment": "Paper for 18th Symposium on Advanced Space Technologies in Robotics and Automation (ASTRA), presented on October 7th at Leiden, Netherlands",
            "pdf_link": "https://arxiv.org/pdf/2510.05985",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05985",
            "arxiv_html_link": "https://arxiv.org/html/2510.05985v1",
            "abstract": "Current planetary rovers operate at traverse speeds of approximately 10 cm/s, fundamentally limiting exploration efficiency. This work presents integrated AI systems which significantly improve autonomy through three components: (i) the FASTNAV Far Obstacle Detector (FOD), capable of facilitating sustained 1.0 m/s speeds via computer vision-based obstacle detection; (ii) CISRU, a multi-robot coordination framework enabling human-robot collaboration for in-situ resource utilisation; and (iii) the ViBEKO and AIAXR deep learning-based terrain classification studies. Field validation in Mars analogue environments demonstrated these systems at Technology Readiness Level 4, providing measurable improvements in traverse speed, classification accuracy, and operational safety for next-generation planetary missions.",
            "introduction": "Planetary exploration is heavily constrained by rover mobility. Contemporary Mars rovers such as Curiosity and Perseverance operate at average speeds on the order of 4.2 cm/s, with daily traverses typically below 100 m [1]. These constraints stem from conservative operational approaches necessitated by communication delays, irreplaceable hardware, and limited onboard processing capabilities.\n\nThe traditional Sense-Model-Plan-Act (SMPA) paradigm requires frequent stops for terrain analysis, preventing continuous motion and severely limiting mission scope and scientific return. Missions requiring long-range access to diverse geological targets (sample-return campaigns) are particularly affected by these mobility constraints [2].\n\nRecent advances in computer vision (CV) algorithms, compact ML models, and space-qualified computing platforms offer a practical path to maintaining safety while increasing autonomy and traverse speeds. In this work, we present a set of AI-enabled systems developed under ESA contracts RAPID, FASTNAV, ViBEKO and AIAXR, and CISRU. These systems were validated in Mars- and Lunar-analogue field trials and demonstrate substantial improvements in mobility and perception accuracy.\n\nThe contributions presented in this work are: (1) a far-obstacle detection component which facilitates continuous motion at speeds in excess of 1.0 m/s; (2) a coordination framework enabling multi-robot human-robot workflows for resource extraction and handling; and (3) a suite of terrain classification models for operations.\n\nThis research demonstrates three integrated AI systems: (1) FASTNAV, which achieves ≈\\approx70x traverse-speed improvements through extended-range obstacle detection and adaptive guidance; (2) CISRU, a multi-robot coordination framework enabling coordinated in-situ resource utilisation; and (3) terrain classification, using convolutional neural networks and achieving ≈\\approx95% accuracy on on the AI4Mars dataset [3]. These systems have been validated through comprehensive field testing in planetary analogue environments and demonstrated measurable gains in mobility, perception accuracy, and operational safety.",
            "llm_summary": "【论文的motivation是什么】  \n1. 提高行星探测器的移动速度，以增强探索效率。  \n2. 解决传统操作方法导致的频繁停顿和移动限制问题。  \n3. 实现人机协作以优化资源利用和任务执行。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统的SMPA方法在移动过程中需要频繁停顿进行地形分析，限制了探测器的速度和效率。  \n2. 现有技术在提高自主性和移动速度方面的应用仍然不足，缺乏有效的障碍物检测和分类方法。  \n\n【提出了什么创新的方法】  \n本研究提出了三种集成的AI系统：  \n1. FASTNAV，利用远程障碍物检测实现超过1.0 m/s的连续移动。  \n2. CISRU，一个多机器人协调框架，支持人机协作以优化资源利用。  \n3. 一套基于卷积神经网络的地形分类模型，准确率达到约95%。  \n这些系统在类火星和类月球环境中进行了全面的现场验证，显示出在移动性、感知准确性和操作安全性方面的显著提升。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "The DISTANT Design for Remote Transmission and Steering Systems for Planetary Robotics",
            "authors": "Cristina Luna,Alba Guerra,Almudena Moreno,Manuel Esquer,Willy Roa,Mateusz Krawczak,Robert Popela,Piotr Osica,Davide Nicolis",
            "subjects": "Robotics (cs.RO)",
            "comment": "Paper for 18th Symposium on Advanced Space Technologies in Robotics and Automation (ASTRA), presented on October 7th at Leiden, Netherlands",
            "pdf_link": "https://arxiv.org/pdf/2510.05981",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05981",
            "arxiv_html_link": "https://arxiv.org/html/2510.05981v1",
            "abstract": "Planetary exploration missions require robust locomotion systems capable of operating in extreme environments over extended periods. This paper presents the DISTANT (Distant Transmission and Steering Systems) design, a novel approach for relocating rover traction and steering actuators from wheel-mounted positions to a thermally protected warm box within the rover body. The design addresses critical challenges in long-distance traversal missions by protecting sensitive components from thermal cycling, dust contamination, and mechanical wear. A double wishbone suspension configuration with cardan joints and capstan drive steering has been selected as the optimal architecture following comprehensive trade-off analysis. The system enables independent wheel traction, steering control, and suspension management whilst maintaining all motorisation within the protected environment. The design meets a 50 km traverse requirement without performance degradation, with integrated dust protection mechanisms and thermal management solutions. Testing and validation activities are planned for Q1 2026 following breadboard manufacturing at 1:3 scale.",
            "introduction": "Planetary exploration missions continue to push the boundaries of autonomous surface operations, with rovers increasingly required to traverse greater distances and operate for extended periods in harsh environments [1, 2]. Current rover designs typically employ hub-mounted motors or chassis-embedded drive systems that expose critical actuators to extreme thermal cycling, dust infiltration, and mechanical stress during long-traverse missions [3, 4].\n\nThe European Space Agency has identified the need for advanced locomotion systems capable of supporting missions requiring traversals of 50 km or more without significant performance degradation. Traditional approaches face limitations in dust protection, thermal management, and component longevity that become critical factors in extended operations [5].\n\nThe DISTANT project addresses these challenges through a paradigm shift in rover architecture, relocating all traction and steering actuators to a thermally controlled warm box within the rover body. This approach enables centralised thermal management, simplified dust protection, and enhanced maintenance accessibility whilst maintaining full locomotion control capabilities. The DISTANT design (Figure 1) employs multiple transmission mechanisms including cardan joints, bevel gears, and capstan drives to control suspension, steering, and wheel rotation from the protected warm-box environment.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要开发能够在极端环境中长时间运行的强大移动系统。  \n2. 现有的探测车设计在长途行驶中面临热循环、灰尘污染和机械磨损的挑战。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有的轮式探测车采用轮毂电机或底盘嵌入式驱动系统，但暴露于极端环境中。  \n2. 传统的灰尘保护和热管理方法效果有限，难以满足长时间操作的需求。  \n\n【提出了什么创新的方法】  \nDISTANT设计通过将牵引和转向执行器从轮子上移动到探测车内部的热保护箱，提出了一种新的架构。该方法实现了集中热管理、简化灰尘保护和增强维护可达性，同时保持完整的运动控制能力。设计采用双叉臂悬挂配置，结合万向节、锥齿轮和绳轮驱动，确保在保护环境内控制悬挂、转向和轮子旋转。该系统满足50公里的行驶要求，且在性能上没有退化，集成了灰尘保护机制和热管理解决方案。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion",
            "authors": "Vaughn Gzenda,Robin Chhabra",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05957",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05957",
            "arxiv_html_link": "https://arxiv.org/html/2510.05957v1",
            "abstract": "Soft robotic crawlers are mobile robots that utilize soft body deformability and compliance to achieve locomotion through surface contact. Designing control strategies for such systems is challenging due to model inaccuracies, sensor noise, and the need to discover locomotor gaits. In this work,\nwe present a model-based reinforcement learning (MB-RL) framework in which\nlatent dynamics inferred from onboard sensors serve as a predictive model that\nguides an actor-critic algorithm to optimize locomotor policies. We evaluate\nthe framework on a minimal crawler model in simulation using inertial measurement units and time-of-flight sensors as observations. The learned latent\ndynamics enable short-horizon motion prediction while the actor-critic discovers\neffective locomotor policies. This approach highlights the potential of latent-dynamics MB-RL for enabling embodied soft robotic adaptive locomotion based solely on noisy\nsensor feedback.",
            "introduction": "Soft Robotic Crawlers (SRCs) [1, 2, 3, 4] represent a class of mobile robots that exploit the inherent compliance and deformability of soft materials to achieve robust, adaptive locomotion in complex and unstructured environments. Unlike traditional rigid-bodied robots, soft crawlers can conform to terrain, absorb impacts, and navigate through confined spaces, making them particularly well-suited for applications in search and rescue, environmental/industrial monitoring, biomedical devices, and exploration in hazardous or delicate settings.\n\nBiological inspiration often plays a central role in the design of soft crawling systems, with organisms such as worms, caterpillars, and snakes providing models. Common actuators include the expansion and contraction of fluidic/pneumatic chambers, electromagnetic material properties, and Shape Memory Alloys (SMAs) [2, 5]. In one design [3], an earthworm inspired crawler actively modulates its contact friction forces to anchor the base and/or head in place and internal shape is extended/retracted to achieve forward locomotion. Another design includes two linear pneumatic actuators able to also perform turning in addition to modulating contact forces [6]. Some soft crawler robots design the contact substrate to have two friction coefficients to achieve locomotion by only modulating the shape of the body making use of anisotropic friction [1, 2]. A multi-modular design of an inchworm with suction cups at the contact points is capable of both horizontal and vertical locomotion [7]. Soft robotic crawlers have been proposed for applications in industrial monitoring such as pipe inspection [8, 7, 9]. A review of the design of soft robots for locomotion can be found in [10].\n\nGait planning in soft robotics is a central challenge due to the interplay between the compliant body mechanics, environmental interactions, and often high dimensional control spaces.\nA classical approach for inchworm robot gait planning is to use a finite state space representation of the anchoring points and extended/contracted body states [11]. Another study of soft crawler locomotion uses a simplified planar rigid-link models with elastic joints to plan the gait sequencing [12].\nThis work [2] develops a shape-based modeling framework that enables predictive gait planning and control for an SMA-actuated inchworm soft robot. Gait planning for a soft multi-legged robot has also been studied and implemented on starfish like robot [13].\n\nA main difficulty in soft robot control arises from balancing accurate modeling of body deformations with the practical requirements of real-time control. Finite element approaches [14] offer more accuracy in a tradeoff with real-time prediction. Finite difference approaches [15] are better suited for real-time prediction, but are numerically unstable under contact forces. In the middle ground, simplified models such as mass-spring-damper models [4] or lumped mass models [16] attempt to balance accuracy with simplicity for control design. Using a minimal model of a SRC, a hill-climbing algorithm is developed to solve an optimal periodic control problem to optimize the gait of the robot [4]. A perturbation approach [17], with a Bernoulli beam model of the robot and nonlinear friction, splits the dynamics into two time scales and performs open loop control of a gait. A model based control of a worm robot actuating its body for peristaltic locomotion has been developed for pipe inspection[18]. This work demonstrates how neural network gradients can be used to derive a linearized state-space model for soft robots, enabling effective model predictive control [19]. Central Pattern Generator (CPG)-based control to regulate gait frequency in a SRC, enabling adaptable locomotion and simple speed regulation [20].\n\nModel Predictive Control (MPC) [21] is widely regarded as a standard tool in robotic motion planning and control. For soft robots, however, the reliance on accurate and tractable models often limits its applicability, spurring research into data-driven alternatives that learn system dynamics from observations. Probabilistic models such as PILCO [22] offer data efficiency but scale poorly to high-dimensional systems, while Probabilistic Ensembles with Trajectory sampling (PETs) leverage neural network ensembles and rollouts within MPC to produce robust plans [23].\n\nLatent dynamics models in Model-Based Reinforcement Learning (MB-RL) have been developed to enable planning directly from high-dimensional observations such as pixels, with notable examples including PlaNet [24], Dreamer [25], and extensions incorporating discrete latent states and improved planning strategies [26, 27]. More broadly, deep learning and reinforcement learning approaches have advanced the development of world models for decision making [28], with recent works such as robotic world models [29] and DayDreamer [30] demonstrating their ability to support long-horizon planning and physical robot learning. Building on these advances, latent dynamics and world models provide a promising foundation for soft robotic control, where learning compact representations of complex dynamics can facilitate gait planning and adaptive locomotion in soft robotic crawlers.\n\nIn this paper, we present a latent dynamics modeling framework for soft robotic crawlers. The model is learned directly from noisy sensor streams, specifically Inertial Measurement Units (IMUs) and Time-Of-Flight (TOF) sensors, without relying on explicit analytical models of the robot’s complex continuum body. By capturing the underlying latent dynamics, the model enables forecasting of the crawler’s future behavior under different control inputs.\nThis predictive capability is then leveraged to train an actor–critic reinforcement learning framework, which optimizes the robot’s locomotion strategy. In particular, the learned latent model serves as a surrogate for real-world interaction, allowing the policy to efficiently explore candidate gaits and converge toward those that maximize forward displacement.\nThe main contributions of this work are summarized as follows:\n\nWe develop a data-driven latent dynamics model for soft robotic crawlers that is estimated directly from noisy IMU and TOF measurements.\n\nWe integrate the learned latent model with an actor–critic reinforcement learning framework to discover gaits that maximize forward locomotion.\n\nThis paper is structured as follows. In Section II review the dynamics of soft robotic crawlers and introduce the IMU and TOF sensors. In Section III we describe the latent model based reinforcement learning framework for gait generation. In Section IV we perform simulation studies. Lastly, in Section V we end with some concluding remarks.\n\n1. We develop a data-driven latent dynamics model for soft robotic crawlers that is estimated directly from noisy IMU and TOF measurements.\n\n2. We integrate the learned latent model with an actor–critic reinforcement learning framework to discover gaits that maximize forward locomotion.",
            "llm_summary": "【论文的motivation是什么】  \n1. 设计控制策略对于软体机器人爬行器来说具有挑战性，因其面临模型不准确、传感器噪声和运动模式发现的需求。  \n2. 现有方法在高维系统的动态学习和实时控制方面存在局限性。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统的控制方法依赖于准确的模型，导致在软体机器人控制中的应用受限。  \n2. 数据驱动的替代方案如PILCO和PETs在高维系统中表现不佳，未能有效解决实时控制需求。  \n\n【提出了什么创新的方法】  \n我们提出了一种基于潜在动态的模型驱动强化学习框架，直接从噪声传感器数据（IMU和TOF传感器）学习动态模型。该模型用于预测爬行器在不同控制输入下的未来行为，并与演员-评论家强化学习算法结合，优化机器人运动策略。通过这种方法，爬行器能够有效探索候选运动模式，并收敛到最大化前进位移的策略。实验表明，该框架在模拟环境中实现了有效的自适应运动。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "A Co-Design Framework for Energy-Aware Monoped Jumping with Detailed Actuator Modeling",
            "authors": "Aman Singh,Aastha Mishra,Deepak Kapa,Suryank Joshi,Shishir Kolathaya",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted at IEEE-RAS 24th International Conference on Humanoid Robots (Humanoids) 2025, Aman Singh, Aastha Mishra - Authors contributed equally",
            "pdf_link": "https://arxiv.org/pdf/2510.05923",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05923",
            "arxiv_html_link": "https://arxiv.org/html/2510.05923v1",
            "abstract": "A monoped’s jump height and energy consumption depend on both, its mechanical design and control strategy. Existing co-design frameworks typically optimize for either maximum height or minimum energy, neglecting their trade-off. They also often omit gearbox parameter optimization and use oversimplified actuator mass models, producing designs difficult to replicate in practice.\nIn this work, we introduce a novel three-stage co-design optimization framework that jointly maximizes jump height while minimizing mechanical energy consumption of a monoped. The proposed method explicitly incorporates realistic actuator mass models and optimizes mechanical design (including gearbox) and control parameters within a unified framework. The resulting design outputs are then used to automatically generate a parameterized CAD model suitable for direct fabrication, significantly reducing manual design iterations. Our experimental evaluations demonstrate a 50% reduction in mechanical energy consumption compared to the baseline design, while achieving a jump height of 0.8m.\nVideo presentation is available at:\nhttp://y2u.be/XW8IFRCcPgM",
            "introduction": "Legged robots are pivotal for navigating complex, unstructured terrains where wheeled or tracked systems fail.\nJumping enables them, including monopeds, to overcome obstacles or terrain gaps. Several quadrupeds [1, 2, 3] and humanoids [4, 5] demonstrate high jumps, while monopeds provide a simpler, highly dynamic platform for performance optimization. Power efficiency remains crucial, as minimizing energy use during jumps is essential for deployment.\n\nMost prior studies on robotic jumping focus on control strategies with heuristic mechanical design, whereas optimal performance requires joint optimization of mechanical structure and control. For example, [6] optimizes actuator design for jump height but ignores energy consumption and link length variation. Recent co-design methods [7, 8, 9] optimize link lengths, transmission ratios, and spring stiffness, but target walking rather than jumping. Similarly, [10] improves energy efficiency at a fixed jump height, limiting performance. Work such as [11] maximizes jump height using passive elements like springs to improve efficiency, yet omits control parameter optimization.\n\nWhile co-design for legged systems has been studied, actuator gearbox modeling, especially for jointly optimizing jumping performance and energy consumption, remains underexplored. Works such as [9, 8, 7, 10] focus on link lengths, transmission ratios, and spring stiffness, but omit detailed planetary gearbox design. Similarly, [12] optimizes link lengths and actuator attachment points without addressing gearboxes. Co-design efforts for manipulators and monopeds [13, 14] consider gear ratios, compliance, and link masses, yet exclude gearbox type, internal parameters, or actuator mass. Other studies [15, 16, 17] model motor and gearbox friction but target belt-driven systems which are non-planetary. Although [6, 18] optimize gearbox parameters, they omit overall actuator mass and joint control optimization.\n\nThe literature shows that many studies optimize design and control parameters, but often target either performance or energy efficiency, not both. Most co-optimization approaches omit detailed gearbox parameters and gearbox type as design variables. Existing methods also rely on nominal models to estimate component masses (motors, gearboxes, and links) leading to notable deviations from actual values. To bridge this gap, we present a co-design optimization framework for a monoped robot. The key contributions of this work are:\n\nWe propose a novel co-design optimization framework that maximizes height in a jump while minimizing energy consumption by jointly optimizing design and control parameters.\n\nWe propose a novel co-design optimization framework that maximizes height in a jump while minimizing energy consumption by jointly optimizing design and control parameters.\n\nWe include gearbox parameters in detail like, number of teeth, module, number of planet gears, and actuator type (Internal and External configuration). To our knowledge, this is the first optimization of gearbox parameters and type within a co-design framework.\n\nWe utilize realistic mass models for hip and knee actuators that incorporate gears, motors, and components like bearings, carriers, casings, and backplates. Link mass models include all constituent parts, offering improved accuracy over previous methods.\n\nThe paper is organized as follows: Section II describes the monoped’s design and control architecture. Section III outlines the methodology, including the co-design framework and optimization objectives. Section IV presents the results, and Section V summarizes the findings and future research directions.\n\n1. We propose a novel co-design optimization framework that maximizes height in a jump while minimizing energy consumption by jointly optimizing design and control parameters.\n\n2. We include gearbox parameters in detail like, number of teeth, module, number of planet gears, and actuator type (Internal and External configuration). To our knowledge, this is the first optimization of gearbox parameters and type within a co-design framework.\n\n3. We utilize realistic mass models for hip and knee actuators that incorporate gears, motors, and components like bearings, carriers, casings, and backplates. Link mass models include all constituent parts, offering improved accuracy over previous methods.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的共设计框架通常只优化跳跃高度或能量消耗，忽视两者之间的权衡。  \n2. 现有方法缺乏对齿轮箱参数的优化，导致设计难以在实践中复制。  \n3. 需要一个能同时优化机械设计和控制参数的框架，以提高单足跳跃的性能和能效。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 许多研究集中于控制策略，机械设计往往采用启发式方法，未能实现最佳性能。  \n2. 现有的共设计方法主要针对行走而非跳跃，缺乏对跳跃性能和能量消耗的联合优化。  \n3. 先前的研究通常忽略了详细的齿轮箱设计，未能将其作为设计变量进行优化。  \n\n【提出了什么创新的方法】  \n我们提出了一种新颖的三阶段共设计优化框架，旨在通过联合优化设计和控制参数，最大化单足的跳跃高度，同时最小化机械能耗。该方法详细考虑了齿轮箱参数，如齿数、模数和行星齿轮数量，并利用现实的质量模型来提高准确性。实验结果表明，与基线设计相比，机械能耗减少了50%，跳跃高度达到了0.8米。该框架显著降低了手动设计迭代的需求，并为直接制造提供了参数化的CAD模型。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation",
            "authors": "Haoran Zhang,Shuanghao Bai,Wanqi Zhou,Yuedi Zhang,Qi Zhang,Pengxiang Ding,Cheng Chi,Donglin Wang,Badong Chen",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05827",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05827",
            "arxiv_html_link": "https://arxiv.org/html/2510.05827v1",
            "abstract": "Robotic grasping is one of the most fundamental tasks in robotic manipulation, and grasp detection/generation has long been the subject of extensive research. Recently, language-driven grasp generation has emerged as a promising direction due to its practical interaction capabilities. However, most existing approaches either lack sufficient reasoning and generalization capabilities or depend on complex modular pipelines. Moreover, current grasp foundation models tend to overemphasize dialog and object semantics, resulting in inferior performance and restriction to single-object grasping.\nTo maintain strong reasoning ability and generalization in cluttered environments, we propose VCoT-Grasp, an end-to-end grasp foundation model that incorporates visual chain-of-thought reasoning to enhance visual understanding for grasp generation. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically focuses on visual inputs while providing interpretable reasoning traces.\nFor training, we refine and introduce a large-scale dataset, VCoT-GraspSet, comprising 167K synthetic images with over 1.36M grasps, as well as 400+ real-world images with more than 1.2K grasps, annotated with intermediate bounding boxes. Extensive experiments on both VCoT-GraspSet and real robot demonstrate that our method significantly improves grasp success rates and generalizes effectively to unseen objects, backgrounds, and distractors. More details can be found at https://zhanghr2001.github.io/VCoT-Grasp.github.io/.",
            "introduction": "Grasping is a fundamental capability in tabletop manipulation and serves as the basis for downstream tasks such as placing, rearranging, or relocating objects. Traditional grasp detection methods [5, 6, 7, 8, 9, 10, 11] typically rely on Convolutional Neural Networks (CNNs) to extract visual and spatial features, or on generative models such as Variational Auto-Encoders (VAEs) [12, 13], which propose feasible grasps across the entire scene. More recently, transformer-based approaches [14, 15] have been introduced. While these methods can generate high-quality grasps, they lack the ability to focus on a specific target object, often requiring additional post-processing to filter relevant grasps. This limitation reduces semantic guidance and poses challenges for seamless human–robot interaction in real-world environments.\n\nTo overcome these limitations, language-driven grasping has emerged, enabling object-specific and instruction-guided manipulation. Some approaches incorporate language as an additional modality for multimodal feature fusion [1, 2, 16, 17, 18], while others exploit large language and vision-language models for planning and grounding to filter or rank grasp candidates [19, 3, 20, 21, 22, 23]. By integrating natural language understanding, these methods enhance practicality in real-world applications.\nHowever, the former lacks explicit reasoning capabilities, while the latter often depends on modular, cascading pipelines, resulting in larger and more complex systems with higher computational and memory costs. Both approaches still exhibit limited generalization. Given that foundation models have already demonstrated strong textual and visual generalization, we aim to develop an end-to-end reasoning foundation model for grasp detection and generation.\n\nRecently, RT-Grasp [4] introduced the first end-to-end foundation model specifically designed for language-driven grasp generation. It demonstrates impressive language reasoning ability to infer and refine predicted grasps. However, RT-Grasp still suffers from several limitations, including inefficient training, applicability restricted to single-object scenarios, and inferior performance that is even slightly worse than traditional methods. We attribute these shortcomings to its overemphasis on dialog and object semantics rather than direct visual understanding, which makes it particularly ineffective in complex and cluttered environments.\n\nTo this end, inspired by recent “think with images” techniques [24, 25, 26], we propose VCoT-Grasp, an end-to-end VLM-based framework that incorporates visual chain-of-thought reasoning to enhance visual understanding for grasp generation. The key insight is that humans interpret complex visual information by selectively focusing on salient regions or fine details, rather than processing the entire scene uniformly. In contrast, most robotic grasp foundation models handle aligned image contexts at a fixed granularity, thereby constraining efficiency and flexibility. To emulate human-like efficient reasoning, models must identify critical regions containing essential visual cues and dynamically zoom in to capture context at the appropriate scale.\nSpecifically, given an image and a language instruction, the model first identifies the target object and predicts its bounding box. The corresponding region is then cropped and resized to a normalized scale, and fed back into the model to generate the final grasp prediction. This multi-turn in-context paradigm improves visual understanding and enables finer-grained reasoning. Moreover, the potential of multi-turn in-context learning and the advantages of chain-of-thought reasoning remain largely unexplored in robotic foundation models, and our work bridges this gap.\n\nFoundation models typically require large-scale datasets for effective training. However, existing large-scale grasp datasets are predominantly synthetic and automatically annotated, leading to substantial noise and limited accuracy. To address this issue, we refine and recollect a new dataset based on [27], namely VCoT-GraspSet. In total, the dataset comprises 167K synthetic images with corresponding grasp annotations and more than 400 real-world images with over 1.2K annotated grasps.\n\nOur main contributions are as follows:\n\nWe propose VCoT-Grasp, an end-to-end foundation model that combines language-driven grasp generation with visual chain-of-thought reasoning, improving visual understanding, grasp quality, and generalization.\n\nWe present VCoT-GraspSet, a refined grasping dataset comprising 167K synthetic images with over 1.36M grasps and 400+ real-world images with more than 1.2K grasps. Each sample includes an image, grasp annotations, and intermediate bounding boxes that serve as chain-of-thought context.\n\nExtensive experiments on both VCoT-GraspSet and real-world scenarios demonstrate that VCoT-Grasp produces high-quality grasps and generalizes effectively to unseen objects, backgrounds, and distractors.\n\n1. We propose VCoT-Grasp, an end-to-end foundation model that combines language-driven grasp generation with visual chain-of-thought reasoning, improving visual understanding, grasp quality, and generalization.\n\n2. We present VCoT-GraspSet, a refined grasping dataset comprising 167K synthetic images with over 1.36M grasps and 400+ real-world images with more than 1.2K grasps. Each sample includes an image, grasp annotations, and intermediate bounding boxes that serve as chain-of-thought context.\n\n3. Extensive experiments on both VCoT-GraspSet and real-world scenarios demonstrate that VCoT-Grasp produces high-quality grasps and generalizes effectively to unseen objects, backgrounds, and distractors.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的语言驱动抓取方法缺乏足够的推理和泛化能力。  \n2. 当前的抓取基础模型过于强调对话和物体语义，导致性能不足。  \n3. 复杂环境中的抓取生成需要更强的视觉理解和推理能力。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统抓取方法依赖CNN和生成模型，但在复杂环境中泛化能力不足。  \n2. 语言驱动抓取方法通过多模态特征融合提升实用性，但缺乏明确的推理能力。  \n3. RT-Grasp虽然引入了端到端的基础模型，但在复杂场景中表现不佳，且训练效率低。  \n\n【提出了什么创新的方法】  \n我们提出了VCoT-Grasp，一个端到端的抓取基础模型，结合了视觉链式推理以增强抓取生成的视觉理解。该模型采用多轮处理范式，动态关注视觉输入并提供可解释的推理轨迹。通过引入VCoT-GraspSet数据集，模型在167K合成图像和400+真实图像上进行训练，显著提高了抓取成功率，并有效泛化到未见物体、背景和干扰物。实验结果表明，VCoT-Grasp在复杂环境中表现出色，克服了现有方法的局限性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Human-in-the-loop Optimisation in Robot-assisted Gait Training",
            "authors": "Andreas Christou,Andreas Sochopoulos,Elliot Lister,Sethu Vijayakumar",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05780",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05780",
            "arxiv_html_link": "https://arxiv.org/html/2510.05780v1",
            "abstract": "Wearable robots offer a promising solution for quantitatively monitoring gait and providing systematic, adaptive assistance to promote patient independence and improve gait. However, due to significant interpersonal and intrapersonal variability in walking patterns, it is important to design robot controllers that can adapt to the unique characteristics of each individual. This paper investigates the potential of human-in-the-loop optimisation (HILO) to deliver personalised assistance in gait training. The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) was employed to continuously optimise an assist-as-needed controller of a lower-limb exoskeleton. Six healthy individuals participated over a two-day experiment. Our results suggest that while the CMA-ES appears to converge to a unique set of stiffnesses for each individual, no measurable impact on the subjects’ performance was observed during the validation trials.\nThese findings highlight the impact of human-robot co-adaptation and human behaviour variability, whose effect may be greater than potential benefits of personalising rule-based assistive controllers. Our work contributes to understanding the limitations of current personalisation approaches in exoskeleton-assisted gait rehabilitation and identifies key challenges for effective implementation of human-in-the-loop optimisation in this domain.",
            "introduction": "Wearable robotic devices hold great promise in enhancing the outcomes of physical therapy and reducing the physical strain on healthcare professionals. However, the growing use of robotic assistance introduces the risk of depersonalising rehabilitation, potentially losing the tailored, highly effective treatments typically provided by healthcare providers [1, 2]. To address this concern, there has been a shift toward the use of collaborative robots designed to cater to the specific needs of patients, offering personalised assistance.\n\nIn gait rehabilitation, several studies have emphasised the importance of providing “assistance as needed”—partial support that encourages the patient’s active participation [1, 3, 4, 5]. Although various control strategies exist to achieve this [6, 7, 8, 9, 10], it remains unclear whether there is a single control\nstrategy that is superior to other strategies or that is in its form optimal for each individual. A prevailing issue with current practices is that many existing controllers are tuned based on the performance of a healthy participant, resulting in generalised solutions that often fail to meet individual needs effectively. Given the significant variability in gait among individuals, it is crucial to explore new methods for adjusting control parameters in wearable devices to deliver personalised and adaptive assistance.\n\nHuman-in-the-loop optimisation (HILO) is a dynamic approach that integrates human feedback directly into the control system of wearable devices allowing for real-time adjustments and ensuring that the device adapts to the user’s unique biomechanics and physiological responses. This method leverages the cyclic nature of gait, where adjustments to robot controllers are made iteratively with each gait cycle or number of gait cycles (Figure 1a). Based on this continuous feedback loop, HILO tailors robotic controllers to the needs of the user providing individualised assistance.\n\nTo date, HILO has been successful in adjusting the assistance provided by wearable robots in order to reduce the metabolic cost of gait [11, 12, 13, 14], increase the self-selected speed of walking [15, 16] in healthy subjects and reduce joint loading in manual material handling activities [17]. Using mostly one-DOF robots, studies have focused on the optimisation of parameterised assistive torque profiles for mainly the hip joint and the ankle joint. However, the effectiveness of HILO for the personalisation of rehabilitation controllers has not been studied. It is hypothesised here that HILO could be used for the personalisation of the open parameters of rehabilitation controllers in order to provide assistance as needed.\n\nSo far, to carry out HILO, two main algorithms have been used: the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and Bayesian optimisation. Both methods, are sample-based derivative-free optimisation methods that search for the global optimum within a constrained space. However, their underlying assumptions differ which will likely affect the efficacy of HILO, particularly when applied to gait training where time-dependent gait variability is expected to be higher.\n\nIn Bayesian optimisation, a surrogate model of a continuous function is constructed based on the sampled observations. Using Gaussian process regression, the posterior probability distribution of the unknown function, f​(x)f(x), is iteratively updated, and is used to update the acquisition function, a​(x)a(x). The updated acquisition function is then used to compute the next best sample point, xix_{i}, and this process repeats. After NN observations, this process terminates and the point where the value of f​(x)f(x) is highest (or lowest) is obtained. To do this, a common acquisition function is the expected improvement function, which balances exploration and exploitation. Using the expected improvement acquisition function the next best sample point is computed as the point with the highest expected quality and the highest posterior standard deviation, based on the assumption that f​(x)f(x) (given observations y1:ny_{1:n} at points x1:nx_{1:n}) is normally distributed [18]. This implementation assumes noise-free evaluations and a constant function ff. However, when it comes to HILO, where ff represents the response of humans to an external force, some of these assumptions may be violated. It is important when using Bayesian optimisation to incorporate methods for updating the acquisition function based on realistic values for expected noise, and account for time-dependent changes in human behaviour due to fatigue, concentration and/or motor learning, which are often hard to model and predict.\n\nIn CMA-ES a stochastic search for the global optimum of an unknown function, ff, is pursued iteratively through a series of generations, gg. On every generation, a total of λ\\lambda samples, {xkg|k∈ℕ,1≤k≤λ}\\{x_{k}^{g}|k\\in\\mathbb{N},1\\leq k\\leq\\lambda\\}, are generated by sampling a multivariate normal distribution with mean, 𝐦g{\\bf m}^{g}, and covariance, 𝐂g{\\bf C}^{g}, and are evaluated. Based on the observations, the new mean of the search distribution, 𝐦g+1∈ℝn{\\bf m}^{g+1}\\in\\mathbb{R}^{n}, the new covariance matrix, 𝐂g+1∈ℝn×n{\\bf C}^{g+1}\\in\\mathbb{R}^{n\\times n}, and the new step size, σg+1∈ℝ>0\\sigma^{g+1}\\in\\mathbb{R}_{>0}, are updated, where nn is the dimension of the search space. A step of size, σ\\sigma, in a direction dictated by the sampled observations is performed, a new sample population is generated around the new mean and this process is repeated for GG generations. This process does not assume that the unknown function, ff, is constant and does not prevent resampling of the same points. This allows time-dependent changes in function ff to be captured and noise in the sampled observations to be accounted for.\n\nIn this work, we propose the personalisation of a lower-limb assist-as-needed controller using HILO and the CMA-ES. Through an experimental study, we observe the ability of HILO to adjust the open parameters of a gait training controller in order to help the users accurately follow a predefined kinematic path with minimal assistance. A continuous optimisation protocol is followed over a multi-day trial as described in [13]. The results obtained from six healthy subjects are presented and discussed.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的可穿戴机器人在步态训练中缺乏个性化适应性。  \n2. 传统控制策略未能有效满足个体需求，导致治疗效果不佳。  \n3. 人机协作优化（HILO）在个性化康复控制中的有效性尚未被充分研究。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有研究主要集中在通过单一控制策略优化步态辅助，但缺乏对个体差异的考虑。  \n2. HILO在减少代谢成本和提高步态速度方面取得了一定成功，但在康复控制的个性化应用上仍然缺乏实证研究。  \n\n【提出了什么创新的方法】  \n本研究提出了一种基于人机协作优化（HILO）和协方差矩阵适应进化策略（CMA-ES）的方法，旨在个性化下肢辅助控制器。通过对六名健康个体进行多日实验，观察HILO如何调整步态训练控制器的开放参数，以帮助用户以最小的辅助准确跟随预定义的运动路径。实验结果表明，尽管CMA-ES能够为每个个体收敛到一组独特的刚度值，但在验证试验中未观察到对受试者表现的可测量影响。这突显了人机协同适应和人类行为变异性的影响，可能超过个性化规则基础辅助控制器的潜在好处。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Federated Split Learning for Resource-Constrained Robots in Industrial IoT: Framework Comparison, Optimization Strategies, and Future Directions",
            "authors": "Wanli Ni,Hui Tian,Shuai Wang,Chengyang Li,Lei Sun,Zhaohui Yang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Systems and Control (eess.SY)",
            "comment": "submitted to the IEEE magazine",
            "pdf_link": "https://arxiv.org/pdf/2510.05713",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05713",
            "arxiv_html_link": "https://arxiv.org/html/2510.05713v1",
            "abstract": "Federated split learning (FedSL) has emerged as a promising paradigm for enabling collaborative intelligence in industrial Internet of Things (IoT) systems, particularly in smart factories where data privacy, communication efficiency, and device heterogeneity are critical concerns.\nIn this article, we present a comprehensive study of FedSL frameworks tailored for resource-constrained robots in industrial scenarios.\nWe compare synchronous, asynchronous, hierarchical, and heterogeneous FedSL frameworks in terms of workflow, scalability, adaptability, and limitations under dynamic industrial conditions. Furthermore, we systematically categorize token fusion strategies into three paradigms: input-level (pre-fusion), intermediate-level (intra-fusion), and output-level (post-fusion), and summarize their respective strengths in industrial applications.\nWe also provide adaptive optimization techniques to enhance the efficiency and feasibility of FedSL implementation, including model compression, split layer selection, computing frequency allocation, and wireless resource management. Simulation results validate the performance of these frameworks under industrial detection scenarios.\nFinally, we outline open issues and research directions of FedSL in future smart manufacturing systems.",
            "introduction": "The rapid evolution of the industrial Internet of Things (IoT) has catalyzed a paradigm shift toward intelligent, autonomous, and interconnected manufacturing systems [1].\nAt the heart of this transformation are networked robots that perform complex tasks such as quality inspection, predictive maintenance, and multi-device collaboration across dynamic production environments.\nThese robots are increasingly equipped with multimodal sensors and onboard computing units, enabling them to perceive, reason, and act in real time [2].\nHowever, the deployment of data-driven artificial intelligence (AI) models in such settings faces critical challenges, including data privacy concerns, limited communication bandwidth, heterogeneous hardware capabilities, and stringent latency requirements.\n\nTraditional centralized learning approaches, which involve collecting raw data on a central server for model training, are not well-suited for industrial IoT systems due to increasing data volumes, high transmission latency, and strict regulations on data sharing.\nFederated learning has emerged as a privacy-preserving alternative, allowing distributed devices to collaboratively train a global model without exchanging local data [3].\nHowever, in resource-constrained robotic systems, federated learning still incurs significant communication overhead due to the transmission of full model gradients, and places heavy computational burdens on edge devices during local training.\nTo address these limitations, federated split learning (FedSL) has recently gained attention as a hybrid learning paradigm that combines the strengths of split learning and federated learning [4].\nAs shown in Fig. 1, deep neural networks are partitioned between client devices (e.g., robots) and edge servers in FedSL, with devices computing partial forward propagation up to a predefined split layer and transmitting intermediate features to the server for completion of one iteration [5].\nThis approach significantly reduces the computational load on client devices and enhances privacy by preventing raw data from leaving the robot. Moreover, by decoupling model execution across heterogeneous participants, FedSL enables fine-grained adaptation to varying communication, computation, and energy constraints across industrial robots [6].\nAlthough FedSL shows potential, the application of FedSL in industrial robotics remains fragmented and insufficiently studied. Existing studies often focus on isolated aspects rather than providing a comprehensive analysis of architectural diversity, multimodal fusion strategies, or cross-layer optimization techniques tailored to industrial tasks [7, 8, 9].\nFurthermore, the heterogeneity of robotic platforms (in terms of sensors, computing power, and task objectives) demands flexible FedSL frameworks that go beyond one-size-fits-all designs.\n\nIn this article, we present a comprehensive analysis of FedSL frameworks in the context of networked robots towards industrial intelligence.\nThe primary contributions of this paper are threefold:\nWe begin by reviewing and comparing four representative FedSL frameworks (synchronous, asynchronous, hierarchical, and heterogeneous FedSL), highlighting their operational workflows, scalability, robustness to network dynamics, and suitability for different industrial tasks.\nSubsequently, we provide a taxonomy of token fusion strategies for multimodal data in industrial environments, such as pre-fusion (early fusion), intra-fusion (intermediate fusion), and post-fusion (late fusion) paradigms, and then analyze their performance trade-offs in real-world tasks such as vision-based inspection, human–robot interaction, and equipment maintenance.\nIn addition, we discuss adaptive optimization techniques that can enhance the efficiency of FedSL frameworks, including split layer selection, resource management, and AI model compression. Simulation results validate the performance of these frameworks under resource-limited industrial conditions.\nFinally, we list several future research directions for deploying FedSL in smart factories, such as dynamic task adaptation, human-in-the-loop collaboration, and energy-aware scheduling.\n\nThe remainder of this paper is organized as follows:\nSection II presents a comparative analysis of FedSL frameworks and introduces token fusion strategies for networked robots.\nSection III discusses adaptive optimization strategies used in FedSL.\nSection IV provides simulation results for FedSL in industrial tasks.\nSection V outlines challenges and future directions, which is followed by the conclusion in Section VI.",
            "llm_summary": "【论文的motivation是什么】  \n1. 资源受限的机器人在工业物联网中的智能协作面临数据隐私和通信效率的挑战。  \n2. 现有的联邦学习方法在资源受限环境下的应用存在通信开销和计算负担过重的问题。  \n3. 需要灵活的框架来适应工业环境中机器人平台的异质性。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统的集中式学习方法不适合工业物联网，因数据量大和高延迟。  \n2. 联邦学习提供了隐私保护的替代方案，但在资源受限的机器人系统中仍存在显著的通信开销。  \n3. 现有研究多集中于孤立的方面，缺乏对架构多样性和优化技术的全面分析。  \n\n【提出了什么创新的方法】  \n本文提出了一种全面的联邦分割学习（FedSL）框架，比较了同步、异步、分层和异构FedSL框架的工作流程和适用性。通过对多模态数据的三种token融合策略进行分类，分析了它们在工业应用中的性能权衡。此外，提出了自适应优化技术以提高FedSL的效率，包括模型压缩和资源管理。模拟结果验证了这些框架在资源受限工业条件下的有效性。最终，文章指出了未来在智能制造系统中应用FedSL的研究方向，如动态任务适应和人机协作。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Stable Robot Motions on Manifolds: Learning Lyapunov-Constrained Neural Manifold ODEs",
            "authors": "David Boetius,Abdelrahman Abdelnaby,Ashok Kumar,Stefan Leue,Abdalla Swikir,Fares J. Abu-Dakka",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG); Optimization and Control (math.OC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05707",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05707",
            "arxiv_html_link": "https://arxiv.org/html/2510.05707v1",
            "abstract": "Learning stable dynamical systems from data is crucial for safe and reliable robot motion planning and control. However, extending stability guarantees to trajectories defined on Riemannian manifolds poses significant challenges due to the manifold’s geometric constraints. To address this, we propose a general framework for learning stable dynamical systems on Riemannian manifolds using neural ordinary differential equations. Our method guarantees stability by projecting the neural vector field evolving on the manifold so that it strictly satisfies the Lyapunov stability criterion, ensuring stability at every system state. By leveraging a flexible neural parameterisation for both the base vector field and the Lyapunov function, our framework can accurately represent complex trajectories while respecting manifold constraints by evolving solutions directly on the manifold.\nWe provide an efficient training strategy for applying our framework and demonstrate its utility by solving Riemannian LASA datasets on the unit quaternion (S3S^{3}) and symmetric positive-definite matrix manifolds, as well as robotic motions evolving on ℝ3×S3\\mathbb{R}^{3}\\times S^{3}.\nWe demonstrate the performance, scalability, and practical applicability of our approach through extensive simulations and by learning robot motions in a real-world experiment.",
            "introduction": "The deployment of robotic systems in complex real-world scenarios demands models that are highly expressive and rigorously stable. Learning from demonstrations enables non-expert users to teach robots new skills by providing example trajectories [1, 2, 3].\nHowever, purely data-driven methods frequently lack formal stability guarantees required, in particular, in safety-critical domains.\n\nRobotic state variables, such as orientation, stiffness, or inertia, naturally reside on smooth non-Euclidean spaces that are inherently structured as Riemannian manifolds [3, 4, 5, 6, 7, 8].\nTo ensure reliable task execution, learned trajectories must accurately capture complex motion patterns on Riemannian manifolds while reliably steering the system toward a desired equilibrium state. Stable dynamical systems offer a principled solution by guaranteeing trajectory convergence and robustness to perturbations, significantly enhancing reliability in practice [9, 10, 11, 12, 13, 14, 15, 16].\n\nRecently, neural ordinary differential equations (NODEs) [17] have emerged as an expressive framework for modelling continuous-time dynamical systems.\nNODEs offer the flexibility to represent intricate trajectories effectively [10, 18, 19, 16, 20], also in high-dimensional spaces [21].\nWhile stability of NODEs has been studied extensively within Euclidean spaces [22, 23, 24], stability guarantees for NODEs on Riemannian manifolds remain an underexplored area.\nWithout stability guarantees, learned dynamics may drift, oscillate, or exhibit unpredictable behaviour, undermining both safety and performance.\n\nIn this paper, we introduce stable neural manifold ordinary differential equations (sNMODE) — a novel stability framework that combines Lyapunov-based stability with NODEs evolving on arbitrary Riemannian manifolds.\nOur approach jointly learns an expressive neural manifold vector field and a neural Lyapunov function.\nProjecting the vector field using a Lyapunov-based corrective term guarantees exponential stability towards a predefined equilibrium state [22].\nWe uncover and resolve a significant issue in several stable NODE frameworks [22, 20, 21] that invalidates their stability guarantees and causes numerical instability in practice.\n\nFor practically applying our framework, we devise an efficient training strategy that divides training sNMODEs into pretraining a base vector field on demonstrations, pretraining a Lyapunov function, and fine-tuning the composed system under the stability projection.\nOur sNMODEs reliably replicate demonstration trajectories while providing strong stability guarantees.\nThey outperform existing approaches [7, 8] in terms of efficiency, respectively, performance and scalability.\nIn summary, our main contributions are\n\nproposing sNMODE, a novel manifold-aware Lyapunov projection method that ensures stability on Riemannian manifolds without imposing restrictive assumptions on the manifold or model structure,\n\nresolving a critical stability issue in previous frameworks [22, 20, 21] that invalidates theoretical stability guarantees and causes numerical instability in practice,\n\nproviding an efficient training strategy for learning stable robot motions from demonstrations,\n\naddressing a fundamental limitation of the Riemannian LASA dataset [8] regarding data realism, and\n\nperforming an extensive experimental evaluation on several Riemannian manifolds, both in simulation and on a real-world robot.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要从数据中学习稳定的动态系统，以确保机器人运动规划和控制的安全性和可靠性。  \n2. 在Riemannian流形上扩展稳定性保证面临显著挑战，尤其是在复杂的真实世界场景中。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有的研究主要集中在欧几里得空间中的NODEs稳定性，但对Riemannian流形上的稳定性保证研究不足。  \n2. 纯数据驱动的方法缺乏在安全关键领域所需的正式稳定性保证，导致学习的动态系统可能出现漂移或不稳定行为。  \n\n【提出了什么创新的方法】  \n我们提出了一种稳定的神经流形常微分方程（sNMODE）框架，结合了基于Lyapunov的稳定性与在任意Riemannian流形上演化的NODEs。该方法通过使用Lyapunov基础的修正项来投影向量场，确保系统朝向预定义的平衡状态的指数稳定性。我们设计了一种高效的训练策略，分为对演示的基础向量场进行预训练、Lyapunov函数的预训练以及在稳定性投影下的系统微调。通过广泛的模拟和真实世界实验，我们的sNMODE能够可靠地复制演示轨迹，同时提供强有力的稳定性保证，显著优于现有方法。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Oracle-Guided Masked Contrastive Reinforcement Learning for Visuomotor Policies",
            "authors": "Yuhang Zhang,Jiaping Xiao,Chao Yan,Mir Feroskhan",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05692",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05692",
            "arxiv_html_link": "https://arxiv.org/html/2510.05692v1",
            "abstract": "A prevailing approach for learning visuomotor policies is to employ reinforcement learning to map high-dimensional visual observations directly to action commands. However, the combination of high-dimensional visual inputs and agile maneuver outputs leads to long-standing challenges, including low sample efficiency and significant sim-to-real gaps. To address these issues, we propose Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL), a novel framework designed to improve the sample efficiency and asymptotic performance of visuomotor policy learning. OMC-RL explicitly decouples the learning process into two stages: an upstream representation learning stage and a downstream policy learning stage. In the upstream stage, a masked Transformer module is trained with temporal modeling and contrastive learning to extract temporally-aware and task-relevant representations from sequential visual inputs. After training, the learned encoder is frozen and used to extract visual representations from consecutive frames, while the Transformer module is discarded. In the downstream stage, an oracle teacher policy with privileged access to global state information supervises the agent during early training to provide informative guidance and accelerate early policy learning. This guidance is gradually reduced to allow independent exploration as training progresses. Extensive experiments in simulated and real-world environments demonstrate that OMC-RL achieves superior sample efficiency and asymptotic policy performance, while also improving generalization across diverse and perceptually complex scenarios.",
            "introduction": "First-person-view navigation for autonomous robots represents a challenging task that emulates human behavioral paradigms [1], with wide-ranging applications across critical domains, including autonomous driving [2], field exploration [3], and urban surveillance [4]. Skilled human operators demonstrate the ability to perform agile and precise maneuvers using only delayed and occasionally occluded visual feedback from onboard monocular cameras, without compromising safety. This remarkable capability is attributed to the human brain’s ability to distill abstract perceptual signals into compact, low-dimensional representations that serve as internal interpretations for guiding action. Given the complexity and unstructured nature of real-world environments, autonomous robotic systems must be equipped with a feature extractor capable of filtering out noise and redundancy from high-dimensional visual feeds, and extracting effective representations for decision-making.\n\nTraditional methods typically follow a mapping–planning paradigm [5], including Simultaneous Localization and Mapping (SLAM) [6] and Structure from Motion (SfM) [7]. These pipelines heavily rely on sophisticated hand-crafted features for keypoint detection and matching. Although effective, their reliance on feature engineering limits flexibility and incurs high computational cost. Moreover, their performance degrades significantly in textureless or visually ambiguous environments. Deep learning (DL) [8] offers an alternative paradigm by leveraging deep neural networks (DNNs) to extract task-relevant features directly from raw sensory observations and map them to action commands. In contrast to heuristic feature engineering, DNN-based encoders extract representations directly from data, allowing the model to adapt to diverse input distributions without manual intervention. However, training visuomotor policies for autonomous robot navigation with DL often demands expert-annotated action datasets as supervision signals, which are expensive to collect, particularly in real-world settings [9].\n\nRecently, reinforcement learning (RL) has achieved expert-level performance, even exceeding human professionals in domains such as Go [10], video games [11], and Mahjong [12]. By integrating RL algorithms with high-capacity DNNs, deep reinforcement learning (DRL) is increasingly adopted to address complex robotic perception and control tasks [13, 14, 15]. Specifically, in the domain of autonomous robot navigation where visuomotor policies take a sequence of image frames as input, existing DRL algorithms suffer from two major limitations: (i) the curse of dimensionality. The input data is extremely high-dimensional, making it difficult for the policy network to directly learn effective mappings from observations to actions, which often leads to unstable learning dynamics and poor generalization; (ii) sample inefficiency. Due to the trial-and-error nature of RL and the complexity of visual navigation tasks, training typically requires a large number of rollout trajectories, reflecting low sample efficiency and incurring substantial computational cost. These limitations jointly affect both the efficiency and effectiveness of DRL algorithms.\n\nTo address the aforementioned challenges, recent studies have explored representation learning, which trains a feature encoder to compress high-dimensional visual inputs into compact representations before feeding them into the downstream policy network. However, it remains non-trivial to design representation learning methods that are both effective and sample-efficient in DRL settings. Existing approaches generally fall into two categories: one category introduces auxiliary self-supervised losses to guide encoder learning [16, 17, 18], and the other learns a world model that captures transition dynamics and enables planning in the latent space [19, 20, 21].\n\nAs a prominent self-supervised strategy, contrastive learning has recently emerged as a powerful tool in representation learning [22, 23, 24]. Unlike class-level supervised learning approaches, it operates at the instance level and is primarily designed to learn generalizable representations through pretext tasks. Specifically, during pre-training, each input image is subjected to data augmentation techniques such as random cropping or color jittering, resulting in two distinct views of the same instance. These views are then passed through a shared feature encoder to produce latent representations. The fundamental objective of contrastive learning is to draw representations of the same instance closer in the feature space while enforcing dissimilarity with the rest in the batch, typically enforced via a contrastive loss [22]. Encouraged by its notable performance in the computer vision domain, contrastive learning has been extended to RL settings, commonly as an auxiliary loss [16, 25]. A prominent example is CURL [25], which stands for contrastive unsupervised representations for RL. It demonstrates that pixel-based RL can rival the performance of state-based RL counterparts.\n\nFurther explorations have applied contrastive learning to autonomous robot navigation tasks [26, 27, 28], where it has shown promise in improving robustness to visual interference and enhancing sample efficiency. Nonetheless, most existing approaches neglect the strong temporal correlations across consecutive frames. In visuomotor policy learning, understanding such temporal coherence is crucial. Intuitively, a representation encoder should not only process the current observation accurately, but also encode contextual relationships across adjacent frames. In addition, jointly optimizing the encoder and policy network often leads to domain-specific overfitting, thereby hindering generalization to unseen environments.\n\nEncouraged by prior advances, this paper proposes a novel framework named oracle-guided masked contrastive RL (OMC-RL) to enhance the sample efficiency and asymptotic performance of visuomotor policy learning. As illustrated in Fig. 1, OMC-RL differs from vanilla teacher-student policy learning and contrastive learning by integrating masked temporal contrastive learning with oracle-guided supervision, leading to joint improvements in both feature encoding and downstream decision-making. Specifically, we decouple the feature encoder from the RL policy to allow stable representation learning and independent control over encoder optimization. For upstream feature extraction, unlike previous approaches that utilize DNNs as pixel encoders [25], we incorporate a Transformer encoder [29] as an auxiliary module to model the temporal correlation across sequential observations. Specifically, the latent representations of certain frames are randomly masked and subsequently reconstructed by the Transformer module, encouraging the model to capture temporally consistent representations. A contrastive loss is subsequently designed to jointly train both the DNN encoder and Transformer, encouraging the reconstructed features to be similar to the unmasked ones while distinct from others. For downstream policy learning, we introduce an oracle teacher with privileged access to complete environment states, which is pretrained to generate expert actions. A Kullback–Leibler (KL) divergence loss is designed between the oracle and agent policies to guide policy learning. We conduct extensive experiments in simulated and real-world environments to validate the effectiveness of OMC-RL. Simulation experiments show that OMC-RL consistently outperforms all learning- and planning-based baselines across diverse scenarios. Moreover, real-world in-flight experiments demonstrate that OMC-RL outperforms the state-of-the-art baseline, further validating the advantages of the proposed framework in practical deployment.\n\nThe contributions of this paper are summarized below.\n\nWe propose a masked contrastive learning method for robot navigation to address the curse of dimensionality and sample inefficiency arising from high-dimensional visual inputs. The encoder is decoupled from the policy network to enable stable and task-agnostic representation learning. By reconstructing masked latent features from sequential inputs, the method improves temporal awareness and enhances robustness to domain shift.\n\nWe introduce an oracle-guided visuomotor policy learning framework, where an expert policy with privileged access to global environment states serves as a training-time oracle. By developing a learning-by-cheating strategy based on KL divergence, the agent is empowered to leverage dense supervision from the oracle, thereby improving sample efficiency and asymptotic performance.\n\nWe demonstrate that OMC-RL achieves strong simulation performance compared with various baselines. More importantly, we show that it surpasses the state-of-the-art baseline in real-world in-flight experiments and maintains robust performance under diverse visual disturbances.\n\nThe remainder of this paper is organized as follows. Section II reviews prior work on visuomotor policy learning, representation learning for RL, and sample-efficient RL. Section III introduces the preliminaries and formalizes the problem. Section IV details the methodology of the proposed framework. Section V describes the experimental setup for evaluating the proposed method. Section VI presents the results and analysis in simulation and physical environments. Finally, Section VII concludes the paper and outlines future research directions.",
            "llm_summary": "【论文的motivation是什么】  \n1. 解决高维视觉输入与灵活控制输出之间的低样本效率问题。  \n2. 缩小模拟与现实环境之间的性能差距。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统方法依赖手工特征，灵活性差且计算成本高。  \n2. 深度学习方法需要专家标注数据，收集成本高且泛化能力有限。  \n3. 现有的对比学习方法未充分利用时间序列信息，导致表现不佳。  \n\n【提出了什么创新的方法】  \n本研究提出了“Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL)”框架，分为两个阶段：上游表示学习和下游策略学习。上游阶段使用掩蔽的Transformer模块进行时间建模和对比学习，提取任务相关的表示；下游阶段通过具有全局状态信息的oracle教师策略指导早期训练，逐步减少指导以促进独立探索。实验表明，OMC-RL在模拟和现实环境中均表现出优越的样本效率和策略性能，尤其在复杂场景下的泛化能力显著提升。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Verifier-free Test-Time Sampling for Vision Language Action Models",
            "authors": "Suhyeok Jang,Dongyoung Kim,Changyeon Kim,Youngsuk Kim,Jinwoo Shin",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "3 figures",
            "pdf_link": "https://arxiv.org/pdf/2510.05681",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05681",
            "arxiv_html_link": "https://arxiv.org/html/2510.05681v1",
            "abstract": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance in robot control. However, they remain fundamentally limited in tasks that require high precision due to their single-inference paradigm. While test-time scaling approaches using external verifiers have shown promise, they require additional training and fail to generalize to unseen conditions. We propose Masking Distribution Guided Selection (MG-Select), a novel test-time scaling framework for VLAs that leverages the model’s internal properties without requiring additional training or external modules. Our approach utilizes KL divergence from a reference action token distribution as a confidence metric for selecting the optimal action from multiple candidates. We introduce a reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs, ensuring maximum uncertainty while remaining aligned with the target task distribution. Additionally, we propose a joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. Our experiments demonstrate that MG-Select achieves significant performance improvements, including a 28%/35% improvement in real-world in-distribution/out-of-distribution tasks, along with a 168% relative gain on RoboCasa pick-and-place tasks trained with 30 demonstrations.",
            "introduction": "Vision-Language-Action models (VLAs; Zitkovich et al. 2023; Kim et al. 2024; Black et al. 2025; Bjorck et al. 2025), trained on large-scale robotic datasets (O’Neill et al., 2024; Bu et al., 2025), have demonstrated remarkable performance in robot control. Among these, autoregressive VLAs represent one of the predominant VLAs (Driess et al., 2023; Kim et al., 2024; Pertsch et al., 2025), leveraging the same autoregressive objective used in training vision and foundation models without requiring architectural modifications, yet achieving comparable performance to more sophisticated architectures. Despite their success, VLAs remain fundamentally limited in tasks that demand high precision; even after extensive pre-training, they often fail on fine-grained manipulation tasks such as grasping or object placement (Nakamoto et al., 2024; Kwok et al., 2025; Gu et al., 2025; Yang et al., 2025). This precision gap is particularly problematic for real-world robotic applications where millimeter-level accuracy can determine task success or failure.\n\nPrevious work (Nakamoto et al., 2024; Kwok et al., 2025) shows that while VLAs can achieve high precision with adequate training, their greedy decoding (always choosing the highest-probability action) becomes a bottleneck. To address this limitation, inspired by the substantial gains observed in LLM reasoning with Test Time Scaling (TTS) (Wang et al., 2023; Wan et al., 2025; Kang et al., 2025), they use repeated sampling paired with an external verifier, i.e., a value function trained on robotic data. However, these approaches have significant drawbacks: First, they require additional training to obtain verifiers with reinforcement learning objectives before inference, which adds substantial computational overhead and complexity to the deployment pipeline. Second, these external verifiers fail to generalize to unseen input conditions (Nakamoto et al., 2024), such as novel task prompts or objects, and their reward modeling is tailored to specific datasets, severely limiting their broader applicability (Kwok et al., 2025).\n\nOur approach. To tackle this problem, our research goal is to develop a test-time scaling framework for VLAs that leverages the model’s internal properties without requiring additional training or external modules. Inspired by verifier-free approaches for TTS (Zheng et al., 2024), we begin with the most straightforward approach: selecting the action with the highest likelihood from multiple sampled actions. We observe that this simple technique alone can improve VLA performance by producing more precise actions in some cases (see Table 5 (a)). However, this approach is not effective in general, as VLAs fine-tuned on target tasks for next action token prediction often memorize expert trajectories, causing the probability distribution over action tokens to become overly concentrated, which leads to multiple sampling converging to the same result.\n\nThese insights motivate us to propose Masking Distribution Guided Selection (MG-Select), a novel TTS framework that leverages the KL divergence from a reference action token distribution as a confidence metric for selecting the optimal action from multiple candidates. Inspired by recent advances in LLM literature that use self-certainty measures (Kang et al., 2025), we adapt this principle to the VLA setting. Specifically, we introduce a reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs. This design ensures the reference distribution represents maximum uncertainty while remaining aligned with the target task distribution, providing a more meaningful baseline for confidence measurement. By selecting actions with the highest KL divergence from this uncertainty-aware reference, MG-Select effectively identifies the most confident action sequences while avoiding the limitations of likelihood-based selection, achieving significant performance improvements in practice. Additionally, we propose a joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution.\n\nIn our experiments, we have validated the effectiveness of our test-time scaling framework on both simulated (Nasiriany et al., 2024; Li et al., 2024; Liu et al., 2023) and real-world benchmarks (Khazatsky et al., 2024). Our results show that MG-Select consistently improves state-of-the-art VLAs (Pertsch et al., 2025) across diverse pick-and-place tasks and various environments. In particular, MG-Select achieves a 28% improvement in real-world in-distribution tasks and 35% in out-of-distribution tasks, along with a 168% relative gain on RoboCasa (Nasiriany et al., 2024) pick-and-place tasks trained with 30 demonstrations.",
            "llm_summary": "【论文的motivation是什么】  \n1. Vision-Language-Action models (VLAs)在高精度任务中的表现有限。  \n2. 现有的测试时间扩展方法依赖外部验证器，增加了训练和推理的复杂性。  \n3. 需要一种无需额外训练的内部属性利用方法来提升VLA性能。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有方法通过外部验证器提高VLA精度，但需要额外的训练和计算开销。  \n2. 外部验证器在未见条件下的泛化能力差，限制了其应用范围。  \n\n【提出了什么创新的方法】  \n提出了Masking Distribution Guided Selection (MG-Select)，一种无需外部模块的测试时间扩展框架。该方法通过KL散度作为信心度量，从多个候选动作中选择最优动作。引入的参考分布通过随机掩蔽状态和语言条件生成，确保最大不确定性并与目标任务分布对齐。通过联合训练策略，模型学习条件和无条件分布，进一步提升参考分布的质量。实验结果表明，MG-Select在真实和模拟基准上均显著提升了VLA的性能，包括在真实世界任务中实现28%和35%的改进，以及在RoboCasa任务上获得168%的相对增益。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation",
            "authors": "Taeyeop Lee,Gyuree Kang,Bowen Wen,Youngho Kim,Seunghyeok Back,In So Kweon,David Hyunchul Shim,Kuk-Jin Yoon",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.05662",
            "code": "https://sites.google.com/view/DeLTa25/",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05662",
            "arxiv_html_link": "https://arxiv.org/html/2510.05662v1",
            "abstract": "Despite the prevalence of transparent object interactions in human everyday life, transparent robotic manipulation research remains limited to short-horizon tasks and basic grasping capabilities.\nAlthough some methods have partially addressed these issues, most\nof them have limitations in generalizability to novel objects and are insufficient for precise long-horizon robot manipulation. To address this limitation, we propose DeLTa (Demonstration and Language-Guided Novel Transparent Object Manipulation), a novel framework that integrates depth estimation, 6D pose estimation, and vision-language planning for precise long-horizon manipulation of transparent objects guided by natural task instructions. A key advantage of our method is its single-demonstration approach, which generalizes 6D trajectories to novel transparent objects without requiring category-level priors or additional training.\nAdditionally, we present a task planner that refines the VLM-generated plan to account for the constraints of a single-arm, eye-in-hand robot for long-horizon object manipulation tasks.\nThrough comprehensive evaluation, we demonstrate that our method significantly outperforms existing transparent object manipulation approaches, particularly in long-horizon scenarios requiring precise manipulation capabilities. Project page: https://sites.google.com/view/DeLTa25/",
            "introduction": "Transparent objects are prevalent across real-world environments, including laboratories, kitchens, and manufacturing facilities. However, conventional depth sensors often fail to perceive these objects accurately. For example, commercial cameras [1, 2] suffer from significant limitations when emitted infrared light undergoes refraction or reflection at transparent surfaces, producing erroneous or missing depth measurements. These sensor limitations cause substantial challenges for reliable robotic manipulation of transparent objects. Effective robotic manipulation in diverse scenarios requires both reliable perception capabilities and robust handling of various object types, with transparent objects being particularly challenging. While simple pick-and-place tasks may tolerate approximate 3D object locations [3, 4], precise manipulation tasks demanding accurate grasping and placement require full 6D object pose estimation [5, 6, 7, 8].\n\nTransparent object pose estimation methods [9, 10, 11, 12] often adopt category prior knowledge to estimate poses of novel object instances within the same category. As a result, robotic manipulation for transparent objects is inherently restricted to category-level object pose estimation [13, 14, 15]. While category-level pose approaches have achieved promising results in generalizing to unknown objects within the same category, they struggle to generalize to novel objects beyond their trained categories. Moreover, their disregard of fine-grained object geometry limits applications to precise manipulation under certain task constraints (e.g., align beverage bottles in a straight row when stocking a grocery shelf) even for novel object instances within the same trained category. This makes novel object instance pose estimation methods [5, 16, 17] more desirable in such scenarios.\n\nIn terms of robotic manipulation policies for transparent objects, existing works [18, 19] have primarily focused on grasping diverse transparent objects.\nExtending these methods to more diverse and challenging scenarios—such as target-constrained placement studied in this work—remains largely under-explored.\nTo address such complex tasks, recent advances in learning from demonstration have proven effective, offering a cost-efficient way to enable diverse trajectory actions [20, 21, 22, 23].\nCompared to other demonstration data collection methods, such as robot demonstrations [24] or wearable devices [25, 26], human demonstrations excel due to their intuitive operation and minimal hardware requirements.\nNevertheless, learning from human demonstrations has been mostly achieved on regular objects [20, 21, 27, 28], leaving its extension to transparent objects insufficiently studied.\nThis limitation arises because the complex optical properties of transparent objects pose challenges to visual sensing, which makes the extraction of action trajectories from demonstrations inherently difficult.\nConsequently, insufficient capability for precise novel object pose estimation and the lack of diverse executable skills restrict long-horizon manipulation in real-world tasks.\nMoreover, the limited exploration of language understanding (e.g., “Can you make a green liquid in the cylinder?” [29]) constrains progress toward natural-language-driven task execution—an essential step for human-robot interaction and generalizable manipulation.\n\nTo tackle these challenges, our contributions can be summarized as follows:\n\nWe propose DeLTa, to our best knowledge, the first framework that achieves precise and long-horizon manipulation tasks for transparent objects, guided by human video demonstration and language instructions, as illustrated in Fig. 1.\n\nFor the first time, we explore 4D modeling of hand-object interaction information extracted from human demonstration video for transparent object manipulation, powered by recent advancements in stereo depth estimation, segmentation, and pose estimation.\n\nWe show that a single human demonstration per primitive skill suffices to generalize to novel objects, with the demonstration trajectory guiding obstacle-aware robot motion planning.\n\nWe propose a VLM-guided planner that decomposes natural language instructions into multi-step actions, refines them with validation and search to enforce robot-specific constraints (e.g., one-armed, eye-in-hand), and retrieves object meshes and demonstration trajectories from pre-computed databases for transparent object manipulation.\n\n1. We propose DeLTa, to our best knowledge, the first framework that achieves precise and long-horizon manipulation tasks for transparent objects, guided by human video demonstration and language instructions, as illustrated in Fig. 1.\n\n2. For the first time, we explore 4D modeling of hand-object interaction information extracted from human demonstration video for transparent object manipulation, powered by recent advancements in stereo depth estimation, segmentation, and pose estimation.\n\n3. We show that a single human demonstration per primitive skill suffices to generalize to novel objects, with the demonstration trajectory guiding obstacle-aware robot motion planning.\n\n4. We propose a VLM-guided planner that decomposes natural language instructions into multi-step actions, refines them with validation and search to enforce robot-specific constraints (e.g., one-armed, eye-in-hand), and retrieves object meshes and demonstration trajectories from pre-computed databases for transparent object manipulation.",
            "llm_summary": "【论文的motivation是什么】  \n1. 透明物体的操作在现实生活中普遍存在，但现有的机器人操作研究主要局限于短期任务和基本抓取能力。  \n2. 现有方法在新物体的泛化能力和精确的长时间操作方面存在不足。  \n3. 透明物体的复杂光学特性使得从人类演示中学习变得困难，限制了机器人在真实场景中的操作能力。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有的透明物体操作研究主要集中在短期抓取任务，缺乏对长时间操作的研究。  \n2. 透明物体的姿态估计方法通常依赖于类别先验知识，限制了对新实例的泛化能力。  \n3. 从人类演示中学习的研究多集中于常规物体，透明物体的学习尚未得到充分探索。  \n4. 现有的VLM（视觉语言模型）方法在处理复杂任务时往往假设对环境对象有完整的先验知识，缺乏对感知误差的处理。  \n\n【提出了什么创新的方法】  \n我们提出了DeLTa，一个集成深度估计、6D姿态估计和视觉语言规划的框架，旨在通过自然语言指令和人类视频演示实现透明物体的精确长时间操作。该方法的关键优势在于其单一演示的能力，能够在不需要类别级先验或额外训练的情况下，将6D轨迹泛化到新透明物体上。此外，我们设计了一个任务规划器，能够根据单臂、眼在手的机器人约束，优化VLM生成的计划。通过全面评估，我们的方法在长时间场景中显著超越了现有的透明物体操作方法，尤其是在需要精确操作能力的任务中。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "GO-Flock: Goal-Oriented Flocking in 3D Unknown Environments with Depth Maps",
            "authors": "Yan Rui Tan,Wenqi Liu,Wai Lun Leong,John Guan Zhong Tan,Wayne Wen Huei Yong,Fan Shi,Rodney Swee Huat Teo",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05553",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05553",
            "arxiv_html_link": "https://arxiv.org/html/2510.05553v1",
            "abstract": "Artificial Potential Field (APF) methods are widely used for reactive flocking control, but they often suffer from challenges such as deadlocks and local minima, especially in the presence of obstacles. Existing solutions to address these issues are typically passive, leading to slow and inefficient collective navigation. As a result, many APF approaches have only been validated in obstacle-free environments or simplified, pseudo-3D simulations. This paper presents GO-Flock, a hybrid flocking framework that integrates planning with reactive APF-based control. GO-Flock consists of an upstream Perception Module, which processes depth maps to extract waypoints and virtual agents for obstacle avoidance, and a downstream Collective Navigation Module, which applies a novel APF strategy to achieve effective flocking behavior in cluttered environments. We evaluate GO-Flock against passive APF-based approaches to demonstrate their respective merits, such as their flocking behavior and the ability to overcome local minima. Finally, we validate GO-Flock through obstacle-filled environment and also hardware-in-the-loop experiments where we successfully flocked a team of nine drones—six physical and three virtual— in a forest environment.",
            "introduction": "Flocking behavior, commonly observed in nature, involves the collective and coordinated movement of groups, such as flocks of birds or schools of fish. This natural phenomenon provides significant advantages that can be leveraged in multi-agent robotic systems. By mimicking flocking behavior, robotic systems can achieve sophisticated collective navigation, resulting in several key benefits such as efficient task distribution among multiple agents and lower production costs compared to a single, more complex system. Additionally, maintaining close proximity between agents enhances communication reliability.\n\nMany robotic flocking systems are derived from foundational flocking models, commonly known as Artificial Potential Field (APF) methods, such as those introduced by Reynolds and Vicsek et al. [1, 2]. To improve real-world applicability, Olfati-Saber [3] introduced obstacle virtual agents, representing obstacles as single points to facilitate avoidance. While numerous studies have adopted similar frameworks, real-world flocking remains an open challenge—particularly in handling environmental complexity and local minima. Despite impressive demonstrations, such as the flocking of 14 and 30 drones in real-world environments [4, 5], existing approaches often assume idealized geometric obstacles to side-step the issue of local minima. Currently, most APF strategies for overcoming local minima [6] are also passive, leading to slow and inefficient progress toward goals. While alternative planning-based methods (e.g., model predictive control [7, 8]) and learning-based approaches [9] to achieve collective navigation can effectively mitigate local minima, they introduce higher computational costs either during training or test time [10].\n\nTo this end, we advance the field of APF methods by extending the work of [3] and [4] to achieve flocking with quicker navigation towards goal while minimizing local minima. Specifically, this paper makes the following contributions:\n\nWe present GO-Flock, a hybrid framework with a novel APF serving as the reactive Collective Navigation Module that integrates inter-agent cohesion-repulsion, waypoint attraction, and obstacle avoidance. This module operates at a lower level, leveraging information from a higher-level planning Perception Module which processes depth maps to identify the required intermediate waypoints and virtual agents.\n\nTo the best of our knowledge, we are the first to demonstrate an APF framework with onboard depth sensor for flocking in realistic 3D environments. We validate our approach through simulations in realistic scenarios such as forests, and showcase a hardware-in-the-loop experiment where 66 real drones interact with 33 virtual drones in a simulated environment.\n\n1. We present GO-Flock, a hybrid framework with a novel APF serving as the reactive Collective Navigation Module that integrates inter-agent cohesion-repulsion, waypoint attraction, and obstacle avoidance. This module operates at a lower level, leveraging information from a higher-level planning Perception Module which processes depth maps to identify the required intermediate waypoints and virtual agents.\n\n2. To the best of our knowledge, we are the first to demonstrate an APF framework with onboard depth sensor for flocking in realistic 3D environments. We validate our approach through simulations in realistic scenarios such as forests, and showcase a hardware-in-the-loop experiment where 66 real drones interact with 33 virtual drones in a simulated environment.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的人工势场(APF)方法在复杂环境中集体导航时面临死锁和局部最小值的问题。  \n2. 现有解决方案通常是被动的，导致集体导航效率低下。  \n3. 真实世界中的APF方法在处理环境复杂性方面仍然存在挑战。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 许多研究采用APF方法，但大多数只在理想化环境中验证其有效性。  \n2. 现有APF策略通常假设已知的几何障碍物，限制了其在真实环境中的应用。  \n3. 被动策略在应对局部最小值时效率低下，且缺乏实时性。  \n\n【提出了什么创新的方法】  \nGO-Flock是一个混合框架，结合了规划与反应式APF控制。其核心在于一个新的APF策略，集成了代理间的凝聚-排斥、路径吸引和障碍物避免。该框架包含一个上游感知模块，处理深度图以提取路径点和虚拟代理，从而实现有效的集体导航。通过在复杂环境中进行仿真和硬件实验，GO-Flock成功展示了其在真实3D环境中进行集体导航的能力，克服了局部最小值问题，并提高了导航效率。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "ARRC: Advanced Reasoning Robot Control - Knowledge-Driven Autonomous Manipulation Using Retrieval-Augmented Generation",
            "authors": "Eugene Vorobiov,Ammar Jaleel Mahmood,Salim Rezvani,Robin Chhabra",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05547",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05547",
            "arxiv_html_link": "https://arxiv.org/html/2510.05547v1",
            "abstract": "We present ARRC (advanced reasoning robot control), a practical system that connects natural language instructions to safe, local robotic control by combining Retrieval-Augmented Generation (RAG) with RGB–D perception and guarded execution on an affordable robot arm. The system indexes curated robot knowledge (movement patterns, task templates, and safety heuristics) in a vector database, retrieves task-relevant context for each instruction, and conditions a large language model (LLM) to synthesize JSON-structured action plans. These plans are executed on a UFactory xArm 850 fitted with a Dynamixel-driven parallel gripper and an Intel RealSense D435 camera. Perception uses AprilTags detections fused with depth to produce object-centric metric poses; execution is enforced via a set of software safety gates (workspace bounds, speed/force caps, timeouts, and bounded retries). We describe the architecture, knowledge design, integration choices, and a reproducible evaluation protocol for tabletop scan/approach/pick–place tasks. Experimental results are reported to demonstrate efficacy of the proposed approach. Our design shows that RAG-based planning can substantially improve plan validity and adaptability while keeping perception and low-level control local to the robot.",
            "introduction": "Robotic manipulation is inherently dual in nature: it requires both high-level reasoning about objects and tasks, and precise low-level execution under physical constraints. Classical robotic systems are strong in motion planning and safety but brittle when adapting to new tasks or linguistic instructions. In contrast, vision-language-action (VLA) models such as RT-1 [1], PaLM-E [2], and RT-2 [3] excel in generalization and zero-shot task understanding, yet they frequently propose unsafe or infeasible plans due to limited embodiment grounding and lack of procedural rule enforcement.\n\nSeveral works have sought to address these limitations. SayCan [4] integrates affordance estimators with language models to filter infeasible actions, while CLIPort [5] fuses semantic embeddings with transport operators for spatially grounded pick-and-place. VIMA [6] leverages multimodal prompts to generalize across manipulation tasks with few demonstrations. Although these approaches improve task grounding, they remain constrained by their training distributions and cannot readily incorporate new procedural or safety knowledge without retraining.\n\nRetrieval-augmented generation (RAG), originally proposed for natural language processing (NLP) [7], enables dynamic grounding of model outputs in external, updatable knowledge sources. Recently, robotics researchers have begun exploring retrieval-based compliance frameworks. SayComply [8], for instance, uses retrieval to enforce adherence to operational manuals in field robotics, demonstrating improved compliance and correctness. However, such approaches often stop at high-level compliance and do not integrate low-level metric grounding, perception-driven plan validation, or safety gating for execution.\n\nIn this paper, we bridge these gaps by introducing a RAG-enabled robotic manipulation pipeline–called ARRC (advanced reasoning robot control)–that unifies perception, retrieval, and safe plan execution. We deploy this system on a UFactory xArm 850 equipped with a RealSense D435 and a Dynamixel gripper, integrating retrieval of robot-centric safety heuristics and procedural templates at inference time. This design offers both adaptability and reliability, enabling the injection of new task knowledge or safety rules without retraining. Our contributions are as follows:\n\nWe develop a hybrid VLA architecture integrating RAG for dynamic injection of procedural and safety knowledge.\n\nWe demonstrate a real-world implementation with RGB-D perception, JSON-structured plan generation, and strict execution safety gates.\n\nWe propose a reproducible ablation protocol to quantify contributions of retrieval, vision gating, and safety checks.\n\n1. We develop a hybrid VLA architecture integrating RAG for dynamic injection of procedural and safety knowledge.\n\n2. We demonstrate a real-world implementation with RGB-D perception, JSON-structured plan generation, and strict execution safety gates.\n\n3. We propose a reproducible ablation protocol to quantify contributions of retrieval, vision gating, and safety checks.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有机器人系统在适应新任务或语言指令时表现脆弱。  \n2. 现有的VLA模型在生成计划时缺乏安全性和可行性。  \n3. 需要一种方法来动态整合程序和安全知识以提高机器人操作的可靠性。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. SayCan和CLIPort等方法通过引入额外的知识来改善任务基础，但仍需重新训练以纳入新知识。  \n2. 现有的检索合规系统如SayComply仅在高层次上执行合规，缺乏低层次的度量感知和执行安全性。  \n\n【提出了什么创新的方法】  \n我们提出了一种名为ARRC的RAG驱动的机器人操作管道，结合了感知、检索和安全执行。该系统在UFactory xArm 850上实现，利用RGB-D感知生成JSON结构的行动计划，并通过严格的安全门进行执行。我们的设计使得在不重新训练的情况下，能够动态注入新的任务知识或安全规则。实验结果表明，该方法在计划有效性和适应性方面显著提升，同时确保了局部感知和低层控制的安全性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation",
            "authors": "Mahboubeh Zarei,Robin Chhabra,Farrokh Janabi-Sharifi",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05536",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05536",
            "arxiv_html_link": "https://arxiv.org/html/2510.05536v1",
            "abstract": "Accurate pose and velocity estimation is essential for effective spatial task planning in robotic manipulators. While centralized sensor fusion has traditionally been used to improve pose estimation accuracy, this paper presents a novel decentralized fusion approach to estimate both pose and velocity. We use dual-view measurements from an eye-in-hand and an eye-to-hand vision sensor configuration mounted on a manipulator to track a target object whose motion is modeled as random walk (stochastic acceleration model). The robot runs two independent adaptive extended Kalman filters formulated on a matrix Lie group, developed as part of this work. These filters predict poses and velocities on the manifold 𝕊​𝔼​(3)×ℝ3×ℝ3\\mathbb{SE}(3)\\times\\mathbb{R}^{3}\\times\\mathbb{R}^{3} and update the state on the manifold 𝕊​𝔼​(3)\\mathbb{SE}(3). The final fused state comprising the fused pose and velocities of the target is obtained using a correlation-aware fusion rule on Lie groups. The proposed method is evaluated on a UFactory xArm 850 equipped with Intel RealSense cameras, tracking a moving target. Experimental results validate the effectiveness and robustness of the proposed decentralized dual-view estimation framework, showing consistent improvements over state-of-the-art methods.",
            "introduction": "Position-Based Visual Servoing (PBVS) for robotic manipulators refers to the problem of controlling a robot’s end-effector by minimizing the error between its current and desired pose (i.e., position and orientation) relative to a target object using vision-based data obtained from a set of cameras [1]. Compared to Image-Based Visual Servoing (IBVS) that regulates the error signal in image space, PBVS offers simpler 3-D control laws, greater robustness, provided that calibration and pose estimation are accurate, and easier integration with spatial tasks like path planning and collision avoidance [2]. A comprehensive comparison between PBVS and IBVS is presented in [3], with a focus on system stability, robustness, sensitivity, and dynamic performance in both 3-D and image spaces. The first step in PBVS is to accurately estimate the pose of the target object relative to a pre-defined reference coordinate frame.\nThe classical vision-based approach to 6-DoF pose estimation typically involves extracting local features from the input image, matching them to those of a known 3-D model, and estimating the pose using algorithms such as Perspective-n-Point (PnP), RANSAC-based alignment, or Hough voting. While effective in controlled scenarios, these methods often struggle in real-world environments due to their sensitivity to occlusion, background clutter, lighting variations, and their reliance on carefully designed hand-crafted features. In recent years, significant progress has been made with deep learning and data-driven approaches, which address many of these limitations by learning robust features and pose representations directly from data [4, 5].\n\nAn important consideration to accurately obtain the pose is the configuration of the vision sensors in the robot work space. In the context of robotic manipulators, cameras are typically arranged in one of three primary configurations: (i) eye-in-hand, where the cameras are rigidly mounted on the robot arm or end-effector and move with it [6, 7, 8, 9, 10]; (ii) eye-to-hand, where the cameras are fixed in the environment or on the robot base, allowing them to observe the target object and part or all of the manipulator from a stationary viewpoint [11, 12, 13]; and (iii) hybrid configurations, which combine both eye-in-hand and eye-to-hand setups to leverage the complementary strengths of each [14, 15, 16]. These systems can dynamically switch between camera sources or fuse information from both, improving overall accuracy, robustness, flexibility, and coverage. Several hybrid approaches have been proposed to exploit the advantages of multiple visual perspectives for real-time pose estimation and control. In [17], a virtual visual servoing data fusion technique is introduced to estimate object pose using a set of eye-to-hand and eye-in-hand cameras. Similarly,[18] presents a PBVS method that employs a hybrid eye-in-hand/eye-to-hand configuration, using an extended Kalman filter and an occlusion-aware feature selection algorithm to ensure accurate and efficient pose estimation. A more recent hybrid approach in[19] addresses the challenges of tracking a dynamically moving target by combining four eye-to-hand RGB-D sensors with an eye-in-hand stereo camera. A supervisory module dynamically switches between the two configurations for the pose estimation task based on the robot–target distance and visibility conditions. Although previous methods have explored sensor-level data fusion and switching mechanisms for pose estimation between eye-in-hand and eye-to-hand configurations, the challenge of consistently fusing pose estimates from multiple cameras in dynamic robotic manipulation remains unaddressed.\n\nTo address these shortcomings, we propose a hybrid eye-to-hand/eye-in-hand robotic system in which each camera independently estimates the pose of a moving target using an adaptive Extended Kalman Filter (EKF) on Lie groups. The resulting estimates, expressed in the matrix Lie group 𝕊​𝔼​(3)×ℝ3×ℝ3\\mathbb{SE}(3)\\times\\mathbb{R}^{3}\\times\\mathbb{R}^{3}, are then fused using a novel pose/velocity-level fusion strategy specifically designed to handle stochastic poses [20]. Therefore, the main contributions of this paper are as follows:\n\nWe develop a correlation-aware decentralized fusion framework for dual-view target state estimation.\n\nThe estimation is propagated on the novel matrix Lie group 𝕊​𝔼​(3)×ℝ3×ℝ3\\mathbb{SE}(3)\\times\\mathbb{R}^{3}\\times\\mathbb{R}^{3}, enabling joint inference of target pose and velocity using pose measurements on 𝕊​𝔼​(3)\\mathbb{SE}(3), as an alternative to existing approaches that rely solely on sensor-level fusion [17, 18] or switching strategies [19].\n\nBuilding on the filter proposed in [21], we develop a new adaptive Lie group-based EKF to enhance robustness against measurement noise and unexpected uncertainties in the target object and robot’s motion.\n\n1. We develop a correlation-aware decentralized fusion framework for dual-view target state estimation.\n\n2. The estimation is propagated on the novel matrix Lie group 𝕊​𝔼​(3)×ℝ3×ℝ3\\mathbb{SE}(3)\\times\\mathbb{R}^{3}\\times\\mathbb{R}^{3}, enabling joint inference of target pose and velocity using pose measurements on 𝕊​𝔼​(3)\\mathbb{SE}(3), as an alternative to existing approaches that rely solely on sensor-level fusion [17, 18] or switching strategies [19].\n\n3. Building on the filter proposed in [21], we develop a new adaptive Lie group-based EKF to enhance robustness against measurement noise and unexpected uncertainties in the target object and robot’s motion.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要提高动态机器人操作中的位姿和速度估计的准确性。  \n2. 现有的集中式传感器融合方法在动态环境中表现不佳。  \n3. 需要有效的传感器配置以应对目标物体运动的不确定性。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统方法依赖于集中式传感器融合，无法有效处理动态场景。  \n2. 现有的混合传感器配置方法未能解决动态机器人操作中的一致性融合问题。  \n\n【提出了什么创新的方法】  \n我们提出了一种新的去中心化融合框架，利用双视角传感器独立估计目标状态，采用自适应扩展卡尔曼滤波器在矩阵李群上进行估计。该方法通过关联感知融合规则实现位姿和速度的联合推断，增强了对测量噪声和不确定性的鲁棒性。实验结果表明，该方法在动态目标跟踪中优于现有技术，展示了更高的准确性和稳定性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "AD-NODE: Adaptive Dynamics Learning with Neural ODEs for Mobile Robots Control",
            "authors": "Shao-Yi Yu,Jen-Wei Wang,Maya Horii,Vikas Garg,Tarek Zohdi",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05443",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05443",
            "arxiv_html_link": "https://arxiv.org/html/2510.05443v1",
            "abstract": "Mobile robots, such as ground vehicles and quadrotors, are becoming increasingly important in various fields, from logistics to agriculture, where they automate processes in environments that are difficult to access for humans. However, to perform effectively in uncertain environments using model-based controllers, these systems require dynamics models capable of responding to environmental variations, especially when direct access to environmental information is limited.\nTo enable such adaptivity and facilitate integration with model predictive control, we propose an adaptive dynamics model which bypasses the need for direct environmental knowledge by inferring operational environments from state-action history. The dynamics model is based on neural ordinary equations, and a two-phase training procedure is used to learn latent environment representations. We demonstrate the effectiveness of our approach through goal-reaching and path-tracking tasks on three robotic platforms of increasing complexity: a 2D differential wheeled robot with changing wheel contact conditions, a 3D quadrotor in variational wind fields, and the Sphero BOLT robot under two contact conditions for real-world deployment. Empirical results corroborate that our method can handle temporally and spatially varying environmental changes in both simulation and real-world systems.",
            "introduction": "Mobile robots such as ground vehicles and quadrotors are increasingly used across applications, from navigating warehouse floors in logistics to large-scale crop monitoring in agriculture (Li et al., 2024; Duggal et al., 2016). These systems provide access to environments difficult to access for humans, enabling greater operational scale and improved efficiency. Mobile robots may encounter various types of environments within a single operation and generally lack prior knowledge of the conditions they will face, making real-time adaptability essential during deployment. To operate robustly in such uncertain environments, mobile robots require adaptive control strategies that can respond to environmental variations such as terrain types, wind conditions, or payload changes. However, achieving such adaptability remains challenging for model-based controllers as they rely on accurate dynamics models for control action planning over long horizons (Seo et al., 2020; Nagabandi et al., 2018). Furthermore, many environmental variations cannot be fully detected using onboard sensors, making it important for the system to infer hidden environmental factors from limited data and adapt its dynamics accordingly.\n\nPrevious efforts in in-context reinforcement learning (RL) have led to major advances in adapting to different environments based on past trajectories (Liang et al., 2023; Zhang et al., 2025; Belkhale et al., 2021). A line of research in adaptive model-free RL proposes specially designed adaptive modules, known as Rapid Motor Adaptation (RMA), to encode environmental information in RL policy (Kumar et al., 2021; Zhang et al., 2023; Qi et al., 2023). However, these model-free RL approaches still require significant pre-training or extensive exploration data to generalize across a variety of tasks. Several model-based RL approaches have been proposed (Seo et al., 2020; Lee et al., 2020; Evans et al., 2022); however, they often model dynamics over a predefined discrete time domain, which overlooks the continuously-evolving dynamics of rigid-body robotic systems (Greydanus et al., 2019). Since the dynamics of these systems are typically governed by ordinary differential equations (ODEs), neural ordinary differential equations (NODE) (Chen et al., 2018), which learn first-order derivatives and compute system states using numerical integrators, are well-suited for modeling continuous-time dynamics. The approach of modeling derivatives has also shown success in time-series prediction tasks across various domains (Lipman et al., 2022; Zhang et al., 2024a; Cranmer et al., 2020). In robotics, learning dynamics with NODE has demonstrated robustness to noisy and irregular data in standard RL tasks (Yildiz et al., 2021). However, the effectiveness of continuous-time models for capturing adaptive dynamics under drastic environmental changes remains an open question.\nIn this work, we propose AD-NODE, an adaptive dynamics model for mobile robots that combines NODE with an adaptive module in the style of RMA to infer environmental conditions from past state-action history. We use a two-phase training framework: in Phase 1, the model focuses on learning the mapping between states, with complete environmental information (referred to as \"privileged information\") included in the training data. In Phase 2, the model learns to reconstruct the environment from historical data during execution. The proposed adaptive dynamics model is used with model predictive control (MPC) (Morari and Lee, 1999; Garg et al., 2013; Chua et al., 2018) to determine the optimal actions for accomplishing navigation tasks on two simulated mobile robotic platforms: a 2D differential wheeled robot navigating surfaces with different textures, and a 3D quadrotor flying through different wind fields. Given the limited availability of models that are both adaptive and continuous, and with the goal of enabling adaptability in mobile robots across varying environments, we select a classic context-aware dynamics model (CaDM) (Lee et al., 2020) as our primary comparison baseline. Figure 1 demonstrates that our proposed model has superior performance in both goal-reaching and path-tracking tasks across both simulated platforms. Furthermore, the model we design can be deployed in a real-world environment where a Sphero BOLT robot navigates across two distinct textures.\n\nWe propose learning a continuous-time adaptive dynamics model with NODE (AD-NODE) for mobile robotic systems that can adapt to the environment during operation. Specifically:\n\nWe propose a novel framework that incorporates adaptability into continuously-evolving dynamics for long-horizon rollouts in MPC.\n\nWe propose a novel framework that incorporates adaptability into continuously-evolving dynamics for long-horizon rollouts in MPC.\n\nWe empirically show that our framework achieves higher accuracy compared to the baselines in both goal-reaching and path-tracking tasks for differential wheeled robot and quadrotor navigation platforms.\n\nWe validate the feasibility of AD-NODE beyond simulation by deploying it on a Sphero BOLT robot across surfaces with different friction, demonstrating adaptability and repeatability under hardware uncertainty.\n\n1. We propose a novel framework that incorporates adaptability into continuously-evolving dynamics for long-horizon rollouts in MPC.\n\n2. We empirically show that our framework achieves higher accuracy compared to the baselines in both goal-reaching and path-tracking tasks for differential wheeled robot and quadrotor navigation platforms.\n\n3. We validate the feasibility of AD-NODE beyond simulation by deploying it on a Sphero BOLT robot across surfaces with different friction, demonstrating adaptability and repeatability under hardware uncertainty.",
            "llm_summary": "【论文的motivation是什么】  \n1. 移动机器人在不确定环境中需要适应性控制策略以应对环境变化。  \n2. 现有模型基控制方法难以处理动态变化的环境。  \n3. 需要一种能够从有限数据中推断环境条件的动态模型。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 以往的强化学习方法在适应不同环境方面取得了一定进展，但仍需大量预训练数据。  \n2. 现有的模型基强化学习方法通常在预定义的离散时间域上建模，忽视了刚体机器人系统的连续动态。  \n3. 现有的连续时间模型在捕捉适应性动态方面的有效性仍然是一个开放问题。  \n\n【提出了什么创新的方法】  \n我们提出了AD-NODE，一个结合了神经常微分方程（NODE）和适应模块的自适应动态模型。该模型通过两阶段训练框架来推断环境条件：第一阶段使用完整的环境信息进行状态映射学习，第二阶段则从历史数据中重建环境。该模型与模型预测控制（MPC）结合，优化移动机器人在不同环境中的导航任务。实验证明，AD-NODE在目标到达和路径跟踪任务中表现优于基线模型，并在真实环境中成功部署，展示了其适应性和重复性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Active Semantic Perception",
            "authors": "Huayi Tang,Pratik Chaudhari",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05430",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05430",
            "arxiv_html_link": "https://arxiv.org/html/2510.05430v1",
            "abstract": "We develop an approach for active semantic perception which refers to using the semantics of the scene for tasks such as exploration.\nWe build a compact, hierarchical multi-layer scene graph that can represent large, complex indoor environments at various levels of abstraction, e.g., nodes corresponding to rooms, objects, walls, windows etc. as well as fine-grained details of their geometry.\nWe develop a procedure based on large language models (LLMs) to sample plausible scene graphs of unobserved regions that are consistent with partial observations of the scene.\nThese samples are used to compute an information gain of a potential waypoint for sophisticated spatial reasoning, e.g., the two doors in the living room can lead to either a kitchen or a bedroom.\nWe evaluate this approach in complex, realistic 3D indoor environments in simulation.\nWe show using qualitative and quantitative experiments that our approach can pin down the semantics of the environment quicker and more accurately than baseline approaches. The source code and additional information can be found on https://github.com/grasp-lyrl/active_semantic_perception.",
            "introduction": "Suppose you are trying to quickly find milk in a new convenience store.\nCompleting this task requires observing the structure of the environment, e.g., where aisles are located, what objects they contain, etc.\nIt also requires some guesswork as to where the cold storage could be (usually at the back of the store).\nFinally, it requires the knowledge that if there is no cold storage in sight, it is sensible to give up on finding milk altogether.\nPlenty of tasks have this flavor, from mundane ones such as finding milk, to critical ones such as a firefighter rescuing a person from a building.\nHumans are spectacular at these tasks.\nRobots cannot perform these kinds of tasks.\nThis is not due to the lack of the ability to sense or to move.\nRobot sensing and mobility are extremely good today—if not as good as that of humans yet.\n\nPerhaps the key distinguishing factor is that humans and robots build very different representations. While robots represent their environment in highly detailed ways in terms of its geometric or photometric properties, human representations are much richer in their semantics.\nThis is why, in spite of often having much poorer fine-grained representations—most of us would be hard-pressed to remember the size of our living room—humans can perform these canonical search tasks much better.\nThis is perhaps how humans can perform “guesswork” and use “knowledge” in the example above.\n\nHow should robots build semantic representations of their environment?\nThe Merriam-Webster dictionary defines:\n\nActive perception [5] requires a robot to (a) maintain a representation of the scene that summarizes past observations and update it to incorporate new observations (a map), (b) synthesize new observations of the scene (a generative model), and (c) to select controls that maximize an information gain (planning) [6].\nThese three tenets motivate our approach described in Fig. 1.\nFirst, we argue that a scene graph, e.g., [7, 4], is well-suited for active semantic perception.\nWe build a compact, hierarchical multi-layer scene graph that captures rooms, objects, negative space, and structural boundaries such as walls, doors, and windows.\nWe show that it can represent large, complex environments at various levels of abstraction, e.g., objects inside a room and rooms within a house.\nThe hierarchy enables richer semantics that allows complex inferences for active perception.\nAt the same time, a scene graph can also represent fine-grained details of the geometry and photometry for navigation.\nSecond, we develop a procedure based on large language models (LLMs) to synthesize a new plausible scene graph of unobserved regions that is consistent with partial observations of a scene graph and contains semantically interesting regions, e.g., synthesizing a bedroom and a kitchen after observing the living room.\nThese scene graphs are often metric-accurate, e.g., the coffee table is towards the center of the living room.\nThis generative model allows us to compute an information gain of a potential waypoint which reflects the reduction in the semantic uncertainty of the scene graph for sophisticated spatial reasoning, e.g., the two doors in the living room can lead to either a kitchen or a bedroom.\nThird, we use a hierarchical planning framework to compute high-level waypoints on the scene graph that correspond to low-level collision-free trajectories that can be executed by the robot.\n\nWe evaluate our approach in simulation, in multiple complex 3D indoor environments, and show that it can recover scene semantics faster and more accurately than baseline methods.\nWhile hypothesized scene graphs need not necessarily match the true environment, our approach generates plausible predictions that, when aligned with the true scene, enable more efficient retrieval tasks.\nOur approach can run on a standard GPU-enabled laptop.",
            "llm_summary": "【论文的motivation是什么】  \n1. 如何让机器人在复杂环境中快速、准确地进行任务，如探索和物品寻找。  \n2. 机器人需要建立更丰富的语义表示，以提升其环境理解能力。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统方法侧重于几何或光度特性，缺乏语义信息。  \n2. 现有的场景表示方法未能有效结合语义推理与导航能力。  \n\n【提出了什么创新的方法】  \n我们提出了一种基于层次化多层场景图的主动语义感知方法，利用大型语言模型生成与部分观察一致的场景图样本。该方法通过计算潜在路径的信息增益，优化机器人在复杂环境中的导航与决策。实验结果表明，该方法在复杂3D室内环境中能够更快、更准确地恢复场景语义，相比基线方法显著提升了任务执行效率。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Towards Online Robot Interaction Adaptation to Human Upper-limb Mobility Impairments in Return-to-Work Scenarios",
            "authors": "Marta Lagomarsino,Francesco Tassi",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05425",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05425",
            "arxiv_html_link": "https://arxiv.org/html/2510.05425v1",
            "abstract": "Work environments are often inadequate and lack inclusivity for individuals with upper-body disabilities.\nThis paper presents a novel online framework for adaptive human-robot interaction (HRI) that accommodates users’ arm mobility impairments, ultimately aiming to promote active work participation. Unlike traditional human-robot collaboration approaches that assume able-bodied users, our method integrates a mobility model for specific joint limitations into a hierarchical optimal controller. This allows the robot to generate reactive, mobility-aware behaviour online and guides the user’s impaired limb to exploit residual functional mobility. The framework was tested in handover tasks involving different upper-limb mobility impairments (i.e., emulated elbow and shoulder arthritis, and wrist blockage), under both standing and seated configurations with task constraints using a mobile manipulator, and complemented by quantitative and qualitative comparisons with state-of-the-art ergonomic HRI approaches. Preliminary results indicated that the framework can personalise the interaction to fit within the user’s impaired range of motion and encourage joint usage based on the severity of their functional limitations.",
            "introduction": "More than one billion people worldwide live with a disability, with nearly 200 million experiencing significant difficulties in daily functioning. Although laws support disabled employment, only 27% of working-age disabled individuals are employed, compared to 56% of non-disabled individuals [1]. Employed individuals with upper-body limitations are frequently assigned low-value tasks, highlighting persistent labour market barriers. This exclusion reduces independence, social participation, and quality of life, in addition to causing economic losses. Despite the incredible advances in rehabilitation, including AI-driven physical therapy and robotic-assisted strategies, complete restoration of pre-impairment functionality is often difficult. With an ageing workforce and rising work-related health conditions, addressing disability employment is more urgent than ever.\n\nPhysically assistive robots offer a valuable opportunity to promote accessibility and independence in the workplace for individuals with upper-body mobility impairments [2]. Assistive exoskeletons, for example, have shown great potential in compensating for arm muscular weakness and enabling users to perform daily activities [3]. However, they also present notable limitations, including discomfort from additional weight, operational pressures that may cause irritation in long-term usage, and restrictions in natural movement, mobility, and postural balance. An alternative approach involves teleoperated robotic systems, where a user remotely controls an external robot or manipulator for assistance in pick-and-place tasks [4]. While effective in certain scenarios, this method does not exploit residual user upper-limb mobility and may inadvertently contribute to physical inactivity, further reducing motor function over time.\n\nBeyond wearable and teleoperated robotic assistance, adapting the work environment itself can enhance functional interactions and improve accessibility for impaired users. In this context, manipulator and mobile collaborative robots (CoBots) constitute a promising solution [2], as they can online adapt to facilitate interaction. CoBots have already demonstrated their ability to improve physical ergonomics by dynamically adjusting interaction poses [6]\nand optimising shared kinodynamics in co-manipulation tasks [7]. Additionally, they can promote cognitive ergonomics by adjusting proximity and responsiveness to balance safety, user comfort, and productivity [8].\nHowever, most existing ergonomic approaches, despite personalising human models based on physical characteristics such as body segment lengths, assume that human workers interacting with robots are fully able-bodied, making them ineffective for users with varying mobility limitations [9], as illustrated in Fig. 1. The integration of mobility limitations into the robot’s optimisation problem can promote user comfort and better adapt interactions to the individual’s abilities and needs.\n\nThe limited existing studies in human-robot collaboration that address upper-limb impairments focused on fully replacing the functionality of the impaired arm with a collaborative robotic system [10] or minimising hand movement in Cartesian space [11]. These approaches overlook the complexity of upper-limb mobility impairments, particularly joint-level constraints.\nAdditionally, compensating for human joint impairments by having the robot move more [12] or attempting to find an interaction point at the human hand pose [13] is not always feasible due to task or safety constraints. Most importantly, these strategies risk exacerbating the phenomenon of “learned non-use”, where the impaired arm is gradually ignored, worsening the condition and fostering dependency and frustration, rather than encouraging job inclusion, active engagement, and autonomy.\n\nThis paper presents the primary yet significant steps towards a framework for online robot adaptation to human upper-limb impairments in interactive tasks (e.g. handovers, drilling, or polishing), aiming to promote active work participation by exploiting residual mobility. We propose a subject-specific mobility model that accounts for individual full or partial joint impairments, body characteristics, and natural redundancies.\nWe then formulate an Augmented Hierarchical Quadratic Programming (AHQP) problem to simultaneously integrate the user mobility model, their initial configuration, robot limitations, and specific task requirements as compatible constraints and objectives. Since the state space includes the joint and Cartesian velocities of both human and robot, the optimisation determines not only the final interaction pose but also the Cartesian reference trajectory during the approach phase for both the robot and the human. Additionally, it generates optimal joint-space trajectories that ensure these references are followed while respecting the joint- and task-level constraints of the human-robot system.\n\nIn summary, the main features and contributions of this work are:\n\nOur framework models full single-joint and partial multi-joint upper-limb impairments and adapts the human-robot interaction (HRI) position and orientation accordingly, considering the initial human configuration and exploiting residual mobility.\n\nBy treating the human-robot system as a coupled chain in the optimisation, we ensure compliance with joint- and task-level constraints in the interaction approach phase and promote impaired arm movement based on joint severity, helping to counteract “learned non-use”.\n\nWe compare the proposed framework with state-of-the-art approaches through multi-subject handover experiments with emulated impairments, evaluating its performance using quantitative and qualitative metrics.",
            "llm_summary": "【论文的motivation是什么】  \n1. 提高残疾人士在工作场所的参与度和独立性。  \n2. 解决现有机器人交互方法对上肢运动障碍用户的适应性不足。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有方法多假设用户为健全人，未考虑不同的运动限制。  \n2. 以往研究主要集中于替代受损手臂功能，忽视了上肢运动障碍的复杂性。  \n\n【提出了什么创新的方法】  \n本研究提出了一种在线适应框架，通过构建个性化的运动模型，结合层次化的优化控制器，实时调整机器人与用户的交互方式。该方法能够根据用户的运动能力生成反应式行为，鼓励残余功能的使用。实验结果表明，该框架能有效个性化交互，适应用户的运动范围，并促进关节的使用，帮助用户克服“学习性非使用”现象，提升工作参与感。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "A multi-modal tactile fingertip design for robotic hands to enhance dexterous manipulation",
            "authors": "Zhuowei Xu,Zilin Si,Kevin Zhang,Oliver Kroemer,Zeynep Temel",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05382",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05382",
            "arxiv_html_link": "https://arxiv.org/html/2510.05382v1",
            "abstract": "Tactile sensing holds great promise for enhancing manipulation precision and versatility, but its adoption in robotic hands remains limited due to high sensor costs, manufacturing and integration challenges, and difficulties in extracting expressive and reliable information from signals.\nIn this work, we present a low-cost, easy-to-make, adaptable, and compact fingertip design for robotic hands that integrates multi-modal tactile sensors. We use strain gauge sensors to capture static forces and a contact microphone sensor to measure high-frequency vibrations during contact. These tactile sensors are integrated into a compact design with a minimal sensor footprint, and all sensors are internal to the fingertip and therefore not susceptible to direct wear and tear from interactions. From sensor characterization, we show that strain gauge sensors provide repeatable 2D planar force measurements in the 0–5 N range and the contact microphone sensor has the capability to distinguish contact material properties. We apply our design to three dexterous manipulation tasks that range from zero to full visual occlusion. Given the expressiveness and reliability of tactile sensor readings, we show that different tactile sensing modalities can be used flexibly in different stages of manipulation, solely or together with visual observations to achieve improved task performance. For instance, we can precisely count and unstack a desired number of paper cups from a stack with 100% success rate which is hard to achieve with vision only. More details and videos can be found in https://sites.google.com/view/tactilefingertip.",
            "introduction": "Robotic manipulation often relies on vision to perceive environmental properties, estimate object poses, and close robot control loops [3, 4]. However, performance degrades when contact regions are occluded by robot manipulators or hidden by clutter [5]. Contact-rich manipulation with robotic hands, such as in-hand object reposing, subtle grasp adjustment, and tool use, frequently exhibit these conditions [6, 7, 8], given the larger enclosure on objects with the multi-finger setting. This limits the effectiveness of robot hand control based solely on vision. Humans compensate for visual occlusion with tactile cues from their hands, where various mechanoreceptors are used to sense pressures, detect slip, and reveal properties of the contact surface [9, 10]. This motivates the deployment of tactile sensing in robotic hands [11].\n\nDespite the development of a variety of tactile sensors [12, 13, 14, 15, 16, 17, 18, 19], developing and deploying tactile sensors for robotic hands presents unique challenges. Robotic hands demand tactile sensors that are compact enough to integrate into fingers without restricting hand’s workspace or mobility. The nature of contact-rich manipulation, where dexterous hands repeatedly make, break, and switch contacts, requires stable and reliable sensor measurements. Inspired by the richness of human touch, multi-modal tactile sensing can expand hand’s versatility by capturing nuanced contact information such as forces, slippage, and textures. In addition, sensor signals must be expressive and interpretable, enabling effective control of high-DoF robotic hand systems and seamless integration with visual perception.\n\nIn this work, we present a multi-modal tactile fingertip that integrates complementary sensing in a compact, hand-compatible design (Fig. 1(a)). We embed strain gauge sensors and a contact microphone sensor in the fingertip structure to capture both force and vibration during contact, reflecting the two types of mechanoreceptors of human touch. All sensors are internal to the fingertip and therefore prevent direct wear and tear from interactions. Our design is compact (1.9​cm×1.9​cm×2.7​cm1.9\\,\\mathrm{cm}\\times 1.9\\,\\mathrm{cm}\\times 2.7\\,\\mathrm{cm}), cost-effective (under $100), and easy to fabricate (within one hour). Through sensor characterization, we demonstrate that the tactile fingertip can reliably predict 2D planar force and distinguish contact surface properties. By equipping four tactile fingertips on a DeltaHand [2] (Fig. 1(c)), we demonstrate the ease of integration with multi-finger robotic hands and their applications in robotic manipulation.\n\nThe main contributions of this work are: (i) a multi-modal tactile fingertip design that integrates force and vibration sensing in a compact form factor, suitable for multi-finger robotic hands; (ii) a systematic characterization on tactile sensors showing that our design provides expressive and reliable sensor measurements; and (iii) evaluation across three manipulation tasks with different levels of visual occlusion to show how different tactile modalities can be flexibly used to mitigate visual occlusion. Experimental results show that our design delivers stable and consistent force measurements that can be directly incorporated into hand control to achieve 100 % success on fragile object pinching (Fig. 1 (d) (e)). Through ablation studies on tasks of unstacking cups and identifying hidden object properties via active touch, we show that vibrotactile signals offer more nuanced and direct contact information which can be easily used to improve task performance with light-weighted data processing and learning when visual cues fall short.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人手的操控性能在视觉被遮挡时显著下降。  \n2. 现有的触觉传感器在集成和成本方面存在挑战。  \n3. 需要一种低成本且易于制造的触觉传感器设计，以增强机器人的灵活性和操控能力。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有的多模态触觉传感器通常体积庞大且易受磨损，限制了其在机器人手中的应用。  \n2. 单模态传感器虽然在敏感性和稳健性上表现良好，但缺乏综合的触觉信息。  \n\n【提出了什么创新的方法】  \n我们提出了一种集成了应变计和接触麦克风传感器的多模态触觉指尖设计，能够在紧凑的形式中捕捉静态力和高频振动。该设计不仅成本低（低于100美元），而且易于制造（在一小时内完成），并且所有传感器均内置于指尖，避免了直接磨损。通过对传感器的表征，我们展示了该触觉指尖能够可靠地预测2D平面力并区分接触表面属性。在三个不同视觉遮挡级别的操控任务中，我们证明了不同的触觉感知方式可以灵活使用，以改善任务表现。例如，在从100个纸杯的堆叠中精确计数和分离所需数量时，成功率达到了100%，这是仅依赖视觉难以实现的。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Adaptive Dynamics Planning for Robot Navigation",
            "authors": "Lu Yuanjie,Mao Mingyang,Xu Tong,Wang Linji,Lin Xiaomin,Xiao Xuesu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05330",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05330",
            "arxiv_html_link": "https://arxiv.org/html/2510.05330v1",
            "abstract": "Autonomous robot navigation systems often rely on hierarchical planning, where global planners compute collision-free paths without considering dynamics, and local planners enforce dynamics constraints to produce executable commands. This discontinuity in dynamics often leads to trajectory tracking failure in highly constrained environments.\nRecent approaches integrate dynamics within the entire planning process by gradually decreasing its fidelity, e.g., increasing integration steps and reducing collision checking resolution, for real-time planning efficiency. However, they assume that the fidelity of the dynamics should decrease according to a manually designed scheme. Such static settings fail to adapt to environmental complexity variations, resulting in computational overhead in simple environments or insufficient dynamics consideration in obstacle-rich scenarios.\nTo overcome this limitation, we propose Adaptive Dynamics Planning (ADP), a learning-augmented paradigm that uses reinforcement learning to dynamically adjust robot dynamics properties, enabling planners to adapt across diverse environments. We integrate ADP into three different planners and further design a standalone ADP-based navigation system, benchmarking them against other baselines. Experiments in both simulation and real-world tests show that ADP consistently improves navigation success, safety, and efficiency.",
            "introduction": "Autonomous robot navigation is critical for robots to operate safely and efficiently in real-world environments. Planning collision-free trajectories while respecting dynamics constraints is particularly challenging in highly constrained environments, ranging from narrow indoor corridors to densely populated outdoor spaces. With the expanding deployment of robots in domains such as last-mile delivery, healthcare assistance, and industrial inspection, navigation systems must deliver both robustness and efficiency across varying environmental complexities.\n\nTraditional navigation systems usually adopt a two-stage architecture that separates global path generation from local trajectory execution [2]. The global planning stage produces a geometrically feasible path over large configuration spaces by modeling the robot as a holonomic point mass, while the local planning stage manages real-time obstacle avoidance and motion execution under full dynamics constraints within limited horizons. This separation creates fundamental mismatches: global paths often ignore physical limits such as curvature and acceleration, forcing local planners to either heavily modify the path or fail in highly constrained environments [3].\n\nRecent research has attempted to address this mismatch by proposing more integrated approaches to dynamics modeling. For example, guided sampling-based motion planning with dynamics [4] aims to generate trajectories that strictly satisfy the dynamics constraints along the entire path. Although theoretically appealing, such methods are computationally prohibitive for real-time navigation, given onboard resource limitations and the need for rapid replanning in dynamic environments. In contrast, Decremental Dynamics Planning (DDP) [1] tackles this computational challenge by starting with high-fidelity dynamics modeling at the early stages of trajectory rollout and gradually reducing model fidelity, thereby striking a balance between dynamics fidelity and computational efficiency. However, DDP’s predetermined reduction schedules lack environmental awareness, resulting in computational overhead in simple environments where simplified models would suffice, or insufficient dynamics consideration in obstacle-rich environments that demand higher fidelity modeling.\n\nTo address these limitations, we propose Adaptive Dynamics Planning (ADP), a learning-augmented navigation paradigm that employs reinforcement learning to adapt dynamics modeling during the entire planning process. Unlike predetermined approaches like DDP that use fixed reduction of dynamics fidelity, ADP uses a learned agent as a meta-controller that dynamically adjusts dynamics model fidelity based on environmental observations. The agent learns to modulate various aspects of dynamics modeling—balancing computational efficiency with trajectory feasibility—while preserving the stability and interpretability of classical planning frameworks. This learned adaptation enables intelligent resource allocation that automatically matches modeling complexity to environmental demands.\nTo validate ADP’s feasibility and effectiveness, we integrate it with three classical planners and compare it against fixed fidelity reduction approaches based on DDP. Experiments in the simulated BARN environments [5] and real-world deployment show consistent improvements in navigation success, safety, and efficiency.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的机器人导航系统在动态约束下的路径规划存在分离问题，导致在复杂环境中轨迹跟踪失败。  \n2. 现有方法在动态建模中缺乏环境适应性，导致在简单环境中计算开销过大或在复杂环境中动态考虑不足。  \n3. 需要一种能够动态调整机器人动态属性的导航方法，以提高导航成功率、安全性和效率。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统方法将全局路径生成与局部轨迹执行分开，导致动态约束未得到有效考虑。  \n2. 现有的集成动态建模方法如DDP，虽然在计算效率上有所改善，但缺乏环境感知，无法动态调整建模复杂度。  \n\n【提出了什么创新的方法】  \n提出了自适应动态规划（ADP），通过强化学习动态调整机器人动态属性，适应不同环境。ADP作为元控制器，实时调整动态建模的保真度，平衡计算效率与轨迹可行性。该方法集成于三种经典规划器中，并与基于DDP的固定保真度减少方法进行比较。实验结果显示，ADP在模拟和现实环境中均显著提高了导航的成功率、安全性和效率。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing",
            "authors": "Yixiao Wang,Mingxiao Huo,Zhixuan Liang,Yushi Du,Lingfeng Sun,Haotian Lin,Jinghuan Shang,Chensheng Peng,Mohit Bansal,Mingyu Ding,Masayoshi Tomizuka",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05213",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05213",
            "arxiv_html_link": "https://arxiv.org/html/2510.05213v1",
            "abstract": "Pretrained vision foundation models (VFMs) advance robotic learning via rich visual representations, yet individual VFMs typically excel only in specific domains, limiting generality across tasks. Distilling multiple VFMs into a unified representation for policy can mitigate this limitation but often yields inflexible task-specific feature selection and requires costly full re-training to incorporate robot-domain knowledge.\nWe propose VER, a Vision Expert transformer for Robot learning. During pretraining, VER distills multiple VFMs into a vision expert library. It then fine-tunes only a lightweight routing network (fewer than 0.4% of parameters) to dynamically select task-relevant experts from the pretrained library for downstream robot tasks. We further introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve both flexibility and precision of dynamic expert selection. Moreover, VER supports parameter-efficient finetuning for scalable expert utilization and adaptive robot-domain knowledge integration.\nAcross 17 diverse robotic tasks and multiple policy heads, VER achieves state-of-the-art performance. We find that VER reduces large-norm outliers in task-irrelevant regions (e.g., background) and concentrates on task-critical regions. Visualizations and codes can be found in https://yixiaowang7.github.io/ver_page/.",
            "introduction": "Developing robotic systems capable of perceiving and interacting with complex, unstructured environments remains a fundamental challenge in embodied AI.\nRecently, visuomotor robot policy learning has emerged as a promising approach, enabling robots to directly map visual observations to control actions.\nPretrained vision foundation models (VFMs) such as DINOv2 (Oquab et al., 2024), CLIP (Radford et al., 2021), and ViT (Dosovitskiy et al., 2020),\nprovide transferable visual representations that support robotic perception and control with certain generalizability, improving\nthe scalability of robotic systems (Huang et al., 2024; Wan et al., 2024).\n\nHowever, executing even a single robotic task, and especially a diverse set of tasks, often requires multiple implicit visual competencies that a single VFM cannot fully capture. Directly integrating multiple VFMs for robot tasks increases computational and operational complexity.\nPrevious works (Ranzinger et al., 2024; Shang et al., 2024; Chen et al., 2025) distill diverse foundation models into a unified representation, but three key challenges remain.\nFirst, heterogeneous VFM features are often misaligned, so a unified representation tends to dilute or discard model-specific capabilities.\nSecond, the policy head must extract task-relevant information from the fused representation, which limits flexibility to leverage the most relevant VFMs across tasks and leads to suboptimal results.\nThird, existing distilled models typically require full re-training to incorporate robot-domain knowledge and it is hard to scale computation (down for simple tasks and up for complex tasks).\n\nTo address these limitations, we propose VER, a Vision Expert transformer for Robot learning via foundation distillation and dynamic routing.\nVER distills knowledge from multiple vision foundation models into a unified representation library and uses a dynamic routing mechanism to selectively activate the most relevant experts for robot policy learning.\nSpecifically, VER introduces a Mixture-of-Experts (MoE)-based Vision Expert Library (VEL), replacing traditional static vision transformer backbones with a collection of specialized experts, each capturing distinct aspects of visual understanding.\nThis design enables robots to selectively leverage the specialized experts best suited for task-aware policy learning.\n\nOur method operates in three stages as shown in Fig. 1.\nFirst, during pretraining, we distill knowledge from multiple VFMs into a vision expert library using Teacher-Specific Routers with mutual-information regularization. This covers a broad spectrum of visual knowledge while maintaining efficiency via sparse expert activation.\nSecond, in the robotic policy learning phase, we freeze all pretrained vision experts and fine-tune only a lightweight Robot Router that dynamically selects task-relevant experts, whose outputs are fed to a policy head to generate actions. To expand selection capacity across patches and layers, enhance exploration, and prevent premature convergence to suboptimal expert combinations, we employ Patchwise Expert Routing with Curriculum Top-K Annealing, leading to more robust policy learning.\nThird, we offer parameter-efficient fine-tuning strategies that scale expert utilization and facilitate the integration of robot-domain knowledge.\n\nAcross different types of policy heads, such as diffusion and flow matching policies (Chi et al., 2023b; Zhang & Gienger, 2024), extensive experiments on diverse robotic benchmarks show that VER achieves state-of-the-art performance. With Patchwise Expert Routing and Curriculum Top-K Annealing, VER suppresses high-norm background outliers and reduces information in task-irrelevant patches while preserving details in task-critical regions, yielding more compact and discriminative visual features and robust policy learning.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的视觉基础模型（VFMs）在特定领域表现优异，但缺乏跨任务的通用性。  \n2. 将多个VFMs蒸馏为统一表示的过程复杂且不灵活，难以适应机器人领域的知识。  \n3. 需要一种方法来动态选择与任务相关的专家，以提高机器人学习的灵活性和精度。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有研究通过知识蒸馏将多种基础模型融合，但通常产生静态表示，缺乏适应性。  \n2. 现有方法在特定任务中提取信息的灵活性不足，导致次优结果。  \n3. 现有蒸馏模型通常需要完全重新训练，难以有效整合机器人领域知识。  \n\n【提出了什么创新的方法】  \nVER（Vision Expert Transformer）通过三个阶段进行操作：  \n1. 在预训练阶段，VER将多个VFMs的知识蒸馏到一个视觉专家库中，使用特定路由器和互信息正则化来保持效率。  \n2. 在机器人策略学习阶段，VER冻结所有预训练的视觉专家，仅微调轻量级的机器人路由器，动态选择与任务相关的专家。  \n3. 采用Patchwise Expert Routing和Curriculum Top-K Annealing来增强选择能力，防止过早收敛到次优组合。  \n通过这些方法，VER在17个多样化的机器人任务上实现了最先进的性能，减少了任务无关区域的干扰，专注于任务关键区域，从而提高了策略学习的鲁棒性和有效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Dropping the D: RGB-D SLAM Without the Depth Sensor",
            "authors": "Mert Kiray,Alican Karaomer,Benjamin Busam",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.06216",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.06216",
            "arxiv_html_link": "https://arxiv.org/html/2510.06216v1",
            "abstract": "We present DropD-SLAM, a real-time monocular SLAM system that achieves RGB-D-level accuracy without relying on depth sensors. The system replaces active depth input with three pretrained vision modules: a monocular metric depth estimator, a learned keypoint detector, and an instance segmentation network. Dynamic objects are suppressed using dilated instance masks, while static keypoints are assigned predicted depth values and backprojected into 3D to form metrically scaled features. These are processed by an unmodified RGB-D SLAM back end for tracking and mapping. On the TUM RGB-D benchmark, DropD-SLAM attains 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences, matching or surpassing state-of-the-art RGB-D methods while operating at 22 FPS on a single GPU. These results suggest that modern pretrained vision models can replace active depth sensors as reliable, real-time sources of metric scale, marking a step toward simpler and more cost-effective SLAM systems.",
            "introduction": "Monocular SLAM remains attractive for its simplicity and minimal hardware requirements, yet it continues to face two persistent limitations: scale ambiguity and sensitivity to dynamic environments. Without metric depth, monocular pipelines suffer from scale drift and unstable landmark initialization, especially during early tracking. In addition, dynamic objects violate the static-world assumption underlying most SLAM formulations, producing incorrect correspondences and degraded pose estimates. These challenges have traditionally motivated the use of active depth sensors to achieve robust, metric-scale SLAM in real-world applications.\n\nActive sensing modalities such as RGB-D cameras and LiDAR provide metrically scaled depth and improved robustness to scene dynamics. However, they introduce cost, power consumption, and system complexity, and they are susceptible to degradation in outdoor settings, on reflective materials, or in low-texture regions [1].\n\nRecent advances in vision models present a promising alternative. Pretrained monocular depth estimators [2, 3] now deliver dense metric predictions with strong generalization across scenes and intrinsics. Learned keypoint detectors such as Key.Net [4] improve repeatability under motion blur and low texture, while instance segmentation networks such as YOLOv11 [5] provide efficient localization of dynamic objects. Together, these modules create an opportunity to recover metric structure and suppress dynamic content using only monocular RGB input.\n\nWe propose DropD-SLAM, a real-time monocular SLAM system that achieves RGB-D-level accuracy without relying on depth sensors. The method replaces the active depth stream of a classical SLAM front end with three pretrained modules: a monocular metric depth estimator, a learned keypoint detector, and an instance segmentation model. Dynamic regions are identified with instance-level masks and filtered using morphological dilation, while the remaining keypoints are assigned predicted metric depth and backprojected into 3D to initialize map points at absolute scale. The resulting features are processed by a standard RGB-D SLAM back end, which remains unmodified (see LABEL:fig:teaser). All components run in real time without task-specific training or fine-tuning, enabling zero-shot deployment across diverse environments.\n\nUnlike prior learned SLAM approaches that rely on end-to-end optimization, scene-specific adaptation, or custom back ends, DropD-SLAM preserves the classical geometric pipeline. By treating pretrained models as modular perception units, it achieves strong performance in both static and dynamic indoor settings while maintaining architectural simplicity and compatibility with existing systems.\n\nContributions.\n\nWe propose DropD-SLAM, a real-time monocular SLAM system that integrates pretrained monocular depth [2, 3], learned keypoints [4], and instance segmentation [5] into a modular front end that remains fully compatible with standard RGB-D back ends [6], which allows metric scale without the need for depth sensors.\n\nWe introduce a dynamic object filtering strategy based on instance-level segmentation with morphological dilation and we show that learned keypoints improve robustness under motion blur and texture-poor conditions.\n\nThrough ablation studies we identify temporal depth consistency rather than per-frame accuracy as the dominant factor for monocular SLAM performance, which provides a new perspective on the role of pretrained depth models.\n\nWe achieve state-of-the-art results on the TUM RGB-D benchmark [7] with 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences, and our system runs in real time (22 FPS) on a single GPU.\n\n1. We propose DropD-SLAM, a real-time monocular SLAM system that integrates pretrained monocular depth [2, 3], learned keypoints [4], and instance segmentation [5] into a modular front end that remains fully compatible with standard RGB-D back ends [6], which allows metric scale without the need for depth sensors.\n\n2. We introduce a dynamic object filtering strategy based on instance-level segmentation with morphological dilation and we show that learned keypoints improve robustness under motion blur and texture-poor conditions.\n\n3. Through ablation studies we identify temporal depth consistency rather than per-frame accuracy as the dominant factor for monocular SLAM performance, which provides a new perspective on the role of pretrained depth models.\n\n4. We achieve state-of-the-art results on the TUM RGB-D benchmark [7] with 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences, and our system runs in real time (22 FPS) on a single GPU.",
            "llm_summary": "【论文的motivation是什么】  \n1. Monocular SLAM面临尺度模糊和动态环境敏感性的问题。  \n2. 传统SLAM依赖主动深度传感器，增加了成本和复杂性。  \n3. 现有方法在动态环境中表现不佳，缺乏实时性能。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统SLAM方法依赖于特征匹配和深度传感器，无法有效处理动态场景。  \n2. 现有的学习型SLAM方法通常需要定制的后端或离线训练，限制了实时应用。  \n3. 先前的单目深度估计模型缺乏度量尺度的能力，难以直接集成到SLAM中。  \n\n【提出了什么创新的方法】  \n我们提出了DropD-SLAM，一个实时单目SLAM系统，通过三个预训练模块（单目度量深度估计器、学习的关键点检测器和实例分割网络）替代传统SLAM中的主动深度输入。动态物体通过实例级掩码进行抑制，静态关键点被赋予预测的深度值并反投影到3D中以形成度量特征。该系统在TUM RGB-D基准测试中实现了7.4 cm的平均绝对误差（ATE）和22 FPS的实时性能，表明现代预训练视觉模型可以有效替代深度传感器，简化SLAM系统。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Hybrid Quantum-Classical Policy Gradient for Adaptive Control of Cyber-Physical Systems: A Comparative Study of VQC vs. MLP",
            "authors": "Aueaphum Aueawatthanaphisut,Nyi Wunna Tun",
            "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "17 equations, 1 algorithm",
            "pdf_link": "https://arxiv.org/pdf/2510.06010",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.06010",
            "arxiv_html_link": "https://arxiv.org/html/2510.06010v1",
            "abstract": "The comparative evaluation between classical and quantum reinforcement learning (QRL) paradigms was conducted to investigate their convergence behavior, robustness under observational noise, and computational efficiency in a benchmark control environment. The study employed a multilayer perceptron (MLP) agent as a classical baseline and a parameterized variational quantum circuit (VQC) as a quantum counterpart, both trained on the CartPole-v1 environment over 500 episodes. Empirical results demonstrated that the classical MLP achieved near-optimal policy convergence with a mean return of 498.7±3.2498.7\\pm 3.2, maintaining stable equilibrium throughout training. In contrast, the VQC exhibited limited learning capability, with an average return of 14.6±4.814.6\\pm 4.8, primarily constrained by circuit depth and qubit connectivity.",
            "introduction": "Reinforcement learning (RL) has emerged as one of the central paradigms for sequential decision making, enabling autonomous agents to learn control strategies through interaction with their environments. Classical RL algorithms such as Q-learning and policy gradient methods have achieved remarkable success in robotics, autonomous driving, and cyber–physical control systems. Nevertheless, their scalability is often hindered by the curse of dimensionality and slow convergence in complex, nonlinear environments.\n\nIn recent years, the intersection of quantum computing and machine learning has given rise to a new class of algorithms—quantum reinforcement learning (QRL)—that seek to exploit quantum mechanical principles such as superposition, entanglement, and quantum parallelism to enhance exploration efficiency and learning speed. The foundational framework of QRL was first proposed by Dong et al. [1], where quantum states were used to represent policy superpositions and measurement collapse was treated as probabilistic action selection. This work demonstrated that quantum probability amplitudes could naturally balance exploration and exploitation. Subsequent studies expanded this concept through probabilistic Q-learning and fidelity-based optimization for control of quantum systems [2].\n\nThe development of near-term noisy intermediate-scale quantum (NISQ) devices has further motivated hybrid quantum–classical approaches. Variational quantum circuits (VQCs) have been adopted as trainable quantum policies that can be integrated with gradient-based optimization. Chen [4] introduced an asynchronous training paradigm for QRL agents using actor–critic structures, showing that quantum agents can achieve comparable or superior performance to classical counterparts with fewer parameters. Similarly, experimental works have demonstrated quantum speed-ups in physical RL systems by exploiting interference and entanglement for faster convergence [8].\n\nDespite these advances, a systematic comparison between classical multilayer perceptron (MLP)–based agents and VQC–based QRL policies in continuous control environments remains limited. This research aims to fill this gap by developing a unified framework for benchmarking both approaches under identical cyber–physical control tasks, quantifying convergence, robustness, and computational efficiency.",
            "llm_summary": "【论文的motivation是什么】  \n1. 经典强化学习在复杂环境中的可扩展性和收敛速度受限。  \n2. 量子强化学习的潜力尚未在连续控制环境中进行系统比较。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 经典RL算法在机器人和控制系统中取得成功，但在高维环境中表现不佳。  \n2. 量子RL的早期研究展示了量子态在策略表示中的应用，但缺乏与经典方法的直接比较。  \n\n【提出了什么创新的方法】  \n本研究开发了一个统一框架，比较经典的多层感知器（MLP）和变分量子电路（VQC）在相同的网络控制任务中的表现。通过在CartPole-v1环境中进行500个回合的训练，评估了两种方法的收敛性、鲁棒性和计算效率。结果表明，MLP在训练过程中实现了接近最优的策略收敛，而VQC的学习能力受到电路深度和量子比特连接性的限制，表现不佳。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Information-Theoretic Policy Pre-Training with Empowerment",
            "authors": "Moritz Schneider,Robert Krug,Narunas Vaskevicius,Luigi Palmieri,Michael Volpp,Joschka Boedecker",
            "subjects": "Artificial Intelligence (cs.AI); Information Theory (cs.IT); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05996",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05996",
            "arxiv_html_link": "https://arxiv.org/html/2510.05996v1",
            "abstract": "Empowerment, an information-theoretic measure of an agent’s potential influence on its environment, has emerged as a powerful intrinsic motivation and exploration framework for reinforcement learning (RL).\nBesides for unsupervised RL and skill learning algorithms, the specific use of empowerment as a pre-training signal has received limited attention in the literature.\nWe show that empowerment can be used as a pre-training signal for data-efficient downstream task adaptation.\nFor this we extend the traditional notion of empowerment by introducing discounted empowerment, which balances the agent’s control over the environment across short- and long-term horizons.\nLeveraging this formulation, we propose a novel pre-training paradigm that initializes policies to maximize discounted empowerment, enabling agents to acquire a robust understanding of environmental dynamics.\nWe analyze empowerment-based pre-training for various existing RL algorithms and empirically demonstrate its potential as a general-purpose initialization strategy: empowerment-maximizing policies with long horizons are data-efficient and effective, leading to improved adaptability in downstream tasks.\nOur findings pave the way for future research to scale this framework to high-dimensional and complex tasks, further advancing the field of RL.",
            "introduction": "Pre-training has emerged as a critical strategy in reinforcement learning (RL) to improve sample efficiency, stability, and generalization, particularly in environments where reward signals are sparse or delayed [1, 2]. By enabling agents to acquire useful priors about the structure of the environment or about transferable behavioral patterns, pre-training can significantly increase data efficiency during downstream task adaptation [2, 3]. Empowerment [4, 5, 6], an information-theoretic measure of an agent’s potential influence over future state distributions, offers a principled approach to unsupervised pre-training [7, 8, 9, 10]. Unlike tasks with externally given rewards, empowerment does not rely on external rewards but instead encourages the agent to explore and occupy states from which it retains a high degree of control in the environment. This intrinsic motivation enables the development of broadly applicable behaviors that can be rapidly adapted to a variety of downstream objectives.\n\nUnsupervised pre-training has become a cornerstone in the development of foundation models in both computer vision [11, 12, 13] and natural language processing [14, 15, 16], where large-scale models are trained on vast amounts of unlabeled data to learn general-purpose representations. Despite these advances, a comparable paradigm has not yet been established within RL. One promising domain for RL pre-training is video data [17, 18], which naturally embodies sequential dynamics and agent-environment interactions. Utilizing such data would be additionally useful due to the difficulty of collecting real-world data for RL agents as video data is broadly available. However, the absence of explicit reward signals in such data precludes the direct application of traditional RL techniques. This limitation necessitates the use of unsupervised RL approaches, where intrinsic objectives—such as curiosity [19], information gain [20, 21] or empowerment [7, 8]—serve as substitutes for task-specific rewards. These unsupervised signals enable agents to learn useful representations and behaviors that can generalize across tasks, potentially paving the way for foundation models in RL analogous to those in vision and language. We show that empowerment can be effectively employed as a pre-training signal, enabling agents to learn useful representations and behaviors that can be rapidly adapted to various downstream tasks.\n\nA key distinction between our work and prior research on empowerment lies in the role of empowerment-based objectives. Previous studies have predominantly employed empowerment and mutual information maximization as intrinsic reward signals aimed at enhancing exploration during RL [22, 9, 23, 24] or complete unsupervised control [7, 8, 10], in contrast to targeting improved initialization of policies before actual downstream task learning. Its application as a standalone pre-training signal remains largely underexplored. In other cases, empowerment has been leveraged to facilitate the unsupervised acquisition of diverse skills, which are typically pre-trained and subsequently utilized in downstream tasks [25, 26, 27]. However, Eysenbach et al. [28] demonstrated that, contrary to conventional assumptions, maximizing mutual information may not necessarily result in the development of distinct skills, but rather serves to generate a beneficial initialization for skill acquisition. We argue that such an initialization would be beneficial for a general policy itself which is not dependent on a skill identifier.\n\nIn short, we address these gaps by the following key contributions:\n\nWe propose a generic framework for unsupervised pre-training in RL based on empowerment, emphasizing its utility for initializing policies in downstream tasks.\n\nWe introduce discounted empowerment as a flexible objective that balances short- and long-term control, enabling a simple pre-training strategy without the need to tune a specific horizon for multi-step empowerment.\n\nWe demonstrate through experiments that empowerment-based pre-training with our discounted empowerment reward accelerates the adaption process and improves learning efficiency on a variety of downstream RL tasks.\n\n1. We propose a generic framework for unsupervised pre-training in RL based on empowerment, emphasizing its utility for initializing policies in downstream tasks.\n\n2. We introduce discounted empowerment as a flexible objective that balances short- and long-term control, enabling a simple pre-training strategy without the need to tune a specific horizon for multi-step empowerment.\n\n3. We demonstrate through experiments that empowerment-based pre-training with our discounted empowerment reward accelerates the adaption process and improves learning efficiency on a variety of downstream RL tasks.",
            "llm_summary": "【论文的motivation是什么】  \n1. 在强化学习中，预训练策略可以提高样本效率、稳定性和泛化能力。  \n2. 现有的预训练方法未充分利用赋权作为预训练信号，限制了其在下游任务中的适应性。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 先前的研究主要集中在赋权作为内在奖励信号以增强探索或实现完全无监督控制。  \n2. 赋权作为独立的预训练信号尚未得到充分探讨，缺乏针对政策初始化的研究。  \n\n【提出了什么创新的方法】  \n我们提出了一种基于赋权的无监督预训练通用框架，强调其在下游任务中的政策初始化效用。通过引入折扣赋权作为灵活目标，平衡短期和长期控制，简化了预训练策略的实现。实验结果表明，基于赋权的预训练显著加速了适应过程，提高了多种下游强化学习任务的学习效率。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "The Safety Challenge of World Models for Embodied AI Agents: A Review",
            "authors": "Lorenzo Baraldi,Zifan Zeng,Chongzhe Zhang,Aradhana Nayak,Hongbo Zhu,Feng Liu,Qunli Zhang,Peng Wang,Shiming Liu,Zheng Hu,Angelo Cangelosi,Lorenzo Baraldi",
            "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05865",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05865",
            "arxiv_html_link": "https://arxiv.org/html/2510.05865v1",
            "abstract": "The rapid progress in embodied artificial intelligence has highlighted the necessity for more advanced and integrated models that can perceive, interpret, and predict environmental dynamics. In this context, World Models (WMs) have been introduced to provide embodied agents with the abilities to anticipate future environmental states and fill in knowledge gaps, thereby enhancing agents’ ability to plan and execute actions. However, when dealing with embodied agents it is fundamental to ensure that predictions are safe for both the agent and the environment.\nIn this article, we conduct a comprehensive literature review of World Models in the domains of autonomous driving and robotics, with a specific focus on the safety implications of scene and control generation tasks. Our review is complemented by an empirical analysis, wherein we collect and examine predictions from state-of-the-art models, identify and categorize common faults (herein referred to as pathologies), and provide a quantitative evaluation of the results.",
            "introduction": "Recently, autonomous agents such as self-driving cars, humanoid robots and manipulators have been driven by the strong reasoning capability and ease of accessibility to large transformer models. However, researchers frequently try to address the questions, ‘can training data for an agent be autonomously generated?’ and ‘can an agent reason and perform complex maneuvers in dynamical scenarios?’ The common theme underlying these questions is the ability of the agent to comprehend the future evolution of its surroundings from the present state and predict the immediate outcome of its actions. Further, the recent emergence of large language models and their unparalleled levels to linguistic comprehension has led to an increased interest in semantically driven agents.\n\nThis creates a three-fold challenge. The agent must be able to 1) correctly comprehend an instruction 2) process information from its environment in order to interact with participants in it and 3) predict the outcome in the immediate future and thereby take steps to execute the instruction.\nTo address the first two challenges, researchers have integrated multiple sources of information, derived from perceptual inputs, with the reasoning capabilities characteristic of language models leading to the creation of Multimodal Large Language Models (MLLMs) Caffagni et al. (2024). The third challenge is addressed by so-called World Models (WMs) Agarwal et al. (2025) that have been proposed as a potential solution to estimate future environmental observations and possibly missing information.\n\nAlthough the concept of WMs is well known in control theory to learn the dynamics of an agent in static or unknown environments, recent research endeavors have significantly advanced WMs development, enabling prediction of future observations and their translation into various forms of output, including videos  Brooks et al. (2024), control actions Wang et al. (2024c), or a combination of both  Hu et al. (2023). Through the training process, WMs implicitly acquire a fundamental understanding of the underlying physical laws, dynamics, and spatiotemporal continuity, which is reflected in the generated predictions. However, it is crucial to acknowledge that faulty predictions can lead to catastrophic results in embodied scenarios like autonomous driving. Therefore the focus should not solely be directed to correctly executing the task, but rather on ensuring that the predicted actions and outcomes do not compromise the safety of the agent, its environment, or other entities.\n\nIn response to the growing research efforts aimed at constructing better World Models, this literature review undertakes a three-fold objective. Firstly, it provides a comprehensive and systematic overview of the current landscape of World Models in the field of autonomous driving and robotics. Secondly, it conducts an in-depth analysis of the safety aspects of WMs, with a focus on the tasks of scene and control generation. This analysis identifies the faults of world models, which are herein referred to as pathologies, as well as it establishes qualitative criteria for their estimation. Finally, it identifies and discusses prominent future research directions for improved safety in WMs.\n\nThis review differs from existing surveys Zhu et al. (2024); Guan et al. (2024) in its focus on the safety aspect of WMs, rather than solely on their performance and taxonomy. Further, it builds on the definition proposed in Zeng et al. (2024), providing detailed analysis on both scene generation and control tasks in the domain of robotics and autonomous driving, including quantitative evaluations on the defined pathology criteria. By doing so, this review aims to provide a comprehensive understanding of the current state of World Models and their safety implications, ultimately contributing to the development of more robust and reliable WMs.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要更先进的模型来提升具身智能体的环境感知和预测能力。  \n2. 确保具身智能体的预测安全性，以避免对自身和环境造成危害。  \n3. 识别和分类世界模型中的常见故障，以改进未来的研究方向。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有的世界模型主要关注性能和分类，而忽视了安全性问题。  \n2. 之前的研究未能系统性地分析场景生成和控制任务中的安全隐患。  \n\n【提出了什么创新的方法】  \n本研究通过文献综述和实证分析，系统性地评估了世界模型在自动驾驶和机器人领域的安全性，识别了模型中的路径病（pathologies），并建立了定性标准来评估这些病态。通过对现有模型的预测进行定量评估，本文为未来的研究方向提供了新的见解，旨在构建更安全、更可靠的世界模型。研究结果表明，关注模型的安全性可以显著降低潜在的风险，并为具身智能体的安全应用奠定基础。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption",
            "authors": "Chen Li,Zhantao Yang,Han Zhang,Fangyi Chen,Chenchen Zhu,Anudeepsekhar Bolimera,Marios Savvides",
            "subjects": "Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2510.05580",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05580",
            "arxiv_html_link": "https://arxiv.org/html/2510.05580v1",
            "abstract": "Vision–Language–Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists—they often require task-specific fine-tuning, incur high compute costs, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism—derived from Attentive Neural Processes—to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ∼\\sim76%. These results show that scalable, low-resource post-training is achievable—paving the way toward general-purpose embodied agents. Code will be available.",
            "introduction": "Recent years have seen rapid progress in embodied Vision–Language–Action (VLA) models, which are typically pretrained from Vision–Language Models (VLMs) and adapted via supervised fine-tuning (SFT)  Kim et al. (2024; 2025); Hung et al. (2025) or reinforcement learning (RL) Zhang et al. (2025); Li et al. (2025) to enable transfer to new embodiment tasks. In one line of work, a pretrained VLA backbone is adapted to autoregressively and discretely decode action tokens, trained on annotated demonstrations consisting of video or image observations paired with natural language instructions Kim et al. (2024); Brohan et al. (2022; 2023); O’Neill et al. (2024). In contrast, another line of research represents output actions as continuous vectors, using techniques such as diffusion policies or flow matching Black et al. (2024); Intelligence et al. (2025a); NVIDIA et al. (2025).\n\nDespite advances in new task adaptation, current VLAs are not yet true generalists—still far from fully out-of-the-box usability and reliant on alignment through post-training (Zhou et al., 2025; Wang et al., 2025; Huang et al., 2025b; Din et al., 2025; Guruprasad et al., 2025; Ma et al., 2025).\nCompounding this, post-training remains practically constrained by benchmarks with low per-task data. Current practice Kim et al. (2024) fine-tunes each downstream task independently, increasing overall training cost, hindering knowledge transfer across related tasks, and ultimately limiting success rate.\nThese task-specific schedules are often brittle: many gradient steps are required before stably meaningful action sequences emerge, raising the risk of poor generalization and slowing adaptation to new task variants. For example, OpenVLA requires 240K training steps to fine-tune across all four LIBERO suites OpenVLA Team (2024), while OpenVLA-OFT Kim et al. (2025) demands approximately 150K ~500K steps, including both diffusion and non-diffusion parts. Long-horizon tasks such as LIBERO-Long further dominate the training schedule and often become the system bottleneck.\n\nWhile recent work Black et al. (2024); Intelligence et al. (2025a); Qu et al. (2025) has focused on expanding datasets and exploring backbone architecture or training protocol innovations during pretraining, we instead tackle it from an orthogonal perspective at the post-training stage. Our experiments begin with a vanilla multi-task co-training setting: applying a standard SFT to a single model across related in-domain tasks (i.e., the four LIBERO suites). Indeed, we observe a reduction in total GPU training hours and improved success rates, which naturally motivates us to raise a question: can we introduce even more auxiliary tasks in the co-training to further boost VLA models? Sadly, we find that naively adding auxiliary tasks with greater domain diversity slows convergence and degrades performance. We attribute this surprise to the optimization instability arising from heterogeneous distributions, where misalignments in both the feature space (e.g., camera views) and action space (e.g., degrees of freedom) hinder the benefits of co-training.\n\nBuilding on these ideas, we propose MetaVLA, a unified framework that fills a critical gap in VLA post-training by intelligently introducing auxiliary tasks without incurring the inefficiencies of per-task SFT or the performance drop of naive multi-task SFT.\nIt introduces Context-Aware Meta Co-Training, a streamlined paradigm that optimizes target tasks while infusing extrinsic signals from auxiliary tasks to elevate performance. MetaVLA integrates VLA with a meta-learning co-training strategy to enhance efficiency and generalization in limited-data adaptation. Specifically, a single model is trained across target tasks (e.g., LIBERO suites) to harness cross-task gradients, while auxiliary tasks are incorporated through a memory-augmented mechanism derived from Attentive Neural Processes (ANP) Kim et al. (2019). This lightweight module injects out-of-domain information gain without disrupting target optimization, enabling scalable and robust adaptation. MetaVLA is maintenance-friendly, backbone-agnostic, and easily extends beyond SFT to training paradigms like reinforcement learning.\nFigure 1 highlights three key advantages of MetaVLA over existing approaches.\n\nExperiments show that MetaVLA with six auxiliary tasks outperforms the OpenVLA baseline by 4.4% and multi-task SFT by 3.1% on average, with gains up to 8.0% on LIBERO-Long. It unifies training into a single model, reducing steps from 240K to 75K and GPU time by 76%—from ∼\\sim100 to ∼\\sim24 hours. Despite its flexibility, the compact memory-augmented module adds only 0.3 ms/token in latency. The following sections present our framework, setup, and results, showing how MetaVLA boosts convergence, efficiency, and action reasoning. Our main contributions are as follows:\n\nWe investigate an underexplored direction: improving post-training efficiency and generalization ability through incorporating diverse auxiliary tasks with negligible optimization overhead.\n\nWe investigate an underexplored direction: improving post-training efficiency and generalization ability through incorporating diverse auxiliary tasks with negligible optimization overhead.\n\nWe propose MetaVLA, a suite of plug-in module and training recipes that enables fast and scalable adaptation with strong generalization. MetaVLA is engineering-friendly and agnostic to backbone architectures and underlying training pipelines.\n\nWe conduct comprehensive experiments to show that MetaVLA delivers superior performance with significant efficiency gains by reducing model count and GPU training hours, while preserving fast inference.\n\n1. We investigate an underexplored direction: improving post-training efficiency and generalization ability through incorporating diverse auxiliary tasks with negligible optimization overhead.\n\n2. We propose MetaVLA, a suite of plug-in module and training recipes that enables fast and scalable adaptation with strong generalization. MetaVLA is engineering-friendly and agnostic to backbone architectures and underlying training pipelines.\n\n3. We conduct comprehensive experiments to show that MetaVLA delivers superior performance with significant efficiency gains by reducing model count and GPU training hours, while preserving fast inference.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的VLA模型在适应新任务时需要特定的微调，导致计算成本高且泛化能力差。  \n2. 当前的后训练方法在低数据的基准测试中受到限制，影响了知识转移和成功率。  \n3. 多任务微调方法在引入辅助任务时容易导致性能下降和收敛不稳定。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 现有工作主要集中在预训练阶段的模型架构和训练协议创新，后训练阶段的研究较少。  \n2. 多任务共训练在VLA中的应用仍未充分探索，尤其是在后训练阶段。  \n3. 现有方法在引入多样化辅助任务时，往往面临高计算成本和不稳定性的问题。  \n\n【提出了什么创新的方法】  \nMetaVLA提出了一种统一的后训练框架，通过上下文感知的元共训练方法，整合多样化的辅助任务以提升VLA模型的效率和泛化能力。该方法通过轻量级的元学习机制，允许模型在不同上下文中快速适应，减少了训练步骤和GPU时间。实验结果表明，MetaVLA在LIBERO基准上相较于OpenVLA提升了4.4%的性能，并将训练步骤从240K减少至75K，GPU时间减少约76%。这些结果表明，MetaVLA在实现高效、可扩展的后训练方面具有显著的优势，为通用的具身智能体铺平了道路。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-10-08 02:22:34",
            "title": "Safety-Critical Control with Bounded Inputs: A Closed-Form Solution for Backup Control Barrier Functions",
            "authors": "David E. J. van Wijk,Ersin Das,Tamas G. Molnar,Aaron D. Ames,Joel W. Burdick",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO)",
            "comment": ". Code available atthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2510.05436",
            "code": "https://github.com/davidvwijk/OI-CBF",
            "arxiv_abs_link": "https://arxiv.org/abs/2510.05436",
            "arxiv_html_link": "https://arxiv.org/html/2510.05436v1",
            "abstract": "Verifying the safety of controllers is critical for many applications, but is especially challenging for systems with bounded inputs. Backup control barrier functions (bCBFs) offer a structured approach to synthesizing safe controllers that are guaranteed to satisfy input bounds by leveraging the knowledge of a backup controller. While powerful, bCBFs require solving a high-dimensional quadratic program at run-time, which may be too costly for computationally-constrained systems such as aerospace vehicles. We propose an approach that optimally interpolates between a nominal controller and the backup controller, and we derive the solution to this optimization problem in closed form. We prove that this closed-form controller is guaranteed to be safe while obeying input bounds. We demonstrate the effectiveness of the approach on a double integrator and a nonlinear fixed-wing aircraft example.",
            "introduction": "Controlling dynamic systems\nwith constraints that arise from\nphysical limitations, environmental interactions, or user-defined requirements is a fundamental aspect of control applications.\nAs such, constrained control [1]—also called safety-critical control when constraints encode safety requirements [2]—has become a top priority in many autonomous or semi-autonomous cyber-physical systems ranging from robotics to aeronautics.\nControl barrier functions (CBFs) [3] have emerged as a framework for synthesizing safe controllers\nthat guarantee the forward invariance of a specified safe set.\nWhile CBFs have demonstrated success in ensuring safety in many domains [4, 5], they also highlight certain challenges.\nNamely, conventional CBFs require one to verify that the safe control inputs are feasible at all points within the safe set, which may be difficult in the presence of input constraints.\n\nMotivated by this feasibility challenge, backup control barrier functions (bCBFs) [6] have been proposed to provide theoretical guarantees for the safety of control systems with input bounds. The bCBF technique is applicable for complex nonlinear systems, and has shown success across a wide range of applications [7, 8, 9, 10, 11, 12, 13, 14, 15].\nHowever, a potential downside of this approach is its online computational requirements: it involves solving a quadratic program (QP) with a large number of constraints at run-time, and to construct these constraints, one must\nforward integrate a number of ordinary differential equations (ODEs) whose number increases quadratically with the state dimension. This may not be feasible for high-dimensional nonlinear systems with limited online computational resources, such as aerospace vehicles. Thus, many safety-critical control solutions for aerospace vehicles have utilized CBFs rather than bCBFs (see e.g., [16, 17, 18, 19, 20]).\n\nTo remedy this, the authors of [21, 22] have introduced a technique for safe controller synthesis that builds on the concept of bCBFs without requiring optimization for the safe input.\nInstead, this method – which we call safe blending – utilizes a smooth blending function to safely\ncombine a nominal controller and a backup controller.\nThis approach does not require expensive gradient computations, nor does it involve solving a QP online.\nWhile effective, this solution is inherently sub-optimal, as the blending function is chosen subjectively and must be hand-tuned. Furthermore, this approach may exhibit undesirable oscillations\nwhen the nominal and backup controllers act against each other,\nresulting in controller chattering and degraded system performance.\n\nInspired by these previous works, we propose a middle ground between the bCBF and the blended approach. Our main contribution is the optimally interpolated (OI) controller—a novel safety-critical control technique for systems with bounded inputs (see Figure˜1). The proposed method optimally interpolates between a nominal controller and a pre-verified backup controller to ensure that the dynamical system remains within a controlled invariant safe set for all time.\nWe derive a closed-form expression for the optimal safe controller, and prove that this solution is feasible even in the presence of input bounds.\nWe demonstrate that the proposed OI controller does not experience the undesirable oscillations observed for the blending approach.\nFurthermore, compared to the bCBF approach, the number of ODEs to be integrated is smaller, while the closed-form expression eliminates the need for solving a high-dimensional QP numerically.\n\nThe rest of the paper is organized as follows. Section II overviews CBFs and bCBFs. Section III discusses the function-based controller blending method.\nIn Section IV, we present the proposed\noptimally interpolated safety-critical control approach.\nIn Section V, we cover the results of our numerical simulations, and we conclude with Section VI.",
            "llm_summary": "【论文的motivation是什么】  \n1. 在受限输入的情况下，验证控制器的安全性是一个重要且具有挑战性的任务。  \n2. 现有的备份控制障碍函数(bCBFs)在高维系统中计算开销过大，限制了其在资源受限平台上的应用。  \n\n【前人的工作如何解决该问题，存在哪些空白】  \n1. 传统的控制障碍函数(CBFs)在确保安全性方面有效，但在输入约束下的可行性验证存在困难。  \n2. bCBFs提供了理论保证，但需要在线解决高维二次规划(QP)，在计算资源有限的情况下难以实现。  \n3. 现有的安全控制合成方法虽然有效，但存在手动调节和控制器抖动的问题，影响系统性能。  \n\n【提出了什么创新的方法】  \n我们提出了一种新的安全控制技术——最优插值(Optimally Interpolated, OI)控制器，该方法在名义控制器和预验证的备份控制器之间进行最优插值，以确保动态系统始终保持在受控不变安全集内。我们推导出该最优安全控制器的闭式表达式，并证明该解决方案在输入约束下是可行的。与bCBF方法相比，该方法所需的常微分方程(ODE)的数量更少，闭式表达式消除了在线数值求解高维QP的需求。此外，OI控制器避免了混合方法中观察到的抖动现象，提升了系统性能。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        }
    ]
}