{
    "2025-09-24": [
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies",
            "authors": "Lars Ankile,Zhenyu Jiang,Rocky Duan,Guanya Shi,Pieter Abbeel,Anusha Nagabandi",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19301",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19301",
            "arxiv_html_link": "https://arxiv.org/html/2509.19301v1",
            "abstract": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor control policies. However, these approaches are limited by the quality of human demonstrations, the manual effort required for data collection, and the diminishing returns from increasing offline data.\nIn comparison, reinforcement learning (RL) trains an agent through autonomous interaction with the environment and has shown remarkable success in various domains. Still, training RL policies directly on real-world robots remains challenging due to sample inefficiency, safety concerns, and the difficulty of learning from sparse rewards for long-horizon tasks, especially for high-degree-of-freedom (DoF) systems.\nWe present a recipe that combines the benefits of BC and RL through a residual learning framework. Our approach leverages BC policies as black-box bases and learns lightweight per-step residual corrections via sample-efficient off-policy RL.\nWe demonstrate that our method requires only sparse binary reward signals and can effectively improve manipulation policies on high-degree-of-freedom (DoF) systems in both simulation and the real world. In particular, we demonstrate, to the best of our knowledge, the first successful real-world RL training on a humanoid robot with dexterous hands.\nOur results demonstrate state-of-the-art performance in various vision-based tasks, pointing towards a practical pathway for deploying RL in the real world.",
            "introduction": "Enabling robots to learn and improve directly in their deployment environments remains a fundamental challenge in robotics. Recently, significant progress has been made in training visuomotor control policies in the real world with behavior cloning (BC) from human demonstrations [1, 2, 3, 4, 5, 6, 7, 8, 9]. However, this success requires significant infrastructure, as well as numerous hours of manual and cumbersome data collection.\nEven if unlimited data could be collected for every task, not only is human teleoperator performance generally suboptimal, but there is also emerging evidence that policy performance saturates with increasing demonstrations [10, 6, 11, 12, 13].\n\nReinforcement learning (RL) offers a complementary paradigm: agents learn autonomously through trial and error in the environment. Deep RL has shown great success in various domains [14, 15, 16, 17, 18, 19, 20, 21], including in-hand manipulation [22, 23] and locomotion [24, 25, 26, 27]. However, strong RL performance generally requires large amounts of data from online interactions, so its application has been mainly in simulation [28, 29] since real-world data are expensive and potentially unsafe to gather in large amounts.\n\nA natural direction to improve BC policies is to leverage online RL [12, 30, 31, 32], combining the strengths of each: BC policies provide a strong prior that can regularize exploration in the RL process, while online RL enhances policy performance by learning from interactions with the environment. However, modern BC architectures are typically deep models with tens of millions to billions of parameters that utilize action chunking or diffusion-based approaches, which can make it challenging to apply RL methods directly to optimize the policy. A simple yet powerful recipe that avoids several of the above issues is residual RL [33, 34, 35, 36, 12, 32, 31], where RL is applied not to learn a full policy, but only to learn corrective terms on top of a fixed base controller. Previous work has demonstrated that residual RL can indeed enhance the reliability of a pre-trained policy. Still, it has so far been limited to learning in simulation [12, 32, 31] or demonstrating results in simple or constrained settings [33, 34, 35, 36]; Applications to high-DoF systems learning directly in the real world are still lacking.\n\nIn this work, we present an off-policy residual fine-tuning (ResFiT) approach that utilizes online RL to enhance BC policies. By treating the base policy as a black box and learning a per-step residual correction that is independent of chunk size and policy parameterization, we sidestep the challenges of directly optimizing huge base policies. By carefully designing our off-policy recipe, we make the RL process sample efficient enough to scale to high-DoF bimanual systems, require only sparse binary reward signals, and be safe enough to deploy in the real world. We demonstrate robust performance on sparse-reward, long-horizon, vision-based tasks, showing that our approach achieves state-of-the-art performance for a range of tasks in simulation. We also investigate each design decision in our recipe. To the best of our knowledge, we provide the first demonstration of RL on a humanoid robot with five-fingered hands, trained entirely in the real world.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在保留行为克隆（BC）基线的同时通过离线/在线强化学习实现对复杂高自由度系统的高效微调与改进\n2. 如何实现对高DoF系统的安全、样本高效的Residual RL，以最少数据和稀疏奖励实现现实世界鲁棒性\n3. 如何在不直接优化大型基准策略的前提下，对其进行逐步残差修正以提升 visuomotor 控制性能\n4. 如何将离线BC与在线RL结合，解决数据质量、数据收集成本、以及现实世界训练的安全性挑战\n5. 如何在真实 humanoid/五指手等高复杂度机械臂上实现端到端的实证RL训练与评估\n\n【用了什么创新的方案】\n核心解决方案：提出离线行为克隆基线上的残差强化学习（ResFiT），将基线策略视为黑盒，通过学习逐步的每步残差信号来实现修正；使用面向离线/在线混合的高效RL配方，确保在高DoF、稀疏二值奖励下也能实现样本友好、真实世界的训练，并在仿真与现实中获得前所未有的鲁棒性与性能提升；通过残差学习避免直接优化庞大基线策略，降低参数化、数据需求和安全风险；首次在真实 humanoid/五指手上实现全现实世界的RL微调。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration",
            "authors": "Yang Jin,Jun Lv,Han Xue,Wendi Chen,Chuan Wen,Cewu Lu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19292",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19292",
            "arxiv_html_link": "https://arxiv.org/html/2509.19292v1",
            "abstract": "Intelligent agents progress by continually refining their capabilities through actively exploring environments. Yet robot policies often lack sufficient exploration capability due to action mode collapse. Existing methods that encourage exploration typically rely on random perturbations, which are unsafe and induce unstable, erratic behaviors, thereby limiting their effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a framework that enhances policy exploration and improvement in robotic manipulation. SOE learns a compact latent representation of task-relevant factors and constrains exploration to the manifold of valid actions, ensuring safety, diversity, and effectiveness. It can be seamlessly integrated with arbitrary policy models as a plug-in module, augmenting exploration without degrading the base policy performance. Moreover, the structured latent space enables human-guided exploration, further improving efficiency and controllability. Extensive experiments in both simulation and real-world tasks demonstrate that SOE consistently outperforms prior methods, achieving higher task success rates, smoother and safer exploration, and superior sample efficiency. These results establish on-manifold exploration as a principled approach to sample-efficient policy self-improvement.",
            "introduction": "“We want AI agents that can discover like we can, not which contain what we have discovered.”\n            — Richard Sutton, The Bitter Lesson\n\nIn recent years, data-driven robot learning [10, 53, 5, 7, 25] has attracted considerable attention, particularly for its potential to enhance robotic manipulation capabilities through large-scale data collection and training. By modeling visuomotor behaviors with neural networks, these approaches allow robot policies to learn from expert demonstrations and achieve near-human performance across a variety of tasks.\n\nDespite these advances, most existing methods still rely heavily on human teleoperation for data acquisition [53, 13] and policy refinement [30, 31], which presents several challenges. A primary concern is the high cost of teleoperation, as it typically requires skilled operators and specialized equipment, thereby limiting the scalability of data collection. More critically, teleoperated demonstrations often fail to cover the diverse scenarios a robot could encounter in the real world, resulting in distributional bias [52] and compounding error [39]. The problem is further exacerbated by the fact that human operators may act based on contextual cues inaccessible to robot sensors. Robots, on the other hand, may internalize human habits rather than task-relevant behaviors. As a result, simply scaling up teleoperated data is not the optimal path toward improving policy performance.\n\nInstead of passively imitating human-provided behavior, a line of research addresses this challenge by enabling robot policy self-improvement [6, 23, 35, 32]—actively exploring the environment to collect diverse experience and leveraging that experience to refine policies. Under this paradigm, robots can autonomously discover novel behaviors that go beyond the coverage of human demonstrations. By iteratively practicing the learned behaviors, they also develop a deeper understanding of the natural variability in their actions, ultimately leading to a more robust and resilient policy.\n\nThe key to sample-efficient robot policy self-improvement lies in effective exploration. Prior work [3, 23] has shown that imitation-learned policies often overfit demonstrations, collapse into single-modal motions, and fail to produce diverse behaviors. Without proper exploration, these policies tend to repeat failed behaviors, limiting their ability to discover improved solutions. While random exploration strategies can occasionally yield novel behaviors [29], they are generally ineffective in high-dimensional action spaces [28] and can pose safety risks in real-world deployment [16], causing potential hardware damage. This necessitates a more structured approach to exploration—one that ensures safety and effectiveness without sacrificing the diversity of experiences.\n\nTo this end, we propose SOE, a novel framework for Sample-Efficient Robot Policy Self-improvement via On-Manifold Exploration. The core idea of our method is to ensure that exploration remains constrained to the manifold of valid actions—critical for both safety and effectiveness. Prior works often perturb the action space directly [29] or inject random noise [23], leading to temporally inconsistent and unsafe behaviors, particularly under “action chunking” representations [53]. In contrast, we perform exploration in a compact latent space learned through a variational information bottleneck (VIB). The latent representation in this space preserves only task-essential information in observation while discarding irrelevant details, ensuring exploration remains structured and efficient. As illustrated in Fig. 1, by operating on this latent representation, our framework enables effective on-manifold exploration and more robust policy improvement. Furthermore, we demonstrate that in the latent space, action chunks are naturally disentangled into distinct modes. Leveraging this property, we achieve controllable exploration, which allows users to guide exploration toward preferred directions, thereby enhancing interpretability and further boosting sample efficiency. Implemented as a plug-in module, our approach can be seamlessly integrated with existing imitation learning algorithms and jointly optimized, without any degradation in their performance.\n\nTo evaluate the effectiveness of our method, we conduct extensive experiments across a variety of robot manipulation tasks in both simulation and real world. The results show that SOE consistently outperforms prior exploration methods in effectiveness, motion smoothness, and sample efficiency. With just one round of policy self-improvement, our method achieves substantial gains over the base policy, including an average relative improvement of 50.8% on real-world tasks. Additional experiments in simulation and ablation studies further confirm multi-round performance improvements and the contribution of each component in our framework. Collectively, these findings demonstrate that on-manifold exploration provides a structured, safe, and effective approach to sample-efficient robot policy self-improvement.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在高维机器人动作空间中实现安全、有效且样本高效的策略自我改进中的探索？\n2. 如何通过潜在表征约束探索到行动流形以避免无效或危险的随机扰动？\n3. 如何实现对探索的可控性与可解释性，同时保持对既有模仿学习的无干扰性？\n4. 如何在仿真与真实世界任务中提升任务成功率、平滑性和样本效率？\n\n【用了什么创新的方案】\nSOE在任务相关因素的紧凑潜在表示上进行探索，利用变分信息瓶颈学习一个仅保留任务本质信息的潜在空间，并在该潜在流形上进行探索以保持动作的有效性和安全性。它将探索与现有策略模型无缝对接，可作为插件模块嵌入到任意模仿学习框架中；潜在空间还使动作块在不同模式上解耦，从而实现可控探索，并支持人工引导以提高效率。实验结果表明在仿真和真实任务中，SOE实现更高的成功率、更平滑且更安全的探索以及更强的样本效率。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces",
            "authors": "Kuanqi Cai,Chunfeng Wang,Zeqi Li,Haowen Yao,Weinan Chen,Luis Figueredo,Aude Billard,Arash Ajoudani",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19261",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19261",
            "arxiv_html_link": "https://arxiv.org/html/2509.19261v1",
            "abstract": "Robotic manipulation in dynamic environments often requires seamless transitions between different grasp types to maintain stability and efficiency. However, achieving smooth and adaptive grasp transitions remains a challenge, particularly when dealing with external forces and complex motion constraints. Existing grasp transition strategies often fail to account for varying external forces and do not optimize motion performance effectively. In this work, we propose an Imitation-Guided Bimanual Planning Framework that integrates efficient grasp transition strategies and motion performance optimization to enhance stability and dexterity in robotic manipulation. Our approach introduces Strategies for Sampling Stable Intersections in Grasp Manifolds for seamless transitions between uni-manual and bi-manual grasps, reducing computational costs and regrasping inefficiencies. Additionally, a Hierarchical Dual-Stage Motion Architecture combines an Imitation Learning-based Global Path Generator with a Quadratic Programming-driven Local Planner to ensure real-time motion feasibility, obstacle avoidance, and superior manipulability. The proposed method is evaluated through a series of force-intensive tasks, demonstrating significant improvements in grasp transition efficiency and motion performance.\nA video demonstrating our simulation results can be viewed at https://youtu.be/3DhbUsv4eDo.",
            "introduction": "Robotic manipulation in dynamic forceful operations—such as collaborative cutting or drilling—demands real-time adaptation to varying external forces that critically affect grasp stability. Consider a human-robot woodworking scenario (Fig. 1) where the robot must continuously adjust between uni-manual and bi-manual grasps to counteract changing cutting and drilling forces. This fundamental requirement exposes two unresolved challenges in existing methods: efficient grasp transitions by minimizing execution time and arm movement and motion performance awareness, as crucial metrics like manipulability and joint limits essential for control safety are often overlooked. To bridge this gap, we propose an imitation-guided planning framework that integrates efficient grasp transitions with motion performance constraints, ensuring both stability and dexterity in forceful tasks.\n\nMulti-step manipulation planners have long tackled regrasping and grasping transitions [1]. Traditional grasping involves transporting an object by repeatedly releasing and regrasping it as needed [2]. Conventional regrasp planners rely on a supporting surface for single-arm manipulation [3, 4], while recent research extends these strategies to dual-arm scenarios [5, 6, 7].\nHowever, existing methods do not explicitly account for dynamic external forces, which vary over time, nor do they optimize regrasp transitions during forceful interactions. Studies in forceful human-robot collaboration [8, 9, 10] focus on regulating contact forces but assume a fixed or pre-determined grasp. The key challenge remains: determining where and how a robot should grasp for stability and when to transition seamlessly under complex external forces. Recent works [11, 12, 13] have made progress, but achieving stable grasps that withstand varying forces while ensuring efficient planning, manipulability, and dexterity remains difficult.\nThis paper addresses two key challenges in forceful robotic manipulation: efficient grasp transitions and motion performance optimization, proposing a novel framework to overcome them.\n\nEfficient Grasp Transitions. Previous methods [11, 13] mostly rely on random sampling-based planners for grasp transitions, often resulting in high computational costs and unstable changes. To reduce task execution time and minimize the movement distance of the robot arm, we introduce Strategies for Sampling Stable Intersections in Grasp Manifolds for seamless uni-manual and bi-manual transitions. Our Directional Gradient-Based Resampling locally adjusts the unimanual manipulator along the negative gradient, ensuring stability while maintaining a secure unimanual grasp and minimizing movement. For tasks with multiple grasp changes, Multi-Grasp Transition Check (MTC) identifies a shared intermediate configuration, reducing redundant regrasping. To further boost efficiency, we propose a Hierarchical Dual-Stage Motion Architecture, combining an Imitation Learning-based Global Path Generator with a QP-driven local planner for real-time motion optimization and obstacle avoidance, enabling faster collision-free path generation than sampling-based methods.\n\nMotion Performance Optimization.\nIn forceful operations, robotic stability is determined by three motion performance factors: manipulability, dexterity, and joint limits, which ensure kinematic feasibility under varying forces. However, many recent methods [14, 15] tend to overlook these critical aspects.\nOur framework optimizes motion performance at both the grasp configuration level and during execution. To enhance manipulability and avoid kinematic limitations, we introduce a Motion Performance Map that encodes feasibility, manipulability, and joint limit proximity. This map guides grasp sampling toward optimal workspace regions, improving selection efficiency. During execution, we enforce manipulability constraints within the QP framework, ensuring control authority over the end-effector while avoiding singularities and joint limits. This real-time optimization enables stable, collision-free motion.\n\nBy integrating these advancements, our framework enhances grasp transition efficiency while ensuring superior motion performance in both grasp selection and execution, effectively addressing key limitations in existing forceful robotic planning approaches.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在动态外部力下实现稳定而高效的双手抓持与无缝转换（uni-manual到bi-manual）？\n2. 如何在抓取转移中兼顾运动性能（可控性、可控度、关节极限）并降低计算成本？\n3. 如何在强力操作场景中实时规划可行路径并避免碰撞，同时优化抓取点选择？\n\n【用了什么创新的方案】\n策略性在抓取流形中采样稳定交点以实现单臂到双臂的平滑转变，并用方向梯度重采样局部调整单臂抓持以保持稳定性。提出多抓取转变检查以寻找共用中间配置，降低冗余重新抓取。建立分层双阶段运动架构：基于模仿学习的全局路径生成器+QP驱动的局部规划器实现实时、避障且高可 manipulability 的路径规划。引入运动性能地图（可行性、可操作性、关节极限接近度）以引导抓取选择并在执行中通过QP约束实现对端执行器的控制权和避免奇异点。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Proactive-reactive detection and mitigation of intermittent faults in robot swarms",
            "authors": "Sinan Oğuz,Emanuele Garone,Marco Dorigo,Mary Katherine Heinrich",
            "subjects": "Robotics (cs.RO); Multiagent Systems (cs.MA); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19246",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19246",
            "arxiv_html_link": "https://arxiv.org/html/2509.19246v1",
            "abstract": "Intermittent faults are transient errors that sporadically appear and disappear. Although intermittent faults pose substantial challenges to reliability and coordination, existing studies of fault tolerance in robot swarms focus instead on permanent faults. One reason for this is that intermittent faults are prohibitively difficult to detect in the fully self-organized ad-hoc networks typical of robot swarms, as their network topologies are transient and often unpredictable. However, in the recently introduced self-organizing nervous systems (SoNS) approach, robot swarms are able to self-organize persistent network structures for the first time, easing the problem of detecting intermittent faults. To address intermittent faults in robot swarms that have persistent networks, we propose a novel proactive–reactive strategy to detection and mitigation, based on self-organized backup layers and distributed consensus in a multiplex network. Proactively, the robots self-organize dynamic backup paths before faults occur, adapting to changes in the primary network topology and the robots’ relative positions. Reactively, robots use one-shot likelihood ratio tests to compare information received along different paths in the multiplex network, enabling early fault detection. Upon detection, communication is temporarily rerouted in a self-organized way, until the detected fault resolves.\nWe validate the approach in representative scenarios of faulty positional data occurring during formation control, demonstrating that intermittent faults are prevented from disrupting convergence to desired formations, with high fault detection accuracy and low rates of false positives.",
            "introduction": "Reliability in networked systems requires consistently accurate information exchange among components, often under dynamic and uncertain conditions [1, 2]. If communication links fail or become unreliable during multi-hop communication, system convergence and performance guarantees can be compromised [3, 4, 5, 6]. In self-organized robot swarms, this challenge is exacerbated by asynchronous ad-hoc communication and decentralized coordination of actuation and decision making. Robots in a self-organized swarm rely solely on local information and communication with nearby robots, without any estimation of the global state of the swarm or its environment, often leading to prolonged convergence times and vulnerability to the spread of incorrect information [7]. Frequent communication between robots can cause faulty information to spread quickly and potentially degrade overall swarm performance or lead to permanent failures.\n\nSelf-organized robot swarms exhibit some inherent fault tolerance, through redundancy and a lack of single points of failure [8, 9].\nHowever, many fault types are not mitigated by this passive tolerance and instead require dedicated mechanisms for detection and mitigation [10, 11, 12, 13]. Somewhat counter-intuitively, self-organized robot swarms are inherently much more tolerant to complete robot failures than to partial ones [14]. For example, a single robot producing faulty or malicious information has been shown to be capable of severe disruption to overall swarm behavior [14, 15]. Faulty robots can also physically obstruct the rest of the swarm, and this interference can paradoxically be worsened by the redundancy that provides swarms with some types of inherent fault tolerance [16].\n\nOther faults to which self-organized robot swarms are vulnerable and which require dedicated mechanisms for detection and mitigation are intermittent faults (IFs). IFs are temporary faults that can appear, disappear, and reappear [17], potentially caused by communication interference, sensor malfunctions, or software bugs [18]. IFs are difficult to detect and diagnose due to their transience [19] and can cause significant disruptions without leaving an easily detectable trace [18].\nA representative example involves intermittent GPS signal degradation in cluttered environments, which can induce sporadic localization errors. These errors propagate through decentralized state estimation protocols, gradually undermining coordination mechanisms without generating explicit failure indicators.\nIn real applications, e.g., in robot swarms deployed in inaccessible or dangerous environments [20, 12], the consequences of IFs to mission performance and to safety can be severe and in some cases could be irreversible. Detecting and resolving IFs before they escalate is key to minimizing disruption: early detection can prevent cascading failures leading to erroneous execution of tasks and can prevent culmination in permanent failures, either of individual robots or the swarm as a whole [21].\n\nIFs are difficult to detect in robot swarms with fully self-organized ad-hoc networks, because the network topology is transient and often unpredictable. IFs are much more straightforward to detect in fully centralized systems and in networks with static structures, for example in sensor networks [22, 23, 24]. However, for multi-robot systems, full centralization and fully static networks also present downsides, such as single points of failure and limited scalability.\n\nOur recently introduced self-organizing nervous systems (SoNS) [25] approach combines aspects of centralization and decentralization through self-organized hierarchy. Using the SoNS approach, robot swarms are coordinated via temporary logical networks that are hierarchical and culminate in a dynamic “brain” robot (i.e., leader), but which are not imposed from the outside, being instead established and maintained in a self-organized manner. This provides robot swarms with persistent and predictable network structures that are more amenable to detecting IFs, without introducing any single points of failure.\nIn short, the SoNS approach allows, for the first time, to apply centralized fault detection and mitigation strategies to robot swarms without sacrificing their oft-cited benefits of scalability, flexibility, and a lack of single points of failure.\n\nSwarm robotics usually studies passive tolerance to permanent faults [26]—that is, faults such as electromechanical failures that will remain unless they are actively repaired.\nWhen relying on passive fault tolerance, studies have usually demonstrated that a swarm continues its mission after some or many robots have failed, either by continuing with fewer robots  [10, 27, 28] or by replacing/repairing the failed robots without pausing the mission [29, 30, 31, 25].\n\nSwarm robotics studies that focus specifically on fault tolerance do not typically rely on passive tolerance, instead developing dedicated mechanisms to handle permanent faults.\nThe majority of these methods detect and react to permanent electromechanical failures after they have occurred [21, 32], often relying on time-out mechanisms in which a robot is considered non-operational if it does not respond to a message within a certain time. Existing methods for detecting permanent faults include LED synchronization [29], simulation comparison [33], shared sensor data analysis [28], and behavioral feature vectors (BFVs) [11].\nThese methods often focus on detection, assuming that once a fault is detected, a repair or other intervention is possible during normal operation (e.g., [34, 29, 21]). Although such repairs might be unrealistic in inaccessible, hazardous, or congested environments [35, 21, 12], future methods for autonomous repair could be developed to complement detection. In short, the existing reactive methods can be considered effective for many types of permanent faults [10].\nHowever, the above-mentioned detection approaches are unlikely to be applicable to the transience of IFs and their long response times [36] would likely be too slow for the early detection and recovery that IFs require. Methods to detect and repair IFs in robot swarms still need to be developed.\n\nTo the best of our knowledge, there are no existing swarm robotics methods focused on IF detection and recovery. Strategies developed for IFs in other types of systems, such as model-based analysis (e.g., discrete-event-system models [37], causal models [38]) and quantitative analysis (e.g., parameter estimation [39], geometric approaches [40], Kalman-like filtering [41]), provide valuable insights but primarily target single-unit systems with static and known system models [24, 42], which is incompatible with self-organized systems such as robot swarms. Likewise, IF strategies developed for sensor networks [22, 23] typically use fully centralized architectures to correct information transmission and reception [24], and are therefore incompatible with self-organized systems.\n\nFurthermore, although fully centralized monitoring is highly effective for detecting and correcting IFs, it can present problems of inflexibility, limited scalability, and single points of failure (e.g., at the point where monitoring is centralized). Fully self-organized approaches, by contrast, would be highly flexible and offer greater scalability and a lack of single points of failure, but would present problems of limited accuracy and potentially slow reaction times.\nIn this paper, we aim to combine elements of each system type to get the benefits of both. Using our proposed proactive–reactive approach, robots can monitor each other using self-organizing hierarchy, detecting IFs accurately and remedying them proactively.\n\nTo demonstrate our proposed proactive–reactive approach, we use the SoNS concept of self-organizing hierarchy in a robot swarm, which has been shown to incorporate temporarily centralized structures into an otherwise self-organized robot swarm without introducing single points of failure or inherently limiting scalability [25, 30, 43, 44, 45, 46]. We build on our recent theoretical foundations for self-organizing hierarchical frameworks: hierarchical Henneberg construction (HHC) [47]. In our previous work [47], we demonstrated HHC for key self-reconfiguration problems (framework merging, robot departure, and framework splitting), derived the mathematical conditions of these problems, and developed algorithms that preserve rigidity and hierarchy using only local information.\n\nIn the remainder of this paper, we assume all graphs are constructed using these already demonstrated HHC algorithms, and refer to such graphs as HHC-constructed graphs. See Appendix A for details on how HHC and SoNS are related.\n\nIn fault tolerance for multi-robot systems, both proactive and reactive mechanisms are important [48].\nIn this paper, we propose a novel proactive–reactive method to detect and mitigate IFs in robot swarms.\nIn the proposed proactive–reactive method, the robots first use distributed consensus to preemptively self-organize dynamic backup communication paths before IFs are detected. Then, the robots compare information received via primary and backup paths to detect IFs, using a one-shot likelihood ratio test. When IFs are detected, the robots react by rerouting communication through the dynamic backup paths. In this paper, we apply the proposed proactive–reactive method to a scenario of intermittently faulty relative positional information within multi-robot formations that have a hierarchical structure towards a fault-free leader, and demonstrate that the method mitigates IFs and robots are able to continue with the desired formations.\n\nThe main technical contributions of this paper can be summarized as follows:\n\nWe address a current gap in robot swarm networking, specifically how to establish back-up communication paths for leader–follower formation control in a self-organized robot swarm. We address this gap by extending the biased minimum consensus (BMC) [49] protocol for shortest path planning in static graphs. We introduce the adaptive biased minimum consensus (ABMC) protocol for dynamic graphs—addressing time-varying topologies, node neighborhoods, and costs. We demonstrate that our ABMC protocol addresses the minimum-cost path problem, with two objectives integrated into a single cost function: to minimize the number of hops to the destination (the leader robot) and to minimize the degree of network congestion (by minimizing the occurrence of parallel edges).We provide the mathematical properties and stability analysis of the ABMC protocol as a distributed consensus mechanism in dynamic graphs with piecewise constancy, including providing the necessary and sufficient conditions to uniquely determine an equilibrium point representing a minimum-cost backup path.\n\nWe address a current gap in robot swarm fault tolerance, specifically tolerance against intermittent faults (IFs). We address this gap by proposing a novel proactive–reactive fault-tolerance strategy for detection and mitigation of IFs in robot swarms. Our proposed strategy uses the ABMC protocol to construct backup network layers and combines it with a distributed likelihood ratio (LR) protocol to dynamically reroute traffic in the constructed multiplex network. We propose the mathematical conditions and design the distributed algorithms for backup layer construction and for execution of the proactive–reactive strategy for IF detection and mitigation. We also provide the time and space complexity and efficiency properties of both distributed algorithms. Finally, we demonstrate the proactive–reactive fault-tolerance strategy in formations of 20 robots with moving leaders.\n\nThe rest of the paper is organized as follows. In Sec. II, the foundational concepts regarding hierarchical frameworks are presented, along with the existing BMC protocol. In Sec. III, we formulate three key problems addressed in this paper: construction of dynamic minimum-cost backup paths, detection of IFs using the constructed backup paths, and mitigation of the detected IFs using the constructed backup paths. The first problem is addressed in Secs. IV and V, and the second and third problem are addressed in Sec. VI. Finally, in Sec. VII we validate our contributions in experiments of representative scenarios. The conclusions are summarized in Sec. VIII.",
            "llm_summary": "【关注的是什么问题】\n1. Intermittent faults (IFs) in self-organized robot swarms and their impact on formation convergence and reliability\n2. 如何在自组织机器人群体中检测与缓解IFs，兼顾可扩展性与无单点故障\n3. 在动态拓扑中实现备份路径的自组织构建与快速故障检测\n4. 将集中式检测策略与去中心化群体控制结合的主动-被动（proactive–reactive）两层防护\n\n【用了什么创新的方案】\n核心解决方案：提出主动-被动（proactive–reactive）IF检测与缓解框架，利用自组织层次化的备份通信路径实现多路径冗余；通过自组织的ABMC协议在动态图构建最小代价备份路径，并使用分布式一次性似然比检验比较主路径与备份路径的信息以实现早期IF检测；IF检测后，故事通路通过自组织方式重新路由通信直至故障消失；在20机器人队形实验中验证方法提升鲁棒性与收敛性。并将ABMC与LR检验结合，给出时空复杂度分析与稳定性条件。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap",
            "authors": "Tianyu Wu,Xudong Han,Haoran Sun,Zishang Zhang,Bangchao Huang,Chaoyang Song,Fang Wan",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 4 figures, accepted to Data@CoRL2025 Workshop",
            "pdf_link": "https://arxiv.org/pdf/2509.19169",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19169",
            "arxiv_html_link": "https://arxiv.org/html/2509.19169v1",
            "abstract": "The transfer of manipulation skills from human demonstration to robotic execution is often hindered by a “domain gap” in sensing and morphology. This paper introduces MagiClaw, a versatile two-finger end-effector designed to bridge this gap. MagiClaw functions interchangeably as both a handheld tool for intuitive data collection and a robotic end-effector for policy deployment, ensuring hardware consistency and reliability. Each finger incorporates a Soft Polyhedral Network (SPN) with an embedded camera, enabling vision-based estimation of 6-DoF forces and contact deformation. This proprioceptive data is fused with exteroceptive environmental sensing from an integrated iPhone, which provides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS application, MagiClaw streams synchronized, multi-modal data for real-time teleoperation, offline policy learning, and immersive control via mixed-reality interfaces. We demonstrate how this unified system architecture lowers the barrier to collecting high-fidelity, contact-rich datasets and accelerates the development of generalizable manipulation policies. Please refer to the iOS app at https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.",
            "introduction": "The success of modern robot learning paradigms, from Learning from Demonstration (LfD) [1, 2] to offline reinforcement learning, is fundamentally dependent on the quality and richness of the underlying data [3]. For contact-rich manipulation tasks, robust policies require more than just kinematic trajectories; they demand a holistic understanding of interaction forces, tactile feedback, and environmental context [4, 5]. Consider a human deftly handling a delicate object: the action is a symphony of precise motion, modulated forces, and continuous tactile adjustments [6]. Replicating such skills requires capturing this multi-modal information stream in its entirety.\n\nHowever, existing data collection methodologies present significant challenges. First, they often rely on a patchwork of disparate, expensive sensors—such as external motion capture systems, wrist-mounted force/torque sensors, and complex tactile skins [7, 8]—resulting in cumbersome and costly setups. This high barrier to entry limits the scale and diversity of data collection efforts [9]. Second, and more critically, a persistent domain gap exists between the human demonstrator and the robotic learner [10]. Data is often collected using one set of hardware (e.g., an instrumented glove) and deployed on a robot with entirely different sensor suites and end-effector morphology. This mismatch necessitates complex domain adaptation techniques and is a primary reason why policies trained on demonstration data often fail to generalize to physical hardware [11].\n\nTo address these challenges, we present MagiClaw, a unified hardware platform designed to seamlessly bridge the gap from human demonstration to robotic deployment. MagiClaw is a dual-purpose, two-fingered gripper that merges three key innovations:\n\nUnified Hardware Form Factor: The exact same MagiClaw device can be used as a hand-held tool for human demonstration or mounted on a robot arm for autonomous execution. This hardware consistency minimizes the sensor and morphological domain gap, facilitating direct policy transfer.\n\nVision-Based Proprioceptive Fingertips: Each finger integrates a Soft Polyhedral Network (SPN) [12] with an embedded miniature camera. This novel design enables visuotactile perception, inferring 6-DoF forces, torque, and high-resolution contact deformation from the distortion of the internal lattice structure, thereby obviating the need for costly external force sensors.\n\nIntegrated Multi-Modal Exteroception: An attached iPhone leverages its powerful sensor suite (LiDAR, RGB cameras, IMU) and ARKit framework [13] to provide synchronized, rich environmental context, including gripper pose, depth maps, and high-resolution video.\n\nOur primary contribution is an integrated system that fundamentally streamlines the collection of holistic, contact-centric data for robot learning. By fusing proprioceptive force/tactile data from the fingertips with exteroceptive visual and spatial data from a commodity smartphone, MagiClaw offers a low-cost, powerful, and user-friendly solution for both teleoperation and autonomous policy development. We posit that by democratizing access to such high-fidelity, multi-modal data, MagiClaw can serve as a catalyst for developing more robust and generalizable manipulation skills, advancing the pursuit of universal action embodiment in robotics.",
            "llm_summary": "【关注的是什么问题】\n1. 领域差距导致的人类示范到机器人执行的迁移困难（≤40词）\n2. 需要高质量多模态数据但现有传感器系统昂贵、零散、耦合度高的问题（≤40词）\n3. 如何在同一硬件上实现手持示范与机器人执行之间的无缝切换（≤40词）\n4. 如何通过 visuotactile 与环境感知实现对力、接触变形的高保真推断（≤40词）\n5. 如何降低数据采集成本并提高数据多样性以提升策略泛化能力（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：MagiClaw 将两指软聚合网格内嵌相机实现视觉-本体感知的六自由度力与接触变形估计；每指作為 SPN 传感器；通过 iPhone 的 LiDAR、RGB、深度数据进行同步的外感知；手持与机器人端可互换的统一硬件形式；自带 iOS 应用实现实时远控、离线学习数据流；降低数据采集成本、提升多模态数据质量与一致性。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination",
            "authors": "Mark Gonzales,Ethan Oh,Joseph Moore",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 Pages, 7 Figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19168",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19168",
            "arxiv_html_link": "https://arxiv.org/html/2509.19168v1",
            "abstract": "In this paper, we present a receding-horizon, sampling-based planner capable of reasoning over multimodal policy distributions. By using the cross-entropy method to optimize a multimodal policy under a common cost function, our approach increases robustness against local minima and promotes effective exploration of the solution space. We show that our approach naturally extends to multi-robot collision-free planning, enables agents to share diverse candidate policies to avoid deadlocks, and allows teams to minimize a global objective without incurring the computational complexity of centralized optimization. Numerical simulations demonstrate that employing multiple modes significantly improves success rates in trap environments and in multi-robot collision avoidance. Hardware experiments further validate the approach’s real-time feasibility and practical performance.",
            "introduction": "Local minima pose a fundamental challenge for finite-horizon, gradient-based planning approaches. In multi-robot scenarios, local minima can arise not only from the environment but also from dynamic factors, such as the changing trajectories of teammates, which may inadvertently block or cut off routes that would otherwise be viable. These pitfalls often cause robots to become stuck, find suboptimal solutions, or fail to coordinate effectively in complex environments.\n\nSampling-based planners, such as Model Predictive Path Integral (MPPI) [1] and Cross-Entropy Method (CEM) [2, 3], attempt to improve the trajectory cost by stochastically sampling and evaluating trajectories in the cost landscape. In practice, these methods utilize hyperparameters, such as sampling variance, number of samples, and the horizon length, to adapt the exploration to the environment. However, both MPPI and CEM typically sample trajectories around the prior best policy, leading to a concentration of samples in a narrow region of the solution space. This localized search impedes the planner’s ability to effectively navigate around traps or escape from local minima once they occur, especially in environments with challenging topology. As a result, the planner can become stuck in suboptimal regions, regardless of the variance or adaptation strategy.\n\nIn multi-robot systems, the difficulty is exacerbated by the need for robots to coordinate planned trajectories. Centralized control approaches [4, 5, 6, 7] can, in principle, achieve globally optimal coordination; however, they suffer from scalability issues and high computational costs as the team size increases. Distributed methods, while scalable, often require robots to individually select their optimal trajectory, subsequently negotiating with teammates to reach a feasible consensus. When each robot contributes only a single candidate trajectory, the team risks deadlock or persistent local minima, as a lack of trajectory diversity reduces the likelihood of discovering collision-free, cooperative maneuvers, especially when teammates dynamically update their plans or block each other’s routes in real-time.\n\nTo overcome these limitations, we introduce a multimodal sampling and clustering framework that maintains multiple policy candidates for each robot, thereby increasing diversity and robustness against local minima in both environmental and collaborative planning contexts.\n\nOur contributions are:\n\nA cross-entropy planning approach capable of preserving multiple policy modes for increased planning robustness.\n\nA multi-robot coordination framework that enables reasoning about sets of candidate policies to avoid local minima and deadlocks more reliably.\n\n1. A cross-entropy planning approach capable of preserving multiple policy modes for increased planning robustness.\n\n2. A multi-robot coordination framework that enables reasoning about sets of candidate policies to avoid local minima and deadlocks more reliably.",
            "llm_summary": "【关注的是什么问题】\n1. 本地最优陷阱与多模态政策在单机器人导航中的鲁棒性（≤40词）\n2. 多机器人协同中的死锁、协调与全局目标优化的可扩展性（≤40词）\n3. 受限搜索空间中多模态策略对避障与探索的提升（≤40词）\n\n【用了什么创新的方案】\n- 引入跨熵法的多模态策略规划，保留多条候选策略以提升对局部极小值的鲁棒性。\n- 基于采样与聚类的框架，在多机器人场景中共享多策略集合，避免死锁并实现分布式协同。\n- 通过 receding-horizon 与多模态分布采样实现对全局成本的高效优化，降低集中化计算开销。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer",
            "authors": "Kangmin Kim,Seunghyeok Back,Geonhyup Lee,Sangbeom Lee,Sangjun Noh,Kyoobin Lee",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 5 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19142",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19142",
            "arxiv_html_link": "https://arxiv.org/html/2509.19142v1",
            "abstract": "Bimanual grasping is essential for robots to handle large and complex objects. However, existing methods either focus solely on single-arm grasping or employ separate grasp generation and bimanual evaluation stages, leading to coordination problems including collision risks and unbalanced force distribution. To address these limitations, we propose BiGraspFormer, a unified end-to-end transformer framework that directly generates coordinated bimanual grasps from object point clouds. Our key idea is the Single-Guided Bimanual (SGB) strategy, which first generates diverse single grasp candidates using a transformer decoder, then leverages their learned features through specialized attention mechanisms to jointly predict bimanual poses and quality scores. This conditioning strategy reduces the complexity of the 12-DoF search space while ensuring coordinated bimanual manipulation. Comprehensive simulation experiments and real-world validation demonstrate that BiGraspFormer consistently outperforms existing methods while maintaining efficient inference speed (<<0.05s), confirming the effectiveness of our framework. Code and supplementary materials are available at https://sites.google.com/bigraspformer",
            "introduction": "Bimanual grasping enables robots to manipulate large, heavy, or unwieldy objects beyond single-arm capabilities, making it essential for tasks such as lifting furniture, carrying long boards, or moving large boxes [1, 2]. However, most robotic grasping research has focused on single-arm systems, primarily on learning to detect 6-DoF grasp poses from point clouds [3, 4, 5, 6, 7, 8]. While effective for single-arm tasks, these approaches cannot be directly extended to bimanual scenarios. First, bimanual grasping expands the action space to 12-DoF, doubling the computational complexity. Second, it introduces new challenges, including collision avoidance, balanced force/torque distribution, and dual-arm coordination for post-grasp manipulation.\n\nFor bimanual grasping, only a few methods have been proposed so far. The DA2 dataset [9] introduced the first benchmark by extending single-arm datasets [10, 11, 3] with dual-arm-specific metrics such as force closure, dexterity, and torque balance [9, 12]. However, most existing approaches adopt modular architectures that separate grasp generation and evaluation. For example, Dual-PointNetGPD [9] evaluates the quality of grasp pairs from given candidates, requiring external single-arm grasp generators. Similarly, CGDF [13] directly generates bimanual grasps but lacks integrated quality prediction, instead relying on additional scoring modules or heuristic pairing strategies [14, 15]. As a result, current methods yield limited diversity, poor coordination, and high computation due to modular pipelines.\n\nIn this paper, we propose BiGraspFormer, the first unified end-to-end framework that directly generates coordinated bimanual grasps from object point clouds (Fig. 1). The key insight is that single-grasp features can effectively guide bimanual grasp generation, rather than treating dual-arm coordination as two independent problems. BiGraspFormer introduces a novel Single-Guided Bimanual (SGB) strategy: it first generates diverse single-arm grasp candidates, then leverages their learned features through specialized attention mechanisms to jointly predict bimanual poses and quality scores. This unified approach eliminates separate modules and explicitly models coordination between grasps, enabling stable and efficient dual-arm manipulation. Comprehensive experiments in both simulation and real-world environments demonstrate that BiGraspFormer achieves superior success, diversity, and speed compared to existing methods.\n\nOur contributions are summarized as follows:\n\nWe propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\nWe propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\nWe introduce the Single-Guided Bimanual (SGB) strategy, which leverages single-arm grasp features to guide bimanual generation, reducing computational complexity and enhancing dual-arm coordination.\n\nWe achieve state-of-the-art bimanual grasping performance while maintaining fast inference suitable for real-world deployment.\n\n1. We propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\n2. We introduce the Single-Guided Bimanual (SGB) strategy, which leverages single-arm grasp features to guide bimanual generation, reducing computational complexity and enhancing dual-arm coordination.\n\n3. We achieve state-of-the-art bimanual grasping performance while maintaining fast inference suitable for real-world deployment.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在点云上直接生成协调的双臂抓取姿态（12-DoF）以实现稳定的双臂物体操作\n2. 现有方法的模块化导致双臂协作差、碰撞风险与力分配不均等问题\n3. 如何在端到端框架中有效建模双臂协调并提高推理速度\n\n【用了什么创新的方案】\nSingle-Guided Bimanual (SGB) 策略：先通过变换器解码器生成多样的单臂抓取候选，再利用这些单臂抓取的特征，通过专门的注意力机制共同预测双臂抓取的姿态与质量分数，从而端到端地产生协调的双臂抓取。该统一框架 eliminates 分离的抓取生成与评估模块，直接从对象点云进行协调抓取的生成与评分。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation",
            "authors": "Sarvesh Prajapati,Ananya Trivedi,Nathaniel Hanson,Bruce Maxwell,Taskin Padir",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 10 figures, submitted to Robotic Computing & Communication",
            "pdf_link": "https://arxiv.org/pdf/2509.19105",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19105",
            "arxiv_html_link": "https://arxiv.org/html/2509.19105v1",
            "abstract": "Successful navigation in outdoor environments requires accurate prediction of the physical interactions between the robot and the terrain. To this end, several methods rely on geometric or semantic labels to classify traversable surfaces. However, such labels cannot distinguish visually similar surfaces that differ in material properties. Spectral sensors enable inference of material composition from surface reflectance measured across multiple wavelength bands. Although spectral sensing is gaining traction in robotics, widespread deployment remains constrained by the need for custom hardware integration, high sensor costs, and compute-intensive processing pipelines. In this paper, we present RGB Image to Spectral Signature Neural Network (RS-Net), a deep neural network designed to bridge the gap between the accessibility of RGB sensing and the rich material information provided by spectral data. RS-Net predicts spectral signatures from RGB patches, which we map to terrain labels and friction coefficients. The resulting terrain classifications are integrated into a sampling-based motion planner for a wheeled robot operating in outdoor environments. Likewise, the friction estimates are incorporated into a contact-force–based MPC for a quadruped robot navigating slippery surfaces. Thus, we introduce a framework that learns the task-relevant physical property once during training and thereafter relies solely on RGB sensing at test time. The code is available at https://github.com/prajapatisarvesh/RS-Net.",
            "introduction": "Autonomous robots are increasingly deployed in everyday settings, ranging from self-driving taxis [1] and search-and-rescue missions [2, 3] to wildfire prevention [4, 5]. In such unstructured environments, reliable autonomy demands more than obstacle avoidance. It requires precise reasoning about how terrain properties influence motion. For example, vehicles must modulate braking on icy roads, and off-road platforms should bypass dense swamps to avoid entrapment. These scenarios show that perception must move beyond geometry and semantics toward reliable estimates of robot–terrain interactions.\n\nSeveral off-road motion planning pipelines use RGB cameras to identify terrain from images [6, 7, 8]. In some cases, visually similar surfaces with different physical properties, such as ice on asphalt, may be mislabeled, leading to invalid traversability cost maps. Depth cameras and LiDAR are often used to estimate the ease of motion over a surface [9, 10]. However, the robot must first drive the terrain to create a dataset, which risks hardware damage and necessitates tuning specific to the operating site.\n\nIn contrast, spectral sensors offer a non-invasive way to estimate material properties. This is accomplished by leveraging distinct patterns of light absorption and reflection, known as spectral signatures, to characterize underlying material composition. These capabilities are finding use in robotics applications such as wildfire risk monitoring [3], manipulation [11, 12], and exploration [13]. By mapping spectral signatures to physical quantities such as moisture content, rigidity, or surface type, the same sensing stack can be repurposed across robots and environments with minimal changes to the processing pipeline. However, challenges such as custom mounts, calibration requirements, large datasets, and high sensor costs currently limit deployment at scale.\n\nRGB cameras are inexpensive, widely available, and already standard in robotic perception pipelines such as object detection [14] and tracking [15]. Compared to hyperspectral systems, they are cheaper, lighter, and more power efficient, which simplifies integration on mobile platforms. Advances in deep learning [16] now allow RGB imagery to approximate measurements traditionally obtained from more information-dense sensors.\n\nMotivated by this, we seek to retain the deployment advantages of RGB cameras while recovering spectral sensor features. We introduce RGB Image to Spectral Signature Neural Network or RS-Net, a deep neural network architecture trained on spectral data collected from diverse materials. It maps RGB image patches to their corresponding spectral signatures. These estimates are passed to a lightweight feedforward neural network whose weights are fine-tuned for the target physical property. We retrain this network once per task, enabling the same neural network architecture to perform terrain classification and friction estimation. Our entire inference pipeline runs at approximately 5 Hz, making it suitable for real-time robotic applications. Fig. LABEL:fig:paper_intro outlines the proposed architecture.\n\nWe validate our method in both simulation and hardware experiments. The terrain classification is used in a sampling-based motion planner for outdoor navigation of a skid-steer robot. Similarly, the friction estimates are integrated into a model predictive control (MPC) scheme for a quadrupedal robot operating on slippery surfaces. Finally, we also discuss how the proposed approach generalizes to other robots and additional physical properties relevant to off-road planning.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在不增加硬件成本的情况下，通过RGB图像推断材料光谱特征以更准确地评估地形与摩擦\n2. how to bridge RGB sensing with spectral properties to improve terrain traversability prediction and motion planning\n3. 将光谱特征转化为可用于采样式路径规划和MPC的物理属性\n4. 实时性与通用性：在多种环境与机器人上实现低成本、快速推断的框架\n\n【用了什么创新的方案】\nRS-Net 将 RGB patch 映射到光谱签名，在此基础上训练一个轻量前馈网络输出地形标签和摩擦系数；整个管线在测试时仅需 RGB，推理约 5 Hz；通过在训练阶段明确学习任务相关的物理属性，使同一架构可一次性对不同任务进行微调并发布到实际机器人上。代码开放，支持仿真与硬件实验中的地形导航和滑移表面的 MPC/规划集成。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation",
            "authors": "Hongli Xu,Lei Zhang,Xiaoyue Hu,Boyang Zhong,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "project website:this https URL, 11 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.19102",
            "code": "https://sites.google.com/view/funcanon",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19102",
            "arxiv_html_link": "https://arxiv.org/html/2509.19102v1",
            "abstract": "General-purpose robotic skills from end-to-end demonstrations often leads to task-specific policies that fail to generalize beyond the training distribution.\nTherefore, we introduce FunCanon, a framework that converts long-horizon manipulation tasks into sequences of action chunks, each defined by an actor, verb, and object.\nThese chunks focus policy learning on the actions themselves, rather than isolated tasks, enabling compositionality and reuse. To make policies pose-aware and category-general, we perform functional object canonicalization for functional alignment and automatic manipulation trajectory transfer, mapping objects into shared functional frames using affordance cues from large vision–language models.\nAn object-centric and action-centric diffusion policy FuncDiffuser trained on this aligned data naturally respects object affordances and poses, simplifying learning and improving generalization ability.\nExperiments on simulated and real-world benchmarks demonstrate category-level generalization, cross-task behavior reuse, and robust sim-to-real deployment, showing that functional canonicalization provides a strong inductive bias for scalable imitation learning in complex manipulation domains.\nDetails of the demo and supplemental material are available on our project website\n https://sites.google.com/view/funcanon.",
            "introduction": "As robots transition from controlled laboratory settings to unstructured real-world environments, developing robust and generalizable manipulation policies becomes increasingly critical. A fundamental challenge is enabling agents to generalize across unseen objects, diverse poses, and varying tasks — a capability that remains elusive for current imitation learning approaches.\n\nImitation learning methods based on RGB images [1] or point clouds [2] often suffer from limited precision and generalization due to viewpoint sensitivity, noisy observations, and redundant scene encodings. In contrast, 3D scene representations have shown promise in improving generalization [3]. To further address these challenges, object-centric representations, which focus on structured, object-level information such as 6D poses [4, 5] and scene flow [6], have gained significant attention. SPOT [4] demonstrates that SE(3) pose diffusion policy can improve cross-embodiment generalization, even when trained solely on passive human videos.\nExisting object-centric approaches [4] often depend on instance-specific, goal-conditioned trajectories, limiting generalization across categories and tasks. We attribute this to viewing manipulation as monolithic programs rather than modular, reusable behaviors.\nIn the field of computer vision, prior work, such as UAD [7] and Object Canonicalization [8], mainly targets improving visual representations and semantic understanding. Related efforts have also investigated category-level affordance pose estimation [9]. However, these methods have not explored how such representations can be leveraged for improving category-level alignment of robotic manipulation data, or augmenting manipulation trajectories.\n\nKey open questions remain: how can generalized representations be leveraged to synthesize diverse manipulation data, model long-horizon tasks, and train robust, generalizable manipulation policies?\n\nTo address these challenges, we propose FunCanon, a framework that models manipulation as compositions of reusable action primitives—such as pouring, grasping, or inserting—defined over functionally aligned bi-object interactions. By leveraging affordance cues from large vision-language models (VLMs), FunCanon canonicalizes semantically related objects (e.g., kettles and pitchers) into shared functional frames. This functional alignment enables the automatic manipulation trajectory transfer and the training of pose-aware, object-centric diffusion policies that focus solely on interaction dynamics, decoupled from specific object identities, camera viewpoints, or task semantics. To achieve robust and generalizable manipulation, we first decompose long-horizon tasks into meaningful action chunks, each specifying an actor, an action, and an object. This segmentation is performed by a large multimodal language model (MLLM), such as GPT-4o, in combination with large vision models (LVM) that extract affordance cues to guide chunking based on functional relevance. Next, we integrate these affordance cues with precise object pose estimates to perform functional alignment, canonicalizing objects into shared functional frames. This process identifies action-related affordance regions and aligns bi-object poses, producing a semantically grounded representation of manipulation interactions. Leveraging this functional alignment, automatic trajectory transfer method is proposed to augment training data on RLBench base tasks to increase data diversity and functional coverage. During policy training and inference, our object-centric diffusion policy receives inputs encoding both affordances, poses of bi-object pairs, point clouds and action verb and estimate actions.\nOur main contributions are:\n\nIntroducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\nIntroducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\nLeveraging large vision-language models for affordance-driven functional canonicalization for functional alignment and automatic trajectory transfer, enabling pose-aware and category-generalizable policy learning.\n\nDeveloping an object-centric diffusion policy trained on functionally aligned data, achieving both instance-level and category-level generalization and robust sim-to-real transfer.\n\n1. Introducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\n2. Leveraging large vision-language models for affordance-driven functional canonicalization for functional alignment and automatic trajectory transfer, enabling pose-aware and category-generalizable policy learning.\n\n3. Developing an object-centric diffusion policy trained on functionally aligned data, achieving both instance-level and category-level generalization and robust sim-to-real transfer.",
            "llm_summary": "【关注的是什么问题】\n1. 如何将长时程操纵任务分解为可重复使用的动作原语以提高泛化性\n2. 如何利用功能对齐和对象-功能框架实现从示例到通用策略的迁移\n3. 如何在无关对象身份、视角和任务语义的前提下实现姿态感知的策略学习\n4. 如何通过对齐数据训练对象中心的扩散策略以增强实例级和类别级泛化能力\n\n【用了什么创新的方案】\n- 将长时程操作分解为由行为者-动作-对象组成的可重用动作原语，并在功能对齐的双对象框架内进行分段\n- 通过大视觉语言模型的 affordance 提供和功能 canonicalization 将相关对象映射到共享功能框架，实现自动轨迹迁移\n- 基于对齐数据训练对象中心的扩散策略 FuncDiffuser，使策略在对象姿态、可供性和交互动力学上具备姿态感知能力并实现良好跨域泛化\n- 结合多模态分段（MLLM/ LVM）与功能对齐，进行自动化的轨迹迁移和增强数据多样性\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation",
            "authors": "Zhennan Jiang,Kai Liu,Yuxin Qin,Shuai Tian,Yupeng Zheng,Mingcai Zhou,Chao Yu,Haoran Li,Dongbin Zhao",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19080",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19080",
            "arxiv_html_link": "https://arxiv.org/html/2509.19080v1",
            "abstract": "Robotic manipulation policies are commonly initialized through imitation learning, but their performance is limited by the scarcity and narrow coverage of expert data. Reinforcement learning can refine polices to alleviate this limitation, yet real-robot training is costly and unsafe, while training in simulators suffers from the sim-to-real gap. Recent advances in generative models have demonstrated remarkable capabilities in real-world simulation, with diffusion models in particular excelling at generation. This raises the question of how diffusion model-based world models can be combined to enhance pre-trained policies in robotic manipulation. In this work, we propose World4RL, a framework that employs diffusion-based world models as high-fidelity simulators to refine pre-trained policies entirely in imagined environments for robotic manipulation. Unlike prior works that primarily employ world models for planning, our framework enables direct end-to-end policy optimization. World4RL is designed around two principles: pre-training a diffusion world model that captures diverse dynamics on multi-task datasets and refining policies entirely within a frozen world model to avoid online real-world interactions. We further design a two-hot action encoding scheme tailored for robotic manipulation and adopt diffusion backbones to improve modeling fidelity. Extensive simulation and real-world experiments demonstrate that World4RL provides high-fidelity environment modeling and enables consistent policy refinement, yielding significantly higher success rates compared to imitation learning and other baselines. More visualization results are available at https://world4rl.github.io/.",
            "introduction": "Despite recent progress in robotic manipulation, the field still faces critical challenges for practical deployment. Imitation learning is widely used to bootstrap policies from demonstrations, but its effectiveness is constrained by the inconsistency[1] and limited diversity[2, 3, 4] of available datasets. Although offline reinforcement learning (RL) can extract better policies from imperfect data, its susceptibility to overestimation[5] still makes it difficult to work effectively with limited datasets. Online RL offers a natural way to refine such pre-trained policies through interaction. However, real-robot RL, while capable of overcoming dataset limitations, suffers from high interaction costs and significant safety risks that hinder large-scale training. Training in simulation avoids these risks but inevitably introduces discrepancies from real-world physics, leading to a persistent sim-to-real gap[6].\n\nIn recent years, generative models have achieved remarkable progress in the visual domain[7], with diffusion models[8] demonstrating particularly strong performance in image[9] and video generation[10, 11]. Such generative capacity opens new opportunities for modeling complex and dynamic environments, offering a promising path toward learnable world simulators that provide realistic yet flexible environments for RL training in robotic manipulation.\n\nBuilding on this idea, we introduce World4RL, a framework that systematically integrates diffusion world models into RL for robotic manipulation. World4RL follows a two-stage paradigm: we first pre-train a diffusion world model on multi-task datasets to capture diverse dynamics, and then refine policies entirely within the frozen model to avoid costly and unsafe online interactions. Serving as a high-fidelity simulator, the world model is composed of a diffusion transition model that predicts future observations conditioned on current observations and actions, and a reward classifier that provides sparse success signals, enabling policy optimization without real-world rollouts.\n\nThis design of framework contrasts with prior approaches such as IRASim[12] and NWM[13], which primarily use generative video models for planning at test time rather than for direct policy training. A closer line of work, DiWA[14], also employs world models for policy learning. However, it relies on recurrent state-space models (RSSM[15]), which lead to blurry generations and compounding rollout errors. In contrast, World4RL leverages diffusion backbones that generate sharper and temporally coherent rollouts, thereby supporting effective end-to-end reinforcement learning.\n\nTo further adapt world models to robotic manipulation, which involves high-dimensional action spaces and complex environment interactions compared to navigation[13] and games[16], we investigate two critical design choices: a two-hot action encoding[17] scheme that provides an efficient representation of continuous actions while enabling lossless reconstruction, thereby serving as a robust bridge between the RL agent and the world model, and diffusion backbone architectures that determine the fidelity and consistency of predictions. These considerations are essential for enabling diffusion world models to serve not only as visual predictors but also as reliable simulators for policy training. To this end, our work makes the following key contributions.\n\nWe propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\nWe propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\nTo improve modeling fidelity and enable more effective policy refinement, we design a two-hot action encoding tailored for robotic manipulation and adopt a diffusion backbone as the world model.\n\nWe validate the effectiveness of World4RL through extensive experiments, showing that it consistently outperforms competitive baselines and significantly enhances policy refinement, improving success rates by 16% and 25% in simulation and real-robot experiments, respectively.\n\n1. We propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\n2. To improve modeling fidelity and enable more effective policy refinement, we design a two-hot action encoding tailored for robotic manipulation and adopt a diffusion backbone as the world model.\n\n3. We validate the effectiveness of World4RL through extensive experiments, showing that it consistently outperforms competitive baselines and significantly enhances policy refinement, improving success rates by 16% and 25% in simulation and real-robot experiments, respectively.",
            "llm_summary": "【关注的是什么问题】\n1. 如何利用扩散模型构建高保真世界模型以提升机器人操控策略的学习效率与安全性（≤40词）\n2. 如何在不在线真实交互的前提下，通过在冻结的扩散世界模型中对策略进行端到端优化来实现改进（≤40词）\n3. 如何设计适合高维持续动作空间的两热编码以及扩散骨干网络以提升生成的时序一致性与预测保真度（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出 World4RL，将扩散式世界模型作为高保真仿真器用于策略的离线强化学习训练；先在多任务数据集上预训练扩散世界模型，再在冻结的模型中进行端到端策略优化；使用两热动作编码和扩散骨干来提升建模保真度与鲁棒性；通过扩散转移模型预测未来观测并结合奖励分类器提供稀疏成功信号，实现无真实滚动的策略 refinement。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions",
            "authors": "Laura Connolly,Aravind S. Kumar,Kapi Ketan Mehta,Lidia Al-Zogbi,Peter Kazanzides,Parvin Mousavi,Gabor Fichtinger,Axel Krieger,Junichi Tokuda,Russell H. Taylor,Simon Leonard,Anton Deguet",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19076",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19076",
            "arxiv_html_link": "https://arxiv.org/html/2509.19076v1",
            "abstract": "Image-guided robotic interventions involve the use of medical imaging in tandem with robotics. SlicerROS2 is a software module that combines 3D Slicer and robot operating system (ROS) in pursuit of a standard integration approach for medical robotics research. The first release of SlicerROS2 demonstrated the feasibility of using the C++ API from 3D Slicer and ROS to load and visualize robots in real time. Since this initial release, we’ve rewritten and redesigned the module to offer greater modularity, access to low-level features, access to 3D Slicer’s Python API, and better data transfer protocols. In this paper, we introduce this new design as well as four applications that leverage the core functionalities of SlicerROS2 in realistic image-guided robotics scenarios.",
            "introduction": "Medical robotics is an evolving and rapidly growing research field with the potential to transform standard clinical practice. It is possible that robots will one day transcend human capabilities while offering higher efficiency, lower costs, improved training outcomes and better safety [1]. The advancement of image-guided robotics in particular, which are systems that rely on both medical imaging and robotics, is critical for achieving this potential. This is because image-guided robots can be used to fuse preoperative and intraoperative realities [2].\n\nThere are several combinations of imaging modalities and robotic systems have been explored in this capacity. For example, the SpineBot uses computed tomography (CT) imaging to help define the trajectory of pedicle screws, and robotics to guide the surgeon through those trajectories [3], [4]. Another example is the MrBot, which was designed to help perform percutaneous needle interventions within the confines of a magnetic resonance imaging (MRI) scanner [5]. Similarly, the Artemis robot was designed to facilitate transrectal prostate biopsy under ultrasound guidance with MRI fusion [6]. These are just a few examples of the numerous procedures and therapies where the use of image-guidance in tandem with robotics has been investigated. More recently, advancements in image-guided robotics have enabled: navigation of catheters into blood vessels with magnetic continuum devices [7], autonomous needle steering for lung biopsy [8] and teleoperated neurovascular interventions [9].\n\nDespite this extensive investigation, there are only a few areas of intervention where image-guided robotic systems that have achieved widespread adoption and financial commercial success such as robotic bronchoscopy, radiation oncology and neurosurgery [2] [10]. One contributor to this slow growth and adoption is the lack of a common integration approach. For any image-guided robotic system, integration of the imaging modality and the robot is the most important factor for usability. However, several companies and research systems take their own unique approach to integration. This results in device-specific software, expensive research licenses, incompatible communication protocols, and overall, a high barrier to entry to develop such systems. Considering these challenges and their potential threat to continued development, it is imperative to provide a common integration scheme for image-guided robotics. We hypothesize that this will prevent re-engineering and promote reproducibility across different clinical applications.\n\nFrom a development perspective, image-guided therapy (IGT) platforms and medical robotics platforms are often separated. In the realm of IGT, the open-source medical imaging platform, 3D Slicer is the most commonly used research platform [11] [12]. With over one million downloads and an active research and support community, 3D Slicer is used for segmentation, virtual reality, image analysis, artificial intelligence, and several other applications [13]. Several research platforms have been enabled by or added to 3D Slicer such as SlicerIGT [12], SlicerVR [14], MONAI label [15], and Total Segmentator [16]. There are also open-source platforms like OpenIGTLink and the PLUS toolkit that allow users to interface commercial hardware with 3D Slicer to build complete IGT systems [17] [18]. As a result of these efforts, 3D Slicer is considered the de-facto open source software for developing navigated, image-guided interventions.\n\nIn the field of medical robotics, robot operating system (ROS), an open-source middleware designed to support robotics development, is the predominant framework. ROS is a modular development framework that provides tools for autonomous navigation, simulation, visualization, and control [19]. Like 3D Slicer, ROS has a very active research community that is constantly contributing to the platform. For medical robotics specifically, several research tools like the Computer Integrated Surgical Systems Surgical Assistant Workstation (CISST-SAW) libraries [20] and the Asynchronous Multi-Body Framework (AMBF) [21] support ROS. The da Vinci Research Kit (dVRK), a popular open-source medical robot [22], also supports ROS for software development. Furthermore, many commercially available robots provide a ROS interface off the shelf.\n\nIn an effort to pursue a common integration scheme for image-guided robotics research, we decided to bridge these two ecosystems. As evidenced by the numerous published papers that employ both 3D Slicer and ROS (71 papers available on Google Scholar using keywords “3D Slicer” AND “robot operating system”), there is also demand from the community for this integration. Previous attempts to bridge 3D Slicer and ROS such as the ROS-IGTL bridge [23], custom applications for specific robots [24] [25], and our initial offering of SlicerROS2 [26] have fallen short of meeting all of the needs of a common integration tool. These needs include greater usability by providing access to low-level features, robust data transfer protocols that support commonly used message types, thorough documentation, and maintainability [27]. We have since redesigned SlicerROS2 to further support image-guided robotics research considering these requirements. The details of this new design are described in the following sections. The contributions of this paper are: 1) A newly designed research module for efficient data transfer between 3D Slicer and ROS 2 and 2) four relevant applications that demonstrate how it can be used for rapid research prototyping.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在图像引导的机器人干预中实现 3D Slicer 与 ROS2 的高效、通用集成\n2. 提高跨平台数据传输的鲁棒性与对低级功能的可访问性\n3. 快速原型化研究所需的模块化、可维护的工具体系\n4. 在医学机器人研究中促进可重复性与协同开发的标准化方案\n【用了什么创新的方案】\n核心解决方案：重新设计的 SlicerROS2 模块实现更高模块化和低级功能访问，整合 3D Slicer 的 Python 与 C++ API，改进数据传输协议，支撑多种消息类型，并提供四个实际应用场景以展示核心功能在图像引导机器人中的应用与原型化能力\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation",
            "authors": "Geonhyup Lee,Yeongjin Lee,Kangmin Kim,Seongju Lee,Sangjun Noh,Seunghyeok Back,Kyoobin Lee",
            "subjects": "Robotics (cs.RO)",
            "comment": "9 pages, 9 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19047",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19047",
            "arxiv_html_link": "https://arxiv.org/html/2509.19047v1",
            "abstract": "Contact-rich manipulation tasks such as precision assembly require precise control of interaction forces, yet existing imitation learning methods rely mainly on vision-only demonstrations. We propose ManipForce, a handheld system designed to capture high-frequency force–torque (F/T) and RGB data during natural human demonstrations for contact-rich manipulation. Building on these demonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT). FMT encodes asynchronous RGB and F/T signals using frequency- and modality-aware embeddings and fuses them via bi-directional cross-attention within a transformer diffusion policy. Through extensive experiments on six real-world contact-rich manipulation tasks—such as gear assembly, box flipping, and battery insertion—FMT trained on ManipForce demonstrations achieves robust performance with an average success rate of 83% across all tasks, substantially outperforming RGB-only baselines. Ablation and sampling-frequency analyses further confirm that incorporating high-frequency F/T data and cross-modal integration improves policy performance, especially in tasks demanding high precision and stable contact.\nHardware, software, and video demos are available at: https://sites.google.com/view/manipforce/홈.",
            "introduction": "Contact-rich manipulation tasks such as precise assembly [1, 2, 3, 4], battery disassembly [5], and non-prehensile handling [6] require high precision and force-aware manipulation. Humans naturally perceive contact forces and their subtle changes when assembling parts, adjusting their strategies accordingly. Yet most robotic approaches rely solely on visual demonstrations, missing the rich F/T information humans provide.\n\nRecent advances in imitation learning [7, 8, 9] have demonstrated strong potential for dexterous and contact-rich manipulation by learning directly from human demonstrations.\nHowever, these methods still rely on high-quality demonstration data, which is costly and difficult to collect for fine-grained interactions.\nHand-held data collection systems such as UMI [10] have been proposed to address this challenge by enabling natural human demonstrations without the expertise requirements and remote-control limitations of teleoperation.\nWhile effective for simplifying demonstration collection, UMI does not capture force–torque (F/T) information, which is essential for accurately modeling contact behaviors.\nMore recent work [11] combines visual and F/T data but relies on point clouds to represent the scene, which introduces complex setup requirements and fundamentally limits the ability to perceive small objects and fine clearances essential for contact-rich manipulation.\nFurthermore, from a learning perspective, this approach down-samples high-frequency F/T signals to match the image frame rate, losing rich temporal information necessary for modeling contact dynamics.\n\nTo address these limitations, we introduce ManipForce a handheld system for simultaneous RGB–F/T data collection during natural human demonstrations, and the Frequency-Aware Multimodal Transformer (FMT), which learns robust policies from the collected data for diverse, precise, and contact-rich manipulation tasks.\n\nManipForce consists of a dual handheld camera setup with a wrist-mounted F/T sensor to capture both visual and high-frequency force signals during human-guided demonstrations.\nThis configuration enables robust perception of small objects, tight clearances, and fine-grained contacts, allowing collected demonstrations to transfer directly to robotic execution.\nWe replace SLAM-based wrist tracking with 3D ArUco marker pose estimation to maintain accuracy during close-contact interactions without environmental dependencies, and apply tool gravity compensation to ensure precise and interaction-focused F/T measurements.\nWe propose the FMT, which learns from asynchronous RGB (30 Hz) and F/T (>200 Hz) signals using a Transformer-based Diffusion Policy [7] architecture.\nTo exploit the higher-frequency force signals relative to images, the model tokenizes both RGB and F/T inputs using learnable frequency and modality embeddings.\nThis design enables the model to effectively handle heterogeneous modalities with asynchronous sampling rates.\nIn addition, bi-directional cross-attention modules fuse complementary information across modalities.\nWe evaluate our approach on six contact-rich manipulation tasks spanning precision assembly, non-prehensile manipulation, and complex disassembly, and observe significant performance gains over RGB-only baselines.\nAblation studies further confirm that high-frequency F/T sensing, unified positional embeddings, and bi-directional cross-attention each make complementary contributions to robust multimodal policy learning.\n\nOur main contributions are:\n\nWe introduce ManipForce, a handheld RGB–F/T data collection system enabling diverse and fine-grained contact-rich manipulation demonstrations.\n\nWe propose FMT, which handles inputs with asynchronous sampling rates through frequency-aware multimodal representation learning and cross-attention within a Transformer architecture, enabling robust policy learning for contact-rich manipulation.\n\nWe demonstrate robust performance on diverse contact-rich manipulation tasks—including gear assembly, plug insertion, battery disassembly, and lid operations—consistently outperforming RGB-only baselines.\n\n1. We introduce ManipForce, a handheld RGB–F/T data collection system enabling diverse and fine-grained contact-rich manipulation demonstrations.\n\n2. We propose FMT, which handles inputs with asynchronous sampling rates through frequency-aware multimodal representation learning and cross-attention within a Transformer architecture, enabling robust policy learning for contact-rich manipulation.\n\n3. We demonstrate robust performance on diverse contact-rich manipulation tasks—including gear assembly, plug insertion, battery disassembly, and lid operations—consistently outperforming RGB-only baselines.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在接触丰富的操纵任务中同时利用高频力矩信息与RGB视觉实现鲁棒仿生策略\n2. 如何在异步采样率下融合多模态数据以提升策略学习的准确性与稳定性\n3. 如何减少对复杂场景感知的依赖并实现自然人类演示的高效数据收集\n\n【用了什么创新的方案】\nManipForce 提供可手持的 RGB–F/T 数据采集系统，结合高频力矩与 RGB 数据；FMT 使用频率感知的多模态表示对异步 RGB(30 Hz) 与 F/T(>200 Hz) 信号进行编码，采用双向交叉注意力在 Transformer diffusion 策略中进行跨模态融合，显著提升对接触力与微小几何的建模能力；并通过 3D ArUco 标定与工具重力补偿实现近距离接触的高精度演示对齐与采样。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors",
            "authors": "Qingzheng Cong,Steven Oh,Wen Fan,Shan Luo,Kaspar Althoefer,Dandan Zhang",
            "subjects": "Robotics (cs.RO)",
            "comment": "14 pages, 8 figures. Equal contribution: Qingzheng Cong, Steven Oh, Wen Fan. Corresponding author: Dandan Zhang (d.zhang17@imperial.this http URL). Additional resources atthis http URL",
            "pdf_link": "https://arxiv.org/pdf/2509.19037",
            "code": "http://stevenoh2003.github.io/TacEva/, http://ac.uk",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19037",
            "arxiv_html_link": "https://arxiv.org/html/2509.19037v1",
            "abstract": "Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because of the high spatial resolution they offer and their relatively low manufacturing costs. However, variations in their sensing mechanisms, structural dimension, and other parameters lead to significant performance disparities between existing VBTSs. This makes it challenging to optimize them for specific tasks, as both the initial choice and subsequent fine-tuning are hindered by the lack of standardized metrics. To address this issue, TacEva is introduced as a comprehensive evaluation framework for the quantitative analysis of VBTS performance. The framework defines a set of performance metrics that capture key characteristics in typical application scenarios. For each metric, a structured experimental pipeline is designed to ensure consistent and repeatable quantification. The framework is applied to multiple VBTSs with distinct sensing mechanisms, and the results demonstrate its ability to provide a thorough evaluation of each design and quantitative indicators for each performance dimension. This enables researchers to pre-select the most appropriate VBTS on a task by task basis, while also offering performance-guided insights into the optimization of VBTS design. A list of existing VBTS evaluation methods and additional evaluations can be found on our website:\nhttps://stevenoh2003.github.io/TacEva/.",
            "introduction": "Robots have yet to attain the level of manipulative dexterity exhibited by humans, a challenge rooted in the difficulty of accurately acquiring detailed contact information in physical environments [1]. Tactile sensing has therefore become indispensable for delicate and precise robotic manipulation in embodied intelligence systems [2]. A notable development in this domain has been the rise of vision-based tactile sensors (VBTSs) [3]. These sensors employ high-resolution cameras to capture detailed contact surface information, thereby integrating seamlessly with computer vision and image-based deep learning methods. Nevertheless, among VBTSs, we see a wide variety of architectures, structural dimensions, and fabrication techniques, depending on the exact nature of the application requirements [4].\n\nThe rapid development of VBTSs has created an urgent need for standardized performance evaluation. Selecting an appropriate VBTS for a specific task scenario remains challenging, as distinct sensor designs offer substantial variation in performance. A universal evaluation protocol would therefore facilitate fair comparison across sensor designs, support informed selection, and guide design optimization. However, a significant gap persists in the field: no standardized framework currently exists for VBTS evaluation, and the inconsistency of current metrics limits objective, reproducible, and comprehensive cross-sensor assessment. This challenge is further compounded by the inherently multi-modal nature of tactile sensing [5], which necessitates coordinated evaluation across multiple performance dimensions.\n\nThe application scenarios for VBTSs are inherently diverse. This makes it difficult to establish universal, broadly applicable evaluation metrics that remain meaningful across the field. Further complexity arises from the fact that VBTSs are commonly fabricated using silicone elastomers, introducing additional considerations related to soft-material mechanics, optics, and imaging. Owing to differences in their underlying sensing mechanisms, each VBTS design inevitably exhibits its own profile of strengths and weaknesses. As a result, certain positive performance characteristics will typically tend to be emphasized, while weaker ones may be under-reported. This is likely to complicate informed selection, making it more challenging for prospective users.\n\nTo address these challenges, we propose TacEva, a systematic evaluation framework that integrates performance quantification with a structured and reproducible assessment pipeline. TacEva is designed to provide consistency and comparability across VBTS designs, enabling practitioners to make evidence-based decisions while obtaining a holistic understanding of sensor performance (see Fig. 1). It also offers sensor developers clear guidance for targeted optimization during the design process. By defining and standardizing a comprehensive set of performance metrics and evaluation protocols, TacEva aims to provide a unified and transparent characterization of VBTS designs, thereby facilitating objective comparison, reliable selection, and more informed innovation for future sensor development.",
            "llm_summary": "【关注的是什么问题】\n1. 缺乏标准化评估框架导致VBTSs在不同设计间难以公平比较与任务级优化（≤40词）\n2. 多模态触觉传感的评估指标缺乏统一、可重复的量化流程（≤40词）\n3. 不同VBTS sensing机制、结构与材料差异造成性能特征分布不一致，难以选型与设计优化（≤40词）\n4. 缺少可跨传感器的全面综合性能指标与任务驱动的性能指示（≤40词）\n\n【用了什么创新的方案】\nTacEva 提供一个综合的性能评估框架，定义一套标准化、可重复的评测指标和实验流水线，覆盖典型应用场景的关键性能维度，并可对多种VBTS 进行横向比较与任务驱动的选型与优化指导。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion",
            "authors": "Shuai Liu,Meng Cheng Lau",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "11 pages, 5 figures, 1 table, Computational Science Graduate Project",
            "pdf_link": "https://arxiv.org/pdf/2509.19023",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19023",
            "arxiv_html_link": "https://arxiv.org/html/2509.19023v1",
            "abstract": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a two-stage reinforcement learning framework for humanoid walking that requires no motion capture data or elaborate reward shaping. In the first stage, a compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via Proximal Policy Optimization. This generates energy-efficient gait templates. In the second stage, those dynamically consistent trajectories guide a full-body policy trained with Soft Actor–Critic augmented by an adversarial discriminator, ensuring the student’s five-dimensional gait feature distribution matches the ROM’s demonstrations. Experiments at 1 m/s and 4 m/s show that ROM-GRL produces stable, symmetric gaits with substantially lower tracking error than a pure-reward baseline. By distilling lightweight ROM guidance into high-dimensional policies, ROM-GRL bridges the gap between reward-only and imitation-based locomotion methods, enabling versatile, naturalistic humanoid behaviors without any human demonstrations.",
            "introduction": "Achieving natural humanoid locomotion is a longstanding goal in both robotics and computer animation. From bipedal robots that can walk and run with human-like grace, to virtual characters that move realistically in games, the ability to synthesize lifelike walking gaits remains a persistent challenge. Traditional model-based control has produced impressive feats, but often requires painstaking design and does not always capture the natural motion nuances of human walking Kuindersma et al. (2016). In recent years, reinforcement learning (RL) has emerged as a promising data-driven paradigm for developing locomotion controllers Radosavovic et al. (2024). RL allows simulated humanoids to learn complex gait behaviors through trial-and-error, offering the potential to discover agile and robust walking strategies that would be difficult to manually design.\n\nReinforcement learning (RL) methods for natural locomotion generally fall into two main paradigms aimed at producing lifelike gait behaviors.\n\nA purely objective‑driven RL policy can discover stable walking gaits by optimizing energy and stability rewards. These reference-free methods train locomotion policies from scratch by optimizing carefully crafted reward functions, without any motion capture examples. The reward terms are designed to encourage physically plausible and human-like traits, such as forward speed with energy efficiency, maintaining center-of-mass stability and upright posture, periodic foot contact patterns, and symmetric gait cycles between left and right legs. By rewarding such objectives, controllers can spontaneously develop stable walking gaits that emerge naturally from learning. Notably, researchers have demonstrated that symmetric bipedal walking can arise solely from reward design and curriculum training, without any reference motions Yu et al. (2018). Indeed, recent work showed that an RL policy trained with biomechanically inspired rewards could produce natural walking behaviors purely through self-exploration Peng et al. (2025). The appeal of this approach is that it does not require any pre-recorded motions – the agent invents its own walking cycle. However, designing the reward function is notoriously difficult: the policy’s behavior is highly sensitive to the choice and weighting of reward terms, often requiring elaborate hand-tuning and expertise. Even with careful tuning, purely objective-driven policies may develop subtle artifacts or unnatural quirks since there is no direct template of “human” motion to imitate. In summary, while motion-free RL can yield impressively natural gaits under the right conditions, it faces challenges in reward engineering and consistency of motion style.\n\nAlternatively, imitation‑driven RL uses human mocap examples to ensure motion realism but at the cost of dataset dependency. To directly ensure natural movement quality, a dominant approach is to imitate example locomotion trajectories from motion capture data. In this paradigm, the RL agent is guided by reference motions of humans (or animals) and receives rewards for matching the reference pose and velocity at each time step, or uses adversarial critics to judge realism against a motion dataset Peng et al. (2018, 2021). DeepMimic pioneered this line of work by showing that standard RL algorithms can learn robust control policies capable of imitating a broad range of example motion clips from a motion capture library Peng et al. (2018). By combining motion imitation objectives with task goals, DeepMimic enabled physically simulated characters to reproduce dynamic skills (flips, spins, walks) with high fidelity to the mocap examples. Following this, numerous studies have pushed the state of the art in imitation-based locomotion. For instance, ASE (Adversarial Skill Embeddings) uses large unstructured motion datasets to train latent skill models via adversarial imitation, yielding a repertoire of reusable behaviors that look remarkably life-like Peng et al. (2022). Another example is CALM (Conditional Adversarial Latent Models), which learns a rich latent representation of human movement through imitation learning, capturing the complexity and diversity of human motion while allowing direct user control over the character’s style and direction Tessler et al. (2023). Most recently, the concept of Behavioral Foundation Models have emerged – here a policy is pre-trained on massive collections of motion data (in an unsupervised manner) to serve as a generalist locomotion model. These foundation models, once trained on unlabeled motion trajectories, can be prompted to perform new tasks in a zero-shot fashion, while retaining human-like gait qualities Tirinzoni et al. (2025a). Motion imitation approaches thus achieve state-of-the-art realism in simulated walking; policies closely mimic human kinematics and can produce motions nearly indistinguishable from motion capture. The downside, however, is their heavy reliance on curated motion data – one must have access to large datasets of reference gaits, and the learned skills are inevitably tied to the distribution of motions in those datasets.\n\nDespite these advances, achieving natural, human‑like locomotion without any motion capture data remains an open challenge. Purely objective‑driven methods often fail to reproduce the fluidity and subtle timing of human gait, while imitation‑based approaches are fundamentally constrained by the availability and diversity of mocap archives.\n\nTo bridge this gap, we introduce a reduced‑order model–guided reinforcement learning(ROM-GRL) framework. It leverages a simplified locomotion model as a stand‑in for motion capture, providing high‑level gait guidance to the RL agent without any human demonstrations. Our framework unfolds in two stages. In the first stage, we train a lightweight teacher model to generate efficient, dynamically consistent gait templates that capture the essence of natural walking. In the second stage, we distill these templates into a full-body controller by rewarding adherence to the teacher’s motion distribution, ensuring smooth, human-like locomotion. By separating high-level gait planning from detailed control, we achieve a single walking policy that is both robust and naturally fluid, all without relying on motion capture data or intricate reward design.\n\nIn summary, our approach marries the insights from model-based gait generation with the flexibility of reinforcement learning. By using a ROM to guide RL instead of direct motion capture, we maintain a purely physics-driven training regime while still inducing realistic movement patterns. The proposed framework demonstrates that natural humanoid locomotion can emerge without demonstrations, closing the gap between reward engineering and motion imitation. We validate that our ROM-guided RL method produces walking controllers that are stable across different speeds and exhibit natural gait symmetry and fluidity comparable to reward-based policies, suggesting a novel solution to produce life-like humanoid locomotion in the absence of motion data.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在没有运动捕捉数据的情况下实现自然、稳定的人形机器人行走\n2. 如何在强化学习中兼顾高层步态规划与全身控制的协同学习\n3. 如何避免繁琐的奖励设计而仍获得自然的步态与对称性\n4. 如何将简化的ROM释放为对高维策略的有效引导\n\n【用了什么创新的方案】\n两阶段ROM-GRL：先用4-DOF简化模型通过PPO生成高效、动态一致的步态模板；再用这些模板对全身策略进行蒸馏，使策略通过对ROM分布的遵循实现自然、对称的行走，并结合带对抗判别的Soft Actor–Critic以匹配ROM示范分布，且无任何人类示范数据。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
            "authors": "Dapeng Zhang,Jin Sun,Chenghui Hu,Xiaoyan Wu,Zhenlong Yuan,Rui Zhou,Fei Shen,Qingguo Zhou",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19012",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19012",
            "arxiv_html_link": "https://arxiv.org/html/2509.19012v1",
            "abstract": "The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.",
            "introduction": "Robotics has long been a prominent area of scientific research. Historically, robots primarily relied on pre-programmed instructions and engineered control policies to perform task decomposition and execution. These methods were commonly applied to simple, repetitive tasks, such as factory assembly lines and logistics sorting. In recent years, the rapid advancement of artificial intelligence has enabled researchers to exploit the feature extraction and trajectory prediction capabilities of deep learning across diverse modalities, including images, text, and point clouds. By integrating techniques such as perception, detection, tracking, and localization, researchers have decomposed robotic tasks into multiple stages to meet execution requirements, thereby advancing the development of embodied intelligence and autonomous driving. However, most of these robots still operate as isolated agents, designed for specific tasks and lacking effective interaction with humans and external environment.\n\nTo address these limitations, researchers have begun exploring the incorporation of large language models (LLMs) and vision language models (VLMs) to enable more accurate and flexible robotic manipulation. Modern robotic manipulation methods [1, 2] typically leverage vision language generative paradigms (e.g., autoregressive models [3, 4, 5, 6] or diffusion models [7]), combined with large-scale datasets [8] and advanced fine-tuning strategies. We refer to these as VLA foundation models, which have substantially improved the quality of robotic manipulations. Fine-grained action control over generated content provides users with greater flexibility, unlocking the practical potential of VLA for task execution.\n\nDespite their promise, reviews of pure VLA methods remain scarce. Existing surveys either focus on taxonomy over VLM foundational models or provide broad overviews of robotic manipulation as a whole. Firstly, VLA methods represent a nascent field in robotics, with no established methodological landscape or consensus taxonomy, making it challenging to systematically summarize these approaches. Secondly, current reviews either classify VLA approaches based on differences in foundational models or present a comprehensive analysis of robotic applications across the entire history of the field, often emphasizing traditional methods at the expense of emerging techniques. While these reviews offer valuable insights, they provide only cursory examinations of robotic models or concentrate primarily on foundational models, leaving a significant gap in the literature regarding pure VLA methods.\n\nIn this paper, we investigate VLA methods and associated resources, providing a focused and comprehensive review of existing approaches. Our goal is to present a clear taxonomy, systematically summarize VLA research, and elucidate the development trajectory of this rapidly evolving field. After a brief overview of LLMs and VLMs, we focus on the policy strategies of VLA models, highlighting the unique contributions and distinctive features of previous studies. We classify VLA approaches into 4 categories: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods, and provide a detailed analysis of their motivations, core strategies, and mechanisms. As shown in Fig. 2, we present a VLA skeleton of these methods.\nWe examine application domains, including robotic arms, quadruped robots, humanoids, and wheeled robots (autonomous vehicles), offering a comprehensive assessment of VLA deployment across diverse scenarios. Given the strong dependence of VLA models on datasets and simulation platforms, we provide a concise overview of these resources. Finally, based on the current VLA landscape, we identify key challenges and outline future research directions—including data limitations, inference speed, and safety—to accelerate the advancement of VLA models and generalizable robotics.\n\nThe overall structure of this survey is illustrated in Fig. 1. First, Section 2 provides an overview of the background for VLA research. Section 3 presents the existing VLA approaches in the robotics field. Section 4 introduces the datasets and benchmarks employed by VLA approaches. Sections 5 and 6 discuss simulation platforms and robotic hardware. Section 7 further discusses the challenges and future directions for VLA-based robotic methods. Finally, we summarize the paper and provide our perspective on future developments.\n\nIn summary, our contributions are as follows:\n\nWe present the well-structured taxonomy of pure VLA methods, classifying approaches based on their action-generation strategies. This facilitates understanding of existing methods and highlights core challenges in the field.\n\nThe survey emphasizes the defining characteristics and methodological innovations of each category and technique, providing a clear perspective on current approaches.\n\nWe provide a comprehensive overview of associated resources (datasets, benchmarks and simulation platforms) for training and evaluating VLA models.\n\nWe investigate the practical impact of VLA in robotics, identify key limitations of existing techniques, and propose potential avenues for further exploration.\n\n1. We present the well-structured taxonomy of pure VLA methods, classifying approaches based on their action-generation strategies. This facilitates understanding of existing methods and highlights core challenges in the field.\n\n2. The survey emphasizes the defining characteristics and methodological innovations of each category and technique, providing a clear perspective on current approaches.\n\n3. We provide a comprehensive overview of associated resources (datasets, benchmarks and simulation platforms) for training and evaluating VLA models.\n\n4. We investigate the practical impact of VLA in robotics, identify key limitations of existing techniques, and propose potential avenues for further exploration.",
            "llm_summary": "【关注的是什么问题】\n1. 如何对纯VLA方法进行清晰分类与系统综述，以揭示其核心动机与实现机制？（≤40词）\n2. VLA模型在机器人执行中的生成策略、数据/基准资源、仿真平台与应用域的整合与挑战是什么？（≤40词）\n3. 现有VLA方法的局限性、安全性与可扩展性如何影响通用机器人能力的提升？（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出基于动作生成策略的纯VLA方法分型（自回归、扩散、强化学习、混合、专用），并对各类方法的动机、核心策略与实现机制进行系统比较；梳理数据集、基准与仿真平台；并给出未来方向与挑战。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Category-Level Object Shape and Pose Estimation in Less Than a Millisecond",
            "authors": "Lorenzo Shaikewitz,Tim Nguyen,Luca Carlone",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18979",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18979",
            "arxiv_html_link": "https://arxiv.org/html/2509.18979v1",
            "abstract": "Object shape and pose estimation is a foundational robotics problem, supporting tasks from manipulation to scene understanding and navigation. We present a fast local solver for shape and pose estimation which requires only category-level object priors and admits an efficient certificate of global optimality. Given an RGB-D image of an object, we use a learned front-end to detect sparse, category-level semantic keypoints on the target object. We represent the target object’s unknown shape using a linear active shape model and pose a maximum a posteriori optimization problem to solve for position, orientation, and shape simultaneously. Expressed in unit quaternions, this problem admits first-order optimality conditions in the form of an eigenvalue problem with eigenvector nonlinearities. Our primary contribution is to solve this problem efficiently with self-consistent field iteration, which only requires computing a 4×44\\times 4 matrix and finding its minimum eigenvalue-vector pair at each iterate. Solving a linear system for the corresponding Lagrange multipliers gives a simple global optimality certificate. One iteration of our solver runs in about 100 microseconds, enabling fast outlier rejection. We test our method on synthetic data and a variety of real-world settings, including two public datasets and a drone tracking scenario. Code is released at https://github.com/MIT-SPARK/Fast-ShapeAndPose.",
            "introduction": "A diverse set of robotics applications benefits from object shape and pose estimation. Autonomous cars, for example, need to locate obstacles and other cars [1], while household manipulators need to locate objects to interact with [2].\nIn many of these applications the object shape is not known exactly but its category is available (e.g., from a semantic segmentation method). We consider this setting and derive a shape and pose estimator using category-level priors.\n\nThe work of Shi et al. [3] established a certifiably optimal approach for category-level shape and pose estimation using a semidefinite relaxation. We consider a similar problem setup but emphasize both speed and certifiability. A fast estimator allows fast reaction to new inputs, performance with limited compute, and comprehensive outlier rejection [4, 5]. Certifiability provides an a posteriori guarantee that the estimate returned is statistically optimal. When the certificate fails, the user can decide to trust the output, try a different initialization, or acquire a new batch of measurements.\n\nOur algorithm relies on the eigenvalue structure of the first-order optimality conditions written in the quaternion representation of rotations. It returns local solutions which are often globally optimal. To verify this, we introduce a fast global optimality certifier based on Lagrangian duality. Specifically, our contributions are:\n\nA fast local solver for category-level shape and pose estimation\nusing self-consistent field iteration [6].\n\nA fast a posteriori certificate of global optimality for our local solutions.\n\nExperimental evaluation of runtime and accuracy on synthetic data, a drone tracking scenario, and two large-scale datasets.\n\nThe remainder of the paper is organized as follows. We begin with a literature review (Section II) and quaternion preliminaries (Section III). Then, we give the problem formulation in Section IV and reformulate it with quaternions in Section V.\nTo solve the nonlinear eigenproblem, we use self-consistent field iteration for local solutions and SDP optimality conditions to certify global optimality in Section VI. In Section VII, we show our method is significantly faster than other local solvers and learned baselines.\n\n1. A fast local solver for category-level shape and pose estimation\nusing self-consistent field iteration [6].\n\n2. A fast a posteriori certificate of global optimality for our local solutions.\n\n3. Experimental evaluation of runtime and accuracy on synthetic data, a drone tracking scenario, and two large-scale datasets.",
            "llm_summary": "【关注的是什么问题】\n1. 在低计算资源下，基于类别级先验进行对象形状与姿态的快速估计\n2. 给定RGB-D，如何通过稀疏语义关键点实现高效、可证伪的全局最优性口径\n3. 如何在四元数表示下将非线性优化转化为可迭代求解并给出全局最优性证据\n\n【用了什么创新的方案】\n核心解决方案：提出基于自洽场迭代的快速局部求解器，用线性主动形状模型表示未知形状，构造最大后验优化以同时估计位置、姿态和形状；在四元数表示下，将一阶最优性条件转化为特征值问题，通过迭代求解一个4×44×4的矩阵并获取最小特征值-向量对；通过拉格朗日乘子求解线性系统，提供快速的全局最优性证书（可后验判断是否可信）。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation",
            "authors": "Minoo Dolatabadi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18954",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18954",
            "arxiv_html_link": "https://arxiv.org/html/2509.18954v1",
            "abstract": "LiDAR-based localization and SLAM often rely on iterative matching algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align sensor data with pre-existing maps or previous scans. However, ICP is prone to errors in featureless environments and dynamic scenes, leading to inaccurate pose estimation. Accurately predicting the uncertainty associated with ICP is crucial for robust state estimation but remains challenging, as existing approaches often rely on handcrafted models or simplified assumptions. Moreover, a few deep learning-based methods for localizability estimation either depend on a pre-built map—which may not always be available—or provide a binary classification of localizable versus non-localizable, which fails to properly model uncertainty.\nIn this work, we propose a data-driven framework that leverages deep learning to estimate the registration error covariance of ICP before matching, even in the absence of a reference map. By associating each LiDAR scan with a reliable 6-DoF error covariance estimate, our method enables seamless integration of ICP within Kalman filtering, enhancing localization accuracy and robustness. Extensive experiments on the KITTI dataset demonstrate the effectiveness of our approach, showing that it accurately predicts covariance and, when applied to localization using a pre-built map or SLAM, reduces localization errors and improves robustness.",
            "introduction": "In recent years, autonomous vehicles have become an integral component of intelligent transportation systems, driving continuous research to push the boundaries of their capabilities. A key requirement for autonomous driving is achieving precise self-localization at the centimeter level [1]. The Global Navigation Satellite System (GNSS) is a cost-effective and widely used method for vehicle localization. Although GNSS delivers reliable positioning in open-sky environments, its accuracy is significantly compromised in urban settings due to factors such as signal blockage, non-line-of-sight (NLOS) conditions, and multipath effects [2]. To address these challenges, vision-based approaches—particularly those leveraging LiDAR—have been proposed as alternative or complementary solutions [3]. In these systems, map matching techniques like the Iterative Closest Point (ICP) algorithm are frequently employed to align sensor data with pre-existing maps, thereby enhancing localization precision [4].\n\nHowever, vision-based localization methods—like other relative localization techniques—are susceptible to error accumulation, where minor errors can progressively lead to significant drift over time. This phenomenon is observed in both Simultaneous Localization and Mapping (SLAM) and pre-built map-based localization approaches, although it tends to be more pronounced in SLAM [5]. In the context of ICP, previous work has demonstrated that factors such as featureless environments (e.g., tunnels) and the presence of dynamic objects can adversely affect the matching accuracy [6].\n\nState estimation methods, such as Kalman filtering, are commonly employed to mitigate these problems. Yet, these techniques depend on an accurate error model (often represented in the simplest form by an error covariance matrix), which is challenging to determine for matching algorithms\n[7].\n\nTo address this limitation, several studies have proposed data-driven approaches to predict ICP error, either using classification-based methods [8] or by directly estimating the covariance [7].\n\nSpecifically, we introduce a data-driven framework that predicts the full six-degree-of-freedom (6-DoF) ICP registration covariance from a single LiDAR scan prior to correspondence search and ICP refinement. The output is a symmetric positive-definite (SPD) 6×66\\times 6 covariance on S​E​(3)SE(3) suitable for probabilistic fusion. An overview of the pipeline is shown in Fig. 1.\n\nContributions.\n\nPre-ICP, per-scan S​E​(3)SE(3) covariance. We predict the full 6×66\\times 6 registration covariance directly from a single LiDAR scan, capturing translation–rotation coupling before ICP is run. Per-scan training targets are obtained by estimating empirical covariances from Monte Carlo ICP under randomized initializations.\n\nKalman fusion and evaluation. Each scan is paired with a reliable 6-DoF covariance, used as the measurement noise in a standard Kalman filter. This improves pose accuracy over fixed or heuristic covariances on the evaluated sequences.\n\nPracticality. The model does not require a pre-built map at inference, supporting both SLAM and map-based operation.\n\n1. Pre-ICP, per-scan S​E​(3)SE(3) covariance. We predict the full 6×66\\times 6 registration covariance directly from a single LiDAR scan, capturing translation–rotation coupling before ICP is run. Per-scan training targets are obtained by estimating empirical covariances from Monte Carlo ICP under randomized initializations.\n\n2. Kalman fusion and evaluation. Each scan is paired with a reliable 6-DoF covariance, used as the measurement noise in a standard Kalman filter. This improves pose accuracy over fixed or heuristic covariances on the evaluated sequences.\n\n3. Practicality. The model does not require a pre-built map at inference, supporting both SLAM and map-based operation.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在 ICP 匹配前直接估计六自由度（6-DoF）注册协方差以提升 LiDAR 基于定位的鲁棒性\n2. 缺少地图时如何进行无地图依赖的协方差预测，并实现与卡尔曼滤波的无缝融合\n3. 面对 featureless 或动态环境，如何更准确地量化 ICP 的不确定性并减少定位误差\n4. 将深度学习预测的协方差用于先验状态估计，从而提升 SLAM/基于地图的定位的鲁棒性\n\n【用了什么创新的方案】\nPre-ICP 直接从单帧 LiDAR 预测对称正定的 6×6 协方差矩阵，捕捉平移-旋转耦合；以蒙特卡洛 ICP 在随机初始值下得到的经验协方差作为训练目标。将该协方差作为卡尔曼滤波的测量噪声，实现 ICP 前后的一体化鲁棒定位；方法在推断阶段不依赖预构建地图，支持 SLAM 与地图匹配场景。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations",
            "authors": "Hanqing Liu,Jiahuan Long,Junqi Wu,Jiacheng Hou,Huili Tang,Tingsong Jiang,Weien Zhou,Wen Yao",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18953",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18953",
            "arxiv_html_link": "https://arxiv.org/html/2509.18953v1",
            "abstract": "Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60%, with object transformations causing up to 97.8% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.",
            "introduction": "Vision-Language-Action (VLA) models represent a paradigm shift in robotic manipulation, integrating visual perception, language understanding, and action generation into unified end-to-end systems [1]. Recent deployments across manufacturing [2], healthcare [3], and service robotics [4, 5] demonstrate their transformative potential. However, in real-world deployments, VLA models inevitably face challenging physical variations, such as spatial transformations, illumination variations, and visual disruptions, which can dramatically alter robot behavior without being immediately detectable, posing significant safety risks. Therefore, it is crucial to investigate VLA robustness across various physical conditions systematically.\n\nExisting research has explored the robustness of VLA-based robotic systems through approaches like adversarial patches [6], which generate localized perturbations via gradient-based white-box attacks to achieve visual interference. However, these methods suffer from critical limitations: they violate physical plausibility constraints and fail to capture the rich spectrum of real-world physical variations. Moreover, their reliance on gradient access restricts applicability to black-box deployment scenarios. Based on these limitations, we aim to generate more diverse and realistic physical variations for comprehensively evaluating VLA robustness, while two key challenges must be addressed: (1) How to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility? (2) How to discover worst-case scenarios without prohibitive real-world data collection costs efficiently?\n\nTo address these challenges, we propose Eva-VLA, a unified framework for evaluating vision-language-action models’ robustness. Our key innovation lies in transforming discrete physical variations into continuous optimization problems. First, as shown in Fig. 1, we decompose real-world variations into three distinct domains: object 3D transformations parameterized with rotation angles(α\\alpha, β\\beta, γ\\gamma), illumination variations defined by point light parameters including position(xx, yy), radius(σ\\sigma), intensity(II), and adversarial patch placement specified by (Δ​x\\Delta x, Δ​y\\Delta y). This parameterization enables systematic exploration of the variation space while maintaining physical plausibility through explicit constraints. Second, to overcome the black-box nature of VLA models and non-differentiable simulation environments, we employ Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [7], a gradient-free optimization algorithm, to efficiently discover worst-case scenarios by iteratively optimizing physical variations parameters. This approach enables comprehensive vulnerability assessment without requiring model gradients or expensive real-world data collection.\n\nOur main contributions are as follows: ❶ To the best of our knowledge, we are the first to decompose real-world physical variations into three key domains—object 3D transformation, illumination variations, and adversarial patches—enabling a comprehensive evaluation of VLA robustness under these physical variations. ❷ We propose Eva-VLA, a novel framework that transforms discrete physical variations into continuous parameter optimization. By leveraging a simulator environment that allows us to reset to the same conditions, we ensure the repeatability and reliability of the evaluation process, which enables efficient exploration of worst-case scenarios without the need for expensive real-world data collection. ❸ Through extensive experiments on state-of-the-art model OpenVLA across multiple benchmarks, we expose significant fragility in current VLA systems, with failure rates exceeding 60% across all variation categories, with object transformations causing up to 97.8% failure in long-horizon tasks. These findings provide crucial insights for developing more robust VLA architectures and underscore the urgent need for improved robustness training methodologies.",
            "llm_summary": "【关注的是什么问题】\n1. 如何系统评估 Vision-Language-Action (VLA) 模型在真实世界物理变异下的鲁棒性（robustness）  \n2. 如何在不依赖大量真实数据的情况下发现最坏场景（worst-case scenarios）并保证评估可重复性  \n3. 如何将离散物理变异转化为连续优化问题以便可控探索  \n4. 如何覆盖三大变异域（object 3D transformation、illumination、adversarial patches）的全面评估  \n【用了什么创新的方案】\n核心方案：提出 Eva-VLA 框架，将离散物理变异转化为连续参数优化问题；将变异分解为 object 3D transformations、illumination variations、adversarial patches 三大域，采用 CMA-ES（协方差矩阵适应进化策略）进行黑盒优化以高效发现 worst-case 场景，并在可复现的仿真环境中重置到相同条件进行评估，从而在无需梯度信息或大量真实数据的情况下系统评估 VLA 的鲁棒性。通过对 OpenVLA 在多基准上的大规模实验，揭示了超过 60% 的失败率，且对象变换在长时任务中可达 97.8% 的失败率，强调了现实部署中的鲁棒性缺口与改进必要性。  \n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands",
            "authors": "Yanyuan Qiao,Kieran Gilday,Yutong Xie,Josie Hughes",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18937",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18937",
            "arxiv_html_link": "https://arxiv.org/html/2509.18937v1",
            "abstract": "Designing robotic hand morphologies for diverse manipulation tasks requires balancing dexterity, manufacturability, and task-specific functionality. While open-source frameworks and parametric tools support reproducible design, they still rely on expert heuristics and manual tuning. Automated methods using optimization are often compute-intensive, simulation-dependent, and rarely target dexterous hands. Large language models (LLMs), with their broad knowledge of human-object interactions and strong generative capabilities, offer a promising alternative for zero-shot design reasoning. In this paper, we present Lang2Morph, a language-driven pipeline for robotic hand design. It uses LLMs to translate natural-language task descriptions into symbolic structures and OPH-compatible parameters, enabling 3D-printable task-specific morphologies. The pipeline consists of: (i) Morphology Design, which maps tasks into semantic tags, structural grammars, and OPH-compatible parameters; and (ii) Selection and Refinement, which evaluates design candidates based on semantic alignment and size compatibility, and optionally applies LLM-guided refinement when needed. We evaluate Lang2Morph across varied tasks, and results show that our approach can generate diverse, task-relevant morphologies. To our knowledge, this is the first attempt to develop an LLM-based framework for task-conditioned robotic hand design.",
            "introduction": "Designing the morphology and structure of a robotic hand for a specific application is a fundamental yet long-standing challenge in robotics.\nUnlike the design of general-purpose manipulators, for example anthropomorphic robotic hands, task-specific robotic hands must balance dexterity and generality with optimization for a specific task, whilst remaining manufacturable [1, 2].\nThe design of dexterous, yet tasks-specific hands requires a full understanding of the task, and also fabrication constraints. For example, whilst increasing finger count or joint complexity improves capability but complicates fabrication and control.\nAs a result, tasks specific hand design has typically relied on expert-driven heuristics and iterative prototyping, which are time-consuming and hard to scale.\n\nThe development of parametric hand designs can provide a design space for task-specific hand morphology design.\nOpen-source hardware projects such as the Yale OpenHand Project [3, 4] have released reproducible underactuated hand designs, lowering the barrier for building functional prototypes. Parametric frameworks like the Open Parametric Hand (OPH) [5] further expose a structured design space with tunable parameters, enabling systematic exploration of hand morphologies.\nHowever, both approaches still depend on human designers to interpret tasks, identify likely grasp types, and manually adjust parameters. Without automated mapping from task requirements to design instantiations, scaling to diverse, task-specific morphologies remains difficult.\n\nIn parallel, the robotics community has explored automated robot morphology optimization and generation through evolutionary optimization, grammar-based synthesis, and differentiable pipelines [6, 7, 8, 9].\nThese works demonstrate that morphology can indeed be generated algorithmically, but they are often computationally expensive, require carefully crafted objective functions, and are typically coupled to physics-based simulators and rely on the curation of algorithmic representation of a fitness function or objective.\nMoreover, they have rarely targeted dexterous hand design, where functional requirements such as fingertip precision, lateral pinching, or stabilizing support are used to determine the resulting morphology.\nThus, these approaches lacks generality and relies on expert identification of key functional metrics or objective functions.\n\nRecently, large language models (LLMs) have been explored in robotics, with applications in navigation and manipulation where language is grounded into action policies [10, 11].\nWhilst these efforts focus mainly on control and planning,\nLLMs are not only capable of semantic reasoning but also encode broad background knowledge across biology, engineering, and everyday practice, which traditional optimization-based methods typically lack.\nThis combination makes them promising candidates for reasoning about form and functionality in design.\nSome recent works have adapted LLMs for computer-aided design (CAD) [12, 13], translating text into parametric part models.\nYet these methods remain restricted to geometry generation and do not address morphology design, where task semantics such as lateral pinching or stabilizing support directly dictate structural choices.\nTo the best of our knowledge, the use of LLMs for robotic hand morphology design remains unexplored, motivating our study.\n\nIn this paper, we propose Lang2Morph, a language-driven pipeline for robotic hand morphology generation. Our key insight is that LLMs are well suited to reasoning about task semantics and mapping them into symbolic and geometric design representations. Unlike prior methods that rely on expert-driven heuristics or costly physics-based simulation, Lang2Morph leverages LLM reasoning and semantic feedback to achieve scalable, task-conditioned morphology generation, as illustrated in Fig. 1, where a user-provided instruction is mapped into a design rationale and a CAD-ready morphology.\n\nLang2Morph builds upon the Open Parametric Hand (OPH) framework [5], which defines a structured and fabricable design space for robotic hands. OPH supports single-piece 3D printing, allowing generated designs to be directly manufactured without additional assembly or simulation. This enables our method to output physically realizable morphologies from natural language instructions.\n\nSpecifically, the pipeline comprises two major modules: (i) LLM-powered Morphology Design, which performs dual-level task analysis (semantic and structural), followed by geometry parameterization and constraint-aware validation to produce OPH-compatible parameters; and (ii) LLM-Guided Selection and Refinement, which ranks rendered variants based on semantic alignment and size compatibility, and optionally provides design refinements through feedback.\nTogether, these components form an end-to-end pipeline that generates fabricable, task-adaptive morphologies directly from natural language task description.\n\nOur main contributions are as follows:\n\nWe present Lang2Morph, a novel framework that generates robotic hand morphologies from natural-language instructions using large language models.\n\nWe design a two-stage pipeline that combines symbolic grammar generation, geometric parameterization, and semantic feedback for task-adaptive hand design.\n\nWe explore the use of LLMs for early-stage robot morphology generation, offering a flexible alternative to expert tuning.\n\nWe evaluate our method on a range of manipulation tasks, demonstrating improved design validity, diversity, and semantic alignment.\n\n1. We present Lang2Morph, a novel framework that generates robotic hand morphologies from natural-language instructions using large language models.\n\n2. We design a two-stage pipeline that combines symbolic grammar generation, geometric parameterization, and semantic feedback for task-adaptive hand design.\n\n3. We explore the use of LLMs for early-stage robot morphology generation, offering a flexible alternative to expert tuning.\n\n4. We evaluate our method on a range of manipulation tasks, demonstrating improved design validity, diversity, and semantic alignment.",
            "llm_summary": "【关注的是什么问题】\n1. 如何从自然语言任务描述自动生成可直接制造的任务适应性机器人手形态\n2. 在确保可制造性与任务相关性的同时，减少对人工经验启发式设计的依赖\n3. 面向 dexterous 手掌的任务特定形态设计的高效自动化路径\n4. 将语言模型用于高层语义分析到几何参数化、结构约束的端到端设计\n\n【用了什么创新的方案】\nLang2Morph 使用大语言模型进行双层任务分析（语义与结构），将自然语言指令映射到 OPH 兼容的参数化几何；结合符号语法生成与几何参数化，通过语义对齐和尺寸兼容性评估进行候选选择与 refinement；实现从文本到可直接 3D 打印的任务特定形态的端到端流程，且无需仿真驱动。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation",
            "authors": "Masato Kobayashi,Thanpimon Buamanee",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18865",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18865",
            "arxiv_html_link": "https://arxiv.org/html/2509.18865v1",
            "abstract": "We propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral control-based imitation learning to handle more than one task within a single model. Conventional bilateral control methods exploit joint angle, velocity, torque, and vision for precise manipulation but require task-specific models, limiting their generality.\nBi-VLA overcomes this limitation by utilizing robot joint angle, velocity, and torque data from leader-follower bilateral control with visual features and natural language instructions through SigLIP and FiLM-based fusion.\nWe validated Bi-VLA on two task types: one requiring supplementary language cues and another distinguishable solely by vision. Real-robot experiments showed that Bi-VLA successfully interprets vision-language combinations and improves task success rates compared to conventional bilateral control-based imitation learning.\nOur Bi-VLA addresses the single-task limitation of prior bilateral approaches and provides empirical evidence that combining vision and language significantly enhances versatility.\nExperimental results validate the effectiveness of Bi-VLA in real-world tasks.\nFor additional material, please visit the website: https://mertcookimg.github.io/bi-vla/",
            "introduction": "Robotic manipulation is increasingly important in human-centered applications such as cooking, eldercare, and interactive service robots [1, 2, 3, 4].\nUnlike traditional industrial robots that excel at repetitive and pre-programmed routines, service and collaborative robots must adapt to dynamic environments and interact with objects of diverse shapes, sizes, and material properties [5, 6]. Achieving such adaptability requires learning frameworks capable of acquiring human-like manipulation strategies [7].\n\nImitation learning (IL) has emerged as a promising approach for transferring human manipulation skills directly to robots [8].\nLeader-follower teleoperation has become a common pipeline for collecting demonstrations. For example, ALOHA and Mobile ALOHA use position-based unilateral control to gather diverse datasets that enable a wide range of manipulation [9, 10].\nAlthough effective for kinematics-driven tasks, such unilateral control omits force feedback, which limits robustness in contact-rich interactions.\n\nBilateral control-based imitation learning addresses these limitations by exchanging both position and force information between the demonstrator and robot [11, 12].\nBilateral control allows demonstrators to feel contact forces directly, yielding richer demonstrations and improving generalization across objects with different hardness and weights.\n\nBuilding on this foundation, recent methods have explored combining bilateral control with modern architectures.\nBilateral Control-Based Imitation Learning via Action Chunking with Transformers (Bi-ACT) [13] integrates bilateral control with visual observations via Transformers, yielding improved manipulation accuracy.\nHowever, it remains restricted to single-task settings, limiting its practicality in dynamic environments where multiple tasks must be handled seamlessly.\nIn parallel, Bilateral Control-Based Imitation Learning via Natural Language and Action Chunking with Transformers (Bi-LAT) [14] introduced natural language instructions into bilateral control-based imitation learning, demonstrating effective force modulation in manipulation.\nWhile this work highlights the promise of language integration, it focuses on regulating applied force and does not address the challenge of enabling a single model to adapt to multiple task contexts.\n\nIn this paper, we propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that unifies robot joint angle, velocity, and torque data from bilateral control with visual and language features, as shown in Fig. 1.\nBy leveraging SigLIP-based text embeddings [15] and FiLM-based EfficientNet feature fusion [16], Bi-VLA learns a shared representation of vision and language aligned with robot state information. Unlike prior bilateral frameworks, Bi-VLA is designed to handle multiple tasks within a single model, enabling flexible task switching without retraining or manual model selection.\n\nThe main contributions of this paper are summarized as follows:\n\nWe propose Bi-VLA, the bilateral control-based imitation learning framework that fuses vision and language features into a unified representation.\n\nWe demonstrate that Bi-VLA enables a single model to perform multiple tasks, overcoming the single-task limitation inherent in prior bilateral control-based imitation learning approaches.\n\nWe validate Bi-VLA through real-robot experiments on two distinct tasks, showing improved performance and adaptability compared to conventional methods.\n\n1. We propose Bi-VLA, the bilateral control-based imitation learning framework that fuses vision and language features into a unified representation.\n\n2. We demonstrate that Bi-VLA enables a single model to perform multiple tasks, overcoming the single-task limitation inherent in prior bilateral control-based imitation learning approaches.\n\n3. We validate Bi-VLA through real-robot experiments on two distinct tasks, showing improved performance and adaptability compared to conventional methods.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在单一模型中实现多任务的双边控制模仿学习与任务切换（≤40词）\n2. 如何将视觉与语言信息与机器人关节角度、速度、力矩数据融合以提升操控鲁棒性（≤40词）\n3. 如何在现实机器人环境中验证视觉-语言融合对多任务泛化的提升（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：在双边控制基础上，结合 SigLIP 文本嵌入和 FiLM 机制对 EfficientNet 提取的视觉特征进行融合，形成一个统一的视觉-语言-状态表示；通过该表示实现单模型处理多任务、并通过任务切换无需重新训练；在两类任务与真实机器人上验证性能提升。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation",
            "authors": "Suzannah Wistreich,Baiyu Shi,Stephen Tian,Samuel Clarke,Michael Nath,Chengyi Xu,Zhenan Bao,Jiajun Wu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "Accepted to CoRL 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18830",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18830",
            "arxiv_html_link": "https://arxiv.org/html/2509.18830v1",
            "abstract": "Human skin provides a rich tactile sensing stream, localizing intentional and unintentional contact events over a large and contoured region.\nReplicating these tactile sensing capabilities for dexterous robotic manipulation systems remains a longstanding challenge.\nIn this work, we take a step towards this goal by introducing DexSkin. DexSkin is a soft, conformable capacitive electronic skin that enables sensitive, localized, and calibratable tactile sensing, and can be tailored to varying geometries.\nWe demonstrate its efficacy for learning downstream robotic manipulation by sensorizing a pair of parallel jaw gripper fingers, providing tactile coverage across almost the entire finger surfaces.\nWe empirically evaluate DexSkin’s capabilities in learning challenging manipulation tasks that require sensing coverage across the entire surface of the fingers, such as reorienting objects in hand and wrapping elastic bands around boxes, in a learning-from-demonstration framework. We then show that, critically for data-driven approaches, DexSkin can be calibrated to enable model transfer across sensor instances, and demonstrate its applicability to online reinforcement learning on real robots.\nOur results highlight DexSkin’s suitability and practicality for learning real-world, contact-rich manipulation.\nPlease see our project webpage for videos and visualizations: https://dex-skin.github.io/.",
            "introduction": "Tactile feedback is essential for robust and dexterous manipulation in natural and artificial systems. In humans, mechanoreceptors within the skin provide a rich sensory stream that guides tasks ranging from handling delicate objects to using tools with force [1]. This tactile feedback enables more precise reactive control than can be achieved with human and proprioceptive feedback alone [2].\n\nEmulating this tactile sensitivity in robotic systems has long been a challenge, starting from sensing hardware. Research efforts typically focus on rigid or partially flexible sensors that offer low spatial coverage, limited adaptability, and poor conformability to complex surfaces. In contrast, everyday tasks such as rotating a key in-hand or picking up delicate berries require sensing coverage in multiple contact regions.\nSimilarly, wrapping an elastic band around an object requires a sensor to detect, distinguish, and localize dynamic contact events on all surfaces of a human hand or end effector accurately, applying the appropriate force without allowing the band to slip.\n\nLearning-based approaches offer a general way to harness tactile sensing information. However, data-driven systems pose requirements beyond sensing coverage. Seemingly small distribution shifts in sensor readings from wear or replacement can render previously trained models unusable, thus, output signals must be replicable across sensor hardware instances. Additionally, for real-world online learning, sensors must remain consistent, durable, and precise under repeated stresses during trial-and-error interaction, and output interpretable signals amenable to reward or cost specification.\n\nTo address these challenges, we introduce a novel soft tactile skin named DexSkin that is particularly suitable for robotic learning applications. DexSkin can be conformably integrated onto robotic end-effectors with unparalleled spatial coverage.\nIt is based on a capacitive mechanism, and features high sensitivity and robustness under repeated interactions.\nBecause each of the dozens of taxels on the skin are individually addressable, DexSkin can localize and characterize simultaneous contacts from distinct regions.\nIt can also be calibrated to provide consistent readings across distinct sensor units, enabling re-use of learned networks. At the same time, it can withstand deformations encountered during typical dexterous tasks such as pinching, twisting, and bending.\n\nIn this work, we introduce the DexSkin framework and its fabrication, as well as a representative integration with a soft cylindrical robotic fingertip that sensorizes the distal dome of the finger and 294∘ of the circumference. Then, we evaluate DexSkin’s applicability to robot learning. First, we test whether DexSkin’s coverage and tailorability expands the range of learnable manipulation tasks. Then, we evaluate its calibration performance, which are critical for working with and transferring learned tactile models. Finally, we demonstrate its suitability for online robotic learning settings by performing real-world reinforcement learning for a delicate object picking task. The results highlight DexSkin’s robust applicability to a wide range of robotic tasks and morphologies, and its particular practicality for robot learning researchers and practitioners.",
            "llm_summary": "【关注的是什么问题】\n1. 需要大面积、可 conform 的触觉传感覆盖以实现高鲁棒的接触丰富操控\n2. 如何在多传感单元上获得高灵敏、可标定、跨传感器的一致性读取\n3. 在 learns-from-demonstration 与 在线强化学习 场景中实现对不同几何和材料的可转移性\n4. 传感硬件对姿态、变形的耐久性及对数据分布偏移的鲁棒性\n\n【用了什么创新的方案】\nDexSkin 是一种软性、可 conform 的电容式电子皮肤，具有大量独立可寻址的触觉单元（taxels），可覆盖手指表面且对变形鲁棒；通过逐个触觉单元标定实现跨传感器的一致性，使已学习的模型可在不同传感器实例间转移；支持对多点同时接触的定位与表面广覆盖的感知，适用于从演示学习到在线强化学习的任务。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations",
            "authors": "Lukas Zanger,Bastian Lampe,Lennart Reiher,Lutz Eckstein",
            "subjects": "Robotics (cs.RO); Multiagent Systems (cs.MA); Software Engineering (cs.SE)",
            "comment": "7 pages, 2 figures, 2 tables; Accepted to be published as part of the 2025 IEEE International Conference on Intelligent Transportation Systems (ITSC 2025), Gold Coast, Australia, November 18-21, 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18793",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18793",
            "arxiv_html_link": "https://arxiv.org/html/2509.18793v1",
            "abstract": "Vehicles are becoming increasingly automated and interconnected, enabling the formation of cooperative intelligent transport systems (C-ITS) and the use of offboard services. As a result, cloud-native techniques, such as microservices and container orchestration, play an increasingly important role in their operation.\nHowever, orchestrating applications in a large-scale C-ITS poses unique challenges due to the dynamic nature of the environment and the need for efficient resource utilization.\nIn this paper, we present a demand-driven application management approach that leverages cloud-native techniques – specifically Kubernetes – to address these challenges. Taking into account the demands originating from different entities within the C-ITS, the approach enables the automation of processes, such as deployment, reconfiguration, update, upgrade, and scaling of microservices.\nExecuting these processes on demand can, for example, reduce computing resource consumption and network traffic.\nA demand may include a request for provisioning an external supporting service, such as a collective environment model.\nThe approach handles changing and new demands by dynamically reconciling them through our proposed application management framework built on Kubernetes and the Robot Operating System (ROS 2).\nWe demonstrate the operation of our framework in the C-ITS use case of collective environment perception and make the source code of the prototypical framework publicly available at https://github.com/ika-rwth-aachen/application_manager.",
            "introduction": "In future cooperative intelligent transport systems (C-ITS), various entities, such as vehicles equipped with driving automation systems, sensor-equipped roadside infrastructure units, edge/cloud servers, and control centers, will be connected, exchange data, and may offer computational resources.\nThese advancements enable new applications – such as collective environment perception, cooperative decision-making, computation offloading, and intelligent traffic management – that can contribute to improved comfort and safety for road users [1], [2], [3].\nNot only cloud and edge servers but also vehicles and roadside units can be part of a distributed computing system.\nHowever, these applications may also introduce complexity that is difficult to manage. The dynamic nature of C-ITS, the presence of resource-constrained entities, and the strict requirements for safety and security pose unique challenges.\n\nCloud-native techniques provide a promising foundation for the development and operation of scalable applications in dynamic environments. Such techniques involve paradigms like containerization, microservice architectures, and container orchestration. They enable loosely coupled systems which are manageable and resilient [4]. Said techniques and paradigms have the potential to contribute to the advancement of C-ITS.\n\nKubernetes has evolved as the de facto standard for orchestrating containerized applications in distributed systems. It is open-source and widely adopted by software companies worldwide [5].\nNevertheless, Kubernetes lacks methods that are domain-specific, e.g., to C-ITS, considering that specific tasks like the deployment of required applications are only needed at certain times or may depend on the specific content of data exchanged in the C-ITS.\nWe have developed the approach RobotKube [6] to extend the regular capabilities of Kubernetes. RobotKube comprises software components designed to automate the identification of requirements and the formulation of specific Kubernetes workloads.\nThese components include the event detector and the application manager.\n\nIn this paper, we propose a demand-driven application management approach and present the methodology behind the application manager as part of an application management framework.\nThis methodology integrates seamlessly into the RobotKube architecture and complements parts of RobotKube which were not detailed yet.\nThe application management framework – comprising the application manager and a set of custom operators – addresses the orchestration challenges in C-ITS through a demand-driven approach.\nIn this context, applications are deployed, reconfigured, scaled, and updated based on the current demands of C-ITS entities.\n\nWith our work, we make the following main contributions:\n\nPresentation of the methodology for demand-driven application management allowing to deploy, reconfigure, update, upgrade, and scale applications based on demands of entities in a C-ITS.\n\nPrototypical implementation of the application manager and the custom operators in an application management framework based on Kubernetes and ROS 2.\n\nDemonstration and evaluation of the capabilities of the application management framework in the complex C-ITS use case of collective environment perception involving various C-ITS entities.\n\nOpen source code publication of the application management framework and the demonstration use case allowing for reproducibility and extensibility.\n\nComplementation of RobotKube [6] by providing the concrete methodology of the application manager.\n\n1. Presentation of the methodology for demand-driven application management allowing to deploy, reconfigure, update, upgrade, and scale applications based on demands of entities in a C-ITS.\n\n2. Prototypical implementation of the application manager and the custom operators in an application management framework based on Kubernetes and ROS 2.\n\n3. Demonstration and evaluation of the capabilities of the application management framework in the complex C-ITS use case of collective environment perception involving various C-ITS entities.\n\n4. Open source code publication of the application management framework and the demonstration use case allowing for reproducibility and extensibility.\n\n5. Complementation of RobotKube [6] by providing the concrete methodology of the application manager.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在动态且资源受限的C-ITS环境中对应用进行按需驱动的管理与部署（≤40词）\n2. 如何将云原生技术（Kubernetes、ROS 2、微服务）整合用于C-ITS中的应用编排与再配置（≤40词）\n3. 如何通过需求感知实现应用的部署、重配置、更新、升级与扩缩以优化资源与网络开销（≤40词）\n【用了什么创新的方案】\n核心解决方案：提出基于需求驱动的应用管理框架，结合 Kubernetes 与 ROS 2，开发应用管理器与自定义 Operator，用事件检测器识别需求并动态编排、部署、重配置、升级和扩缩，支持对外部服务等需求的 provisioning；框架作为 RobotKube 的扩展，公开源码以便复现与扩展。\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Human-Interpretable Uncertainty Explanations for Point Cloud Registration",
            "authors": "Johannes A. Gaus,Loris Schneider,Yitian Shi,Jongseok Lee,Rania Rayyes,Rudolph Triebel",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18786",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18786",
            "arxiv_html_link": "https://arxiv.org/html/2509.18786v1",
            "abstract": "In this paper, we address the point cloud registration problem, where well-known methods like ICP fail under uncertainty arising from sensor noise, pose‐estimation errors, and partial overlap due to occlusion. We develop a novel approach, Gaussian Process Concept Attribution (GP-CA), which not only quantifies registration uncertainty but also explains it by attributing uncertainty to well-known sources of errors in registration problems. Our approach leverages active learning to discover new uncertainty sources in the wild by querying informative instances. We validate GP-CA on three publicly available datasets and in our real-world robot experiment. Extensive ablations substantiate our design choices. Our approach outperforms other state-of-the-art methods in terms of runtime, high sample-efficiency with active learning, and high accuracy. Our real-world experiment clearly demonstrates its applicability. Our video also demonstrates that GP-CA enables effective failure-recovery behaviors, yielding more robust robotic perception.",
            "introduction": "Point cloud registration refers to the problem of estimating a relative transformation between two sets of 3D points [1]. This problem is essential in many robotic perception tasks, such as simultaneous localization and mapping (SLAM) [2], 3D reconstruction [3], and 6-DoF object pose estimation [4], to name a few. A widely used approach for point cloud registration is Iterative Closest Point (ICP) [5].\nHowever, in ICP, well-known causes\nof uncertainty are\nsensor noise, poor initialization of the optimization process of the relative transformation between the two point clouds, and insufficient overlap between the two point clouds (e.g., due to occlusions), which can make ICP inaccurate or unreliable in practice  [6]. Hence, several researchers attempt to improve the robustness of the registration process [7].\n\nAmong others, several probabilistic approaches were proposed to quantify uncertainty in point cloud registration [6, 8, 9, 10, 11]. The underlying idea is that uncertainty estimates provide insight into the reliability of the obtained results, enabling the identification and rejection of unreliable registrations. Moreover, information about uncertainty is often used in many downstream tasks, including sensor fusion, state estimation, and 3D reconstruction [6]. To this end, a variety of tools have been developed to quantify uncertainty in point cloud registration, ranging from closed-form Gaussian solvers [11] to particle-based methods [6].\n\nWhile quantifying uncertainty helps to improve the registration, in practice, the magnitude of uncertainty is rarely sufficient in practical robotic perception tasks, as it does not reveal why registration failed or how to recover from it. Distinct failure causes — such as sensor noise, poor initialization, or occlusion [6] — require different actions to recover from failure.\nIn this work, we propose a novel approach, Gaussian Process Concept Attribution (GP-CA). A major difference to existing work is that, our approach not only quantifies uncertainty, but also explains uncertainty in a human-interpretable manner. Our GP-CA integrates active learning, learned point-cloud representations, and a Gaussian Process classifier to provide concept-level uncertainty attribution. The active learning allows fast adaptation to new uncertainty sources.\nIn preliminary work [12], the explainability method SHAP was used to analyze the influence of different uncertainty sources on ICP-based registration. However, it cannot identify which uncertainty source is present in a registration and depends on manual control over uncertainty sources and heavy computation, limiting applicability and hindering its feasibility for robotic decision-making.\n\nGP-CA attributes uncertainty to semantically meaningful concepts that can be used later to enable robots to select targeted recovery actions. An example is depicted in Fig. 1, where the robot is tasked to perform a 6D pose estimation of objects using the ICP algorithm. Initially, an object of interest (the bottle) is occluded by another object, which causes the ICP algorithm to fail. Using our method, the occlusion is identified as the cause of failure, thereby enabling targeted recovery actions (e.g., changing the viewpoint).\nHence, our main idea of explaining uncertainty in point cloud registration can improve the robustness of the perception tasks.\n\nWhen registration fails or reports high uncertainty, GP-CA embeds the ICP-aligned source point cloud using a learned representation. Then, a multi-class Gaussian Process classifier (GPC) maps these representations to confidence scores over different concepts. In this way, we attribute the uncertainty to specific concepts, thereby identifying its source. The concepts are predefined by sets of examples. Still, a robot may encounter unknown sources of uncertainty during its operations. Therefore, GP-CA is equipped with an active learning mechanism [13, 14], which can learn a new concept by querying the user for the label and choosing the most informative data to learn from. Across experiments on the LINEMOD [15], the YCB [16], and the Coffee Cup [17] dataset and a custom real-world RGB-D dataset from our laboratory, we validate GP-CA and provide extensive ablation studies. We further propose how GP-CA can enable a more robust robotic perception by executing a set of failure recovery actions associated with uncertainty explanations.\n\nIn summary, our main contributions are as follows.\n\nWe propose a novel approach, GP-CA, which enables human-interpretable uncertainty explanations in point cloud registration. To the best of our knowledge, this is the first work to show how explainability can advance point cloud registration for real-world applications\n\nWe extend our approach with active learning to adapt and integrate new concepts.\n\nWe validate the GP-CA design through an extensive ablation study\n\nWe demonstrate clearly the runtime-efficiency, the high accuracy, and the sample-efficiency of our method, with SOTA baseline comparison across four datasets.\n\nWe propose how GP-CA can be employed for recovery actions in a real-world robot setting.\n\n1. We propose a novel approach, GP-CA, which enables human-interpretable uncertainty explanations in point cloud registration. To the best of our knowledge, this is the first work to show how explainability can advance point cloud registration for real-world applications\n\n2. We extend our approach with active learning to adapt and integrate new concepts.\n\n3. We validate the GP-CA design through an extensive ablation study\n\n4. We demonstrate clearly the runtime-efficiency, the high accuracy, and the sample-efficiency of our method, with SOTA baseline comparison across four datasets.\n\n5. We propose how GP-CA can be employed for recovery actions in a real-world robot setting.",
            "llm_summary": "【关注的是什么问题】\n1. 点云配准中的不确定性量化与原因解释（≤40词）\n2. 如何将不确定性解释转化为人类可理解的概念级因果归因（≤40词）\n3. 在实际机器人场景中实现快速、鲁棒的失败恢复策略（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出高斯过程概念归因GP-CA，结合点云嵌入、GPC分类器实现对不确定性的概念级归因，并通过主动学习发现新不确定源；可用于在ICP失败时给出针对性的恢复操作，提升运行时鲁棒性与效率。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models",
            "authors": "Shijia Ge,Yinxin Zhang,Shuzhao Xie,Weixiang Zhang,Mingcai Zhou,Zhi Wang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "submitted to AAAI 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.18778",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18778",
            "arxiv_html_link": "https://arxiv.org/html/2509.18778v1",
            "abstract": "Visual imitation learning frameworks allow robots to learn manipulation skills from expert demonstrations. While existing approaches mainly focus on policy design, they often neglect the structure and capacity of visual encoders—limiting spatial understanding and generalization. Inspired by biological vision systems, which rely on both visual and proprioceptive cues for robust control, we propose VGGT-DP, a visuomotor policy framework that integrates geometric priors from a pretrained 3D perception model with proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer (VGGT) as the visual encoder and introduce a proprioception-guided visual learning strategy to align perception with internal robot states, improving spatial grounding and closed-loop control. To reduce inference latency, we design a frame-wise token reuse mechanism that compacts multi-view tokens into an efficient spatial representation. We further apply random token pruning to enhance policy robustness and reduce overfitting. Experiments on challenging MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines such as DP and DP3, particularly in precision-critical and long-horizon scenarios.",
            "introduction": "未获取到引言",
            "llm_summary": "【关注的是什么问题】\n1. 如何将视觉编码器的结构与能力用于提升机器人视觉-动作嵌入的泛化和精确控制\n2. 如何融合预训练3D感知模型的几何先验与机器人本体的 proprioceptive 信息以改善空间定位与闭环控制\n3. 如何降低多视角视觉输入的推理时延并提升鲁棒性\n4. 如何通过 token 重用和随机剪枝提升效率与抗过拟合能力\n【用了什么创新的方案】\n- 以 Visual Geometry Grounded Transformer (VGGT) 作为视觉编码器，将几何先验整合到 visuomotor 策略中\n- 引入 proprioception-guided visual learning，使感知对齐内部状态并增强空间着陆与闭环控制\n- 设计 frame-wise token reuse 机制，将多视图 token 压缩为高效的空间表示以降低推理延迟\n- 采用随机 token pruning 增强策略鲁棒性并抑制过拟合\n- 在 MetaWorld 任务上对比 DP、DP3，显示显著提升，尤其在精度关键与长时程场景\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning",
            "authors": "Omar Rayyan,John Abanes,Mahmoud Hafez,Anthony Tzes,Fares Abu-Dakka",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "For project website and videos, see httpsthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18757",
            "code": "https://mv-umi.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18757",
            "arxiv_html_link": "https://arxiv.org/html/2509.18757v1",
            "abstract": "Recent advances in imitation learning have shown great promise for developing robust robot manipulation policies from demonstrations. However, this promise is contingent on the availability of diverse, high-quality datasets, which are not only challenging and costly to collect but are often constrained to a specific robot embodiment. Portable handheld grippers have recently emerged as intuitive and scalable alternatives to traditional robotic teleoperation methods for data collection. However, their reliance solely on first-person view wrist-mounted cameras often creates limitations in capturing sufficient scene contexts. In this paper, we present MV-UMI (Multi-View Universal Manipulation Interface), a framework that integrates a third-person perspective with the egocentric camera to overcome this limitation. This integration mitigates domain shifts between human demonstration and robot deployment, preserving the cross-embodiment advantages of handheld data-collection devices. Our experimental results, including an ablation study, demonstrate that our MV-UMI framework improves performance in sub-tasks requiring broad scene understanding by approximately 47% across 3 tasks, confirming the effectiveness of our approach in expanding the range of feasible manipulation tasks that can be learned using handheld gripper systems, without compromising the cross-embodiment advantages inherent to such systems.\nVideos can be found here: https://mv-umi.github.io",
            "introduction": "Imitation Learning (IL) provides a compelling pathway toward acquiring general robot policies capable of performing long-horizon tasks across diverse environments. This approach, particularly through supervised methods like Behavioral Cloning (BC), enables robots to acquire complex behaviors by learning to imitate human-directed actions in response to observations. Recent advances in architectures that better model this mapping [1, 2, 3, 4], coupled with enhancements in embodiments and hardware integrations [5, 6], have made this route increasingly convincing.\n\nRecent studies on data scaling laws in imitation learning [7] show that robot policy performance follows training scenario diversity, emphasizing the need for extensive and diverse data for robust policies. Data collection typically lies between two extremes. On one end, robot teleoperation enables the acquisition of high-quality, precise data with minimal embodiment discrepancies. However, this method is time-consuming and costly, as it requires an actively operated robot. On the other end, the internet is replete with videos of humans performing various tasks. However, substantial effort is required to establish structured explicit mappings between observed states and actions from these videos.\n\nAs a middle-ground, portable handheld grippers [8, 9, 10, 11, 12] have emerged as inexpensive and intuitive to use data collection devices. By relying exclusively on a wrist-mounted camera, they enable non-experts to record demonstrations without the need for a robotic manipulator. While this egocentric viewpoint minimizes visual discrepancies between training and deployment, resulting in cross-embodiment policies, it demands that the robot maintain a longer memory context to recall scene elements that move out of the constraining wrist view.\n\nIn this work, we propose a novel framework that augments the conventional wrist-mounted camera in handheld gripper systems with a third-person camera viewpoint, without incurring distributional shifts. We achieve this by performing real-time masking of the human demonstrator in the third-person video stream, effectively removing the operator’s presence from the training data. As a result, the model benefits from a broader view of the environment, while relying less on memory for scene remembrance. A side benefit we find of this masking is its removal of correlations between the demonstrator’s motions and the gripper’s actions, encouraging the policy to focus on task-relevant cues such as the manipulated objects rather than overfitting to human-specific signals. We also utilize a custom-made three-jaw gripper for some of the tasks that require greater dexterity in this work. This design allows for greater payload weight in comparison to other hand-held devices, at the cost of its volume. Schematics and instructions to reproduce the hardware are open-sourced separately. The hardware aspect is not a key focus in this paper.\n\nSummary of Contributions:\n\nMulti-View Cross-Embodiment Framework: MV-UMI fuses wrist-mounted and third-person views using SAM-2 segmentation and inpainting to eliminate domain shift, boosting performance in context-dependent tasks by 47%.\n\nEnd-to-End Open-Source System: Complete pipeline, including hardware design, data collection, training code, and deployment tools, is publicly released to advance cross-embodiment manipulation research, https://mv-umi.github.io.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在跨 embodiment 的示范学习中利用多视角来缓解域间差异（跨人体/设备）的影响\n2. 如何在手持抓取设备的数据集上融合第一视角和第三视角以提升上下文理解能力\n3. 如何通过去除示范者在第三视角中的影像来减少示范者动作与抓手动作的相关性\n4. 如何在不增加分布漂移的前提下扩展数据视角以提升对场景理解的鲁棒性\n5. 如何在跨视角学习中保持对任务相关 cues 的聚焦，避免过拟合人类信号\n\n【用了什么创新的方案】\n核心解决方案：在手持夹具的第一视角与第三视角之间进行实时多视图融合，利用 SAM-2 对人类示范者进行分割并在第三视角视频中进行去人化处理，从而获得更广的环境观察并降低域偏移；结合自定义三撬式抓手以提高耐受载荷，输出端到端的开放源码数据收集、训练与部署管线；通过掩码化第三视角实现对示范者的去域化，减少示范者动作与抓取动作的相关性，提升跨嵌入学习的任务自适应性。实验结果显示在需要广域场景理解的子任务上约提升47%。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation",
            "authors": "Nishant Doshi,Amey Sutvani,Sanket Gujar",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18734",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18734",
            "arxiv_html_link": "https://arxiv.org/html/2509.18734v1",
            "abstract": "One of the challenges faced by Autonomous Aerial\nVehicles is reliable navigation through urban environments.\nFactors like reduction in precision of Global Positioning System\n(GPS), narrow spaces and dynamically moving obstacles make the\npath planning of an aerial robot a complicated task. One of the\nskills required for the agent to effectively navigate through such an\nenvironment is to develop an ability to avoid collisions using\ninformation from onboard depth sensors. In this paper, we propose\nReinforcement Learning of a virtual quadcopter robot agent\nequipped with a Depth Camera to navigate through a simulated\nurban environment.",
            "introduction": "In recent years, Quadcopters have been extensively used\nfor civilian task like object tracking, disaster rescue, wildlife\nprotection and asset localization. It presents interesting\napplication avenues especially in tasks such as automated mail\ndelivery system, fire protection and disaster management.\nHowever, quadcopter navigation through urban environments\nis a complex task because of frequent dynamic obstacles\n(Humans, Posters, etc.). Also, the GPS navigation system can\nperform poorly when surrounded by tall buildings in urban\nenvironment, dilating the precision of the 3D position fix. It\nbecomes more dangerous when the quadcopter is flying\nthrough tight spaces and is uncertain of its position, increasing\nthe chances of collision. The quadcopter also needs to take\nsmart action after detecting dynamic obstacles (Humans,\nVehicles, animals, traffic signals etc.) during navigation in\nruntime in urban environment. Traditionally, obstacle\navoidance techniques have been designed as end point solution\nin an aerial robot navigation. One of the promising approach\nfor this problem is deep reinforcement learning. In this paper a\nsimple model is developed for the task of detecting and\navoiding common civilian obstacles encountered by a\nquadcopter while navigating a path in an urban environment.\n\nFrom the reinforcement learning view, the main challenge\nhere is that, the policy should update itself during runtime for\nstochastic obstacles detected in the environment and take the\noptimal action accordingly. Also, the navigation problem has\nsparse distributed reward in state space which is a challenge for\nlearning the shortest distance.\n\nThe objective of this project is to train a quadcopter to\nnavigate without hitting obstacles and taking a shortest path\naround through a high-rise urban environment where stochastic\nand dynamic obstacles are frequent.\n\nThe organization of the paper is as follows: Section I\nprovides a general introduction to the challenges for\nquadcopter urban navigation. Section II provides a\nprerequisites required to understand the experiments. Section\nIII defines the problem outlining the agent used and the\nenvironment. Section IV gives a brief description about the\nAirSim simulator, while section V defines the solution\napproaches for the problem defined. Section VI describes the\nexperiments and training and testing arena used. Section VII\ndiscusses the results for the experiments, while section VIII\ndescribes the future attempt that can be made and section IX\ndescribes the challenges faced during the experiments.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在城市环境中实现四旋翼无人机的可靠导航。  \n2. 如何处理动态障碍物对无人机路径规划的影响。  \n3. 如何在稀疏奖励环境中优化深度强化学习策略。  \n\n【用了什么创新的方案】  \n本研究提出了一种基于深度强化学习的虚拟四旋翼机器人代理，利用深度摄像头在模拟城市环境中进行导航。该方法通过实时更新策略来应对环境中的随机障碍物，并优化路径选择以避免碰撞，同时解决了稀疏奖励的问题。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Query-Centric Diffusion Policy for Generalizable Robotic Assembly",
            "authors": "Ziyi Xu,Haohong Lin,Shiqi Liu,Ding Zhao",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "8 pages, 7 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.18686",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18686",
            "arxiv_html_link": "https://arxiv.org/html/2509.18686v1",
            "abstract": "The robotic assembly task poses a key challenge in building generalist robots due to the intrinsic complexity of part interactions and the sensitivity to noise perturbations in contact-rich settings. The assembly agent is typically designed in a hierarchical manner: high-level multi-part reasoning and low-level precise control. However, implementing such a hierarchical policy is challenging in practice due to the mismatch between high-level skill queries and low-level execution. To address this, we propose the Query-centric Diffusion Policy (QDP), a hierarchical framework that bridges high-level planning and low-level control by utilizing queries comprising objects, contact points, and skill information. QDP introduces a query-centric mechanism that identifies task-relevant components and uses them to guide low-level policies, leveraging point cloud observations to improve the policy’s robustness. We conduct comprehensive experiments on the FurnitureBench in both simulation and real-world settings, demonstrating improved performance in skill precision and long-horizon success rate. In the challenging insertion and screwing tasks, QDP improves the skill-wise success rate by over 50% compared to baselines without structured queries.",
            "introduction": "Contact-rich manipulation has been widely recognized as a critical task when building generalist intelligent robots [1, 2, 3]. In this field, robot assembly [4, 5] stands out because it requires policies that are both precise and versatile to control the robot arm and interact with multiple objects.\nDespite the access to some offline demonstration data from human priors, robotic assembly poses two key challenges: inter-part relational reasoning and intra-part precise control in the online deployment. As is visualized in Figure 1, the first challenge arises from the long-horizon, multi-part nature of the task, which demands accurate prediction of the next skill based on current observations, as well as identifying the correct objects for interaction. The second challenge becomes especially difficult in contact-rich scenarios where successful assembly hinges on precise object alignment. Even minor noise or occlusions in raw sensory observations can completely fail the sim-to-real transfer of low-level non-prehensile control policies.\n\nVarious methods have been explored to address these challenges. One line of work focuses on high-level reasoning by leveraging cross-modality affordance-based approaches [6], graph-based key point reasoning [7], or skill-based retrieval [8, 9]. However, these methods rely heavily on heuristic-based low-level controllers and predefined skill libraries, thus limiting their adaptability and precision.\nMeanwhile, recent advances in robot learning have significantly improved the precision and adaptability of low-level policies, thanks to (i) powerful imitation learning backbones such as diffusion models [10, 11, 12] and transformer-based architectures [13, 14], (ii) enhanced learning regimes like residual policies [15] and safe failure prevention methods [16], and (iii) the integration of sensor modalities beyond vision and proprioception, such as tactile sensing [17, 9] or point clouds [18] for improved contact modeling.\nStill, transferring these policies from simulation to the real world remains challenging due to the inherent difficulty of accurately simulating contact dynamics.\n\nIn addition to the individual challenges of high-level and low-level policy design, integrating these two levels in a hierarchical framework introduces further complications. For instance, high-level policies may mis-specify objects or skills, causing the low-level policy to rely on the false queries that include irrelevant factors such as background pixels or non-impactful objects, ultimately leading to failure in real-world environments. Hence, it is crucial to establish a parsimonious proxy between high-level and low-level modules.\n\nTo address these challenges and establish a robust interface between high-level and low-level modules, we propose Query-centric Diffusion Policy (QDP). Our framework leverages powerful pre-trained foundation models to extract high-level information by specifying both the requisite skills and target objects as a query. This query then serves as a powerful precondition to guide point cloud-based low-level control, forcing the robot agent to focus on the current task-relevant components when generating the action chunks under different contexts.\nOur contributions are threefold:\n\nWe introduce QDP, a query-centric diffusion policy framework that selects task-relevant queries for guiding low-level policies, enabling accurate skill selection and precise object interaction.\n\nWe introduce QDP, a query-centric diffusion policy framework that selects task-relevant queries for guiding low-level policies, enabling accurate skill selection and precise object interaction.\n\nWe propose a query-conditioned policy learning scheme to model the complex geometry captured by point cloud observations, improving precision and facilitating sim-to-real transfer.\n\nWe demonstrate the effectiveness of our approach on FurnitureBench in both simulation and real-world settings, showcasing robust performance under object misalignment and human intervention.\n\n1. We introduce QDP, a query-centric diffusion policy framework that selects task-relevant queries for guiding low-level policies, enabling accurate skill selection and precise object interaction.\n\n2. We propose a query-conditioned policy learning scheme to model the complex geometry captured by point cloud observations, improving precision and facilitating sim-to-real transfer.\n\n3. We demonstrate the effectiveness of our approach on FurnitureBench in both simulation and real-world settings, showcasing robust performance under object misalignment and human intervention.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂的机器人组装任务中实现高层次的多部件推理与低层次的精确控制。  \n2. 如何解决在接触丰富的环境中，低层次控制策略的噪声敏感性和仿真到现实的转移问题。  \n3. 如何有效整合高层和低层策略，以避免错误的对象或技能指定导致的失败。  \n\n【用了什么创新的方案】  \n提出了Query-centric Diffusion Policy (QDP)框架，通过查询机制选择任务相关的组件，指导低层策略的执行。QDP利用点云观察来增强政策的鲁棒性，改善技能选择的准确性和物体交互的精确性。该框架在FurnitureBench上进行了全面实验，展示了在技能精度和长时间成功率上的显著提升。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space",
            "authors": "Sangjun Noh,Dongwoo Nam,Kangmin Kim,Geonhyup Lee,Yeonguk Yu,Raeyoung Kang,Kyoobin Lee",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "7 main scripts + 2 reference pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18676",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18676",
            "arxiv_html_link": "https://arxiv.org/html/2509.18676v1",
            "abstract": "Learning robust visuomotor policies that generalize across diverse objects and interaction dynamics remains a central challenge in robotic manipulation. Most existing approaches rely on direct observation-to-action mappings or compress perceptual inputs into global or object-centric features, which often overlook localized motion cues critical for precise and contact-rich manipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework that leverages scene-level 3D flow as a structured intermediate representation to capture fine-grained local motion cues. Our approach predicts the temporal trajectories of sampled query points and conditions action generation on these interaction-aware flows, implemented jointly within a unified diffusion architecture. This design grounds manipulation in localized dynamics while enabling the policy to reason about broader scene-level consequences of actions. Extensive experiments on the MetaWorld benchmark show that 3D FDP achieves state-of-the-art performance across 50 tasks, particularly excelling on medium and hard settings. Beyond simulation, we validate our method on eight real-robot tasks, where it consistently outperforms prior baselines in contact-rich and non-prehensile scenarios. These results highlight 3D flow as a powerful structural prior for learning generalizable visuomotor policies, supporting the development of more robust and versatile robotic manipulation. Robot demonstrations, additional results, and code can be found at https://sites.google.com/view/3d-fdp.",
            "introduction": "Learning robust manipulation skills in unstructured environments is a fundamental challenge in robotics. Policies deployed in such settings must generalize across diverse object geometries, appearances, dynamics, and contextual variations. Visual imitation learning offers a scalable alternative by enabling robots to acquire visuomotor skills from expert demonstrations without requiring task-specific reward functions [1, 2]. Recent approaches [3, 4, 5, 6, 7] often employ end-to-end architectures that map perception directly to control, leveraging transformers whose attention mechanisms excel at capturing global context and integrating multiple modalities.\n\nMore recently, diffusion models have shown significant promise in this domain [8, 4]. By formulating action generation as an iterative denoising process, they provide stable training and support complex, multimodal action distributions. When combined with structured inputs such as point clouds, these models achieve strong performance across diverse manipulation tasks [9, 10, 11]. Despite these advances, most existing methods still follow a direct observation-to-action mapping, often compressing perceptual inputs into global or object-centric features. Such representations can overlook localized motion cues that are essential for precise and contact-rich manipulation. While some works introduce intermediate representations, approaches based on object poses [12, 13] can struggle to capture scene-level dynamics, and those predicting future frames [14, 15, 16, 17, 18, 19] are often computationally intensive.\n\nTo address these limitations, we introduce the 3D Flow Diffusion Policy (3D FDP), an architecture that explicitly models local interaction dynamics through an intermediate representation of 3D scene flow. Our method predicts the temporal trajectories of sampled 3D query points and uses this predicted flow to infer actions. Unlike prior approaches [12, 20] that rely on object-level representations, our model learns both flow prediction and action generation jointly within a unified diffusion framework (Fig. 1). This integrated design offers two key advantages. First, 3D flow captures fine-grained correlations between the robot gripper and manipulated objects, providing spatial grounding that supports contact-aware behavior. Second, by modeling how local motion propagates across the scene, the policy can reason about the downstream effects of manipulation, such as how inserting a book might disturb surrounding objects on a cluttered shelf. These capabilities improve generalization across both geometric and dynamic variations.\n\nWe evaluate 3D FDP on the MetaWorld benchmark [21] and a set of real-world manipulation tasks. On MetaWorld, our approach achieves state-of-the-art performance across all difficulty levels, with particularly strong improvements on medium and hard tasks. In real-robot experiments involving diverse physical interactions, 3D FDP demonstrates capabilities beyond baseline policies, successfully handling contact-rich and non-prehensile tasks where previous methods fail. These results demonstrate that 3D scene flow provides an effective inductive bias for learning generalizable visuomotor policies. Our main contributions are as follows:\n\nWe introduce a scene-level 3D flow representation that captures interaction-aware motion throughout the scene.\n\nWe develop a unified diffusion-based architecture that jointly learns 3D flow prediction and action generation in an end-to-end manner.\n\nWe demonstrate that 3D FDP achieves state-of-the-art results on 50 MetaWorld tasks and outperforms prior baselines in real-world manipulation scenarios.\n\nWe provide a comprehensive analysis that demonstrates the effectiveness of our 3D flow representation, investigating key aspects such as conditioning strategies and query point density.\n\n1. We introduce a scene-level 3D flow representation that captures interaction-aware motion throughout the scene.\n\n2. We develop a unified diffusion-based architecture that jointly learns 3D flow prediction and action generation in an end-to-end manner.\n\n3. We demonstrate that 3D FDP achieves state-of-the-art results on 50 MetaWorld tasks and outperforms prior baselines in real-world manipulation scenarios.\n\n4. We provide a comprehensive analysis that demonstrates the effectiveness of our 3D flow representation, investigating key aspects such as conditioning strategies and query point density.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何学习能够在多样化对象和交互动态中泛化的稳健视觉运动策略。  \n2. 现有方法如何忽视局部运动线索，影响接触丰富的操控精度。  \n3. 如何有效地结合3D场景流与动作生成以改善操控性能。  \n\n【用了什么创新的方案】  \n我们提出了3D Flow Diffusion Policy (3D FDP)，一种新颖的框架，通过场景级3D流作为结构化中间表示，捕捉细粒度的局部运动线索。该方法预测采样查询点的时间轨迹，并基于这些交互感知流条件生成动作，所有这些都在统一的扩散架构中共同实现。此设计使操控 grounded 在局部动态上，同时使策略能够推理动作的更广泛场景级后果。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout",
            "authors": "Kaixin Chai,Hyunjun Lee,Joseph J. Lim",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18671",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18671",
            "arxiv_html_link": "https://arxiv.org/html/2509.18671v1",
            "abstract": "In mobile manipulation, the manipulation policy has strong preferences for initial poses where it is executed. However, the navigation module focuses solely on reaching the task area, without considering which initial pose is preferable for downstream manipulation.\nTo address this misalignment, we introduce N2M, a transition module that guides the robot to a preferable initial pose after reaching the task area, thereby substantially improving task success rates. N2M features five key advantages: (1) reliance solely on ego-centric observation without requiring global or historical information; (2) real-time adaptation to environmental changes; (3) reliable prediction with high viewpoint robustness; (4) broad applicability across diverse tasks, manipulation policies, and robot hardware; and (5) remarkable data efficiency and generalizability.\nWe demonstrate the effectiveness of N2M through extensive simulation and real-world experiments.\nIn the PnPCounterToCab task, N2M improves the averaged success rate from 3% with the reachability-based baseline to 54%.\nFurthermore, in the Toybox Handover task, N2M provides reliable predictions even in unseen environments with only 15 data samples, showing remarkable data efficiency and generalizability.\nProject website: https://clvrai.github.io/N2M/",
            "introduction": "Mobile manipulators, which integrate mobility and environmental interaction capabilities, hold significant promise for a wide range of real-world applications.\nBy leveraging scene understanding Rana et al. (2023); Hughes et al. (2022); Rosinol et al. (2020) and navigation modules Zheng et al. (2025); Chai et al. (2024); Chang et al. (2023), these robots can reach the task area based on the task descriptions, and subsequently accomplish the task by executing pre-trained manipulation policies Fu et al. (2024); Chi et al. (2024); Black et al. (2024).\n\nHowever, existing works mainly focus on enhancing navigation and manipulation independently, while not giving sufficient attention to the interplay between them.\nIn this paper, we identify an inherent misalignment between navigation and manipulation, which significantly reduces the task success rate.\nSpecifically, due to factors such as joint limitation and training data distribution, the performance of the manipulation policy is sensitive to the initial pose from which execution begins.\nMeanwhile, navigation merely focuses on guiding the robot to task areas without considering which initial pose is preferable for executing the manipulation policy.\n\nThe most direct solution would be to develop an end-to-end model handling both navigation and manipulation Yang et al. (2024), thereby avoiding challenges in inter-module coordination. However, due to the inherent complexity of both navigation and manipulation, the design, training, and data collection for such end-to-end models remain an open problem.\nAn alternative approach within modular frameworks is to enhance the robustness of the manipulation policy. However, visuomotor policies are sensitive to viewpoint changes Heo et al. (2023), necessitating data collection from various initial poses throughout the task area Gu et al. (2022), which is costly.\n\nIn this paper, we propose a simple but effective transition module, named N2M (Navigation-to-Manipulation), serving as a bridge between navigation and manipulation. As depicted in Fig. 1, after reaching the task area, the robot is transferred from the end pose of navigation to an initial pose that is preferable for executing the manipulation policy, thereby improving the task success rate.\n\nWe identify five fundamental challenges for bridging navigation and manipulation, and propose our corresponding solutions.\n\nAdaptability to non-static environments.\nThe environments are typically non-static, requiring predictions to adapt to environmental changes. To support this, N2M predicts the preferable initial pose from the ego-centric RGB point cloud with a single forward pass. This efficient design enables N2M to generate real-time predictions in dynamic environments, as demonstrated in Section 5.2.\n\nMulti-modality of preferable initial poses.\nMultiple preferable initial poses may exist within the task area. Consequently, predicting a single pose is insufficient, as it can cause the model to learn an interpolation between viable poses Bishop (1994), which may not be preferable to execute manipulation policies. To address this multi-modality, N2M predicts the distribution of preferable initial poses, which is represented with a Gaussian Mixture Model (GMM) Bahl et al. (2023).\n\nCriterion of preferable initial poses.\nManipulation performance depends on multiple factors: policy architecture, training data distribution, robot configuration, task, and environment. Rather than attempting to model these complex relationships, we directly evaluate the pose through policy rollouts.\nDuring data collection, we position the robot at various poses and execute the manipulation policy, and successful execution indicates a preferable initial pose.\nLearning initial pose preferences directly from policy rollouts ensures that N2M’s predictions align with the policy’s actual performance while simultaneously enabling broad applicability across diverse policies, tasks, and robot hardware, as shown in Sections 4.2, 5.1, and 5.2.\n\nViewpoint Robustness.\nSince the robot navigation end poses can be anywhere within the task area, N2M needs to provide reliable predictions at various viewpoints. To achieve this, we augment N2M’s training data from multiple viewpoints. Experiments in Sections 4 and 5 demonstrate that N2M reliably predicts preferable initial poses across the whole task area. Interestingly, we note that our proposed data augmentation approach also significantly improves data efficiency and generalizability. We will further analyze the reason behind these benefits in Section 6.\n\nData Efficiency.\nCollecting rollouts requires substantial time and human effort, as each rollout must be monitored and manually labeled with success or failure.\nWe incorporate two main strategies to make N2M data-efficient:\nFirst, we design the module to directly predict the initial pose distribution, rather than low-level action Lee et al. (2019);\nSecond, we augment the dataset through viewpoint rendering to increase its coverage and diversity. In Sections 4.3, 4.4, and 5, we demonstrate that N2M has remarkable data efficiency and generalizability.\n\nOur contributions are as follows:\n\nFirst, we identify a critical misalignment between navigation and manipulation modules and introduce N2M, which predicts preferable initial poses on the fly from ego-centric observations.\n\nSecond, we propose learning policy initial pose preferences from rollouts, without making additional assumptions about tasks, policies, and hardware, thereby endowing N2M with broad applicability.\n\nThird, we achieve remarkable data efficiency and generalizability through a novel data augmentation approach combined with our carefully designed input-output architecture.\n\nFinally, we conduct extensive experiments validating the effectiveness of our proposed N2M module across various settings and release our code to facilitate community exploration.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有导航和操作模块之间的协调不足，导致任务成功率低。  \n2. 操作策略对初始姿态的敏感性未被导航模块考虑。  \n3. 数据收集和训练过程中的高成本和低效率问题。  \n\n【用了什么创新的方案】  \nN2M（Navigation-to-Manipulation）模块作为导航与操作之间的桥梁，实时预测可执行操作的初始姿态。它依赖于自我中心的RGB点云进行高效预测，采用高斯混合模型表示多种可行姿态，直接从策略回放中学习初始姿态偏好，显著提高数据效率和泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Distributionally Robust Safe Motion Planning with Contextual Information",
            "authors": "Kaizer Rahaman,Simran Kumari,Ashish R. Hota",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18666",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18666",
            "arxiv_html_link": "https://arxiv.org/html/2509.18666v1",
            "abstract": "We present a distributionally robust approach for collision avoidance by incorporating contextual information. Specifically, we embed the conditional distribution of future trajectory of the obstacle conditioned on the motion of the ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional kernel mean embedding operator. Then, we define an ambiguity set containing all distributions whose embedding in the RKHS is within a certain distance from the empirical estimate of conditional mean embedding learnt from past data. Consequently, a distributionally robust collision avoidance constraint is formulated, and included in the receding horizon based motion planning formulation of the ego agent. Simulation results show that the proposed approach is more successful in avoiding collision compared to approaches that do not include contextual information and/or distributional robustness in their formulation in several challenging scenarios.",
            "introduction": "Safe motion planning in uncertain and dynamic environments is a fundamental challenge in the field of robotics and autonomous systems [1, 2]. Past works have proposed a plethora of approaches, including sampling-based methods [3], collision avoidance using velocity obstacles [4], and optimization-based techniques [5, 6]. In recent years, optimal control with a receding horizon implementation, popularly known as model predictive control (MPC), have gained popularity as it enables the designer to systematically include the effects of robot dynamics, constraints on robot states and inputs as well as collision avoidance constraints while planning the motion of the robot.\n\nWhile early works on MPC based motion planning focused on avoiding static obstacles [7, 8, 6], more recent works have considered dynamic obstacles in the robust and stochastic MPC frameworks. While robust MPC tends to result in conservative solutions, stochastic MPC techniques formulate collision avoidance conditions in terms of probabilistic constraints or via suitable risk measures [9]. However, the safety guarantees provided by the above techniques may not hold when the probability distribution of the future obstacle position is unknown and/or time-varying.\n\nConsequently, several recent works have proposed distributionally robust motion planning techniques where the stochastic collision avoidance constraints hold for an entire family of distributions (or ambiguity set) that are close (in terms of the Wasserstein distance) to the empirical distribution constructed from the observed data [10, 11, 12, 13, 14]. However, not all available past data are equally relevant at a given context or scenario. In particular, when we consider multiple agents that share the same environment, and are in the vicinity of each other, the future position of the obstacle is a function of the future position of the ego agent. Therefore, in order to obtain more accurate solutions, it is essential to consider ambiguity sets that depend on the actions of the ego agent and other available contextual information. Despite its significance, there is no prior work which considers decision-dependent ambiguity sets or any other contextual information while formulating the distributionally robust safe motion planning problem.\n\nA few recent works have explicitly included the behavior of the ego vehicle for trajectory prediction. Specifically, [15] investigated the impact of ego vehicle planning on nearby vehicles’ trajectories using an LSTM-based encoder combined with a convolutional social pooling module. The authors in [16] modeled each vehicle as a wave characterized by amplitude and phase, proposing that wave-pooling better captures dynamic states and high-order interactions through wave superposition. More recently, [17] provided a detailed comparison of several contextual trajectory prediction techniques. However, the robustness of neural network based trajectory prediction to distribution shifts is not adequately explored in the literature. Neural network based techniques also do not provide any rigorous (e.g., finite-sample) guarantees on the probability of collision among vehicles.\n\nIn this paper, we aim to fill this research gap. First, we compute the empirical estimate of the conditional kernel mean embedding (CKME) [18] of the (conditional) distribution of the future obstacle position as a function of the (i.e., conditioned on the) current states and the predicted trajectory of the ego agent. Then, we formulate an optimal control problem that embeds the ego-conditioned predicted trajectory in the constraints. In particular, the CKME operator provides a closed-form expression of the future trajectory of other vehicles as a function of the states and inputs of the ego vehicle, i.e., the decision variables of the optimal control problem. In order to robustify against distribution shifts, the conditional value at risk of the collision avoidance constraint is required to hold for all distributions whose mean embeddings are within a specified maximum-mean discrepancy (MMD) distance from the empirical estimate of the CKME; a similar approach was recently examined in [19] in the context of optimal control. Another recent paper [20] explored RKHS to select probable trajectories and adopted a sampling-based optimization approach, which is distinct from our approach. We present a tractable approximation of the above constraints following the reformulations developed in [21, 22] such that off-the-shelf nonlinear optimization solvers, such as IPOPT, can be deployed to solve the optimal control problem.111Neural network based predictors do not provide a simple closed form expression of the predicted trajectories as a function of the input applied to the ego vehicle. Therefore, computing optimal control inputs with predicted trajectories used in the constraints is not straightforward.  We provide detailed simulation results involving an autonomous ground vehicle moving on a road with obstacle agent(s), and show that the proposed approach successfully avoids collision in several challenging scenarios.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在不确定和动态环境中实现安全的运动规划。  \n2. 如何有效利用上下文信息来增强碰撞避免的鲁棒性。  \n3. 如何处理未来障碍物位置的概率分布未知或时变的情况。  \n\n【用了什么创新的方案】  \n本文提出了一种基于条件核均值嵌入（CKME）的分布鲁棒运动规划方法，通过将障碍物未来轨迹的条件分布嵌入到再生核希尔伯特空间（RKHS）中，构建了一个依赖于自我代理动作的模糊集合。通过优化控制问题，确保在所有与CKME的经验估计保持在特定距离内的分布下，碰撞避免约束的条件值风险得以满足。该方法在多个挑战场景中展现出优越的碰撞避免能力。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer",
            "authors": "Yarden As,Chengrui Qu,Benjamin Unger,Dongho Kang,Max van der Hart,Laixi Shi,Stelian Coros,Adam Wierman,Andreas Krause",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18648",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18648",
            "arxiv_html_link": "https://arxiv.org/html/2509.18648v1",
            "abstract": "Safety remains a major concern for deploying reinforcement learning (RL) in real-world applications. Simulators provide safe, scalable training environments, but the inevitable sim-to-real gap introduces additional safety concerns, as policies must satisfy constraints in real-world conditions that differ from simulation. To address this challenge, robust safe RL techniques offer principled methods, but are often incompatible with standard scalable training pipelines. In contrast, domain randomization, a simple and popular sim-to-real technique, stands out as a promising alternative, although it often results in unsafe behaviors in practice. We present SPiDR, short for Sim-to-real via Pessimistic Domain Randomization—a scalable algorithm with provable guarantees for safe sim-to-real transfer. SPiDR uses domain randomization to incorporate the uncertainty about the sim-to-real gap into the safety constraints, making it versatile and highly compatible with existing training pipelines. Through extensive experiments on sim-to-sim benchmarks and two distinct real-world robotic platforms, we demonstrate that SPiDR effectively ensures safety despite the sim-to-real gap while maintaining strong performance.",
            "introduction": "Reinforcement learning (RL) has made significant strides in recent years, demonstrating remarkable progress across a range of domains. These include achieving superhuman capabilities in games (Mnih et al., 2015; Silver et al., 2016), fine-tuning large language models (Ouyang et al., 2022), advancing applications in healthcare (Fox et al., 2020; Zhu et al., 2020), robotics (Lee et al., 2020; Degrave et al., 2022; Lin et al., 2025) and autonomous driving (Cusumano-Towner et al., 2025; Cornelisse et al., 2025).\nYet despite these achievements, ensuring safety and preventing harmful behaviors remains a critical challenge and a prerequisite for unlocking the full potential of RL as a ubiquitous element in everyday life (Amodei et al., 2016; Gu et al., 2022).\n\nThe use of simulators has been a key component behind the success of many of the mentioned applications  (Visentin et al., 2014; Makoviychuk et al., 2021; Degrave et al., 2022; Kazemkhani et al., 2024). Training in simulation allows agents to learn from unsafe interactions, which in reality would lead to catastrophic outcomes. In addition, learning complex behaviors fully online can be prohibitively time-consuming. Modern simulators accelerate training, reducing hours of real-world experience to minutes on consumer-grade GPUs (Rudin et al., 2022). However, while being a major driver in the development of the above examples, even state-of-the-art simulators often fall short in precisely mirroring the real-world. Indeed, “all models are wrong” (Box, 1976)—the so-called sim-to-real gap can make simulation-trained policies violate real-world constraints, which can be particularly dangerous in high-stakes settings where safety must be guaranteed on first contact.\n\nExisting literature to address this challenge often relies on tools from robust optimization (Queeney and Benosman, 2024; Kitamura et al., 2024; Zhang et al., 2024). While being theoretically grounded, such methods typically require practitioners to significantly alter their existing training pipelines, rendering them less prevalent in practice. In contrast, due to its simplicity, domain randomization has become the de facto tool for sim-to-real transfer (Tobin et al., 2017; Peng et al., 2018; Lee et al., 2020; Degrave et al., 2022). Despite its success, in problems that require adherence to safety constraints, domain randomization lacks safety guarantees and often fails to satisfy the constraints in practice (cf. Queeney and Benosman, 2024, and Figure˜3). Therefore, a method that provably guarantees safe sim-to-real transfer, while being highly compatible with standard training practices, is still missing.\n\nIn this work, we address this gap by presenting a simple method that builds on domain randomization while ensuring safety under sim-to-real transfer. We theoretically show that unsafe transfer can be associated with large uncertainty about the sim-to-real gap, quantified as the disagreement among next-state predictions from domain-randomized dynamics models. This key idea is illustrated in Figure˜1, where spikes in uncertainty (e.g. at t=4.6t=4.6 and t=5.3t=5.3) coincide with unstable or unsafe behaviors, such as stumbling or flipping. Motivated by this insight, we propose to penalize the cost with the uncertainty to achieve safe sim-to-real transfer, leading to the design of SPiDR. Notably, SPiDR seamlessly integrates with state-of-the-art RL algorithms (Schulman et al., 2017; Haarnoja et al., 2019), delivering strong empirical performance on both in simulated and real-world safe RL tasks while ensuring constraint satisfaction, even under severe model mismatch.\n\nWe address an important challenge to real-world adoption of RL: zero-shot safe sim-to-real transfer, where an agent must learn a safe and effective policy using only simulated interactions. We propose SPiDR, a practical algorithm with formal safety guarantees that integrates easily into popular sim-to-real pipelines.\n\nWe address an important challenge to real-world adoption of RL: zero-shot safe sim-to-real transfer, where an agent must learn a safe and effective policy using only simulated interactions. We propose SPiDR, a practical algorithm with formal safety guarantees that integrates easily into popular sim-to-real pipelines.\n\nWe validate SPiDR on two real-world robotic platforms, where it achieves zero-shot constraint satisfaction, substantially outperforming other baselines in terms of safety and performance. These results provide empirical evidence that our theoretical guarantees translate to the real-world, suggesting that SPiDR can be safely used in real-world deployment.\n\nFinally, we extensively evaluate SPiDR on well established simulated continuous control benchmarks, including the RWRL benchmark (Dulac-Arnold et al., 2020), Safety Gym (Ray et al., 2019) and RaceCar environments (Kabzan et al., 2020), where SPiDR consistently satisfies safety constraints while achieving strong task performance.\n\n1. We address an important challenge to real-world adoption of RL: zero-shot safe sim-to-real transfer, where an agent must learn a safe and effective policy using only simulated interactions. We propose SPiDR, a practical algorithm with formal safety guarantees that integrates easily into popular sim-to-real pipelines.\n\n2. We validate SPiDR on two real-world robotic platforms, where it achieves zero-shot constraint satisfaction, substantially outperforming other baselines in terms of safety and performance. These results provide empirical evidence that our theoretical guarantees translate to the real-world, suggesting that SPiDR can be safely used in real-world deployment.\n\n3. Finally, we extensively evaluate SPiDR on well established simulated continuous control benchmarks, including the RWRL benchmark (Dulac-Arnold et al., 2020), Safety Gym (Ray et al., 2019) and RaceCar environments (Kabzan et al., 2020), where SPiDR consistently satisfies safety constraints while achieving strong task performance.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在sim-to-real转移中确保安全性，尤其是零-shot情况下。  \n2. 现有的安全强化学习方法在实际应用中往往不兼容标准训练流程。  \n3. 域随机化在满足安全约束方面的不足。  \n\n【用了什么创新的方案】  \nSPiDR（Sim-to-real via Pessimistic Domain Randomization）是一种新颖的算法，通过将对sim-to-real差距的不确定性纳入安全约束，确保在模拟到现实的转移中实现安全性。该方法与现有的强化学习算法无缝集成，能够在模拟和现实世界中有效地满足安全约束，同时保持强大的任务性能。SPiDR的设计使其在保持高兼容性的同时，提供了形式化的安全保证。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Do You Need Proprioceptive States in Visuomotor Policies?",
            "authors": "Juntu Zhao,Wenbo Lu,Di Zhang,Yufeng Liu,Yushen Liang,Tianluo Zhang,Yifeng Cao,Junyuan Xie,Yingdong Hu,Shengjie Wang,Junliang Guo,Dequan Wang,Yang Gao",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18644",
            "code": "https://statefreepolicy.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18644",
            "arxiv_html_link": "https://arxiv.org/html/2509.18644v1",
            "abstract": "Imitation-learning–based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0% to 85% in height generalization and from 6% to 64% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment.",
            "introduction": "Imitation-learning-based visuomotor policies [1, 2, 3, 4, 5] have been widely used in robotic manipulation.\nLeveraging large-scale demonstration datasets [6, 7, 8, 9] and fine-tuning powerful pre-trained policies have enabled robots to achieve remarkable performance across diverse real-world tasks.\n\nFor precise and reliable control, these visuomotor policies typically incorporate not only visual observations of the task environment but also proprioceptive state (hereafter referred to as state) inputs [10, 3, 11], such as end-effector poses and joint angles. The state inputs provide compact and accurate information about the robot’s configuration, but they also make the policy prone to overfitting by simply memorizing the training trajectories.\nTherefore it severely limits spatial generalization [12, 13, 14] if the training data lacks diversity [15].\nIn today’s context, where collecting demonstration data with wide state coverage (i.e., diverse spatial locations of task-relevant objects) is prohibitively expensive, this has become a critical bottleneck for the development of visuomotor policies.\n\nIn this study, we propose to completely remove the state input in visuomotor policies to enhance their spatial generalization ability, hereafter referred to as “State-free Policies.”\nThis design is built upon two conditions:\n\nRelative end-effector (EEF) action space [16]: The visuomotor policies predict relative displacements of the end-effector based on the current observation. Among different action spaces, the relative EEF action space most naturally supports the generalization of policies.\n\nFull task observation: Another key condition for effective State-free Policies is to ensure sufficient task-relevant visual information, which we term “full task observation”. This enables visuomotor policies to fully “see” the task-relevant objects in the task.\n\nThis mechanism of State-free Policies forces the policy to develop a deeper understanding of the task environment rather than simply memorizing the trajectories, thereby enabling State-free Policies to achieve advantages that state-based policies cannot provide:\n\nSpatial Generalization: Since State-free Policies do not rely on state inputs, they avoid overfitting to the training trajectories.\nTherefore, they exhibit strong height and horizontal generalization abilities, where height refers to variations of the task-relevant object’s location in the vertical direction, and horizontal refers to variations of the object’s location in the 2D plane.\n\nData efficiency: Even in in-domain settings, state-based policies require diverse demonstrations to avoid overfitting to specific trajectories. In contrast, removing the state input eliminates this dependence on trajectory diversity, allowing State-free Policies to be fine-tuned with less demonstration data. This reduces the cost of data collection, which is often a major bottleneck in deploying real-world robots.\n\nCross-embodiment adaptation: Since State-free Policies rely only on visual inputs and predict actions in the relative EEF space, they exhibit stronger cross-embodiment adaptation ability than state-based policies. They do not require additional adaptation to different state spaces, so the same task can be easily adapted to new embodiments with fewer fine-tuning steps.\n\nCross-embodiment adaptation: Since State-free Policies rely only on visual inputs and predict actions in the relative EEF space, they exhibit stronger cross-embodiment adaptation ability than state-based policies. They do not require additional adaptation to different state spaces, so the same task can be easily adapted to new embodiments with fewer fine-tuning steps.\n\nWe have conducted extensive experiments across a diverse range of tasks, robot embodiments, and policy architectures.\nIn both real-world and simulation environments, State-free Policies achieve comparably great in-domain performance to state-based policies.\nMost importantly, when trained on strictly collected real-world demonstration data\n(i.e., the task-relevant object location has a constrained initial distribution range),\nState-free Policies exhibit significantly stronger spatial generalization ability than state-based policies.\nFor further benefits, e.g., data efficiency and cross-embodiment adaptation ability, they also demonstrate obvious advantages over state-based policies, highlighting their potential for scalable and practical deployment in real-world robotic systems.\n\n1. Relative end-effector (EEF) action space [16]: The visuomotor policies predict relative displacements of the end-effector based on the current observation. Among different action spaces, the relative EEF action space most naturally supports the generalization of policies.\n\n2. Full task observation: Another key condition for effective State-free Policies is to ensure sufficient task-relevant visual information, which we term “full task observation”. This enables visuomotor policies to fully “see” the task-relevant objects in the task.\n\n1. Spatial Generalization: Since State-free Policies do not rely on state inputs, they avoid overfitting to the training trajectories.\nTherefore, they exhibit strong height and horizontal generalization abilities, where height refers to variations of the task-relevant object’s location in the vertical direction, and horizontal refers to variations of the object’s location in the 2D plane.\n\n2. Data efficiency: Even in in-domain settings, state-based policies require diverse demonstrations to avoid overfitting to specific trajectories. In contrast, removing the state input eliminates this dependence on trajectory diversity, allowing State-free Policies to be fine-tuned with less demonstration data. This reduces the cost of data collection, which is often a major bottleneck in deploying real-world robots.\n\n1. Cross-embodiment adaptation: Since State-free Policies rely only on visual inputs and predict actions in the relative EEF space, they exhibit stronger cross-embodiment adaptation ability than state-based policies. They do not require additional adaptation to different state spaces, so the same task can be easily adapted to new embodiments with fewer fine-tuning steps.",
            "llm_summary": "【关注的是什么问题】  \n1. 过度依赖proprioceptive状态输入导致策略对训练轨迹的过拟合。  \n2. 现有的visuomotor策略在空间泛化能力上存在严重限制。  \n3. 数据收集的多样性不足成为开发visuomotor策略的瓶颈。  \n\n【用了什么创新的方案】  \n提出了一种“无状态策略”（State-free Policy），完全去除proprioceptive状态输入，仅基于视觉观察预测动作。这种策略在相对末端执行器动作空间中构建，确保充分的任务相关视觉信息，从而增强空间泛化能力。实验结果表明，该策略在多种真实世界任务中表现出显著的空间泛化能力、数据效率和跨躯体适应性，提升了实际部署的可行性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments",
            "authors": "Yuan Zhou,Jialiang Hou,Guangtong Xu,Fei Gao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18636",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18636",
            "arxiv_html_link": "https://arxiv.org/html/2509.18636v1",
            "abstract": "Formation maintenance with varying number of drones in narrow environments hinders the convergence of planning to the desired configurations. To address this challenge, this paper proposes a formation planning method guided by Deformable Virtual Structures (DVS) with continuous spatiotemporal transformation. Firstly, to satisfy swarm safety distance and preserve formation shape filling integrity for irregular formation geometries, we employ Lloyd algorithm for uniform P​A¯\\underline{PA}rtitioning and Hungarian algorithm for A​S¯\\underline{AS}signment (PAAS) in DVS. Subsequently, a spatiotemporal trajectory involving DVS is planned using primitive-based path search and nonlinear trajectory optimization. The DVS trajectory achieves adaptive transitions with respect to a varying number of drones while ensuring adaptability to narrow environments through affine transformation. Finally, each agent conducts distributed trajectory planning guided by desired spatiotemporal positions within the DVS, while incorporating collision avoidance and dynamic feasibility requirements. Our method enables up to 15% of swarm numbers to join or leave in cluttered environments while rapidly restoring the desired formation shape in simulation. Compared to cutting-edge formation planning method, we demonstrate rapid formation recovery capacity and environmental adaptability. Real-world experiments validate the effectiveness and resilience of our formation planning method.",
            "introduction": "In recent years, formation flight becomes the foundation requirement for aerial swarms in practical applications, such as collaborative exploration [1], light show [2], search and rescue [3]. For large-scale swarms [4], [5], formation inevitably encounters agent loss in narrow environments [6, 7, 8]. Naturally, the integration of new members can restore the impaired formation and enhance the efficiency of mission execution [9], [10]. Therefore, dynamic adaptation of formation reconfiguration for changes in the number of drones is essential to ensure operational resilience under unforeseen disturbance. Besides, formation needs to experience adaptively overall shape transformations to navigate through narrow environments and maintain its configuration as much as possible to keep resilience [11, 12, 13, 14].\n\nWhen the number of drones in formations varies, the abrupt variation of formation cooperative constraints tends to degrade the feasibility of trajectory optimization. In narrow environments, the inherent conflict between formation maintenance and collision avoidance also deteriorates the feasibility, resulting in no solution for the classical planning method without global formation adjustment mechanism.\n\nNumerous works have demonstrated formation systems with swarm number variations.\nArtificial Potential Fields (APF) [4], [8], [15] are commonly used to guide the formation generation with variable swarm numbers. This approach eliminates the need for explicit assignment of desired positions, enabling adaptive number variation of formation. However, the slow convergence rate and inadequate swarm collision avoidance hinder its applicability in real-time formation planning. Although explicit position and assignment during number variations enable rapid formation recovery [16], [17], [18], they often fail to guarantee robust performance in narrow environments, where collision avoidance remains challenging during formation recovery.\nAdaptive deformation of the formation shape is widely adopted for safe and rapid navigation through obstacle environments [11], [13]. Nevertheless, distributed cooperative planning is insufficient without effective guidance for formation in narrow environments. Consequently, adjusting the overall deformation of the swarm provides an effective resolution [11]. However, this category of approaches is limited by either locally reactive mechanisms [13] and simply adjusts the local target that hinders formation guidance and environmental adaptability [11], or an excessive pursuit of collision-free passage that severely compromises formation maintenance integrity while neglecting post-maneuver recovery capability. These methods often struggle to rapidly restore desired formations in cluttered environments. Furthermore, most studies of formation planning do not address adaptive adjustment with variations in swarm number [11], [19], [20].\n\nTo achieve real-time formation flight with a variable number of drones, we propose a number adaptive formation planning system. According to the desired formation shape, we employ explicit partitioning and assignment to designate formation positions for the swarm. We focus on designing a Deformable Virtual Structures (DVS) trajectory with spatiotemporal motion and deformation that can effectively guide formation recovery and maintenance. Specifically, we introduce virtual rigid structures with two additional degrees of freedom, including scaling and affine transformation in DVS. The concept of virtual structures has been applied in multiple fields, but predominantly uses rigid virtual structures which are unable to adaptively deform [21, 22, 23]. In this work, we design a spatiotemporal trajectory optimization method for DVS, which utilizes compact piecewise polynomial representations in Cartesian and deformable spaces to formulate nonlinear optimization problems. This enables adaptive adjustment of the scale of formations according to swarm number variations, while permitting appropriate affine deformation of the virtual structures to facilitate the formations to navigate in narrow environments. Integrating PAAS and DVS with spatiotemporal transformation, we designate this framework as Deformation Guidance (DG), and agents perform distributed trajectory planning under the guidance of DG. Extensive simulation and real-world experiments demonstrate that our method supports variable number formation planning while achieving a high success rate in navigating through narrow environments.\n\nThe contributions of this paper are as follows:\n\nTo satisfy swarm safety distance and preserve shape filling integrity for irregular formation geometries, we implement the Lloyd algorithm for uniform partitioning in formation planning with variable number, with assignment via the Hungarian algorithm. We experimentally verified that the computational efficiency of PAAS is sufficient to achieve real-time planning.\n\nTo satisfy swarm safety distance and preserve shape filling integrity for irregular formation geometries, we implement the Lloyd algorithm for uniform partitioning in formation planning with variable number, with assignment via the Hungarian algorithm. We experimentally verified that the computational efficiency of PAAS is sufficient to achieve real-time planning.\n\nWe propose a spatiotemporal trajectory optimization method for DVS with scaling and affine transformation to guide formation planning in narrow environments. We employ a primitive path search to efficiently obtain high-dimensional DVS paths as initial values for DVS trajectory optimization, which can be transformed into unconstrained optimization problems for real-time solution.\n\nWe incorporate DVS-based spatiotemporal formation constraint in distributed trajectory optimization to achieve rapid formation recovery and enhanced environmental adaptability. Extensive experiments and benchmarks validate the efficiency and advantages of our system.",
            "llm_summary": "【关注的是什么问题】  \n1. 变动数量的无人机在狭窄环境中的编队维护问题。  \n2. 传统规划方法在动态调整编队形状和避免碰撞方面的局限性。  \n3. 如何实现实时的编队飞行以适应无人机数量的变化。  \n\n【用了什么创新的方案】  \n本文提出了一种基于可变形虚拟结构（DVS）的编队规划方法，通过连续的时空变换来指导编队恢复和维护。采用Lloyd算法进行均匀划分，并结合匈牙利算法进行分配，以满足安全距离和形状完整性要求。设计了一种具有缩放和仿射变换能力的DVS轨迹优化方法，能够在狭窄环境中有效导航。通过将DVS与时空变换相结合，支持动态的编队调整，确保在复杂环境中快速恢复所需的编队形状。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training",
            "authors": "Shuo Cheng,Liqian Ma,Zhenyang Chen,Ajay Mandlekar,Caelan Garrett,Danfei Xu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18631",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18631",
            "arxiv_html_link": "https://arxiv.org/html/2509.18631v1",
            "abstract": "Behavior cloning has shown promise for robot manipulation, but real-world demonstrations are costly to acquire at scale. While simulated data offers a scalable alternative, particularly with advances in automated demonstration generation, transferring policies to the real world is hampered by various simulation and real domain gaps. In this work, we propose a unified sim-and-real co-training framework for learning generalizable manipulation policies that primarily leverages simulation and only requires a few real-world demonstrations. Central to our approach is learning a domain-invariant, task-relevant feature space. Our key insight is that aligning the joint distributions of observations and their corresponding actions across domains provides a richer signal than aligning observations (marginals) alone. We achieve this by embedding an Optimal Transport (OT)-inspired loss within the co-training framework, and extend this to an Unbalanced OT framework to handle the imbalance between abundant simulation data and limited real-world examples. We validate our method on challenging manipulation tasks, showing it can leverage abundant simulation data to\nachieve up to a 30% improvement in the real-world success rate\nand even generalize to scenarios seen only in simulation.",
            "introduction": "Behavior cloning pomerleau1988alvinn  is a promising approach for acquiring robot manipulation skills directly in the real world, due to its simplicity and effectiveness in mimicking expert demonstrations robomimic2021 ; florence2022implicit . However, achieving robust and generalizable performance requires collecting large-scale datasets khazatsky2024droid ; open_x_embodiment_rt_x_2023  across diverse environments, object configurations, and tasks. This data collection process is labor-intensive, time-consuming, and costly, posing significant challenges to scalability in real-world applications.\n\nRecently, with rapid advancements in physics simulators Genesis ; Xiang_2020_SAPIEN , procedural scene generation raistrick2024infinigen ; deitke2022️ , and motion synthesis techniques mandlekar2023mimicgen ; cheng2023nod , there has been growing interest in leveraging simulation as an alternative source of training data. These simulation-based approaches enable scalable and controllable data generation, allowing for diverse and abundant supervision at a fraction of the real-world cost. However, transferring policies trained in simulation to the physical world remains a non-trivial challenge due to sim-to-real gap—the discrepancies between the simulated and real-world environments that a policy encounters during execution. These differences can manifest in various forms, such as variations in visual appearance, sensor noise, and action dynamics andrychowicz2020learning ; tobin2017domain . In particular, learning visuomotor control policies that remain robust under changing perceptual conditions during real-world deployment continues to be an open area of research.\n\nCommon strategies to bridge this domain gap include domain randomization andrychowicz2020learning ; tobin2017domain  and data augmentation hansen2020self ; yarats2021mastering , though these often require careful tuning. Domain adaptation (DA) techniques aim to explicitly align distributions, either at pixel bousmalis2017unsupervised ; james2019sim  or feature levels tzeng2014deep ; long2015learning ; zhao2019learning . However, many feature-level methods align only marginal observation distributions (e.g., MMD tzeng2014deep ; long2015learning ), which can be insufficient for fine-grained manipulation alignment as it may not preserve action-relevant relationships across domains. More recently, sim-and-real co-training—simply training a single policy on mixed data from both domains wei2025empirical ; maddukuri2025sim —has shown surprising effectiveness. We argue that while beneficial for data diversity, such co-training approaches typically lack explicit constraints for feature space alignment across domains, potentially hindering optimal transfer and generalization because they don’t enforce a consistent mapping of task-relevant structures.\n\nWe present a unified sim-and-real co-training framework that explicitly learns a shared latent space where observations from simulation and the real world are aligned and preserve action-relevant information. Our key insight is that aligning the joint distributions of observations and their corresponding actions or task-relevant states across domains provides a direct signal for learning transferable features.\nConcretely, we leverage Optimal Transport (OT) courty2016optimal  as an alignment objective to learn representations where the geometric relationships crucial for action prediction are consistent, irrespective of whether the input comes from simulation or the real world. Further more, to robustly handle the data imbalance in co-training with abundant simulation data and limited real-world data, we further extend to an Unbalanced OT (UOT) formulation fatras2021unbalanced ; chizat2018scaling  and develop a temporally-aware sampling strategy to improve domain alignment learning in a mini-batch OT setting.\n\nOur contributions are: (1) a sim-and-real co-training framework that learns a domain-invariant yet task-salient latent space to improve real-world performance with abundant simulation data, (2) an Unbalanced Optimal Transport framework and temporally-aware sampling strategy to mitigate data imbalance and improve alignment quality in mini-batch OT training, (3) comprehensive experiments using both image and point-cloud modalities, evaluating sim-to-sim and sim-to-real transfer across diverse manipulation tasks, demonstrating up to a 30% average success rate improvement and achieving generalization to real-world scenarios for which the training data only appears in simulation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效地将模拟数据中的策略转移到真实世界中。  \n2. 如何处理模拟与真实世界之间的域间差距。  \n3. 如何在有限的真实世界示例下利用丰富的模拟数据进行训练。  \n\n【用了什么创新的方案】  \n提出了一种统一的sim-and-real共训练框架，通过学习一个域不变且任务相关的特征空间，来提高机器人操作策略的可迁移性。核心思想是对观察和对应动作的联合分布进行对齐，利用Optimal Transport (OT)作为对齐目标，确保无论输入来自模拟还是现实，几何关系在动作预测中保持一致。此外，扩展到Unbalanced OT框架以处理数据不平衡，并开发了时间感知采样策略，以改善小批量OT训练中的域对齐学习。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving",
            "authors": "Jay Patrikar,Apoorva Sharma,Sushant Veer,Boyi Li,Sebastian Scherer,Marco Pavone",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "8 pages, 5 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.18626",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18626",
            "arxiv_html_link": "https://arxiv.org/html/2509.18626v1",
            "abstract": "Learning-based autonomous driving systems are trained mostly on incident-free data, offering little guidance near safety–performance boundaries. Real crash reports contain precisely the contrastive evidence needed, but they are hard to use: narratives are unstructured, third-person, and poorly grounded to sensor views. We address these challenges by normalizing crash narratives to ego-centric language and converting both logs and crashes into a unified scene–action representation suitable for retrieval. At decision time, our system adjudicates proposed actions by retrieving relevant precedents from this unified index; an agentic counterfactual extension proposes plausible alternatives, retrieves for each, and reasons across outcomes before deciding. On a nuScenes benchmark, precedent retrieval substantially improves calibration, with recall on contextually preferred actions rising from 24% to 53%. The counterfactual variant preserves these gains while sharpening decisions near risk.",
            "introduction": "End-to-end learning-based autonomous vehicle (AV) systems are trained primarily through imitation learning on positive, incident-free driving data [1, 2].\nThis data is typically collected by expert human drivers driving sensor-instrumented vehicles in a variety of driving scenarios, resulting in a dataset pairing the sensor observations that the AV will encounter with the action that the human driver chose in that moment.\nWhile this data helps define “good” driving that an AV should imitate, it does not provide direct supervision of what behaviors are to be avoided.\nSome have aimed to address this gap through auxiliary reward functions defining a rules-based definition of risky driving [3], but such rules can be challenging to specify: Risk is difficult to quantify due to uncertainty over other road user’s behaviors. Moreover, competent driving requires appropriately managing the risk that is taken on to make progress; remaining stopped is the safest policy, but not competent driving behavior.\n\nInstead, in this work, we consider an alternative data-driven approach to provide negative supervision for AV decision making. Specifically, we explore the use of crash reports as a complementary source of driving knowledge.\nAgencies such as the National Highway Traffic Safety Administration (NHTSA) collect structured narrative accounts of real-world accidents, including the actions taken and the conditions under which failures occurred. While these reports lack the rich multimodal data of first-person human-driven AV logs, they contain valuable causal and contextual information that can support counterfactual reasoning. While these reports can’t directly be used in policy training, recent advances in vision-language models (VLMs) capable of reasoning across sensor and text domains offer a compelling avenue for bringing such valuable sources of negative data into AV decision making.\n\nIn this paper, we study how negative data influences VLM reasoning capabilities in AV decision making tasks by developing a retrieval-augmented-generation (RAG) pipeline for AV safety adjudication. Specifically our contributions are as follows:\n(i) a GraphRAG [4] style retrieval pipeline for both positive and negative driving precedent, using a unified structured language representation for both sensor-domain positive data and language-domain negative data; (ii) an agentic extension which uses additional test-time compute to reason about counterfactuals prior to making a safety judgment; (iii) evaluation of both approaches in terms of alignment with human judgement on the safety of possible actions in driving scenarios, showing the impact of negative crash report data on VLM decision making capability.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用负数据（如事故报告）来改善自动驾驶系统的决策能力。  \n2. 如何将非结构化的事故叙述转换为可用于自动驾驶的结构化信息。  \n3. 如何在决策时使用检索机制来提高安全性判断的准确性。  \n\n【用了什么创新的方案】  \n本研究提出了一种基于检索增强生成（RAG）管道的方法，利用事故报告作为负监督数据来提升自动驾驶决策的安全性。通过将正负驾驶先例统一为结构化语言表示，构建了一个GraphRAG风格的检索管道。同时，增加了一个代理扩展，在决策前进行反事实推理，以提高与人类判断的一致性，并评估了负数据对视觉语言模型（VLM）决策能力的影响。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones",
            "authors": "Maximilian Adang,JunEn Low,Ola Shorinwa,Mac Schwager",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18610",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18610",
            "arxiv_html_link": "https://arxiv.org/html/2509.18610v1",
            "abstract": "Large vision-language models have driven remarkable progress in open-vocabulary robot policies, e.g., generalist robot manipulation policies, that enable robots to complete complex tasks specified in natural language. Despite these successes, open-vocabulary autonomous drone navigation remains an unsolved challenge due to the scarcity of large-scale demonstrations, real-time control demands of drones for stabilization, and lack of reliable external pose estimation modules.\nIn this work,\nwe present SINGER for language-guided autonomous drone navigation in the open world using only onboard sensing and compute. To train robust, open-vocabulary navigation policies, SINGER leverages three central components: (i) a photorealistic language-embedded flight simulator with minimal sim-to-real gap using Gaussian Splatting for efficient data generation, (ii) an RRT-inspired multi-trajectory generation expert for collision-free navigation demonstrations, and these are used to train (iii) a lightweight end-to-end visuomotor policy for real-time closed-loop control.\nThrough extensive hardware flight experiments, we demonstrate superior zero-shot sim-to-real transfer of our policy to unseen environments and unseen language-conditioned goal objects. When trained on ∼\\sim700k-1M observation action pairs of language conditioned visuomotor data and deployed on hardware, SINGER outperforms a velocity-controlled semantic guidance baseline by reaching the query 23.33%\\mathbf{23.33\\%} more on average, and maintains the query in the field of view 16.67%\\mathbf{16.67\\%} more on average, with 𝟏𝟎%\\mathbf{10\\%} fewer collisions.",
            "introduction": "Everyday, humans demonstrate notable semantic and physical understanding of their environments. For example, given a task to go to a specified location, a person relatively easily transforms the language instruction into a physical goal location using semantic cues and navigates to the desired location, safely avoiding collisions.\nAlthough autonomous drones excel at agile flight, they are often limited to controlled environments with pre-specified goal locations. In this work, we ask the question: “Can we train a vision-language drone navigation policy to reach previously unseen goal objects in a previously unseen environment using only on board sensing and compute?”\n\nAdvances in diffusion policies [1] and vision-language-action (VLA) models [2, 3] have led to significant research breakthroughs in robot policy learning from expert demonstration via imitation, particularly in robot manipulation.\nSpecifically, leveraging imitation learning on large-scale robot manipulation datasets [4, 5], state-of-the-art policies endow robots with the requisite task understanding and planning capabilities necessary to perform complex tasks entirely from task descriptions provided in natural language, e.g., to “pick up the apple and place it on a plate.”\nHowever, this paradigm has been largely unsuccessful in drone navigation, due to scarcity of large-scale drone navigation datasets, and effective semantic distillation methods for open-world drone navigation. This is exacerbated by inherent challenges in collecting large quantities of high quality visuomotor data on highly dynamic and naturally unstable drones.\n\nTo address the data scarcity challenge, prior work [6, 7] trains visuomotor policies for drone navigation in simulation, but the effectiveness of the resulting policies are often limited by the non-negligible sim-to-real gap. SOUS-VIDE [8] introduces FiGS, a high-fidelity Gaussian-Splatting-based drone simulator to narrow the sim-to-real gap for stronger real-world transfer; however, FiGS lacks the semantic knowledge required for open-world drone navigation, limiting its deployment to only environments and trajectories seen during training.\n\nIn this paper, we introduce SINGER (Semantic In-situ Navigation and Guidance for Embodied Robots), a pipeline for training language-conditioned drone navigation policies addressing the aforementioned limitations. SINGER consists of three central components: (i) a semantics-rich photorealistic flight simulator based on 3D Gaussian Splatting for efficient data generation with expert demonstrations,\n(ii) a high-level rapidly exploring random trees (RRT*) based planner that efficiently computes spatially spanning collision-free paths to a language-specified goal by time-inverting an expanded tree, and\n(iii) a robust low-level visuomotor policy that tracks the resulting high-level plans with real-time feedback.\nWith these components, SINGER trains a lightweight viusal policy that runs onboard a drone in real-time for online navigation given a natural-language goal object.\n\nTo build an effective flight simulator, we blend the high-fidelity scene-reconstruction capabilities of Gaussian Splatting [9] with the generalizable open-world vision-language semantic features computed by CLIP [10], achieving minimal sim-to-real gap.\nThis core design choice underpins SINGER’s strong zero-shot generalization capabilities to unseen tasks and environments at inference time. In particular, by abstracting goal specification to a semantic (vision-language) space, SINGER effectively aligns a small dataset of synthetic expert trajectories with a broad set of tasks, yielding a data-efficient training scheme for robust visuomotor policies. We augment this training approach with domain randomization for added robustness.\n\nAt deployment, we inference CLIPSeg [11] to produce open-vocabulary semantic images of the environment as conditioning inputs, processed by an end-to-end visuomotor drone policy for low-level drone commands.\n\nThrough our experiments, we show that SINGER outperforms baseline methods in achieving sub-meter proximity to goal by 23.33%23.33\\% with 10%10\\% less collisions and keeping the query in the field of view 16.67%16.67\\% more often without relying on external pose estimation or map-based navigation methods.\n\nWe summarize our contributions as follows:\n\nWe introduce a high-fidelity drone simulator for efficient imitation learning in language-specified drone navigation problems built on language embedded Gaussian Splatting.\n\nWe design a RRT* trajectory planner that efficiently finds thousands of collision-free feasible trajectories across multiple Gaussian Splatting scenes and multiple semantic classes, used to produce large quantities of data for training a generalist policy.\n\nWe present a real-time, lightweight, low-level visual policy architecture for language guided drone navigation using onboard sensing and compute.\n\nUsing these components, we train robust visuomotor policies for drone guidance given a natural language goal specification that generalizes to never before seen environments and semantic queries.\n\n1. We introduce a high-fidelity drone simulator for efficient imitation learning in language-specified drone navigation problems built on language embedded Gaussian Splatting.\n\n2. We design a RRT* trajectory planner that efficiently finds thousands of collision-free feasible trajectories across multiple Gaussian Splatting scenes and multiple semantic classes, used to produce large quantities of data for training a generalist policy.\n\n3. We present a real-time, lightweight, low-level visual policy architecture for language guided drone navigation using onboard sensing and compute.\n\n4. Using these components, we train robust visuomotor policies for drone guidance given a natural language goal specification that generalizes to never before seen environments and semantic queries.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现无人机在开放环境中的语言引导导航？  \n2. 如何解决无人机导航中的数据稀缺和sim-to-real差距问题？  \n3. 如何在无人机上实现实时的低级视觉控制策略？  \n\n【用了什么创新的方案】  \nSINGER提出了一种新的无人机导航策略，通过三个核心组件实现语言引导的自主导航：首先，利用基于3D Gaussian Splatting的高保真飞行模拟器生成高效的专家演示数据；其次，设计了一种基于RRT*的多轨迹生成规划器，以确保碰撞自由的导航；最后，开发了一种轻量级的端到端视觉运动策略，实现实时闭环控制。该方法通过在未见环境和目标对象上的实验，展示了优越的零-shot sim-to-real迁移能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving",
            "authors": "Chengran Yuan,Zijian Lu,Zhanqi Zhang,Yimin Zhao,Zefan Huang,Shuo Sun,Jiawei Sun,Jiahui Li,Christina Dao Wen Lee,Dongen Li,Marcelo H. Ang Jr",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18609",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18609",
            "arxiv_html_link": "https://arxiv.org/html/2509.18609v1",
            "abstract": "End-to-end motion planning is promising for simplifying complex autonomous driving pipelines. However, challenges such as scene understanding and effective prediction for decision-making continue to present substantial obstacles to its large-scale deployment. In this paper, we present PIE, a pioneering framework that integrates advanced perception, reasoning, and intention modeling to dynamically capture interactions between the ego vehicle and surrounding agents. It incorporates a bidirectional Mamba fusion that addresses data compression losses in multimodal fusion of camera and LiDAR inputs, alongside a novel reasoning-enhanced decoder integrating Mamba and Mixture-of-Experts to facilitate scene-compliant anchor selection and optimize adaptive trajectory inference. PIE adopts an action-motion interaction module to effectively utilize state predictions of surrounding agents to refine ego planning. The proposed framework is thoroughly validated on the NAVSIM benchmark. PIE, without using any ensemble and data augmentation techniques, achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of prior state-of-the-art methods. Comprehensive quantitative and qualitative analyses demonstrate that PIE is capable of reliably generating feasible and high-quality ego trajectories.",
            "introduction": "End-to-end motion planning has emerged as a promising paradigm for general robotic systems, including autonomous vehicles (AVs). This data-driven approach has the potential to enable AVs to handle complex and previously unseen scenarios, a capability that becomes increasingly critical as urban environments grow denser and more intricate. By leveraging sensor data directly, end-to-end methods [1, 2, 3] aim to consolidate the traditionally segmented autonomy pipeline—encompassing perception, prediction, and planning—into a single, cohesive framework.\n\nDespite the promising performance of end-to-end methods, several key challenges remain. First, fusing multimodal data (e.g., image and LiDAR inputs) often leads to compression-induced losses when reducing historical information or fusing features from different sources. Second, while data-driven approaches offer the potential for enhancing environmental understanding, the complexity of real-world driving requires more sophisticated models that are capable of both reasoning and dynamically adjusting their strategies. Third, incorporating the predictions of other traffic participants into the end-to-end planning pipeline often introduces substantial computational overhead. Developing efficient methods to seamlessly integrate these predictions into the planning process remains an open challenge, presenting considerable opportunities for further advancements.\n\nTo address these issues, we present PIE, an encoder-decoder framework designed to model the interaction between the action of ego vehicle and the motion of nearby agents and to enable more nuanced reasoning about the driving environment. Our approach mitigates data loss and integrates prediction and planning effectively. The contributions of this work are threefold:\n\nBidirectional Mamba Fusion We introduce a bidirectional Mamba fusion that effectively improves the multimodal data fusion between camera and LiDAR. A notable improvement of 1.9 PDM score can be achieved by merely employing this fusion approach based on the Transfuser backbone.\n\nBidirectional Mamba Fusion We introduce a bidirectional Mamba fusion that effectively improves the multimodal data fusion between camera and LiDAR. A notable improvement of 1.9 PDM score can be achieved by merely employing this fusion approach based on the Transfuser backbone.\n\nReasoning-Enhanced Decoder To improve scene reasoning in complex driving scenarios, we design an efficient decoder integrating the MoE, harnessing Mamba to enhance trajectory generation.\n\nAction-Motion Interaction We propose an action-motion interaction module via a shared cross-attention that directly integrates the velocity predictions of surrounding agents into ego action to model the dynamic interactions between traffic users.\n\nOur approach surpasses the previous state-of-the-art DiffusionDrive [4] by achieving an 88.9 PDM score and 85.6 EPDM score on the NAVSIM navtest split, demonstrating the superiority and effectiveness of the proposed modules.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效融合多模态数据（如相机和LiDAR）以减少压缩损失？  \n2. 如何在复杂的驾驶场景中实现更高效的推理和动态策略调整？  \n3. 如何将其他交通参与者的预测无缝整合进端到端规划流程？  \n\n【用了什么创新的方案】  \n核心解决方案：本文提出了PIE框架，通过引入双向Mamba融合技术来改善相机和LiDAR数据的多模态融合，减少数据压缩损失。设计了一个集成Mixture-of-Experts的推理增强解码器，以提高复杂驾驶场景的推理能力。同时，提出了一个动作-运动交互模块，通过共享的交叉注意力机制，将周围代理的速度预测直接整合到自我动作中，从而更好地建模交通用户之间的动态交互。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning",
            "authors": "Ana Luiza Mineiro,Francisco Affonso,Marcelo Becker",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Accepted to the 22nd International Conference on Advanced Robotics (ICAR 2025). 7 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18608",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18608",
            "arxiv_html_link": "https://arxiv.org/html/2509.18608v1",
            "abstract": "Reliable navigation in under-canopy agricultural environments remains a challenge due to GNSS unreliability, cluttered rows, and variable lighting. To address these limitations, we present an end-to-end learning-based navigation system that maps raw 3D LiDAR data directly to control commands using a deep reinforcement learning policy trained entirely in simulation. Our method includes a voxel-based downsampling strategy that reduces LiDAR input size by 95.83%, enabling efficient policy learning without relying on labeled datasets or manually designed control interfaces. The policy was validated in simulation, achieving a 100% success rate in straight-row plantations and showing a gradual decline in performance as row curvature increased, tested across varying sinusoidal frequencies and amplitudes.",
            "introduction": "Autonomous robots have seen significant growth in modern agriculture, particularly for under-canopy tasks such as plant phenotyping, crop row harvesting, and disease scouting. These applications require platforms that are not only compact and agile but also capable of accurately navigating between dense crop rows (Fig. 1) [1]. However, reliable navigation in such environments remains an active area of research due to several challenges, including clutter and occlusions caused by narrow row spacing and the high visual variability introduced by different plant growth stages [2].\n\nTo enable decision-making systems capable of navigating through plantations in under-canopy environments, robots typically rely on exteroceptive sensors [3]. This choice is driven by the unreliability of GNSS-based localization in such environments, where signal degradation is common due to foliage occlusion [4]. As a result, there is a need for local perception strategies that allow the robot to infer navigation information from its immediate surroundings.\n\nRecent methods have adopted learning-based approaches that aim to extract row-following features directly from curated and labeled datasets, using cameras or LiDAR sensors as the primary source of perception data. On one hand, cameras provide rich semantic and textural information; however, their performance often degrades in low-light conditions, which are common in dense canopy environments [5, 6]. On the other hand, LiDAR offers accurate depth measurements and is unaffected by lighting variations, making it more robust in shaded or poorly lit areas [7, 8]. Additionally, while a sim-to-real gap exists for LiDAR data, the structural consistency of point clouds tends to generalize well across training scenarios. This characteristic allows learning approaches based on point cloud data to effectively leverage large-scale simulated datasets [9].\n\nIn addition, most of these methods typically divide the navigation task into separate modules (e.g., perception, locomotion controller), which introduces challenges in designing effective interfaces between them [10]. As a result, important environmental features may be lost or overly simplified when translated into reference values for the controller. Combined with the high cost and impracticality of collecting labeled data across diverse agricultural conditions, these limitations hinder generalization and reduce overall system robustness.\n\nFor instance, to address the limitations of curated datasets and the challenge of designing interfaces between navigation modules, deep reinforcement learning (RL) can be explored. This approach enables agents to learn end-to-end policies by mapping perceptual observations directly to navigation actions through interaction with the environment. The control policy is optimized via trial-and-error, guided by reward functions that encode the desired task [11, 12].\n\nIn this paper, we present an end-to-end row-following navigation system for under-canopy agricultural environments that maps raw 3D LiDAR data directly to control commands using a RL policy trained in simulation. Our approach avoids the need for manually designed interfaces between navigation modules and does not rely on expert-labeled datasets. Additionally, we propose a LiDAR downsampling technique that transforms raw point clouds into compact row maps, significantly reducing the observation space while preserving essential structural features for learning.\n\nThe key contributions of this work are:\n\nAn end-to-end learning system for crop row-following using only raw 3D LiDAR data.\n\nA voxel-based downsampling technique to convert LiDAR point clouds into compact 2D row maps.\n\n1. An end-to-end learning system for crop row-following using only raw 3D LiDAR data.\n\n2. A voxel-based downsampling technique to convert LiDAR point clouds into compact 2D row maps.",
            "llm_summary": "【关注的是什么问题】  \n1. 在农业环境中，如何实现可靠的导航以应对GNSS不可靠性和环境复杂性？  \n2. 如何有效利用3D LiDAR数据进行自主导航而不依赖于标注数据或手动设计的控制接口？  \n3. 如何通过深度强化学习优化导航策略以提高系统的鲁棒性和通用性？  \n\n【用了什么创新的方案】  \n本研究提出了一种端到端的学习系统，利用原始3D LiDAR数据直接映射到控制命令，采用深度强化学习策略在模拟环境中训练。通过引入体素下采样技术，将LiDAR点云转换为紧凑的2D行地图，显著减少了观察空间，同时保留了学习所需的结构特征，从而避免了手动设计模块接口的需求。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
            "authors": "Neel P. Bhatt,Yunhao Yang,Rohan Siva,Pranay Samineni,Daniel Milan,Zhangyang Wang,Ufuk Topcu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Systems and Control (eess.SY)",
            "comment": "Codebase, datasets, and videos for VLN-Zero are available at:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18592",
            "code": "https://vln-zero.github.io/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18592",
            "arxiv_html_link": "https://arxiv.org/html/2509.18592v1",
            "abstract": "Rapid adaptation in unseen environments is essential for scalable real-world autonomy, yet existing approaches rely on exhaustive exploration or rigid navigation policies that fail to generalize. We present VLN-Zero, a two-phase vision-language navigation framework that leverages vision-language models to efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic navigation. In the exploration phase, structured prompts guide VLM-based search toward informative and diverse trajectories, yielding compact scene graph representations. In the deployment phase, a neurosymbolic planner reasons over the scene graph and environmental observations to generate executable plans, while a cache-enabled execution module accelerates adaptation by reusing previously computed task–location trajectories. By combining rapid exploration, symbolic reasoning, and cache-enabled execution, the proposed framework overcomes the computational inefficiency and poor generalization of prior vision-language navigation methods, enabling robust and scalable decision-making in unseen environments. VLN-Zero achieves 2x higher success rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned baselines, and reaches goal locations in half the time with 55% fewer VLM calls on average compared to state-of-the-art models across diverse environments.††The full codebase, datasets, and videos for VLN-Zero are available at https://vln-zero.github.io/.",
            "introduction": "Deploying autonomous agents in new environments remains a fundamental challenge: policies trained in one setting often fail in another due to novel layouts, obstacles, or constraints, and consequently require fine-tuning or multi-shot inference.\nFor example, a robot trained to navigate one office building may struggle in a different building unless retrained, a process that is both slow and impractical for rapid deployment.\n\nThe central problem is twofold: (i) How can an agent efficiently construct a representation of an unseen environment, such as a scene graph, without exhaustive search?\n(ii) Given this representation, how can the agent efficiently generate constraint-satisfying plans in real time without fine-tuning or multi-shot inference?\n\nExisting approaches, ranging from frontier-based exploration to reinforcement learning with fixed policies, struggle with either computational inefficiency, lack of generalization, or both. Vision-language navigation models (VLNs) offer promise; however, current approaches suffer from slow, exhaustive exploration, weak task decomposition, and high training and query cost.\n\nWe argue that addressing these limitations requires rethinking the interaction between perception, symbolic reasoning, and policy adaptation. Specifically, agents must be able to (i) rapidly acquire symbolic representations of their environment to minimize exploration cost, and (ii) leverage these representations for efficient navigation in new environments without retraining or extensive fine-tuning.\n\nTo address these challenges, we introduce VLN-Zero, a two-phase zero-shot framework that combines vision-language model (VLM) guided exploration with neurosymbolic navigation. In the exploration phase, the agent interacts with the environment using structured and compositional task prompts, guiding exploration toward informative and diverse trajectories to construct a compact scene graph with semantic area labels. In the deployment phase, a neurosymbolic planner reasons over this scene graph, environmental observations to generate executable plans, eliminating reliance on fixed policies. To further improve scalability, we propose a cache-enabled execution procedure that stores previously computed task–location trajectories for reuse, accelerating both exploration and deployment.\n\nIn summary, VLN-Zero offers three key contributions:\n\nVLM-guided rapid exploration: We design structured, compositional prompts that steer a VLN agent to propose exploration actions while incrementally constructing compact symbolic scene graphs. This enables coverage of novel environments within a time- and compute-constrained exploration budget while avoiding unsafe behaviors.\n\nZero-shot neurosymbolic navigation: We introduce a planner that reasons jointly over\nscene graphs, task prompts, and real-time observations, transforming free-form natural language\ninstructions into constraint-satisfying action sequences without fine-tuning or multi-shot inference.\n\nCache-enabled execution for fast adaptation: We develop a trajectory-level caching mechanism\nthat stores validated task–location pairs, allowing the system to reuse previously computed plans.\nThis reduces redundant VLM queries to minimize execution time, cost, and compute demands which accelerates real-world deployment.\n\n1. VLM-guided rapid exploration: We design structured, compositional prompts that steer a VLN agent to propose exploration actions while incrementally constructing compact symbolic scene graphs. This enables coverage of novel environments within a time- and compute-constrained exploration budget while avoiding unsafe behaviors.\n\n2. Zero-shot neurosymbolic navigation: We introduce a planner that reasons jointly over\nscene graphs, task prompts, and real-time observations, transforming free-form natural language\ninstructions into constraint-satisfying action sequences without fine-tuning or multi-shot inference.\n\n3. Cache-enabled execution for fast adaptation: We develop a trajectory-level caching mechanism\nthat stores validated task–location pairs, allowing the system to reuse previously computed plans.\nThis reduces redundant VLM queries to minimize execution time, cost, and compute demands which accelerates real-world deployment.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在未见环境中高效构建场景图表示，而不依赖于耗时的探索？  \n2. 如何在实时生成约束满足的导航计划，而无需进行微调或多次推理？  \n\n【用了什么创新的方案】  \n核心解决方案：VLN-Zero是一个两阶段的零-shot框架，结合了基于视觉语言模型（VLM）的快速探索和神经符号导航。在探索阶段，使用结构化提示引导VLM进行有效的探索，构建紧凑的场景图。在部署阶段，神经符号规划器基于场景图和环境观察生成可执行计划，同时通过缓存机制加速适应，重用先前计算的任务-位置轨迹，从而提高决策效率和可扩展性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA",
            "authors": "Zeyi Kang(1),Liang He(2),Yanxin Zhang(3),Zuheng Ming(4),Kaixing Zhao(5) ((1) Northwestern Polytechnical University, (2) University Sorbonne Paris Nord)",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18576",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18576",
            "arxiv_html_link": "https://arxiv.org/html/2509.18576v1",
            "abstract": "Multimodal semantic learning plays a critical role in embodied intelligence, especially when robots perceive their surroundings, understand human instructions, and make intelligent decisions. However, the field faces technical challenges such as effective fusion of heterogeneous data and computational efficiency in resource-constrained environments. To address these challenges, this study proposes the lightweight LCMF cascaded attention framework, introducing a multi-level cross-modal parameter sharing mechanism into the Mamba module. By integrating the advantages of Cross-Attention and Selective parameter-sharing State Space Models (SSMs), the framework achieves efficient fusion of heterogeneous modalities and semantic complementary alignment. Experimental results show that LCMF surpasses existing multimodal baselines with an accuracy of 74.29% in VQA tasks and achieves competitive mid-tier performance within the distribution cluster of Large Language Model Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a 4.35-fold reduction in FLOPs relative to the average of comparable baselines while using only 166.51M parameters (image-text) and 219M parameters (video-text), providing an efficient solution for Human-Robot Interaction (HRI) applications in resource-constrained scenarios with strong multimodal decision generalization capabilities.",
            "introduction": "In contemporary research, powerful multimodal understanding capabilities have emerged as the foundational element for enabling robotic perception, cognition, and interaction within complex dynamic environments[1]. In the domain of embodied intelligence, Vision-Language Pre-training (VLP) [2] has advanced into a critical technological paradigm for the development of sophisticated intelligent robotic systems, offering substantial support for the realization of more intelligent HRI [3]. Concurrently, to address the challenges inherent in multimodal learning, such as the scarcity of labeled data and the prohibitive costs of annotation, self-supervised learning [4] has garnered considerable attention and research focus. More precisely, by setting multi-task optimization objectives (multimodal masked modeling, contrastive learning, etc.) [5], these methods provide possibilities for common robotic tasks, such as environmental understanding [6], decision-making (Visual Question Answering (VQA) [7], Embodied Question Answering (EQA) [8]) or even more advanced cross-modal general understanding [9].\n\nHowever, embodied intelligence [10] still faces numerous challenges that limit learning capabilities in visual-language decision tasks. At the semantic understanding level, current models [11, 12, 13] struggle to reconstruct fine-grained mask labels, resulting in an information gap between local features, mask features, and global scene understanding. In addition, the efficiency problem of long sequence modeling cannot be ignored, as the Transformer architecture’s computational complexity grows quadratically when processing large-scale sequence data [14, 15], making it difficult to achieve optimal trade-offs between cross-modal understanding performance and hardware efficiency.\n\nIn response to the above challenges, this paper proposes the lightweight LCMF architecture, which achieves high-quality multimodal understanding and inference acceleration on low-computation robotic platforms. LCMF uses a semantic diffusion mechanism [11] to address the information gap in multi-scale visual semantics and enhance the ability to model fine-grained masked information. For cross-modal interaction, Cross-Modality Mamba (CMM) extends the Mamba state-space model to the multimodal domain, achieving comprehensive optimization in hardware awareness, time efficiency, and lightweight design. At the level of multimodal semantic fusion, Enhanced Mamba Fusion (EMF) introduces efficient semantic bridging mechanisms and fine-grained feature modulation techniques, enabling the effective integration of heterogeneous modality semantics.\n\nIn summary, our contributions include:\n\n1) CMM implements multi-level sharing of state space parameters and parallel modeling of multimodal long sequence semantics, achieving linear computational complexity and inference acceleration.\n\n2) LCMF has implemented a lightweight Mamba-Transformer (Selective SSMs-Attention) architecture in the fields of unimodal feature extraction, multimodal (image, text, video) interaction, and multimodal fusion.\n\n3) Under a significantly reduced parameter scale compared to existing multimodal baselines and LLM Agents, LCMF achieves improved computational efficiency while maintaining strong performance on downstream tasks such as VQA and EQA, demonstrating its effectiveness in efficient multimodal modeling.\n\nThe rest of this article is organized as follows.\n\nSection II reviews research on Mamba variant architecture, VQA, EQA.\nSection III details the LCMF model architecture, pretraining, fine-tuning, and evaluation methods.\nSection IV describes the experimental setup, performance evaluation, and ablation experiments on specific downstream tasks.\nFinally, Section V concludes this article.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效融合异构数据以提升机器人在复杂环境中的理解能力？  \n2. 如何在资源受限的环境中实现高效的多模态学习和决策？  \n3. 当前模型在视觉语言决策任务中面临的学习能力限制是什么？  \n\n【用了什么创新的方案】  \n本研究提出了轻量级的LCMF框架，通过引入多级跨模态参数共享机制，结合Cross-Attention和选择性状态空间模型（SSMs），实现了异构模态的高效融合和语义互补对齐。该框架在保持较低计算复杂度的同时，显著提升了多模态理解能力，并在视觉问答（VQA）和视频问答（EQA）任务中表现出色，展示了其在资源受限场景下的有效性和强大的多模态决策泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Spatial Envelope MPC: High Performance Driving without a Reference",
            "authors": "Siyuan Yu,Congkai Shen,Yufei Xi,James Dallas,Michael Thompson,John Subosits,Hiroshi Yasuda,Tulga Ersal",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18506",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18506",
            "arxiv_html_link": "https://arxiv.org/html/2509.18506v1",
            "abstract": "This paper presents a novel envelope-based model predictive control (MPC) framework designed to enable autonomous vehicles to handle high-performance driving across a wide range of scenarios without a predefined reference.\nIn high-performance autonomous driving, safe operation at the vehicle’s dynamic limits requires a real-time planning and control framework capable of accounting for key vehicle dynamics and environmental constraints when following a predefined reference trajectory is suboptimal or even infeasible. State-of-the-art planning and control frameworks, however, are predominantly reference-based, which limits their performance in such situations.\nTo address this gap, this work first introduces a computationally efficient vehicle dynamics model tailored for optimization-based control and a continuously differentiable mathematical formulation that accurately captures the entire drivable envelope.\nThis novel model and formulation allow for the direct integration of dynamic feasibility and safety constraints into a unified planning and control framework, thereby removing the necessity for pre-defined references.\nThe challenge of envelope planning, which refers to maximally approximating the safe drivable area, is tackled by combining reinforcement learning with optimization techniques.\nThe framework is validated through both simulations and real-world experiments, demonstrating its high performance across a variety of tasks, including racing, emergency collision avoidance and off-road navigation.\nThese results highlight the framework’s scalability and broad applicability across a diverse set of scenarios.",
            "introduction": "High-performance autonomous driving technology has advanced rapidly and significantly in the past decade [1].\nThe grand opening of the first autonomous racing competition has thrust autonomous driving technology into the spotlight, showcasing performance that rivals expert human drivers in tasks demanding high levels of precision and speed [2]. Recent developments in autonomous driving have enabled reliable performance in advanced collision avoidance capabilities [3, 4, 5], complex drifting maneuvers [6, 7, 8], and challenging off-road navigation [9, 10, 11].\nThese advancements have greatly expanded the potential applications of autonomous vehicles across various fields. To execute these extreme maneuvers safely, the vehicle must operate at the limits of its capabilities.\nHowever, this is a challenge, because even a small deviation from the desired trajectory can result in catastrophic outcomes.\n\nIn this regard, the autonomous system should be designed to effectively utilize all feasible operational regions including the limits, ensuring performance is not unnecessarily sacrificed for safety via overly conservative constraints.\nFor instance, every inch of the race track is critical for drivers to adjust their strategies and enhance their performance. In addition, the algorithm’s design should ensure scalability when applied to a diverse set of scenarios with varying levels of complexity.\nIn this context, the term ‘scalability’ refers to the algorithm’s ability to handle an increasing amount of tasks or scenarios without a significant compromise in performance or without significant redesign.\nIt is also desirable that the system be capable of generating optimal trajectories online in real-time without having to depend on a predefined reference, because deviations from the original plan may render the predefined reference suboptimal or even infeasible.\n\nHowever, as the literature review in Sec. II reveals, existing methods typically rely on predefined references and therefore either limit vehicle performance to the quality of that reference or do not provide optimal and scalable solutions in performance-demanding scenarios.\n\nTo address this gap, this paper presents a novel, spatial envelope model predictive control (MPC) framework for reference-free high performance driving.\nThe proposed framework builds upon a new, computationally efficient vehicle dynamics model tailored for closed-loop optimization based planning and control, capturing the essential dynamics required for aggressive maneuvers.\nA twice continuously differentiable mathematical formulation of the entire driving envelope is introduced to conservatively estimate the drivable region to be used in MPC.\nThis enables the MPC to break from the restrictive constant-speed assumption previously used in spatial envelope MPC [12, 13, 14, 5], and instead optimize speed, as well, to maximize performance while maintaining safety.\n\nTo the authors’ knowledge, this is the first published MPC algorithm that is experimentally validated for safe and effective high-performance driving at the handling limits in a fully reference-free setting.\nThe algorithm is validated across a wide range of scenarios, including racing, off-road navigation and emergency collision avoidance, demonstrating both generality and real-world applicability.\n\nFinally, a new spatial envelope planning technique is introduced to further enhance applicability. A hybrid approach that combines optimization-based formulation with reinforcement learning is developed to segment the drivable area into blocks, enabling scalable planning in complex environments.\n\nThe original contributions are summarized as follows:\n\nA validated 3-DoF single-track dynamic model that accounts for longitudinal load transfer and the friction circle limit, while remaining computationally efficient in a fully reference-free setting.\n\nA validated 3-DoF single-track dynamic model that accounts for longitudinal load transfer and the friction circle limit, while remaining computationally efficient in a fully reference-free setting.\n\nA hard constraint formulation to mathematically express the spatial envelope with guaranteed conservativeness.\n\nA real-time Model Predictive Control (MPC) formulation that leverages the first two contributions to optimize vehicle trajectories online without any predefined path.\n\nA reinforcement learning approach to design a set of blocks to approximate arbitrary shapes of spatial envelopes in real time.\n\nValidation of the proposed MPC formulation in racing, emergency collision avoidance and off-road environments.\n\nThe rest of the paper is organized as follows. Sec. II reviews the relevant literature. Sec. III-A describes the 3 DoF single-track vehicle dynamics. Sec. III-B describes the MPC formulation including the conservative spatial envelope constraints. Sec. III-C describes the real-time spatial envelope planner. Sec. IV describes the model fidelity test. The results and discussion of the proposed spatial envelope MPC are presented in Sec. IV.\nFrom Sec. V-A to E, the simulation and experimental results of spatial envelope MPC are conducted and analyzed in multiple scenarios.\nIn Sec. III-F, the proposed spatial envelope planning technique is presented and discussed.\nFinally, Sec. VI concludes the study.",
            "llm_summary": "【关注的是什么问题】  \n1. 高性能自动驾驶中缺乏有效的参考轨迹规划与控制方法。  \n2. 现有方法过于依赖预定义参考，限制了车辆性能。  \n3. 如何在复杂环境中实现实时、无参考的轨迹优化。  \n\n【用了什么创新的方案】  \n本研究提出了一种新颖的空间包络模型预测控制（MPC）框架，旨在实现高性能的无参考自动驾驶。该框架结合了高效的车辆动力学模型和连续可微的数学公式，能够准确捕捉可行驶区域。通过将动态可行性和安全约束直接整合到规划和控制框架中，消除了对预定义参考的依赖。此外，采用强化学习与优化技术相结合的方法，能够在复杂环境中进行可扩展的规划。该框架在多种场景下进行了验证，包括赛车、紧急避障和越野导航，展示了其广泛的适用性和高性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain",
            "authors": "Junnosuke Kamohara,Feiyang Wu,Chinmayee Wamorkar,Seth Hutchinson,Ye Zhao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18466",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18466",
            "arxiv_html_link": "https://arxiv.org/html/2509.18466v1",
            "abstract": "Model predictive control (MPC) has demonstrated effectiveness for humanoid bipedal locomotion; however, its applicability in challenging environments, such as rough and slippery terrain, is limited by the difficulty of modeling terrain interactions.\nIn contrast, reinforcement learning (RL) has achieved notable success in training robust locomotion policies over diverse terrain, yet it lacks guarantees of constraint satisfaction and often requires substantial reward shaping.\nRecent efforts in combining MPC and RL have shown promise of taking the best of both worlds, but they are primarily restricted to flat terrain or quadrupedal robots.",
            "introduction": "Legged locomotion conventionally employs model-based controllers (MBCs), particularly Model Predictive Control (MPC), due to their optimization-based constraint satisfaction [1, 2].\nWhile whole-body dynamics models [3] are more accurate,\nresearchers use simplified models [4, 5, 6, 7] for computational efficiency and consequently suffer from model mismatch due to simplification of the dynamics.\nAs a result, simplified models exhibit poorer tracking accuracy and instability, particularly during contact [8].\nAdditionally, MPC with simplified dynamics usually requires predefined contact sequence and swing leg trajectory, which limits its adaptivity to diverse terrains.\nOverall, the deterministic but inaccurate dynamic model and manual constraint design of MPC restrict its robustness and versatility, limiting its applicability to diverse terrains in the real world.\n\nIn contrast, learning-based controls (LBC), exemplified by Reinforcement Learning (RL) methods, have gained wide attention for their robustness and agility [9, 10, 11, 12, 13].\nBy training policies parameterized by neural networks, RL policies can achieve zero-shot transfer from simulation to reality.\nHowever, training robust policies requires substantial environmental interactions and extensive reward shaping.\nFurthermore, RL policies lack explicit constraint satisfaction because of the absence of explicit constraints.\n\nMotivated by the unique advantages of both sides, recent years have witnessed a surge of methods combining model-based and learning-based approaches, leveraging the safety offered by MPC’s explicit constraints as well as powerful reactive behaviors offered by RL [14, 15].\nIn legged robotics, there are two main threads of combination.\nThe first thread uses MPC within a policy.\nRecent works either adopt a hierarchical architecture, where RL parametrizes MPC’s components, including system dynamics, center of mass reference trajectory, and gait frequency [16, 8, 17, 18]; or follows a parallel architecture, where RL policies refine MPC outputs by adding corrective actions such as footholds and joint commands [19, 20, 21].\nAnother thread uses MPC as an expert policy, training the policy through behavior cloning or RL with imitation loss to increase sample efficiency and motion accuracy [22, 13, 23, 24].\nEach of these designs carries trade-offs:\nMPC as an expert improves training efficiency by imitating MPC motions, yet it incurs significant computational overhead during training due to repeated optimization solves, making training in parallelized RL environments particularly challenging [24].\nWhile parallel architectures offer flexibility by directly augmenting MPC outputs, they raise safety concerns since the RL policy bypasses feasibility constraints from optimization.\nHierarchical architectures, in contrast, preserve the optimization structure and computational complexity, as the policy is evaluated before solving the optimization problem.\nThis ensures the feasibility and constraint satisfaction within the optimization framework.\n\nDespite these advances, most combined approaches for bipedal locomotion remain limited to flat terrain, as prior works primarily emphasize improving tracking accuracy rather than adaptability [19, 21], leaving integration of MPC and RL for rough-terrain-adaptive locomotion unexplored.\nIn this work, we aim to enhance the adaptability of humanoid locomotion, enabling responsive and robust behaviors in the face of terrain disturbances.\nWe leverage a hierarchical method that augments MPC via RL by incorporating rich whole-body information into the simplified system model, adjusting the gait frequency to modulate step length, and modifying the swing foot trajectory to improve robustness against challenging terrain.\nWe focus on addressing the limitations of MPC with simplified dynamics: model mismatch, predefined swing leg curve, and static gait frequency.\nThe RL policy learns residual dynamics through whole-body dynamics simulation, as well as swing leg curve parameters, including apex height and control points, and dynamic gait frequency within one locomotion cycle.\n\nThese learned adaptations enable reactive behaviors, including recovery from foot entrapment and severe slippage.\nWe implement our method on bipedal locomotion tasks with the HECTOR robot [25] in NVIDIA IsaacLab, a state-of-the-art GPU-accelerated simulator [26].\nOur framework significantly improves robustness against disturbances on diverse terrains, including slippery surfaces, stairs, and stepping stones.\nAdditionally, we conduct ablation studies on the three residual modules to analyze the contribution of each component.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高双足机器人在复杂地形上的适应性和鲁棒性。  \n2. 现有的模型预测控制（MPC）在动态建模和适应性方面的局限性。  \n3. 强化学习（RL）在约束满足和训练效率方面的不足。  \n\n【用了什么创新的方案】  \n本研究提出了一种层次化方法，通过将强化学习（RL）与模型预测控制（MPC）相结合，增强了双足机器人在复杂地形上的适应性。该方法利用丰富的全身信息来调整简化系统模型，学习残余动态和摆腿曲线参数，并动态调整步频，从而提高在滑面、楼梯和踏石等多样地形上的鲁棒性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task",
            "authors": "Jannick van Buuren,Roberto Giglio,Loris Roveda,Luka Peternel",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18463",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18463",
            "arxiv_html_link": "https://arxiv.org/html/2509.18463v1",
            "abstract": "This paper explores how deliberate mutations of reward function in reinforcement learning can produce diversified skill variations in robotic manipulation tasks, examined with a liquid pouring use case. To this end, we developed a new reward function mutation framework that is based on applying Gaussian noise to the weights of the different terms in the reward function. Inspired by the cost-benefit tradeoff model from human motor control, we designed the reward function with the following key terms: accuracy, time, and effort. The study was performed in a simulation environment created in NVIDIA Isaac Sim, and the setup included Franka Emika Panda robotic arm holding a glass with a liquid that needed to be poured into a container. The reinforcement learning algorithm was based on Proximal Policy Optimization. We systematically explored how different configurations of mutated weights in the rewards function would affect the learned policy. The resulting policies exhibit a wide range of behaviours: from variations in execution of the originally intended pouring task to novel skills useful for unexpected tasks, such as container rim cleaning, liquid mixing, and watering. This approach offers promising directions for robotic systems to perform diversified learning of specific tasks, while also potentially deriving meaningful skills for future tasks.",
            "introduction": "For robots to successfully operate in unstructured and unpredictable real-world environments, they need the ability to constantly adapt and learn many tasks. One way to do this is to learn from human demonstration [1]. However, human involvement can be costly, and humans are not always available to correct or teach robots new skills. Indeed, an alternative is reinforcement learning (RL) that allows the robots to autonomously acquire new skills through trial‑and‑error interaction with their environment [2]. The robot is given an objective function (typically from a human), which then guides its autonomous exploration to obtain a policy of how to perform a given task. At each timestep, the robot observes the current state, executes an action according to its policy, and receives a scalar reward. Over many episodes, it refines its policy to maximise the expected sum of discounted rewards, thereby acquiring skills optimised for long‑term success.\n\nRL has been successfully applied to robots to solve a diverse range of tasks, ranging from pick-and-place actions [3, 4], object lifting [5], to assembly [6], as well as play ball-in-the-cup game [7], table tennis [8], and air hockey [9]. Within this context, the liquid pouring task [10, 11, 12, 13] stands out as a particularly compelling benchmark for investigating the role of reward function design in shaping learned behaviours. Unlike binary success criteria seen in stacking or placement tasks, pouring involves balancing multiple continuous objectives, such as avoiding spillage, reducing effort, and maximising efficiency, making it highly sensitive to how learning is incentivised.\n\nRL has achieved impressive results in specific robotic tasks with well-crafted and tailored reward functions. This typically results in good skills specialised for the given task, but lacks generalisation capabilities when new tasks arise. Learning new tasks is typically relatively long and sample-inefficient, especially in complex tasks without prior knowledge and where rewards are sparse or delayed. Rather than relying solely on environmental feedback, an agent can benefit from understanding the reward logic itself, such as temporal dependencies, conditional sequences, or subgoals—thereby improving learning efficiency and policy quality [14, 15, 16]. By leveraging structured reward representations, agents can more effectively sequence and reuse behaviours, enabling them to adjust previously learned skills to new or modified tasks. This structured approach facilitates faster adaptation, as agents can generalise from prior experience rather than starting from scratch each time [17].\n\nAnother approach to reduce learning time and improve generalisability is to utilise direct prior task knowledge from models or human demonstrations [18, 2]. In that way, an agent already has a rough policy, which then only needs to be refined and optimised for the given specifics of the robot and the environment. However, resulting policies that are not considered optimal for the given specific task are often discarded. We argue that such “failed” or “suboptimal” policies should not be discarded, since such skill variations resulting from various mutations might be useful starting points for learning new tasks.\n\nMutation of policy in robot learning when subject to physical interaction with humans and unpredictable environments has been observed in [19]. A follow-up study [20] investigated how the policy mutations occur during the learning and what kind of variations of skill emerge in a sawing task. The study concluded that certain policy variations may not be optimal or suitable for the original task, but can be useful for optimising some other parameters/tasks. This highlighted the potential for diversification of skills and the importance of not discarding the policies that appear to be suboptimal for the current task, as they might be a good starting point for unforeseen new tasks. However, the work so far relied on mutations from a passive environment, while intentional mutations with a systematic mechanism are still missing.\n\nTo address this challenge, we introduce a reward mutation framework that treats the reward function as a tunable mechanism for active skill diversification (Fig. 1). While online reward-shaping has been investigated for improvement of sample efficiency and optimisation of a specific task in [21], differently, the proposed approach mutates the reward function to discover new skills. As a starting point, we study a liquid pouring task performed by a Franka Emika Panda robotic manipulator in the NVIDIA Isaac Sim simulation environment, where the reward is composed of three weighted terms: pouring accuracy, time spent, and effort spent. The goal of the study is to systematically explore how the mutation of the weights of these reward terms induces the emergence of diverse policies. We perform training of agents with Proximal Policy Optimization (PPO) [22] under 25 distinct reward configurations. Some of the emerging policies are useful for executing the original task in different ways (fast and slow), some are identified to be useful for unforeseen other tasks (container rim cleaning, liquid mixing, watering), and some are not useful for any identifiable tasks.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何通过变异奖励函数来实现机器人技能的多样化。  \n2. 现有的强化学习方法在新任务学习中的效率和泛化能力不足。  \n3. 如何有效利用“次优”策略作为新任务学习的起点。  \n\n【用了什么创新的方案】  \n本研究提出了一种奖励变异框架，通过对奖励函数中不同权重施加高斯噪声，系统性地探索了不同奖励配置对学习策略的影响。该方法允许机器人在液体倒入任务中产生多样化的技能，从而不仅优化原有任务的执行方式，还为意外任务（如容器边缘清洁、液体混合和浇水）提供了有用的技能。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems",
            "authors": "Haeyoon Han,Mahdi Taheri,Soon-Jo Chung,Fred Y. Hadaegh",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18460",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18460",
            "arxiv_html_link": "https://arxiv.org/html/2509.18460v1",
            "abstract": "Perception systems provide a rich understanding of the environment for autonomous systems, shaping decisions in all downstream modules. Hence, accurate detection and isolation of faults in perception systems is important. Faults in perception systems pose particular challenges: faults are often tied to the perceptual context of the environment, and errors in their multi-stage pipelines can propagate across modules. To address this, we adopt a counterfactual reasoning approach to propose a framework for fault detection and isolation (FDI) in perception systems. As opposed to relying on physical redundancy (i.e., having extra sensors), our approach utilizes analytical redundancy with counterfactual reasoning to construct perception reliability tests as causal outcomes influenced by system states and fault scenarios. Counterfactual reasoning generates reliability test results under hypothesized faults to update the belief over fault hypotheses. We derive both passive and active FDI methods. While the passive FDI can be achieved by belief updates, the active FDI approach is defined as a causal bandit problem, where we utilize Monte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find control inputs that maximize a detection and isolation metric, designated as Effective Information (EI). The mentioned metric quantifies the informativeness of control inputs for FDI. We demonstrate the approach in a robot exploration scenario, where a space robot performing vision-based navigation actively adjusts its attitude to increase EI and correctly isolate faults caused by sensor damage, dynamic scenes, and perceptual degradation.",
            "introduction": "Autonomous systems such as self-driving cars, unmanned aerial vehicles (UAV), and autonomous robots rely on perception systems to convert heterogeneous sensor measurements into a coherent representation of their surrounding environment [1]. The role of the perception system is to provide accurate and timely information on objects, terrain, and the surrounding environment so that higher-level modules in an autonomous system (e.g., localization, motion planning, and control) can guarantee safety and achieve mission objectives [2]. The combination of utilizing heterogeneous sensors (e.g., LiDAR, radar, cameras) and deep learning-based algorithms has led to recent advances in perception-based control. However, this has also resulted in an increased level of complexity in perception systems, which makes detecting their faults and algorithmic errors challenging [3, 4]. Considering the importance of a perception system in the guidance and control of an autonomous system, perception faults can result in the complete loss of a mission. For instance, on 6 June 2025, the Japanese lunar lander Resilience (Hakuto-R Mission 2) had a hard landing during its final descent on the Moon when its laser range finder began outputting erroneous altitude values in the last few kilometers before touchdown [5]. This highlights the need for accurate monitoring systems that can address the problem of fault detection and isolation (FDI) in perception systems.\n\nThe method presented in this paper can handle a broad range of fault and failure types, including both physical malfunctions and algorithmic errors in perception systems that cause deviations from their intended functionality. On the physical side, sensors can suffer calibration shifts, temporary occlusions, and environmental interference [7]. At the algorithmic level, deep neural networks (DNN) can misclassify objects due to distribution shifts (i.e., out-of-distribution inputs), and multi-sensor fusion can become erroneous due to calibration issues [8, 9, 10]. Moreover, faults that occur at an early stage of a perception system’s pipeline propagate through it and do not remain isolated [11]. Hence, FDI methodologies that rely on physical redundancy may not be sufficient [12]. Thus, one needs to study and investigate FDI methodologies based on the available analytical redundancy in perception systems. Once a certain fault is detected and isolated, a fault recovery control can be implemented.\n\nThe faults that occur in Simultaneous Localization and Mapping (SLAM) and Visual Inertial Odometry (VIO) systems are sensor faults [3, 7], tracking failures [15, 16], data association failures [17, 18], and filtering inconsistency problems [19]. Sensor faults are caused by hardware damage or software malfunction. Faults in front-end modules, such as tracking and data association failures, are often caused by visually deprived conditions (i.e., textureless surfaces and repetitive patterns), dynamic scenes (i.e., aggressive camera motion), and undesirable lighting conditions (i.e., high-contrast images). Lastly, the filtering inconsistency problems, a type of fault in back-end modules, result from large inter-frame transformations that trigger the accumulation of linearization errors.\n\nThe work in [8] compares perception outputs with a predefined fault threshold for runtime monitoring. Additionally, [3] developed fault diagnostic graphs to associate errors with individual perception module outputs, as evaluated by diagnostic tests. Although these works enable FDI, they rely on having redundant sensors, which can be costly. To enhance the robustness of SLAM [20] developed image quality metrics to select confident features or scenes. Similarly, feature quality metrics that assess keypoint co-visibility between frames [15, 21, 3] and the dynamic scene metrics that leverage vehicle velocity [15], optical flow [22], and image sharpness [23, 24] have been proposed.\n\nWe define perception reliability tests for various fault modes to capture differences between fault-free and fault-induced behaviors. We utilize the structural causal model (SCM) formalism of Pearl [25] and its operational rules for interventions and counterfactual queries, where we treat each hypothesized fault mode as an intervention on the perception pipeline. We then introduce and define an information-theoretic metric based on the Kullback–Leibler (KL) divergence between the reliability test results and those from a baseline fault-free case to measure the detectability and isolability of the hypothesized faults. This metric, designated as Effective Information (EI), captures how control inputs influence the reliability test results by affecting the autonomous system’s state. To the best of our knowledge, this is the first work that studies the FDI as a counterfactual reasoning problem for a closed-loop autonomous system and also connects the informativeness of control inputs to the detection and isolation of the hypothesized faults. Finally, we show that finding the control input that helps maximizing the EI leads to having a causal bandit problem [26], where each action arm corresponds to an intervention on the control input that improves our FDI accuracy. A Monte-Carlo Tree Search (MCTS) approach with Upper Confidence Bound (UCB) [27, 28] that penalizes large deviations from primary mission objectives (e.g., tracking a trajectory) is employed to solve the mentioned causal bandit problem.\n\nThe main contributions of this paper are as follows.\n\nWe exploit analytical redundancy of the perception system and actively use control inputs for FDI by applying the do-operator from causal inference. This is achieved via a counterfactual reasoning approach, where it is analyzed how control inputs affect reliability test outcomes under various fault hypotheses. A quantitative detection and isolation metric measuring the informativeness of each control input for FDI is introduced.\n\nWe formulate the problem of selecting control inputs for FDI as a causal bandit problem. Using a MCTS strategy with UCB, we maximize a weighted reward function that prioritizes inputs informative about the most likely fault modes. In addition, our reward function penalizes large deviations from the desired trajectory of the system.\n\nOur FDI method uses the distribution of reliability test results under various fault modes and accounts for the uncertainty inherent in the perception system’s outputs. Thus, our method encodes more information than mean value and threshold-based FDI methods, which only reflect the central tendency of a distribution.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在感知系统中准确检测和隔离故障。  \n2. 传统的物理冗余方法在故障检测中的局限性。  \n3. 如何利用反事实推理提高故障检测和隔离的有效性。  \n\n【用了什么创新的方案】  \n本研究提出了一种基于反事实推理的故障检测和隔离（FDI）框架，利用分析冗余而非物理冗余来构建感知可靠性测试。通过生成假设故障下的可靠性测试结果，更新对故障假设的信念。我们将FDI问题建模为因果赌博问题，使用蒙特卡洛树搜索（MCTS）和上置信界（UCB）策略来最大化有效信息（EI），从而优化控制输入以提高故障检测和隔离的准确性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands",
            "authors": "Yunshuang Li,Yiyang Ling,Gaurav S. Sukhatme,Daniel Seita",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18455",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18455",
            "arxiv_html_link": "https://arxiv.org/html/2509.18455v1",
            "abstract": "Nonprehensile manipulation, such as pushing and pulling, enables robots to move, align, or reposition objects that may be difficult to grasp due to their geometry, size, or relationship to the robot or the environment. Much of the existing work in nonprehensile manipulation relies on parallel-jaw grippers or tools such as rods and spatulas. In contrast, multi-fingered dexterous hands offer richer contact modes and versatility for handling diverse objects to provide stable support over the objects, which compensates for the difficulty of modeling the dynamics of nonprehensile manipulation.\nTherefore, we propose Geometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile manipulation with dexterous robotic hands. We study pushing and pulling by framing the problem as synthesizing and learning pre-contact dexterous hand poses that lead to effective manipulation. We generate diverse hand poses via contact-guided sampling, filter them using physics simulation, and train a diffusion model conditioned on object geometry to predict viable poses.\nAt test time, we sample hand poses and use standard motion planners to select and execute pushing and pulling actions.\nWe perform 840 real-world experiments with an Allegro Hand, comparing our method to baselines. The results indicate that GD2P offers a scalable route for training dexterous nonprehensile manipulation policies. We further demonstrate GD2P on a LEAP Hand, highlighting its applicability to different hand morphologies. Our pre-trained models and dataset, including 1.3 million hand poses across 2.3k objects, will be open-source to facilitate further research.\nOur project website is available at: geodex2p.github.io.",
            "introduction": "Nonprehensile actions are fundamental to how humans and robots interact with the physical world [4, 5, 6, 7].\nThese actions permit the manipulation of objects that may be too large, heavy, or geometrically complex to grasp directly.\nWhile there has been tremendous progress in nonprehensile robot manipulation [8, 9, 10, 11, 12], most work uses simple end-effectors such as parallel-jaw grippers, rods [13, 14], or spatulas [15]. In contrast, multi-fingered hands with high degrees-of-freedom (DOF) such as the Allegro Hand or LEAP Hand [16] enable contact patterns that can be especially useful for stabilizing complex, awkward, or top-heavy objects, or for coordinating contact across multiple objects, compensating for the challenges of modeling nonprehensile manipulation dynamics.\nHowever, despite their promise and recent progress [17] [18], leveraging high-DOF hands for nonprehensile manipulation remains relatively underexplored due to the challenges of modeling hand-object relationships and planning feasible contact-rich motions.\n\nIn this paper, we study pushing and pulling objects using the 4-finger, 16-DOF Allegro and LEAP Hands. We select pushing and pulling as representative tasks of nonprehensile manipulation because they are more commonly used for manipulating general daily objects and are\nmore amenable to scaling.\nOur insight is to recast this problem into one of synthesizing effective pre-contact hand poses, an approach inspired by recent success in generating large-scale datasets for dexterous manipulation [19, 20, 21, 22, 23, 24].\nWe propose a scalable pipeline for generating hand poses for pushing and pulling objects. This involves contact-guided optimization and validation via GPU-accelerated physics simulation with IsaacGym [25].\nThese filtered hand poses are then used to train a generative diffusion policy conditioned on object geometry, represented using basis point sets [1].\n\nAt test time, we use visual data to reconstruct an object mesh in physics simulation. The trained diffusion policy uses this mesh to generate diverse hand poses for pushing or pulling. We then validate the resulting hand poses in simulation, and execute the best-performing action in the real world.\nWe call this pipeline Geometry-aware Dexterous Pushing and Pulling (GD2P) with multi-fingered hands.\nFigure LABEL:fig:pull shows several real-world examples where the hand pose differs depending on object geometry. Overall, our experimental results across diverse daily objects demonstrate that GD2P is a promising approach for generalizable object pushing and pulling. It outperforms alternative methods such as querying the nearest hand pose in our data or using a fixed spatula-like hand pose, highlighting the need for a diffusion model to generate diverse hand poses.\n\nTo summarize, the contributions of this paper include:\n\nA scalable pipeline for generating and filtering dexterous hand poses for nonprehensile pushing and pulling.\n\nA diffusion model for geometry-conditioned hand pose prediction for nonprehensile pushing and pulling.\n\nA motion planning framework to execute these poses in the real world, with results across 840 trials showing that GD2P outperforms alternative methods.\n\nA dataset of 1.3 million hand poses for nonprehensile pushing and pulling across 2.3k objects with corresponding canonical point cloud observations.\n\n1. A scalable pipeline for generating and filtering dexterous hand poses for nonprehensile pushing and pulling.\n\n2. A diffusion model for geometry-conditioned hand pose prediction for nonprehensile pushing and pulling.\n\n3. A motion planning framework to execute these poses in the real world, with results across 840 trials showing that GD2P outperforms alternative methods.\n\n4. A dataset of 1.3 million hand poses for nonprehensile pushing and pulling across 2.3k objects with corresponding canonical point cloud observations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用多指灵巧手进行非抓取操作（如推拉）以处理复杂物体。  \n2. 如何生成有效的手部预接触姿势以提高非抓取操作的成功率。  \n3. 如何在真实环境中执行基于物体几何的手部姿势以实现有效的推拉操作。  \n\n【用了什么创新的方案】  \n提出了一种几何感知的灵巧推拉（GD2P）方法，通过接触引导采样生成多样的手部姿势，并利用物理仿真进行过滤。训练一个条件于物体几何的扩散模型来预测可行的手部姿势。测试时，使用标准运动规划器选择并执行推拉动作。该方法在840次真实实验中表现优于基线，展示了其在不同手形态上的适用性，并提供了130万个手部姿势的数据集以促进后续研究。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction",
            "authors": "Rishabh Madan,Jiawei Lin,Mahika Goel,Angchen Xie,Xiaoyu Liang,Marcus Lee,Justin Guo,Pranav N. Thakkar,Rohan Banerjee,Jose Barreiros,Kate Tsui,Tom Silver,Tapomayukh Bhattacharjee",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Conference on Robot Learning (CoRL)",
            "pdf_link": "https://arxiv.org/pdf/2509.18447",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18447",
            "arxiv_html_link": "https://arxiv.org/html/2509.18447v1",
            "abstract": "Physical human–robot interaction (pHRI) requires robots to adapt to individual contact preferences, such as where and how much force is applied. Identifying preferences is difficult for a single contact; with whole-arm interaction involving multiple simultaneous contacts between the robot and human, the challenge is greater because different body parts can impose incompatible force requirements. In caregiving tasks, where contact is frequent and varied, such conflicts are unavoidable. With multiple preferences across multiple contacts, no single solution can satisfy all objectives–trade-offs are inherent, making prioritization essential. We present PrioriTouch, a framework for ranking and executing control objectives across multiple contacts. PrioriTouch can prioritize from a general collection of controllers, making it applicable not only to caregiving scenarios such as bed bathing and dressing but also to broader multi-contact settings. Our method combines a novel learning-to-rank approach with hierarchical operational space control, leveraging simulation-in-the-loop rollouts for data-efficient and safe exploration. We conduct a user study on physical assistance preferences, derive personalized comfort thresholds, and incorporate them into PrioriTouch. We evaluate PrioriTouch through extensive simulation and real-world experiments, demonstrating its ability to adapt to user contact preferences, maintain task performance, and enhance safety and comfort. Website: https://emprise.cs.cornell.edu/prioritouch.",
            "introduction": "Physical human–robot interaction (pHRI) requires physical contact. Contact is not uniform: individuals have distinct preferences for acceptable forces and contact locations [1, 2, 3, 4, 5]. For pHRI to be safe and effective, robots must personalize their behavior, and a critical aspect of personalization is contact preferences. Even for a single contact, identifying and respecting these preferences while ensuring task success is challenging. Many physical robot caregiving tasks, such as bathing [6, 7], dressing [8, 9], and transferring [10], require whole‑arm pHRI [11], where multiple segments of the robot arm simultaneously touch the human body. For example, during bed bathing (Fig. 1), the robot may need to reach over a user to wipe the upper arm while maintaining comfortable forces on the torso and shoulder. Although whole‑arm manipulation expands workspace and improves maneuverability, it also exacerbates conflicts: different body parts can impose incompatible force requirements, and no single policy can satisfy all objectives.\n\nTo bootstrap personalization, we elicit population‑level contact preferences offline and use them to seed a conservative base policy. However, a one‑size‑fits‑all policy is insufficient: (i) stated preferences can diverge from realized comfort under true contact (pressure/shear, approach, speed, duration); and (ii) preferences are context‑dependent and time‑varying (posture, clothing, fatigue). Therefore, online interaction is necessary to accommodate individual preferences. Experimenting directly with the user is risky and inefficient because each update can involve repeated physical contact and multiple feedback exchanges. This increases the user’s cognitive workload, prolongs the interaction, and may cause discomfort when forces are suboptimal or excessive.\n\nWe introduce PrioriTouch, a framework that casts contact preference learning as a learning‑to‑rank problem over control objectives. Given a reference trajectory produced by a high‑level policy (e.g., a contact‑aware planner generating end‑effector or joint‑space paths), PrioriTouch instantiates pose‑tracking and force‑regulation objectives from the current contact state. We develop LinUCB‑Rank, a contextual bandit that learns a priority policy; H‑OSC [12] then executes this ordering as a null space hierarchy, translating high‑level preferences into low‑level control. We initialize the policy with conservative priors derived from population-level user-study statistics. During interaction, LinUCB-Rank adapts the ordering online using sparse user feedback while safely refining the policy via simulation-in-the-loop learning before deploying it in real-world interactions. The framework is controller-agnostic: it can rank heterogeneous objectives, enabling principled trade-offs across simultaneous objectives.\n\nWe evaluate PrioriTouch across simulated and real-world environments, progressively increasing in complexity and realism. First, we design a simplified simulation scenario with predefined contacts and a static end-effector pose to isolate and specifically assess LinUCB-Rank’s ability to learn user contact preferences. Second, we demonstrate PrioriTouch in a simulated caregiving scenario involving robot-assisted bed bathing, requiring whole-arm contact to safely wipe a user’s limbs. Third, we showcase our approach’s capability in intricate multi-contact scenarios through a real-world 3D goal-reaching maze with multiple vertical cylinders representing distinct body-part contact preferences. Finally, we validate PrioriTouch’s practical feasibility by performing a realistic caregiving task in a user study with human subjects.\n\nOur contributions are summarized as follows:\n\nWe propose PrioriTouch, a framework that formulates contact preference learning as a ranking problem over control objectives and executes the learned priority ordering as a null space hierarchy via H-OSC for whole-arm pHRI.\n\nWe introduce LinUCB‑Rank, a contextual bandit that learns priority orderings from sparse user feedback while accounting for inter‑objective coupling in hierarchical control.\n\nWe enable safe and data-efficient learning through simulation-in-the-loop validation, where candidate priority updates are tested in a digital twin before real-world deployment.\n\nWe conduct a user study to inform realistic models of contact preferences for robot-initiated touch, which we leverage to simulate authentic user feedback in our evaluation.\n\nWe evaluate PrioriTouch through extensive simulation, real-world experiments, and a realistic caregiving user study, demonstrating effective adaptation to individual contact preferences without compromising task performance or comfort.\n\nOur framework integrates user contact preference learning with low-level control by parameterizing operational space control using the outputs of a learned ranking policy. This structured integration ensures that high-level feedback is directly translated into low-level force regulation and pose tracking, effectively bridging the gap between user preferences and robot control.\n\n1. We propose PrioriTouch, a framework that formulates contact preference learning as a ranking problem over control objectives and executes the learned priority ordering as a null space hierarchy via H-OSC for whole-arm pHRI.\n\n2. We introduce LinUCB‑Rank, a contextual bandit that learns priority orderings from sparse user feedback while accounting for inter‑objective coupling in hierarchical control.\n\n3. We enable safe and data-efficient learning through simulation-in-the-loop validation, where candidate priority updates are tested in a digital twin before real-world deployment.\n\n4. We conduct a user study to inform realistic models of contact preferences for robot-initiated touch, which we leverage to simulate authentic user feedback in our evaluation.\n\n5. We evaluate PrioriTouch through extensive simulation, real-world experiments, and a realistic caregiving user study, demonstrating effective adaptation to individual contact preferences without compromising task performance or comfort.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在物理人机交互中适应个体的接触偏好。  \n2. 在多接触场景中，如何处理不同身体部位的力要求冲突。  \n3. 如何高效地学习和调整接触偏好以确保安全和舒适。  \n\n【用了什么创新的方案】  \nPrioriTouch框架将接触偏好学习视为控制目标的排名问题，通过LinUCB-Rank上下文赌博机从稀疏用户反馈中学习优先级排序，并利用H-OSC以空心空间层次结构执行该排序。该方法结合了模拟环中的验证，确保在真实世界中的安全部署，同时通过用户研究获得个性化的接触阈值，以增强适应性和任务性能。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Latent Action Pretraining Through World Modeling",
            "authors": "Bahey Tharwat,Yara Nasser,Ali Abouzeid,Ian Reid",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18428",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18428",
            "arxiv_html_link": "https://arxiv.org/html/2509.18428v1",
            "abstract": "Vision-Language-Action (VLA) models have gained popularity for learning robotic manipulation tasks that follow language instructions. State-of-the-art VLAs, such as OpenVLA and π0\\pi_{0}, were trained on large-scale, manually labeled action datasets collected through teleoperation. More recent approaches, including LAPA and villa-X, introduce latent action representations that enable unsupervised pretraining on unlabeled datasets by modeling abstract visual changes between frames. Although these methods have shown strong results, their large model sizes make deployment in real-world settings challenging.\nIn this work, we propose LAWM, a model-agnostic framework to pretrain imitation learning models in a self-supervised way, by learning latent action representations from unlabeled video data through world modeling. These videos can be sourced from robot recordings or videos of humans performing actions with everyday objects. Our framework is designed to be effective for transferring across tasks, environments, and embodiments. It outperforms models trained with ground-truth robotics actions and similar pretraining methods on the LIBERO benchmark and real-world setup, while being significantly more efficient and practical for real-world settings.",
            "introduction": "Self-supervised learning has been a key enabler of recent breakthroughs in Large Language Models (LLMs) such as ChatGPT [1] and Gemini [2], where models learn from large amounts of text on the Internet. Inspired by this success, the robotics community is now ready for its own transformative moment, where we can build systems that learn action representations directly from raw, unstructured video data, rather than relying on curated action labels.\n\nMost current approaches to robot learning are heavily based on supervised learning frameworks. Methods like imitation learning and VLA models, including OpenVLA [3] and π0\\pi_{0} [4], require paired image action datasets often obtained through teleoperation. These action annotations are expensive to collect, difficult to scale, and prone to bias, limiting the generalizability of these systems across tasks, environments, and embodiments.\n\nIn this work, we introduce LAWM, a Latent Action pretraining framework through World Modeling that aims to overcome these limitations by combining an Imitation Learning Model with a World Model. Our objective, as shown in Fig. LABEL:fig:first-page, is to learn action representations from both robot-collected and human demonstration videos in a fully self-supervised way. These learned representations serve as action priors that can be effectively leveraged during finetuning on downstream tasks. The proposed framework, illustrated in Figure 2, is designed to be model-agnostic, meaning that it does not depend on any specific architecture for the imitation learning model or the world model. This flexibility allows for the integration of a variety of different models.\nOur pipeline follows a two-stage steps. The first stage is an end-to-end pretraining in a self-supervised way, with the learning signal derived from predicting the next image in a video sequence. The inputs to the system consist of: (i) an image frame from a human or robot performing a manipulation task, and (ii) a natural language instruction describing the goal of the task. The imitation learning model takes these inputs to produce action chunks representations. These representations of nn actions are paired with the current image frame and the next n−1n-1 frames, then fed into the world model to generate the next image frames that would result from executing the action in the current environment. The second stage is a finetuning stage, where labeled data are used to finetune only the imitation learning model to downstream tasks. During this phase, the world model is no longer used. The learned imitation learning model is now equipped with a robust prior from large-scale unlabeled videos and can be finetuned efficiently.\n\nWe summarize our main contributions and findings below:\n\nWe propose LAWM, a model-agnostic framework, to learn action chunk representations for imitation learning models from both robot and human videos without action labels.\n\nOur experiments show that our framework can learn superior action priors from human demonstrations and robotic manipulation videos without using ground-truth action labels, compared to supervised pretraining.\n\nWe demonstrate that our framework with small models such as BAKU [5] and Dreamerv3 [6] outperforms similar methods with large models such as villa-X [7] on the LIBERO benchmark  [8].\n\n1. We propose LAWM, a model-agnostic framework, to learn action chunk representations for imitation learning models from both robot and human videos without action labels.\n\n2. Our experiments show that our framework can learn superior action priors from human demonstrations and robotic manipulation videos without using ground-truth action labels, compared to supervised pretraining.\n\n3. We demonstrate that our framework with small models such as BAKU [5] and Dreamerv3 [6] outperforms similar methods with large models such as villa-X [7] on the LIBERO benchmark  [8].",
            "llm_summary": "【关注的是什么问题】  \n1. 当前机器人学习方法依赖于昂贵的监督学习框架，限制了系统的可扩展性。  \n2. 大规模标注数据集的收集困难且容易产生偏差，影响模型的泛化能力。  \n3. 现有方法在真实世界应用中面临模型规模过大和部署效率低的问题。  \n\n【用了什么创新的方案】  \n提出LAWM框架，通过世界建模在自监督方式下学习潜在动作表示，利用机器人和人类视频数据，无需动作标签。该框架采用两阶段流程：第一阶段进行自监督预训练，第二阶段在有标签数据上微调模仿学习模型。LAWM在LIBERO基准测试中表现优于依赖真实动作标签的模型，且在真实环境中更高效、实用。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections",
            "authors": "Navya Tiwari,Joseph Vazhaeparampil,Victoria Preston",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
            "comment": "6 pages, 5 figures. Accepted as a poster at Northeast Robotics Colloquium (NERC 2025). Extended abstract",
            "pdf_link": "https://arxiv.org/pdf/2509.18407",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18407",
            "arxiv_html_link": "https://arxiv.org/html/2509.18407v1",
            "abstract": "未获取到摘要",
            "introduction": "Intersections account for nearly 40% of U.S. crashes [1], with many occurring at uncontrolled or partially controlled locations [2]. Ambiguous right of way, compounded by occlusions, non-compliant drivers, and limited sensing leave drivers uncertain how to act [3]. Addressing these challenges requires assistive technology that reduces driver uncertainty and improves awareness. We propose an Advanced Driver Assistance System (ADAS) that fuses sensor data, interprets intersection context, and applies uncertainty-aware frameworks to recommend safe actions at uncontrolled intersections. We pose three research questions (RQs):\n\nHow can the accuracy of ego- and external-vehicle state estimation be improved while constraining uncertainty through frustum-based fusion of camera and lidar data, given real-time, computationally limited resources?\n\nHow can the accuracy of ego- and external-vehicle state estimation be improved while constraining uncertainty through frustum-based fusion of camera and lidar data, given real-time, computationally limited resources?\n\nTo what extent can a driver-assist system enable safe navigation of uncontrolled intersections by efficiently interpreting intersection context (lane markings, stop signs, traffic flow patterns, and pedestrian presence) under partial observability?\n\nHow effectively can different decision-making frameworks handle uncertainty at uncontrolled intersections, and what trade-offs emerge between safety, efficiency, and computational feasibility in real-time deployment?\n\nHere, we present an initial analysis of RQ3 in synthetic uncontrolled intersections. We show that probabilistic planners, particularly POMCP (Partially Observable Monte Carlo Planning) [7] and DESPOT (Determinized Sparse Partially Observable Tree) [15], outperform deterministic approaches in predicting the intent of other drivers and selecting collision-free actions, while maintaining safety under complex, partially observable scenarios. Continued work will integrate Sensor Fusion (RQ1) and Environment Perception (RQ2) modules for end-to-end, real-time navigation under realistic traffic and environmental conditions.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高自我和外部车辆状态估计的准确性，同时在实时计算资源有限的情况下约束不确定性？  \n2. 如何在部分可观测的情况下高效解读交叉口环境，以实现安全导航？  \n3. 不同决策框架在处理不确定性时的有效性如何，以及在实时部署中安全性、效率和计算可行性之间的权衡是什么？  \n\n【用了什么创新的方案】  \n提出了一种先进的驾驶辅助系统（ADAS），通过融合传感器数据、解释交叉口上下文，并应用不确定性感知框架，推荐在无人控制交叉口的安全行动。初步分析表明，概率规划方法（如POMCP和DESPOT）在预测其他驾驶员意图和选择无碰撞行动方面优于确定性方法，同时在复杂的部分可观测场景中保持安全。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation",
            "authors": "Rajitha de Silva,Jonathan Cox,James R. Heselden,Marija Popovic,Cesar Cadena,Riccardo Polvara",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "Sumbitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.18342",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18342",
            "arxiv_html_link": "https://arxiv.org/html/2509.18342v1",
            "abstract": "Accurate localisation is critical for mobile robots in structured outdoor environments, yet LiDAR-based methods often fail in vineyards due to repetitive row geometry and perceptual aliasing. We propose a semantic particle filter that incorporates stable object-level detections, specifically vine trunks and support poles into the likelihood estimation process. Detected landmarks are projected into a bird’s eye view and fused with LiDAR scans to generate semantic observations. A key innovation is the use of semantic walls, which connect adjacent landmarks into pseudo-structural constraints that mitigate row aliasing. To maintain global consistency in headland regions where semantics are sparse, we introduce a noisy GPS prior that adaptively supports the filter. Experiments in a real vineyard demonstrate that our approach maintains localisation within the correct row, recovers from deviations where AMCL fails, and outperforms vision-based SLAM methods such as RTAB-Map.",
            "introduction": "Accurate localisation is a critical component of mobile robot navigation in outdoor environments [1]. Among the various approaches, LiDAR-based localisation remains widely adopted due to its reliable and precise perception of geometric structure. However, these methods rely solely on scene geometry, which can be problematic in outdoor agricultural settings like vineyards, where repetitive and ambiguous structures are common [2]. In such environments, incorporating semantic information complements the geometric structure offering a promising alternative to enhance localisation performance [3].\n\nIn this paper, we tackle the challenge of semantic ambiguity in geometry-based localisation within vineyard environments. The repetitive structure of vineyard rows often induces perceptual aliasing in LiDAR range data, resulting in localisation drift and errors. To overcome this limitation, we exploit semantically meaningful landmarks, specifically vine trunks and support poles whose distinctive spatial distributions provide stronger discriminative cues. Our approach detects these semantic objects and estimates their relative positions from RGB-D imagery, which are then projected onto the LiDAR frame. This enables a semantic-LiDAR particle filter that offers a robust alternative to conventional localisation methods.\n\nTraditional particle filters, such as Adaptive Monte Carlo Localisation (AMCL) [4], estimate a robot’s pose by evaluating the geometric consistency between sensor observations and a known map, an approach that has proven highly effective in structured indoor and urban settings where distinctive geometric features are abundant. Vineyards, however, present a markedly different challenge: their long, repetitive rows induce strong perceptual aliasing, while unstable elements such as foliage and grape clusters provide little reliability for long-term localisation [5]. We contend that robust localisation in such environments requires moving beyond raw geometry and explicitly exploiting semantics. Our key insight is that vine trunks and support poles serve as stable, distinctive landmarks whose consistent spatial distribution across rows can disambiguate pose estimates. Moreover, we introduce the concept of semantic walls, where the space between consecutive landmarks is modelled as a pseudo-structural boundary. This transforms sparse semantic detections into continuous row-level constraints, creating a representation that is far more robust to vineyard aliasing and seasonal variation. Together, these ideas lay the foundation for a semantic-LiDAR particle filter that redefines localisation in repetitive agricultural environments, as illustrated in Fig. 1.\n\nThe main contributions of this paper are threefold:\n(i) the design of a semantic particle filter that integrates object-level detections of stable vineyard landmarks (trunks and support poles) with a 2D semantic map, enabling robust localisation in highly repetitive environments;\n(ii) the introduction of the semantic walls concept, which augments sparse landmark detections by modelling pseudo-structural boundaries between adjacent landmarks, thereby strengthening row-level constraints and mitigating perceptual aliasing; and\n(iii) a systematic evaluation against established baselines, demonstrating the trade-offs in accuracy, robustness, and sensor requirements.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在重复结构的环境中实现准确的移动机器人定位。  \n2. 如何克服LiDAR方法在葡萄园中因感知别名而导致的定位漂移和错误。  \n3. 如何利用语义信息增强几何结构以提高定位性能。  \n\n【用了什么创新的方案】  \n提出了一种语义粒子滤波器，将稳定的对象级检测（如葡萄藤干和支撑杆）融入到定位过程中，通过将检测到的地标投影到鸟瞰图并与LiDAR扫描融合生成语义观测。此外，引入了语义墙的概念，将相邻地标连接成伪结构约束，以减轻行别名问题，并在语义稀疏的区域引入噪声GPS先验以维持全局一致性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020",
            "authors": "Marsette Vona",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18330",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18330",
            "arxiv_html_link": "https://arxiv.org/html/2509.18330v1",
            "abstract": "The Landform contextual mesh fuses 2D and 3D data from up to thousands of Mars 2020 rover images, along with orbital elevation and color maps from Mars Reconnaissance Orbiter, into an interactive 3D terrain visualization. Contextual meshes are built automatically for each rover location during mission ground data system processing, and are made available to mission scientists for tactical and strategic planning in the Advanced Science Targeting Tool for Robotic Operations (ASTTRO). A subset of them are also deployed to the ”Explore with Perseverance” public access website.",
            "introduction": "Like its predecessors including Spirit, Opportunity, and Curiosity, the Mars 2020 Perseverance rover carries a suite of stereo cameras to image the surrounding terrain [1, 2, 3]. Data from those instruments is used for multiple purposes including both on-board and ground-based navigation, engineering operations, and science analysis. An important use case is tactical and strategic science planning, where teams of mission scientists use the imagery to select areas of interest and plan subsequent observations.\n\nStereo vision produces a “tactical wedge” 3D terrain mesh for each stereo image pair, so-called because often a radial panorama of such wedges is acquired from a single rover location using the pan/tilt mast. Of course, only a fraction of nearby terrain is included since the cameras have limited fields of view, effective resolution decreases with distance, the rover occludes areas underneath itself, nearby rocks and hills create self-occlusions in the terrain, and stereo reconstruction fails in areas with insufficient texture. Nevertheless, viewing such panoramas of tactical wedges in 2D and 3D (latter also called the tactical mesh) has been a standard approach for science planning on Mars 2020 and its predecessors. Figure 1(a) shows such a view for site 40, drive 132 of the Perseverance rover, acquired on sols 821–832111One sol is the equivalent of a day on Mars..\n\nAn alternative to the tactical mesh is to use a portion of a digital elevation map (DEM) derived from orbital observations, as shown in Figure 1(b). The Mars 2020 mission typically uses colored DEM data from the HiRISE instrument on the Mars Reconnaissance Orbiter [4]. This orbital mesh can cover a much larger extent—up to 10s of km—and typically has no gaps in its coverage. However, at a typical resolution of 1 elevation sample (and 16 color samples) per square meter, it’s much coarser than the tactical mesh, which can have sub-millimeter resolution near the rover.\n\nIn this paper we introduce the contextual mesh, which we have developed to fuse up to thousands of images from in-situ stereo cameras together with orbital DEM data into a single 3D scene, shown in Figure 1(c) and Figure 2. The contextual mesh is produced by Landform, a subsystem within the Mars 2020 ground data system (M20 GDS), and typically viewed in the Advanced Science Targeting Tool for Robotic Operations (ASTTRO) collaborative web application [5, 6], also part of the M20 GDS [7].\n\nWhereas the tactical mesh offers the highest fidelity local terrain reconstruction, and the orbital mesh the longest range reconstruction, the intention of the contextual mesh is to provide spatial awareness. It is typically visualized from a first-person navigable 3D point of view in ASTTRO, showing not only local terrain features such as sand, pebbles, rocks, ridges, and hills, but also distant landmarks on the horizon. ASTTRO also displays a 3D model of the Perseverance rover on the terrain as it was posed at the corresponding time in the mission.\n\nEach contextual mesh is comprised of two tilesets in the open-standard 3DTiles format [8]. One tileset contains the terrain itself, typically extending to a 1km square with a 100m square central detail area. The other tileset is a hemispherical representation of the surrounding horizon and sky, enabling visualization of distant features potentially many kilometers away. ASTTRO displays both of these simultaneously so that users can see the context of local terrain features relative to both nearby hills and to the horizon.\n\nThe 3DTiles format enables data to be progressively streamed to distributed users in the web-based ASTTRO client. Only the subset of tile data required depending on the user’s current viewpoint is transferred and rendered, enabling dynamic level-of-detail, fast load times, and deployment to resource limited clients.\n\nLike the products of many data fusion and reconstruction algorithms, e.g. computed tomography, and considering that the input data contains noise and outliers, the contextual mesh may contain some artifacts. For example\n\nboundaries between areas reconstructed primarily from surface vs orbital data may have some discontinuities\n\noutlier images with extreme brightness variations may not be completely blended\n\nreconstructed geometry may have “island” topological artifacts due to noise and residual misalignment in the input data.\n\nThe Landform contextual mesh differs from many other photogrammetry and terrain fusion systems not only because it combines both surface and orbital data and has a sky sphere, but also in that it is entirely automated, whereas most other systems require some human intervention. It heavily leverages properties of Mars mission datasets, including pose priors from rover navigation, calibrated stereo camera data, and co-registered orbital data. These enable automated processing while maintaining reasonable quality relative to manual and semi-manual approaches.\n\nThe Landform codebase will soon be released as open source. And, throughout the mission, a selection of contextual meshes have been made publicly available for interactive viewing at the “Explore with Perseverance” website [9].\n\nIn this paper we summarize the research context of related terrain fusion approaches, describe the novel algorithms we developed to implement the contextual mesh system in Landform, and present examples of contextual mesh data products.\n\n1. boundaries between areas reconstructed primarily from surface vs orbital data may have some discontinuities\n\n2. outlier images with extreme brightness variations may not be completely blended\n\n3. reconstructed geometry may have “island” topological artifacts due to noise and residual misalignment in the input data.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效融合火星探测器的2D和3D数据以生成高质量的地形可视化。  \n2. 在自动化处理过程中如何保持数据融合的质量，减少人为干预。  \n3. 如何在动态环境中实现3D场景的实时可视化和交互。  \n\n【用了什么创新的方案】  \n本文提出了一种名为“Landform contextual mesh”的新方法，通过自动融合来自火星2020探测器的数千张立体图像与轨道数字高程图（DEM）数据，生成一个交互式的3D地形可视化。该方法利用了火星任务数据集的特性，自动处理并生成高保真度的地形重建，提供了局部和远程地形特征的空间意识。此外，使用3DTiles格式实现数据的渐进式流式传输，支持动态细节级别和快速加载时间。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Haptic Communication in Human-Human and Human-Robot Co-Manipulation",
            "authors": "Katherine H. Allen,Chris Rogers,Elaine S. Short",
            "subjects": "Robotics (cs.RO)",
            "comment": "9 pages, 18 figures, ROMAN 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18327",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18327",
            "arxiv_html_link": "https://arxiv.org/html/2509.18327v1",
            "abstract": "When a human dyad jointly manipulates an object, they must communicate about their intended motion plans. Some of that collaboration is achieved through the motion of the manipulated object itself, which we call “haptic communication.” In this work, we captured the motion of human-human dyads moving an object together with one participant leading a motion plan about which the follower is uninformed. We then captured the same human participants manipulating the same object with a robot collaborator. By tracking the motion of the shared object using a low-cost IMU, we can directly compare human-human shared manipulation to the motion of those same participants interacting with the robot. Intra-study and post-study questionnaires provided participant feedback on the collaborations, indicating that the human-human collaborations are significantly more fluent, and analysis of the IMU data indicates that it captures objective differences in the motion profiles of the conditions. The differences in objective and subjective measures of accuracy and fluency between the human-human and human-robot trials motivate future research into improving robot assistants for physical tasks by enabling them to send and receive anthropomorphic haptic signals.",
            "introduction": "In physical collaboration tasks like carrying a couch or moving a table, haptic signals are an important channel of communication between participants to coordinate the group action. In human-human interactions, the communication and interpretation of these signals is primarily subconscious, but prior research suggests that they may enable more efficient human-robot collaboration [1]. In order for robots to participate in this haptic conversation, we need to develop a more robust understanding of how haptic communication occurs in both human-human and human-robot interaction. This knowledge can then be used to develop models for interpreting haptic intent, provide robots with comprehensible and predictable behavior, and avoid unwanted oscillations in collaborative manipulation.\n\nIn this paper, we present a study of haptic interaction, without use of visual or auditory signaling, during the collaborative manipulation of a shared object. We compare human-human and human-robot dyads to test whether there are observable differences in the subjective fluency of human-human and human-robot dyads, and whether these correlate with changes in the character of acceleration profiles of the co-manipulated objects. We additionally collect data on human perceptions of robot collaborators to identify potential co-variables in subjective fluency.\n\nWe conducted a user study with 34 participants. In the study, two agents collaborated to move a shared object, with one participant designated as the motion leader and one as a follower. Each participant acted in one these roles, and interacted with both another human participant and a mobile manipulator robot. We collected measures of subjective and objective measures of task fluency, as well as video and IMU recordings. We found that the acceleration data from an IMU mounted on the shared object changes more smoothly in human-human dyads than in human-robot dyads, with more fluent collaborations having smaller accelerations overall and smaller changes in acceleration during the task. We further find that common objective measures of collaboration fluency (e.g. task duration) do not correlate linearly with subjective fluency measures, and propose alternate measures based on our data. This work contributes to our understanding of the differences and similarities in current human-human and human-robot haptic communication during collaborative manipulation, and provides insights that can inform future methods for autonomous haptic signaling by robots.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何理解人类之间的触觉通信在共同操控中的作用？  \n2. 人类与机器人之间的触觉通信是否存在显著差异？  \n3. 如何提高机器人在物理任务中的协作能力？  \n\n【用了什么创新的方案】  \n本研究通过用户研究比较了人类与人类和人类与机器人在共同操控共享物体时的触觉交互。使用低成本IMU捕捉物体运动，分析了不同条件下的加速度数据，发现人类之间的协作流畅性显著高于人机协作。研究还探讨了主观流畅性与客观加速度特征之间的关系，为未来机器人自主触觉信号的研究提供了见解。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Fine-Tuning Robot Policies While Maintaining User Privacy",
            "authors": "Benjamin A. Christie,Sagar Parekh,Dylan P. Losey",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18311",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18311",
            "arxiv_html_link": "https://arxiv.org/html/2509.18311v1",
            "abstract": "Recent works introduce general-purpose robot policies.\nThese policies provide a strong prior over how robots should behave — e.g., how a robot arm should manipulate food items.\nBut in order for robots to match an individual person’s needs, users typically fine-tune these generalized policies — e.g., showing the robot arm how to make their own preferred dinners.\nImportantly, during the process of personalizing robots, end-users leak data about their preferences, habits, and styles (e.g., the foods they prefer to eat).\nOther agents can simply roll-out the fine-tuned policy and see these personally-trained behaviors.\nThis leads to a fundamental challenge: how can we develop robots that personalize actions while keeping learning private from external agents?\nWe here explore this emerging topic in human-robot interaction and develop PRoP, a model-agnostic framework for personalized and private robot policies.\nOur core idea is to equip each user with a unique key; this key is then used to mathematically transform the weights of the robot’s network.\nWith the correct key, the robot’s policy switches to match that user’s preferences — but with incorrect keys, the robot reverts to its baseline behaviors.\nWe show the general applicability of our method across multiple model types in imitation learning, reinforcement learning, and classification tasks.\nPRoP is practically advantageous because it retains the architecture and behaviors of the original policy, and experimentally outperforms existing encoder-based approaches.\nSee videos and code here: https://prop-icra26.github.io",
            "introduction": "Generalist policies enable robots to learn multiple tasks [1, 2].\nSo far these methods have traditionally been used in research labs and factories.\nBut we envision a future where robots enter domestic settings for assisting humans [3].\nFor example, consider a robot that is developed to help in a kitchen.\nThis robot will have some initial policy π0\\pi_{0} that users may want to finetune to match their own preferences and requirements.\nFor instance, perhaps the robot knows how to make a hamburger, but individual users prefer different ingredients, condiments, or even specific ways of stacking the burger.\nThis finetuning raises privacy concerns: the manufacturers can share the users’ data collected during finetuning with third-parties.\nConsequently, there is increasing demand for exploring new avenues to maintain the privacy and transparency of robotic agents [4].\nFollowing this, we come to a fundamental scientific question: how do we make systems that can learn and adapt to individual end-users, while still maintaining those user’s privacy?\n\nPrivacy in machine learning has traditionally been examined from two perspectives.\nFirst is data privacy, which concerns safeguarding the sensitive information of individuals represented in the dataset [5, 6, 7, 8].\nSecond is model privacy, which focuses on protecting the learned parameters of a neural network through techniques such as encryption or differentially-private learning [9, 10, 11, 12].\nIn this work, we adopt a third perspective with respect to robot learning: ensuring that a trained, personalized robot does not leak user preference information to other users.\nReturning to our example, privacy in this context means that the robot can be finetuned to learn your preferred way of making a burger while preventing unauthorized users from accessing those preferences even if they have access to the trained model.\nIn practice, this can be difficult to achieve because — if someone has access to the finetuned model — they can roll-out this model and infer the previous user’s preference by watching the robot actions.\nSo how do we safeguard privacy of user preferences?\nOur insight is that:\n\nConcretely, we leverage keys (Figure 1).\nA key is any feature that is unique to the user such as facial structures, vocal patterns, or a textual password.\nWhen finetuning the robot under out approach, a user combines their unique key with the intermediate features of the network and trains it to output their personalized actions.\nThis unique mechanism for personalizing robots safeguards user privacy since preference information remains inaccessible to anyone who does not have the user’s key.\nWithout careful design, keys may unintentionally cause the robot to forget its general-purpose policy.\nBut our technical approach avoids this pitfall — and preserves the initial model architecture — by leveraging the key to perform mathematical operations on the intermediate weights.\nOur proposed mechanism is not tied to a specific network architecture or application as we later demonstrate in our experiments with visual data, imitation learning, MLP classifiers, and reinforcement learning.\nIndeed, as shown in our experiments on robot arms, users can finetune the robot to make their desired hamburger without losing the robot’s previously learned behaviors, and without exposing their preference to other agents.\nWe see this work as a step towards safe and personalized human-robot interaction.\n\nOverall, we make the following contributions:\n\nKey-based Personalization of Robot Policies.\nWe present a formulation for key-based personalization of robot control policies.\nUnder this formalism, the robot learns to personalize to new users’ specifications while retaining its original, general behavior.\nThis formalism is nontrivial to implement in a learning algorithm, since the original and conditional policies operate in different domains, i.e., adding a key as the input requires changing the size of the pre-trained architecture.\nInstead, we use keys to transform the intermediate features of the pre-trained policy, circumventing the need for changing the architecture size.\n\nPersonalized and Private Robot Policies.\nWe present our implementation of the aforementioned key-based personalization with privacy guarantees.\nOur method, PRoP (Personalized and Private Robot Policies) retains the original network architecture, exhibits behavior of the original robot policy for unprivileged users, and personalizes to specific users through a privacy-oriented mechanism.\nImportantly, PRoP extends to arbitrary learning rules and architectures that enables simple, end-to-end training of the model.\n\nReal-world Validation and Empirically Verified Robustness.\nWe empirically test the performance of PRoP in a collection of controlled simulations and real-world studies, including Imitation Learning, Reinforcement Learning, Image Classification, and Task Allocation.\nWe further extend PRoP to more complex settings, such as language prose personalization and key-based obfuscation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在个性化机器人行为的同时保护用户隐私？  \n2. 现有的机器人政策如何防止用户偏好信息泄露？  \n3. 如何在保持原有政策架构的情况下实现个性化？  \n\n【用了什么创新的方案】  \n本研究提出了一种名为PRoP的模型无关框架，通过为每个用户分配一个唯一的密钥来实现个性化和隐私保护。该密钥用于对机器人的网络权重进行数学变换，使得机器人可以根据用户的偏好进行调整，而未授权用户则无法访问这些偏好信息。此外，该方法保留了原始政策的架构和行为，并在多种学习任务中表现优于现有方法。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies",
            "authors": "Jesse Zhang,Marius Memmel,Kevin Kim,Dieter Fox,Jesse Thomason,Fabio Ramos,Erdem Bıyık,Abhishek Gupta,Anqi Li",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "11 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18282",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18282",
            "arxiv_html_link": "https://arxiv.org/html/2509.18282v1",
            "abstract": "Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: (1) end-effector paths specifying what actions to take, and (2) task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4×\\times real-world improvement for a 3D policy trained only in simulation, and 2–3.5×\\times gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need—where, what, and how.\nWebsite at https://peek-robot.github.io.",
            "introduction": "Imagine walking through a crowded store when your child suddenly cries out, “I want the Labubu!”\nThough you’ve never heard the word before, context clues guide your eyes to the fuzzy toy on the shelf, and you effortlessly weave through the crowd to grab it.\nWhat makes this possible is not raw perception ability, but the ability to interpret ambiguous instructions and distill them into just the right cues—where to focus, what actions to take, and how to perform these actions at the low level. Similarly, if given where to focus and what motions to take, a robot manipulation policy should be able to achieve the visual robustness and semantic generalization necessary for open-world deployment by focusing only on how to perform actions.\n\nA common tactic for training manipulation policies is through imitation learning of human-collected robotics data [1, 2, 3, 4], which attempts to learn the where, what, and how all at the same time.\nYet their performance degrades on novel objects, clutter, or semantic variations [5, 6], since the policy alone bears the burden of handling task, semantic, and visual complexity.\nSuch failures often entangle the axes of where, what, and how—for example, grasping a distractor simultaneously reflects misplaced attention, an incorrect object choice, and a wrong motion.\n\nOur key idea is to offload high-level reasoning to vision-language models (VLMs), which can excel at semantic and visual generalization [7, 8], leaving the policy to determine how low-level behavior should be executed. Instead of forcing the policy to directly parse raw images and instructions, a high-level VLM modulates the input representation to the low-level policy by providing: (1) a path that encodes what the policy should do, and (2) masks showing where to attend. By “absorbing” semantic and visual variation, the VLM provides the policy a simplified, annotated “peek” of the scene that gives the what and the where, while the policy only needs to learn how to perform the low-level actions. This intermediate representation helps policy execution inherit many of the VLM’s semantic and visual generalization capabilities. Our VLM-modulated representation is naturally policy-agnostic, allowing it to be applied to arbitrary image-input robot manipulation policies, including state-of-the-art RGB and 3D manipulation policies [9, 1, 3].\n\nTo concretely instantiate this insight into a practical algorithm, we introduce PEEK (Policy-agnostic Extraction of Essential Keypoints), which proposes a unified, point-based intermediate representation that trains VLMs to predict what policies should do and where to focus on. Specifically, we propose to finetune pretrained VLMs [10] to predict a sequence of points corresponding to (1) a path that guides the robot end-effector in what actions to take and (2) a set of task-relevant masking points that show the policy where to focus on (see Figure 1). During low-level visuomotor policy training and inference, we modulate the policy’s image observations by directly drawing these VLM-predicted paths and masks onto the image, allowing the policy to simply focus on how to act, rather than learning all three simultaneously. Doing so significantly bolsters policy generalization, combining the generality of high-level VLM predictions with the precision of low-level policy learning. In this paper, we instantiate a full-stack implementation of PEEK, from devising a scalable data annotation scheme that enables large-scale VLM finetuning on robotic datasets to representation-modulated training of low-level robot policies from simulation and real world data.\n\nIn 535 real-world evaluations across 17 task variations, we demonstrate that PEEK consistently boosts zero-shot policy generalization: a 3D policy (3DDA [9]) trained only in simulation achieves 41.4×\\times higher success in the real world when guided by PEEK, and both large-scale vision-language-action models (π0\\pi_{0} [3]) and small transformer-based policies [1] see 2–3.5×\\times success rate improvements. These results demonstrate the power of using high-level VLMs to absorb task complexity, providing low-level policies with exactly the minimal cues they need for generalizable manipulation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高机器人操作策略的零-shot泛化能力。  \n2. 如何有效地将高层次推理任务分配给视觉-语言模型（VLMs）。  \n3. 如何简化机器人策略的输入表示以增强其性能。  \n\n【用了什么创新的方案】  \nPEEK（Policy-agnostic Extraction of Essential Keypoints）提出了一种统一的基于点的中间表示，旨在通过微调视觉-语言模型（VLMs）来预测机器人操作所需的路径和关注区域。这种表示直接叠加在机器人观察上，使得策略可以专注于如何执行操作，而不是同时学习“在哪里”、“做什么”和“如何做”。通过引入自动标注管道，PEEK能够在多个机器人数据集上生成标注数据，从而提升零-shot泛化能力，显著提高了在真实世界中的成功率。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "A Fast Initialization Method for Neural Network Controllers: A Case Study of Image-based Visual Servoing Control for the multicopter Interception",
            "authors": "Chenxu Ke,Congling Tian,Kaichen Xu,Ye Li,Lingcong Bao",
            "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19110",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19110",
            "arxiv_html_link": "https://arxiv.org/html/2509.19110v1",
            "abstract": "Reinforcement learning-based controller design methods often require substantial data in the initial training phase. Moreover, the training process tends to exhibit strong randomness and slow convergence. It often requires considerable time or high computational resources.\nAnother class of learning-based method incorporates Lyapunov stability theory to obtain a control policy with stability guarantees. However, these methods generally require an initially stable neural network control policy at the beginning of training.\nEvidently, a stable neural network controller can not only serve as an initial policy for reinforcement learning, allowing the training to focus on improving controller performance, but also act as an initial state for learning-based Lyapunov control methods.\nAlthough stable controllers can be designed using traditional control theory, designers still need to have a great deal of control design knowledge to address increasingly complicated control problems.\nThe proposed neural network rapid initialization method in this paper achieves the initial training of the neural network control policy by constructing datasets that conform to the stability conditions based on the system model.\nFurthermore, using the image-based visual servoing control for multicopter interception as a case study, simulations and experiments were conducted to validate the effectiveness and practical performance of the proposed method.\nIn the experiment, the trained control policy attains a final interception velocity of 15 m/s.",
            "introduction": "In recent years, an increasing number of studies have utilized learning-based methods to address control issues[1]. There are two types of learning-based methods for control problem.\nOne is the reinforcement learning (RL) method, and the other is the learning-based Lyapunov control (LLC) method.\nIn contrast to conventional control methods, RL techniques engage with the environment via trial and error to identify optimal strategies and may complete intricate tasks without dependence on exact models of the controlled entities.\nRL, akin to control systems, functions through feedback mechanisms.\nWhile RL largely uses input to refine its decision-making processes, control systems focus on achieving predetermined targets mainly by using static controller techniques during operation.\nThe training process of RL can be unstable and unsafety[2], especially in safety-critical situations like the unmanned aerial vehicle (UAV) visual servoing control.\nIn order to ensure that the trained policy can be applied in practice, the datasets used in the training process should encompass the Region of Interest (RoI)[3; 4], which is exceedingly challenging before obtaining a available control policy.\n\nThe Lyapunov stability method provides a definitive analytical and design framework in control theory, especially for nonlinear systems [5].\nNumerous research studies have recently integrated Lyapunov stability approaches into learning-based control, referred to as Lyapunov function learning, thereby providing formal stability guarantees for deep neural network policies.\nIn the studies [6; 7], the Lyapunov function is utilized as a critic function to assess policies performance.\nIn [8] and [9], Lyapunov functions are integrated into optimization frameworks to guarantee system stability.\nThe Lyapunov stability condition is incorporated into the reward design in [10] and [11].\nReferences [10] and [11] develop the target control policy by incorporating a Lyapunov function into the reward design.\nThe research in [12] proposes learning the Lyapunov function and its derivative (referred to as the D-function) from expert demonstration data while adhering to stability constraints, thus facilitating the development of a control policy that inherently ensures Lyapunov stability.\nNote that uniformly sampled data is necessary for this approach.\nOtherwise, the D-function employed may not accurately represent the actual system model.\nWhile these methods offer formal stability guarantees for the target policy and yield favorable outcomes, they depend on the posteriori expert controllers or trajectories and are not suitable for the original design of the control policy.\n\nThe posteriori expert controllers are also used as the initial policy of RL to circumvent the drawback of slow convergence at the beginning of the training.\nAlthough the conventional control theory can be applied for the controller designing, the rich experience of that is also important to solve a complex control problem.\nTherefore, this paper proposed an initial policy training method that involves constructing datasets that meet stability requirements and then training a neural network control policy based on the datasets.\nMoreover, acquiring a group of datasets without a stable control policy is exceedingly challenging.\nConversely, without a controller, acquiring the model of controlled objects may be more attainable than gathering data.\nUtilizing the datasets produced by mathematical models that adhere to Lyapunov stability, an untrained neural network may be directly developed into a control policy, circumventing the conventional control design and debugging procedures.\nThe trained neural network control policy can be enhanced further by RL methods or the LLC method that needs an initial stable control policy [2].\nIn this paper, the case of image-based visual servoing control for the multicopter interception is adopted to demonstrate the effectiveness of the proposed method.\nIn the experiment, a final flight speed of up to 15 m/s was achieved.\nNoted that the purpose of this method is to obtain a usable control policy with mediocre performance at least, and the optimization of the control policy still needs to be accomplished through RL and LLC methods.\n\nThe paper is organized as follows: Section II outlines the coordinate systems and mathematical models employed. Section III presents the rapid methodology for training neural network policies, applies it to the design of multicopter interception control, and validates the stability of the trained policies by the almost Lyapunov condition[13]. Section IV presents the experimental result of the trained policies implemented on quadrotor platforms.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何快速初始化神经网络控制器以提高训练效率？  \n2. 如何确保在强化学习和Lyapunov控制中使用的控制策略具备稳定性？  \n\n【用了什么创新的方案】  \n提出了一种快速初始化方法，通过构建符合稳定性条件的数据集来实现神经网络控制策略的初始训练。该方法避免了传统控制设计的复杂性，使得未训练的神经网络可以直接发展为控制策略，从而提高训练效率并确保稳定性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Guaranteed Robust Nonlinear MPC via Disturbance Feedback",
            "authors": "Antoine P. Leeman,Johannes Köhler,Melanie N. Zeilinger",
            "subjects": "Optimization and Control (math.OC); Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "Code:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18760",
            "code": "https://github.com/antoineleeman/robust-nonlinear-mpc",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18760",
            "arxiv_html_link": "https://arxiv.org/html/2509.18760v1",
            "abstract": "Robots must satisfy safety-critical state and input constraints despite disturbances and model mismatch.\nWe introduce a robust model predictive control formulation that is fast, scalable, and compatible with real-time implementation.\nOur formulation guarantees robust constraint satisfaction, input-to-state stability and recursive feasibility.\nThe key idea is to decompose the uncertain nonlinear system into (i) a nominal nonlinear dynamic model, (ii) disturbance-feedback controllers, and (iii) bounds on the model error. These components are optimized jointly using sequential convex programming.\nThe resulting convex subproblems are solved efficiently using a recent disturbance-feedback MPC solver.\nThe approach is validated across multiple dynamics, including a rocket-landing problem with steerable thrust. An open-source implementation is available at https://github.com/antoineleeman/robust-nonlinear-mpc.",
            "introduction": "Autonomous robots, whether agile drones, wheeled machines, or (autonomous) spacecrafts, must operate in dynamic and uncertain environments while satisfying strict safety and performance requirements [1].\nIn addition, model mismatch arises naturally due to many factors, such as wind gusts, actuators misalignments, or unmodelled frictions.\nIn robotics applications, disturbances such as wind gusts, actuator misalignments, or unmodeled friction are typically handled by introducing ad hoc safety margins in the control design, resulting in slower motions, reduced maneuverability, and under-utilization of the system’s capabilities.\n\nReinforcement learning, often with domain randomization, has recently shown success in achieving robust sim2real performance[2], particularly in contact-rich tasks. While learned policies can be executed in real time, training requires extensive offline computation, careful reward design, and heuristics to ensure convergence.\n\nIn contrast, trajectory-optimization methods enforce constraint satisfaction for nonlinear dynamics and are widely used in practice as a model-based control technique based on sequential convex programming [3].\nHowever, these methods typically do not ensure safety or stability in the presence of disturbances, which is critical for real-world deployment.\n\nIn this paper, we introduce a scalable robust model predictive control formulation for nonlinear systems that is safe-by-design.\nRobust model predictive control commonly accounts for disturbances by predicting a set containing all possible future states [4].\nTo reduce conservatism, these robust predictions are based on closed-loop predictions and a corresponding feedback law is typically optimized offline, e.g., using contraction metrics [5, 6].\nHowever, fixing the feedback a priori can limit closed-loop performance and the offline computations also limit scalability.\nRobust model predictive control approaches that optimize feedback laws to reduce conservatism have been proposed in [7, 8], which rely on (conservative) sequential over-approximations of the robust predictions.\nIn contrast, the disturbance feedback MPC [9] framework framework (also known as system level synthesis [10]) provides an exact characterization of the robust prediction for linear time-varying systems, thereby avoiding this compounding effect. Recent extensions [11, 12] further enable its application to nonlinear systems.\nWhile these formulations improve performance compared to fixed policy approaches, they generally do not provide guarantees of recursive feasibility or stability. However, such guarantees are crucial, since loss of feasibility at any step can cause safety constraint violations.\nContribution: Building on the nonlinear SLS formulation in [11],\nwe propose a fast and scalable robust model predictive control formulation for nonlinear systems with robust closed-loop guarantees.\nOur approach jointly optimizes the nominal nonlinear trajectory, a disturbance-feedback controller, and an upper bound on the prediction error.\n\nFormal guarantees are provided, i.e., robust constraint satisfaction, recursive feasibility (Thm. 1), and input-to-state stability (Thm. 2). Recursive feasibility is ensured by a novel treatment of the mismatch with respect to the nonlinear nominal prediction.\n\nFormal guarantees are provided, i.e., robust constraint satisfaction, recursive feasibility (Thm. 1), and input-to-state stability (Thm. 2). Recursive feasibility is ensured by a novel treatment of the mismatch with respect to the nonlinear nominal prediction.\n\nAn efficient sequential convex programming algorithm tailored to the robust MPC formulation is provided to enable real-time deployment. Each iteration consists of solving a nominal trajectory optimization with a quadratic program, updating a disturbance-feedback controller via Riccati recursions, and evaluating Jacobians of the nonlinear dynamics. The design is general and the provided code can be directly applied to systems with large state and input dimensions and long prediction horizons.\n\nReal-time feasibility (computation times) is demonstrated across different dynamics, including a quadcopter and a rocket landing. Robust performance is validated on the rocket-landing problem with steerable thrust including actuator dynamics, illustrated in Fig. 1, demonstrating robust constraint satisfaction with an average total latency of 19.7 [ms] per iteration.\nA comparison to a soft-constrained MPC baseline highlights increased safety and stability of the proposed approach.\n\nNotation:\nFor vectors or matrices aa and bb with the same number of rows, we denote their horizontal concatenation by [a,b][a,~b].\nWe denote stacked vectors or matrices by (a,b)=[a⊤,b⊤]⊤\\left(a,b\\right)=[a^{\\top},~b^{\\top}]^{\\top}. For a vector r∈ℝnr\\in\\mathbb{R}^{n}, we denote its ithi^{\\text{th}} component by rir_{i}.\nFor a sequence of matrices Mk,j∈ℝp×qM_{k,j}\\in\\mathbb{R}^{p\\times q}, indexed by k>j≥0k>j\\geq 0,\nwe define the shorthand horizontal concatenation M(k):=[Mk,k−1,Mk,k−2,…,Mk,0]∈ℝp×k​q.M_{(k)}\\vcentcolon=[M_{k,k-1},~M_{k,k-2},~\\dots,~M_{k,0}]\\in\\mathbb{R}^{p\\times kq}.\nFor a vector v∈ℝnv\\in\\mathbb{R}^{n}, we write its 1-norm as ‖v‖1=|v1|+…+|vn|\\|v\\|_{1}=|v_{1}|+\\ldots+|v_{n}| and its infinity norm as ‖v‖∞=maxi=1,…,n⁡|vi|\\|v\\|_{\\infty}=\\max_{i=1,\\ldots,n}|v_{i}|.\nFor a matrix M∈ℝm×nM\\in\\mathbb{R}^{m\\times n}, the matrix infinity norm is ‖M‖∞=maxi​∑j|Mi​j|\\|M\\|_{\\infty}=\\max_{i}\\sum_{j}|M_{ij}|.\nWe denote sets with calligraphic letters, e.g., 𝒲⊆ℝn\\mathcal{W}\\subseteq\\mathbb{R}^{n}.\nLet ℬm\\mathcal{B}^{m} be the unit ball defined by ℬm:={d∈ℝm|‖d‖∞≤1}\\mathcal{B}^{m}\\vcentcolon=\\{d\\in\\mathbb{R}^{m}|~\\|d\\|_{\\infty}\\leq 1\\}. The Minkowski sum of two sets 𝒜,𝒟⊆ℝn\\mathcal{A},\\mathcal{D}\\subseteq\\mathbb{R}^{n} is defined as 𝒜⊕𝒟:={a+d|a∈𝒜,d∈𝒟}\\mathcal{A}\\oplus\\mathcal{D}\\vcentcolon=\\{a+d|a\\in\\mathcal{A},d\\in\\mathcal{D}\\}.\nFor a vector-valued function f:ℝn→ℝqf:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{q}, we denote the Jacobian by ∂ϕ∂x|x∈ℝq×n\\frac{\\partial\\phi}{\\partial x}|_{x}\\in\\mathbb{R}^{q\\times n}. For the ithi^{\\text{th}} component (f)i(f)_{i}, we denote its Hessian by\n∂2(f)i∂x2|x∈ℝn×n\\frac{\\partial^{2}(f)_{i}}{\\partial x^{2}}|_{x}\\;\\in\\;\\mathbb{R}^{n\\times n}.\n\n1. Formal guarantees are provided, i.e., robust constraint satisfaction, recursive feasibility (Thm. 1), and input-to-state stability (Thm. 2). Recursive feasibility is ensured by a novel treatment of the mismatch with respect to the nonlinear nominal prediction.\n\n2. An efficient sequential convex programming algorithm tailored to the robust MPC formulation is provided to enable real-time deployment. Each iteration consists of solving a nominal trajectory optimization with a quadratic program, updating a disturbance-feedback controller via Riccati recursions, and evaluating Jacobians of the nonlinear dynamics. The design is general and the provided code can be directly applied to systems with large state and input dimensions and long prediction horizons.\n\n3. Real-time feasibility (computation times) is demonstrated across different dynamics, including a quadcopter and a rocket landing. Robust performance is validated on the rocket-landing problem with steerable thrust including actuator dynamics, illustrated in Fig. 1, demonstrating robust constraint satisfaction with an average total latency of 19.7 [ms] per iteration.\nA comparison to a soft-constrained MPC baseline highlights increased safety and stability of the proposed approach.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在存在扰动和模型不匹配的情况下确保机器人控制的安全性和稳定性。  \n2. 如何实现快速、可扩展的非线性模型预测控制，以满足实时实施的需求。  \n\n【用了什么创新的方案】  \n提出了一种鲁棒模型预测控制（MPC）方法，通过将不确定的非线性系统分解为名义非线性动态模型、扰动反馈控制器和模型误差界限，联合优化这些组件以确保鲁棒约束满足、输入到状态的稳定性和递归可行性。采用高效的顺序凸编程算法实现实时部署，验证了在不同动态下的实时可行性，特别是在火箭着陆问题中展示了鲁棒性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "An Extended Kalman Filter for Systems with Infinite-Dimensional Measurements",
            "authors": "Maxwell M. Varley,Timothy L. Molloy,Girish N. Nair",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO)",
            "comment": "8 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18749",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18749",
            "arxiv_html_link": "https://arxiv.org/html/2509.18749v1",
            "abstract": "This article examines state estimation in discrete-time nonlinear stochastic systems with finite-dimensional states and infinite-dimensional measurements, motivated by real-world applications such as vision-based localization and tracking.\nWe develop an extended Kalman filter (EKF) for real-time state estimation, with the measurement noise\nmodeled as an infinite-dimensional random field.\nWhen applied to vision-based state estimation, the measurement Jacobians required to implement the EKF are shown to correspond to image gradients.\nThis result provides a novel system-theoretic justification for the use of image gradients as features for vision-based state estimation, contrasting with their (often heuristic) introduction in many computer-vision pipelines.\nWe demonstrate the practical utility of the EKF on a public real-world dataset involving the localization of an aerial drone using video from a downward-facing monocular camera.\nThe EKF is shown to outperform VINS-MONO, an established visual-inertial odometry algorithm, in some cases achieving mean squared error reductions of up to an order of magnitude.",
            "introduction": "In this paper we focus on state estimation for systems with finite-dimensional states and infinite-dimensional measurements.\nThis focus is motivated by vision-based state estimation, control, and localization problems that arise across robotics [1, 2, 3] and control [4, 5, 6].\nIn such problems, the measurements take the form of images with dimensions determined by the camera’s resolution (i.e., number of pixels), while the underlying state of interest (e.g., position and orientation) is typically relatively low-dimensional.\nWith modern cameras offering increasingly high-resolution images, the emergent challenge in many of these problems is how best to estimate a low-dimensional state with arbitrarily high-dimensional measurements.\nTraditional approaches from computer vision and robotics for processing high-dimensional measurements rely on (spatial) feature extraction [3].\nHowever, such approaches may fail to exploit the dynamics and uncertainty of the state estimates in determining which features to extract or how to weight them in computing a state estimate.\nIn this paper, we therefore take a different approach by formulating an extended Kalman filter (EKF) capable of processing an entire (infinite-dimensional) image domain, and assigning dynamic weights (via gains) to every pixel based on its contribution to state estimates.\n\nAlthough Kalman filters and their nonlinear variants, such as the EKF, have long been used for estimation in robotics and control, the vast majority of vision-based filters are feature-based [7, 8, 6, 1, 3].\nThe use of these filters thus typically involves first reducing images to a sparse set of extracted keypoints before applying standard Kalman filter techniques.\nIn contrast, our novel EKF is capable of operating directly on dense image data (i.e., pixel intensities) directly in real-time, avoiding the need to extract features, and preserving the image structure.\nA key insight that allows the formulation of our filter, is the modeling of the measurement noise as an infinite-dimensional random field. This allows us to construct a continuous image-domain measurement model that naturally integrates with the structure of an infinite-dimensional EKF. In doing so, we are able to derive a system-theoretic justification for using image gradients in the filter update step. These image gradients are usually introduced heuristically in computer-vision pipelines (cf. [3]), but here emerge from the principles of the filter design itself.\n\nA number of different approaches have been employed in early works to derive the Kalman filter for distributed parameter systems [9, 10, 11, 12], although none of these works examined systems with finite-dimensional states and infinite-dimensional measurements as presented here. A survey contextualizing the methods and results of these early derivations is given in [13] and a modern, comprehensive examination of control and estimation of distributed parameter systems is given in [14]. In the case of nonlinear distributed parameter systems, the EKF is generally utilized either by reducing the dimensionality of the underlying system before designing the estimator (the early lumping approach) [15], or designing an infinite-dimensional distributed parameter EKF and using some discretization scheme for real-world implementation (the late lumping approach) [16].\n\nThis article makes the following key contributions, extending the optimal linear filter work in [17, 18].\nFirstly, the optimal linear filter originally derived in that prior work is generalized to construct an EKF for systems with finite-dimensional states and infinite-dimensional measurements, and with both nonlinear state dynamics and nonlinear measurement equations.\nWe provide a derivation of this EKF and establish and interpret the measurement Jacobians that arise within it, with the latter relating to image gradients in the case of image measurements.\nWe verify the efficacy of the EKF for vision-based state estimation on a real-world dataset.\nSpecifically, the filter estimates the position, velocity, acceleration, and yaw of an aerial drone equipped with an Inertial Measurement Unit (IMU) as well as optical cameras providing measurements in the form of grayscale downward-facing images. The estimates are evaluated against the ground truth included in the dataset, and the results are compared with the performance of the well-established monocular visual-inertial odometry algorithm VINS-MONO [2], showing that our filter generally achieves superior or comparable state estimation performance.\n\nThis article is structured as follows. Section II presents the notational definitions used throughout this work. Section III will define the system model that we will analyze, as well as the assumptions used throughout. Section IV will describe a linearization of the previously described system model, in preparation for an application of the optimal linear filter derived in [18] to this linearized system. Section V will present the filter procedure and two results, Proposition V.1 and Proposition V.2, which help to simplify the implementation and reduce the computational complexity of our algorithm. Section VI will give details pertaining to the dataset, as well as how the filter is implemented on the data within, and our chosen measures of filter performance. Using these performance metrics, Section VII demonstrates an empirical verification of the proposed filter and compares the results with those of VINS-MONO.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在有限维状态和无限维测量的系统中进行状态估计。  \n2. 传统特征提取方法在处理高维测量时的局限性。  \n3. 如何实时处理整个图像域以提高状态估计的准确性。  \n\n【用了什么创新的方案】  \n本文提出了一种扩展卡尔曼滤波器（EKF），能够直接在密集图像数据上进行实时状态估计，而无需提取特征。通过将测量噪声建模为无限维随机场，EKF能够动态地为每个像素分配权重，从而更好地利用图像梯度进行状态更新。这种方法提供了对图像梯度作为特征使用的系统理论依据，并在实际应用中表现出优于现有视觉惯性里程计算法的性能。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Dual Iterative Learning Control for Multiple-Input Multiple-Output Dynamics with Validation in Robotic Systems",
            "authors": "Jan-Hendrik Ewering,Alessandro Papa,Simon F.G. Ehlers,Thomas Seel,Michael Meindl",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO)",
            "comment": "11 pages, 4 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.18723",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18723",
            "arxiv_html_link": "https://arxiv.org/html/2509.18723v1",
            "abstract": "Solving motion tasks autonomously and accurately is a core ability for intelligent real-world systems.\nTo achieve genuine autonomy across multiple systems and tasks, key challenges include coping with unknown dynamics and overcoming the need for manual parameter tuning, which is especially crucial in complex  Multiple-Input Multiple-Output (MIMO) systems.",
            "introduction": "Accurate reference tracking is a critical control capability for a wide range of real-world applications, from industrial manufacturing to service robotics and biomedical systems [1, 2, 3], which often involve complex MIMO system dynamics.\nFor these systems to be effective and user-friendly, they must be capable of self-reliantly adapting to new tasks and environments. In other words, it is required to learn to perform reference tracking autonomously.\nThis capability is crucial to eliminate the need for expert-provided model information or time-consuming manual tuning.\n\nIn repetitive settings,  Iterative Learning Control (ILC) is an established method that enables highly accurate reference tracking, given a reference trajectory [4, 5, 6].\nHowever, satisfactory learning performance is almost always dependent on human expert knowledge, such as access to model information or the manual tuning of algorithmic (hyper) parameters.\nThe necessary manual tuning effort is typically even more aggravated in MIMO systems due to cross-coupling effects and scale variations between different inputs and outputs [7].\nMoreover, learning performance is often system- or reference-specific, which hinders genuine autonomous deployment across numerous systems and tasks.\n\nHence, an ILC method must possess the following three characteristics in order to enable real-world systems to autonomously learn to solve reference tracking tasks.\nFirst, the ILC method must neither require prior model information nor the manual tuning of parameters to enable autonomous application.\nSecond, the ILC method has to be applicable to MIMO dynamics, as these are often present in realistic settings.\nAnd third, the ILC method should be validated – ideally on multiple – real-world systems.\nBased on these criteria, we continue to review the state of research in ILC.\n\nFirst, there exists an extensive class of so-called  Model-Based Iterative Learning Control (MB-ILC) methods that can yield remarkable reference tracking performance in various real-world applications.\nFor example,  Norm-Optimal Iterative Learning Control (NO-ILC) schemes have been applied to gantry robots [8] and stroke rehabilitation [9], and NO-ILC can readily be applied to MIMO dynamics [10, 11].\nSimilarly,  Frequency-Domain Iterative Learning Control (FD-ILC) has been successfully applied to real-world systems with MIMO dynamics such as a marine vibrator [12] or nano-positioning systems [13].\nDespite these achievements, the aforementioned and other MB-ILC methods are limited because they require prior model information and typically involve manual tuning of learning parameters, which hinders their autonomous application.\n\nTo overcome the need for prior model information, so-called  Data-Driven Iterative Learning Control (DD-ILC) methods have been developed.\nA common approach is to use the input/output trajectory pairs from previous trials to estimate the gradient of the tracking errors with respect to the input trajectory to update the latter [14, 15, 16].\nOn the other hand, several DD-ILC schemes use experimental data to estimate a model of the plant dynamics and combine this plant approximation with well-known MB-ILC methods [11, 17].\nFor instance, some approaches use  Recursive Least Squares (RLS) to estimate a model of the system dynamics and combine it with NO-ILC or adaptive ILC [18, 19, 20].\nOther approaches combine  Iterative Learning Identification (ILI) and ILC [21, 22] to iteratively learn a model and input trajectory without prior model information.\nFD-ILC has been combined with an iterative learning approach for MIMO dynamics using a pseudo-inversion approach [23].\nNotably, there are approaches that iteratively learn a dynamic linearization of the plant dynamics, which can be utilized in a NO-ILC update law [24, 25, 26, 27, 28].\nWhat is common among all of these methods is that they overcome the need for prior model information, and many of them are applicable to MIMO dynamics.\nHowever, most of these methods have not been validated in real-world experiments, and all of these methods require the manual tuning of learning parameters, which precludes the autonomous application of the learning methods.\n\nTo overcome the need for prior model information and manual parameter tuning, MB-ILC has been combined with repeated model learning using Gaussian processes and self-parametrization schemes [29, 30, 31], and some of the methods have been validated on different real-world systems [29].\nHowever, these approaches are limited in terms of their applicability and validation in MIMO dynamics.\nWe, hence, conclude that there is no DD-ILC method that is autonomous in the sense that it neither requires prior model information nor manual parameter tuning, is applicable to MIMO dynamics, and has been validated on multiple—possibly real-world—systems.\n\nTo address these three issues, we propose a novel MIMO DILC framework that builds on previous results [32] and enables autonomous learning of reference tracking tasks in real-world systems with MIMO dynamics.\nSpecifically, the contributions of this paper are threefold:\n\nFirst, a novel DILC scheme for simultaneous model and control learning in MIMO systems, while requiring neither prior model information nor manual parameter tuning. It exploits a novel iterative learning paradigm that generalizes ILC approaches for iterative model learning, thus enabling the learning of system models using established ILC methods. The algorithmic architecture is illustrated in Figure 1.\n\nSecond, a theoretical analysis providing convergence conditions of the proposed algorithm under mild assumptions. We emphasize that iterative model learning in complex MIMO systems poses significant challenges, such as an overparametrized model, for which we present novel analysis to prove convergence.\n\nThird, an extensive empirical validation with two real-world MIMO systems and a six-degree-of-freedom industrial robot simulation. We demonstrate, in contrast to the vast majority of existing works, the truly autonomous learning capabilities of DILC without any model information or human tuning effort. To the best of our knowledge, this is the first time that a DD-ILC method has solved different reference tracking tasks in multiple real-world systems with MIMO dynamics, without requiring prior model information or manual parameter tuning. We highlight that DILC solves many reference tracking tasks within 1010-2020 trials and learns even complex motions in less than 100100 iterations.\n\nThis paper is structured as follows.\nWe formally define the considered problem in Section II and introduce preliminaries on ILC in Section III.\nThe proposed method and its theoretical properties are detailed in Section IV.\nThe simulative and experimental results are presented in Section V.\nFinally, we conclude the paper in Section VI.\n\nNotation: We denote the set of real numbers by ℝ\\mathbb{R}, the set of natural numbers by ℕ\\mathbb{N}, the set of all natural numbers greater than or equal to a∈ℕa\\in\\mathbb{N} by ℕ≥a\\mathbb{N}_{\\geq a}, and the set of natural numbers in the interval [a,b]⊂ℕ[a,b]\\subset\\mathbb{N} by ℕ[a,b]\\mathbb{N}_{[a,b]}.\nWe denote vectors (matrices) by lower-case (upper-case) letters in bold, e. g., 𝐯∈ℝN\\mathbf{v}\\in\\mathbb{R}^{N} (𝐀∈ℝN×N\\mathbf{A}\\in\\mathbb{R}^{N\\times N}).\nIf not explicitly stated, all vectors are column vectors, and by writing [𝐯]i[\\mathbf{v}]_{i}, we refer to the ii-th entry of 𝐯\\mathbf{v}.\nBy writing [𝐀]i,j[\\mathbf{A}]_{i,j}, we refer to the ii-th entry of the jj-th column of 𝐀\\mathbf{A}.\nTo vectorize 𝐀\\mathbf{A}, we write vec​(𝐀)\\mathrm{vec}(\\mathbf{A}).\nThe Euclidean norm of a vector 𝐯\\mathbf{v} is denoted by ‖𝐯‖\\left\\lVert\\mathbf{v}\\right\\rVert, and the induced Euclidean norm of a matrix 𝐀\\mathbf{A} is denoted by ‖𝐀‖\\left\\lVert\\mathbf{A}\\right\\rVert.\nThe weighted norm with respect to a positive definite matrix 𝐖≻0\\mathbf{W}\\succ 0 with 𝐖=𝐖⊤\\mathbf{W}=\\mathbf{W}^{\\top} is denoted by ‖𝐯‖𝐖=𝐯⊤​𝐖𝐯\\left\\lVert\\mathbf{v}\\right\\rVert_{\\mathbf{W}}=\\sqrt{\\mathbf{v}^{\\top}\\mathbf{W}\\mathbf{v}}.\nWe denote the identity matrix of size N×NN\\times N by 𝐈N\\mathbf{I}_{N}, and the zero matrix of suitable dimension by 𝟎\\mathbf{0}.\nThe Kronecker product of two matrices 𝐀\\mathbf{A} and 𝐁\\mathbf{B} is 𝐀⊗𝐁\\mathbf{A}\\otimes\\mathbf{B}.\nWe recall that a function α:ℝ≥0→ℝ≥0\\alpha:\\mathbb{R}_{\\geq 0}\\rightarrow\\mathbb{R}_{\\geq 0} is of class 𝒦\\mathscr{K} if it is continuous, strictly increasing, and satisfies α​(0)=0\\alpha(0)=0.\nBy ℒ\\mathscr{L}, we refer to the class of functions θ:ℝ≥0→ℝ≥0\\theta:\\mathbb{R}_{\\geq 0}\\rightarrow\\mathbb{R}_{\\geq 0} that are continuous, non-increasing, and satisfy lims→∞θ​(s)=0\\lim_{s\\rightarrow\\infty}\\theta(s)=0, and by 𝒦​ℒ\\mathscr{K}\\negthinspace\\negthinspace\\mathscr{L} to the class of functions β:ℝ≥0×ℝ≥0→ℝ≥0\\beta:\\mathbb{R}_{\\geq 0}\\times\\mathbb{R}_{\\geq 0}\\rightarrow\\mathbb{R}_{\\geq 0} with β​(⋅,s)∈𝒦\\beta(\\cdot,s)\\in\\mathscr{K} and β​(r,⋅)∈ℒ\\beta(r,\\cdot)\\in\\mathscr{L} for any fixed s∈ℝ≥0s\\in\\mathbb{R}_{\\geq 0} and r∈ℝ≥0r\\in\\mathbb{R}_{\\geq 0}, respectively.\nLast, we denote the space of all block-lower-triangular Toeplitz matrices of dimension NN with sub-matrices, ∀n∈ℕ[1,N]\\forall n\\in\\mathbb{N}_{[1,N]}, 𝐓¯n∈ℝL×M\\bar{\\mathbf{T}}_{n}\\in\\mathbb{R}^{L\\times M}, by 𝒯NL,M\\mathcal{T}^{L,M}_{N}, that is,\n\n1. First, a novel DILC scheme for simultaneous model and control learning in MIMO systems, while requiring neither prior model information nor manual parameter tuning. It exploits a novel iterative learning paradigm that generalizes ILC approaches for iterative model learning, thus enabling the learning of system models using established ILC methods. The algorithmic architecture is illustrated in Figure 1.\n\n2. Second, a theoretical analysis providing convergence conditions of the proposed algorithm under mild assumptions. We emphasize that iterative model learning in complex MIMO systems poses significant challenges, such as an overparametrized model, for which we present novel analysis to prove convergence.\n\n3. Third, an extensive empirical validation with two real-world MIMO systems and a six-degree-of-freedom industrial robot simulation. We demonstrate, in contrast to the vast majority of existing works, the truly autonomous learning capabilities of DILC without any model information or human tuning effort. To the best of our knowledge, this is the first time that a DD-ILC method has solved different reference tracking tasks in multiple real-world systems with MIMO dynamics, without requiring prior model information or manual parameter tuning. We highlight that DILC solves many reference tracking tasks within 1010-2020 trials and learns even complex motions in less than 100100 iterations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂的MIMO系统中实现自主的参考跟踪任务？  \n2. 如何消除对先前模型信息和手动参数调整的需求？  \n3. 如何在多个真实世界系统中验证学习方法的有效性？  \n\n【用了什么创新的方案】  \n提出了一种新的MIMO DILC框架，能够在不需要先前模型信息或手动参数调整的情况下，实现对参考跟踪任务的自主学习。该框架结合了迭代学习控制（ILC）方法，提供了理论分析以证明算法的收敛性，并通过在两个真实世界的MIMO系统和一个六自由度工业机器人仿真中的广泛验证，展示了其真正的自主学习能力。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction",
            "authors": "Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO); Image and Video Processing (eess.IV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18566",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18566",
            "arxiv_html_link": "https://arxiv.org/html/2509.18566v1",
            "abstract": "Reconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond temporal resolution, making them a superior sensing choice for dynamic human reconstruction. Accordingly, we present a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. Specifically, a unified set of 3D Gaussians carries a learnable semantic attribute; only Gaussians classified as human undergo deformation for animation, while scene Gaussians stay static. To combat blur, we propose an event-guided loss that matches simulated brightness changes between consecutive renderings with the event stream, improving local fidelity in fast-moving regions. Our approach removes the need for external human masks and simplifies managing separate Gaussian sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.",
            "introduction": "Human reconstruction from monocular videos is a critical task in computer vision and graphics, with applications spanning virtual reality [1], augmented reality [2], and film production [3]. Recent neural rendering advancements, including Neural Radiance Fields (NeRFs)[4] and 3D Gaussian Splatting (3DGS)[5], enable highly-fidelity, photorealistic 3D reconstruction. Building on this, various 3D human reconstruction methods have emerged. Examples include 3DGS-Avatar [6] and ASH [7], which focus on animatable avatars, and HUGS [8], which reconstructs human and scene simultaneously using separate Gaussian sets.\n\nDespite these promising results, existing methods still face significant challenges.\nFirst, most approaches require an external human mask, necessitating a prior segmentation step that can introduce artifacts.\nSecond, rapid human motion in frame-based camera captures often leads to motion blur, deteriorating image quality.\nWhile some methods attempt to deblur RGB images or integrate event data for reconstruction, their generalizability is limited.\nExFMan [9] is a notable exception that leverages event data for dynamic human reconstruction but lacks static scene modeling.\n\nTo address these challenges, we introduce a unified framework for reconstructing animatable humans and static scenes from a monocular event camera (Fig. 1).\nUnlike HUGS [8], which uses separate Gaussian sets, our method encodes both human and scene in a single set of 3D Gaussians with semantic attributes, refined during training via rendering feedback.\nFurthermore, synthetic events generated from rendered images are aligned with real event streams, providing supervision that alleviates motion blur.\n\nWe evaluate our method on two newly created datasets, ZJU-MoCap-Blur and MMHPSD-Blur, generated by simulating motion blur to test performance under challenging conditions.\nExperiments show that our unified human-scene reconstruction framework surpasses the state-of-the-art HUGS [8], with notable gains on ZJU-MoCap-Blur: +19.5%19.5\\% PSNR, +3.95%3.95\\% SSIM, and –32.5%32.5\\% LPIPS.\nIn summary, our main contributions are:\n\nA novel framework for unified human and scene reconstruction using a single semantically attributed set of 3D Gaussians.\n\nThe integration of event data to mitigate motion blur and enhance the reconstruction quality of fast-moving subjects.\n\nAn extensive evaluation on self-generated motion-blurred datasets that demonstrates state-of-the-art performance in challenging high-speed scenarios.\n\n1. A novel framework for unified human and scene reconstruction using a single semantically attributed set of 3D Gaussians.\n\n2. The integration of event data to mitigate motion blur and enhance the reconstruction quality of fast-moving subjects.\n\n3. An extensive evaluation on self-generated motion-blurred datasets that demonstrates state-of-the-art performance in challenging high-speed scenarios.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何从单目视频中重建动态人类与静态场景，尤其是在快速运动下。  \n2. 现有方法依赖外部人类掩码，增加了处理复杂性与潜在伪影。  \n3. RGB帧在快速运动中容易产生运动模糊，影响重建质量。  \n\n【用了什么创新的方案】  \n提出了一种新颖的事件引导的人类-场景重建框架，通过单一的3D高斯点集联合建模动态人类和静态场景。该方法利用事件数据来减轻运动模糊，并通过事件引导损失提高快速运动区域的重建质量。此外，框架简化了高斯点集的管理，消除了对外部人类掩码的需求。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Policy Gradient with Self-Attention for Model-Free Distributed Nonlinear Multi-Agent Games",
            "authors": "Eduardo Sebastián,Maitrayee Keskar,Eeman Iqbal,Eduardo Montijano,Carlos Sagüés,Nikolay Atanasov",
            "subjects": "Systems and Control (eess.SY); Multiagent Systems (cs.MA); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18371",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18371",
            "arxiv_html_link": "https://arxiv.org/html/2509.18371v1",
            "abstract": "Multi-agent games in dynamic nonlinear settings are challenging due to the time-varying interactions among the agents and the non-stationarity of the (potential) Nash equilibria. In this paper we consider model-free games, where agent transitions and costs are observed without knowledge of the transition and cost functions that generate them. We propose a policy gradient approach to learn distributed policies that follow the communication structure in multi-team games, with multiple agents per team. Our formulation is inspired by the structure of distributed policies in linear quadratic games, which take the form of time-varying linear feedback gains. In the nonlinear case, we model the policies as nonlinear feedback gains, parameterized by self-attention layers to account for the time-varying multi-agent communication topology. We demonstrate that our distributed policy gradient approach achieves strong performance in several settings, including distributed linear and nonlinear regulation, and simulated and real multi-robot pursuit-and-evasion games.",
            "introduction": "Multi-robot problems encompass a variety of expected behaviors [1, 2, 3, 4], including cooperative, conflicting or competitive actions. For instance, in a perimeter-defense setting [5, 6], multiple teams must coordinate to effectively defend a region from potential attackers (Fig. 1). These kinds of problems can be formulated as multi-team dynamic games [7, 8, 9, 10, 11], where each multi-agent team is viewed as a player with specific goals and constraints, and where agents interact with teammates (intra-team interactions) and agents on other teams (inter-team interactions). These settings are typically nonlinear and dynamic, requiring complex interactions that evolve with time as a function of how the agents play the game. These challenging features are specially relevant when we seek distributed policies subject to the communication constraints imposed by the topology of the teams; and in the absence of a mathematical description of the game dynamics and costs, demanding model-free approaches that only rely on transition and cost samples to assess the performance of the teams. Inspired by distributed policies in linear quadratic games, we present a novel policy gradient approach to learn distributed policies for nonlinear dynamic games that are both effective and scalable.\n\nModel-based methods for nonlinear multi-agent dynamic games rely on iterative linearization of the system dynamics and quadratic approximation of the game cost [12, 13, 14]. This allows for fast computation with guarantees of convergence to a saddle configuration [15] but imposes a centralized calculation that limits the applicability in distributed settings, where the agent communication is restricted according to a graph topology. To overcome such limitations, it is possible to restrict the class of nonlinear multi-agent dynamic games to potential games [16, 17, 18], where it is assumed that a potential function exists such that the relative incentives in modifying one agent’s policy is equal to the difference in value of the potential function. Under this constraint, it is possible to derive algorithms that compute open-loop optimal trajectories for the agents under centralized [19] or distributed topological constraints [20, 17]. However, open-loop policies lack robustness and require knowledge on how the multi-agent topology will evolve with time. In contrast, we propose a novel policy parameterization that is distributed by construction and does not require network topology prediction. In all previous cases, a model of the system dynamics and the structure of the cost function is needed to compute the actions. Instead, to address general nonlinear multi-agent dynamic games, we propose a model-free policy gradient approach that relies only on transition and cost samples.\n\nModel-free solutions for games are limited due to the non-stationarity of the Nash equilibria (if one exists) [21, 22]. Traditional approaches either focus on providing theoretical guarantees of convergence or addressing practical settings assuming the existence of such Nash equilibria. An instance of the former is [23], where distributed linear quadratic regulators are learned assuming that the sequence of graphs representing the communication structure of the game is known. From a different perspective, when the linear cost function is known and the strategies of all players are available, the problem can be posed as a multi-team distributed optimization program [24, 25]. In practical settings, existing solutions rely on multi-agent reinforcement learning algorithms [26] that consider independent heterogeneous agents to apply policy gradient methods [27, 28, 29, 30]. In this work, we bring together the benefits of both alternatives by proposing a self-attention-based policy parameterization built from first principles and which enforces distributed execution constraints. The distributed policy is trained using a policy gradient learning method that considers, simultaneously, the policies of all the agents from all different teams, addressing the non-stationarity of the game in practice.\n\nOur main contribution is a policy gradient method for learning distributed policies in model-free nonlinear multi-agent dynamic games (Sec. II). Our approach uses a nonlinear feedback gain formulation of the agent policies, parameterized using self-attention layers (Sec. III). The use of self-attention enables to enforce intra- and inter-team graph constraints, handling time-varying communication and achieving invariance with respect to the total number of agents. Furthermore, a neural network parameterization of the policies motivates the use of a policy gradient method to learn effective and scalable policies in the model-free setting.\nThe method also allows to learn heterogeneous policies per team, such that the teams adjust to specific goals. We demonstrate that our method applies broadly, from linear quadratic settings under topology constraints to multi-agent reinforcement learning in competitive games with simulated and real robots (Sec. IV).",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在动态非线性多智能体游戏中学习分布式策略。  \n2. 如何处理多智能体之间的时间变化通信拓扑。  \n3. 如何在缺乏系统动态和成本函数模型的情况下实现模型无关的学习。  \n\n【用了什么创新的方案】  \n提出了一种基于自注意力机制的策略梯度方法，用于学习动态非线性多智能体游戏中的分布式策略。该方法通过非线性反馈增益的参数化，能够处理时间变化的多智能体通信结构，并且不需要对网络拓扑进行预测。通过使用神经网络参数化，方法能够有效地学习异质策略，适应不同团队的具体目标，并在多种设置中展示了良好的性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata",
            "authors": "Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "Accepted at NeurIPS 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18350",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18350",
            "arxiv_html_link": "https://arxiv.org/html/2509.18350v1",
            "abstract": "Accurate visual localization from aerial views is a fundamental problem with applications in mapping, large-area inspection, and search-and-rescue operations. In many scenarios, these systems require high-precision localization while operating with limited resources (e.g., no internet connection or GNSS/GPS support), making large image databases or heavy 3D models impractical. Surprisingly, little attention has been given to leveraging orthographic geodata as an alternative paradigm, which is lightweight and increasingly available through free releases by governmental authorities (e.g., the European Union). To fill this gap, we propose OrthoLoC, the first large-scale dataset comprising 16,425 UAV images from Germany and the United States with multiple modalities. The dataset addresses domain shifts between UAV imagery and geospatial data. Its paired structure enables fair benchmarking of existing solutions by decoupling image retrieval from feature matching, allowing isolated evaluation of localization and calibration performance. Through comprehensive evaluation, we examine the impact of domain shifts, data resolutions, and covisibility on localization accuracy. Finally, we introduce a refinement technique called AdHoP, which can be integrated with any feature matcher, improving matching by up to 95% and reducing translation error by up to 63%. The dataset and code are available at: https://deepscenario.github.io/OrthoLoC.",
            "introduction": "Visual localization for Unmanned Aerial Vehicles is essential for digital-twin modeling [60, 74], surveillance [29], search-and-rescue [51], and infrastructure inspection [34], yet faces unique challenges not addressed by ground-level localization systems. While ground-level approaches [56, 71, 70] benefit from similar viewpoints between images [59, 49, 57], aerial applications encounter dramatic perspective differences and require scalability over large areas [69, 72].\n\nCurrent UAV localization algorithms rely on retrieving the closest match from a database of posed images [72, 77], which is inaccurate, or on 3D models of the scene [69, 66], which are memory and computationally expensive. In limited resources settings, as it is often the case for connectivity-limited environments, this can result in accuracy degradation. Recent approaches like LoDLoc [78] improve storage efficiency by using  Level-of-Detail (LoD) but still assume unchanged environments, perform poorly in building-sparse areas such as highways, and its initialization depends on positioning sensors.\n\nIn contrast, a compelling solution involves geodata, such as orthographic aerial views (Digital Orthophotos) and elevation maps (Digital Surface Models). These provide a reliable, lightweight source for localizing UAV images, as shown in Figure˜1. Such data is increasingly accessible through free releases from European government geoportals [46, 17], and where public access is limited, can be synthesized using photogrammetric tools [20]. Geodata are scalable and better suited for low-resource settings. For example, covering an area of approximately 0.265 km² would require a 3D model of around 8 GB [69], whereas geodata requires about 30 times less memory. Surprisingly, no existing UAV localization approach seems to fully leverage these data sources. We believe this is mainly due to the absence of aligned cross-domain datasets and the lack of full-pose paired large-scale benchmarks specifically designed for localization using these types of geodata.\n\nTo fill this gap, we capture and release the  Orthographic Aerial Localization and Calibration Dataset (OrthoLoC). It comprises 5 main modalities such as UAV imagery, DOPs, DSMs, 3D point maps, and 3D meshes with a total of 16.4K images captured in 47 regions in 19 cities across 2 countries. Our dataset is the first to offer three key advantages: (1) paired UAV-geodata structure that decouples pose estimation from image retrieval, eliminating confounding error sources in the evaluations; (2) precise 6-DoF poses obtained through multi-view georeferenced photogrammetric reconstruction; and (3) additional reference data sources to increase the domain gaps in the dataset.\n\nWe have evaluated state-of-the-art methods on this novel localization and calibration task in a comprehensive benchmark. Additionally, we introduce a method-agnostic refinement technique called  Adaptive Homography Preconditioning (AdHoP) that further improves localization and calibration accuracy. The technique exploits the uniform structure of DOPs to perform homography-based warping by assuming quasi-planar surfaces common in built environments.\n\nOur evaluation reveals several insights. First, state-of-the-art matching algorithms can generalize to aerial perspectives but struggle with the substantial domain gap between perspective UAV imagery and orthographic reference data. Second, our AdHoP technique significantly reduces the perspective disparity, improving all metrics across the tested methods, particularly achieving up to 95% and 63% enhancements in matching and translation accuracy, respectively. Third, camera calibration in aerial settings presents unique challenges due to fundamental geometric ambiguities that affect parameters estimation. Finally, reference data characteristics including domain shifts, data resolutions, and covisibility. significantly impact localization performance, with higher resolution geodata providing improvement in accuracy.\n\nThe main contributions of this paper are: (1) OrthoLoC, the first UAV dataset providing alignment with geodata across multiple modalities and locations; (2) a unified benchmarking framework for UAV localization and calibration that integrates with state-of-the-art matching algorithms and includes our AdHoP technique for addressing perspective disparities; and (3) benchmarking results for camera localization and calibration and an analysis of performance factors including cross-domain challenges, data resolution effects, and covisibility.",
            "llm_summary": "【关注的是什么问题】  \n1. UAV视觉定位的准确性和资源限制下的挑战  \n2. 现有UAV定位算法对地理数据的利用不足  \n3. 跨域数据集缺乏导致的评估困难  \n\n【用了什么创新的方案】  \n本研究提出了OrthoLoC数据集，包含16,425幅UAV图像及多种地理数据，旨在解决UAV图像与地理数据之间的领域转移问题。通过配对结构，该数据集能够独立评估定位和校准性能。此外，提出的AdHoP技术可与任何特征匹配器集成，显著提高匹配精度和降低平移误差。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought",
            "authors": "Yu Ti Huang",
            "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18200",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18200",
            "arxiv_html_link": "https://arxiv.org/html/2509.18200v1",
            "abstract": "Conversational agents must translate egocentric utterances (e.g., “on my right”) into allocentric orientations (N/E/S/W). This challenge is particularly critical in indoor or complex facilities where GPS signals are weak and detailed maps are unavailable. While chain-of-thought (CoT) prompting has advanced reasoning in language and vision tasks, its application to multimodal spatial orientation remains underexplored. We introduce Conversational Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese conversational navigation projected from real-world environments, addressing egocentric-to-allocentric reasoning in non-English and ASR-transcribed scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: (1) extracting spatial relations, (2) mapping coordinates to absolute directions, and (3) inferring user orientation. A curriculum learning strategy progressively builds these capabilities on Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of resource-constrained settings. Experiments show that MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. Moreover, MCoT demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching. The model also maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity. These findings highlight the potential of structured MCoT spatial reasoning as a path toward interpretable and resource-efficient embodied navigation. Code and data are available at https://github.com/yu-ti-huang/Conversational-Orientation-Reasoning.",
            "introduction": "Humans naturally describe spatial environments in egocentric (agent-centric) terms (e.g., “The exit is on my right”), whereas navigation systems typically operate on allocentric (world-centric) orientations such as north, south, east, and west. Conversational navigation has emerged as a promising paradigm that enables users to specify goals through dialogue, offering a natural and human-centered means of guidance in unfamiliar environments Sundar et al. (2024); Sheshadri & Hara (2024); Kaniwa et al. (2024); Liu et al. (2024); Levi & Kadar (2025). However, the crucial problem of grounding egocentric language into allocentric orientation remains underexplored. Current approaches typically assume access to GPS, detailed maps, or fixed global frames de Vries et al. (2018); Chen et al. (2020), and have concentrated primarily on English-based scenarios. Recent progress has also relied heavily on large-scale models Ghosh et al. (2024); Tang et al. (2023), which show strong reasoning abilities but demand substantial computational resources, hindering deployment in resource-constrained settings such as mobile navigation and edge devices.\n\nResearch in embodied AI and MCoT has advanced vision-language navigation and action planning Mu et al. (2023); Sun et al. (2024); Liu et al. (2025); Shen et al. (2025); Pareek et al. (2024), but orientation reasoning from natural language has been largely overlooked. These approaches typically assume that the agent’s orientation is already known or operate on high-level action spaces rather than inferring fundamental spatial relationships de Vries et al. (2018); Chen et al. (2020). Meanwhile, large audio-language models (LALMs) Zhang et al. (2023); Xie & Wu (2024); Fu et al. (2025); Défossez et al. (2024) have advanced speech understanding and dialogue Tang et al. (2024); Gong et al. (2023); Ghosh et al. (2024); Kong et al. (2024), yet their reasoning abilities remain limited to perception-level tasks such as transcription or summarization yu Huang et al. (2024); Yang et al. (2024); Wang et al. (2025); Shi et al. (2025). While recent efforts like Audio-CoT Ma et al. (2025) show promise for enhanced speech-based reasoning, the challenge of transforming egocentric spatial descriptions into allocentric orientation inference remains unaddressed.\n\nTo address this gap, we introduce Conversational Orientation Reasoning (COR), a new benchmark for egocentric-to-allocentric orientation reasoning in Traditional Chinese conversational navigation. COR is derived from real-world urban transportation environments in Taiwan, projected into structured grid representations. Unlike prior studies that rely on vision or raw audio, COR combines ASR-transcribed egocentric language with structured landmark coordinates, evaluating both clean text and ASR transcripts to simulate realistic recognition errors. COR addresses the lack of non-English benchmarks in multimodal spatial reasoning, particularly under noisy ASR conditions.\n\nOur study is guided by three research questions:\n\nRQ1 (Effectiveness): How effective is multimodal CoT prompting for orientation reasoning compared to unimodal and unstructured baselines?\n\nRQ2 (Component analysis): What are the contributions of ASR preprocessing, multimodal fusion, and structured CoT steps?\n\nRQ3 (Robustness and generalization): How robust is the approach to linguistic variation, and how well does it generalize across different spatial domains?\n\nOur contributions are as follows:\n\nTask and benchmark. We introduce the COR benchmark for egocentric-to-allocentric orientation reasoning, combining ASR-transcribed speech with landmark coordinates.\n\nFramework. We develop a multimodal CoT framework with a structured three-step reasoning process that integrates noisy transcripts with spatial signals for orientation inference.\n\nEvaluation. We provide extensive experiments in Traditional Chinese, demonstrating effectiveness, component contributions, and robustness validation across linguistic variation, cross-domain generalization, and referential ambiguity beyond English-centric research.\n\n1. RQ1 (Effectiveness): How effective is multimodal CoT prompting for orientation reasoning compared to unimodal and unstructured baselines?\n\n2. RQ2 (Component analysis): What are the contributions of ASR preprocessing, multimodal fusion, and structured CoT steps?\n\n3. RQ3 (Robustness and generalization): How robust is the approach to linguistic variation, and how well does it generalize across different spatial domains?",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将自我中心的语言描述转换为世界中心的方向推理？  \n2. 在缺乏GPS和详细地图的情况下，如何实现室内导航的有效性？  \n3. 如何在多模态环境中处理ASR识别错误和语言变化？  \n\n【用了什么创新的方案】  \n我们提出了对话方向推理（COR），这是一个新的基准，旨在解决自我中心到世界中心的方向推理问题。我们开发了一个多模态链式思维（MCoT）框架，通过结构化的三步推理过程，将ASR转录的语音与地标坐标相结合，提取空间关系、映射坐标到绝对方向，并推断用户的方向。该框架在资源受限的环境中表现出色，能够有效应对噪声条件下的对话和跨领域评估。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation",
            "authors": "Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin",
            "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18198",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18198",
            "arxiv_html_link": "https://arxiv.org/html/2509.18198v1",
            "abstract": "Autonomous systems have advanced significantly, but challenges persist in accident-prone environments where robust decision-making is crucial. A single vehicle’s limited sensor range and obstructed views increase the likelihood of accidents. Multi-vehicle connected systems and multi-modal approaches, leveraging RGB images and LiDAR point clouds, have emerged as promising solutions. However, existing methods often assume the availability of all data modalities and connected vehicles during both training and testing, which is impractical due to potential sensor failures or missing connected vehicles. To address these challenges, we introduce a novel framework MMCD (Multi-Modal Collaborative Decision-making) for connected autonomy. Our framework fuses multi-modal observations from ego and collaborative vehicles to enhance decision-making under challenging conditions. To ensure robust performance when certain data modalities are unavailable during testing, we propose an approach based on cross-modal knowledge distillation with a teacher-student model structure. The teacher model is trained with multiple data modalities, while the student model is designed to operate effectively with reduced modalities. In experiments on connected autonomous driving with ground vehicles and aerial-ground vehicles collaboration, our method improves driving safety by up to 20.7%{\\it 20.7}\\%, surpassing the best-existing baseline in detecting potential accidents and making safe driving decisions. More information can be found on our website https://ruiiu.github.io/mmcd.",
            "introduction": "Autonomous technology has rapidly evolved over the past few decades, with advancements in perception [1, 2, 3, 4, 5], decision-making [6, 7, 8], and control systems [9, 10]. However, the deployment of autonomous vehicles still face challenges, particularly in accident-prone scenarios. These scenarios demand high robustness and reliability, as any failure in decision-making could have severe consequences. A single vehicle navigating these scenarios is prone to have accidents due to occlusions and limited sensor range. One promising solution to mitigate these risks is to have multi-vehicle connected systems [11, 12, 13]. By sharing information, vehicles can expand their field of view and reduce the chances of accidents. Another promising direction is the use of multi-modal data [14, 15, 16, 17], such as RGB images and LiDAR point clouds, to enhance the perception and decision-making capabilities of autonomous systems. Recent works have combined these two paradigms to develop multi-vehicle, multi-modal systems [18, 19, 20, 21], leveraging both connectivity and diverse sensor data to further improve autonomous driving performance.\n\nHowever, existing works often assume that the ego vehicle has consistent access to all sensors and connected vehicles during both training and testing. For example, methods utilizing both RGB and LiDAR data for training [14, 15, 22] assume the availability of both modalities during testing. This is not always realistic; for instance, LiDAR sensors may malfunction or become unavailable during testing, leaving only RGB data accessible, or some connected vehicles may not be able to share data, as shown in Fig. 1. Additionally, cost efficiency is a crucial consideration, as LiDAR sensors are more expensive than RGB cameras. Reducing the reliance on LiDAR sensors while still achieving high performance with RGB-only models during testing presents a more cost-effective solution.\n\nTo address these challenges, we introduce a novel multi-modal collaborative decision-making framework for connected autonomy, enabling the ego vehicle to make informed decisions by leveraging shared multi-modal data from collaborative vehicles. To handle scenarios where certain data modalities are missing during testing, we propose an approach based on knowledge distillation (KD) with a teacher-student model structure. Our multi-modal framework serves as the teacher model, trained with multiple data modalities (e.g., RGB and LiDAR), while the student model operates with reduced modalities (e.g., RGB). The knowledge distillation process ensures the student model maintains robust performance even with missing modalities during test time.\n\nIn summary, the main contributions of this paper are:\n\nWe introduce MMCD, a novel multi-modal collaborative decision-making framework for connected autonomy. Our approach fuses single or multi-modal observations provided by ego or connected vehicles in a principled way to make decisions for the ego vehicle in accident-prone scenarios. Our method improves the driving safety by up to 20.7%\\bf 20.7\\% in experiments on connected autonomous driving with ground vehicles and aerial-ground vehicles collaboration, outperforming the best-existing baseline.\n\nWe propose a cross-modal knowledge distillation-based approach for MMCD. Our model is trained with multi-modal cues (e.g., LiDAR and RGB) from connected vehicles but executes using single-modality observations (e.g., RGB). This design ensures robust performance in the presence of missing modalities during testing.\n\n1. We introduce MMCD, a novel multi-modal collaborative decision-making framework for connected autonomy. Our approach fuses single or multi-modal observations provided by ego or connected vehicles in a principled way to make decisions for the ego vehicle in accident-prone scenarios. Our method improves the driving safety by up to 20.7%\\bf 20.7\\% in experiments on connected autonomous driving with ground vehicles and aerial-ground vehicles collaboration, outperforming the best-existing baseline.\n\n2. We propose a cross-modal knowledge distillation-based approach for MMCD. Our model is trained with multi-modal cues (e.g., LiDAR and RGB) from connected vehicles but executes using single-modality observations (e.g., RGB). This design ensures robust performance in the presence of missing modalities during testing.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在事故多发环境中实现鲁棒的决策制定？  \n2. 如何处理测试时缺失数据模态的情况？  \n\n【用了什么创新的方案】  \n提出了MMCD（Multi-Modal Collaborative Decision-making）框架，通过融合来自自我和协作车辆的多模态观察，增强决策能力。采用基于知识蒸馏的教师-学生模型结构，教师模型使用多模态数据进行训练，而学生模型则在测试时能够有效地使用减少的模态，从而确保在某些数据模态缺失时仍能保持鲁棒性能。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        }
    ],
    "2025-09-25": [
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation",
            "authors": "Shaofeng Yin,Yanjie Ze,Hong-Xing Yu,C. Karen Liu,Jiajun Wu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "Website:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.20322",
            "code": "https://visualmimic.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20322",
            "arxiv_html_link": "https://arxiv.org/html/2509.20322v1",
            "abstract": "Humanoid loco-manipulation in unstructured environments demands tight integration of egocentric perception and whole-body control. However, existing approaches either depend on external motion capture systems or fail to generalize across diverse tasks. We introduce VisualMimic, a visual sim-to-real framework that unifies egocentric vision with hierarchical whole-body control for humanoid robots. VisualMimic combines a task-agnostic low-level keypoint tracker—trained from human motion data via a teacher-student scheme—with a task-specific high-level policy that generates keypoint commands from visual and proprioceptive input. To ensure stable training, we inject noise into the low-level policy and clip high-level actions using human motion statistics. VisualMimic enables zero-shot transfer of visuomotor policies trained in simulation to real humanoid robots, accomplishing a wide range of loco-manipulation tasks such as box lifting, pushing, football dribbling, and kicking. Beyond controlled laboratory settings, our policies also generalize robustly to outdoor environments.\nVideos are available at: visualmimic.github.io",
            "introduction": "How do humans manage to push a box that is too heavy to move with only their arms? We start with vision perception to localize the box and rely on visual feedback to guide our interaction with the box. To generate sufficient force, we might bend down and push with our hands, lean in with the strength of our arms and shoulders, or even nudge the box forward with our feet. In such cases, every part of the body can be brought into play to accomplish the task. These strategies underscore two fundamental aspects of human loco-manipulation: egocentric visual perception and whole-body dexterity.\n\nEquipping humanoid robots with such human-like object interaction abilities has been a long-standing challenge. Current approaches can be categorized into three main paradigms based on tasks: First, locomotion-focused methods [1, 2] that excel at terrain traversal but do not address object interaction. Second, approaches that rely on external motion capture systems [3, 4] for object state estimation, restricting their deployment to controlled laboratory environments. Third, vision-based methods for object interaction, which follow two distinct paths: 1) imitation learning approaches [5, 6, 7] that train visuomotor policies via human demonstrations, which are constrained by the scarcity of large-scale demonstration data and result in limited generalization capabilities; and 2) sim-to-real reinforcement learning (RL) methods [8, 9] that offer greater robustness and generalizability; however, vision-based RL is currently limited to simple environmental interactions such as sitting [8] and stair climbing [8, 9], falling significantly short of human-level object interaction abilities, due to the large exploration and action space of humanoid robots.\n\nWe aim to take one step forward on the pathway of sim-to-real RL for visual humanoid-object interaction. To make sim-to-real RL generalize better, we adopt a hierarchical design comprising low-level and high-level policies. In such a hierarchical framework, the task-agnostic low-level policy takes care of balanced control and tracks the command sent by the high-level policy, and the task-specific high-level policy generates simplified tracking commands conditioning on egocentric vision input. This design enables more effective task-specific training. We formulate the command interface as body keypoints (root, hands, feet, head) to ensure both compactness and expressiveness.\n\nTo obtain a low-level keypoint tracker that performs human-like behaviors while tracking commands, we curate human motion data and supervise the tracker via motion imitation rewards. However, because keypoint commands alone do not capture the entirety of human motion, we observe that the keypoint tracker can track target keypoints while not perfectly producing human-like behaviors. To address this problem, we adopt a teacher–student training scheme: 1) We first train a motion tracker with full access to current and future whole-body motions, thereby capable of precisely following human reference motions; 2) We then distill this motion tracker into a keypoint tracker that operates on simplified keypoint commands. By doing so, our keypoint tracker captures human motion behaviors while still maintaining a compact command space. Notably, our keypoint tracker is task-agnostic and shared across tasks once trained.\n\nBuilt upon this general keypoint tracker, we train a high-level keypoint generator via sim-to-real RL. Directly training polices via visual RL significantly slows down the training and leads to non-optimal solutions. Therefore, we also apply a teacher–student scheme: 1) We first train a state-based policy with privileged access to object states, enabling them to solve tasks effectively; 2) We then distill the state-based policy into the visuomotor policy that rely solely on egocentric vision and robot proprioception, making it ready for real-world deployment without external object state estimation. To address the large visual sim-to-real gap (Fig. 8), we apply heavy masking to depth images in simulation, approximating real-world sensor noise.\n\nDue to the exploration nature of RL, we find that the high-level policy training is not stable when the high-level policies explore the action space that is beyond the human motion space (HMS) present in training motion datasets. We adopt two strategies to alleviate this problem: 1) injecting noise during training the low-level policy to help it adapt to potentially noisy commands from the high-level policy, and 2) clipping actions from the high-level policy to keep them within the feasible HMS.\n\nThe resulting framework, VisualMimic, enables us to obtain robust and generalizable visuomotor policies that can zero-shot transfer to the real robot, across a broad range of humanoid loco-manipulation tasks, with relatively simple task-specific reward design and without requiring paired human-object motion data. For real-world experiments (Fig. 4 and Fig. 3), we show that our humanoid robot can 1) lift a 0.5-kilogram box to a height of 1 meter, 2) push a very large box (similar height as the robot and weight 3.8 kilograms) straight and steady with its whole body, 3) dribble a football with the fluency of an experienced player, and 4) kick a box forward with alternating feet. Notably, we also show that our visuomotor policies achieve stable performance in outdoor scenarios, showing strong robustness to real-world variability such as lighting changes and uneven ground.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现人形机器人在非结构化环境中的物体交互与运动控制。  \n2. 现有方法依赖于外部运动捕捉系统，限制了其在多样化任务中的泛化能力。  \n3. 如何通过视觉反馈和全身控制来提升人形机器人的物体操控能力。  \n\n【用了什么创新方法】  \nVisualMimic提出了一种视觉sim-to-real框架，结合了任务无关的低级关键点跟踪器和任务特定的高级策略。低级策略通过教师-学生方案从人类运动数据中训练，确保稳定训练并适应高层策略的指令。通过注入噪声和剪辑高层动作，VisualMimic实现了在多种人形运动操控任务中的零-shot转移，表现出强大的鲁棒性和泛化能力，能够在真实环境中稳定执行如箱子搬运、足球运球等任务。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies",
            "authors": "Remo Steiner,Alexander Millane,David Tingdahl,Clemens Volk,Vikram Ramasamy,Xinjie Yao,Peter Du,Soha Pouya,Shiwei Sheng",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted to CoRL 2025 Workshop RemembeRL",
            "pdf_link": "https://arxiv.org/pdf/2509.20297",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20297",
            "arxiv_html_link": "https://arxiv.org/html/2509.20297v1",
            "abstract": "End-to-end learning of robot control policies, structured as neural networks, has emerged as a promising approach to robotic manipulation. To complete many common tasks, relevant objects are required to pass in and out of a robot’s field of view.\nIn these settings, spatial memory - the ability to remember the spatial composition of the scene - is an important competency.\nHowever, building such mechanisms into robot learning systems remains an open research problem.\nWe introduce mindmap (Spatial Memory in Deep Feature Maps for 3D Action Policies), a 3D diffusion policy that generates robot trajectories based on a semantic 3D reconstruction of the environment.\nWe show in simulation experiments that our approach is effective at solving tasks where state-of-the-art approaches without memory mechanisms struggle.\nWe release our reconstruction system111github.com/nvidia-isaac/nvblox, training code222github.com/NVlabs/nvblox_mindmap, and evaluation tasks22footnotemark: 2 to spur research in this direction.",
            "introduction": "Designing generalist robot manipulation policies remains a holy grail of robotics.\nSuch policies would perform manipulation tasks with a high level of competence and be instructed to do so in natural language.\nRecent advances in deep learning, vision, and natural language processing have, for the first time, brought this goal within reach; however, significant challenges remain.\n\nExisting approaches to developing learned manipulation policies generally aim to learn a mapping from sensor observations to robot control signals [2, 3, 4, 5].\nThese models typically employ transformer-based architectures to process image and proprioceptive inputs to generate control signals.\nSuch methods have shown an impressive ability to complete language-guided manipulation tasks.\nOne limitation of several leading approaches, however, is that the generation of output signals is conditioned on current visual observations only.\nSuch approaches lack spatial memory - the ability to remember the spatial and semantic composition of the scene (see [6] for a taxonomy of robot memory).\nThis leads to surprising limitations to their capabilities.\nAlthough some methods incorporate temporal information by maintaining a temporal window of past images, these approaches have drawbacks of their own (see Section 2).\n\nIn this work, we introduce mindmap, an approach that combines a diffusion policy with a metric-semantic 3D reconstruction of the scene.\nmindmap generates trajectories of 3D end-effector poses in the reconstructed space.\nThis approach allows the policy to generate actions that depend on parts of the scene that are outside of the camera’s current  Field of View (FOV).\nOur experiments show that, on tasks requiring spatial memory, mindmap is effective in completing tasks on which several current approaches struggle.\n\nContributions: \nIn this paper, we contribute tools for extending 3D manipulation policies with spatial memory.\nIn particular, we release metric-semantic mapping333nvidia-isaac.github.io/nvblox/pages/torch_examples_deep_features in nvblox [7], our GPU-accelerated reconstruction library11footnotemark: 1, in addition to our training code22footnotemark: 2, and simulation environments22footnotemark: 2 for testing spatial memory.\nWe demonstrate the efficacy of these tools by extending a state-of-the-art 3D diffusion policy [1].\nWe show that by making changes to the architecture and training, the policy’s performance, on challenging tasks that require spatial memory, is significantly improved.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在机器人控制策略中整合空间记忆以提高任务完成能力。  \n2. 现有方法在处理需要空间记忆的任务时表现不佳。  \n3. 如何利用3D语义重建来生成有效的机器人轨迹。  \n\n【用了什么创新方法】  \n本研究提出了mindmap，一个结合扩散策略与度量-语义3D重建的框架。该方法生成的3D末端执行器姿态轨迹能够依赖于当前视野外的场景部分，从而有效解决了需要空间记忆的任务。实验结果表明，mindmap在这些任务上的表现显著优于现有方法，展示了其在机器人操控中的潜力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video",
            "authors": "Georgios Tziafas,Jiayun Zhang,Hamidreza Kasaei",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20286",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20286",
            "arxiv_html_link": "https://arxiv.org/html/2509.20286v1",
            "abstract": "Learning visuomotor policies from expert demonstrations is an important frontier in modern robotics research, however, most popular methods require copious efforts for collecting teleoperation data and struggle to generalize out-of-distribution. Scaling data collection has been explored through leveraging human videos, as well as demonstration augmentation techniques. The latter approach typically requires expensive simulation rollouts and trains policies with synthetic image data, therefore introducing a sim-to-real gap. In parallel, alternative state representations such as keypoints have shown great promise for category-level generalization. In this work, we bring these avenues together in a unified framework: PAD (Parse-Augment-Distill), for learning generalizable bimanual policies from a single human video. Our method relies on three steps: (a) parsing a human video demo into a robot-executable keypoint-action trajectory, (b) employing bimanual task-and-motion-planning to augment the demonstration at scale without simulators, and (c) distilling the augmented trajectories into a keypoint-conditioned policy. Empirically, we showcase that PAD outperforms state-of-the-art bimanual demonstration augmentation works relying on image policies with simulation rollouts, both in terms of success rate and sample/cost efficiency.\nWe deploy our framework in six diverse real-world bimanual tasks such as pouring drinks, cleaning trash and opening containers, producing one-shot policies that generalize in unseen spatial arrangements, object instances and background distractors.\nSupplementary material can be found in the project webpage https://gtziafas.github.io/PAD_project/.",
            "introduction": "Visuomotor policy learning for robot manipulation has seen great success in recent years [1, 2, 3, 4, 5], yet it typically demands costly and time-consuming data collection from expert demonstrators.\nThis data-hungriness stems from the different required axes of generalization: a competent policy must generalize in unseen object arrangements (spatial) and object instances (object), as well as be robust to environmental conditions such as scene background, camera placement etc. (background).\nAs a result, most common policies struggle to generalize in out-of-distribution scenarios where corresponding data has not been collected.\nA recent methodology to tackle this data scarcity is to tap into the vast repository of videos available in the web, showcasing humans interacting with objects in diverse scenarios [6, 7, 8, 9, 10].\nHere the main challenge is bridging the embodiment gap between humans and robot morphologies [11, 12, 13].\nAlternatively, a recent line of works aims at dealing with spatial generalization by augmenting a small number of source demos with structured, object-centric task-and-motion planning (TAMP) procedures [14, 15, 16, 17].\nHowever, most works train image policies that require calibrated digital twins and expensive on-robot rollouts to generate the augmentations, therefore introducing a visual sim-to-real gap, while still struggling with object and background generalization.\nWhen it comes to bimanual manipulation, additional considerations related to arm collaboration strategies for different task scenarios further complicate data collection / generation.\n\nIn this work we wish to tackle these challenges by proposing PAD (Parse-Augment-Distill), a unified framework for learning bimanual visuomotor policies from a single human video demonstration.\nOur framework works in three steps (see Fig. 1): (a) parsing the video into robot-executable data, (b) augmenting the data in a simulation-free fashion and, (c) distilling the augmented data into a closed-loop policy.\n\nIn our work, we explicitly seek spatial, object and background generalization.\nTo accommodate this, we utilize 3D keypoint coordinates as state representations for our trained policy, which offers three important advantages:\nFirst, keypoints abstract the visual scene into a low-dimensional geometric representation, which is task-specific and decoupled from object semantics, and therefore has empirically shown to aid in sample-efficiency and robustness to background noise [18, 13, 19].\nSecond, keypoints facilitate category-level object generalization, inherited by the open-world capabilities of pretrained vision models for identifying semantic correspondences [20, 21, 22].\nFinally, 3D point states enable efficient spatial augmentations, as keypoint coordinates can be computed on-the-fly through 3D rigid geometry assumptions [23].\nThis alleviates the need for a digital twin and expensive simulation rollouts, which would be required by a typical image policy to obtain image observations [14, 15, 16, 17].\nIn turn, this significantly improves data collection time and bridges the sim-to-real gap introduced by simulators.\n\nConcretely, in PAD we introduce a general TAMP framework for spatial demo augmentations, specialized for bimanual manipulation.\nTo that end, we introduce bimanual task templates, symbolic representations that declare information about each arm’s object assignments, involved contacts and requirements for arm synchronization, while abstracting away the specific semantics of the task.\nWe particularly focus on handling issues related to bimanual manipulation, such as out-of-range arm-object assignments and re-synchronization between the arms during motion planning, which are missing from previous works in bimanual demo augmentation [17].\nOur augmentation framework is general, cost-efficient and embodiment-agnostic, as it uses human video as the source demo that can be mapped to any given morphology.\nFinally, we use prescribed 3D keypoints as our state representation instead of RGB images or point-clouds, and accompany them with augmentations that aid the policy in object generalization.\nTo distill the augmented data, we introduce Kp-RDT, an adapted version of RDT [4] for learning bimanual diffusion policies with keypoint conditioning.\n\nEmpirically, we show that our framework outperforms state-of-the-art bimanual demo augmentation methods [17] in four simulation tasks from the DexMimicGen robosuite benchmark [24], both in terms of success rates, as well as sample-efficiency and data generation time.\nWe further apply our framework in six diverse real-world tasks and show that PAD obtains policies that generalize to unseen spatial arrangements, object instances and background scene noise, while doing so from a single human demonstration.\n\nIn summary, our contributions with this work are threefold:\n\nWe introduce PAD, a unified framework for generalizable bimanual policy learning from a human video.\n\nWe propose a general bimanual TAMP framework for spatial demo augmentations, applicable to a wide variety of manipulation skills and arm-coordination strategies, as well as open-ended object categories.\n\nWe perform extensive robot experiments in 10 tasks, 4 in simulation and 6 with hardware, demonstrating significant gains compared to previous works in terms of success rates and sample/cost efficiency, as well as strong generalization in real-world tasks.\n\n1. We introduce PAD, a unified framework for generalizable bimanual policy learning from a human video.\n\n2. We propose a general bimanual TAMP framework for spatial demo augmentations, applicable to a wide variety of manipulation skills and arm-coordination strategies, as well as open-ended object categories.\n\n3. We perform extensive robot experiments in 10 tasks, 4 in simulation and 6 with hardware, demonstrating significant gains compared to previous works in terms of success rates and sample/cost efficiency, as well as strong generalization in real-world tasks.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何从单个视频中学习可泛化的双手视觉运动策略。  \n2. 如何解决数据收集的高成本和时间消耗问题。  \n3. 如何实现空间、对象和背景的泛化能力。  \n\n【用了什么创新方法】  \n本研究提出了PAD（Parse-Augment-Distill）框架，通过三个步骤实现从单个视频学习双手策略：解析视频为机器人可执行的关键点-动作轨迹，利用无模拟器的任务与运动规划进行演示数据增强，最后将增强的数据蒸馏为关键点条件政策。实验结果表明，PAD在成功率和样本/成本效率上优于现有的双手演示增强方法，能够在六个真实世界任务中生成可泛化的策略，适应未见的空间排列和背景干扰。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "HL-IK: A Lightweight Implementation of Human-Like Inverse Kinematics in Humanoid Arms",
            "authors": "Bingjie Chen,Zihan Wang,Zhe Han,Guoping Pan,Yi Cheng,Houde Liu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20263",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20263",
            "arxiv_html_link": "https://arxiv.org/html/2509.20263v1",
            "abstract": "Traditional IK methods for redundant humanoid manipulators emphasize end-effector (EE) tracking, frequently producing configurations that are valid mechanically but not human-like. We present Human-Like Inverse Kinematics (HL-IK), a lightweight IK framework that preserves EE tracking while shaping whole-arm configurations to appear human-like—without full-body sensing at runtime. The key idea is a learned elbow prior: using large-scale human motion data retargeted to the robot, we train a FiLM-modulated spatio-temporal attention network (FiSTA) to predict the next-step elbow pose from the EE target and a short history of EE–elbow states. This prediction is incorporated as a small residual alongside EE and smoothness terms in a standard Levenberg–Marquardt optimizer, making HL-IK a drop-in addition to numerical IK stacks. Over 183k simulation steps, HL-IK reduces arm-similarity position and direction error by 30.6% and 35.4% on average, and by 42.2% and 47.4% on the most challenging trajectories. Hardware teleoperation on a robot distinct from simulation further confirms the gains in anthropomorphism. HL-IK is simple to integrate, adaptable across platforms via our pipeline, and adds minimal computation, enabling human-like motions for humanoid robots. Project page: https://hl-ik.github.io/",
            "introduction": "A robotic arm can be defined as a series of links connected together by joints [1]. Inverse kinematics (IK) is a fundamental problem in such robotics, traditionally formulated to compute joint configurations that achieve a specified end-effector (EE) pose. For industrial manipulators, this formulation is often sufficient, since the primary objective is to place the tool center point at the desired location with high precision. Classical IK solvers—whether based on closed-form derivations [2, 3, 4, 5], numerical iterations [6, 7], or optimization frameworks [8, 9, 10] — focus almost exclusively on EE tracking.\n\nFor redundant robotic arms, the inverse solution to a given EE pose is often not unique, with infinitely many possible configurations [11, 12]. When only the EE pose is constrained, the intermediate joints remain underdetermined [13], which can lead to solutions that are mechanically valid but visually unnatural and non-human-like. In scenarios such as humanoid robot teleoperation [14, 15, 16], beyond accurate EE tracking, we also aim for the robot’s overall arm configuration to closely resemble that of the human arm, thereby achieving a higher level of anthropomorphism. Existing methods [17, 18] often rely on external cameras to capture human body keypoints and align them with robot joints to improve configuration similarity. Yet, such approaches not only require additional perception inputs but also typically do not treat EE tracking as the primary constraint, and thus cannot be regarded as strict IK solutions. Therefore, our goal is to develop a system that, given only the desired EE pose as input (as in traditional IK), not only ensures precise EE tracking but also achieves close similarity between the human and robot arm configurations.\n\nTo realize this goal, we first model the human arm as a four-point, three-segment kinematic chain comprising the shoulder, elbow, wrist, and fingertips (the human EE) [19]. For a fixed EE pose, the dominant redundancy manifests as the elbow “swivel” about the shoulder–wrist axis. Aligning the elbow pose effectively sets the arm plane and the forearm pointing direction, thereby resolving the main ambiguity and yielding anthropomorphic configurations without sacrificing EE accuracy. In other words, once the elbow is aligned, the overall arm configuration becomes perceptually natural and significantly more similar to that of a human. Furthermore, given a desired EE pose, determining a prior elbow pose that best reflects the natural human form becomes a central aspect of our approach. In summary, the primary contributions of this paper are:\n\n1) Human-like data acquisition framework: We propose an automatic EE–elbow data collection scheme based on large-scale human motion trajectory datasets, which can be readily adapted to different robots.\n\n2) Elbow prediction network: We design a FiLM-modulated Spatio-Temporal Attention Network (FiSTA) that uses only a partial history of EE and elbow frames to predict the desired human-like elbow pose for a given EE target.\n\n3) Comprehensive validation: The effectiveness of our approach is validated both in simulation and on real hardware, as well as across different robotic configurations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在保证末端执行器（EE）跟踪精度的同时，实现类人臂的运动配置？  \n2. 现有的逆向运动学（IK）方法如何导致机械有效但视觉上不自然的解决方案？  \n3. 如何在没有全身传感器的情况下，实现类人运动的机器人臂？  \n\n【用了什么创新方法】  \n本研究提出了一种轻量级的类人逆向运动学（HL-IK）框架，通过学习的肘部先验来改善机械臂的运动配置。使用大规模人类运动数据训练的FiLM调制时空注意力网络（FiSTA）预测肘部姿态，并将其作为小残差与EE和光滑性项结合，应用于标准的Levenberg-Marquardt优化器中。实验结果显示，HL-IK在183k次仿真步骤中，平均减少了30.6%和35.4%的臂部相似性位置和方向误差，并在最具挑战性的轨迹上分别减少了42.2%和47.4%。此外，硬件遥操作验证了其在类人运动方面的提升，表明HL-IK易于集成并适应不同平台。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving",
            "authors": "Jinhao Chai,Anqing Jiang,Hao Jiang,Shiyi Mu,Zichong Gu,Shugong Xu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "IWACIII 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.20253",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20253",
            "arxiv_html_link": "https://arxiv.org/html/2509.20253v1",
            "abstract": "End-to-end multi-modal planning has become a transformative paradigm in autonomous driving, effectively addressing behavioral multi-modality and the generalization challenge in long-tail scenarios. We propose AnchDrive, a framework for end-to-end driving that effectively bootstraps a diffusion policy to mitigate the high computational cost of traditional generative models. Rather than denoising from pure noise, AnchDrive initializes its planner with a rich set of hybrid trajectory anchors. These anchors are derived from two complementary sources: a static vocabulary of general driving priors and a set of dynamic, context-aware trajectories. The dynamic trajectories are decoded in real-time by a Transformer that processes dense and sparse perceptual features. The diffusion model then learns to refine these anchors by predicting a distribution of trajectory offsets, enabling fine-grained refinement. This anchor-based bootstrapping design allows for efficient generation of diverse, high-quality trajectories. Experiments on the NAVSIM benchmark confirm that AnchDrive sets a new state-of-the-art and shows strong generalizability.",
            "introduction": "End-to-end autonomous driving algorithms have gained substantial attention in recent years owing to their superior scalability and adaptability over traditional rule-based motion planning approaches. By learning control signals directly from raw sensor data—such as camera images or LiDAR point clouds—these methods bypass the complexity of modular design pipelines, mitigate the accumulation of perception errors, and enhance overall system consistency and robustness. Earlier end-to-end planners, including UniAD[1], VAD[2], and Transfuser[3], relied on ego queries to regress single-modal trajectories, while more recent approaches such as SparseDrive[4] explored sparse perception modules in combination with parallel motion planners. Nevertheless, in complex traffic conditions—such as intersections or high-speed lane changes—potential vehicle behaviors can be highly ambiguous and diverse. Ignoring the inherent uncertainty in driving behavior and the multi-modal decision-making requirements imposed by environmental perception often leads to overconfident or outright failed predictions when relying on a single predicted trajectory.\n\nRecent research has therefore begun to incorporate multi-modal modeling strategies, producing multiple trajectory proposals consistent with current scene constraints to improve decision coverage. Methods such as VADv2[5] and Hydra-MDP[6] achieve this by using predefined discrete trajectory sets. While this increases coverage to some extent, the reliance on fixed trajectory sets inherently discretizes what is fundamentally a continuous control process, thus constraining expressiveness and flexibility.\n\nDiffusion models have emerged as a promising alternative, offering generative and adaptive capabilities well suited for multi-modal trajectory planning. They enable direct sampling from the high-dimensional joint distribution of the ego vehicle and surrounding agents’ trajectories, and have demonstrated strong modeling capacity in high-dimensional continuous control spaces—evidenced by successes in domains such as image synthesis and robotic motion planning. Their ability to naturally model conditional distributions makes it straightforward to integrate key contextual inputs, including trajectory history, map semantics, and ego objectives, thereby improving both consistency and contextual relevance in policy generation. Moreover, their controllable test-time sampling allows for incorporating additional constraints without retraining, unlike many Transformer-based architectures.\n\nDespite improvements such as DDIM[7] for accelerating sampling, conventional diffusion models require numerous iterative denoising steps, resulting in high computational and latency costs at inference. To address this, prior work has shown that initializing the generation process from non-standard noise distributions can shorten the sampling path by leveraging prior information. Building on this idea, DiffusionDrive[8] proposed a truncated diffusion strategy that anchors the process to a fixed set of trajectory anchors, enabling sampling to begin from intermediate states and thus reducing the number of required iterations. However, such fixed anchor sets lack the flexibility to adapt to scenarios demanding dynamically generated anchors.\n\nWe address this limitation with AnchDrive, a novel end-to-end multi-modal autonomous driving framework. AnchDrive employs a multi-head trajectory decoder to dynamically generate a set of dynamic trajectory anchors informed by scene perception, capturing behavioral diversity under local environmental conditions. Simultaneously, we construct a broad-coverage static anchor set from large-scale human driving data, providing cross-domain behavioral priors. These dynamic anchors provide context-aware guidance tailored to the immediate scene, while the static anchor set mitigates overfitting to training distributions and improves generalization to unseen environments. By leveraging this hybrid anchor set, our diffusion-based planner can produce high-quality and diverse predictions within a reduced number of denoising steps.\n\nWe evaluate AnchDrive in closed-loop settings on the Navsim-v2[9] simulation platform, which features reactive background traffic agents and high-fidelity synthetic multi-view imagery. Experiments on a navtest set show that AnchDrive achieves 85.5 EPDMS, indicating robust and contextually appropriate behavior generation in complex driving scenarios.\n\nOur key contributions are as follows:\n\nWe propose AnchDrive, an end-to-end autonomous driving framework that employs a truncated diffusion process initialized from a hybrid set of trajectory anchors. This approach, which integrates both dynamic and static anchors, significantly improves initial trajectory quality and enables robust planning. We validate its effectiveness on the challenging Navsim-v2[9] benchmark.\n\nWe propose AnchDrive, an end-to-end autonomous driving framework that employs a truncated diffusion process initialized from a hybrid set of trajectory anchors. This approach, which integrates both dynamic and static anchors, significantly improves initial trajectory quality and enables robust planning. We validate its effectiveness on the challenging Navsim-v2[9] benchmark.\n\nWe design a hybrid perception model with dense and sparse branches. The dense branch builds a bird’s-eye-view (BEV) representation for the planner’s primary input, while the sparse branch extracts instance-level cues—such as detected obstacles, lane boundaries, centerlines, and stop lines—to enhance the planner’s understanding of obstacles and road geometry.\n\n1. We propose AnchDrive, an end-to-end autonomous driving framework that employs a truncated diffusion process initialized from a hybrid set of trajectory anchors. This approach, which integrates both dynamic and static anchors, significantly improves initial trajectory quality and enables robust planning. We validate its effectiveness on the challenging Navsim-v2[9] benchmark.\n\n2. We design a hybrid perception model with dense and sparse branches. The dense branch builds a bird’s-eye-view (BEV) representation for the planner’s primary input, while the sparse branch extracts instance-level cues—such as detected obstacles, lane boundaries, centerlines, and stop lines—to enhance the planner’s understanding of obstacles and road geometry.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂交通条件下生成多模态的驾驶轨迹以应对行为的不确定性。  \n2. 如何降低传统生成模型在推理时的计算成本和延迟。  \n3. 如何结合动态和静态轨迹锚点以提高轨迹生成的质量和多样性。  \n\n【用了什么创新方法】  \nAnchDrive框架通过引入混合轨迹锚点，采用截断扩散过程来初始化生成模型。动态轨迹锚点由Transformer实时解码，结合静态锚点提供广泛的行为先验。该方法显著提高了初始轨迹的质量，并在Navsim-v2基准测试中表现出强大的泛化能力，达到了85.5 EPDMS，展示了在复杂驾驶场景中的稳健和上下文适应行为生成能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Techno-Economic analysis for Smart Hangar inspection operations through Sensing and Localisation at scale",
            "authors": "Angelos Plastropoulos,Nicolas P. Avdelidis,Argyrios Zolotas",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20229",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20229",
            "arxiv_html_link": "https://arxiv.org/html/2509.20229v1",
            "abstract": "The accuracy, resilience, and affordability of localisation are fundamental to autonomous robotic inspection within aircraft maintenance and overhaul (MRO) hangars. Hangars typically feature tall ceilings and are often made of materials such as metal. Due to its nature, it is considered a GPS-denied environment, with extensive multipath effects and stringent operational constraints that collectively create a uniquely challenging environment. This persistent gap highlights the need for domain-specific comparative studies, including rigorous cost, accuracy, and integration assessments, to inform a reliable and scalable deployment of a localisation system in the Smart Hangar. This paper presents the first techno-economic roadmap that benchmarks motion capture (MoCap), ultra-wideband (UWB), and a ceiling-mounted camera network across three operational scenarios: robot localisation, asset tracking, and surface defect detection within a 40 × 50 m hangar bay. A dual-layer optimisation for camera selection and positioning framework is introduced, which couples market-based camera-lens selection with an optimisation solver, producing camera layouts that minimise hardware while meeting accuracy targets. The roadmap equips MRO planners with an actionable method to balance accuracy, coverage, and budget, demonstrating that an optimised vision architecture has the potential to unlock robust and cost-effective sensing for next-generation Smart Hangars.",
            "introduction": "Accurate localisation is fundamental to enabling autonomous robotic inspection in MRO hangars, where metallic structures, extensive multipath effects, and strict operational constraints define a unique and challenging environment [1]. Existing localisation technologies, including infrared MoCap, UWB real-time location systems, and camera-based or Simultaneous Localisation and Mapping (SLAM) approaches, offer different trade-offs in terms of achievable accuracy, infrastructure complexity, cost, and robustness to occlusion and interference [2]. Sensor fusion frameworks that combine vision, inertial, and UWB data can improve robustness and deliver cost–accuracy trade-offs, as demonstrated in large-scale warehouse deployments. However, their performance and economic viability remain highly dependent on environment-specific factors [3]. In addition, this study also treats localisation as the backbone for real-time asset monitoring, allowing ground support equipment, tooling, and spares to be tracked seamlessly in the bay. On the opposite scale, artefact localisation is addressed, with an exploration of how sensing can facilitate the identification of surface defects and other critical features on the airframe. The framing of these macroscopic and microscopic needs together sets the stage for the optimisation framework, comparative experiments, and cost analyses developed in the remainder of the paper.\n\nDespite technical advances, a notable lack of comprehensive, real-world techno-economic analyses remains, specifically focused on aircraft hangar deployments. The available literature provides only partial benchmarking or component-level comparisons for individual or hybrid localisation modalities [4, 5], with little empirical evidence on their robustness to the full spectrum of hangar-specific challenges such as dynamic occlusion, specular reflections, and integration with existing maintenance workflows. To date, no studies have presented a holistic side-by-side evaluation of MoCap, UWB, and vision-based solutions in an operational metallic aircraft hangar context. This persistent gap underscores an urgent need for domain-specific comparative studies, including rigorous assessments of cost, accuracy, and integration, to inform the reliable and scalable deployment of robotic inspection systems in aviation environments. The economic impact of daily maintenance practices has begun to be quantified by recent hangar-focused studies; for instance, significant rework costs in narrow-body bays can be avoided by improving technicians’ awareness of composite repair, as demonstrated by Jong et al. [6]. Across the timeline, Moenck et al. [7] outline how the forthcoming Industry 5.0 automation could reshape the trade-offs of labour hours and logistics on large MRO campuses. At the same time, the classic aerodynamic analysis of the enclosed engine test hangars by Wallis and Ruglen still provides a valuable historical baseline for energy throughput economics [8].\n\nIn summary, this study offers five significant contributions: (i) it presents the inaugural techno-economic roadmap that evaluates MoCap, UWB, and ceiling-camera vision in parallel for full-scale aircraft hangars; (ii) it introduces a dual-layer optimisation framework that combines market-driven camera-lens selection with a Mixed-Integer Linear Programming-based set-cover placement, resulting in the minimal number of cameras needed; (iii) it supplies quantified design-to-cost case studies converting three typical MRO tasks into specific bills of materials and cost estimates; (iv) determines the optimal balance for defect-detection accuracy, illustrating how ceiling cameras and drone close-ups converge at various defect sizes; and (v) provides the first cost/accuracy comparison between camera localisation and commercial UWB/MoCap systems for a conventional 40 × 50 m bay. Collectively, these contributions deliver an actionable and comprehensive methodology for MRO decision-makers to select, size, and cost localisation and inspection systems within large hangars.\n\nInspired by the aviation industry’s shift towards Industry 5.0, which sees mobile robots and AI-based decision support systems taking on routine maintenance duties, the hangar should transition from a passive shelter to a dynamic sensing platform. The end goal is a ceiling infrastructure dense enough to localise robots, track assets, and even surface defects in real-time but lean enough to be economically retrofitted into legacy bays. Against this backdrop, the remainder of the paper is organised as follows. Section 2 reviews the state-of-the-art in MoCap, UWB and ceiling-camera vision, clarifying their respective accuracy, cost, and integration trade-offs. Section 3 introduces a dual-layer optimisation framework that first selects a market-ready camera–lens pair and then solves a set-cover problem to minimise hardware while meeting resolution targets. Section 4 translates those algorithms into three design-to-cost frameworks: robot localisation, asset tracking, and defect detection, each with a detailed bill of materials for practitioner guidance. It also presents MoCap and UWB implementation options for benchmarking.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在GPS-denied环境中实现准确、可靠的机器人定位和检测？  \n2. 如何在航空维护和检修（MRO）环境中平衡成本、准确性和集成性？  \n3. 当前的定位技术在复杂环境中的经济可行性和性能如何？  \n\n【用了什么创新方法】  \n本文提出了首个针对飞机机库的技术经济路线图，比较了运动捕捉（MoCap）、超宽带（UWB）和天花板摄像头网络在机器人定位、资产跟踪和表面缺陷检测中的应用。引入了一个双层优化框架，结合市场驱动的摄像头镜头选择与混合整数线性规划的设置覆盖问题，最小化所需硬件数量，同时满足准确性目标。研究表明，优化的视觉架构能够实现强大且具有成本效益的传感能力，为下一代智能机库的部署提供了可行的方法。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "A Biomimetic Vertebraic Soft Robotic Tail for High-Speed, High-Force Dynamic Maneuvering",
            "authors": "Sicong Liu,Jianhui Liu,Fang Chen,Wenjian Yang,Juan Yi,Yu Zheng,Zheng Wang,Wanchao Chi,Chaoyang Song",
            "subjects": "Robotics (cs.RO)",
            "comment": ". Submitted Under Review",
            "pdf_link": "https://arxiv.org/pdf/2509.20219",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20219",
            "arxiv_html_link": "https://arxiv.org/html/2509.20219v1",
            "abstract": "Robotic tails can enhance the stability and maneuverability of mobile robots, but current designs face a trade-off between the power of rigid systems and the safety of soft ones. Rigid tails generate large inertial effects but pose risks in unstructured environments, while soft tails lack sufficient speed and force. We present a Biomimetic Vertebraic Soft Robotic (BVSR) tail that resolves this challenge through a compliant pneumatic body reinforced by a passively jointed vertebral column inspired by musculoskeletal structures. This hybrid design decouples load-bearing and actuation, enabling high-pressure actuation (up to 6 bar) for superior dynamics while preserving compliance. A dedicated kinematic and dynamic model incorporating vertebral constraints is developed and validated experimentally. The BVSR tail achieves angular velocities above 670°/s and generates inertial forces and torques up to 5.58 N and 1.21 Nm, indicating over 200% improvement compared to non-vertebraic designs. Demonstrations on rapid cart stabilization, obstacle negotiation, high-speed steering, and quadruped integration confirm its versatility and practical utility for agile robotic platforms.",
            "introduction": "The tail is a masterful evolutionary solution for dynamic locomotion, enabling animals to achieve remarkable feats of stability and agility [1]. The functional utility of this appendage is rooted in the principles of classical mechanics, particularly the conservation of angular momentum. This is often accomplished through a process known as inertial adjustment, where rapid, controlled movements of the tail generate reaction forces and torques that are imparted onto the main body to regulate its orientation and momentum in real time. For instance, cheetahs, during their high-speed bounding gait, use their long, muscular tails as aerodynamic rudders and inertial counterweights to modulate yaw and roll, allowing for exceptionally sharp turns and stable braking [2, 3]. In the aerial realm, animals like geckos and lizards execute rapid mid-air self-righting maneuvers by swinging their tails, inducing a counter-rotation in their bodies to ensure a safe landing orientation [4, 5]. Even during terrestrial or arboreal locomotion, animals from kangaroos to squirrels leverage their tails for a spectrum of dynamic tasks, from providing a “fifth leg” for postural stability to recovering from unexpected falls and slips [6, 7]. These biological archetypes, which demonstrate a sophisticated functional integration of sensing, neural control, and musculoskeletal actuation, provide a rich foundation for designing robotic tails that augment the dynamic performance of mobile robots [8, 9].\n\nInspired by these natural mechanisms, roboticists have long sought to replicate their function to enhance the agility and robustness of mobile systems. Early pioneering work included the Uniroo, a monopedal hopping robot that employed a simple tail for pitching stabilization [10]. Subsequent research in this domain has primarily focused on rigid, articulated tails, which typically manifest as pendulum-like mechanisms. These designs range from single-degree-of-freedom (DOF) systems for planar regulation in the pitch, yaw, or roll axes [2, 11, 12, 13, 14, 3, 15, 16, 17], to 2-DOF mechanisms that provide more generalized spatial control [18, 19]. The high stiffness and well-defined kinematics of these rigid systems allow them to generate significant and predictable inertial effects at high speeds. However, their fundamental design presents a critical set of trade-offs. The primary limitation is their lack of compliance, which restricts their utility in unstructured environments and poses significant safety risks for any physical human-robot interaction [20]. Furthermore, the high reflected inertia of a rigid tail necessitates large, powerful actuators, which adds considerable mass and increases power consumption. This demand for high-bandwidth torque control to manage large inertial loads and mitigate potentially destabilizing impact forces adds significant complexity to the control system.\n\nTo address the inherent safety and compliance limitations of rigid systems, the field of soft robotics presents a compelling alternative paradigm. The intrinsic compliance of soft robots, derived from their deformable materials and structures, offers inherent safety, adaptability to uncertain environments, and robustness to physical impacts [21, 22, 23, 24, 25]. This has motivated the development of a new class of soft robotic tails, including hyper-redundant continuum structures for inertial adjustment [26], aquatic robots with flexible tails that emulate fish locomotion for propulsion and maneuvering [27], and even novel wearable tails for human balance assistance [28]. However, while these pioneering systems validate the potential of soft appendages, they also reveal a persistent performance gap. This deficit is rooted in the fundamental properties of the soft materials themselves, which typically exhibit low stiffness and significant viscoelasticity, leading to challenges in generating sufficient force at high frequencies. The low actuation authority of most soft systems, combined with the immense difficulty of accurately modeling and controlling their near-infinite degrees of freedom, limits their ability to produce the rapid, high-magnitude accelerations necessary for effective dynamic regulation of a large robotic platform. Consequently, the field is faced with a critical unmet need: a robotic appendage that unites the raw inertial authority of rigid systems with the inherent safety and adaptability of soft structures. The absence of such a system currently precludes the deployment of agile, dynamic robots in unstructured, human-centric environments.\n\nThis paper introduces a Biomimetic Vertebraic Soft Robotic (BVSR) tail, as shown in Fig. 1, specifically designed to address the trade-offs above by establishing a novel hybrid design approach. Our method involves the functional integration of a compliant pneumatic body with an internal, passively jointed vertebral column. This central element performs a crucial dual role: it acts as a structural backbone that bears the tensile loads from high-pressure actuation, allowing the system to generate large forces without material failure, and it serves as a kinematic constraint that reduces the complex, high-dimensional deformation of the soft body into a predictable, low-dimensional bending motion. This architectural choice makes the system’s modeling and control more tractable while enabling a level of dynamic performance previously unattainable in soft robotic appendages. The primary contributions of this work are threefold:\n\nThe formulation and physical realization of a vertebraic soft robotic design principle, wherein a passive kinematic constraint enables high-pressure (6 Bar) actuation in an otherwise compliant structure to achieve superior angular velocity and inertial output.\n\nA comprehensive Euler-Lagrange dynamic model that, among the first in this class of robots, explicitly incorporates the kinematic constraints imposed by an internal vertebral structure, demonstrating high fidelity between theoretical predictions and experimental results.\n\nRigorous experimental validation of the tail’s performance envelope and a demonstration of its functional efficacy in dynamic tasks, including inertial assistance for a wheeled mobile robot and successful integration with a quadrupedal platform, validating its versatility.\n\nThe remainder of this paper is organized as follows. Section 2 describes the concept and modeling of the BVSR tail. Section 3 presents its physical implementation and characterization. Section 4 details the experimental validation and functional demonstrations. Section 5 discusses the implications of our findings, limitations, and comparisons to the state of the art. Finally, Section 6 concludes the paper and discusses future work.\n\n1. The formulation and physical realization of a vertebraic soft robotic design principle, wherein a passive kinematic constraint enables high-pressure (6 Bar) actuation in an otherwise compliant structure to achieve superior angular velocity and inertial output.\n\n2. A comprehensive Euler-Lagrange dynamic model that, among the first in this class of robots, explicitly incorporates the kinematic constraints imposed by an internal vertebral structure, demonstrating high fidelity between theoretical predictions and experimental results.\n\n3. Rigorous experimental validation of the tail’s performance envelope and a demonstration of its functional efficacy in dynamic tasks, including inertial assistance for a wheeled mobile robot and successful integration with a quadrupedal platform, validating its versatility.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何设计一种既能提供高速度和高力量又具备安全性的机器人尾部？  \n2. 现有的刚性和柔性尾部设计之间的性能差距如何弥补？  \n3. 如何有效建模和控制具有高维自由度的软体机器人？  \n\n【用了什么创新方法】  \n本研究提出了一种生物仿生的脊椎软体机器人尾部（BVSR），结合了顺应性气动体和被动关节脊柱，解决了刚性和柔性系统之间的权衡。该设计通过高压驱动（最高6 Bar）实现了超过670°/s的角速度和5.58 N的惯性力，显示出200%以上的性能提升。通过建立和验证包含脊柱约束的动态模型，实验结果表明该尾部在动态任务中的有效性，成功集成于四足机器人，展示了其多功能性和实用性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving",
            "authors": "Pengxiang Li,Yinan Zheng,Yue Wang,Huimin Wang,Hang Zhao,Jingjing Liu,Xianyuan Zhan,Kun Zhan,Xianpeng Lang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20109",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20109",
            "arxiv_html_link": "https://arxiv.org/html/2509.20109v1",
            "abstract": "End-to-End (E2E) solutions have emerged as a mainstream approach for autonomous driving systems, with Vision-Language-Action (VLA) models representing a new paradigm that leverages pre-trained multimodal knowledge from Vision-Language Models (VLMs) to interpret and interact with complex real-world environments. However, these methods remain constrained by the limitations of imitation learning, which struggles to inherently encode physical rules during training. Existing approaches often rely on complex rule-based post-refinement, employ reinforcement learning that remains largely limited to simulation, or utilize diffusion guidance that requires computationally expensive gradient calculations. To address these challenges, we introduce ReflectDrive, a novel learning-based framework that integrates a reflection mechanism for safe trajectory generation via discrete diffusion. We first discretize the two-dimensional driving space to construct an action codebook, enabling the use of pre-trained Diffusion Language Models for planning tasks through fine-tuning. Central to our approach is a safety-aware reflection mechanism that performs iterative self-correction without gradient computation. Our method begins with goal-conditioned trajectory generation to model multi-modal driving behaviors. Based on this, we apply local search methods to identify unsafe tokens and determine feasible solutions, which then serve as safe anchors for inpainting-based regeneration. Evaluated on the NAVSIM benchmark, ReflectDrive demonstrates significant advantages in safety-critical trajectory generation, offering a scalable and reliable solution for autonomous driving systems.",
            "introduction": "Autonomous driving (AD) is guiding the transportation industry toward a safer and more efficient future (Tampuu et al., 2020). Within this trend, End-to-End (E2E) systems (Hu et al., 2023; Chen et al., 2023) have emerged as the mainstream alternative to traditional modular designs (Bansal et al., 2018), which are prone to error accumulation between interdependent modules. They have also largely replaced rule-based methods (Fan et al., 2018; Treiber et al., 2000) that demand extensive human engineering effort. Meanwhile, Vision-Language-Action (VLA) models (Kim et al., 2024; Hwang et al., 2024) offer a new solution by incorporating pre-trained knowledge from Vision-Language Models (VLMs) (Hurst et al., 2024; Bai et al., 2025). Equipped with enhanced generalization capabilities, VLA models can interpret visual scenes and understand human instructions to directly output planning trajectories, thereby improving adaptability in challenging situations.\n\nHowever, eixsting learning-based methods does not resolve the core challenge in imitation learning-based driving systems. Specifically, behavior cloning fails to inherently encode inviolable physical rules, such as collision avoidance or adherence to drivable areas (Lu et al., 2023). As a result, a generated trajectory may be highly probable under the model’s distribution yet still violate critical safety constraints. Consequently, existing deployed solutions often rely on significant human priors, such as trajectory anchors (Li et al., 2024) or rule-based generated paths (Dauner et al., 2023). These priors offer a reliable initial solution for the learning system, but they also necessitate substantial post-processing, particularly in complex scenarios. Concurrently, more advanced solutions are emerging. Some methods integrate reinforcement learning (Kaelbling et al., 1996; Kendall et al., 2019; Jaeger et al., 2025; Cusumano-Towner et al., 2025) with human-designed reward functions to enhance causal reasoning. However, most existing studies remain confined to the simulation level. From a deployment perspective, these approaches typically require unsafe online rollouts and suffer from training instability, especially in large-scale models (Zheng et al., 2024). Although guidance mechanisms in diffusion models provide a promising alternative by enabling controllable generation during inference (Zheng et al., 2025; Jiang et al., 2023; Zhong et al., 2023), they often experience slow sampling speeds due to gradient computations and are highly sensitive to parameter tuning, which can lead to numerical instability.\n\nTo address these challenges, we pioneer the use of discrete diffusion (Austin et al., 2021) for planning to meet the demand for verifiable and controllable E2E driving systems. A key advantage of this approach is its operation in a discrete action space, which facilitates the seamless incorporation of critical safety constraints through search, masking, and sampling techniques during trajectory generation. This results in a hybrid framework in which learned behaviors can be rigorously guided by prior knowledge, shifting away from black-box planning toward trustworthy and interpretable decision-making. Inspired by these insights, we propose ReflectDrive, a novel learning-based framework that integrates a reflection mechanism for safe trajectory generation via discrete diffusion. Specifically, we first discretize the two-dimensional driving space to construct a action codebook, enabling the representation of vehicle trajectories through discrete codebook embeddings. This representation allows us to leverage a pre-trained Diffusion Language Models (DLMs) (You et al., 2025; Nie et al., 2025) for planning tasks via fine-tuning. The approach facilitates parallel decoding and bidirectional feature fusion within a unified architecture that supports scalable training. Based on this fine-tuned model, our reflection mechanism begins with goal-conditioned generation, where the goal point guides the generation process to capture diverse multi-modal driving behaviors. Furthermore, the framework integrates safety metrics to evaluate the generated multi-modal trajectories. For unsafe waypoints, we perform a local search to identify a feasible solution, which then serves as a safe anchor token for trajectory inpainting. The entire process operates without gradient computation, enabling parallel generation and the injection of safety constraints during trajectory regeneration. Evaluations on the real-world autonomous driving benchmark NAVSIM (Dauner et al., 2024) demonstrate the feasibility of employing discrete diffusion for trajectory generation. Equipped with our reflection mechanism, ReflectDrive achieves near human-level closed-loop performance. Our contributions are summarized as follows:\n\nWe pioneer the application of discrete diffusion for E2E autonomous driving trajectory generation and integrate it into a VLA model for scalable training.\n\nWe introduce reflection mechanism, a novel inference-time guidance framework specifically designed for the denoising process in discrete diffusion, integrating external safety validation with efficient discrete token optimization.\n\nWe evaluate our method on real-world driving benchmarks, proving that the framework can enforce hard safety constraints without compromising behavioral coherence.\n\n1. We pioneer the application of discrete diffusion for E2E autonomous driving trajectory generation and integrate it into a VLA model for scalable training.\n\n2. We introduce reflection mechanism, a novel inference-time guidance framework specifically designed for the denoising process in discrete diffusion, integrating external safety validation with efficient discrete token optimization.\n\n3. We evaluate our method on real-world driving benchmarks, proving that the framework can enforce hard safety constraints without compromising behavioral coherence.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在自主驾驶中有效地生成安全的轨迹以满足物理规则？  \n2. 现有的模仿学习方法如何克服对复杂场景的依赖和不稳定性？  \n3. 如何在不依赖梯度计算的情况下实现高效的轨迹生成？  \n\n【用了什么创新方法】  \n本研究提出了ReflectDrive，一个基于离散扩散的学习框架，集成了反射机制以实现安全的轨迹生成。首先，我们将二维驾驶空间离散化，构建动作代码本，并通过微调利用预训练的扩散语言模型进行规划任务。核心是一个安全意识的反射机制，能够在不进行梯度计算的情况下进行迭代自我修正。通过目标条件的轨迹生成，我们能够捕捉多模态驾驶行为，并通过局部搜索识别不安全的轨迹点，确保生成的轨迹符合安全约束。评估结果表明，ReflectDrive在NAVSIM基准上表现出显著的安全性优势，提供了一种可扩展且可靠的自主驾驶解决方案。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Hybrid Safety Verification of Multi-Agent Systems using $ψ$-Weighted CBFs and PAC Guarantees",
            "authors": "Venkat Margapuri,Garik Kazanjian,Naren Kosaraju",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20093",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20093",
            "arxiv_html_link": "https://arxiv.org/html/2509.20093v1",
            "abstract": "This study proposes a hybrid safety verification framework for closed-loop multi-agent systems under bounded stochastic disturbances. The proposed approach augments control barrier functions with a novel ψ\\psi-weighted formulation that encodes directional control alignment between agents into the safety constraints. Deterministic admissibility is combined with empirical validation via Monte Carlo rollouts, and a PAC-style guarantee is derived based on margin-aware safety violations to provide a probabilistic safety certificate. The results from the experiments conducted under different bounded stochastic disturbances validate the feasibility of the proposed approach.",
            "introduction": "Safety within multi-agent systems is essential for real-world applications such as autonomous driving [1, 2] and robotic swarm deployments in agriculture [3, 4], manufacturing [5, 6], and search and rescue operations [7], where agents must navigate safely through their environment. Safety in a stochastic multi-agent dynamical system requires that all agent trajectories remain within a predefined safe set under specified control inputs and time horizons.\nTraditional approaches include reachability-based formulations [8, 9], where a Hamilton-Jacobi partial differential equation is solved to characterize the backward-reachable set that avoids unsafe regions. However, these methods are computationally expensive and scale poorly to high-dimensional or multi-agent systems. More recently, barrier certificates [10, 11] and control barrier functions (CBFs) [12, 13] have emerged as tractable alternatives for certifying safety. By enforcing forward invariance of a safe set via control-affine constraints, CBFs offer real-time safety guarantees under deterministic assumptions. Yet, such guarantees may fail in the presence of noise or unmodeled disturbances. While stochastic CBF variants address this, they often rely on strong distributional assumptions or chance-constrained formulations.\n\nTo bridge the gap, this work introduces a hybrid safety verification framework that unifies ψ\\psi-weighted CBFs for forward invariance with finite-sample probably approximately correct (PAC)-style guarantees for margin-aware safety under bounded stochastic disturbances, where ψ\\psi is a term inspired by quantum walk dynamics [14] to promote pairwise safety among different agents. Rather than assuming complete knowledge of the noise distributions, the proposed method combines deterministic admissibility with empirical validation via Monte Carlo rollouts under bounded stochasticity, yielding a distribution-free safety certificate with high-probability guarantees. The proposed method is feasible in multi-agent applications where uncertainty is prevalent and exact noise modeling is infeasible.",
            "llm_summary": "【关注的是什么问题】  \n1. 多智能体系统在随机干扰下的安全性验证问题。  \n2. 现有方法在高维或多智能体系统中的计算复杂性和可扩展性不足。  \n3. 如何在不完全知识的情况下提供概率安全证书。  \n\n【用了什么创新方法】  \n本研究提出了一种混合安全验证框架，结合了ψ加权控制障碍函数（CBFs）与基于有限样本的PAC风格保证。该方法通过引入量子行走动态启发的ψ项，促进不同智能体间的方向控制一致性。通过Monte Carlo回放进行经验验证，结合确定性可接受性，最终实现了在有界随机干扰下的分布无关安全证书。实验结果表明，该方法在多智能体应用中具有良好的可行性和高概率保证。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "C-3TO: Continuous 3D Trajectory Optimization on Neural Euclidean Signed Distance Fields",
            "authors": "Guillermo Gil(1),Jose Antonio Cobano(1),Luis Merino(1),Fernando Caballero(1) ((1) Service Robotics Laboratory, Universidad Pablo de Olavide, Seville, Spain)",
            "subjects": "Robotics (cs.RO)",
            "comment": "submitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.20084",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20084",
            "arxiv_html_link": "https://arxiv.org/html/2509.20084v1",
            "abstract": "This paper introduces a novel framework for continuous 3D trajectory optimization in cluttered environments, leveraging online neural Euclidean Signed Distance Fields (ESDFs). Unlike prior approaches that rely on discretized ESDF grids with interpolation, our method directly optimizes smooth trajectories represented by fifth-order polynomials over a continuous neural ESDF, ensuring precise gradient information throughout the entire trajectory. The framework integrates a two-stage nonlinear optimization pipeline that balances efficiency, safety and smoothness. Experimental results demonstrate that C-3TO produces collision-aware and dynamically feasible trajectories. Moreover, its flexibility in defining local window sizes and optimization parameters enables straightforward adaptation to diverse user’s needs without compromising performance. By combining continuous trajectory parameterization with a continuously updated neural ESDF, C-3TO establishes a robust and generalizable foundation for safe and efficient local replanning in aerial robotics. The source code is open source and can be found at: https://anonymous.4open.science/r/icra2026_neural_trajectory_planner_C3TO_anon/",
            "introduction": "Aerial robots have become increasingly popular for a wide range of real-world applications due to their ability to perform hazardous tasks more efficiently and, most importantly, more safely than humans [1][2]. Fast trajectory replanning remains a critical area of research, particularly in dynamic and unstructured environments. Equally important is maintaining a continuously updated representation of the drone’s surroundings, which is essential for generating continuous, safe, and smooth 3D local trajectories in real time. This paper presents a framework for planning a continuous local trajectory on an online, neurally-generated, distance field.\n\nChosing an adequate map representation is key. Having an efficient calculation of the free space and the direction to the closest obstacle are some of those desirable features for such representation. Euclidean Signed Distance Fields (ESDFs) have become increasingly popular as a method for representing and modeling robot surroundings for planning purposes [3, 4] and present many very useful properties that are desirable for planning: it is continuous, differentiable everywhere except at the cut-locus, its gradient is Lipschitz-continuous everywhere except at the cut-locus, and the norm of its spatial gradient is one [5]. Representations of ESDFs are typically based on discrete voxel grids or neural networks. Discrete methods for online ESDF representation, such as Voxblox [6], FIESTA [3], and Voxfield [7], have gained popularity, but require interpolation to produce continuous ESDF values. In contrast, neural networks can represent ESDFs in a continuous manner [8]. HIO-SDF [9] introduced an incremental, online, and global ESDF model represented by a Sinusoidal Representation Neural Network (SIREN) [10]. Compared to iSDF [8], HIO-SDF can capture finer details, producing smoother surfaces and incorporating more geometric information throughout the environment.\n\nRegarding path and trajectory planning for drones, the traditional approaches are sampling-based or searching-based planners. They can generate optimal paths, but overlook path safety, which makes them undesirable for real-world operations in occluded spaces without post-processing. There are several state-of-the-art entries showing that ESDFs can be very convenient and useful tools for path planning methods [4, 11, 12, 13]. Heuristic search planners, by integrating ESDFs and leveraging their properties, can inherently address the safety problem and have demonstrated the capability to compute feasible, safe, and fast paths. However, these discretized paths, defined by a sequence of intermediate waypoints, are neither continuous nor smooth, and do not take into account kinematic or dynamic constraints [14].\nTrajectory planning through non-linear optimization is the next step, as it results in trajectories that can comply to complex restrictions, which can be assessed in the form of cost functions and can be easily customized depending on the needs of the user. Although trajectory replanning has been widely investigated, most methods depend on discretized ESDFs with interpolation, limiting gradient accuracy and trajectory quality.\n\nThis work focuses on performing local continuous 3D trajectory planning using non-linear optimization directly on an online generated neural ESDF. We present a framework that starts by building the drone’s environment representation using 3D LIDAR measurements to train a SIREN-like network, based on the network described in [9]. The framework then leverages the properties of the neural representation to perform trajectory optimization on that online ESDF, taking into account distance to obstacles as an indication of safety in addition to other restrictions.\n\nThe main contribution of the proposed framework is to maintain a level of computational optimization sufficient for it to be suitable for use in trajectory replanning, while highlighting its robustness and flexibility compared to existing approaches. The novelty of the framework lies in the optimization of continuous trajectories over continuous ESDFs that are updated online.\n\nWe have organized this paper into six sections. Section II describes the current state of the art. Section III provides an overview of the framework implemented. Section IV provides a detailed description of the trajectory planner on the neural ESDF. The experimental validation can be found in Section V and conclusions are presented in Section VI.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在动态和复杂环境中进行连续的3D轨迹优化？  \n2. 如何利用神经欧几里得有符号距离场（ESDF）提高轨迹规划的安全性和效率？  \n3. 如何实现对障碍物距离的实时更新以优化轨迹？  \n\n【用了什么创新方法】  \n本研究提出了一种新颖的框架C-3TO，通过在线神经ESDF实现连续的3D轨迹优化。该方法使用第五阶多项式表示平滑轨迹，并通过两阶段非线性优化管道，平衡效率、安全性和光滑性。实验结果表明，C-3TO能够生成碰撞感知和动态可行的轨迹，并且其灵活性使其能够适应不同用户需求而不影响性能。通过将连续轨迹参数化与持续更新的神经ESDF相结合，C-3TO为安全高效的局部重规划奠定了稳健且可推广的基础。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots",
            "authors": "Surov Maksim",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20082",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20082",
            "arxiv_html_link": "https://arxiv.org/html/2509.20082v1",
            "abstract": "This paper presents a control methodology for achieving orbital stabilization\nwith simultaneous time synchronization of periodic trajectories in\nunderactuated robotic systems. The proposed approach extends the classical\ntransverse linearization framework to explicitly incorporate time-desynchronization\ndynamics. To stabilize the resulting extended transverse dynamics,\nwe employ a combination of time-varying LQR and sliding-mode control.\nThe theoretical results are validated experimentally through the implementation\nof both centralized and decentralized control strategies on a group\nof six Butterfly robots.",
            "introduction": "The problem of trajectory tracking for underactuated robots has been\naddressed in a series of publications [1, 2, 3, 4, 5, 6, 7].\nMost of these works focus on designing control algorithms for orbital\nstabilization, where the system state converges to a reference periodic\ntrajectory up to a phase shift. Formulating the control objective\nin this way has enabled the development of algorithms that have demonstrated\neffectiveness in real-world applications [8, 9, 10, 11].\nHowever, in some practical scenarios, orbital asymptotic stability\nalone may be insufficient. For example, in cooperative or synchronized\ntasks involving multiple underactuated robots, it may be necessary\nto ensure asymptotic stability of the full state rather than only\nthe orbit itself, particularly when the robots share the same clock.\n\nA straightforward method for tracking a reference trajectory is based\non linearization of the tracking error dynamics, followed by the design\nof an LQR for the resulting linear time-varying (LTV) system. This\nmethod is described in Chapter 12 of [12] and has been\nshown to achieve asymptotic stability for small tracking errors, as\ndemonstrated in experiments with a triple pendulum on a cart [13].\nCompared to orbital stabilization methods, this approach is sensitive\nto initial time shifts, and the control system may lose stability\nif the robot becomes desynchronized.\n\nAlternative approaches for synchronization of closed orbits in underactuated\nrobots involve modifications of orbital tracking algorithms to ensure\nsynchronization between robots in a group [2, 14, 15, 16, 17].\nFor example, in [2], the authors employ transverse\nlinearization of the dynamics of a group of three robots to design\na centralized control law that achieves synchronization. In [17]\norbital stabilization together with synchronization is attained using\nthe dynamic virtual holonomic constraints approach. In [15],\nthe authors propose an ad-hoc modification of the transverse–linearization\napproach, and demonstrate its effectiveness experimentally on the\nsynchronization of two real robots.\n\nOur method for orbital stabilization with simultaneous time synchronization\nalso represents a modification of orbital stabiliation feedback. It\nbuilds on the transverse linearization framework [18, 2, 1].\nFor a given periodic trajectory, we augment the transverse dynamics\nwith the dynamics of robot desynchronization, defined as the difference\nbetween the physical time and the reference time corresponding to\nthe “closest” point on the trajectory. As we show, the linearization\nof this extended transverse dynamics takes the form of an LTV system,\nwhich can be stabilized using a combination of LQR and sliding-mode\ncontrol, similarly to [7]. The resulting feedback\nlaw naturally decomposes into an orbital stabilization component and\na synchronization component. The synchronization term is bounded,\nwith the desynchronization variable entering through the signum function.\nThis structure allows the control law to preserve the benefits of\norbital stabilization while providing bounded corrective actions,\neven for large desynchronizations.\n\nThe remainder of the paper is organized as follows. Section II\nformulates the problem of periodic trajectory tracking for a class\nof nonlinear control systems. Section III\nbriefly reviews the orbital-stabilization algorithm based on the transverse\nlinearization approach. The main results are presented in Section IV,\nwhere the transverse dynamics are extended with a desynchronization\nvariable and two methods for extended dynamics stabilization are proposed.\nThe first method applies an LQR design to linearization of the extended\ntransverse dynamics, while the second employs a sliding-mode control\nmethodology. Section V reports experimental\nresults obtained on a group of six butterfly robots [26].\nConcluding remarks are provided in Section VI.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在欠驱动机器人中实现轨迹的轨道稳定性与时间同步。  \n2. 现有方法在处理多机器人同步任务时的局限性。  \n3. 需要在轨道稳定性之外确保全状态的渐近稳定性。  \n\n【用了什么创新方法】  \n本研究提出了一种控制方法，通过扩展经典的横向线性化框架，结合时间变化的LQR和滑模控制，实现欠驱动机器人周期轨迹的轨道稳定性与时间同步。通过将机器人去同步化动态纳入考虑，形成了一个线性时变系统，从而能够同时处理轨道稳定性和同步性。实验结果表明，该方法在六个蝴蝶机器人上有效验证了理论成果，展示了良好的稳定性和同步性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping",
            "authors": "Jose E. Maese,Luis Merino,Fernando Caballero",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20081",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20081",
            "arxiv_html_link": "https://arxiv.org/html/2509.20081v1",
            "abstract": "This paper presents a high-efficiency, CPU-only volumetric mapping framework based on a Truncated Signed Distance Field (TSDF). The system incrementally fuses raw LiDAR point-cloud data into a voxel grid using a directional bitmask-based integration scheme, producing dense and consistent TSDF representations suitable for real-time 3D reconstruction. A key feature of the approach is that the processing time per point-cloud remains constant, regardless of the voxel grid resolution, enabling high resolution mapping without sacrificing runtime performance. In contrast to most recent TSDF/ESDF methods that rely on GPU acceleration, our method operates entirely on CPU, achieving competitive results in speed. Experiments on real-world open datasets demonstrate that the generated maps attain accuracy on par with contemporary mapping techniques. The source code is publicly available at https://github.com/robotics-upo/DB-TSDF",
            "introduction": "Volumetric mapping is a fundamental capability in mobile robotics, supporting tasks such as collision avoidance, motion planning, and the construction of consistent world models under real-time constraints. Point clouds and occupancy grids remain widely used on CPU-only platforms, as their simple data structures allow efficient processing without specialized hardware. However, they are prone to aliasing at high resolutions and often produce geometric artifacts that hinder downstream processing. Truncated Signed Distance Fields (TSDFs) address these limitations by storing per-voxel distances to the nearest surface and providing smooth proximity information. Despite their advantages, many existing TSDF and ESDF (Euclidean Signed Distance Fields) pipelines rely heavily on GPU acceleration or exhibit computational costs on the CPU that grow unfavorably with map resolution and update rate.\n\nThis work introduces DB-TSDF, a mapping method that integrates TSDFs using a directional bitmask representation specifically designed for fast operation on a discrete voxel grid using only the CPU. Each voxel encodes a compact 32-bit distance mask, a sign flag, and a hit counter. For each LiDAR return, the system selects a precomputed, direction-dependent kernel applied over a fixed neighborhood. A single bitwise AND operation per voxel updates the mask, while a directional shadow mechanism assigns occupied or free-space evidence. Since the kernel size remains constant, the integration time per scan is bounded and remains largely unaffected by the total grid dimensions. Increasing resolution increases memory usage but does not compromise real-time performance. The implementation is fully parallelized using multi-threading and relies exclusively on integer operations.\n\nDB-TSDF builds upon the Truncated Distance Field mapping backend initially developed for the D-LIO framework [1], which was primarily focused on localization. In this work, the mapping formulation is expanded and refined: the field representation is extended from unsigned to signed distances, directional evidence accumulation is introduced, and the memory layout is optimized for improved cache efficiency. These changes result in a high-resolution mapping method capable of maintaining stable runtimes on CPU-constrained platforms and delivering robust performance even in feature-sparse environments.\n\nThe method is evaluated on public LiDAR datasets, with results reported in terms of geometric accuracy, runtime, and the trade-off between resolution, and update latency. The experiments show that DB-TSDF achieves mapping quality comparable to established volumetric approaches while maintaining competitive performance on CPU at resolutions typically requiring GPU acceleration. An example of such high-resolution reconstruction is shown in Figure 1, generated from the Newer College dataset.\n\nThe main contributions of this work are: (i) a TSDF integration scheme based on directional kernels and bitmask distance encoding, implemented entirely on CPU; (ii) a mapping approach with constant per-scan computational cost that is independent of the global grid size; (iii) a signed and memory-efficient voxel structure designed for high-resolution mapping and ROS2 integration; and (iv) an experimental evaluation that quantifies the method’s accuracy, speed, and memory requirements.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在CPU上实现高效的体积映射，避免GPU加速的依赖。  \n2. 如何在高分辨率下保持实时性能，同时减少计算成本和内存使用。  \n3. 如何利用方向性位掩码提高TSDF的集成效率和准确性。  \n\n【用了什么创新方法】  \n本研究提出了一种基于方向位掩码的TSDF集成方案，专门设计用于在离散体素网格上快速操作。每个体素编码了一个紧凑的32位距离掩码、符号标志和命中计数器。通过预计算的方向依赖内核，系统对每个LiDAR返回进行处理，使用位与操作更新掩码，确保每次扫描的集成时间保持恒定。该方法在公共LiDAR数据集上的实验表明，DB-TSDF在几何准确性和运行速度上与现有的体积映射方法相当，同时在CPU平台上实现了高分辨率映射，展示了其在特征稀疏环境中的稳健性能。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning",
            "authors": "Xun Li,Rodrigo Santa Cruz,Mingze Xi,Hu Zhang,Madhawa Perera,Ziwei Wang,Ahalya Ravendran,Brandon J. Matthews,Feng Xu,Matt Adcock,Dadong Wang,Jiajun Liu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20077",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20077",
            "arxiv_html_link": "https://arxiv.org/html/2509.20077v1",
            "abstract": "To enable robots to comprehend high-level human instructions and perform complex tasks, a key challenge lies in achieving comprehensive scene understanding: interpreting and interacting with the 3D environment in a meaningful way. This requires a smart map that fuses accurate geometric structure with rich, human-understandable semantics.\nTo address this, we introduce the 3D Queryable Scene Representation (3D QSR), a novel framework built on multimedia data that unifies three complementary 3D representations: (1) 3D-consistent novel view rendering and segmentation from panoptic reconstruction, (2) precise geometry from 3D point clouds, and (3) structured, scalable organization via 3D scene graphs. Built on an object-centric design, the framework integrates with large vision-language models to enable semantic queryability by linking multimodal object embeddings, and supporting object-level retrieval of geometric, visual, and semantic information. The retrieved data are then loaded into a robotic task planner for downstream execution.",
            "introduction": "For robots to perform complex tasks in 3D environments under human instruction, they must relate high-level semantics in natural language commands to actual content in their surrounding environment. Even a simple instruction such as “Robot, I’m thirsty.” demands the ability to infer intent, locate relevant items (e.g., a water bottle), assess affordances, and plan a path for retrieval. Although trivial for humans, the tasks are exceptionally challenging for robots, as they must simultaneously reason about the spatial structure and semantic meaning of the environment based on human queries. While it is essential to enhance the robot’s intelligence for navigating and manipulating complex environments, it is equally important to make the environment more understandable. We address this dual necessity by introducing the concept of a queryable 3D scene representation (QSR), which embeds intelligence directly into the scene. This enables both robots and humans to interact with their surroundings in a more collaborative, context-aware, and semantically grounded manner.\n\nTraditional 3D maps for robotic systems, such as voxel-based occupancy grids, point clouds, and mesh models (Fredriksson et al., 2024; Liu, 2015; Chen et al., 2021; Bandyopadhyay et al., 2024), are predominantly geometric and often constructed using SLAM algorithms (Taheri and Xia, 2021; Ramezani et al., 2022). However, these representations lack the semantic information necessary for understanding and interacting with the scene. Semantic understanding, on the other hand, is typically derived from 2D object detection/segmentation models. The central challenge is aligning 2D semantics with 3D geometry to form a unified representation that enables complex reasoning and interaction. Moreover, human queries often span multiple levels of granularity and conceptual domains (e.g., “a pillow with a tree pattern”), requiring far richer semantics than conventional models can provide.\nFinally, human understanding of environments is inherently structural, involving hierarchical organisation and inter-object relationships. Capturing and reflecting the structural organisation in the map is essential for enhancing analytical capabilities and enabling more intuitive interaction with complex environments.\n\nTo address these challenges, we introduce 3D QSR, a scene-understanding multimodal framework built using multimedia data. It combines state-of-the-art 3D reconstruction techniques, such as NeRF (Mildenhall et al., 2021) and point clouds, with advanced semantic understanding through panoptic segmentation and vision-language embeddings. We also incorporate a 3D scene graph as an abstract layer, providing a structured, explicit, and lightweight representation enriched with object properties and inter-object relationships. Unlike single-modality systems, 3D QSR supports object-level queries involving location, appearance, function, and relational context, significantly enhancing scene understanding and interaction.\nWith a Large Language Model (LLM), our framework supports advanced language querying and reasoning grounded in QSR content. We demonstrate the capability of this representation through various downstream robotic task planning scenarios simulated in Unity (Juliani, 2018) using the Replica dataset (Straub et al., 2019). In summary, the 3D QSR framework provides:\n\nUnified alignment of semantic, geometric, and structural information, enabling robots to reason over spatial and semantic context simultaneously.\n\nUnified alignment of semantic, geometric, and structural information, enabling robots to reason over spatial and semantic context simultaneously.\n\nNatural language-driven interaction, supporting intuitive query-answering for object retrieval.\n\nComprehensive support for robotic task planning, such as autonomous navigation, object retrieval, and adaptive decision-making in complex scenarios.\n\n1. Unified alignment of semantic, geometric, and structural information, enabling robots to reason over spatial and semantic context simultaneously.\n\n2. Natural language-driven interaction, supporting intuitive query-answering for object retrieval.\n\n3. Comprehensive support for robotic task planning, such as autonomous navigation, object retrieval, and adaptive decision-making in complex scenarios.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现机器人对3D环境的全面理解，以执行复杂任务。  \n2. 如何将2D语义信息与3D几何信息对齐，形成统一的表示。  \n3. 如何支持自然语言驱动的交互，以增强机器人与环境的互动能力。  \n\n【用了什么创新方法】  \n提出了3D Queryable Scene Representation (3D QSR)框架，该框架结合了3D重建技术、全景分割和视觉-语言嵌入，形成了一个多模态的场景理解系统。通过3D场景图的引入，3D QSR能够支持对象级查询，增强场景理解和交互能力。该框架通过与大型语言模型的结合，实现了基于自然语言的查询和推理，展示了在复杂场景下的机器人任务规划能力。实验结果表明，3D QSR显著提高了机器人在复杂环境中的导航、物体检索和决策能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "LLM Trainer: Automated Robotic Data Generating via Demonstration Augmentation using LLMs",
            "authors": "Abraham George,Amir Barati Farimani",
            "subjects": "Robotics (cs.RO)",
            "comment": ". Submitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.20070",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20070",
            "arxiv_html_link": "https://arxiv.org/html/2509.20070v1",
            "abstract": "We present LLM Trainer, a fully automated pipeline that leverages the world knowledge of Large Language Models (LLMs) to transform a small number of human demonstrations (as few as one) into a large robot dataset for imitation learning. Our approach decomposes demonstration generation into two steps: (1) offline demonstration annotation that extracts keyframes, salient objects, and pose–object relations; and (2) online keypose retargeting that adapts those keyframes to a new scene, given an initial observation. Using these modified keypoints, our system warps the original demonstration to generate a new trajectory, which is then executed, and the resulting demo, if successful, is saved. Because the annotation is reusable across scenes, we use Thompson sampling to optimize the annotation, significantly improving generation success rate. We evaluate our method on a range of tasks, and find that our data annotation method consistently outperforms expert-engineered baselines. We further show an ensemble policy that combines the optimized LLM feed-forward plan with a learned feedback imitation learning controller. Finally, we demonstrate hardware feasibility on a Franka Emika Panda robot. For additional materials and demonstration videos, please see the project website: https://sites.google.com/andrew.cmu.edu/llm-trainer",
            "introduction": "Recent advances in Large Language Models (LLMs) have revolutionized the field of robot learning, with applications ranging from task planning [1], to tool use in long horizon tasks [2], to deformable object manipulation [3].\nAt the core of these works is the LLM’s broad base of world knowledge, gathered from training on internet-scale data, which allows these agents to be extremely generalizable. In this work, we seek to leverage the world knowledge of LLMs to fully automate demonstration generation through human demo augmentation. To do this, we employ a similar pipeline as [4] and [5] for data generation: first, identify key robot poses in a demonstration, then generate a new environment and modify the key poses based on an initial observation, and finally, use the new key poses to warp the demonstration trajectory, resulting in a new trajectory, which is rolled out in the new environment. However, unlike prior works which rely on human annotation and hard-coded methods to identify key poses and modify them in response to the new environments [4, 5, 6], our system seeks to fully automate this process by leveraging large language models (LLMs). An outline of our method can be seen in Fig. 1.\n\nOur method for LLM-based data generation has two main steps: First, the LLM annotates the human demonstration, identifying keyframes (timesteps that are important inflection points for the task), listing relevant objects at each keyframe, and explaining the relationship between the robot and these objects. Second, the LLM uses this annotation, along with an initial observation of a newly initialized scene, to determine how the robot’s pose should be adjusted at each keypoint. Because the first step of this process does not require information from the new scene, we can reuse these annotations, saving compute cost and opening the door for optimization. By employing a multi-armed bandit-based method, we are able to optimize the demo annotation step, improving data generation success rate by 2-3 times.\n\nOnce the data generation process is complete, we can use the generated data to train imitation learning agents. Additionally, thanks to our annotation optimization process, we develop a highly effective LLM-based feed-forward policy during data generation. In addition to serving as a viable agent on its own, we show that this feed-forward policy, when combined with the feedback agent, can form an effective ensembled policy, combining the long-horizon planning and generalizability of LLMs with the feedback control of imitation learning.\n\nThis work has three main contributions:\n\nAn LLM-based data generation method which can autonomously generate data using only a single, unannoted demonstration and a short (one sentence) description of the task.\n\nA multi-armed bandit-based optimization method which significantly improves demo generation success rate, allowing our method to outperform baselines that rely on expert annotations.\n\nAn ensembling strategy to combine a learned IL policy with the optimized LLM-based feedforward controller developed during data collection.\n\n1. An LLM-based data generation method which can autonomously generate data using only a single, unannoted demonstration and a short (one sentence) description of the task.\n\n2. A multi-armed bandit-based optimization method which significantly improves demo generation success rate, allowing our method to outperform baselines that rely on expert annotations.\n\n3. An ensembling strategy to combine a learned IL policy with the optimized LLM-based feedforward controller developed during data collection.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用LLMs自动生成机器人模仿学习所需的数据？  \n2. 如何优化演示生成过程以提高成功率？  \n3. 如何将LLM生成的策略与反馈控制策略结合以提升性能？  \n\n【用了什么创新方法】  \n本研究提出了LLM Trainer，一个自动化的数据生成管道，利用大型语言模型（LLMs）将少量人类演示转化为大量机器人数据。该方法分为两个步骤：首先，通过LLM进行离线演示注释，提取关键帧、显著对象和姿态-对象关系；其次，进行在线关键姿态重定向，根据初始观察调整关键帧。通过使用Thompson采样优化注释过程，成功率显著提高。实验结果表明，该方法在多项任务中优于专家设计的基线，并展示了将优化的LLM前馈计划与学习的反馈模仿学习控制器结合的有效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping",
            "authors": "Yinzhao Dong,Ji Ma,Liu Zhao,Wanyue Li,Peng Lu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20036",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20036",
            "arxiv_html_link": "https://arxiv.org/html/2509.20036v1",
            "abstract": "Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have demonstrated impressive performance on challenging terrains, allowing robots to execute complex skills such as climbing, running, and jumping. However, existing blind locomotion controllers often struggle to ensure safety\nand efficient traversal through risky gap terrains, which are typically highly complex, requiring robots to perceive terrain information and select appropriate footholds during locomotion accurately. Meanwhile, existing perception-based controllers still present several practical limitations, including a complex multi-sensor deployment system and expensive computing resource requirements. This paper proposes a DRL controller named MAstering Risky Gap Terrains (MARG), which integrates terrain maps and proprioception to dynamically adjust the action and enhance the robot’s stability in these tasks. During the training phase, our controller accelerates policy optimization by selectively incorporating privileged information (e.g., center of mass, friction coefficients) that are available in simulation but unmeasurable directly in real-world deployments due to sensor limitations. We also designed three foot-related rewards to encourage the robot to explore safe footholds. More importantly, a terrain map generation (TMG) model is proposed to reduce the drift existing in mapping and provide accurate terrain maps using only one LiDAR, providing a foundation for zero-shot transfer of the learned policy. The experimental results indicate that MARG maintains stability in various risky terrain tasks.",
            "introduction": "Legged robots have significantly advanced locomotion capabilities, demonstrating impressive skills across various movement modes, such as climbing stairs [1, 2], descending ramps [3], high-speed running [4], parkour [5], bipedal locomotion [6], and backflipping [7]. These abilities enable robots to perform well in continuous and highly challenging terrains, including rugged mountain paths, narrow passages, stairwells, slippery or unstable surfaces, etc. However, existing blind locomotion controllers often struggle to overcome risky gap terrains due to shortcomings in ensuring the safety and balance of quadruped robots.\n\nRisky gap terrains exhibit numerous complex characteristics, imposing nearly stringent demands on robots regarding footholds and balance capabilities during locomotion. As shown in Fig. 1 (a), robots must not only strive to maintain the stability of their center of gravity on a narrow single-plank bridge but also respond in real time to potential lateral disturbances. Once a robot makes errors during locomotion, such as slipping or shifting its center of gravity, it may quickly step on the air or lose stability, leading to a fall and potentially causing severe damage to the robot. When traversing balance beams, the quadruped robot must not only accurately perceive terrain information such as height variations, gap width, and edges, but also select appropriate landing footholds and timing for exertion based on its locomotion capabilities and current state to avoid missteps, as shown in Figs. 1 (b-c).\n\nThe majority of existing quadrupedal locomotion controllers are blind, which means that they do not utilize perception sensors like cameras and LiDARs [8, 9, 10, 11, 12]. It is nearly impossible for these controllers to traverse risky terrains as shown in Fig. 1. Recently, perception sensors have been used to obtain an elevation map of the environment [13, 1]. However, they do not take risky terrains into consideration. Only a few studies consider risky terrains, and they either rely on multiple sensors [14], which significantly increases the complexity of hardware deployment, or use motion capture systems to obtain prior information about the terrain [15]. In this paper, we only use one sensor to construct a robot-centered map and do not rely on motion capture systems.\n\nExisting model-based controllers rely on precise modeling of robots to calculate the optimal joint torques or footholds required for locomotion. For example, Singh et al. [16] compute second-order derivatives of rigid-body inverse and forward dynamics, achieving significant speed-ups over automatic differentiation in optimization-driven robot control. The CAFE-MPC framework [17] employs a cascaded-fidelity model predictive control scheme paired with a tuning-free whole-body controller, enabling quadruped robots to execute agile maneuvers without manual parameter tuning. Meduri et al. [18] splits the nonlinear MPC problem into biconvex centroidal dynamics and full-body kinematics, enabling real-time generation of dynamic whole-body motions for legged robots. These models can generate accurate control commands, enabling the robot to achieve stable and efficient locomotion in an ideal simulation and simple terrains [19]. However, uncertainty factors in real-world environments, such as terrain irregularity, changing friction, and external disturbances, present significant challenges to model-based methods. These factors are difficult to accurately incorporate into models, leading to potential mismatches between the model and the real world. Even slight discrepancies can cause robot locomotion failures, especially in risky gap terrains.\n\nTo address these challenges, researchers [20, 21] have attempted to simplify the dynamics model by utilizing Nonlinear Model Predictive Control (NMPC) to enhance the locomotion of robots in complex and dynamic environments. Yin et al. [22] propose an optimization algorithm to improve the robot’s locomotion performance by transforming the discrete terrain height map into a continuous cost map to adjust the footholds dynamically. [23] proposes a novel control system that integrates adaptive control into a force-based control system for legged robots, enabling them to dynamically locomotion on uneven terrains. However, the computational complexity and slow convergence rates limit the applicability of robots in dynamic environments.\n\nIn addition, studies [24, 25] are also exploring the use of multiple sensors to enhance the accuracy and reliability of the model. Alongside the robot’s inertial measurement unit (IMU), external perception devices such as depth cameras and LiDARs are employed to gather environmental information, including terrain width, height, and edge shape, and integrate this information into the dynamic model to assist robot control [26]. The synchronization of sensor data, the design of fusion algorithms for different sensor inputs, and the computational burden of data processing will further adversely affect the real-time control performance of robots.\n\nModel-free methods, such as deep reinforcement learning (DRL), have shown promise in enabling legged robots to adapt to complex terrains without relying on precise dynamic models. These methods focus on training robots to learn optimal policies through trial and error, allowing them to manage uncertainties and dynamic changes in their environment effectively [27]. The blind locomotion controllers [2, 10] have shown impressive progress in enabling robots to traverse challenging continuous terrains. However, these controllers often struggle in risky terrains due to the absence of environmental perception.\n\nIntegrating data from other external sensors, such as depth cameras, motion capture, etc, into the DRL framework is an effective way to help robots comprehensively understand their surrounding environment. Pioneering works [28, 5, 29] utilize deep learning models, such as GRU [30] and LSTM [31], to process depth images and extract terrain features, including height, slope, and distribution. Robots can successfully perform high-difficulty parkour tasks by incorporating these terrain factors into their decision-making processes. Challenges such as lighting changes, occlusion issues, high dimensionality, and complexity of images [32] may lead to inaccurate terrain feature extraction or high computational complexity during the training process, ultimately affecting the real-time performance of the robot [33]. Meanwhile, [34] and [15] use motion capture and an offline map to derive the height map around the robot’s feet, which limits the practical applicability of this algorithm.\n\nTo obtain more accurate terrain information in the real world, previous DRL controllers [13, 35, 14] utilize multiple depth cameras or LiDARs simultaneously for elevation mapping, which can significantly enhance the accuracy of terrain representation. However, this approach increases the complexity of hardware deployment, as it requires sophisticated processing capabilities to handle the data from multiple sensors. Additionally, existing localization technologies  [36, 37, 38, 39] heavily rely on the pose estimation of floating bases within the global frame. Any inaccuracy in this estimation may lead to map drift, thereby affecting the movement of legged robots in risky terrains. Thus, designing safe and reliable controllers, developing efficient algorithms to simplify deployment processes, and obtaining precise terrain maps remain challenging in risky gaps tasks.\n\nIn summary, we propose a DRL controller for quadrupedal locomotion—MAstering Risky Gap (MARG)—which integrates terrain maps, privileged information, and proprioceptive into the policy to enhance the locomotion performance of quadrupedal robots in risky terrains. The key contributions of this work can be listed as follows:\n\nWe propose a safe and robust robot controller for locomotion, which can predict the body velocity and the contact state of feet on each step, significantly enhancing the robot’s stability in risky gap terrains.\n\nFor risky tasks, we have designed three foot-related rewards: feet air time, feet stumble, and feet center, which promote the policy to explore safe footholds, enhancing the safety of movement.\n\nWe propose a terrain map generation model that uses a single LiDAR to obtain the robot-centered height map. Our method minimizes drift compared to the traditional localization approaches while achieving zero-shot transfer capability and optimal computational efficiency.\n\nThe MARG controller empowers quadruped robots to adeptly handle risky gap terrains in the real world, including 65 cm large gaps, 18 cm narrow single-plank bridges, and balance beams with varying sizes, heights, and inclinations.\n\n1. We propose a safe and robust robot controller for locomotion, which can predict the body velocity and the contact state of feet on each step, significantly enhancing the robot’s stability in risky gap terrains.\n\n2. For risky tasks, we have designed three foot-related rewards: feet air time, feet stumble, and feet center, which promote the policy to explore safe footholds, enhancing the safety of movement.\n\n3. We propose a terrain map generation model that uses a single LiDAR to obtain the robot-centered height map. Our method minimizes drift compared to the traditional localization approaches while achieving zero-shot transfer capability and optimal computational efficiency.\n\n4. The MARG controller empowers quadruped robots to adeptly handle risky gap terrains in the real world, including 65 cm large gaps, 18 cm narrow single-plank bridges, and balance beams with varying sizes, heights, and inclinations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高四足机器人在复杂和危险的间隙地形中的稳定性和安全性。  \n2. 现有的盲目运动控制器在感知和选择合适的落脚点方面的局限性。  \n3. 如何减少传统定位方法中的地图漂移并实现零-shot转移能力。  \n\n【用了什么创新方法】  \n本文提出了一种名为MAstering Risky Gap Terrains (MARG)的深度强化学习控制器，集成了地形图和本体感知，以动态调整行动并增强机器人在危险地形中的稳定性。通过选择性地整合特权信息（如重心和摩擦系数），加速了策略优化。设计了三种与脚相关的奖励机制，以鼓励机器人探索安全的落脚点。此外，提出了一种地形图生成模型，仅使用一个LiDAR来生成准确的地形图，从而减少了映射中的漂移。实验结果表明，MARG在各种危险地形任务中保持了稳定性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "An effective control of large systems of active particles: An application to evacuation problem",
            "authors": "Albina Klepach,Egor E. Nuzhin,Alexey A. Tsukanov,Nikolay V. Brilliantov",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19972",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19972",
            "arxiv_html_link": "https://arxiv.org/html/2509.19972v1",
            "abstract": "Manipulation of large systems of active particles is a serious challenge across diverse domains, including crowd management, control of robotic swarms, and coordinated material transport. The development of advanced control strategies for complex scenarios is hindered, however, by the lack of scalability and robustness of the existing methods, in particular, due to the need of an individual control for each agent.\nOne possible solution involves controlling a system through a leader or a group of leaders, which other agents tend to follow. Using such an approach we develop an effective control strategy for a leader, combining reinforcement learning (RL) with artificial forces acting on the system. To describe the guidance of active particles by a leader we\nintroduce the generalized Vicsek model.\nThis novel method is then applied to the problem of\nthe effective evacuation by a robot-rescuer (leader) of large groups of people from hazardous places. We demonstrate, that while a straightforward application of RL yields suboptimal results, even for advanced\narchitectures, our approach provides a robust and efficient evacuation strategy.\nThe source code supporting this study is publicly available at: https://github.com/cinemere/evacuation.",
            "introduction": "The manipulation of large systems of active particles, especially controlling their collective behavior, has become a fundamental problem in recent decades, initiated by emerging new areas of application.\nThe examples, across diverse domains,\ninclude crowd management [1, 2], controlling of robotic swarms\n[3], coordinated material transport [4], etc. The concept of active matter [5] offers an appropriate framework for modeling such ensembles,\ncomprised of agents that consume energy to move, interact with their environment, and adjust their direction of motion subject to external signals [6, 7, 8, 9].\nSynthetic microswimmers, swarming robots, colonies of bacteria, fish schools, flocks of birds, groups of humans [10, 9] – all these systems of living and non-living agents represent an active matter.\n\nThe primary goals of controlling active particle systems include navigating them in complex environments and supporting synchronized collective behavior [11, 12, 13]. This can be addressed by classical optimal control theory, which requires a complete knowledge of the environment and dynamics (similar to Zermelo’s navigation problem [14]), or by reinforcement learning (RL), where an agent learns strategies through trial-and-error [15, 16]. The latter is particularly applicable when an agent has only partial, local information or when the impact of noise is significant. A key limitation of such control problems\nis that their solution relies on the individual manipulation of each participating particle. For ensembles comprising hundreds of agents a straightforward application of RL, with individual-level control, becomes computationally intractable.\n\nTo overcome this limitation, a common strategy involves guiding a group through a leader or multiple leaders, when active particles just follow the leader(s) [17, 18, 19, 20]. This leader-follower paradigm significantly simplifies the control problem by focusing strategies solely on the leader(s). Nevertheless, complexities persist, particularly when managing multiple groups of different size and location, or when groups cannot be manipulated simultaneously or possess non-coinciding goals [21, 22]. Among\nimportant examples of such challenging problems is an effective guidance of large groups of people, especially – their evacuation from hazardous places. Here a leader (rescuer)\nhas the goal to evacuate all people\nguiding them to the exit(s) in shortest time.\nGiven the inherent randomness and non-negligible noise within these systems, reinforcement learning (RL) seems to be the most\nsuitable approach. However, a straightforward application of standard RL\nis neither computationally efficient nor effective in achieving the ultimate goal.\n\nTo this end, we propose an application of auxiliary (artificial)\n“pseudo-gravitational” forces acting on the system, as a part of the environment; this helps the leader to find the most effective guidance strategy.\nSuch a combination of RL with artificial forces [23] results in a very effective method to control large ensembles of active particles.\nTo implement our new method for the evacuation problem, we utilize the generalized Vicsek model. This model takes into account\nnot only interactions between active particles\n(as in the conventional Vicsek model [5]), but also between particles and the leader.\n\nThe rest of the paper is organized as follows. In the next Sec. II we formulate the problem of an effective evacuation from a hazardous place by an informed rescuer-leader. In Sec. III we describe the methods for training optimal evacuation policy and propose an approach based on “pseudo-gravitational” forces, viz the new method of RL with artificial fields. In Sec. IV we report the results of our numerical experiments. Finally, in Sec. V, we summarize our findings.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效控制大规模活性粒子系统以实现集体行为的协调。  \n2. 如何在复杂环境中进行有效的人员疏散，特别是在危险场所的疏散。  \n3. 现有方法在处理多个群体时的可扩展性和鲁棒性问题。  \n\n【用了什么创新方法】  \n本研究提出了一种结合强化学习（RL）与人工“伪引力”力的控制策略，通过引导者控制活性粒子群体的行为。利用广义Vicsek模型，研究者能够有效地指导大规模人群的疏散。实验结果表明，尽管传统RL方法效果不佳，采用新方法后，疏散策略表现出显著的鲁棒性和效率。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Generalist Robot Manipulation beyond Action Labeled Data",
            "authors": "Alexander Spiridonov,Jan-Nico Zaech,Nikolay Nikolov,Luc Van Gool,Danda Pani Paudel",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted at Conference on Robot Learning 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.19958",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19958",
            "arxiv_html_link": "https://arxiv.org/html/2509.19958v1",
            "abstract": "Recent advances in generalist robot manipulation leverage pre-trained Vision–Language Models (VLMs) and large-scale robot demonstrations to tackle diverse tasks in a zero-shot manner. A key challenge remains: scaling high-quality, action-labeled robot demonstration data, which existing methods rely on for robustness and generalization. To address this, we propose a method that benefits from videos without action labels—featuring humans and/or robots in action—enhancing open-vocabulary performance and enabling data-efficient learning of new tasks. Our method extracts dense, dynamic 3D point clouds at the hand or gripper location and uses a proposed 3D dynamics predictor for self-supervision. This predictor is then tuned to an action predictor using a smaller labeled dataset for action alignment. We show that our method not only learns from unlabeled human and robot demonstrations—improving downstream generalist robot policies—but also enables robots to learn new tasks without action labels (i.e., out-of-action generalization) in both real-world and simulated settings.",
            "introduction": "Robust zero-shot manipulation across diverse tasks and environments is one of the biggest bottlenecks towards truly autonomous robots. Inspired by the open-world reasoning capabilities of Large-Language (LLM) and Vision-Language Models (VLM), Vision-Language-Action (VLA) models have emerged for generalist robot manipulation. Approaching this challenge, VLAs extend the semantic reasoning abilities of VLMs with embodied understanding and adapt them for robotic control by training on large datasets of teleoperated robot demonstrations [1, 2, 3, 4, 5, 6, 7]. This has led to impressive progress in learning robust manipulation policies. However, most success is centered around in-domain settings, and performance quickly degrades as the tasks move outside the training distribution. While collecting yet larger robot datasets seems straightforward, it remains unclear what resources would be required to achieve generalist manipulation.\n\nMultimodal training with videos of human demonstrations is a promising alternative to prohibitively expensive robot demonstrations. Such videos contain valuable spatiotemporal information highly relevant to learning robotic control, are readily available at internet scale, and provide diverse tasks and environments. However, learning from human demonstration datasets comes with a range of challenges; Videos provide no direct action labels supervision, exhibit human-to-robot domain gaps, and include redundant or distracting features irrelevant to robotic control.\n\nBeing a fundamental challenge, learning motion priors from humans and unlabeled data has been widely explored. Yet existing work remains confined to specialist, small-scale policies. Some focus on visual representations [8, 9, 10, 11], not considering unseen motions. Others predict visual plans that require a bespoke inverse-dynamics model for execution [12, 13, 14, 15, 16, 17]. Another line retargets human hands to robot grippers [18, 19, 20, 21, 22], but suffers from a large domain gap.\n\nIn this work, we bridge this gap and present MotoVLA, a generalist robot manipulation policy that enables new tasks from human and robot videos without action labels. To achieve this, we propose a VLA model and two-stage training approach using a combination of large-scale labeled and unlabeled\n111Unlabeled refers to non-action-labeled, as action labels are the main challenge in acquiring manipulation data.\nhuman and robot videos.\nIn the first training stage, a dynamic point cloud predictor is trained on the unlabeled data, which establishes a common embodiment-agnostic action representation. Since the dynamic point cloud strongly correlates with the end-effector actions up to hand-eye calibration, the second stage training of an action expert on action-labeled data is simplified. This natural correspondence between dynamic point clouds and 3D robot actions makes our approach particularly effective for learning from unlabeled data. An overview of our method is shown in Figure 1.\nIn summary, our contributions are:\n\nMotoVLA, the first end-to-end VLA model that allows the use of unlabeled data for learning motion priors required for the generalist robot manipulation.\n\nA two-stage training approach enabling the use of dynamic point clouds as a common embodiment-agnostic representation, which is both scalable and intuitive.\n\nExtensive real and simulated evaluations of our model for in-domain, out-of-domain, and transfer learning tasks, demonstrating the effective use of unlabeled data by our model.\n\n1. MotoVLA, the first end-to-end VLA model that allows the use of unlabeled data for learning motion priors required for the generalist robot manipulation.\n\n2. A two-stage training approach enabling the use of dynamic point clouds as a common embodiment-agnostic representation, which is both scalable and intuitive.\n\n3. Extensive real and simulated evaluations of our model for in-domain, out-of-domain, and transfer learning tasks, demonstrating the effective use of unlabeled data by our model.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在缺乏高质量、标注动作的数据情况下实现通用机器人操作？  \n2. 如何利用无标签视频数据提升机器人在多样任务中的表现？  \n3. 如何解决人类与机器人之间的领域差距以实现有效学习？  \n\n【用了什么创新方法】  \n提出了MotoVLA，一个通用机器人操作策略，利用无标签的人类和机器人视频数据进行学习。方法包括一个两阶段的训练流程：第一阶段在无标签数据上训练动态点云预测器，建立通用的动作表示；第二阶段在小规模标注数据上训练动作专家，实现动作对齐。通过这种方式，MotoVLA能够有效地从无标签数据中学习运动先验，提升机器人在多种任务中的表现，尤其是在真实和模拟环境中的迁移学习任务。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation",
            "authors": "Pinhao Song,Yurui Du,Ophelie Saussus,Sofie De Schrijver,Irene Caprara,Peter Janssen,Renaud Detry",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19954",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19954",
            "arxiv_html_link": "https://arxiv.org/html/2509.19954v1",
            "abstract": "We propose a probabilistic shared-control solution for navigation, called Robot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe, effective assistance in human–robot interaction. RT-V2 jointly models a user’s long-term behavioral patterns and their noisy, low-dimensional control signals by combining a prior intent model with a posterior update that accounts for real-time user input and environmental context. The prior captures the multimodal and history-dependent nature of user intent using recurrent neural networks and conditional variational autoencoders, while the posterior integrates this with uncertain user commands to infer desired actions. We conduct extensive experiments to validate RT-V2 across synthetic benchmarks, human–computer interaction studies with keyboard input, and brain–machine interface experiments with non-human primates. Results show that RT-V2 outperforms the state of the art in intent estimation, provides safe and efficient navigation support, and adequately balances user autonomy with assistive intervention. By unifying probabilistic modeling, reinforcement learning, and safe optimization, RT-V2 offers a principled and generalizable approach to shared control for diverse assistive technologies. Code will be available in https://mousecpn.github.io/RTV2_page/.",
            "introduction": "Shared control is a collaborative approach between a human operator and a robot, designed to reduce operator workload and facilitate the more efficient and safer completion of complex tasks with the robot. This approach is widely used in various fields, such as subsea maintenance, surgery, driving, and assistive devices. In the context of assistive devices, it enables individuals with disabilities to regain autonomy through technologies like robotic wheelchairs and manipulators. The key challenge here is that while robots have many degrees of freedom, the input devices available to disabled users are typically low-DoF and noisy due to the nature of their disabilities. Examples include chin joysticks (Rulik et al., 2022) and neural implants (Hochberg et al., 2012). Using these devices to control a wheelchair or robot arm can be slow, tiring, and prone to errors. Shared control addresses these issues by identifying the user’s intent and facilitating smoother, more effortless goal achievement. To enhance this process, shared control often utilizes additional sources of information to interpret user input in context. For instance, cameras can provide images of the surrounding environment to aid in this interpretation.\n\nAccurately assisting and executing a user’s desired action requires understanding their intent.\nPredicting user intent is particularly challenging due to three defining characteristics:\n(i) Multi-modal: a user may approach a goal through different sub-optimal paths;\n(ii) Non-Markovian: past experiences continue to influence current actions (e.g., a previous car accident may make a driver more cautious);\n(iii) Non-stationary: user performance fluctuates, improving when they are focused and declining when they are fatigued.\nThis complexity creates a dynamic interplay between the user’s and the assistive controller’s authority. Users often require more control to effectively convey their intent, while assistive controllers may need increased authority to reduce the user’s effort. This tension gives rise to two critical dilemmas frequently faced by assistive controllers:\n\n(i) Action–noise dilemma:\nAn assistive controller must execute the user’s intended actions while filtering out noise from the user interface. However, this task is complicated by the challenge of distinguishing noise from the effect of the three defining characteristics of user intent listed above.\nOver-reliance on the assistive controller may reduce noise effectively but risks suppressing genuine changes in the user’s intent. Conversely, relying more on the user preserves their autonomy but fails to alleviate their control burden or input noise. For example, as illustrated in Fig. 1 (a), at the current timestep, the assistive controller predicts an action toward goal 1, while the user issues a command toward goal 2. Treating the user’s command as noise and filtering it out may result in the user reaching an unintended goal if the command represents their true intent. On the other hand, fully relying on the user’s command offers no assistance if the command is indeed noise.\nSome approaches attempt to address this dilemma heuristically (Demeester et al., 2008; Song et al., 2024a). By introducing a disagreement threshold, the system differentiates between intended actions and noise, returning control to the user when the threshold is exceeded. However, this method does not fundamentally solve the problem, as there is no guarantee that a user command exceeding the threshold represents true intent. Additionally, the process of setting such a threshold lacks theoretical justification.\nTo fully resolve the action–noise dilemma, the assistive controller must adaptively blend actions by considering the uncertainties of both the user and the environment.\n\n(ii) Disagreement Dilemma: A common approach to shared control involves linearly blending the user’s commands with those of the controller, facilitated by an arbitrator (Dragan and Srinivasa, 2013; Song et al., 2024a; Maeda, 2022). However, as Trautman points out (Trautman, 2015), tasks such as collision avoidance often allow for multiple equally optimal trajectories due to the multi-modal nature of human intent. The user may select any of these trajectories, which can differ from the controller’s prediction.\nWhen the user disagrees with the assistive controller at a given timestep, blending a safe user command with a safe controller-proposed action may unintentionally result in an unsafe shared action. For example, as shown in Fig. 1 (b), the assistive controller predicts an action to the left, while the user commands movement to the right. A linear blend of these two actions could lead to a collision with the obstacle.\nExisting approaches, such as those based on probabilistic models (Trautman, 2015), constraint-based shared control (Iregui et al., 2021), and model predictive control (MPC) (Lu et al., 2019), attempt to address this issue by implicitly blending policies. However, these methods often reduce to linear blending, limiting their ability to fully resolve the disagreement dilemma.\nTo effectively tackle this challenge, the assistive controller should adopt a multi-modal blending strategy. For instance, it could generate multiple trajectory proposals and blend the user’s command with the proposal most aligned with their intent, ensuring both safety and responsiveness.\n\nIn this paper, we propose an assistive controller named Robot Trajectron V2 (RT-V2) for navigation tasks. RT-V2 is designed within a Bayesian framework, as:\n\nIn summary:\n\nOur paper proposes Robot Trajectron V2 (RT-V2), a Bayesian-based assistive controller designed for navigation tasks. RT-V2 models user behavior using a prior trained on past data and a posterior that adapts to real-time user commands, addressing both action-noise and disagreement dilemmas.\n\nThe prior model is built using a recurrent neural network and a conditional variational autoencoder (CVAE), enabling it to capture multi-modal and non-Markovian aspects of human intent. This enhances the controller’s ability to accurately interpret and blend user commands in a dynamic shared control setting.\n\nTo overcome causal confusion in imitation learning, we introduce Imagined Rollout Reinforcement Learning, where RT-V2 simulates future interactions to receive reward signals and refine its autonomous navigation capabilities.\n\nA sampling-based trajectory optimization method with safety constraints is employed to ensure the controller’s actions are collision-free.\n\nThe novel contributions of the paper are:\n\nA shared-control model grounded in a probabilistic formulation of the intention prior and posterior, and their acquisition from data via a combination of imitation learning, reinforcement learning, and sampling-based optimization.\n\nExtensive experiments show that RT-V2 achieves high accuracy in intent estimation and safe, efficient navigation. Tests with human users (keyboard interface) and monkey users (BMI interface) demonstrate its effectiveness in optimizing shared autonomy with respect to agreeability, safety, and efficiency.\n\n1. Our paper proposes Robot Trajectron V2 (RT-V2), a Bayesian-based assistive controller designed for navigation tasks. RT-V2 models user behavior using a prior trained on past data and a posterior that adapts to real-time user commands, addressing both action-noise and disagreement dilemmas.\n\n2. The prior model is built using a recurrent neural network and a conditional variational autoencoder (CVAE), enabling it to capture multi-modal and non-Markovian aspects of human intent. This enhances the controller’s ability to accurately interpret and blend user commands in a dynamic shared control setting.\n\n3. To overcome causal confusion in imitation learning, we introduce Imagined Rollout Reinforcement Learning, where RT-V2 simulates future interactions to receive reward signals and refine its autonomous navigation capabilities.\n\n4. A sampling-based trajectory optimization method with safety constraints is employed to ensure the controller’s actions are collision-free.\n\n1. A shared-control model grounded in a probabilistic formulation of the intention prior and posterior, and their acquisition from data via a combination of imitation learning, reinforcement learning, and sampling-based optimization.\n\n2. Extensive experiments show that RT-V2 achieves high accuracy in intent estimation and safe, efficient navigation. Tests with human users (keyboard interface) and monkey users (BMI interface) demonstrate its effectiveness in optimizing shared autonomy with respect to agreeability, safety, and efficiency.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何准确预测用户意图以实现安全有效的导航支持。  \n2. 如何解决助理控制器在用户命令和控制器建议之间的行动噪声和不一致性困境。  \n\n【用了什么创新方法】  \n本研究提出了Robot Trajectron V2 (RT-V2)，一个基于贝叶斯框架的助理控制器，旨在解决导航任务中的用户意图建模问题。RT-V2结合了先验模型和后验更新，利用递归神经网络和条件变分自编码器捕捉用户的多模态和非马尔可夫特性。通过引入想象滚动强化学习，RT-V2能够模拟未来交互以优化自主导航能力。此外，采用基于采样的轨迹优化方法确保安全性。实验结果表明，RT-V2在意图估计和安全导航方面优于现有方法，展示了其在共享自主性优化中的有效性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference",
            "authors": "Zijun Che,Yinghong Zhang,Shengyi Liang,Boyu Zhou,Jun Ma,Jinni Zhou",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19916",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19916",
            "arxiv_html_link": "https://arxiv.org/html/2509.19916v1",
            "abstract": "Autonomous exploration in structured and complex indoor environments remains a challenging task, as existing methods often struggle to appropriately model unobserved space and plan globally efficient paths.\nTo address these limitations, we propose GUIDE, a novel exploration framework that synergistically combines global graph inference with diffusion-based decision-making.\nWe introduce a region-evaluation global graph representation that integrates both observed environmental data and predictions of unexplored areas, enhanced by a region-level evaluation mechanism to prioritize reliable structural inferences while discounting uncertain predictions.\nBuilding upon this enriched representation, a diffusion policy network generates stable, foresighted action sequences with significantly reduced denoising steps. Extensive simulations and real-world deployments demonstrate that GUIDE consistently outperforms state-of-the-art methods, achieving up to 18.3% faster coverage completion and a 34.9% reduction in redundant movements.",
            "introduction": "Autonomous exploration remains a cornerstone of modern robotics research, with pivotal applications in scenarios where unknown environment coverage is critical: environmental monitoring, warehouse logistics, and search-and-rescue operations [1, 2]. A defining challenge in these tasks is efficiently covering all reachable areas under stringent constraints—limited time, finite energy, and constrained computational resources. Despite decades of progress, existing exploration strategies still struggle to appropriately model unobserved space and plan globally efficient paths.\n\nCurrent exploration methodologies can be broadly categorized into model-based and learning-based approaches, each exhibiting fundamental limitations in addressing the global coverage challenge. Early model-based techniques, including frontier-based methods [3, 4] and sampling-based exploration strategies [5], rely exclusively on observed map information to determine exploration directions. While these approaches demonstrate reasonable performance in structured environments, their inherent myopia toward unobserved areas frequently results in redundant revisits, inefficient path planning, and suboptimal coverage—particularly in environments with complex topologies. Coverage path-based methods [6, 7] attempt to address this limitation by incorporating explicit coverage objectives; however, their reliance on uniform grid decomposition implicitly assumes environmental regularity, leading to performance degradation in spaces with irregular layouts or varying structural complexity.\n\nMore recently, learning-based approaches have emerged as promising alternatives, yet they too face significant challenges in achieving comprehensive spatial understanding. The first category employs neural networks to directly map observed environments to exploration actions [8, 9, 10]. Although these methods improve adaptability to specific environments, they fundamentally operate with limited information—encoding only observed areas while remaining agnostic to the structure of unknown spaces. This inherent constraint severely limits their capacity to achieve globally efficient exploration, often requiring extensive training across diverse environments to achieve moderate performance. The second category explicitly predicts unobserved areas and associated information gain [11, 12]. While conceptually promising, these approaches typically utilize predicted maps only for local planning rather than incorporating them into a comprehensive global planning framework, thereby failing to fully leverage the predictive information for long-horizon path optimization.\n\nThese limitations collectively highlight a critical research gap: the absence of a unified framework that effectively integrates predictions of unknown areas with globally optimized exploration planning. Specifically, existing methods lack mechanisms to (1) construct a comprehensive environmental representation that coherently combines observed information with predictions of unexplored areas, (2) leverage credible predictions to guide exploration decisions, and (3) generate stable, long-horizon trajectories that maximize coverage efficiency while minimizing redundant movements.\n\nTo address these challenges, we propose GUIDE, a novel exploration framework that synergistically combines global graph inference with diffusion-based decision-making. At its core, GUIDE constructs a region-evaluation global graph representation that integrates both observed environmental data and predictions of unexplored areas. This representation is enhanced through a region-level evaluation mechanism that prioritizes significant regional structural inferences, effectively creating an informative yet compact environmental model that prioritizes credible structural inferences while appropriately discounting uncertain predictions. Building upon this enriched representation, GUIDE employs a diffusion policy network that generates stable, foresighted action sequences with significantly reduced denoising steps compared to conventional approaches—enabling efficient long-horizon planning that effectively balances immediate information gain with comprehensive coverage objectives. The reduced computational overhead ensures real-time responsiveness—a critical advantage for resource-constrained robotic platforms.\n\nWe rigorously evaluate GUIDE across diverse simulation environments with varying structural complexities and through real-world deployments on physical robotic platforms. Quantitative results demonstrate consistent improvements over state-of-the-art methods, with our approach achieving up to 18.3% faster coverage completion and a 34.9% reduction in redundant movements across benchmark environments. Qualitative analysis further showcases GUIDE’s superior capability in structural inference and adaptive exploration behavior.\nThe main contributions of this work are threefold:\n\n1) We introduce a region-evaluation global graph inference module that constructs a unified environmental representation by integrating observed information with predictions of unexplored areas. It incorporates a novel region-evaluation mechanism that assesses the reliability and decision relevance of predicted areas, enabling robust and reliable exploration planning under uncertainty.\n\n2) We develop a diffusion-based decision-making framework that leverages the global graph representation to generate stable, long-horizon exploration trajectories, significantly reducing the computational burden of conventional diffusion policies while producing foresighted and efficient exploration paths.\n\n3) We conduct comprehensive evaluations across multiple simulation environments and real-world scenarios, demonstrating GUIDE’s superior performance in both structural inference accuracy and exploration efficiency metrics, establishing a new benchmark for autonomous exploration systems.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效建模未观察空间以实现自主探索？  \n2. 现有方法在复杂环境中的路径规划效率不足。  \n3. 缺乏统一框架整合已知与未知区域的预测信息。  \n\n【用了什么创新方法】  \n本研究提出了GUIDE框架，结合全球图推理与扩散决策。首先，构建了一个区域评估的全球图表示，整合已观察环境数据与未探索区域的预测。通过区域级评估机制，优先考虑可靠的结构推断。然后，利用扩散策略网络生成稳定的长远行动序列，显著减少去噪步骤。实验结果显示，GUIDE在覆盖完成速度上提高了18.3%，并减少了34.9%的冗余移动，展现出优越的探索效率和结构推断能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "D3Grasp: Diverse and Deformable Dexterous Grasping for General Objects",
            "authors": "Keyu Wang,Bingcong Lu,Zhengxue Cheng,Hengdi Zhang,Li Song",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19892",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19892",
            "arxiv_html_link": "https://arxiv.org/html/2509.19892v1",
            "abstract": "Achieving diverse and stable dexterous grasping for general and deformable objects remains a fundamental challenge in robotics, due to high-dimensional action spaces and uncertainty in perception. In this paper, we present D3Grasp, a multimodal perception-guided reinforcement learning framework designed to enable Diverse and Deformable Dexterous Grasping. We firstly introduce a unified multimodal representation that integrates visual and tactile perception to robustly grasp common objects with diverse properties. Second, we propose an asymmetric reinforcement learning architecture that exploits privileged information during training while preserving deployment realism, enhancing both generalization and sample efficiency.\nThird, we meticulously design a training strategy to synthesize contact-rich, penetration-free, and kinematically feasible grasps with enhanced adaptability to deformable and contact-sensitive objects.\nExtensive evaluations confirm that D3Grasp delivers highly robust performance across large-scale and diverse object categories, and substantially advances the state of the art in dexterous grasping for deformable and compliant objects, even under perceptual uncertainty and real-world disturbances. D3Grasp achieves an average success rate of 95.1% in real-world trials—outperforming prior methods on both rigid and deformable objects benchmarks.",
            "introduction": "Dexterous robotic hands, with their human-like kinematic structures and multi-finger adaptability, hold transformative potential across industrial assembly, elderly care, and hazardous material handling. Recent advances in hardware design, exemplified by Shadow Dexterous Hand Robot (2025), Allegro Hand Hand (2025), and Paxini Dexhand13 Paxini (2025), have enabled 16+ degree-of-freedom (DoF) manipulation capabilities approaching human-level dexterity. However, two fundamental challenges persist in bridging this mechanical potential to real-world applications Xiao et al. (2025); An et al. (2025): multimodal perception integration and data-efficient policy learning, particularly for long-horizon manipulation tasks.\nContemporary robotic manipulation systems primarily depend on single sensing modality, each with inherent limitations: vision enables global localization but struggles with transparency or occlusion; tactile sensing offers precise contact feedback, yet lacks global awareness; proprioception monitors internal states but provides minimal environmental understanding. Hybrid architectures, such as visual-tactile fusion networks  Li et al. (2024b); Akinola et al. (2024); Dave et al. (2024); Ferrandis et al. (2024); Jin et al. (2023); Parsons et al. (2022), attempt to address these constraints through direct sensor concatenation. However, this approach induces a high-dimensional observation space, hindering policy convergence Tao et al. (2024). Crucially, fixed fusion weights cannot adapt to the varying sensory dominance across manipulation phases Li et al. (2022a); Wang et al. (2025), often resulting in conflicting signals that degrade control stability Akinola et al. (2024).\n\nFor data-efficient policy learning, modern simulation platforms such as IsaacSim Mittal et al. (2023), PyBullet Coumans and Bai (2016–2021), Genesis Authors (2024), Robotwin Mu et al. (2025) allow safe parallelized reinforcement learning (RL) exploration Li (2017). However, sim2real transfer is fundamentally hindered by sparse rewards and exponential exploration complexity in long-horizon tasks Wang et al. (2022), and catastrophic error propagation across sequential subtasks owing to compounding inaccuracies. Consequently, data-driven approaches leveraging imitation learning (IL) Hussein et al. (2017) and policy distillation (PD) Rusu et al. (2015) are gaining traction for improved sample efficiency Mandlekar et al. (2021), although scaling high-quality teleoperation data remains prohibitively expensive due to human-robot morphological differences Darvish et al. (2023). While integrated RL/IL/PD strategies Zhang et al. (2025a); Wan et al. (2023) mitigate data costs, they often overlook the critical influence of diverse object properties and grasp configurations in sim2real deployment.\n\nTo overcome these limitations, we introduce a multimodal learning framework for dexterous manipulation. Our primary contributions are threefold: (1) We develop a tactile-based multimodal perception representation capable of maximally leveraging environmental information and proprioception, while dynamically selecting optimal contact force outputs based on object material texture. (2) We construct an asymmetric actor-critic (AAC) network architecture utilizing privileged information; this framework employs simulated privileged data (e.g., deformable object deformation states) for policy value estimation within simulation, enabling optimal control mode selection and reducing the excessive reliance on perceptual precision in contact-intensive operations. (3) We propose a hybrid training paradigm that incorporates multiple category of objects and defines task-specific grasping postures, enabling the agent to acquire enhanced generalization capabilities.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现对一般和可变形物体的多样化和稳定的灵巧抓取。  \n2. 如何有效整合多模态感知以提高抓取性能。  \n3. 如何在长时间操作任务中实现数据高效的策略学习。  \n\n【用了什么创新方法】  \n本研究提出了D3Grasp，一个多模态感知引导的强化学习框架，旨在实现多样化和可变形的灵巧抓取。首先，构建了一个统一的多模态表示，集成视觉和触觉感知，以增强对不同物体的抓取能力。其次，设计了一个不对称的强化学习架构，利用特权信息进行训练，同时保持部署的现实性，从而提高了泛化能力和样本效率。最后，开发了一种训练策略，合成接触丰富、无穿透和运动学可行的抓取，增强了对可变形和接触敏感物体的适应性。D3Grasp在真实世界试验中取得了95.1%的平均成功率，显著超越了先前方法。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "SAGE:State-Aware Guided End-to-End Policy for Multi-Stage Sequential Tasks via Hidden Markov Decision Process",
            "authors": "BinXu Wu,TengFei Zhang,Chen Yang,JiaHao Wen,HaoCheng Li,JingTian Ma,Zhen Chen,JingYuan Wang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19853",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19853",
            "arxiv_html_link": "https://arxiv.org/html/2509.19853v1",
            "abstract": "Multi-stage sequential (MSS) robotic manipulation tasks are prevalent and crucial in robotics. They often involve state ambiguity, where visually similar observations correspond to different actions. We present SAGE, a state-aware guided imitation learning framework that models tasks as a Hidden Markov Decision Process (HMDP) to explicitly capture latent task stages and resolve ambiguity. We instantiate the HMDP with a state transition network that infers hidden states, and a state-aware action policy that conditions on both observations and hidden states to produce actions, thereby enabling disambiguation across task stages. To reduce manual annotation effort, we propose a semi-automatic labeling pipeline combining active learning and soft label interpolation. In real-world experiments across multiple complex MSS tasks with state ambiguity, SAGE achieved 100% task success under the standard evaluation protocol, markedly surpassing the baselines. Ablation studies further show that such performance can be maintained with manual labeling for only about 13% of the states, indicating its strong effectiveness.",
            "introduction": "Robotic manipulation tasks have attracted significant attention due to their broad applications. Vision-based strategies have been widely adopted [1], and have demonstrated remarkable performance across a variety of real-world scenarios [2, 3, 4, 5, 6]. However, a particular class of tasks—Multi-Stage Sequential (MSS) tasks—introduces distinctive challenges to vision-based policies. MSS tasks are characterized by a sequence of interdependent stages that must be executed in a prescribed temporal order, often requiring the policy to perform long-horizon reasoning, retain contextual information from prior steps, and ensure coherent progression across successive stages.\n\nIn many MSS tasks, conventional vision-based policies struggle in scenarios involving state ambiguity. In such cases, visually similar observations may correspond to different actions, resulting in ambiguity during action selection. An illustrative case is the Push Buttons task shown in Fig. 1. The visual observations at stages 1-1, 2-1, and 3-1 are nearly indistinguishable; however, the correct action—pressing the yellow, pink, or blue button—requires knowledge of the current task stage to be correctly determined. This requires the policy to map similar observations to distinct actions, a phenomenon we refer to as State Ambiguity. Similar challenges also arise in other real-world contexts, such as assessing whether a container has been filled in a warehouse packaging task, or judging whether a cloth is wet or dry during household cleaning. These examples highlight the inherent difficulty of resolving state ambiguity when relying solely on visual input.\n\nTo handle state ambiguity in MSS tasks, existing methods mainly fall into two categories: Memory-based approaches and hierarchical task decomposition. (1) In robotic manipulation, memory-based methods use models like recurrent neural networks [7], attention mechanisms [8], Transformer-XL [9] to capture historical context, as incorporating earlier observations can help distinguish visually similar states that lead to ambiguity. Although these methods are flexible, they often struggle with redundant information, high computational cost, and difficulties in deciding how much history to retain. (2)Hierarchical approaches, on the other hand, structure the policy into multiple levels of controllers. A high-level controller manages stage transitions, while low-level policies are responsible for executing specific actions [10, 11, 12, 13]. While this structure helps reduce ambiguity, designing the high-level controller typically requires extensive manual effort. For example, some methods use a nine-layer decision tree to handle transitions [11]. Moreover, since low-level modules directly execute action primitives, they are often constrained to be simple and modular, which limits flexibility. Transitions between primitives further introduce delays, reducing execution efficiency and stability in real-world scenarios.\n\nTo address the issue of State Ambiguity, we propose SAGE, a State-Aware Guided End-to-End Imitation Learning framework based on the Hidden Markov Decision Process (HMDP). Specifically, in Section III, we provide a theoretical analysis. The key idea is to treat observations as partial manifestations of a latent environment state and to explicitly model this state as a hidden variable. With this formulation, the HMDP can distinguish between different underlying physical states that share similar visual appearances, thereby resolving ambiguity. The HMDP consists of two components: hidden state estimation and decision-making agent. In Section IV, guided by the theoretical formulation, we implement these components as two neural networks. The state transition network infers hidden states from the current observation together with the previously estimated state, while the state-aware action policy generates actions by conditioning jointly on visual observations and the inferred states. We integrate them into an End-to-End architecture and train the framework using actions from expert demonstrations and human-labeled states as supervision. Furthermore, as detailed in Section V, to reduce the manual annotation cost of state labels required by supervised learning, we propose a semi-automatic labeling pipeline that integrates active learning with soft label interpolation. It substantially reduces manual labeling effort by annotating only a small subset of representative segments and automatically labeling the remaining data.\n\nExtensive real-world experiments on three MSS tasks, all of which involve state ambiguity, were conducted to evaluate the effectiveness of SAGE. The results show that it achieves up to a 100% stage success rate under standard evaluation settings, significantly outperforming competitive baselines. Our method remains robust under visually distracting conditions and continuous execution, completing 50-step sequences without error. Ablation studies show that our semi-automatic annotation strategy in SAGE achieves full success with only 13% of episodes manually labeled, demonstrating efficient annotation. These results validate the effectiveness and generality of our approach in tackling real-world state ambiguity.\n\nTherefore, we propose a unified imitation learning framework. Our main contributions are as follows:\n\nTo the best of our knowledge, we are the first to formulate MSS tasks with state ambiguity as a Hidden Markov Decision Process, offering a principled framework to address this challenge.\n\nWe realize the HMDP formulation through two dedicated neural modules for hidden state inference and state-aware action generation, which are jointly integrated into an End-to-End training pipeline.\n\nWe propose a semi-automatic state annotation strategy that substantially reduces the human effort required to provide state supervision signals for the HMDP.\n\nWe conduct extensive real-world experiments on multiple MSS tasks with state ambiguity, demonstrating the superior performance and robustness of SAGE.\n\n1. To the best of our knowledge, we are the first to formulate MSS tasks with state ambiguity as a Hidden Markov Decision Process, offering a principled framework to address this challenge.\n\n2. We realize the HMDP formulation through two dedicated neural modules for hidden state inference and state-aware action generation, which are jointly integrated into an End-to-End training pipeline.\n\n3. We propose a semi-automatic state annotation strategy that substantially reduces the human effort required to provide state supervision signals for the HMDP.\n\n4. We conduct extensive real-world experiments on multiple MSS tasks with state ambiguity, demonstrating the superior performance and robustness of SAGE.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何处理多阶段顺序（MSS）任务中的状态模糊性问题。  \n2. 如何通过隐马尔可夫决策过程（HMDP）建模MSS任务以捕捉潜在任务阶段。  \n3. 如何减少对手动标注的依赖，提高状态标注的效率。  \n\n【用了什么创新方法】  \n提出了一种名为SAGE的状态感知引导模仿学习框架，通过隐马尔可夫决策过程（HMDP）建模MSS任务，显式捕捉潜在状态并解决状态模糊性。该方法结合了状态转移网络和状态感知动作策略，能够根据观察和隐状态生成动作。通过引入半自动标注管道，结合主动学习和软标签插值，显著降低了手动标注的工作量。实验结果显示，SAGE在多个复杂MSS任务中实现了100%的任务成功率，远超基线方法，并且在仅标注13%状态的情况下仍能保持高效性能。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "DynaFlow: Dynamics-embedded Flow Matching for Physically Consistent Motion Generation from State-only Demonstrations",
            "authors": "Sowoo Lee,Dongyun Kang,Jaehyun Park,Hae-Won Park",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19804",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19804",
            "arxiv_html_link": "https://arxiv.org/html/2509.19804v1",
            "abstract": "This paper introduces DynaFlow, a novel framework that embeds a differentiable simulator directly into a flow matching model. By generating trajectories in the action space and mapping them to dynamically feasible state trajectories via the simulator, DynaFlow ensures all outputs are physically consistent by construction. This end-to-end differentiable architecture enables training on state-only demonstrations, allowing the model to simultaneously generate physically consistent state trajectories while inferring the underlying action sequences required to produce them.\nWe demonstrate the effectiveness of our approach through quantitative evaluations and showcase its real-world applicability by deploying the generated actions onto a physical Go1 quadruped robot.\nThe robot successfully reproduces diverse gait present in the dataset, executes long-horizon motions in open-loop control and translates infeasible kinematic demonstrations into dynamically executable, stylistic behaviors. These hardware experiments validate that DynaFlow produces deployable, highly effective motions on real-world hardware from state-only demonstrations, effectively bridging the gap between kinematic data and real-world execution.",
            "introduction": "Generative models, such as Diffusion Models and Flow Matching, have recently achieved unprecedented success across various domains, including image[1, 2, 3], audio[4], and text generation[5]. They have demonstrated a remarkable ability to learn intricate data distributions from large-scale datasets, producing highly natural and diverse outputs. Inspired by this success, these models are increasingly being recognized as powerful tools for generating complex motion trajectories in fields like robotics and computer graphics[6, 7, 8].\nIndeed, their application to kinematic motion generation has seen significant progress, largely driven by the increasing availability of state demonstration data from sources such as motion capture and raw video.\n\nHowever, directly applying these generative models to character animation and robot control presents significant challenges.\nA primary limitation is the lack of physical consistency.\nMost generative models learn a statistical approximation of the data distribution from a finite set of examples, rather than the underlying physical principles governing the data. Consequently, there is no guarantee that the generated outputs will adhere to intrinsic physical principles or dynamic constraints.\nThis means that when generating novel behaviors, the resulting motions can be either physically implausible, with artifacts like ground penetration, character floating, and foot sliding, or dynamically inconsistent, making them unsuitable to execute in the physical world.\nThe issue becomes particularly pronounced when models are trained on datasets with inherent physical inconsistencies, such as those sourced from motion capture or generated through kinematic retargeting.\n\nAnother major hurdle is the scarcity of action-labeled data. While state trajectories can often be obtained from motion capture or other sources, the corresponding action sequences (e.g., joint torques or motor commands) are rarely available and are costly to collect, typically being hardware-specific. Prior diffusion-based control approaches can be categorized into two principal ways. One option is to directly train a policy to predict actions from true action data, but this is rarely viable given the limited availability of true action data. Alternatively, a hierarchical framework can be employed, where the diffusion model generates desired state trajectories that are then executed by a low-level tracking controller. However, this hierarchical strategy has its own drawback: the tracking controller usually requires extensive fine-tuning for each new motion, and ensuring robustness against the distributional gap between planned trajectories and the controller remains a significant challenge [9].\n\nTo overcome these limitations, we propose DynaFlow, a novel framework that guarantees physically consistent motion generation by embedding dynamics directly into the generation process, while simultaneously inferring actions from state-only demonstration data.\nThe core idea of DynaFlow is to integrate a differentiable simulator at the output of the flow matching prediction module. This simulator layer acts as a mapping from the space of action trajectories to the space of dynamically feasible state trajectories, ensuring that the model’s output strictly adheres to the laws of physics by construction. Furthermore, its differentiable nature allows the entire model to be trained end-to-end. During this process, the model naturally discovers the action trajectory required to reconstruct a given state trajectory in a dynamically consistent manner, even without explicit action labels.\n\nWe conduct a series of experiments to validate the effectiveness of DynaFlow. Our quantitative analysis compares DynaFlow against several baselines, evaluating both dynamic feasibility and distributional similarity on two distinct datasets: a rich, strictly feasible dataset of quadruped locomotion and a challenging single-trajectory dataset from retargeted motion capture.\nOur results demonstrate that DynaFlow consistently generates strictly feasible trajectories, even when trained on a physically inconsistent dataset, while remaining competitive in distributional similarity. To showcase its real-world applicability, we deploy action trajectories generated by DynaFlow on a physical Go1 quadruped robot.\nThe robot successfully reproduces diverse gaits observed in the training data and executes long-horizon motions with high accuracy in challenging open-loop experiments, validating the precision and coherence of the generated actions.\nFurthermore, we demonstrate its ability to translate infeasible retargeted motions into dynamically executable and stylistic behaviors on hardware, bridging the gap between kinematic demonstration and real-world execution.\n\nThe main contributions of this paper are as follows:\n\nDynamics-embedded generative modeling:\nWe propose DynaFlow, a novel generative model that embeds a differentiable simulator into a flow matching framework to guarantee all generated trajectories are strictly dynamically consistent by construction.\n\nLearning from state-only data:\nOur method overcomes the common challenge of unavailable action data by leveraging analytical gradients, learning action directly from state-only demonstrations.\n\nReal-world validation:\nWe demonstrate the practical viability of our approach through successful hardware deployment of the generated motion on a physical quadruped robot, bridging the gap between motion generation and real-world execution.\n\n1. Dynamics-embedded generative modeling:\nWe propose DynaFlow, a novel generative model that embeds a differentiable simulator into a flow matching framework to guarantee all generated trajectories are strictly dynamically consistent by construction.\n\n2. Learning from state-only data:\nOur method overcomes the common challenge of unavailable action data by leveraging analytical gradients, learning action directly from state-only demonstrations.\n\n3. Real-world validation:\nWe demonstrate the practical viability of our approach through successful hardware deployment of the generated motion on a physical quadruped robot, bridging the gap between motion generation and real-world execution.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何确保生成的运动轨迹在物理上是一致的？  \n2. 如何从仅有的状态演示数据中推断出相应的动作序列？  \n3. 如何将生成的动作有效地应用于现实世界的机器人？  \n\n【用了什么创新方法】  \nDynaFlow框架通过将可微分模拟器嵌入流匹配模型中，确保生成的运动轨迹在物理上始终一致。该方法允许从状态演示数据中直接学习动作序列，克服了缺乏动作标注数据的挑战。通过一系列实验，DynaFlow在动态可行性和分布相似性方面表现出色，并成功在物理四足机器人上部署生成的动作，验证了其在真实环境中的有效性和精确性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training",
            "authors": "Rushuai Yang,Hangxing Wei,Ran Zhang,Zhiyuan Feng,Xiaoyu Chen,Tong Li,Chuheng Zhang,Li Zhao,Jiang Bian,Xiu Su,Yi Chen",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19752",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19752",
            "arxiv_html_link": "https://arxiv.org/html/2509.19752v1",
            "abstract": "Vision-language-action (VLA) models have shown strong generalization across tasks and embodiments; however, their reliance on large-scale human demonstrations limits their scalability owing to the cost and effort of manual data collection. Reinforcement learning (RL) offers a potential alternative to generate demonstrations autonomously, yet conventional RL algorithms often struggle on long-horizon manipulation tasks with sparse rewards. In this paper, we propose a modified diffusion policy optimization algorithm to generate high-quality and low-variance trajectories, which contributes to a diffusion RL-powered VLA training pipeline. Our algorithm benefits from not only the high expressiveness of diffusion models to explore complex and diverse behaviors but also the implicit regularization of the iterative denoising process to yield smooth and consistent demonstrations. We evaluate our approach on the LIBERO benchmark, which includes 130 long-horizon manipulation tasks, and show that the generated trajectories are smoother and more consistent than both human demonstrations and those from standard Gaussian RL policies. Further, training a VLA model exclusively on the diffusion RL-generated data achieves an average success rate of 81.9%, which outperforms the model trained on human data by +5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight our diffusion RL as an effective alternative for generating abundant, high-quality, and low-variance demonstrations for VLA models.",
            "introduction": "Vision-language-action (VLA) is a promising model toward general-purpose robots capable of generalizing across a wide array of manipulation tasks [1, 2, 3]. However, this paradigm is critically dependent on massive datasets of human demonstrations, such as the Open X-Embodiment dataset [4]. The process of collecting this data via manual teleoperation is notoriously expensive and labor-intensive. The reliance on manual data collection fundamentally caps the scalability of VLA models, presenting a major bottleneck to further progress.\n\nReinforcement learning (RL) has emerged as a powerful paradigm for enabling robots to acquire sophisticated physical skills directly through environmental interaction.\nThe fundamental strength of reinforcement learning stems from its trial-and-error process: by optimizing for a reward signal, an agent can autonomously discover highly effective and efficient strategies that often surpass what can be learned by simply mimicking human demonstrations.\nHowever, a significant limitation of this approach is that the resulting policies are often highly specialized.\nA policy trained to excel under one specific set of conditions typically struggles to adapt or generalize its skills when faced with new task variations or different environmental setups [5].\nHowever, making a general RL algorithm effective enough to generate high-quality data across diverse, complex manipulation tasks is challenging.\nThe long-horizon, sparse-reward tasks prevalent in benchmarks like LIBERO [6] expose critical weaknesses of conventional RL algorithms, often leading to unstable learning process or high-variance and suboptimal trajectories [7].\n\nTo this end, we propose a general framework that utilizes a modified diffusion policy optimization algorithm for diffusion RL-powered data generation.\nWe find that diffusion offer a superior alternative for this problem.\nFirst, diffusion policy provide good expressiveness to fit complex expert distribution.\nCompared with Gaussian RL, diffusion-based RL provides more space for RL exploration when interacting with environment.\nSecond, the inherent structure of the iterative denoising process acts as a powerful implicit regularizer on the action space, The model is trained to predict the noise for the entire action chunk at every step of the denoising process. This forces the model to learn the underlying structure of smooth, physically plausible motions. A single, jerky movement in the final action would require a very specific and complex sequence of denoising steps, which is less likely to be learned than a smooth, coherent refinement process. This naturally encourages the generation of temporally smooth, low-variance motion. We further enhance this process with a stabilized fine-tuning regimen, incorporating modifications to the architecture and training strategies to ensure robust performance across the 130 challenging tasks in LIBERO. This property allows our RL agent to explore more effectively and converge to near-optimal and low-variance policies.\n\nOur experiments yield a clear message:\nA VLA model trained exclusively on our RL-generated data consistently and significantly surpasses the ones trained on human data and Gaussian RL, both on in-distribution tasks and in challenging OOD generalization. A quantitative analysis reveals the mechanism behind this success: Our generated trajectories are smoother and less variable, providing a more stable learning signal for VLA training.\nOur contributions are threefold:\n\nA diffusion RL-powered VLA training pipeline for autonomously generating high-quality and low-variance data for VLA training, including validated effective modifications on the model architecture and training strategies.\n\nCompelling empirical evidence on the 130 complex manipulation tasks of the LIBERO benchmark shows that our synthetic data provides superior training signal to human demonstrations, significantly improving both the in-distribution success rates and out-of-distribution generalization of VLA models.\n\nAn in-depth quantitative analysis that relates trajectory-level properties (e.g., efficiency, smoothness, and consistency) with the performance of fine-tuned VLA, providing a clear explanation for why optimized data is more effective.\n\n1. A diffusion RL-powered VLA training pipeline for autonomously generating high-quality and low-variance data for VLA training, including validated effective modifications on the model architecture and training strategies.\n\n2. Compelling empirical evidence on the 130 complex manipulation tasks of the LIBERO benchmark shows that our synthetic data provides superior training signal to human demonstrations, significantly improving both the in-distribution success rates and out-of-distribution generalization of VLA models.\n\n3. An in-depth quantitative analysis that relates trajectory-level properties (e.g., efficiency, smoothness, and consistency) with the performance of fine-tuned VLA, providing a clear explanation for why optimized data is more effective.",
            "llm_summary": "【关注的是什么问题】  \n1. VLA模型依赖于大量人工示范数据，限制了其可扩展性。  \n2. 传统的强化学习算法在长时间跨度的稀疏奖励任务中表现不佳。  \n3. 如何生成高质量、低方差的演示数据以提高VLA训练效果。  \n\n【用了什么创新方法】  \n提出了一种改进的扩散策略优化算法，以生成高质量、低方差的轨迹，构建了一个基于扩散强化学习的VLA训练管道。该算法利用扩散模型的高表达能力探索复杂行为，并通过迭代去噪过程的隐式正则化生成平滑、一致的演示。实验结果表明，使用扩散RL生成的数据训练的VLA模型在130个长时间跨度的操作任务中，成功率达到81.9%，超越了基于人工数据和标准高斯RL生成数据的模型，展示了扩散RL在生成丰富、高质量演示数据方面的有效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Trajectory Planning Using Safe Ellipsoidal Corridors as Projections of Orthogonal Trust Regions",
            "authors": "Akshay Jaitly,Jon Arrizabalaga,Guanrui Li",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19734",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19734",
            "arxiv_html_link": "https://arxiv.org/html/2509.19734v1",
            "abstract": "Planning collision free trajectories in complex environments remains a core challenge in robotics. Existing corridor based planners which rely on decomposition of the free space into collision free subsets scale poorly with environmental complexity and require explicit allocations of time windows to trajectory segments. We introduce a new trajectory parameterization that represents trajectories in a nonconvex collision free corridor as being in a convex cartesian product of balls. This parameterization allows us to decouple problem size from geometric complexity of the solution and naturally avoids explicit time allocation by allowing trajectories to evolve continuously inside ellipsoidal corridors. Building on this representation, we formulate the Orthogonal Trust Region Problem (Orth-TRP), a specialized convex program with separable block constraints, and develop a solver that exploits this parallel structure and the unique structure of each parallel subproblem for efficient optimization. Experiments on a quadrotor trajectory planning benchmark show that our approach produces smoother trajectories and lower runtimes than state-of-the-art corridor based planners, especially in highly complicated environments.",
            "introduction": "Collision free trajectory optimization is a core challenge in robotics, useful in mobile robot navigation and mobile manipulation [1, 2, 3]. By mathematically defining sets of feasible trajectories that a system can undergo, and assigning a cost to each element of the set, trajectory optimization can be posed as a mathematical programming problem. Several methods exist to characterize these sets to facilitate efficient searches for optimal feasible trajectories.\n\nConventional sampling based planners [4, 5, 6] build tree or graph like structures in the configuration space, with nodes corresponding to collision free states and edges representing locally feasible motions. Paths through these structures define potential trajectories, which can then be searched efficiently using graph search or shortest path algorithms. On the other hand, some optimization based approaches [7, 8, 9, 10, 11, 12] impose constraints on robot’s states (enforcing that a barrier between collision objects is respected) to define sets of feasible solutions, then minimize a cost function within this nonconvex feasible set. These enable powerful nonconvex optimization solvers for trajectory optimization. In more complicated scenarios, with larger dimensional spaces or with numerous obstacles, the above methods inevitably face the curse of dimensionality.\n\nRecent work has introduced convex approximations of free space through “safe corridors”, using either polytopic or ellipsoidal regions [13, 14, 15, 16]. In these approaches, a trajectory is divided into segments, and each segment is constrained to lie within a corresponding convex subset of the obstacle free space at a specific point in time. By enforcing convex constraints on the robot’s configuration, these methods convert a difficult global planning problem into convex optimization, which can be solved using efficient convex programming techniques [1, 15, 16].\n\nHowever, a key challenge in these methods is allocating time across the convex regions. Each region requires a specified time interval, and if this interval is too short, the system must accelerate sharply to meet continuity constraints, while overly long intervals produce inefficient motions. As the number of convex subsets increases, these timing decisions become increasingly difficult to manage, often introducing sensitivity and limiting the scalability of convex corridor methods.\n\n[17] introduced Differentiable Collision Free Parametric Corridors which model free space as a smooth nonconvex corridor, made of a continuously deforming convex set rather than discrete segments, offering a more unified description of collision free regions. Building on this idea, we view these nonconvex corridors as time-varying projections of orthogonal trust regions in a higher dimensional parameter space. By lifting trajectories into a space where feasible solutions form a cartesian product of high dimensional balls, each point in this lifted space naturally corresponds to a collision free trajectory within the corridor. This decouples problem size from environmental complexity while allowing representations of paths in the safe corridor as points in the lifted space.\n\nThis representation results in a favorable convex feasible set for trajectory optimization. We pose the resulting problem as the Orthogonal Trust Region Problem (Orth-TRP), which can be expressed as a collection of interconnected trust region subproblems (TRPs). Because each block of variables has its own seperable constraint, the Orth-TRP naturally supports a parallelizable algorithm, where each block can be updated efficiently using trust region steps that resemble simple one dimensional line searches.\n\nIn summary, our contributions are:\n\nWe develop a convex representation of sets of trajectories within a nonconvex set of configurations using a product of multiple high dimensional balls, decoupling the solution complexity from the size of the optimization problem.\n\nWe develop a convex representation of sets of trajectories within a nonconvex set of configurations using a product of multiple high dimensional balls, decoupling the solution complexity from the size of the optimization problem.\n\nWe create a solver that exploits both, the parallelizable structure and the unique Trust-Region-like structure of our resulting problem to solve quadratically constrained quadratic optimization problems where constraints are enforced on separable variables.\n\nTaken together, these components create a scalable and geometrically intuitive framework for collision-free trajectory optimization in complex environments. Our experiments show that this approach consistently outperforms state-of-the-art implementations on challenging benchmarks. In direct comparisons with the method of [15], even when using a powerful solver like OSQP [18], our method solves long-horizon planning problems faster while producing comparable or smoother results. To the best of our knowledge, this is the first continuous parameterization of trajectories that enables optimization within a collision-free corridor where problem size and runtime are agnostic to horizon length and solution complexity. We achieve this by combining a new convex programming formulation with a solver that fully exploits the problem’s separable trust-region structure, eliminating the scaling and time-allocation issues inherent to existing corridor-based approaches and enabling both speed and robustness at scale.\n\n1. We develop a convex representation of sets of trajectories within a nonconvex set of configurations using a product of multiple high dimensional balls, decoupling the solution complexity from the size of the optimization problem.\n\n2. We create a solver that exploits both, the parallelizable structure and the unique Trust-Region-like structure of our resulting problem to solve quadratically constrained quadratic optimization problems where constraints are enforced on separable variables.",
            "llm_summary": "【关注的是什么问题】  \n1. 复杂环境中的无碰撞轨迹规划仍然是机器人技术中的核心挑战。  \n2. 现有基于走廊的规划方法在环境复杂性增加时表现不佳，且需要显式的时间分配。  \n3. 如何有效地优化非凸走廊中的轨迹，同时避免时间分配问题。  \n\n【用了什么创新方法】  \n本研究提出了一种新的轨迹参数化方法，通过将轨迹表示为高维球体的笛卡尔积，形成非凸走廊的凸表示。这种方法使得问题规模与解决方案的几何复杂性解耦，并允许轨迹在椭圆走廊内连续演变。我们将其形式化为正交信任区域问题（Orth-TRP），并开发了一种利用该问题并行结构的求解器。实验结果表明，该方法在复杂环境中产生了更平滑的轨迹，并且运行时间低于现有的走廊规划方法，尤其在高复杂度环境中表现优越。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Simultaneous estimation of contact position and tool shape with high-dimensional parameters using force measurements and particle filtering",
            "authors": "Kyo Kutsuzawa,Mitsuhiro Hayashibe",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted to The International Journal of Robotics Research (IJRR)",
            "pdf_link": "https://arxiv.org/pdf/2509.19732",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19732",
            "arxiv_html_link": "https://arxiv.org/html/2509.19732v1",
            "abstract": "Estimating the contact state between a grasped tool and the environment is essential for performing contact tasks such as assembly and object manipulation.\nForce signals are valuable for estimating the contact state, as they can be utilized even when the contact location is obscured by the tool.\nPrevious studies proposed methods for estimating contact positions using force/torque signals; however, most methods require the geometry of the tool surface to be known.\nAlthough several studies have proposed methods that do not require the tool shape, these methods require considerable time for estimation or are limited to tools with low-dimensional shape parameters.\nHere, we propose a method for simultaneously estimating the contact position and tool shape, where the tool shape is represented by a grid, which is high-dimensional (more than 1000 dimensional).\nThe proposed method uses a particle filter in which each particle has individual tool shape parameters, thereby to avoid directly handling a high-dimensional parameter space.\nThe proposed method is evaluated through simulations and experiments using tools with curved shapes on a plane.\nConsequently, the proposed method can estimate the shape of the tool simultaneously with the contact positions, making the contact-position estimation more accurate.",
            "introduction": "When robots manipulate objects or use them as tools, they often need to recognize the contact states between the grasped objects/tools and the environment.\nFor instance, when inserting a key into a keyhole, the robot must know the contact position and conditions of the key and lock.\nIn addition, when cutting the bone-in meat with a knife, the robot must detect where the knife contacts with the bone.\nIn such situations, robots need to detect contact states indirectly because tools usually have no sensors.\nMoreover, because tools hide the contact location, robots must estimate the contact information from force signals instead of vision.\n\nConventional methods for contact-position estimation from force signals require the shape and position of the tools.\nA technique for contact-position estimation (Salisbury, 1984) often requires the shape of the tool surface to be known.\nHowever, shape measurements using cameras generally exhibit large errors in the depth direction and are sensitive to occlusion, reflection, and transparency.\nAlthough there are several methods for contact-position estimation without shape information (Tsuji et al., 2017; Koike et al., 2017), the estimation is slow, and these methods require that contact force constantly fluctuates during estimation.\nAs those drawbacks are unavoidable unless using shape information, it is beneficial to estimate tool shape for estimating the contact position.\nAdditionally, the shape of the object/tool is necessary to assemble tasks and plan a control strategy (von Drigalski et al., 2020).\n\nFor tools made of transparent or reflective materials, it would be helpful to be able to estimate the tool shape from force signals instead of vision.\nRecently, a method that simultaneously estimates the contact position and tool shape from force signals was proposed (Kutsuzawa et al., 2020).\nThis method gradually estimates the contact position and tool shape under uncertainty using an unscented particle filter (UPF) (van der Merwe et al., 2000a, b).\nHowever, it requires the tool shape to be expressed using a small number of parameters.\nIt is practically impossible to apply that method to general shapes because the dimensionality of the tool-shape parameters becomes high, which requires an exponential number of particles for a reliable estimation.\nThere is another method that can detect the contact position while estimating the tool shape of voxels from force measurements (Bimbo et al., 2022), but this method requires the geometry of the environment being static.\n\nHere, we propose a method to estimate the tool shape with a large number of parameters from force signals while simultaneously estimating the contact position, based on the Rao–Blackwellized particle filter (RBPF) (Murphy, 1999) used in SLAM (Murphy, 1999; Grisetti et al., 2005, 2007).\nThe conventional method (Kutsuzawa et al., 2020) is affected by the curse of dimensionality owing to the application of a particle filter to high-dimensional parameters.\nBy contrast, the proposed method avoids this issue by associating shape information with individual particles rather than scattering particles into the shape-parameter space.\nThus, the proposed method enables the tool shape to be expressed using a high-dimensional grid representation (voxels or pixels).\nIn addition, in contrast with Bimbo et al. (2022), the proposed method does not need any assumption of environment geometry.\nTherefore, the proposed method is available even for the contact object in the environment being deformable and movable, e.g., a human touching the tool.\nIn this study, we address the simple case of a curved object on a plane.\nAlthough simple, this setup can demonstrate the effectiveness of the proposed method because the challenge of this study is the high-dimensional shape parameter.\nThe contributions of this study are listed as follows:\n\nThis study proposes a force-signal-based estimation method for tool shapes represented by high-dimensional variables (grid representation) without any assumptions of environment geometry.\n\nWe formulated a probabilistic model for contact-position and tool-shape estimation from force signals.\n\nThis study also proposes a method for updating the tool-shape parameters represented by a grid using the estimated contact position and measurements.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何同时估计接触位置和高维工具形状。  \n2. 现有方法在高维参数空间处理时的效率问题。  \n3. 在缺乏工具几何信息的情况下进行接触状态估计的挑战。  \n\n【用了什么创新方法】  \n提出了一种基于力信号的估计方法，通过粒子滤波（RBPF）同时估计接触位置和工具形状，工具形状用高维网格表示。该方法避免了直接处理高维参数空间的问题，通过将形状信息与个别粒子关联，显著提高了接触位置的估计准确性。实验结果表明，该方法在处理曲面工具时表现出色，能够在不依赖环境几何假设的情况下进行有效估计。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Towards Autonomous Robotic Electrosurgery via Thermal Imaging",
            "authors": "Naveed D. Riaziat,Joseph Chen,Axel Krieger,Jeremy D. Brown",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted for publication in the proceedings of the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)",
            "pdf_link": "https://arxiv.org/pdf/2509.19725",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19725",
            "arxiv_html_link": "https://arxiv.org/html/2509.19725v1",
            "abstract": "Electrosurgery is a surgical technique that can improve tissue cutting by reducing cutting force and bleeding. However, electrosurgery adds a risk of thermal injury to surrounding tissue. Expert surgeons estimate desirable cutting velocities based on experience but have no quantifiable reference to indicate if a particular velocity is optimal. Furthermore, prior demonstrations of autonomous electrosurgery have primarily used constant tool velocity, which is not robust to changes in electrosurgical tissue characteristics, power settings, or tool type. Thermal imaging feedback provides information that can be used to reduce thermal injury while balancing cutting force by controlling tool velocity. We introduce Thermography for Electrosurgical Rate Modulation via Optimization (ThERMO) to autonomously reduce thermal injury while balancing cutting force by intelligently controlling tool velocity. We demonstrate ThERMO in tissue phantoms and compare its performance to the constant velocity approach. Overall, ThERMO improves cut success rate by a factor of three and can reduce peak cutting force by a factor of two. ThERMO responds to varying environmental disturbances, reduces damage to tissue, and completes cutting tasks that would otherwise result in catastrophic failure for the constant velocity approach.",
            "introduction": "Electrosurgery is a surgical technique for cutting tissue using energy from a high-frequency voltage source. Eight in ten surgical procedures use electrosurgery, often to remove diseased tissue [1]. Monopolar electrosurgery uses a grounding pad to dissipate current through the body from a tool, directly heating the local tissue. However, electrosurgery can significantly damage nearby healthy tissue as well. Excess thermal damage can adversely affect surgery outcomes. Surgeons try to avoid generating thermal damage by modulating cut speed while ensuring cut accuracy.\n\nWhen using electrosurgery to remove tissue, surgeons aim to spare as much healthy tissue as possible. Since signs of excess thermal energy are often not obvious until irreversible damage has been done, surgeons minimize dwell time by moving faster, as fast motions reduce the heat deposited in a specific area. However, fast motion comes at the cost of decreased cut accuracy and increased cutting force, which can also damage tissue. The ideal electrosurgical cut technique creates a small denatured tissue zone ahead of the tool tip that reduces mechanical stiffness and, thus, the required cut force. However, as the tool speed increases, this denatured tissue zone becomes smaller and eventually disappears, allowing tissue to accumulate on the tool tip and increasing the force needed to cut. This results in increased tissue damage, bulk tissue deformation, and lowered cut quality. Therefore, the optimal cutting velocity should maintain a balanced speed to reduce thermal damage and minimize excess force on the tissue.\n\nWhile there have been no prior demonstrations of techniques for optimizing cut velocity in electrosurgery, there have been recent investigations into sensor-based automation for controlling thermal damage. Bao and Mazumder showed that thermal imaging can measure the denaturation zone and control it to a specific size using a novel computer-controllable electro electrosurgical unit (ESU) [2]. This method, however, contrasts with the fixed power level typically used by surgeons. El-Kebir et al. used thermal sensing and data-driven models to control the thermal damage boundary along a cut by pausing at discrete decision points [3]. Unfortunately, neither approach simultaneously considers cut deformation and thermal damage to optimize the cut velocity.\n\nThe same velocity modulation problem exists in autonomous robotic electrosurgery, which has had recent success in medical robotics research. While these autonomous approaches promise improved cut accuracy and decreased surgeon error, they have largely avoided real-time velocity optimization. Opfermann et al.  [4] demonstrated a visual servoing approach for electrosurgical cutting on the Smart Tissue Autonomous Robot (STAR) [5]. Saeidi et al. similarly showed that STAR could perform tissue-cutting tasks using predetermined cut depth, power, and speed [6]. Ge et al. demonstrated tumor detection and excision using a suction gripper [7]. Each approach relies on a predetermined cut velocity, chosen based on clinical observation or simulation. Researchers have generally opted to move slowly to avoid excess tissue deformation. However, these approaches fail to account for the excess heat damage caused by slow cutting velocities.\n\nHere, we introduce ThERMO. ThERMO uses thermal imaging to determine the optimal cut velocity by 1) dually identifying thermal and mechanical parameters online via a “Truncated Unscented Kalman Filter” (TUKF) and 2) minimizing a parametrized cost function of denaturation width and cutting force. Thermal measurements inform the adaptation of thermal and mechanical parameters, which are applied to generic thermal and mechanical models to maximize cut accuracy and minimize thermal damage with respect to cut velocity. Collectively, we contribute first steps towards 1) infrared (IR)-camera-based tissue denaturation and force measurement method with accompanying validation, 2) a combined adaptive identification and optimization-based approach to control velocity based on thermal spread and cutting force, and 3) rigorous comparisons to the current cutting-edge autonomous electrosurgery approach.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在电外科手术中优化切割速度以减少热损伤？  \n2. 现有电外科技术未能实时优化切割速度，导致组织损伤和切割质量下降。  \n3. 传统的电外科方法依赖于固定的切割速度，缺乏对环境变化的适应性。  \n\n【用了什么创新方法】  \n本研究提出了“热成像电外科速率调制优化”（ThERMO）方法，通过热成像反馈实时调整工具速度，以减少热损伤并平衡切割力。ThERMO使用“截断无味卡尔曼滤波器”（TUKF）在线识别热和机械参数，并最小化与组织变性宽度和切割力相关的参数化成本函数。实验结果表明，ThERMO在组织模型中将切割成功率提高了三倍，并将峰值切割力降低了一半，能够有效应对环境干扰，减少组织损伤，完成传统固定速度方法无法实现的切割任务。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies",
            "authors": "Liquan Wang,Jiangjie Bian,Eric Heiden,Animesh Garg",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19712",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19712",
            "arxiv_html_link": "https://arxiv.org/html/2509.19712v1",
            "abstract": "Robotic manipulation tasks involving cutting deformable objects remain challenging due to complex topological behaviors, difficulties in perceiving dense object states, and the lack of efficient evaluation methods for cutting outcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for multi-step robotic cutting tasks, that integrates cutting environment and generalized policy learning. TopoCut is built upon three core components: (1) We introduce a high-fidelity simulation environment based on a particle-based elastoplastic solver with compliant von Mises constitutive models, augmented by a novel damage-driven topology discovery mechanism that enables accurate tracking of multiple cutting pieces.\n(2) We develop a comprehensive reward design that integrates the topology discovery with a pose-invariant spectral reward model based on Laplace–Beltrami eigenanalysis, facilitating consistent and robust assessment of cutting quality. (3) We propose an integrated policy learning pipeline, where a dynamics-informed perception module predicts topological evolution and produces particle-wise, topology-aware embeddings to support PDDP—Particle-based Score-Entropy Discrete Diffusion Policy—for goal-conditioned policy learning.\nExtensive experiments demonstrate that TopoCut supports trajectory generation, scalable learning, precise evaluation, and strong generalization across diverse object geometries, scales, poses, and cutting goals. Project page: https://topocut.github.io/.",
            "introduction": "Robotic manipulation involving the cutting of deformable objects plays a critical role across diverse domains such as food processing, medical surgery, and manufacturing. Many real-world tasks require not just a single cut, but a sequence of cutting actions to segment objects into complex or structured shapes. From slicing ingredients into uniform pieces in culinary automation, to performing multi-incision procedures in robotic surgery, and executing multi-pass segmentation in industrial workflows, multi-step cutting is essential for achieving fine-grained precision. The ability to reliably plan and execute these sequential cutting operations significantly enhances efficiency, safety, and quality in autonomous systems.\n\nDespite recent progress in robotic cutting of single-material deformable objects with fixed trajectories [1, 2, 3], goal-conditioned multi-step cutting of complex deformable geometries remains a major challenge. Deformable objects often fail to separate cleanly after each cut, making outcome evaluation ambiguous [1]. Existing evaluation metrics are sensitive to pose variations and typically require explicit alignment [3]. Furthermore, dense topological changes resulting from sequential cuts are difficult to perceive from sparse or noisy observations [4, 3], hindering the effectiveness of policy learning in such settings.\n\nTo address these challenges, we introduce TopoCut, a unified framework for multi-step robotic cutting that combines high-fidelity simulation, robust evaluation, and goal-conditioned policy learning. At its core, TopoCut features a particle-based elastoplastic simulator equipped with a novel damage-driven topology discovery mechanism that enables precise tracking of multiple cutting-induced topological changes. We further design a pose-invariant spectral reward based on Laplace–Beltrami eigenanalysis to evaluate cutting outcomes consistently across varying object geometries and poses. Finally, we propose a learning pipeline that leverages a dynamics-informed perception module to produce topology-aware, particle-wise embeddings—explicitly designed to operate on sparse visual input, making it suitable for real-world robotic settings—and supports PDDP, a discrete diffusion policy model for scalable and generalizable multi-step cutting.\n\nOur contributions are organized into three core components:\n\nHigh-fidelity Simulation and Topology Discovery: We develop a robust simulation environment utilizing a novel particle-based elastoplastic solver with compliant von Mises constitutive models, coupled with an advanced particle-based topology discovery method to precisely capture and track topological changes during cutting.\n\nPose-invariant Spectral Reward: We introduce a novel reward formulation integrating the real-time topology discovery with a spectral reward function based on Laplace–Beltrami eigenanalysis, enabling consistent, pose-invariant evaluation of cutting outcomes across arbitrary object poses.\n\nDynamics-informed Policy Learning: We propose a goal-conditioned policy learning framework that employs dynamics-informed perception modules to predict topology evolution and generate particle-level, topology-aware embeddings. These embeddings support conditional score-based discrete diffusion models, enhancing the robustness and generalizability of the learned cutting strategies.\n\n1. High-fidelity Simulation and Topology Discovery: We develop a robust simulation environment utilizing a novel particle-based elastoplastic solver with compliant von Mises constitutive models, coupled with an advanced particle-based topology discovery method to precisely capture and track topological changes during cutting.\n\n2. Pose-invariant Spectral Reward: We introduce a novel reward formulation integrating the real-time topology discovery with a spectral reward function based on Laplace–Beltrami eigenanalysis, enabling consistent, pose-invariant evaluation of cutting outcomes across arbitrary object poses.\n\n3. Dynamics-informed Policy Learning: We propose a goal-conditioned policy learning framework that employs dynamics-informed perception modules to predict topology evolution and generate particle-level, topology-aware embeddings. These embeddings support conditional score-based discrete diffusion models, enhancing the robustness and generalizability of the learned cutting strategies.",
            "llm_summary": "【关注的是什么问题】  \n1. 多步切割任务中的复杂拓扑行为如何有效处理。  \n2. 如何在稀疏观测下准确评估切割结果。  \n3. 现有的切割策略学习方法在复杂可变形几何体上的局限性。  \n\n【用了什么创新方法】  \nTopoCut框架结合了高保真模拟、稳健评估和目标条件策略学习。首先，开发了一种基于粒子弹塑性求解器的模拟环境，配合损伤驱动的拓扑发现机制，精确跟踪切割引起的拓扑变化。其次，设计了一种基于拉普拉斯-贝尔特拉米特征值分析的姿态不变谱奖励，确保在不同物体几何和姿态下的一致评估。最后，提出了一种动态感知模块，生成粒子级拓扑感知嵌入，支持条件分数基础离散扩散策略（PDDP），增强了切割策略的稳健性和泛化能力。实验结果表明，TopoCut在轨迹生成、可扩展学习、精确评估和强泛化能力方面表现出色。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks",
            "authors": "Noah Geiger,Tamim Asfour,Neville Hogan,Johannes Lachner",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19696",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19696",
            "arxiv_html_link": "https://arxiv.org/html/2509.19696v1",
            "abstract": "Learning methods excel at motion generation in the information domain but are not primarily designed for physical interaction in the energy domain. Impedance Control shapes physical interaction but requires task-aware tuning by selecting feasible impedance parameters. We present Diffusion-Based Impedance Learning, a framework that combines both domains. A Transformer-based Diffusion Model with cross-attention to external wrenches reconstructs a simulated Zero-Force Trajectory (sZFT). This captures both translational and rotational task-space behavior. For rotations, we introduce a novel SLERP-based quaternion noise scheduler that ensures geometric consistency. The reconstructed sZFT is then passed to an energy-based estimator that updates stiffness and damping parameters. A directional rule is applied that reduces impedance along non-task axes while preserving rigidity along task directions.\nTraining data were collected for a parkour scenario and robotic-assisted therapy tasks using teleoperation with Apple Vision Pro. With only tens of thousands of samples, the model achieved sub-millimeter positional accuracy and sub-degree rotational accuracy. Its compact model size enabled real-time torque control and autonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller achieved smooth parkour traversal within force and velocity limits and 30/30 success rates for cylindrical, square, and star peg insertions without any peg-specific demonstrations in the training data set.\nAll code for the Transformer-based Diffusion Model, the robot controller, and the Apple Vision Pro telemanipulation framework is publicly available. These results mark an important step towards Physical AI, fusing model-based control for physical interaction with learning-based methods for trajectory generation.",
            "introduction": "Robotic behavior emerges at the interface of two fundamentally different domains. Motion planning belongs to the information domain, where learning-based methods have recently shown remarkable progress. In contrast, physical interaction is governed by the energy domain, and model-based Impedance Control has been widely adopted to guarantee stability [1] and safety [2]. Robots that manipulate in unstructured environments must bridge these domains: motions must be inferred as information, yet executed through stable, energy-consistent exchange with the environment [3]. This requirement is critical in assembly, rehabilitation, and other contact-rich tasks where visual feedback is limited and success depends on regulating interaction rather than simply following a path.\n\nWhile Impedance Control provides the framework for shaping robot–environment interaction [4], its performance critically depends on selecting appropriate stiffness and damping parameters (which may need to be adapted online) [5]. Too much stiffness can lead to jamming, too little can prevent task execution.\n\nRecent advances in contact-rich manipulation can be divided into two main strands. Model-based approaches incorporate explicit contact dynamics like friction cones, complementarity constraints, and mixed-integer formulations [6, 7, 8, 9, 10, 11, 12, 13, 14]. These methods produce behavior consistent with physical contact but face challenges: high computational cost, parameter sensitivity, and limited robustness in unstructured settings. Learning-based approaches, in contrast, operate in the information domain. Reinforcement learning and sampling-based optimization [15, 16, 17, 18], along with more recent generative models such as diffusion policies [19], flow-matching policies [20, 21], and Transformer-based policy models [22, 23], excel at motion generation across tasks and embodiments [24]. However, execution in these frameworks often relies on velocity-based or fixed-gain PD controllers. While fixed-gain PD controllers can avoid hardware damage, apparent compliance is only a byproduct of low control gains. In unstructured environments, where vision may be unreliable or occluded, explicit impedance regulation is essential to control interaction.\n\nThe Adaptive Compliance Policy of [25] is a first step toward combining generative models with compliance adaptation, but it is formulated within an admittance-control framework. Admittance control is known to struggle with transitions into and out of contact [4, 26], making it unsuitable for many assembly tasks such as peg-in-hole insertion. To our knowledge, no approach has successfully combined learning-based trajectory generation (information domain) with impedance adaptation (energy domain).\n\nThis paper addresses that gap. We present Diffusion-Based Impedance Learning, a framework that unifies generative modeling with energy-consistent Impedance Control. A conditional Transformer-based Diffusion Model reconstructs simulated Zero-Force Trajectories111The Zero-Force Trajectory (ZFT), introduced by Hogan [5], refers to the commanded equilibrium motion in the unconstrained case: the unique end-effector pose at which the interaction wrench vanishes. from contact-perturbed displacement and external wrench signals. The sZFT serves as a reconstructed equilibrium used to modulate task-space impedance. A directional adaptation scheme adapts stiffness primarily along non-task-relevant directions and preserves rigidity where needed for execution. In this way, our approach combines the strengths of learning-based methods (motion generation in the information domain) with those of model-based methods (impedance regulation in the energy domain).\n\nWe validate the framework on a KUKA LBR iiwa in two contact-rich scenarios: parkour-style obstacle traversal and multi-geometry peg-in-hole insertion. Training data were collected through telemanipulation, using Apple Vision Pro (AVP) equipped with a markerless pose tracking framework [27] integrated into the robot controller. Both tasks highlight the shortcomings of fixed impedance and simple adaptation schemes, which either jam at obstacles or fail to achieve insertion. In contrast, Diffusion-Based Impedance Learning achieves consistent success. The outcomes of Table I preview the central result of this work: bridging the information and energy domains is key to robust manipulation in unstructured environments.\n\nAll code for the Transformer-based Diffusion Model, the robot controller,\nand the Apple Vision Pro telemanipulation framework is available on our\nGitHub repository222https://github.com/StrokeAIRobotics/DiffusionBasedImpedanceLearning. Demonstration videos of all\nexperiments can be found on the project website333https://strokeairobotics.github.io/DiffusionBasedImpedanceLearning.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效结合学习方法与阻抗控制以实现物理交互？  \n2. 如何在接触丰富的任务中实现稳定的运动生成和阻抗调节？  \n3. 如何在不确定环境中提高机器人操作的成功率和鲁棒性？  \n\n【用了什么创新方法】  \n提出了一种Diffusion-Based Impedance Learning框架，结合了生成建模与能量一致的阻抗控制。通过条件Transformer-based Diffusion Model重建模拟的零力轨迹（sZFT），并采用方向适应方案调节任务空间的阻抗。该方法在KUKA LBR iiwa机器人上进行验证，展示了在公园风格障碍物穿越和多几何体插入任务中的成功率，达到了亚毫米的位置精度和亚度的旋转精度。结果表明，信息域与能量域的结合是实现不确定环境中稳健操作的关键。\n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Formal Safety Verification and Refinement for Generative Motion Planners via Certified Local Stabilization",
            "authors": "Devesh Nath,Haoran Yin,Glen Chou",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG); Systems and Control (eess.SY); Optimization and Control (math.OC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19688",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19688",
            "arxiv_html_link": "https://arxiv.org/html/2509.19688v1",
            "abstract": "We present a method for formal safety verification of learning-based generative motion planners. Generative motion planners (GMPs) offer advantages over traditional planners, but verifying the safety and dynamic feasibility of their outputs is difficult since neural network verification (NNV) tools scale only to a few hundred neurons, while GMPs often contain millions. To preserve GMP expressiveness while enabling verification, our key insight is to imitate the GMP by stabilizing references sampled from the GMP with a small neural tracking controller and then applying NNV to the closed-loop dynamics. This yields reachable sets that rigorously certify closed-loop safety, while the controller enforces dynamic feasibility. Building on this, we construct a library of verified GMP references and deploy them online in a way that imitates the original GMP distribution whenever it is safe to do so, improving safety without retraining. We evaluate across diverse planners, including diffusion, flow matching, and vision-language models, improving safety in simulation (on ground robots and quadcopters) and on hardware (differential-drive robot).",
            "introduction": "Motion planning has been transformed by generative models like diffusion and conditional flow matching (CFM) [1, 2], which learn multimodal trajectory distributions and enable generative motion planners (GMPs) that produce diverse plans from inputs like language or images [3, 4, 5, 6].\nHowever, ensuring that GMP-generated trajectories satisfy safety and dynamic feasibility is difficult: GMPs often contain millions of parameters, making neural network verification (NNV) [7] intractable, limiting their use in safety-critical settings [6]. NNV provides hard guarantees via set-based reachability but only scales to controllers with a few hundred neurons [8, 9, 10, 11]. More scalable statistical methods [12, 13, 3, 14] yield weaker probabilistic guarantees or require prohibitive samples over long horizons. Thus, existing work trades off between expressive large models lacking hard guarantees and small verifiable models unable to capture complex behaviors.\n\nTo bridge this gap, we propose SaGe-MP (Safe Generative Motion Planning), a method that provides hard safety and dynamic feasibility guarantees for GMP-generated motion plans. Our key insight is that while NNV tools cannot directly certify the GMP, they can certify a small neural tracking controller that locally stabilizes the system around GMP-sampled references.\nReachability analysis of the resulting closed-loop system yields hard assurances of safety and dynamic feasibility over a continuum of inputs. Here, the GMP acts only as an open-loop plan generator, while verification is performed on the closed-loop dynamics induced by tracking a fixed GMP plan, resulting in a smaller computational graph that makes NNV tractable. By tracking GMP references under the true dynamics, the controller also projects potentially dynamically-infeasible GMP plans onto feasible trajectories. To preserve the original GMP behavior if possible, we develop a trajectory-library approach: multiple GMP references are sampled offline, certified as safely trackable via NNV, and deployed online in a way that mimics the potentially multimodal GMP output. In this sense, our method is a lightweight GMP refinement that enhances safety and dynamic feasibility without costly GMP retraining. Our contributions are:\n\nAn NNV-based method for formal safety verification of large GMPs that decouples trajectory generation from neural feedback loop verification, preserving planner expressiveness while providing hard safety guarantees.\n\nAn NNV-based method for formal safety verification of large GMPs that decouples trajectory generation from neural feedback loop verification, preserving planner expressiveness while providing hard safety guarantees.\n\nA method to certify a neural trajectory tracking controller that stabilizes dynamically-infeasible GMP references, producing safe, feasible trajectories.\n\nA trajectory-library method that stores certified-safe GMP plans and executes them online in a way that preserves the original behavior whenever safe. We prove sample complexity bounds for a target imitation error.\n\nExtensive simulation and real-world validation demonstrating safe stabilization of references from diverse generative models (diffusion, flow matching, VLMs, neural ODEs) on challenging nonlinear systems (e.g., 12D quadcopter, and learned NN dynamics).",
            "llm_summary": "【关注的是什么问题】  \n1. 如何确保生成运动规划器（GMP）生成的轨迹满足安全性和动态可行性？  \n2. 如何在不重新训练GMP的情况下提高其安全性？  \n\n【用了什么创新方法】  \n提出了一种名为SaGe-MP的方法，通过使用小型神经跟踪控制器对GMP生成的参考轨迹进行局部稳定化，从而实现对GMP的形式安全验证。该方法将轨迹生成与神经反馈回路验证解耦，允许使用神经网络验证（NNV）工具对闭环动态进行可行性和安全性分析。通过构建一个经过验证的轨迹库，能够在安全时在线执行这些轨迹，保持GMP的原始行为。该方法在多种生成模型上进行了广泛的仿真和实际验证，展示了在复杂非线性系统（如12维四旋翼和学习的神经网络动态）上的安全稳定化效果。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains",
            "authors": "Dongzhe Zheng,Wenjie Mei",
            "subjects": "Robotics (cs.RO); Dynamical Systems (math.DS)",
            "comment": "Accepted by NeurIPS 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.19672",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19672",
            "arxiv_html_link": "https://arxiv.org/html/2509.19672v1",
            "abstract": "Stochastic optimal control methods often struggle in complex non-convex landscapes, frequently becoming trapped in local optima due to their inability to learn from historical trajectory data. This paper introduces Memory-Augmented Potential Field Theory, a unified mathematical framework that integrates historical experience into stochastic optimal control. Our approach dynamically constructs memory-based potential fields that identify and encode key topological features of the state space, enabling controllers to automatically learn from past experiences and adapt their optimization strategy. We provide a theoretical analysis showing that memory-augmented potential fields possess non-convex escape properties, asymptotic convergence characteristics, and computational efficiency. We implement this theoretical framework in a Memory-Augmented Model Predictive Path Integral (MPPI) controller that demonstrates significantly improved performance in challenging non-convex environments. The framework represents a generalizable approach to experience-based learning within control systems (especially robotic dynamics), enhancing their ability to navigate complex state spaces without requiring specialized domain knowledge or extensive offline training.",
            "introduction": "Stochastic optimal control has proven highly effective for handling nonlinear systems and uncertain environments, finding widespread application in robotics, reinforcement learning, and complex system control. Among these approaches, Model Predictive Path Integral (MPPI) control stands out for its ability to handle continuous state-action spaces through stochastic sampling and exponentially weighted averaging. However, these methods still face significant theoretical and practical challenges when confronting highly non-convex value function landscapes.\n\nFrom an optimization perspective, stochastic optimal control problems can be viewed as trajectory optimization over a value function landscape. When this landscape exhibits complex non-convex characteristics, optimization processes may become trapped in local optima, unable to reach global solutions. While introducing noise sampling (as in MPPI’s random perturbations) can somewhat mitigate this issue, significantly non-convex features often lead to inefficient sampling or control instability when noise is simply increased.\n\nFrom a dynamical systems perspective, non-convex value functions correspond to systems with multiple attractors and unstable equilibrium points. Control algorithms need to identify these features and, when necessary, guide the system across energy barriers to escape suboptimal attractor regions. Traditional stochastic control methods have limited capabilities in this regard, as they lack awareness and memory of the state space’s topological structure.\n\nTraditional stochastic optimal controllers lack memory—operating solely on current states without learning from past trajectories. This design means controllers might repeatedly fall into the same suboptimal regions, failing to extract experience from previous \"failures.\" In contrast, advanced cognitive systems (like humans) dynamically adjust decision strategies based on prior experience when exploring complex environments.\n\nThis paper addresses a fundamental question: How can we integrate \"memory\" mechanisms into stochastic optimal control frameworks, enabling controllers to automatically learn state space topological features from historical trajectories and adjust optimization strategies accordingly? We introduce Memory-Augmented Potential Field Theory, integrating historical state experience into stochastic optimal control through dynamic potential fields that automatically identify and encode topological features of the state space during execution. These fields act as correction terms to reshape the value function landscape, enabling adaptive navigation of non-convex optimization problems. Our framework provides: 1) automatic detection and encoding of problematic regions like local minima and low-gradient areas, 2) dynamic reshaping of value functions for efficient escape from suboptimal attractors, 3) theoretical guarantees for convergence to global optima, and 4) significant performance improvements in complex control tasks without requiring extensive offline training.\n\nOur approach uniquely integrates memory mechanisms with dynamical systems theory and stochastic optimal control, analyzing memory’s impact on non-convex optimization topologically. Beyond simply storing experiences, our method automatically identifies key state space features and dynamically reshapes value function landscapes, enabling \"meta-optimization\" capabilities. The code has been anonymized and is available at https://anonymous.4open.science/r/MA_MPPI-6555.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将“记忆”机制整合到随机最优控制框架中，以便自动学习状态空间的拓扑特征？  \n2. 如何在复杂的非凸环境中提高控制器的优化策略和性能？  \n\n【用了什么创新方法】  \n本研究提出了记忆增强潜力场理论，通过动态构建基于记忆的潜力场，自动识别和编码状态空间的关键拓扑特征。该方法允许控制器从历史轨迹中学习，调整优化策略。理论分析表明，记忆增强潜力场具有非凸逃逸特性和渐近收敛性。通过实现记忆增强模型预测路径积分（MPPI）控制器，在复杂非凸环境中显著提高了性能，展示了经验学习在控制系统中的广泛适用性。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "RoboSSM: Scalable In-context Imitation Learning via State-Space Models",
            "authors": "Youngju Yoo,Jiaheng Hu,Yifeng Zhu,Bo Liu,Qiang Liu,Roberto Martín-Martín,Peter Stone",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19658",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19658",
            "arxiv_html_link": "https://arxiv.org/html/2509.19658v1",
            "abstract": "In-context imitation learning (ICIL) enables robots to learn tasks from prompts consisting of just a handful of demonstrations. By eliminating the need for parameter updates at deployment time, this paradigm supports few-shot adaptation to novel tasks.\nHowever, recent ICIL methods rely on Transformers, which have computational limitations and tend to underperform when handling longer prompts than those seen during training.\nIn this work, we introduce RoboSSM, a scalable recipe for in-context imitation learning based on state-space models (SSM).\nSpecifically, RoboSSM replaces Transformers with Longhorn – a state-of-the-art SSM that provides linear-time inference and strong extrapolation capabilities, making it well-suited for long-context prompts.\nWe evaluate our approach on the LIBERO benchmark and compare it against strong Transformer-based ICIL baselines.\nExperiments show that RoboSSM extrapolates effectively to varying numbers of in-context demonstrations, yields high performance on unseen tasks, and remains robust in long-horizon scenarios.\nThese results highlight the potential of SSMs as an efficient and scalable backbone for ICIL.\nOur code is available at https://github.com/youngjuY/RoboSSM.",
            "introduction": "Imitation Learning (IL) is a powerful framework that enables robots to learn behaviors from demonstrations without explicit programming or reward design [1, 2].\nWhile IL has achieved notable success in manipulation and navigation tasks, a key limitation of conventional imitation learning lies in its restricted adaptation capability, particularly when faced with new tasks.\nEven with models trained on large multi-task datasets [3, 4, 5, 6], adapting to novel tasks still requires collecting a large amount of task-specific data and retraining, which can be computationally costly and often unstable [7, 8].\nTo address this challenge, In-Context Imitation Learning (ICIL) introduces a new paradigm, inspired by the success of large language models (LLMs) [9, 10, 11] in adapting to unseen language tasks through few-shot learning [12].\nICIL integrates the concept of prompting into imitation learning [13, 14, 15, 16, 12, 17, 18, 19], allowing the model to infer and perform tasks based on a prompt composed of demonstrations, with no post-demonstration training.\n\nGiven that ICIL formulates imitation learning as a sequence modeling problem, recent ICIL approaches have naturally adopted Transformer-based models as their primary architecture [17, 19, 18].\nAlthough Transformers are the dominant architecture for sequence modeling [20], their time complexity scales quadratically with sequence length, and they struggle to extrapolate beyond training lengths [21, 22].\nFor ICIL to handle long prompts efficiently at test time, it is essential to adopt alternatives to Transformers that enhance scalability with input length.\n\nIn this paper, we introduce RoboSSM, a scalable in-context learning framework that replaces Transformers with state-space models (SSMs).\nSpecifically, RoboSSM utilizes Longhorn [23], a state-of-the-art SSM with linear inference time and strong extrapolation capability for long-context sequences.\nLeveraging these properties, RoboSSM can process substantially longer prompts at test time compared to previous Transformer-based ICIL methods.\nWe also investigate adapting Longhorn to ICIL via β\\beta-scaling ablations, which encourage the model to attend to demonstration prompts.\n\nOn the LIBERO [24] benchmark, RoboSSM uniquely benefits from using more in-context examples, maintaining high success rates on unseen tasks when trained with only a few demonstrations.\nFor instance, on the task pick up the plate and place it in the tray, where the plate object was unseen during training, RoboSSM achieves its highest performance when prompted with 32 demonstrations, despite being trained on only two.\nFurthermore, our framework performs well on unseen long-horizon tasks, which we simulate by repeating frames in the demonstrations to create time-dilated scenarios.\nConsequently, RoboSSM handles test-time demonstration prompts up to 16 times longer than those seen in training while maintaining linear inference time, whereas Transformer-based ICIL methods sharply degrade once the test prompt exceeds the training length.\nThese findings confirm that RoboSSM establishes a scalable in-context imitation learning framework by effectively leveraging long-range contextual information.\nOur project video is available at RoboSSM-video.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高机器人在新任务上的适应能力，特别是在少量示范的情况下。  \n2. 传统的Transformer模型在处理长上下文提示时的计算限制和性能不足。  \n3. 如何在不进行参数更新的情况下实现高效的在-context模仿学习。  \n\n【用了什么创新方法】  \nRoboSSM引入了一种基于状态空间模型（SSM）的可扩展在-context模仿学习框架，替代了计算复杂度高的Transformer。使用Longhorn SSM，RoboSSM实现了线性推理时间和强大的外推能力，能够有效处理长上下文提示。实验表明，RoboSSM在LIBERO基准测试中表现优异，能够在仅用少量示范的情况下，成功适应未见过的任务，并在长时间范围任务中保持高性能。具体而言，在未见物体的任务中，RoboSSM在使用32个示范时表现最佳，且能够处理比训练时长16倍的测试提示。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Minimalistic Autonomous Stack for High-Speed Time-Trial Racing",
            "authors": "Mahmoud Ali,Hassan Jardali,Youwei Yu,Durgakant Pushp,Lantao Liu",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "The data associated with this paper is available atthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.19636",
            "code": "https://doi.org/10.5281/zenodo.17187680",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19636",
            "arxiv_html_link": "https://arxiv.org/html/2509.19636v1",
            "abstract": "Autonomous racing has seen significant advancements, driven by competitions such as the Indy Autonomous Challenge (IAC) and the Abu Dhabi Autonomous Racing League (A2RL). However, developing an autonomous racing stack for a full-scale car is often constrained by limited access to dedicated test tracks, restricting opportunities for real-world validation.\nWhile previous work typically requires extended development cycles and significant track time, this paper introduces a minimalistic autonomous racing stack for high-speed time-trial racing that emphasizes rapid deployment and efficient system integration with minimal on-track testing.\nThe proposed stack was validated on real speedways, achieving a top speed of 206 km h−1206\\text{\\,}\\mathrm{km}\\text{\\,}{\\mathrm{h}}^{-1} within just 11 hours’ practice run on the track with 325 km325\\text{\\,}\\mathrm{km} in total. Additionally, we present the system performance analysis, including tracking accuracy, vehicle dynamics, and safety considerations, offering insights for teams seeking to rapidly develop and deploy an autonomous racing stack with limited track access.",
            "introduction": "Autonomous racing has gained significant traction in recent years, advancing both research and real-world deployment in high-speed autonomy. Competitions such as\nIAC [1] and A2RL [2] provide a platform for testing cutting-edge autonomous systems in extreme conditions. These events have driven advancements in perception, planning, and control algorithms [3, 4, 5, 6], leading to fully autonomous race cars competing at 290 km h−1290\\text{\\,}\\mathrm{km}\\text{\\,}{\\mathrm{h}}^{-1} [7].\nDespite these achievements, developing an autonomous racing stack for full-scale vehicles remains a resource-intensive endeavor due to the limited availability of dedicated racetracks for testing. Prior work has demonstrated impressive results but often relies on years of development and extensive track-testing time, making rapid deployment difficult for new teams.\n\nTo address this challenge, we present a minimalistic autonomous racing stack designed for high-speed time-trial racing with a focus on single-car speed performance and rapid deployment.\nOur approach strategically maximizes track time utilization, enabling a fully functional autonomy stack with minimal on-track testing.\nThe proposed system was implemented on the IAC AV-24 race car [8] and validated on real speedways, achieving a top speed of 206 km h−1206\\text{\\,}\\mathrm{km}\\text{\\,}{\\mathrm{h}}^{-1} with only 11 track hours and 325 km325\\text{\\,}\\mathrm{km} of practice runs.\nThe key contributions of this work include:\nI) A minimalistic autonomous racing stack tailored for high-speed solo racing.\nII) A system integration of the proposed stack into the AV-2424 racing car.\nIII) An evaluation of the proposed stack\nwith emphasis on controllers performance and dynamics analysis.\nIV) A discussion of the safety measures incorporated into the system, along with an analysis of failure cases.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在有限的赛道测试条件下快速开发和部署自主赛车系统？  \n2. 如何提高自主赛车在高速度下的性能和安全性？  \n\n【用了什么创新方法】  \n本研究提出了一种简约的自主赛车堆栈，专注于高速度单车时间试验，旨在通过最大化赛道时间利用率来实现快速部署。该系统在真实赛道上进行了验证，仅用11小时的练习运行便达到了206 km/h的最高速度，总计325 km的测试里程。研究还分析了系统性能，包括跟踪精度、车辆动态和安全性，为寻求快速开发和部署自主赛车的团队提供了宝贵的见解。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data",
            "authors": "Ryan Punamiya,Dhruv Patel,Patcharapong Aphiwetsa,Pranav Kuppili,Lawrence Y. Zhu,Simar Kareer,Judy Hoffman,Danfei Xu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "Accepted at 39th Conference on Neural Information Processing Systems (NeurIPS 2025) and Oral at Conference on Robot Learning (CoRL 2025)",
            "pdf_link": "https://arxiv.org/pdf/2509.19626",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19626",
            "arxiv_html_link": "https://arxiv.org/html/2509.19626v1",
            "abstract": "Egocentric human experience data presents a vast resource for scaling up end-to-end imitation learning for robotic manipulation. However, significant domain gaps in visual appearance, sensor modalities, and kinematics between human and robot impede knowledge transfer. This paper presents EgoBridge, a unified co-training framework that explicitly aligns the policy latent spaces between human and robot data using domain adaptation. Through a measure of discrepancy on the joint policy latent features and actions based on Optimal Transport (OT), we learn observation representations that not only align between the human and robot domain but also preserve the action-relevant information critical for policy learning.\nEgoBridge achieves a significant absolute policy success rate improvement by 44% over human-augmented cross-embodiment baselines in three real-world single-arm and bimanual manipulation tasks. EgoBridge also generalizes to new objects, scenes, and tasks seen only in human data, where baselines fail entirely. Videos and additional information can be found at https://ego-bridge.github.io/",
            "introduction": "Supervised imitation learning methods such as behavior cloning have emerged as a promising path to scaling robot performance across diverse objects, tasks, and environments. However, while large-scale models in vision and language have achieved remarkable generalization through Internet-sourced data, replicating this success in robotics remains challenging due to the labor-intensive nature of collecting teleoperated demonstrations. Deploying physical robots to many new environments to collect data with enough coverage and diversity is economically and practically intractable.\n\nIn this work, we aim to enable robots to learn from egocentric recordings of natural human behavior, collected by increasingly ubiquitous wearable devices (e.g., XR devices and smart glasses). Without a robot in the loop, such data is cheap and scalable to collect and captures natural human interactions with the world. More importantly, it reflects the embodied human experience, as it contains both observations (e.g., egocentric RGB images) and actions (e.g., hand motions). Unlike unstructured data sources such as Internet videos, the rich embodied information allows us to treat human data and robot data as equal parts in a continuous spectrum of demonstration data and potentially learn from both with a unified learning framework.\n\nHowever, the multitudes of domain gaps between human and robot pose significant challenges in designing such a framework. Human bodies and robots have different visual appearances. Even within a shared action space, kinematic differences can lead to behavior distribution shifts. Robots also have additional sensing modalities such as wrist cameras that are often missing from embodied human data. While recent works such as EgoMimic kareer2024egomimicscalingimitationlearning  have attempted to bridge the embodiment gaps with techniques such as visual masking, data normalization, and motion retargeting, such domain gaps still largely remain. More broadly, simply co-training from cross-domain data does not automatically yield effective knowledge transfer, as suggested by recent studies wei2025empirical . Such challenges prevent policies from scaling their performance primarily with human data.\n\nWe formalize the human-robot cross-embodiment learning problem as a domain adaptation problem, where human and robot data represent two labeled distributions with significant covariate shifts in observations due to embodiment gaps. Standard domain adaptation approaches often rely on global distribution alignment techniques such as adversarial training tzeng2017adversarialdiscriminativedomainadaptation  and maximum mean discrepancy minimization long2017deeptransferlearningjoint . However, they primarily address high-level tasks such as image classification and fail to preserve detailed action-relevant information—a critical requirement for robot learning where actions and observations are temporally correlated under compounding covariate shift.\n\nTo address these challenges, we propose EgoBridge, a novel domain adaptation approach that uses Optimal Transport (OT) to align latent representations from human and robot domains as part of the policy co-training objective. Unlike conventional domain alignment methods, our OT formulation explicitly exploits the inherent relationship between motion similarities in human and robot domains to form pseudo-pairs as supervision for the adaptation process. Concretely, we use the dynamic time warping (DTW) distance among human and robot motion trajectories to shape the OT ground cost. This encourages the transport map to find a minimal-cost coupling between human and robot data exhibiting similar behaviors. As such, EgoBridge aligns policy representations across domains via a differentiable OT loss (Sinkhorn distance), while preserving action-relevant information for policy learning. Importantly, we show that EgoBridge learns a shared latent representation that generalizes beyond the paired data. This enables the policy to learn behaviors observed only within the human dataset, effectively enabling the policy to scale primarily with human data.\n\nWe evaluate EgoBridge on both a reproducible simulation benchmark task and three challenging real-world manipulation tasks. Our results show that EgoBridge consistently improves policy success rates compared to human-augmented cross-embodiment baselines, for up to 44% absolute success rate improvement, and effectively transfers behaviors from diverse human demonstrations to robotic execution in tasks requiring spatial, visual, and task generalization.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效地从人类的第一人称数据中学习模仿行为以提升机器人操作能力。  \n2. 人类与机器人之间在视觉外观、传感器模态和运动学上的显著领域差距如何影响知识转移。  \n3. 现有的领域适应方法如何未能有效保留与动作相关的信息，导致机器人学习性能受限。  \n\n【用了什么创新方法】  \n本研究提出了EgoBridge，一个统一的共训练框架，通过最优传输（Optimal Transport, OT）对人类和机器人数据的策略潜在空间进行显式对齐。该方法利用动态时间规整（Dynamic Time Warping, DTW）距离来构建人类与机器人运动轨迹之间的伪配对，从而在适应过程中形成监督信号。EgoBridge通过可微分的OT损失（Sinkhorn距离）对跨领域的策略表示进行对齐，同时保留与动作相关的信息。实验结果表明，EgoBridge在三个真实世界的单臂和双臂操作任务中实现了高达44%的绝对成功率提升，并能够有效地将人类演示中的行为转移到机器人执行中，展示了其在新物体、场景和任务上的泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots",
            "authors": "Qingxi Meng,Emiliano Flores,Carlos Quintero-Peña,Peizhu Qian,Zachary Kingston,Shannan K. Hamlin,Vaibhav Unhelkar,Lydia E. Kavraki",
            "subjects": "Robotics (cs.RO)",
            "comment": "under review",
            "pdf_link": "https://arxiv.org/pdf/2509.19610",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19610",
            "arxiv_html_link": "https://arxiv.org/html/2509.19610v1",
            "abstract": "In this work, we address the problem of planning robot motions for a high-degree-of-freedom (d\n\no\nf) robot that effectively achieves a given perception task while the robot and the perception target move in a dynamic environment. Achieving navigation and perception tasks simultaneously is challenging, as these objectives often impose conflicting requirements. Existing methods that compute motion under perception constraints fail to account for obstacles, are designed for low-d\n\no\nf robots, or rely on simplified models of perception. Furthermore, in dynamic real-world environments, robots must replan and react quickly to changes and directly evaluating the quality of perception (e.g., object detection confidence) is often expensive or infeasible at runtime. This problem is especially important in human-centered environments such as homes and hospitals, where effective perception is essential for safe and reliable operation. To address these challenges, we propose a GPU-parallelized perception-score-guided probabilistic roadmap planner with a neural surrogate model (ps-prm). The planner explicitly incorporates the estimated quality of a perception task into motion planning for high-d\n\no\nf robots. Our method uses a learned model to approximate perception scores and leverages GPU parallelism to enable efficient online replanning in dynamic settings. We demonstrate that our planner, evaluated on high-d\n\no\nf robots, outperforms baseline methods in both static and dynamic environments in both simulation and real-robot experiments.",
            "introduction": "Achieving mobile manipulation (e.g., navigating to a table to grasp a tool) alongside perception tasks (e.g., tracking an object or a human) remains highly challenging. In this work, we address the problem of planning robot motions for a high-degree-of-freedom (d\n\no\nf) robot that effectively achieves a given perception task while the robot and the perception target move in a dynamic environment. A fundamental challenge emerges: the objectives of motion and perception often conflict—perception tasks impose non-trivial constraints on robot motion, as performance varies with object shape, occlusion, distance, and viewing angle [1, 2, 3]. For example, detecting human faces often requires maintaining a frontal view, while reliable object tracking typically benefits from close proximity. At the same time, motion planning for high-degree-of-freedom (d\n\no\nf) robots must follow kinematic constraints, avoid obstacles and nearby humans, and account for the complex, nonlinear relationship between the robot’s configuration and the camera pose.\n\nAs an example, consider the situation in Fig. 1. A mobile robot with wrist-mounted camera must navigate a cluttered environment while observing a monitor at the end of the hallway. The robot must navigate through clutter with a differential drive base while simultaneously maintaining tracking performance, i.e., it avoids occlusions and attains high tracking confidence.\n\nThese challenges are further compounded in dynamic environments, where objects and people move. In such settings, robots must continuously adapt to changes while preserving high perception quality [4, 5, 6, 7].\nFor example, a key motivation of this work comes from human-centered environments such as nurse training settings [8, 9], where robotic tutors are used to support skill acquisition. In such a system [10], the robot must continuously monitor a trainee to detect compliance with sterile techniques and provide timely feedback while simultaneously navigating the environment. Fig. 7 and Fig. 10 illustrate this challenge, where the robot must continuously track nurses’ faces while navigating through a cluttered clinical environment.\nHowever, directly evaluating perception performance (e.g., detection confidence) in such settings can be prohibitively expensive or even impossible. This motivates the need for efficient mechanisms to approximate perception quality and support fast, responsive replanning in high-dimensional spaces.\n\nHow do existing planning methods tackle planning for perception in high d\n\no\nf setting?\nExisting planning methods that consider both motion and perception have several limitations. Informative path planners [11, 12, 13, 14] aim to find paths that maximize information gain while respecting constraints like path length or budget, but they often become computationally expensive, especially when the problem size grows. Perception-aware path planners are typically designed for low-d\n\no\nf systems [15, 16, 17, 18, 19, 20] or rely on overly simplified perception models [21], such as keeping a point of interest centered in the field of view. Active perception approaches [22, 23, 24, 25, 26, 27, 28, 29, 30] focus on selecting discrete viewpoints to maximize scene understanding, rather than ensuring consistent perception quality along a motion trajectory. Moreover, most of these approaches are not designed to operate in dynamic environments where objects or humans may move and where continuous adaptation is needed. As a result, they lack the responsiveness required for real-time perception-aware planning in practical settings.\n\nIn this paper, we propose Perception-Score-guided Probabilistic Roadmap Planning (PS-PRM)—a GPU-parallelized, roadmap-based planner that explicitly incorporates the estimated quality of a given perception task (e.g., object detection confidence) into motion planning for high-degree-of-freedom (DOF) robots. The perception score is predicted by a neural surrogate model trained to approximate perception scores for specific tasks.\nps-prm jointly optimizes for both motion and perception by associating each configuration with an estimated perception score and using these scores to guide roadmap construction and path selection. To efficiently approximate perception quality during planning, we introduce a neural surrogate model trained on data from various objects and human targets. To account for occlusions in cluttered environments, we incorporate a ray-casting-based pipeline that dynamically adjusts perception estimates. Finally, to enable fast replanning in dynamic environments, we develop a GPU-parallelized framework that executes collision checking, forward kinematics (FK), and perception evaluation in batch. These components allow our method to scale to high-dimensional planning problems while maintaining robust perception performance in both static and dynamic scenarios.\n\nIn this work, we make the following key contributions:\n\nA sampling-based roadmap planner, ps-prm, that integrates perception quality into motion planning for high-d\n\no\nf robots.\n\nA perception score estimation pipeline that uses a neural surrogate model and ray casting to efficiently approximate detection quality while accounting for occlusions in cluttered scenes.\n\nA batch-processing infrastructure that accelerates collision checking, forward kinematics, and perception evaluation using GPU parallelism, enabling real-time replanning.\n\nExtensive simulation and real-robot experiments on three different high-d\n\no\nf robots, demonstrating that ps-prm consistently improves both perception performance and planning efficiency compared to baseline methods.\n\n1. A sampling-based roadmap planner, ps-prm, that integrates perception quality into motion planning for high-d\n\no\nf robots.\n\n2. A perception score estimation pipeline that uses a neural surrogate model and ray casting to efficiently approximate detection quality while accounting for occlusions in cluttered scenes.\n\n3. A batch-processing infrastructure that accelerates collision checking, forward kinematics, and perception evaluation using GPU parallelism, enabling real-time replanning.\n\n4. Extensive simulation and real-robot experiments on three different high-d\n\no\nf robots, demonstrating that ps-prm consistently improves both perception performance and planning efficiency compared to baseline methods.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在动态环境中为高自由度机器人规划运动和感知任务？  \n2. 现有方法在处理高自由度机器人运动和感知时的局限性是什么？  \n3. 如何有效评估感知质量以支持快速的在线重规划？  \n\n【用了什么创新方法】  \n提出了一种名为ps-prm的GPU并行化感知评分引导概率路网规划器，该方法将感知任务的质量估计纳入高自由度机器人的运动规划中。通过训练神经代理模型来近似感知评分，并利用GPU并行性实现动态环境中的高效在线重规划。实验结果表明，ps-prm在静态和动态环境中均优于基线方法，显著提高了感知性能和规划效率。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting",
            "authors": "Sander Tonkens,Nikhil Uday Shinde,Azra Begzadić,Michael C. Yip,Jorge Cortés,Sylvia L. Herbert",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "The first three authors contributed equally. This work has been accepted for publication at the Conference on Robot Learning",
            "pdf_link": "https://arxiv.org/pdf/2509.19597",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19597",
            "arxiv_html_link": "https://arxiv.org/html/2509.19597v1",
            "abstract": "The widespread deployment of autonomous systems in safety-critical environments such as urban air mobility hinges on ensuring reliable, performant, and safe operation under varying environmental conditions.\nOne such approach, value function-based safety filters, minimally modifies a nominal controller to ensure safety.\nRecent advances leverage offline learned value functions to scale these safety filters to high-dimensional systems.\nHowever, these methods assume detailed priors on all possible sources of model mismatch, in the form of disturbances in the environment – information that is rarely available in real world settings.\nEven in well-mapped environments like urban canyons or industrial sites, drones encounter complex, spatially-varying disturbances arising from payload-drone interaction, turbulent airflow, and other environmental factors.\nWe introduce space2time, which enables safe and adaptive deployment of offline-learned safety filters under unknown, spatially-varying disturbances.\nThe key idea is to reparameterize spatial variations in disturbance as temporal variations, enabling the use of precomputed value functions during online operation.\nWe validate space2time on a quadcopter through extensive simulations and hardware experiments, demonstrating significant improvement over baselines.",
            "introduction": "Autonomous systems are increasingly deployed in safety-critical environments subject to variable conditions, where ensuring reliable and safe operation is of paramount importance.\nFor instance, a drone operating in mapped environments such as urban canyons or shipyards must remain within a known safe region despite complex, spatially-varying wind disturbances.\nRather than designing a bespoke performant, yet safe, controller for each task, a more modular approach uses a safety filter.\nThese filters monitor a nominal, high-performance controller in real-time and intervene minimially-only when necessary to enforce guarantees without unduly compromising task performance [1].\nPopular approaches for constructing such filters include Control Barrier Functions (CBFs) [2] and Hamilton-Jacobi Reachability (HJR) analysis [3].\nA recent line of work merges these two paradigms, leveraging reachability-based value functions as barrier certificates to construct safety filters with formal guarantees [4, 5, 6].\n\nHowever, these methods face significant practical challenges. A primary limitation is their reliance on an accurate, pre-specified model of the system’s dynamics and its operational domain-the set of conditions, such as expected wind patterns, the system is designed to operate in.\nSecond, each method faces inherent hurdles: HJR analysis is limited by the curse of dimensionality, making it intractable for high-dimensional systems, while the systematic synthesis of a valid CBF for general nonlinear systems remains an open problem.\n\nTo overcome these practical limitations, learning-based approaches have gained prominence, seeking to approximate safety value functions or barrier certificates directly from data [7, 8, 9].\nHowever, these learned approaches often assume a static operational domain that is known beforehand.\nThis makes them brittle when faced with environmental conditions that shift during and across deployments, forcing a choice between unsafe behavior in the face of novelty or an overly conservative policy designed for the worst case [10].\n\nOffline learning of a value function for a safety filter relies on a joint system-environment model that captures the true system’s runtime behavior.\nSuch a model is infeasible in environments with spatially varying disturbances, e.g., wind in urban canyons [11, 12], which are unknown a priori and even differ across deployments.\nA compounding challenge arises because disturbance measurements are typically obtained at a slower rate than control inputs, due to practical sensing and computational constraints.\nHowever, this slower update rate means unmodeled spatial variation can cause significant changes between consecutive measurements, leading to safety violations if ignored.\nOur insight is that spatial variations in disturbance appear as temporal variations along a trajectory.\nBy learning a time-varying safety value function that explicitly accounts for disturbance evolution over time, we implicitly capture spatial variations along trajectories, enabling their use as online safety filters.\nThis work takes a step towards bridging offline-learned value functions with online adaptation in evolving operational domains.\nOur main contributions are:\n\nWe introduce a safety value function formulation that is explicitly conditioned on a disturbance’s temporal rate of change.\n\nWe introduce a safety value function formulation that is explicitly conditioned on a disturbance’s temporal rate of change.\n\nWe use this value function formulation to propose space2time.\nOur approach reparameterizes spatial variations as temporal variations in disturbance.\nThis ensures safety in the presence of unknown, spatially varying disturbances through the use of an adaptive safety filter that leverages our offline-learned value functions.\n\nWe validate space2time on a quadcopter through extensive simulations and hardware experiments demonstrating substantial improvements in safety compared to existing approaches without significantly sacrificing performance.\n\n1. We introduce a safety value function formulation that is explicitly conditioned on a disturbance’s temporal rate of change.\n\n2. We use this value function formulation to propose space2time.\nOur approach reparameterizes spatial variations as temporal variations in disturbance.\nThis ensures safety in the presence of unknown, spatially varying disturbances through the use of an adaptive safety filter that leverages our offline-learned value functions.\n\n3. We validate space2time on a quadcopter through extensive simulations and hardware experiments demonstrating substantial improvements in safety compared to existing approaches without significantly sacrificing performance.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在未知的空间变化干扰下确保自主系统的安全性和性能。  \n2. 现有的安全过滤方法依赖于准确的系统动态模型，如何克服这一限制。  \n3. 如何将离线学习的价值函数与在线适应相结合，以应对不断变化的操作环境。  \n\n【用了什么创新方法】  \n本文提出了space2time方法，通过将空间变化的干扰重新参数化为时间变化，利用离线学习的安全价值函数进行在线适应。该方法显著提高了在复杂环境中的安全性，经过广泛的仿真和硬件实验验证，结果显示与现有方法相比，安全性有显著改善，同时性能损失不大。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping",
            "authors": "Chad R. Samuelson,Abigail Austin,Seth Knoop,Blake Romrell,Gabriel R. Slade,Timothy W. McLain,Joshua G. Mangelson",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19579",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19579",
            "arxiv_html_link": "https://arxiv.org/html/2509.19579v1",
            "abstract": "Outdoor intelligent autonomous robotic operation relies on a sufficiently expressive map of the environment. Classical geometric mapping methods retain essential structural environment information, but lack a semantic understanding and organization to allow high-level robotic reasoning. 3D scene graphs (3DSGs) address this limitation by integrating geometric, topological, and semantic relationships into a multi-level graph-based map. Outdoor autonomous operations commonly rely on terrain information either due to task-dependence or the traversability of the robotic platform. We propose a novel approach that combines indoor 3DSG techniques with standard outdoor geometric mapping and terrain-aware reasoning, producing terrain-aware place nodes and hierarchically organized regions for outdoor environments. Our method generates a task-agnostic metric-semantic sparse map and constructs a 3DSG from this map for downstream planning tasks, all while remaining lightweight for autonomous robotic operation. Our thorough evaluation demonstrates our 3DSG method performs on par with state-of-the-art camera-based 3DSG methods in object retrieval and surpasses them in region classification while remaining memory efficient. We demonstrate its effectiveness in diverse robotic tasks of object retrieval and region monitoring in both simulation and real-world environments.",
            "introduction": "Autonomous robotic systems within large-scale outdoor environments have the potential to address a wide range of fundamental societal problems, including search and rescue, forest fires, food delivery, and others.\nHowever, such robotic systems require the ability to robustly and reliably localize within, map, and interpret outdoor scenes at large scales. In this context, we focus on the development of metric-semantic mapping techniques that enable large-scale autonomy in outdoor scenes.\n\nDue to its range, accuracy, and 360∘360^{\\circ} field-of-view, LiDAR has become a standard sensor for large-scale outdoor geometric mapping [1, 2, 3]. Beyond geometry, some approaches train models to semantically classify points in LiDAR scans [4, 5, 6, 7]. While promising, these models are still largely closed-set (restricted to a fixed set of semantic classes).\n\nCameras, in contrast, provide rich visual data and have achieved remarkable success in semantic scene understanding. Recent techniques enable more general open-set scene understanding and even enable grounding of visual data with natural language through vision-language models (VLMs) [8, 9, 10].\n\nOver the last several years, 3D scene graphs (3DSGs) have emerged as a structured approach to build semantically and hierarchically organized metric-semantic maps. Many 3DSG-based methods utilize VLMs and large language models (LLMs) for both scene graph construction and autonomous task planning. Most existing 3DSG methods focus on indoor environments using camera imagery and depth data to build a semantically-classified mesh that forms the base layer of the 3DSG. However, constructing mesh maps over large areas is both computationally and memory intensive. Additionally, camera-derived depth has limited range (≤20\\leq 20 meters). These both restrict indoor 3DSG techniques from scaling to large outdoor settings.\n\nIn this work, we combine indoor 3DSG techniques with geometrically robust outdoor LiDAR SLAM methods, enabling metric-semantic mapping in large-scale outdoor environments, we term our method Terra (see Fig. 1).\nWe structure the resulting map into a hierarchical scene graph specifically designed to support autonomous outdoor robotic tasks. In particular, since terrain is a key factor for outdoor navigation, we integrate a terrain layer into our 3DSG.\nThe key contributions of our paper are:\n\nA novel, memory-efficient, and task-agnostic approach for open-set metric-semantic mapping in large-scale outdoor environments,\n\nA terrain layer in the outdoor 3DSG that supports terrain-aware tasks where VLMs alone struggle,\n\nHierarchical region layers to handle multiple levels of task abstraction,\n\nAn in-depth evaluation on simulated and real world data comparing Terra with state-of-the-art (SOTA) indoor 3DSG methods.\n\nThe rest of our paper is outlined as follows. Section II provides a brief overview of research in semantic mapping and 3DSG methods. Section III gives an explanation for the Terra method. Section IV provides experiments across simulation and real-world datasets demonstrating the capabilities of our Terra method compared to SOTA methods and techniques. Section V concludes our findings and explores future work in the area of outdoor 3DSG generation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在大规模户外环境中实现有效的metric-semantic mapping？  \n2. 如何结合几何和语义信息以支持自主机器人的任务规划？  \n3. 如何处理户外环境中的地形信息以提高导航能力？  \n\n【用了什么创新方法】  \n本研究提出了一种名为Terra的创新方法，将室内3D场景图（3DSG）技术与户外几何映射和地形感知推理相结合，生成任务无关的稀疏地图。该方法通过构建层次化的场景图，集成地形层以支持户外导航任务，显著提高了区域分类的性能，并在内存效率上优于现有的相机基础3DSG方法。经过全面评估，Terra在物体检索和区域监控等多种机器人任务中表现出色，验证了其在模拟和真实环境中的有效性。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning",
            "authors": "Zachary Olkin,Kejun Li,William D. Compton,Aaron D. Ames",
            "subjects": "Robotics (cs.RO)",
            "comment": "Submitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.19573",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19573",
            "arxiv_html_link": "https://arxiv.org/html/2509.19573v1",
            "abstract": "Achieving highly dynamic behaviors on humanoid robots, such as running, requires controllers that are both robust and precise, and hence difficult to design. Classical control methods offer valuable insight into how such systems can stabilize themselves, but synthesizing real-time controllers for nonlinear and hybrid dynamics remains challenging. Recently, reinforcement learning (RL) has gained popularity for locomotion control due to its ability to handle these complex dynamics. In this work, we embed ideas from nonlinear control theory, specifically control Lyapunov functions (CLFs), along with optimized dynamic reference trajectories into the reinforcement learning training process to shape the reward. This approach, CLF-RL, eliminates the need to handcraft and tune heuristic reward terms, while simultaneously encouraging certifiable stability and providing meaningful intermediate rewards to guide learning. By grounding policy learning in dynamically feasible trajectories, we expand the robot’s dynamic capabilities and enable running that includes both flight and single support phases. The resulting policy operates reliably on a treadmill and in outdoor environments, demonstrating robustness to disturbances applied to the torso and feet. Moreover, it achieves accurate global reference tracking utilizing only on-board sensors, making a critical step toward integrating these dynamic motions into a full autonomy stack.",
            "introduction": "Humanoid running is a challenging task that involves executing highly dynamic motion on a nonlinear and hybrid system. Achieving performant and robust running demands controllers that can reject disturbances arising from model mismatch and environmental uncertainty, all while operating near the limits of the robot’s dynamic capabilities. Running inherently involves alternating between a flight phase, where both feet are off the ground, and a single-support phase, where only one foot is in contact. Effectively handling control across these hybrid domains is critical, as improper treatment can lead to instability.\n\nBipedal running has been studied for decades with early examples including the Raibert heuristic [1]. In the 2010’s, a number of planar bipeds were developed and running was achieved [2, 3, 4]. These methods fall under the category of Hybrid Zero Dynamics (HZD) [5] where offline trajectory optimization leveraging the idea of virtual constraints is used to generate a stable and periodic trajectory. Then, online, the trajectories are tracked using tools from nonlinear control theory such as feedback linearization and control Lyapunov functions (CLFs). These controllers have been shown to be certifiably stable if the convergence to the virtual constraints is sufficiently quick relative to the destabilizing effect of the foot-ground impact [6]. In general, these methods optimize for a steady-state motion (i.e. a periodic orbit) while the ability to get to the steady state motion is entirely dependent on the region of attraction of the tracking controller. Because these controllers operate only on the continuous dynamics, their capacity to generate transient and robust behaviors is inherently limited.\n\nOne of the difficulties with generating transients motions lies in the ability to reason through contact, specifically determining the subsequent contact schedule. A number of model predictive control (MPC) schemes have attempted to solve this issue through various numerical methods. Contact implicit MPC (CI-MPC) uses gradient-based optimization to implicitly yield a hybrid domain sequence, but have not yielded bipedal running and are quite computationally intensive in general [7, 8, 9]. Sample-based methods circumvent the need for an explicit domain sequence by rolling out sampled inputs and optimizing the inputs in an MPC fashion [10, 11, 12]. Yet these methods still require large amounts of on-board compute and have not yet produced humanoid running.\n\nReinforcement learning (RL) has recently emerged as a dominant approach for controlling legged robots [13, 14, 15, 16, 17]. RL methods are attractive due to their robustness, ability to generate diverse motions, lightweight on-board execution requirements, and their capacity to learn contact-rich behaviors directly from experience rather than requiring explicit contact modeling. Notably, RL has achieved bipedal running [18, 19]. However, many RL schemes require extensive hand tuning of rewards to produce a performant and robust policy. Poorly shaped rewards can lead to unstable learning, failure to achieve the desired behaviors, or prohibitively long training times. To mitigate these issues, a number of works have merged pre-computed trajectories with RL. This includes reduced-order model trajectories [20, 21] and full-order model trajectories such as from the HZD framework [22]. Yet, even in these cases, the rewards generally incentivize being close to the trajectories in an ad-hoc manner. Building on the ideas of [23], we embed a CLF tracking controller’s stability condition and Lyapunov function into the reward to provide meaningful intermediate rewards and incentivize certifiably stable behavior.\n\nMany prior works focus on bipeds, not humanoids, and have not shown running with low positional drift required for treadmill operation. Hand designed rewards are often used and the policies may fail to match the desired velocity [19]. Even when fast locomotion is achieved, a flight phase is not necessarily attained [20]. Alternative approaches use RL to imitate human motion data [24, 25, 26, 27]. Although such methods can reproduce the demonstrated motion, they have not shown the ability to produce dynamically stable steady-state motions, like running, with tracking capabilities ready for use in an autonomy stack. In contrast, our proposed method provides a principled way of synthesizing running controllers for full humanoids, not just bipeds, with minimal tuning and without relying on human data. The resulting policy produces both transient and steady state running motions with accurate position and velocity tracking.\n\nIn this paper, we develop a model-guided approach to enable running on a humanoid. We leverage multi-domain trajectory optimization, control Lyapunov functions (CLFs), and reinforcement learning (RL) to create a robust and performant controller. Trajectory optimization is used to generate nominal motions, which are then incorporated into CLF-based tracking controllers. These CLFs are embedded directly into the RL reward, eliminating the need for heuristic reward design. The resulting RL policy no longer requires trajectories or CLFs at runtime. Fig. 1 shows an overview of the framework. We demonstrate this controller on a Unitree G1 humanoid robot both on a treadmill and outdoors. The resulting policy shows accurate position and velocity tracking on the hardware, and exhibits robustness to various objects on the ground all while maintaining running speeds and achieving a full flight phase.\n\nThe rest of the paper is organized as follows: in section II the mathematical preliminaries are presented, including the hybrid system model and CLFs. Section III describes how multi-domain trajectories can be generated and embedded into the RL via CLFs. Then, section IV showcases the simulation and hardware experiments, demonstrating the performance and robustness of the policy. Lastly section V gives the paper’s conclusions.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何设计一个既稳健又精确的人形机器人跑步控制器。  \n2. 如何利用强化学习（RL）处理复杂的非线性和混合动力学。  \n3. 如何消除手工调节奖励项的需求，同时确保控制的稳定性。  \n\n【用了什么创新方法】  \n本研究提出了一种控制Lyapunov函数（CLF）引导的强化学习（CLF-RL）方法，通过将非线性控制理论的思想与优化的动态参考轨迹嵌入到强化学习训练过程中，形成了一种新的奖励机制。该方法消除了对手工设计和调节奖励项的需求，同时鼓励可证明的稳定性，并提供有意义的中间奖励以指导学习。通过基于动态可行轨迹的策略学习，扩展了机器人的动态能力，使其能够在跑步中有效地处理飞行和单支撑阶段。实验结果表明，该策略在跑步机和户外环境中均表现出可靠性，能够抵御对机器人躯干和脚部施加的干扰，并实现准确的全局参考跟踪，仅依赖于机载传感器。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action",
            "authors": "Sacha Morin,Kumaraditya Gupta,Mahtab Sandhu,Charlie Gauthier,Francesco Argenziano,Kirsty Ellis,Liam Paull",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.19571",
            "code": "https://montrealrobotics.ca/agentic-scene-policies.github.io/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19571",
            "arxiv_html_link": "https://arxiv.org/html/2509.19571v1",
            "abstract": "Executing open-ended natural language queries is a core problem in robotics. While recent advances in imitation learning and vision-language-actions models (VLAs) have enabled promising end-to-end policies, these models struggle when faced with complex instructions and new scenes. An alternative is to design an explicit scene representation as a queryable interface between the robot and the world, using query results to guide downstream motion planning. In this work, we present Agentic Scene Policies (ASP), an agentic framework that leverages the advanced semantic, spatial, and affordance-based querying capabilities of modern scene representations to implement a capable language-conditioned robot policy. ASP can execute open-vocabulary queries in a zero-shot manner by explicitly reasoning about object affordances in the case of more complex skills. Through extensive experiments, we compare ASP with VLAs on tabletop manipulation problems and showcase how ASP can tackle room-level queries through affordance-guided navigation and a scaled-up scene representation. We encourage readers to visit our project page.",
            "introduction": "Generalist language-conditioned robot policies need to manage the complex interplay between language, space, and action. Much of the recent progress on this problem has been driven by vision-language models (VLMs) trained on internet-scale data and showing strong general visual understanding in the open-world. Applying VLMs to the robotics domain has broadly followed two paradigms. In the first paradigm, VLMs can serve as backbones for end-to-end policy learning, yielding “vision-language actions” models (VLAs) that directly map sensor data and language commands to robot actions [1, 2, 3, 4, 5]. In the second paradigm, VLMs are primarily used for perception in the construction and querying of structured scene representations with advanced capabilities for object retrieval and spatial reasoning [6, 7, 8, 9, 10, 11, 12, 13, 14, 15].\n\nVLAs are increasingly showing zero-shot potential on new tasks [16, 4] but in practice still require task-specific fine-tuning to be truly proficient, which poses challenges in terms of data collection and infrastructure that limit overall deployment. For their part, scene representations preserve the generality of VLMs—they can practically represent any object—but do not offer a direct solution to the motion problem and are often constrained to navigation and pick-and-place tasks as a result [6, 17, 18].\n\nWe observe that a large number of language queries can be solved through a (potentially repeated) three-step process consisting of 1) object grounding, 2) spatial reasoning, and 3) part-level interaction. In this work, we demonstrate that state-of-the-art zero-shot performance can be achieved across a wide range of robotics tasks by implementing all three steps as scene queries. We expose querying functionalities as tools that can be freely called by a large language model (LLM) agent to execute language commands. For interaction, we design an expressive set of skill primitives supported by the strong affordance detection capabilities of VLMs. Our modular policy can map language queries (Ring the desk bell, Remove the thumbtack) to specific affordances and affordance-based skills (tip_push, pinch_pull), as well as solve a range of mobile manipulation queries. In summary, our key contributions include:\n\nAgentic Scene Policies (ASP), a language-conditioned manipulation policy that can solve a broad range of queries involving specific semantics, spatial reasoning, and affordances.\n\nAn extensive empirical comparison with leading VLAs on 15 manipulation tasks, providing a valuable data point in the ongoing debate between modular and end-to-end methods.\n\nA mobile version of ASP that tackles room-level queries through affordance-guided navigation and a scaled-up scene representation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何执行开放式自然语言查询以实现机器人操作。  \n2. 现有的视觉-语言-动作模型在复杂指令和新场景下的局限性。  \n3. 如何设计一个明确的场景表示作为机器人与世界之间的可查询接口。  \n\n【用了什么创新方法】  \n本研究提出了Agentic Scene Policies (ASP)，通过结合语义、空间和可供性查询能力，创建了一个语言条件的机器人操作政策。ASP通过三个步骤（对象定位、空间推理和部分交互）实现复杂任务的零-shot执行。实验表明，ASP在15个操作任务中表现优于现有的视觉-语言-动作模型，能够有效处理房间级查询并实现基于可供性的导航。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space",
            "authors": "Sankalp Agrawal,Junwon Seo,Kensuke Nakamura,Ran Tian,Andrea Bajcsy",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19555",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19555",
            "arxiv_html_link": "https://arxiv.org/html/2509.19555v1",
            "abstract": "Recent works have shown that foundational safe control methods, such as Hamilton–Jacobi (HJ) reachability analysis, can be applied in the latent space of world models. While this enables the synthesis of latent safety filters for hard-to-model vision-based tasks, they assume that the safety constraint is known a priori and remains fixed during deployment, limiting the safety filter’s adaptability across scenarios. To address this, we propose constraint-parameterized latent safety filters that can adapt to user-specified safety constraints at runtime. Our key idea is to define safety constraints by conditioning on an encoding of an image that represents a constraint, using a latent-space similarity measure. The notion of similarity to failure is aligned in a principled way through conformal calibration, which controls how closely the system may approach the constraint representation. The parameterized safety filter is trained entirely within the world model’s imagination, treating any image seen by the model as a potential test-time constraint, thereby enabling runtime adaptation to arbitrary safety constraints. In simulation and hardware experiments on vision-based control tasks with a Franka manipulator, we show that our method adapts at runtime by conditioning on the encoding of user-specified constraint images, without sacrificing performance. Video results can be found on the project website.",
            "introduction": "World models offer a promising paradigm for generalizing robot control to hard-to-simulate physical tasks by learning compact latent state spaces and dynamics directly from high-dimensional observations [1, 2, 3, 4]. Recent works have demonstrated that foundational safe control methods, such as Hamilton–Jacobi (HJ) reachability analysis [5, 6], can be applied directly in a world model’s latent space, enabling safety analysis directly from high-dimensional sensor inputs. By computing robot policies that anticipate and avoid future failures within the world model’s imagination, these latent safety filters can proactively steer robots away from hard-to-model constraints, such as spilling the contents of deformable bags [7] or toppling complex rigid-body structures [8].\n\nHowever, most safe control frameworks assume that the state constraints that robots should avoid are determined a priori and remain fixed during deployment [9, 6]. In practice, this assumption is overly restrictive: at deployment time, a robot may need to adapt its notion of what is a safety constraint based on changing environments or end-user requirements. For example, consider the robot manipulator in Fig. 1 that must sweep clutter from a table. In one scenario, it needs to avoid sweeping objects in a particular region (top row), but later it may be tasked with intentionally collecting objects into that same region while avoiding a different one (bottom row). This raises the central question of our work:\n\nIn this work, we design constraint-parameterized latent safety filters (called AnySafe). The core challenge with parameterizing safety constraints in the latent space is that, unlike in hand-designed state spaces, the structure needed to represent and optimize against a suite of safety constraints does not naturally emerge. In hand-designed state spaces, one can design a low-dimensional parameterization of the constraint set (e.g., a circle by its center and radius) alongside a dense distance measure for guiding policy optimization (e.g., signed distance to the constraint set); this allows for the safety filter to be effectively computed for all possible constraint variations. In latent spaces, by contrast, constraints are typically only implicitly defined by classifiers on the latent states [7, 8] which do not admit a continuous parameterization to represent diverse safety constraints nor yield a notion of proximity from a state to such constraints.\n\nWe propose three key ingredients that enable constraint-parameterization in latent safety filters. First, we specify safety constraints via a similarity measure between the embedding of a constraint image and the robot’s current latent state; this provides a dense signal of how close the policy is to failure. Then, we calibrate the resulting constraint set with conformal prediction [10, 11] to align with an end-user’s semantic notion of failure. Lastly, we train the safety filter by treating any image in the world model dataset as a possible test-time safety constraint. At runtime, we adapt the latent safety filter by conditioning it on an encoding of a user-specified constraint image, thereby adapting it to the runtime safety specification.\n\nWe evaluate our framework on vision-based safe-control tasks, including a simulated vehicle collision-avoidance domain and real-world object sweeping with a Franka manipulator. Our results highlight four key findings: (1) by parameterizing the safety filter with constraint representations, AnySafe can adapt to arbitrary constraints provided as images; (2) this adaptability does not come at the cost of performance, as for a given constraint, the parameterized safety filter achieves performance comparable to a specialized filter trained solely on that constraint;\n(3) AnySafe generalizes to constraints beyond those that specialized safety filters can model; and (4) since AnySafe learns from continuous latent similarity signals, conformal calibration allows us to control how conservatively the robot avoids specified constraints by adjusting the effective size of the failure set.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在运行时适应用户指定的安全约束以提高机器人控制的灵活性？  \n2. 如何在潜在空间中有效地参数化安全约束以支持多样化的任务？  \n3. 如何在不牺牲性能的情况下实现安全过滤器的适应性？  \n\n【用了什么创新方法】  \n本研究提出了一种名为AnySafe的约束参数化潜在安全过滤器。其核心方法包括通过相似性度量将安全约束与机器人的潜在状态进行关联，使用符合预测来校准约束集，并在训练过程中将任何图像视为潜在的测试时安全约束。在模拟和硬件实验中，AnySafe展示了其在运行时根据用户指定的约束图像进行适应的能力，且在给定约束下，其性能与专门训练的过滤器相当。此外，AnySafe能够超越专门安全过滤器的建模能力，提供更广泛的适应性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots",
            "authors": "Min Dai,Aaron D. Ames",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19545",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19545",
            "arxiv_html_link": "https://arxiv.org/html/2509.19545v1",
            "abstract": "We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots. RoMoCo’s modular architecture unifies state-of-the-art planners and whole-body locomotion controllers under a consistent API, enabling rapid prototyping and reproducible benchmarking. By leveraging reduced-order models for platform-agnostic gait generation, RoMoCo enables flexible controller design across diverse robots.\nWe demonstrate its versatility and performance through extensive simulations on the Cassie, Unitree H1, and G1 robots, and validate its real-world efficacy with hardware experiments on the Cassie and G1 humanoids.",
            "introduction": "Bipedal locomotion remains one of the central challenges in robotics, given bipeds’ high-dimensional, non-linear, hybrid, and underactuated nature. A widely adopted strategy to address this complexity is the use of reduced-order models (ROM) that capture the essential dynamics of walking while abstracting full-body details. These models, including the linear inverted pendulum (LIP) [1] and its variants [2, 3], have enabled the design of theoretically grounded and robust locomotion controllers. However, despite their success in research, deploying ROM-based planners and integrating them with whole-body controllers (WBC) remains a significant challenge, requiring expertise in contact and state estimation, robot kinematics and dynamics, and nonlinear control.\n\nIn recent years, there has been a surge of interest in learning-based locomotion, largely fueled by the release of frameworks such as IsaacLab [4] and IsaacGym [5], which provide scalable reinforcement learning (RL) environments and simulation infrastructure. These platforms, combined with open-source RL algorithms implementations such as RSL-RL [6], have lowered the barrier to entry for training locomotion policies at scale, enabling impressive demonstrations of locomotion in simulation and on hardware.\n\nIn contrast to learning-based approaches, model-based methods, though more interpretable, computationally efficient, and theoretically grounded, lack equivalent open-source support. Researchers typically face steep implementation hurdles, not only in developing reduced-order planners but also in integrating them with whole-body controllers and simulators. To date, only isolated efforts exist, such as the ALIP controller for Cassie [7] implemented in Simulink Real-Time, which, while effective, is tied to a specific robot platform and simulation ecosystem, preventing its generalization. This lack of a standardized, extensible framework limits reproducibility, comparative benchmarking, and rapid prototyping across diverse robotic platforms.\n\nA number of open-source libraries provide essential components for the planning and control of robots. FROST [8] supports trajectory optimization for bipedal robots, though it emphasizes offline trajectory generation rather than real-time controllers. Drake [9] offers a versatile platform for trajectory optimization and dynamics simulation, but its focus remains primarily on manipulation. OCS2 [10] implements efficient optimal control solvers for switched systems, but it requires significant integration effort to connect with whole-body controllers or reduced-order abstractions. Other optimal control tools, such as OpenSoT [11], Crocoddyl [12], and TSID [13], operate largely in isolation from dynamic locomotion planning. Collectively, these libraries form a rich ecosystem of building blocks; yet, researchers still assemble them manually, as there remains no unified framework that seamlessly couples reduced-order planning with whole-body control in a reproducible, extensible manner.\n\nTo address these challenges, we introduce RoMoCo—an open-source software toolbox designed to unify the development, evaluation, and deployment of ROM-based planners and WBC algorithms for bipedal and humanoid locomotion. The key contributions of this work are:\n\nA unified mathematical formulation of popular LIP-based planners (ALIP, H-LIP, MLIP, DCM), enabling their modular implementation and direct comparison.\n\nA modular software architecture that decouples planners, output mappings, whole-body controllers, and robot interfaces, allowing for rapid prototyping across different hardware.\n\nAn open-source library with integrated MuJoCo [14] simulation, demonstrated hardware deployments on multiple bipedal and humanoid platforms, and a fully available anonymized repository111https://anonymous.4open.science/r/RoMoCo-6E85 for review purposes.222The repository will be linked to a permanent public release upon acceptance.\n\nA comparative analysis of different ROM planners and whole-body controllers on the Cassie and Unitree G1, offering insights into their performance trade-offs.\n\n1. A unified mathematical formulation of popular LIP-based planners (ALIP, H-LIP, MLIP, DCM), enabling their modular implementation and direct comparison.\n\n2. A modular software architecture that decouples planners, output mappings, whole-body controllers, and robot interfaces, allowing for rapid prototyping across different hardware.\n\n3. An open-source library with integrated MuJoCo [14] simulation, demonstrated hardware deployments on multiple bipedal and humanoid platforms, and a fully available anonymized repository111https://anonymous.4open.science/r/RoMoCo-6E85 for review purposes.222The repository will be linked to a permanent public release upon acceptance.\n\n4. A comparative analysis of different ROM planners and whole-body controllers on the Cassie and Unitree G1, offering insights into their performance trade-offs.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效整合减少阶模型（ROM）与全身控制器（WBC）以实现双足和类人机器人运动控制？  \n2. 现有的开源工具缺乏统一框架，限制了不同机器人平台间的可重复性和快速原型开发。  \n\n【用了什么创新方法】  \nRoMoCo是一个开源C++工具箱，旨在统一ROM规划器和WBC算法的开发、评估和部署。它采用了模块化的软件架构，允许快速原型设计，并提供了对多种双足和类人机器人的硬件实验验证。通过比较不同的ROM规划器和全身控制器，RoMoCo展示了其在多种平台上的灵活性和性能，促进了对运动控制的深入理解。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Autonomous Elemental Characterization Enabled by a Low Cost Robotic Platform Built Upon a Generalized Software Architecture",
            "authors": "Xuan Cao,Yuxin Wu,Michael L. Whittaker",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19541",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19541",
            "arxiv_html_link": "https://arxiv.org/html/2509.19541v1",
            "abstract": "Despite the rapidly growing applications of robots in industry, the use of robots to automate tasks in scientific laboratories is less prolific due to lack of generalized methodologies and high cost of hardware.\nThis paper focuses on the automation of characterization tasks necessary for reducing cost while maintaining generalization,\nand proposes a software architecture for building robotic systems in scientific laboratory environment.\nA dual-layer (Socket.IO and ROS) action server design is the basic building block, which facilitates the implementation of a web-based front end for user-friendly operations and the use of ROS Behavior Tree for convenient task planning and execution.\nA robotic platform for automating mineral and material sample characterization is built upon the architecture, with an open source, low-cost three-axis computer numerical control gantry system serving as the main robot.\nA handheld laser induced breakdown spectroscopy (LIBS) analyzer is integrated with a 3D printed adapter, enabling automated 2D chemical mapping. We demonstrate the utility of automated chemical mapping by scanning of the surface of a spodumene-bearing pegmatite core\nsample with a 1071-point dense hyperspectral map acquired at a rate of 1520 bits per second.\nAutomated LIBS scanning enables controlled chemical quantification in the laboratory that complements field-based measurements acquired with the same handheld device, linking resource exploration and processing steps in the supply chain for lithium-based battery materials.",
            "introduction": "The rapid development of robotics in recent years has given a boost of its applications in industry, such as machine tending [1], palletizing [2], and assembly [3]. The operational stock of industrial robots worldwide increased from about 1.3 million in 2013 to 4.3 million in 2023 [4].\n\nSimilarly, robotic automation in research laboratories has become an emerging field, since “Robotics and automation can enable scientific experiments to be conducted faster, more safely, more accurately, and with greater reproducibility, allowing scientists to tackle large societal problems in domains such as health and energy on a shorter timescale” [5].\nAlthough there have been successful applications of robotic automation in laboratories [6, 7, 8, 9, 10], the use of robots to automate laboratory operations is still limited in general due to the automation gap caused by the variety of tasks and protocols [11], ultimately resulting in high costs.\n\nThis work sheds some light on the automation of characterization tasks in labs, which determine the properties, composition, and behavior of substances (e.g. spectrometry, microscopy, thermal analysis, etc.), and hence are essential in scientific research.\nOne common pattern in characterization tasks is sample-move-instrument-stay (SMIS), where a sample is placed to a specific position for an analytical instrument to start working. Automating this pattern using robots requires precise pick-and-place operations and enough degrees of freedom.\n\nBy contrast, this paper focuses on the sample-stay-instrument-move (SSIM) pattern, where an instrument is held by a robot and moved around a sample during characterization. Automating this pattern does not require pick-and-place operations since the instrument is mounted on the robot all the time. Sample standardization, such as positioning on a 2-d horizontal plane, reduces the robot’s required degrees of freedom to reach the samples, which could potentially lower the hardware cost.\n\nTowards this end, this paper introduces a robotic platform for automating SSIM characterization tasks for mineral and material samples. The platform consists of: (1) a low-cost 3D (translational movements in X, Y, and Z directions) gantry system commonly used in traditional computer numerical control (CNC) machining as the primary robot, (2) an analytical instrument mounted to the gantry system for sample characterization, and (3) a stereo camera capable of depth sensing for locating samples to be measured. All components, and samples to be measured, are placed on a benchtop. The general workflow consists of the following steps: (1) a sample location is either predefined, or else identified by the camera; (2) the gantry system takes the analytical instrument to the sample location; (3) the instrument starts characterization and collects raw data; (4) the raw data are processed and optional feedback is generated.\n\nThe core of the software is a generalized custom-designed architecture for automation systems in laboratory environments. The basis of the architecture is a dual-layer action server design for every hardware component, which monitors incoming operation requests through both Socket.IO [12] and Robot Operating System (ROS) [13] communication protocols and commands the hardware to act accordingly.\nOn top of all action servers lies a Behavior Tree (BT) [14] which orchestrates the hardware components by interacting with their action servers to automate the characterization workflow.\nA web-based front end is developed to ensure user-friendly operations of the platform, including both manual control of each individual hardware and execution of the BT.\n\nTo showcase the efficacy of the platform, we integrate a handheld laser induced breakdown\nspectroscopy (LIBS) analyzer to the gantry system and use the platform to perform dense LIBS scanning on the surface of a spodumene-bearing pegmatite core sample with 1071 measurement points, each containing optical emission spectra between 190 nm and 950 nm with 0.03 nm resolution, corresponding to 22800 data channels per measurement. The resulting 2×1072\\times 10^{7} data are automatically quantified using a custom algorithm, yielding spatially-resolved, comprehensive chemical analysis with parts-per-million levels for most chemical elements.\nThe automated LIBS scanning (1) frees researchers from tedious operations,\n(2) accelerates LIBS characterization by at least 3 times the rate of manual operations, and (3) provides crucial information about downstream processing chemistry.\n\nThis paper makes three contributions.\nFirst, a generalized software architecture for building robotic automation systems in scientific laboratory environment is proposed.\nSecond, a low-cost gantry system commonly used in CNC machining is shown to be capable of working as a robot for the automation of SMIS characterization tasks.\nThird, automated dense LIBS scanning using the developed robotic platform and automatic data reduction is achieved.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何降低科学实验室中自动化任务的成本和复杂性？  \n2. 如何实现对矿物和材料样本的自动化表征任务？  \n3. 如何设计通用的软件架构以支持实验室机器人系统的开发？  \n\n【用了什么创新方法】  \n本研究提出了一种通用的软件架构，采用双层（Socket.IO和ROS）动作服务器设计，支持实验室环境中的机器人自动化。通过集成手持激光诱导击穿光谱（LIBS）分析仪，构建了一个低成本的三轴计算机数控（CNC）龙门系统，实现了对矿物样本的自动化化学映射。该平台能够以1520比特每秒的速度扫描样本，生成1071点的密集高光谱图，显著提高了实验效率，减少了人工操作的繁琐性，并提供了重要的下游处理化学信息。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot",
            "authors": "James Avtges,Jake Ketchum,Millicent Schlafly,Helena Young,Taekyoung Kim,Allison Pinosky,Ryan L. Truby,Todd D. Murphey",
            "subjects": "Robotics (cs.RO)",
            "comment": "Published at IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.19525",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19525",
            "arxiv_html_link": "https://arxiv.org/html/2509.19525v1",
            "abstract": "Closed-loop control remains an open challenge in soft robotics. The nonlinear responses of soft actuators under dynamic loading conditions limit the use of analytic models for soft robot control. Traditional methods of controlling soft robots underutilize their configuration spaces to avoid nonlinearity, hysteresis, large deformations, and the risk of actuator damage. Furthermore, episodic data-driven control approaches such as reinforcement learning (RL) are traditionally limited by sample efficiency and inconsistency across initializations. In this work, we demonstrate RL for reliably learning control policies for dynamic balancing tasks in real-time single-shot hardware deployments. We use a deformable Stewart platform constructed using parallel, 3D-printed soft actuators based on motorized handed shearing auxetic (HSA) structures. By introducing a curriculum learning approach based on expanding neighborhoods of a known equilibrium, we achieve reliable single-deployment balancing at arbitrary coordinates. In addition to benchmarking the performance of model-based and model-free methods, we demonstrate that in a single deployment, Maximum Diffusion RL is capable of learning dynamic balancing after half of the actuators are effectively disabled, by inducing buckling and by breaking actuators with bolt cutters. Training occurs with no prior data, in as fast as 15 minutes, with performance nearly identical to the fully-intact platform. Single-shot learning on hardware facilitates soft robotic systems reliably learning in the real world and will enable more diverse and capable soft robots.",
            "introduction": "Soft robots offer the potential for improved adaptability and safety compared to their rigid counterparts due to compliance and material redundancies [1]. However, soft-actuated robots are traditionally difficult to control due to their nonlinear dynamics and high degrees of freedom [2, 3]. Soft-actuated robot dynamics—which may be impractical or impossible to model classically—are often stochastic and vary with a number of factors including strain, actuator fatigue, and even manufacturing processes [4, 5].\n\nData-driven methods such as reinforcement learning (RL) have been used to circumvent many of the modeling difficulties associated with controlling soft robots [6]. Prior applications of RL in this domain have explored various approximations of soft actuator dynamics, including piecewise constant curvature models, Cosserat rod models, and rigid NN-link pendulum approximations [7, 8, 9, 5, 10]. However, lower-dimensional approximations may oversimplify the dynamics of highly nonlinear actuators, which may be compounded by external forces and changes to the dynamics during use. Additionally, models parameterized using these approximations are almost exclusively used to train control policies in simulation, requiring dedicated approaches to address the sim-to-real gap. Existing approaches also often assume quasi-static behavior and focus on relatively simple tasks such as reaching or tracing with continuum arms.\n\nWhen learning approaches are applied to soft robot control, current works often constrain the size or dimensionality of the configuration space to regions with smaller deformations, or where their dynamics are approximately linear [11, 7, 9]. This may include directly constraining actuator outputs, choices of robot orientation, and evaluating control approaches only on undamaged, unloaded actuators.  While these techniques are effective, utilizing the full variety of soft robots’ diverse configuration spaces despite their stochastic and nonlinear dynamics that may be impossible to simulate is paramount for producing behaviors that involve buckling, large deformations, or actuator breakages. Exploiting these unique properties of soft robots has the potential to enable more diverse task learning, biomimetic behaviors, and adaptation to loading conditions or damage.\n\nOur experimental platform is a six degree-of-freedom (DoF), soft-actuated parallel mechanism with a structure similar to that of a Stewart platform [12]. The struts of the platform are motorized soft actuators based on 3D-printed Handed Shearing Auxetics (HSAs) that lengthen and shorten upon the turn of a servo motor [13, 14]. In this work, we balance a sliding puck on the platform, at both the center and at arbitrary coordinates. We utilize deep RL to train closed-loop controllers and benchmark multiple model-based and model-free RL frameworks. Our training occurs fully on-hardware without simulation or bootstrapped data.\n\nLearning in real-time on hardware has its challenges [15, 16], some unique to soft robotics. The limited lifespan of flexible, strain-dependent actuators makes sample efficiency and adaptability valuable attributes  for minimizing and adapting to actuator damage throughout training [9]. Our approach combines a classical kinematics model for a rigid Stewart platform with a deep RL controller to efficiently learn nonlinear control policies for the system. We train all of our policies using single-shot reinforcement learning, a non-episodic problem formulation for continual learning without any environment resets. Learning in a singular real-time episode may be necessary for a variety of tasks well-suited to soft robotics—such as functioning in extreme environments or human interactions—where leveraging prior data, simulations, and reset routines may be undesirable or impossible. While episodic soft robot RL has been implemented on-hardware in works such as [9, 17, 18], and reset-free hardware learning has been implemented in works such as [19, 20], to the best of our knowledge this work is the first to accomplish all of the above in a soft-actuated robot.\n\nAn additional challenge with learning a balancing task in single-shot episodes is that reliably learning control policies is not necessarily an inevitable outcome in reasonable timescales—catastrophic failure modes do exist. While the balancing puck is constrained from falling off the platform, without access to a reset routine, either learned or programmed, the puck can become stuck in a corner during training, providing little to no variance in data required for learning. This is especially  relevant when learning to balance at arbitrary coordinates on the platform, where the reward landscape is parameterized by an observable setpoint that may not be near the platform’s center. Other works conducting soft-actuated RL such as [9, 17, 8, 18] structure their experiments such that providing zero control results in an intervention-free environment reset—such as orienting a continuum arm downwards—stable states such as these do not exist in our experimental platform with an unarticulated sliding puck.\n\nTo overcome this challenge, we employ a curriculum learning approach based on expanding neighborhoods of a known equilibrium: the platform center. With this we reliably learn to balance in a single deployment, without it the task is impossible to accomplish consistently, or at all.\n\nFurthermore, we show how RL can learn to balance the puck despite major changes to the robot’s dynamics during training. We buckle half of the HSAs, introducing singularities, hysteresis, and an out-of-distribution configuration. We also damage the platform by cutting through half of the HSAs with bolt cutters. Despite these alterations, RL attains evaluation performance indistinguishable to the default case.\n\nThe contributions of this work are:\n\nDemonstrations and benchmarking of single-shot learning for dynamic tasks on a soft robot,\n\nDemonstrations and benchmarking of single-shot learning for dynamic tasks on a soft robot,\n\nIntroduction of a curriculum learning procedure to improve single-shot learning outcomes, and\n\nAn illustration of a setting where RL can accommodate changing dynamics such as buckling or breaking actuators during single-shot training.\n\n1. Demonstrations and benchmarking of single-shot learning for dynamic tasks on a soft robot,\n\n2. Introduction of a curriculum learning procedure to improve single-shot learning outcomes, and\n\n3. An illustration of a setting where RL can accommodate changing dynamics such as buckling or breaking actuators during single-shot training.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在软机器人中实现闭环控制以应对非线性动态和高自由度问题？  \n2. 如何提高强化学习在动态平衡任务中的样本效率和一致性？  \n3. 如何在没有模拟或先前数据的情况下实现实时单次学习？  \n\n【用了什么创新方法】  \n本研究采用了一种基于已知平衡点邻域扩展的课程学习方法，以提高软机器人在动态平衡任务中的单次学习效果。通过在真实硬件上进行训练，系统能够在不重置环境的情况下快速学习控制策略。即使在半数执行器失效的情况下，强化学习仍能实现与完整平台相当的性能，展示了软机器人在动态环境下的适应能力和学习效率。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Bioinspired SLAM Approach for Unmanned Surface Vehicle",
            "authors": "Fabio Coelho,Joao Victor T. Borges,Paulo Padrao,Jose Fuentes,Ramon R. Costa,Liu Hsu,Leonardo Bobadilla",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19522",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19522",
            "arxiv_html_link": "https://arxiv.org/html/2509.19522v1",
            "abstract": "This paper presents OpenRatSLAM2, a new version of OpenRatSLAM—a bioinspired SLAM framework based on computational models of the rodent hippocampus. OpenRatSLAM2 delivers low-computation-cost visual-inertial based SLAM, suitable for GPS-denied environments. Our contributions include a ROS2-based architecture, experimental results on new waterway datasets, and insights into system parameter tuning. This work represents the first known application of RatSLAM on USVs. The estimated trajectory was compared with ground truth data using the Hausdorff distance. The results show that the algorithm can generate a semimetric map with an error margin acceptable for most robotic applications.",
            "introduction": "The increasing use of unmanned surface vehicles (USVs) for scientific, military, and commercial purposes requires the development of robust navigation systems [1]. Common applications include oceanographic data collection, oil and gas exploration, environmental surveys, mine countermeasures, and surveillance [2, 3]. To autonomously perform such tasks, a mobile robot must be able to localize itself within its environment [4].\n\nCommon approaches include combining GPS with an inertial measurement unit (IMU) and Kalman filtering algorithms for state estimation in USVs [5]–[6]. However, these methods fail in GPS-denied environments where satellite signals are obstructed [7]. Moreover, GPS signals are vulnerable to various disruptions and cyberattacks, including jamming and spoofing [8].\n\nTo address these limitations, Simultaneous Localization and Mapping (SLAM) is an alternative that enables a vehicle to build a map of its surroundings while estimating its position relative to that map. Many existing SLAM implementations rely on computationally intensive sensors, such as LiDAR or depth cameras. These sensors often require high processing and storage demands, making them less suitable for real-time applications on resource-constrained platforms [9].\n\nMotivated by recent advances in neuroscience, several brain-inspired SLAM systems have been proposed [10]. A pioneering work is the RatSLAM framework, a biologically inspired SLAM algorithm based on computational models of the rodent hippocampus. RatSLAM employs a Continuous Attractor Neural Network (CANN) to construct a cognitive map of an environment using only a low-resolution monocular camera [11]. Compared to probabilistic SLAM approaches, RatSLAM offers reduced computational complexity and efficient memory usage and is well-suited for both indoor and large-scale outdoor mapping.\n\nIn recent years, several RatSLAM-based variants have been proposed [12]. For instance, [13] introduced a MATLAB-based RatSLAM implementation in a rat robot, demonstrating its capability to learn spatial layouts. However, the system’s performance was too slow for real-time operation in large environments. Another approach, OpenRatSLAM, was proposed as an open-source RatSLAM implementation based on the Robot Operating System (ROS) [14]. This version benefits from ROS’s node parallelization and modular integration with diverse robotic architectures [12].\n\nThe emergence of ROS 2 as the dominant middleware for new robotic systems has created integration challenges, as OpenRatSLAM was primarily developed for ROS 1. In this context, xRatSLAM was developed as an extensible, parallel, open-source framework implemented as a C++ library to facilitate the development and testing of RatSLAM algorithm variants [12].\n\nWhile most applications have targeted ground robots, RatSLAM-inspired algorithms have also been explored in other domains. One aerial application, NeuroSLAM, is a neuro-inspired SLAM system with four degrees of freedom (4DoF), based on computational models of 3D grid cells and multilayered head direction cells. It integrates visual and self-motion cues through a dedicated vision system [15]. In underwater environments, two RatSLAM-based systems have been developed: DolphinSLAM [16], a 3D variant, and a more recent system that implements Pose Cells using Spiking Neural Networks (SNNs) [17]. Both were developed using ROS 1 distributions, which are now deprecated and unsupported.\n\nIn summary, the contributions of this work are as follows:\n\nA new version of OpenRatSLAM, implemented using ROS 2 Rolling, referred to as OpenRatSLAM2. This version benefits from ROS 2’s advantages, including improved maintainability and easier integration with modern tools. Additionally, the communication middleware is more robust than ROS 1, providing streamlined transition from simulation to physical robot deployment;\n\nTo the best of our knowledge, this is the first application of RatSLAM to a USV;\n\nA visual-inertial dataset collected using a USV for evaluating SLAM performance in aquatic environments.\n\n1. A new version of OpenRatSLAM, implemented using ROS 2 Rolling, referred to as OpenRatSLAM2. This version benefits from ROS 2’s advantages, including improved maintainability and easier integration with modern tools. Additionally, the communication middleware is more robust than ROS 1, providing streamlined transition from simulation to physical robot deployment;\n\n2. To the best of our knowledge, this is the first application of RatSLAM to a USV;\n\n3. A visual-inertial dataset collected using a USV for evaluating SLAM performance in aquatic environments.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在GPS-denied环境中实现低计算成本的SLAM？  \n2. 如何将RatSLAM算法应用于无人水面车辆（USV）？  \n3. 如何提高SLAM系统在水域环境中的性能评估？  \n\n【用了什么创新方法】  \n本研究提出了OpenRatSLAM2，一个基于ROS 2的生物启发式SLAM框架，利用低分辨率单目相机和视觉惯性传感器进行导航。通过在水域环境中收集的数据集进行实验，结果表明该算法能够生成具有可接受误差范围的半度量地图，适合大多数机器人应用。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion",
            "authors": "Najeeb Ahmed Bhuiyan,M. Nasimul Huq,Sakib H. Chowdhury,Rahul Mangharam",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19521",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19521",
            "arxiv_html_link": "https://arxiv.org/html/2509.19521v1",
            "abstract": "Gesture-based control for mobile manipulators faces persistent challenges in reliability, efficiency, and intuitiveness. This paper presents a dual-hand gesture interface that integrates TinyML, spectral analysis, and sensor fusion within a ROS framework to address these limitations. The system uses left-hand tilt and finger flexion, captured using accelerometer and flex sensors, for mobile base navigation, while right-hand IMU signals are processed through spectral analysis and classified by a lightweight neural network. This pipeline enables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3 manipulator. By supporting simultaneous navigation and manipulation, the framework improves efficiency and coordination compared to sequential methods. Key contributions include a bimanual control architecture, real-time low-power gesture recognition, robust multimodal sensor fusion, and a scalable ROS-based implementation. The proposed approach advances Human–Robot Interaction (HRI) for industrial automation, assistive robotics, and hazardous environments, offering a cost-effective, open-source solution with strong potential for real-world deployment and further optimization.",
            "introduction": "Robot remote control underpins a wide range of transformative technologies, enabling precise operations across domains like space exploration [1, 2, 3], disaster response [4, 5], military operations [6, 7], and assistive robotics [8, 9]. By harnessing human input to guide robotic systems, this field has unlocked capabilities that extend beyond manual reach, adapting to environments where direct intervention is impractical or hazardous. Gesture-based control emerges as a natural evolution of this concept, leveraging intuitive human movements to command robots with minimal training. This approach proves particularly effective in scenarios requiring immediate, instinctual interaction, such as directing medical neuro-arms with laser scalpels for pinpoint accuracy [10] or steering assistive devices to support daily tasks. Its appeal lies in bridging the gap between human intent and robotic action, offering a direct and expressive interface.\n\nBuilding on this foundation, mobile manipulators—systems that pair a mobile base with a robotic arm—amplify the potential of gesture control, combining locomotion with manipulation for versatile applications. In industrial automation, they streamline assembly lines [11, 12]; in healthcare, they assist with patient care [13, 14]; and in search-and-rescue missions, they navigate complex terrains [15]. Their utility extends to extreme conditions: in coal mines, gesture-controlled robots dig under searing heat where workers cannot endure [16]; in bomb defusing, they shield human operators from lethal risks [17]; and in nuclear reactors, they manage radioactive waste without exposing personnel [18]. This synergy of mobility and dexterity makes mobile manipulators a cornerstone of modern robotics. However, gesture-based control, despite its promise, grapples with practical hurdles—unreliable recognition, environmental sensitivity, and resource intensity—that impede its widespread adoption, necessitating innovative solutions.\n\nThese limitations arise from fundamental flaws in existing gesture recognition methods, each posing distinct challenges to effective HRI. Vision-based systems, such as Microsoft’s Kinect or Leap Motion, depend on cameras that falter under variable lighting, occlusions, or when users move beyond a narrow range, disrupting gesture detection [19, 20]. Similarly, approaches employing Deep Neural Networks (DNNs) or Convolutional Neural Networks (CNNs) deliver high accuracy but at the cost of substantial power and computational demands, rendering them impractical for lightweight or real-time systems [21, 22]. Electromyography (EMG)-based methods offer an alternative by capturing muscle signals, with studies like one using the Myo Armband achieving 78.94% accuracy across 10 gestures [23], yet their reliability wanes as muscle fatigue degrades signal quality over time [24]. Such challenges are especially pronounced in rehabilitation and mobility assistance, where intuitive, dependable control is paramount. Stroke, a leading cause of disability per the World Stroke Organization [25], often induces hemiparesis, impairing one side of the body, while paralysis afflicts millions globally, severely restricting mobility. Early robot-assisted therapy with patient-driven devices can enhance recovery, and related efforts, such as a Robot Wheelchair (RW) using sensor-based hand gestures via gloves or handles, underscore the demand for accessible interfaces in these contexts [26].\n\nTo address these shortcomings, a more robust system is essential—one that overcomes environmental constraints, reduces resource demands, and maintains consistency across diverse users and conditions. Wearable sensor-based approaches, enhanced by TinyML and edge computing, provide a compelling solution, offering real-time, low-power performance adaptable to dynamic environments [27, 28, 29]. By integrating sensor fusion—combining data from accelerometers, IMUs, and flex sensors—these systems achieve greater accuracy and robustness, circumventing the pitfalls of vision, neural networks, and EMG [30, 31]. This paradigm shift not only improves technical feasibility but also aligns with the urgent need for intuitive HRI in rehabilitation, hazardous operations, and beyond, setting the stage for advanced robotic control frameworks.\n\nInspired by these insights, we propose a dual-hand gesture control system for a ROS-based mobile manipulator, harnessing TinyML and sensor fusion to deliver seamless, efficient operation. The left hand governs the mobile base using an Arduino Nano equipped with an accelerometer and two flex sensors—tilting to command directional movement (forward, backward, left, right) and flexing to fine-tune acceleration or deceleration. Concurrently, the right hand directs the 7-DOF Kinova Gen3 manipulator via an Arduino Nano 33 BLE Sense with TinyML and an LSM9DS1 IMU, mapping distinct gestures: “Forward-Backward” to a pickup pose, “Left-Right” to placement on the mobile base, “Flat Rectangle” to table placement, “Rectangle” to elevated placement, “Circle” to alternative motion, and “Up-Down” to homing. Built on ROS (ros2-jazzy), the system simulates the mobile base in Gazebo Harmonic and controls the manipulator via RViz with MoveIt!. Our primary contributions include:\n\nA dual-hand gesture control architecture that separates mobile base and manipulator functions, enhancing coordination and natural interaction.\n\nA TinyML-based gesture recognition system for real-time, low-power classification of complex manipulator gestures.\n\nA sensor fusion approach combining accelerometer and flex sensor data for precise mobile base control.\n\nA comprehensive ROS-based simulation framework integrating a custom mobile base and the Kinova Gen3 manipulator.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高移动操控器的手势识别的可靠性和效率。  \n2. 如何实现双手手势控制以同时导航和操作机器人。  \n3. 如何克服现有手势识别方法在环境敏感性和资源消耗上的不足。  \n\n【用了什么创新方法】  \n本文提出了一种基于TinyML和传感器融合的双手手势控制系统，利用左手的倾斜和手指弯曲控制移动底座，右手通过IMU信号进行手势识别。该系统实现了实时、低功耗的手势识别，显著提高了移动操控器的协调性和效率。通过ROS框架的集成，系统在复杂环境中表现出更高的准确性和鲁棒性，推动了人机交互在工业自动化和助理机器人中的应用。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Supercomputing for High-speed Avoidance and Reactive Planning in Robots",
            "authors": "Kieran S. Lachmansingh,José R. González-Estrada,Ryan E. Grant,Matthew K. X. J. Pan",
            "subjects": "Robotics (cs.RO); Distributed, Parallel, and Cluster Computing (cs.DC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19486",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19486",
            "arxiv_html_link": "https://arxiv.org/html/2509.19486v1",
            "abstract": "This paper presents SHARP (Supercomputing for High-speed Avoidance and Reactive Planning), a proof-of-concept study demonstrating how high-performance computing (HPC) can enable millisecond-scale responsiveness in robotic control. While modern robots face increasing demands for reactivity in human–robot shared workspaces, onboard processors are constrained by size, power, and cost. Offloading to HPC offers massive parallelism for trajectory planning, but its feasibility for real-time robotics remains uncertain due to network latency and jitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator must dodge high-speed foam projectiles. Using a parallelized multi-goal A* search implemented with MPI on both local and remote HPC clusters, the system achieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote,  300 km away), with avoidance success rates of 84% and 88%, respectively. These results show that when round-trip latency remains within the tens-of-milliseconds regime, HPC-side computation is no longer the bottleneck, enabling avoidance well below human reaction times. The SHARP results motivate hybrid control architectures: low-level reflexes remain onboard for safety, while bursty, high-throughput planning tasks are offloaded to HPC for scalability. By reporting per-stage timing and success rates, this study provides a reproducible template for assessing real-time feasibility of HPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable pathway toward dependable, reactive robots in dynamic environments.",
            "introduction": "Modern robots are increasingly expected to operate in unstructured and dynamic environments, often in close collaboration with humans. These scenarios demand not only accurate planning but also highly reactive control: a robot must perceive changes and adapt within tens of milliseconds to avoid unsafe or undesirable interactions. Traditionally, such responsiveness has relied on local computing platforms (commodity CPUs or GPUs embedded on the robot or on nearby workstations), which are constrained by size, power, and cost [1, 2]. As robots integrate more resource-intensive artificial intelligence (AI) and machine learning models, these platforms are reaching their limits, creating bottlenecks that threaten truly responsive human–robot interaction (HRI).\n\nHigh-performance computing (HPC), or supercomputing, offers an alternative. HPC systems perform billions of computations per second and excel at large-scale graph search and optimization—problems central to robotics tasks such as inverse kinematics, trajectory planning, and collision avoidance [3, 4]. In principle, HPC can deliver solutions orders of magnitude faster than local compute, enabling behaviours that are more optimal, adaptive, and reactive.\n\nThe main obstacle is latency [5]. HPC resources are typically non-local and accessed via network connections that introduce delays. In time-critical robotics, where milliseconds matter, the central question is whether the raw compute advantage of HPC can overcome communication costs and still enable real-time action [6, 7].\n\nThis paper addresses that question through SHARP—Supercomputing for High-speed Avoidance and Reactive Planning—a proof-of-concept system that evaluates whether HPC offloading can support real-time avoidance. To stress-test responsiveness, we examine a deliberately stringent scenario: a 7-DOF manipulator tasked with dodging high-speed foam projectiles. This setup serves two purposes. First, it creates a controlled, time-critical environment in which HPC planning must deliver trajectories fast enough to influence live robot behaviour. Second, it provides a proxy for broader applications such as collision avoidance in dynamic, human–robot shared workspaces, where comparable millisecond-scale reaction budgets apply.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用高性能计算（HPC）实现机器人在动态环境中的实时反应与规划？  \n2. 网络延迟是否会影响HPC在机器人控制中的实时性和有效性？  \n3. 传统本地计算平台在处理复杂机器人任务时的瓶颈是什么？  \n\n【用了什么创新方法】  \n本研究提出了SHARP系统，通过将高性能计算（HPC）应用于机器人控制，进行高速度规避和反应规划。采用并行化的多目标A*搜索算法，结合MPI在本地和远程HPC集群上进行评估。在应对高速度泡沫投射物的压力测试中，系统实现了平均规划延迟为22.9毫秒（本地）和30.0毫秒（远程），规避成功率分别为84%和88%。这些结果表明，当往返延迟保持在十毫秒以内时，HPC计算不再是瓶颈，从而使机器人能够在低于人类反应时间的情况下实现有效规避。SHARP的结果为混合控制架构提供了动力，低级反应留在机器人本体以确保安全，而高吞吐量的规划任务则可离线处理以实现可扩展性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Crater Observing Bio-inspired Rolling Articulator (COBRA)",
            "authors": "Adarsh Salagame,Henry Noyes,Alireza Ramezani,Eric Sihite,Arash Kalantari",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19473",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19473",
            "arxiv_html_link": "https://arxiv.org/html/2509.19473v1",
            "abstract": "NASA aims to establish a sustainable human basecamp on the Moon as a stepping stone for future missions to Mars and beyond. The discovery of water ice on the Moon’s craters located in permanently shadowed regions, which can provide drinking water, oxygen, and rocket fuel, is therefore of critical importance. However, current methods to access lunar ice deposits are limited. While rovers have been used to explore the lunar surface for decades, they face significant challenges in navigating harsh terrains, such as permanently shadowed craters, due to the high risk of immobilization. This report introduces COBRA (Crater Observing Bio-inspired Rolling Articulator), a multi-modal snake-style robot designed to overcome mobility challenges in Shackleton Crater’s rugged environment. COBRA combines slithering and tumbling locomotion to adapt to various crater terrains. In snake mode, it uses sidewinding to traverse flat or low inclined surfaces, while in tumbling mode, it forms a circular barrel by linking its head and tail, enabling rapid movement with minimal energy on steep slopes. Equipped with an onboard computer, stereo camera, inertial measurement unit, and joint encoders, COBRA facilitates real-time data collection and autonomous operation. This paper highlights COBRA’s robustness and efficiency in navigating extreme terrains through both simulations and experimental validation.",
            "introduction": "Robots have become essential tools in our scientific exploration of space and celestial bodies. Equipped with advanced sensors and designed to withstand severe conditions, they safely explore, sample, and analyze, yielding invaluable insights that would otherwise remain unattainable. Traditionally, robots for space exploration have relied on wheeled locomotion [1]. However, recent innovations have shifted this paradigm. Ingenuity, the Mars Helicopter [2], has successfully demonstrated the first aerial locomotion on another planet, while alternate surface mobility systems such as Dragonfly, EELS, and DuAxel [3, 4, 5] are in advanced stages of development. These solutions are typically tailored to address the locomotion challenges in specific regions of interest, such as recurring slope lineae on Mars for DuAxel or the icy moon Enceladus for EELS.\n\nHowever, some regions of interest, particularly the Moon’s permanently shadowed regions (PSRs), are still unexplored due to limitations of existing surface mobility solutions. In this work, we introduce our bio-inspired mobile robot, COBRA, Crater Observing Bio-inspired Rolling Articulator, designed to address challenges unmet by current solutions for energy-efficient exploration of steep and unknown craters in the Moon’s PSRs. As a case study, we focus on Shackleton Crater (see Fig. 2) at the lunar South Pole, a site of immense scientific potential for water ice discovery [6], but significant challenges for existing exploration methods. We present the envisioned mission scenario and the locomotion solution that led to COBRA winning the 2022 Breakthrough, Innovative, and Game-changing (BIG) Idea Challenge [7], an initiative under NASA’s Space Technology Mission Directorate’s (STMD’s) Game Changing Development Program (GCD), for alternative lunar mobility solutions. This report summarizes the design, dynamical modeling, contact-rich and optimization-based gait discovery, as well as the numerical and experimental evaluation of COBRA’s mobility in unstructured, contested environments. Specifically, the technical contributions of this work are:  (1) Hardware design, including head, tail, and body module mechanical and electronics design. (2) Design of the head-tail docking mechanism to substantiate transitions between crawling and tumbling. (3) Numerical modeling of crawling and tumbling locomotion feats based on Lagrange and mixed Hamilton-Lagrange (partitioned state-space model for tumbling) dynamics. (4) Contact-implicit optimization-based gait discovery and joint motion planning. (5) Simulation and experimental validation of hex-ring tumbling, spiral tumbling, sidewinding, lateral rolling, and vertical undulation. (6) Demonstration of field-tested locomotion modes in dusty, steep, and bumpy environments, with self-sustained dust mitigation. (7) Demonstration of full deployment from a lander, including unmanned transitions between various locomotion modes in experiments.\n\nWith the Artemis program, NASA and collaborating space agencies aim to revitalize lunar and space exploration. One Artemis objective is to create a sustainable human base camp on the Moon with the hope to then propel further missions to Mars and beyond. Accomplishing such a feat is contingent on In-Situ Resource Utilization (ISRU) on the Moon [8]. One resource of significant interest is lunar water ice, which can potentially supply drinking water, oxygen, and rocket propellant. However, the means to access the ice deposits on the Moon still require additional development [9, 10, 11]. In 2018, NASA confirmed the presence of water ice on the Moon’s poles, concentrated chiefly in PSRs [12]. The near-permanent lack of sunlight in these regions results in extremely low temperatures (as low as -238 °C) and allows for the accumulation of water ice and other volatiles [13].\nOne such PSR is Shackleton Crater.\n\nWhile the presence of water ice in Shackleton Crater is evident [6], there are no precise measurements of its quantity or chemical composition. This detailed information, along with topographic maps of crater terrain, is critical to initiate targeted mining operations for ISRU. However, acquiring these measurements requires proximate investigations in extreme lunar environments that pose significant mobility challenges to exploration platforms.\n\nHigh Porosity Regolith Surface: The surface of the lunar South Pole is characterized by high porosity regolith that poses significant locomotion challenges [14]. Due to the high porosity, traditional wheeled rovers suffer sinkage and slipping, reducing energy efficiency and increasing the risk of immobilization. The high porosity lunar dust is also abrasive and invades machinery due to its particulate nature [15].\n\nTerrain Profile: Traversing the immense slopes to reach all areas of scientific interest inside the crater also poses significant challenges. Shackleton Crater is a massive geographic feature 21 kilometers in diameter. The crater slope leading to the crater floor has an average slope of 30.5 degrees and covers a horizontal distance of approximately 8 kilometers. This steep crater slope is difficult to both ascend and descend. On the descent, there is limited traction due to the lower lunar gravity further reducing normal force, and the regolith substrate being more prone to yield, leading to slipping and sinkage.\n\nThis mobility challenge also extends to the uneven crater floor and surrounding areas. The crater slope and floor have a root mean square (RMS) surface roughness of approximately 1 meter. This makes it difficult for wheeled systems to traverse, as the height of the obstacle they can overcome is limited by their wheel diameter. Boulder fields outside the crater, the unknown mechanical properties of regolith with ice, and a lack of detailed topography of the crater floor further raise the requirements for an adaptable system.\n\nA large of number of studies have investigated the mechanics of wheeled and legged locomotion in such conditions, using both analytical terramechanics models and experimental proxies. For instance, Ishigami et al. [16] and Shrivastava et al. [17] analyze wheel-soil interactions using lunar simulants, including deformable media like poppy seeds, to understand sinkage and traction loss. Similarly, recent work by Kolvenbach et al. [18] and Karsai et al. [19] explores how legged robots navigate granular slopes, addressing challenges in slip, footing stability, and dynamic control. However, the systematic consideration of substrate phenomena–such as mass wasting, sinkage, and traction loss–during inclined traversal remains largely unexplored.\n\nLack of sunlight: Shackleton Crater is a PSR, leading to two challenges: power generation and near-absolute zero temperature. Traversal of the crater is limited due to the lack of solar power. Therefore, large distances must be covered with minimal power consumption, or systems must rely on heavy power generation mechanisms such as radioisotope thermoelectric generators (RTGs). Low temperatures interfere with mechanisms that rely on lubricants and liquid batteries. This results in less efficient power systems.\n\nTo overcome locomotion challenges in the Moon’s hostile environment, a robotic system must be designed to minimize sinkage and slippage, while incorporating features to prevent immobilization. It must traverse long distances in an energy-efficient manner while maintaining passive and active regolith mitigation strategies and be able recover from, or continue to operate with, component failures.\n\nCurrent state-of-the-art wheeled rovers [20, 21, 22, 23] shown in Fig. 1 struggle on steep inclines due to inadequate traction on the porous regolith and decreased stability from the reduced normal force. Hybrid systems such as SherpaTT [24] and Scarab [25] partially address this issue by incorporating articulated legs that can lift the wheels off the ground to reposition them. More advanced multi-modal platforms like Robosimian [26] combine wheels with fully articulated legs, enabling both wheeled and legged locomotion, which improves steep slope traversal but remains slow and energy-intensive. An alternative approach is to use tether-assisted mobility, implemented in the DuAxel system [27], where one half of the rover anchors itself while the other rappels down steep terrain using a tether. While effective in certain terrains, the scale and steepness of craters such as Shackleton limit the practicality of tethered solutions.\n\nLegged systems, such as ETH Zurich’s Spacebok [40], JPL’s Llama [38], and ANYmal [39], offer another solution by allowing precise control of foot placement, enabling stable locomotion on loose surfaces. Despite their adaptability, these systems are generally energy-intensive. To address this, recent works like SpaceHopper [37] have explored hopping-based locomotion to leverage the Moon’s low gravity for energy-efficient travel over long distances. For steeper slopes, climbing robots equipped with micro-grippers, such as JPL’s LEMUR 3 [41], have been developed, but these designs are tailored for hard, rocky surfaces and are unsuitable for the soft, granular lunar regolith.\n\nFlying systems can traverse large distances quickly, as demonstrated by Ingenuity [48] on Mars. NASA’s Dragonfly [3], the Mars Science Helicopter [29] and the Sample Return Helicopter [36] are further exploring flight-based solutions for planetary exploration. However, replicating this approach on the Moon, which lacks an atmosphere, would require the use of ion or hydrogen-based propulsion systems. These technologies are extremely energy-intensive, making frequent recharging or carrying significant fuel reserves necessary for sustained operation. Thruster-assisted multi-modal designs like LEONARDO [32], which combine hopping and flight, present a potential compromise, but their exhaust may alter the chemical composition of the lunar regolith, complicating scientific investigations. Similarly, other multi-modal systems such as the M4 [33] and JPL’s Rollocopter [34], which combine wheeled mobility and flight, would encounter similar challenges.\n\nUnconventional morphologies, such as the screw-driven snake robot EELS [4] designed for icy terrains, and micro-scale jumping robots like Frogbot [45], offer new paradigms for locomotion. To achieve safe locomotion down steep crater walls, a particularly promising approach is seen in JPL’s Hedgehog [46], a tumbling robot that uses internal flywheels to generate momentum. Tumbling locomotion takes advantage of gravity to descend slopes, eliminating the need for active actuation and resulting in highly energy-efficient traversal. Despite its long history, tumbling has seen limited application in space exploration, leaving its full potential largely unexplored. Given its simplicity and efficiency, it presents an exciting opportunity for navigating the Moon’s challenging terrain.\n\nThe merits and limitations of tumbling robots are well documented. Passive systems like NASA/JPL’s Mars Tumbleweed Rover [49] are energy-efficient, making them ideal for remote exploration where energy conservation is critical. Active rolling spherical robots, such as MIT’s Kickbot [50], with their low center of gravity and omnidirectional mobility, are robust against external perturbations and excel in tight spaces. However, tumbling robots face challenges. Passive systems often sacrifice controllability for energy efficiency, relying on their shape and external forces for maneuvering. Additionally, rolling robots typically use their entire body for locomotion, leaving no stable platform for sensors, which complicates tasks like localization and perception.\n\nEarly rolling robots like Rollo [51] and the Spherical Mobile Robot (SMR) [52] used a spring-mass system with a driving wheel to create a mass imbalance for movement, but they were unreliable, as the driving wheel often lost contact with the sphere. Furthermore, significant central weight was required to generate enough inertia for propulsion. Other notable designs include the University of Pisa’s Sphericle [53] and Spider Rolling Robot (SRR) [54]. Sphericle used a car inside a sphere to drive the structure, but relied on gravity to keep the car wheels in contact with the inside of the sphere. Large perturbations led by mobility on rough terrain could dislodge the car and incapacitate the robot.\n\nA more precise tumbling method involves shifting internal weights to control the center of mass, as seen in the University of Michigan’s Spherobot [55] and University of Tehran’s August Robot [56], though these systems are not energy-efficient due to the added weight required.\n\nA more energy-efficient means of positioning the center of gravity for tumbling is by using deformable structures. Successful examples [57, 58, 59, 60, 61] can be identified that have attempted rolling by articulated structural designs that allow such deformation. Notable examples are Ritsumeikan University’s Deformable Robot [62], and Ourobot [63] with an articulated closed-loop structure.\n\nNASA’s Hedgehog [46] is a notable example developed specifically for space applications, derived from ETH’s Cubli [47]. It combines hopping, oblique hopping, tumbling, escape motion, and yawing (pointing) to traverse asteroids and comets. While it offers robust locomotion on a flat surface, because of its simple operation principle, Hedgehog has a few serious shortcomings that make it unsuitable for long-distance mobility. First, it can get stuck in soft terrain due to its jagged shape (a cube with protusions at its corners). A tornado (fast rotations around body axes) maneuver is proposed by its designers to escape; however, any fast body movements in these conditions can aggravate the situation. Next, because of its cubic shape, Hedgehog’s tumbling can be rough compared to other smoother circular geometries. Last, Hedgehog’s actuation relies on spinning three internal flywheels with considerable inertia to produce reaction torque. This actuation can be very costly, particularly for larger versions of Hedgehog. Therefore, if large-payload systems are considered in future NASA Moon missions, Hedgehog may suffer from scalability issues.\n\nOur proposed system, COBRA, is a snake-inspired multimodal rover designed for challenging terrain exploration (Fig. 3). COBRA combines the advantages of tumbling locomotion with the morphology afforded by snake robots to effectively address the locomotion challenges of steep crater walls as well as the uneven terrain of the crater floor. COBRA is lightweight at just 7.11 kg, with a compact diameter of 10 cm and a length of 1.7 m. Its eleven actuated degrees of freedom (DOF) enable it to morph its body shape to achieve various locomotion modes that can adapt to unpredictable and rough terrain. To achieve tumbling locomotion, COBRA raises and links its head and tail modules together to transform into a wheel-like structure. We identify these two distinct configurations as (1) Snake Configuration, where the head and tail is unconnected, and (2) Hex-Ring Configuration, where the head and tail are connected to form a closed loop.\n\nIn Snake Configuration, COBRA employs sidewinding and other slithering gaits to move efficiently across flat or uphill terrain. Sidewinding is a gait used by snakes to traverse loose or slippery surfaces like sand, and is particularly relevant for traversing lunar regolith, which shares similar properties. Variations of sidewinding locomotion have been shown to be fast and efficient, able to push off rocks and other obstacles to minimize energy consumption and increase traction [64, 65, 66]. Further, the symmetric snake-like morphology makes it excellent at traversing uneven and unknown terrain by virtue of not needing to reorient itself. The system’s weight is distributed along the entire length of its body which mitigates sinkage, and also allows for a large payload capacity, which can carry sensors such as a spectrometer to determine the concentration of hydrogen in the lunar regolith. By swinging its joints back and forth, the robot can also dig into the ground, exposing scientific samples below the surface or clearing material away if the system is stuck. It can also lift sections of its body into the air to overcome obstacles, climb out of holes, and aim instruments for navigation or communication.\n\nFor tumbling down slopes, COBRA enters the Ring Configuration and shifts its center of gravity to initiate tumbling. Through manipulation of its posture within the ring configuration using its joint actuators, COBRA can actively steer itself with minimal effort, allowing it to track desired paths to reach points of interest along the slope or avoid obstacles. These capabilities are not limited to lunar exploration. As a ground based system, it is a powerful tool for exploration of similar environments on Mars, such as Valles Marineris, a canyon system that features large, sloped terrain that our system can efficiently travel down to perform in-situ measurements in channels that potentially contain water.\n\nSome examples of tumbling platforms employing active deformation have been discussed above. COBRA distinguishes itself from these platforms in two significant ways: (1) Dynamic Tumbling Locomotion: As a tumbling system, it is field-tested and capable of dynamic tumbling locomotion with steering in two dimensions (2) Multimodal Locomotion and Manipulation: COBRA is a multimodal robot that employs the Snake Configuration to perform various locomotion and manipulation tasks on flat ground. This feature extends its operational scope beyond that of typical tumbling robots, enabling it to address diverse mission requirements.\n\nPrevious studies have extensively explored snake locomotion across various robotic platforms [67, 68, 69, 70, 71, 72]. COBRA builds on these works by implementing a broad range of snake gaits, achieving untethered operation, and integrating these capabilities with tumbling locomotion. This enables COBRA to operate effectively across a wide range of environments of various slopes and roughness, making it highly suitable for the exploratory nature of the proposed task.\n\nA comparable platform of note is NASA JPL’s EELS snake robot [4], which employs a screw propulsion mechanism along the length of its body, enabling locomotion on low-friction ice surfaces such as those found on Europa. However, this mechanism is less suited for granular and rocky terrains like those encountered on the Moon. COBRA is designed with locomotion strategies tailored for high-roughness surfaces, utilizing slithering and sidewinding gaits for flat or slightly inclined terrain and tumbling locomotion for rapid movement across steep slopes. This combination makes COBRA uniquely suited for environments where rapid elevation changes and substrate variability are critical considerations, for example lunar sites such as Shackleton Crater.\n\nHere we present the mission scenario envisioned for COBRA’s operation considering the case study of Shackleton Crater. COBRA will be deployed from a Commercial Lunar Payload Services (CLPS) lander near the edge of Shackleton Crater, as illustrated in Fig. 2. Following deployment, the system will navigate to the slope of the crater using sidewinding locomotion. In this mode, segments of the robot’s body are lifted and shifted forward, while others remain in contact with the ground, minimizing shear forces at contact points and preventing slippage on the soft regolith.\n\nNext, COBRA leverages the Moon’s partial gravity to tumble down the steep slope in its wheel-like configuration. Transitioning from the snake configuration, the system shifts its internal weight onto the slope to initiate tumbling. During descent, the joints remain static, conserving energy.\n\nThe system will tumble incrementally, halting every 500 meters to disconnect its head and tail for in-situ measurements. The scientific payload, a spectrometer housed in the tail, can be positioned using COBRA’s joints. This allows the system to create a detailed hydrogen concentration map at various depths within the crater. After collecting data, COBRA transmits the results to a lunar orbiter via a radio antenna in its head module, eliminating the need for the system to climb out of the crater for data transmission.\n\nFollowing communication at the sampling location, COBRA reforms its tumbling structure, tumbles another 500 meters, and repeats the process. Upon reaching the flatter center of Shackleton Crater, it will switch to sidewinding to continue its exploration and data collection until its power is exhausted.\n\nBased on this mission scenario, we come up with four objectives that we focus on for this work. 1) Locomotion over flat or slightly sloped ground using snake-like gaits. 2) Locomotion down slopes using two tumbling configurations. 3) Multi-modal operation allowing seamless switching between the modes of locomotion with no human contact with the robot. 4) Modular design that is robust to potential failures that can occur during remote operation in a space mission.\n\nFor this prototype, we focused on developing COBRA’s locomotion capabilities and designing a robust, modular system. While not using space-grade materials, the prototype is built to withstand outdoor terrestrial conditions. The following sections detail our development and testing process towards achieving these objectives.",
            "llm_summary": "大模型总结失败",
            "llm_score": 0,
            "llm_error": "API 状态码异常：403，响应：{\"error\":{\"message\":\"免费API限制模型输入token小于4096，如有更多需求，请访问 https://api.chatanywhere.tech/#/shop 购买付费API。The number of prompt tokens for free accounts is limited to 4096. If you have additional requirements, please visit https://api.chatanywhere.tech/#/shop to purchase a premium key.(当前请求使用的ApiKey: sk-8l9****i4zt)【如果您遇到问题，欢迎加入QQ群咨询：1048463714】\",\"type\":\"chatanywhere_error\",\"param\":null,\"code\":\"403 FORBIDDEN\"}}"
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "CU-Multi: A Dataset for Multi-Robot Collaborative Perception",
            "authors": "Doncey Albin,Daniel McGann,Miles Mena,Annika Thomas,Harel Biggie,Xuefei Sun,Steve McGuire,Jonathan P. How,Christoffer Heckman",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19463",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19463",
            "arxiv_html_link": "https://arxiv.org/html/2509.19463v1",
            "abstract": "A central challenge for multi-robot systems is fusing independently gathered perception data into a unified representation. Despite progress in Collaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of dedicated multi-robot datasets. Many evaluations instead partition single-robot trajectories, a practice that may only partially reflect true multi-robot operations and, more critically, lacks standardization, leading to results that are difficult to interpret or compare across studies. While several multi-robot datasets have recently been introduced, they mostly contain short trajectories with limited inter-robot overlap and sparse intra-robot loop closures. To overcome these limitations, we introduce CU-Multi, a dataset collected over multiple days at two large outdoor sites on the University of Colorado Boulder campus. CU-Multi comprises four synchronized runs with aligned start times and controlled trajectory overlap, replicating the distinct perspectives of a robot team. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined ground-truth odometry. By combining overlap variation with dense semantic annotations, CU-Multi provides a strong foundation for reproducible evaluation in multi-robot collaborative perception tasks. The dataset, support code, and updates are available at https://arpg.github.io/cumulti.",
            "introduction": "Multi-robot systems significantly enhance capabilities across diverse domains, particularly in large-scale environments, by accelerating exploration through distributed sensing and collaborative decision-making [7]. A central challenge in realizing these advantages lies in fusing perception data collected independently by multiple robots into a unified global representation, complicated by spatial and temporal misalignment. This challenge is further compounded in many practical scenarios where external positioning methods (e.g. GPS, motion capture) are impractical, unreliable, or hazardous. Multi-robot Collaborative SLAM (C-SLAM) algorithms are typically first developed and validated offline using datasets before doing field tests in real-world conditions, where system-level issues often arise. Consequently, the availability of realistic, well-structured multi-robot datasets is essential for supporting reproducible research and bridging the gap between offline development and real-world deployment.\n\nSeveral multi-robot datasets have recently been introduced for C-SLAM verification [3, 6, 1]. These datasets include synchronized multi-agent sensor data collected across both indoor and/or outdoor environments. While these datasets represent strong starting points, there remains a need for longer-trajectory multi-robot datasets with explicitly varied overlap (see Table I). Despite recent advancements in multi-robot datasets, there remains a prevalent practice of artificially segmenting a single trajectory into multiple parts to simulate a multi-robot scenario for verification [8]. Single-robot SLAM datasets still offer a few benefits over many of the existing multi-robot datasets, such as large trajectories with multiple loop closures, as well as the availability of both camera and LiDAR semantics [8, 9]. However, without careful consideration, arbitrary segmentation may not accurately represent realistic observational overlap typically encountered in multi-robot operations [10].\n\nIn this paper, we introduce CU-Multi, a dataset designed to support the evaluation of methods relevant to multi-robot perception. These methods include C-SLAM, LiDAR and visual data association, and multi-session LiDAR-based place recognition. CU-Multi offers the following key features:\n\nA multi-robot dataset consisting of two large-scale environments, each with four robots. Our dataset contains a total of eight diverse trajectories, spanning a combined length of 16.7 km across the CU Boulder campus (Figure 1).\n\nSystematically varied trajectory overlaps and a rendezvous-based trajectory design, enabling evaluation of multi-robot perception tasks under different levels of observational redundancy and variation.\n\nRefined ground truth poses using RTK GPS, a digital elevation model, lidar-inertial SLAM, and a highly accurate scan-matching algorithm, calculated at each LiDAR timestamp.\n\nSemantic labels for all LiDAR scans, dataset tools for interacting with the data, as well as initial benchmarks on a well-known C-SLAM and LiDAR place recognition method to demonstrate dataset usability and utility of the provided tools.",
            "llm_summary": "【关注的是什么问题】  \n1. 多机器人系统在融合独立收集的感知数据时面临的挑战。  \n2. 现有多机器人数据集缺乏长轨迹和标准化，导致评估结果难以比较。  \n3. 需要一个结构良好的多机器人数据集以支持可重复的研究。  \n\n【用了什么创新方法】  \nCU-Multi数据集在两个大型户外环境中收集，包含四个同步运行的机器人，具有控制的轨迹重叠和多样化的重叠变化。数据集提供RGB-D传感、RTK GPS、语义LiDAR和精细的真实轨迹，支持多机器人协作感知任务的评估。通过丰富的语义注释和系统化的轨迹设计，CU-Multi为多机器人感知方法的评估提供了坚实的基础，并展示了其在C-SLAM和LiDAR位置识别方法上的初步基准效果。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Self-evolved Imitation Learning in Simulated World",
            "authors": "Yifan Ye,Jun Cen,Jing Chen,Zhihe Lu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19460",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19460",
            "arxiv_html_link": "https://arxiv.org/html/2509.19460v1",
            "abstract": "Imitation learning has been a trend recently, yet training a generalist agent across multiple tasks still requires large-scale expert demonstrations, which are costly and labor-intensive to collect.\nTo address the challenge of limited supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework that progressively improves a few-shot model through simulator interactions.\nThe model first attempts tasks in the simulator, from which successful trajectories are collected as new demonstrations for iterative refinement.\nTo enhance the diversity of these demonstrations, SEIL employs dual-level augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model to collaborate with the primary model, and (ii) Environment-level, introducing slight variations in initial object positions.\nWe further introduce a lightweight selector that filters complementary and informative trajectories from the generated pool to ensure demonstration quality.\nThese curated samples enable the model to achieve competitive performance with far fewer training examples.\nExtensive experiments on the LIBERO benchmark show that SEIL achieves a new state-of-the-art performance in few-shot imitation learning scenarios.\nCode is available at https://github.com/Jasper-aaa/SEIL.git.",
            "introduction": "Imitation Learning (IL) [1, 2, 3, 4] has demonstrated remarkable success across a variety of tasks by leveraging large-scale expert demonstration datasets [5].\nHowever, collecting such datasets is often time-consuming, labor-intensive [6], and in certain domains (e.g., surgical practice) can even be impractical [7].\nThis motivates the study of IL under limited supervision, which is both a more practical and challenging setting.\nHowever, learning from only a few demonstrations typically results in a substantial performance drop. For instance, in the one-shot setting, a train-from-scratch Diffusion Policy [2] achieves only 0.8% success rate, compared to 50.5% with full training.\n\nInspired by the empirical success of Reinforcement Learning (RL) [8, 9, 10, 11, 12], where agents explore through extensive trial-and-error within simulators, a natural solution is to leverage simulation environments to autonomously generate additional demonstrations for fine-tuning few-shot imitation policies.\nThat is, a policy is initially trained on a limited set of expert demonstrations and then deployed in a simulator, where it interacts with the environment and records successful trajectories based on simple success/failure feedback.\nOnce a sufficient number of demonstrations across tasks are collected, the policy is refined using standard imitation learning updates on the newly acquired data.\nHowever, this approach gives rise to two critical challenges:\n(i) How can we ensure sufficient diversity in the simulated demonstrations to support iterative learning?\n(ii) How can we identify and select the most informative samples to optimize model performance while maintaining computational efficiency?\n\nIn this paper, we investigate a few-shot imitation learning setting, address the aforementioned two key challenges and propose Self-Evolved Imitation Learning (SEIL), a novel framework that leverages simulator to enable policy self-evolution from limited expert demonstrations, often with significant improvements as shown in Figure 1.\nSEIL is built upon two core components: dual-level augmentation, comprising model-level and environment-level strategies, for generating diverse trajectories, and a sample selector tailored to identify and retain the most informative demonstrations for efficient policy refinement.\nNotably, SEIL supports multiple rounds of interaction with the simulator, enabling the policy to evolve progressively over time.\nSpecifically, for model-level augmentation, SEIL introduces an auxiliary model to conduct additional rollouts alongside the primary model.\nTo avoid incurring extra training overhead, particularly important in a multi-stage evolution setting where cost accumulates over iterations, this auxiliary policy is implemented as an Exponential Moving Average (EMA) of the main model.\nThe EMA model maintains diversity in the policy space without the need for separate training, thereby supporting efficient and scalable demonstration generation.\nIn addition to model-level augmentation, we introduce environment-level augmentation, which enables the policy to interact with the simulator under diverse initial conditions.\nThis variation in starting states enhances environmental diversity, thereby enriching the recorded demonstrations and improving the robustness of policy learning.\n\nAfter generating a pool of diverse demonstrations via dual-level augmentation, we introduce a lightweight selector to identify informative samples.\nThe efficiency nature of this selector comes from taking as input only an initial image and a trajectory, rather than full video sequences.\nTrained on few-shot data via a trajectory classification task, the selector learns category-specific feature patterns.\nOnce trained, the selector is frozen and applied to all recorded demonstrations.\nSamples with the lowest confidence, i.e., those most distinct from expert demonstrations, are selected for policy training.\n\nOur contributions are summarized as follows.\n\nWe investigate a few-shot imitation learning setting and propose Self-Evolved Imitation Learning (SEIL), a novel framework that leverages simulation environments to enable policy self-evolution from limited expert demonstrations.\n\nWe propose dual-level augmentations, i.e., model-level and environment-level augmentation, to guarantee the diversity of demonstration generation.\n\nWe propose a lightweight selector to identify the most informative demonstrations, enhancing both policy performance and training efficiency.\n\nExtensive experiments demonstrate that SEIL can effectively evolve weak few-shot trained models, achieving substantial improvements, e.g., a 217.3% performance growth over the 1-shot baseline on Libero-Long.\n\n1. We investigate a few-shot imitation learning setting and propose Self-Evolved Imitation Learning (SEIL), a novel framework that leverages simulation environments to enable policy self-evolution from limited expert demonstrations.\n\n2. We propose dual-level augmentations, i.e., model-level and environment-level augmentation, to guarantee the diversity of demonstration generation.\n\n3. We propose a lightweight selector to identify the most informative demonstrations, enhancing both policy performance and training efficiency.\n\n4. Extensive experiments demonstrate that SEIL can effectively evolve weak few-shot trained models, achieving substantial improvements, e.g., a 217.3% performance growth over the 1-shot baseline on Libero-Long.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在有限的专家演示下进行有效的模仿学习？  \n2. 如何确保模拟演示的多样性以支持迭代学习？  \n3. 如何识别和选择最具信息量的样本以优化模型性能？  \n\n【用了什么创新方法】  \n提出了自我进化模仿学习（SEIL）框架，通过模拟器交互逐步改进少量模型。采用双层增强策略，分别在模型和环境层面生成多样化轨迹，并引入轻量级选择器过滤出有用的演示样本。实验结果表明，SEIL在少样本模仿学习场景中达到了新的最先进性能，尤其在Libero-Long基准上实现了217.3%的性能增长。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation",
            "authors": "Jason Chen,I-Chun Arthur Liu,Gaurav Sukhatme,Daniel Seita",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19454",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19454",
            "arxiv_html_link": "https://arxiv.org/html/2509.19454v1",
            "abstract": "Training robust bimanual manipulation policies via imitation learning requires demonstration data with broad coverage over robot poses, contacts, and scene contexts.\nHowever, collecting diverse and precise real-world demonstrations is costly and time-consuming, which hinders scalability. Prior works have addressed this with data augmentation, typically for either eye-in-hand (wrist camera) setups with RGB inputs or for generating novel images without paired actions, leaving augmentation for eye-to-hand (third-person) RGB-D training with new action labels less explored. In this paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation (ROPA), an offline imitation learning data augmentation method that fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D observations of novel robot poses. Our approach simultaneously generates corresponding joint-space action labels while employing constrained optimization to enforce physical consistency through appropriate gripper-to-object contact constraints in bimanual scenarios. We evaluate our method on 5 simulated and 3 real-world tasks. Our results across 2625 simulation trials and 300 real-world trials demonstrate that ROPA outperforms baselines and ablations, showing its potential for scalable RGB and RGB-D data augmentation in eye-to-hand bimanual manipulation.\nOur project website is available at: https://ropaaug.github.io/.",
            "introduction": "Bimanual manipulation is critical for a wide range of daily tasks [1] such as lifting large objects [2, 3], handling deformable objects [4, 5, 6, 7], and opening containers [8, 9].\nThese activities require coordinated motion of both arms and awareness of nearby objects and surfaces. A third-person camera can capture both arms and the surrounding scene from one viewpoint, which is appealing for vision-based imitation learning [10].\nRecent imitation learning approaches show that large demonstration datasets yield increasingly general bimanual policies [11, 12, 13], but collecting sufficiently large and diverse action-labeled data is costly, which limits scalability.\n\nData augmentation has emerged as a promising strategy to address this bottleneck, particularly in single-arm manipulation, where novel views and corresponding action labels can be synthesized offline [14].\nHowever, extending these techniques to bimanual manipulation, depth images, and beyond wrist-mounted (eye-in-hand) viewpoints [15, 16], introduces new difficulties. These include enforcing scene-wide visual consistency across two robot arms, preserving action label correctness, and handling the higher degrees-of-freedom (DOF).\n\nIn this paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation (ROPA), a data augmentation method for vision-based imitation learning of bimanual manipulation. Our key insight in ROPA is to adapt Pose-Guided Person Image Synthesis techniques [17] (originally developed for human pose generation) by conditioning a diffusion model on robot joint configurations. This enables the diffusion model to yield realistic and consistent third-person views for varied robot arm poses. We maintain a tight correspondence between augmented visual observations and valid actions, so that policies trained on the augmented dataset remain executable on real hardware.\n\nWe evaluate ROPA across 5 simulated and 3 real-world bimanual manipulation tasks, based on the PerAct2 [18] benchmark in simulation and a physical bimanual UR5 setup. See Figure 1 for an example rollout. Compared to an Action Chunking with Transformers (ACT) [19] baseline trained only on raw demonstrations, data augmentation using ROPA enables ACT to achieve higher success rates across bimanual manipulation tasks that require coordinated motion and fine-grained precision.\nBy enabling third-person vision-based data augmentation for bimanual systems, ROPA scales training data and broadens coverage without extra manual data collection.\n\nThe contributions of this paper include:\n\nROPA, a novel pose-guided image synthesis method for offline data augmentation in bimanual manipulation, which supports learning from both RGB and RGB-D data.\n\nDepth image synthesis that generates depth maps consistent with the augmented joint positions and RGB images.\n\nAction-consistent augmentation that outputs images and joint-space labels, with constraints to ensure feasibility.\n\nSimulation and real-world experiments showing that bimanual policies with ROPA achieve significantly improved performance over baseline methods and ablations.\n\n1. ROPA, a novel pose-guided image synthesis method for offline data augmentation in bimanual manipulation, which supports learning from both RGB and RGB-D data.\n\n2. Depth image synthesis that generates depth maps consistent with the augmented joint positions and RGB images.\n\n3. Action-consistent augmentation that outputs images and joint-space labels, with constraints to ensure feasibility.\n\n4. Simulation and real-world experiments showing that bimanual policies with ROPA achieve significantly improved performance over baseline methods and ablations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效生成多样化的机器人姿态以增强双手操作的模仿学习数据？  \n2. 如何在数据增强过程中保持视觉一致性和动作标签的正确性？  \n3. 如何在不增加手动数据收集成本的情况下扩展训练数据的覆盖范围？  \n\n【用了什么创新方法】  \n提出了一种名为ROPA的合成机器人姿态生成方法，通过微调Stable Diffusion模型，生成第三人称RGB和RGB-D观察数据。该方法在生成图像的同时，确保动作标签的一致性，并通过约束优化来维持物理一致性，确保夹持器与物体的接触约束。经过在5个模拟和3个真实任务上的评估，ROPA在2625次模拟试验和300次真实试验中表现优于基线和消融实验，展示了其在双手操作中的数据增强潜力。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames",
            "authors": "Alessandro Saviolo,Jeffrey Mao,Giuseppe Loianno",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19452",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19452",
            "arxiv_html_link": "https://arxiv.org/html/2509.19452v1",
            "abstract": "Search and rescue operations require unmanned aerial vehicles to both traverse unknown unstructured environments at high speed and track targets once detected. Achieving both capabilities under degraded sensing and without global localization remains an open challenge. Recent works on relative navigation have shown robust tracking by anchoring planning and control to a visible detected object, but cannot address navigation when no target is in the field of view.\nWe present HUNT (High-speed UAV Navigation and Tracking), a real-time framework that unifies traversal, acquisition, and tracking within a single relative formulation. HUNT defines navigation objectives directly from onboard instantaneous observables such as attitude, altitude, and velocity, enabling reactive high-speed flight during search. Once a target is detected, the same perception–control pipeline transitions seamlessly to tracking. Outdoor experiments in dense forests, container compounds, and search-and-rescue operations with vehicles and mannequins demonstrate robust autonomy where global methods fail.\nVideo: http://bit.ly/4n5Tp5D",
            "introduction": "Unmanned Aerial Vehicles (UAVs), especially quadrotors, have become essential tools in Search-and-Rescue (SAR) operations, where their maneuverability and speed enable rapid deployment in unstructured, GPS-denied environments. These missions require two critical capabilities: the ability to safely traverse unknown terrain at high speed to search for victims or objects of interest, and the ability to reliably track and follow detected targets once they are identified. Achieving both in degraded sensing and unknown environments remains a fundamental challenge in robotics.\n\nTraditional autonomy relies on global navigation anchored by GPS, Visual-Inertial Odometry (VIO), or Simultaneous Localization and Mapping (SLAM) [1]. These methods degrade in exactly the conditions most critical to SAR: dense forests, collapsed buildings, or urban canyons where GPS is degraded and visual features are sparse, dynamic, or occluded. Their dependence on persistent landmarks and loop closures makes global pose estimation fragile, resulting in drift and unsafe behavior at high speed.\n\nAn alternative paradigm, recently advanced in prior works [2], is to abandon global consistency and instead define navigation directly in a target-relative reference frame. By continuously re-anchoring perception, planning, and control to the instantaneous detection of a moving object, UAVs can robustly pursue targets even in unstructured or degraded environments. This instantaneous relative navigation formulation has demonstrated strong performance for the tracking phase of SAR missions. However, it presumes that a target is visible from the outset and remains continuously observable—an assumption that rarely holds in practice. What is missing is a capability for safe, high-speed traversal in unknown terrain without any target directly in view, and without reverting to the fragile assumptions of traditional GPS, VIO, or SLAM.\n\nThis paper introduces HUNT, a reactive autonomy framework that unifies high-speed traversal, acquisition, and tracking under a single instantaneous relative formulation. At its core is a novel loitering mode, which defines navigation objectives solely from directly observable quantities—attitude, altitude, and instantaneous velocity—allowing the UAV to traverse unknown cluttered environments safely even in the absence of a target. When a target is detected, the same perception–control pipeline seamlessly re-anchors the reference frame to the target, turning tracking into a special case of loitering without requiring global state.\n\nSafety is enforced throughout by embedding high-order Control Barrier Functions (CBFs) directly into a Nonlinear Model Predictive Controller (NMPC), guaranteeing dynamically feasible, collision-free trajectories in real time. Robust transitions between modes are enabled by a confidence-based switching mechanism that prevents oscillations and ensures stability as detections appear and disappear.\n\nWe validate HUNT in extensive outdoor SAR-like missions spanning urban compounds, semi-structured layouts, and dense forests. Experiments include loitering flights over city blocks and forest canopy, dense-clutter traversal below canopy, and full missions where the UAV searches, acquires, and pursues static and dynamic targets such as vehicles and mannequins. Across all settings, HUNT achieves consistent high-speed performance where global methods fail.\n\nTo the best of our knowledge, HUNT is the first framework to integrate traversal, acquisition, and tracking in cluttered, GPS-denied environments under a unified instantaneous relative formulation, eliminating reliance on global pose.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在未知、无结构环境中实现无人机的高速度导航和目标跟踪。  \n2. 传统的全球导航方法在SAR任务中面临的挑战和局限性。  \n3. 在没有目标可见的情况下进行安全的高速度穿越的能力。  \n\n【用了什么创新方法】  \nHUNT框架通过将高速度穿越、目标获取和跟踪统一在一个瞬时相对参考框架下，提出了一种新颖的巡航模式。该模式仅依赖于可直接观察的量（如姿态、高度和瞬时速度）来定义导航目标，使无人机能够在没有目标的情况下安全穿越未知环境。当目标被检测到时，感知-控制管道无缝切换到跟踪模式，确保高效的目标追踪。通过将高阶控制障碍函数嵌入到非线性模型预测控制器中，HUNT保证了实时的动态可行和无碰撞轨迹。实验结果表明，HUNT在复杂的城市和森林环境中表现出一致的高速度性能，超越了传统方法的局限。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models",
            "authors": "JuanaJuana Valeria Hurtado,Rohit Mohan,Abhinav Valada",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20107",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20107",
            "arxiv_html_link": "https://arxiv.org/html/2509.20107v1",
            "abstract": "Hyperspectral imaging (HSI) captures spatial information along with dense spectral measurements across numerous narrow wavelength bands. This rich spectral content has the potential to facilitate robust robotic perception, particularly in environments with complex material compositions, varying illumination, or other visually challenging conditions. However, current HSI semantic segmentation methods underperform due to their reliance on architectures and learning frameworks optimized for RGB inputs. In this work, we propose a novel hyperspectral adapter that leverages pretrained vision foundation models to effectively learn from hyperspectral data. Our architecture incorporates a spectral transformer and a spectrum-aware spatial prior module to extract rich spatial-spectral features. Additionally, we introduce a modality-aware interaction block that facilitates effective integration of hyperspectral representations and frozen vision Transformer features through dedicated extraction and injection mechanisms.\nExtensive evaluations on three benchmark autonomous driving datasets demonstrate that our architecture achieves state-of-the-art semantic segmentation performance while directly using HSI inputs, outperforming both vision-based and hyperspectral segmentation methods. We make the code available at https://hyperspectraladapter.cs.uni-freiburg.de.",
            "introduction": "Advancing robot perception in complex environments requires an understanding of detailed scene characteristics beyond surface appearance [1]. Hyperspectral imaging (HSI), which captures reflectance information across tens to hundreds of narrow spectral bands, provides material-specific information that is significantly richer than conventional RGB imaging [2]. By encoding distinctive spectral signatures, HSI enables fine-grained discrimination of objects and materials, offering the potential to enhance perception tasks [3, 4] under challenging conditions such as varying illumination, occlusions [5], scene clutter [6], and complex material compositions. These properties make HSI particularly promising for autonomous driving, where reliable perception in diverse and visually complex environments is essential.\n\nHyperspectral semantic segmentation aims to classify each pixel of an image into predefined categories based on HSI inputs. Despite its potential, semantic segmentation with hyperspectral data remains a challenging problem. Most segmentation models are tailored for RGB images and fail to fully leverage the spectral richness of HSI. Standard vision semantic segmentation architectures, although proven powerful for extracting spatial semantics from RGB data, are not inherently equipped to model the complex inter-channel dependencies of hyperspectral inputs. As a result, models pretrained on RGB datasets often underperform when directly applied to hyperspectral inputs [7, 8, 9]. Although pseudo-RGB approaches can partially adapt existing models, they overlook the spectral diversity inherent in HSI and fail to address critical challenges such as high dimensionality, limited labeled data, and increased computational costs [2]. We illustrate the difference between these approaches in Fig. 1.\n\nVision foundation models trained on large-scale datasets generalize well across domains [10]. However, their full finetuning is computationally prohibitive, and linear probing lacks capacity for complex tasks such as hyperspectral segmentation. Adapter tuning provides an efficient alternative by inserting lightweight modules into a frozen backbone, allowing for task-specific adaptation without retraining the entire model. However, existing adapters are designed for RGB inputs and, when applied to HSI, they face initialization sensitivity and fail to capture spectral dependencies, limiting their effectiveness.\n\nTo address these limitations, we propose the HSI-Adapter, a modular architecture that enables leveraging strong pretrained vision foundation models with HSI inputs for hyperspectral semantic segmentation. Our approach introduces a spectral transformer and a spectral-enhanced spatial prior module to jointly extract rich spectral and spatial features from hyperspectral inputs. To bridge the modality gap between hyperspectral and RGB representations, we propose a modality-aware interaction block that enables effective bidirectional feature exchange between hyperspectral features and foundational vision features. This topology preserves the strengths of the pretrained foundation model while incorporating the unique characteristics of hyperspectral data. Extensive experiments on HSI-DriveV2 [11], HyperspectralCityV2.0 (HCV2) [12], and HyKo2-VIS [13] benchmark autonomous driving datasets demonstrate that our method achieves state-of-the-art performance, surpassing both RGB-based and hyperspectral segmentation approaches. In addition to quantitative gains, qualitative results show that our method generalizes well to diverse and complex driving environments, handling cluttered scenes, dense urban layouts, and visually similar material classes such as painted and unpainted metal.\n\nOur primary contributions are summarized as follows:\n\nA novel hyperspectral adapter architecture tailored for semantic segmentation using HSI inputs.\n\nA spectral transformer and spectrum-aware spatial prior module to jointly model spatial and spectral context.\n\nA modality-aware interaction block that enables effective feature fusion between hyperspectral and pretrained foundational vision representations.\n\nComprehensive experiments and ablation studies across three benchmark datasets, demonstrating the effectiveness and generalization of our approach.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效利用高光谱成像（HSI）数据进行语义分割？  \n2. 当前基于RGB的语义分割模型在处理HSI数据时表现不佳的原因是什么？  \n3. 如何在不完全重训练的情况下，适应预训练视觉基础模型以处理HSI输入？  \n\n【用了什么创新方法】  \n本研究提出了一种新颖的高光谱适配器架构，结合了光谱变换器和光谱增强空间先验模块，以联合提取高光谱输入的丰富光谱和空间特征。此外，提出的模态感知交互块实现了高光谱特征与预训练视觉特征之间的有效双向特征交换。通过在三个基准数据集上的广泛评估，证明了该方法在语义分割性能上达到了最先进的水平，超越了现有的RGB和高光谱分割方法。定量和定性结果表明，该方法在复杂的驾驶环境中表现良好，能够处理拥挤场景和视觉相似材料类别。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Embodied AI: From LLMs to World Models",
            "authors": "Tongtong Feng,Xin Wang,Yu-Gang Jiang,Wenwu Zhu",
            "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Robotics (cs.RO)",
            "comment": "Accepted by IEEE CASM",
            "pdf_link": "https://arxiv.org/pdf/2509.20021",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20021",
            "arxiv_html_link": "https://arxiv.org/html/2509.20021v1",
            "abstract": "Embodied Artificial Intelligence (AI) is an intelligent system paradigm for achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications and driving the evolution from cyberspace to physical systems. Recent breakthroughs in Large Language Models (LLMs) and World Models (WMs) have drawn significant attention for embodied AI. On the one hand, LLMs empower embodied AI via semantic reasoning and task decomposition, bringing high-level natural language instructions and low-level natural language actions into embodied cognition. On the other hand, WMs empower embodied AI by building internal representations and future predictions of the external world, facilitating physical law-compliant embodied interactions. As such, this paper comprehensively explores the literature in embodied AI from basics to advances, covering both LLM driven and WM driven works. In particular, we first present the history, key technologies, key components, and hardware systems of embodied AI, as well as discuss its development via looking from unimodal to multimodal angle. We then scrutinize the two burgeoning fields of embodied AI, i.e., embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs, meticulously delineating their indispensable roles in end-to-end embodied cognition and physical laws-driven embodied interactions. Building upon the above advances, we further share our insights on the necessity of the joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds. In addition, we examine representative applications of embodied AI, demonstrating its wide applicability in real-world scenarios. Last but not least, we point out future research directions of embodied AI that deserve further investigation.",
            "introduction": "Embodied Artificial Intelligence (AI) originated from the Embodied Turing Test by Alan Turing in 1950 [1], which is designed to explore whether agents can imitate human intelligence to achieve Artificial General Intelligence (AGI). Among them, agents that only solve abstract problems in digital world (cyberspace) are generally defined as disembodied AI, while those that also can interact with the physical world are regarded as embodied AI. Embodied AI builds on foundational insights from cognitive science and neuroscience [2, 3], which claims that intelligence emerges from the dynamic coupling of perception, cognition, and interaction. As shown in Fig. 1, embodied AI includes three key components in a closed-loop manner, i.e., 1) active perception (sensor-driven environmental observation), 2) embodied cognition (historical experience-driven cognition updating), and 3) dynamic interaction (actuator-mediated action control). Besides, hardware embodiment [4, 5, 6] is also critical due to escalating computational and energy demands, particularly under latency and power constraints of devices in real-world deployment scenarios.\n\nThe development of embodied AI has evolved from unimodal to multimodal paradigm. In early stage, embodied AI is primarily studied through focusing on individual components with single modality such as vision, language, or action, where the perception, cognition, or interaction component is driven by one sensory input [7, 8], e.g., perception tends to be dominated by the visual modality [9], cognition tends to be dominated by the language modality [10, 11], and interaction tends to be dominated by the action modality [12, 13]. Although these methods perform well within individual components, they are limited by the narrow scope of information provided by each modality and the inherent gaps between modalities across components. The continued development of embodied AI witnesses the limitations of unimodal approaches, promoting a significant shift toward integration of multiple sensory modalities [14, 15, 16]. As such, multimodal embodied AI [17, 18] naturally arises to create more adaptive, flexible, and robust agents capable of performing complex tasks in dynamic environments.\n\nLarge Language Models (LLMs) empower embodied AI via semantic reasoning [19] and task decomposition [20, 21], bringing high-level natural language instructions and low-level natural language actions into embodied cognition. Representative LLM driven works include SayCan [22], which i) provides a real-world pretrained natural language action library to constrain LLMs from proposing infeasible and contextually inappropriate actions; ii) uses LLMs to convert natural language instructions into natural language action sequences; and iii) utilizes value functions to verify the feasibility of natural language action sequences in a particular physical environment. These works suggest that LLMs are extremely useful to robots which aim at acting upon high-level, temporally extended instructions expressed in natural language. However, LLMs are only a part of the entire embodied AI system (e.g., embodied cognition), which is limited by a fixed natural language action library and a specific physical environment, making it difficult for LLM driven embodied AI to achieve adaptive expansion for new robots and environments.\n\nRecent breakthroughs in Multimodal LLMs (MLLMs) [23, 24] and World Models (WMs) [25, 26, 27] have opened up a new frontier in embodied AI research. MLLMs can act on the entire embodied AI system, bridging high-level multimodal inputting and low-level motor action sequences into end-to-end embodied applications. Semantic reasoning [28, 29, 30] leverages MLLMs’ cross-modal comprehension to interpret semantics from visual, auditory, or tactile inputs, e.g., identifying objects, inferring spatial relationships, predicting environmental dynamics. Concurrently, task decomposition [31, 32, 33] employs MLLMs’ sequential logic to break complex objectives into sub-tasks while dynamically adapting plans based on sensor feedback. However, MLLMs often fail to ground predictions in physics-compliant dynamics [34] and exhibit poor real-time adaptation [35] to environmental feedback.\n\nOn the other hand, WMs empower embodied AI by building internal representations [36, 37, 38, 39, 40] and making future predictions [41, 42, 43, 44] of the external world. Such WM driven embodied AI is able to facilitate physical law-compliant embodied interactions in dynamic environments. Internal representations compress rich sensory inputs into structured latent spaces, capturing object dynamics, physics laws, and spatial structures, as well as allowing agents to reason about “what exists” and “how things behave” in their surroundings. Simultaneously, future predictions simulate potential rewards of sequence actions across multiple time horizons aligned with physical laws, thereby preempting risky or inefficient behaviors. However, WM driven approaches struggle with open-ended semantic reasoning [45] and lack the ability of generalizable task decomposition [26] without explicit priors.\n\nBuilding upon the above advances, we further share our insights on the necessity of developing a joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds. MLLMs enable contextual task reasoning but overlook physical constraints, while WMs excel at physics-aware simulation but lack high-level semantics. The joint of MLLM and WM can bridge semantic intelligence with grounded physical interaction. For instance, EvoAgent [46] designs an autonomous-evolving agent with a joint MLLM-WM driven embodied AI architecture, which can autonomously complete various long-horizon tasks across environments through self-planning, self-reflection, and self-control, without human intervention. We believe that designing joint MLLM-WM driven embodied AI architectures will dominate next-generation embodied systems, bridging the gap between specialized AI agents and general physical intelligence.\n\nWe summarize the representative applications of embodied AI as service robotics, rescue UAVs, industrial Robots, and others etc., demonstrating its wide applicability in real-world scenarios. We also point out potential future directions of embodied AI, including but not limited to autonomous embodied AI, embodied AI hardware, and swarm embodied AI etc.\n\nAs shown in Fig. 2, the rest of this paper is organized as follows. Section II introduces the history, key technologies, key components, and hardware system of embodied AI, discussing the development of embodied AI from unimodal to multimodal angle. Section III presents embodied AI with LLMs/MLLMs, and Section IV presents embodied AI with WMs. Section V introduces our insights on designing a joint MLLM-WM driven embodied AI architecture. Section VI briefly examines applications of embodied AI. Potential future directions are discussed in Section VII.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现从LLMs到世界模型的联合驱动的具身人工智能架构？  \n2. 如何在复杂的物理环境中实现具身AI的高效任务执行？  \n3. 具身AI如何克服单一模态方法的局限性，提升多模态感知与交互能力？  \n\n【用了什么创新方法】  \n本文提出了一种联合MLLM-WM驱动的具身AI架构，通过整合多模态语言模型和世界模型，增强了具身智能的语义推理和物理交互能力。该架构能够在动态环境中实现复杂任务的自我规划、自我反思和自我控制，展现出在真实场景中广泛的应用潜力。研究表明，这种方法显著提高了具身AI在物理法则遵循下的交互能力，并推动了下一代具身系统的发展。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents",
            "authors": "Filippo Ziliotto,Jelin Raphael Akkara,Alessandro Daniele,Lamberto Ballan,Luciano Serafini,Tommaso Campari",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19843",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19843",
            "arxiv_html_link": "https://arxiv.org/html/2509.19843v1",
            "abstract": "Recent advances in Embodied AI have enabled agents to perform increasingly complex tasks and adapt to diverse environments. However, deploying such agents in realistic human-centered scenarios, such as domestic households, remains challenging, particularly due to the difficulty of modeling individual human preferences and behaviors.\nIn this work, we introduce PersONAL  (PERSonalized Object Navigation And Localization), a comprehensive benchmark designed to study personalization in Embodied AI.\nAgents must identify, retrieve, and navigate to objects associated with specific users, responding to natural-language queries such as find Lily’s backpack.\nPersONAL comprises over 2,000 high-quality episodes across 30+ photorealistic homes from the HM3D dataset. Each episode includes a natural-language scene description with explicit associations between objects and their owners, requiring agents to reason over user-specific semantics.",
            "introduction": "In recent years, Embodied AI has significantly advanced, enabling agents to perform complex tasks and interact more naturally with their environments. Modern methods combine end-to-end training with zero-shot capabilities powered by large language models (LLMs), allowing agents to answer dynamically to user input [1, 2, 3, 4, 5, 6, 7].\nYet, their application to user-centric scenarios, where agents must interpret local, implicit information not directly encoded in pretrained models, such as object ownership, remains largely unexplored.\nBridging this gap is key to deploying embodied agents in real-world environments like homes or offices.\nWhile personalized vision-language models (VLMs) [8, 9, 10] have been developed for user-specific visual grounding, they are typically limited to static, image-based contexts.\nIn contrast, embodied agents must operate in complex physical settings, reasoning and acting over time.\n\nRecently, a few works have begun to explore personalization in embodied scenarios [11, 12], but the field remains in its early stages and these works mainly focus on guiding agents using image-based queries or continuous human-robot interaction, which limits scalability and real-world applicability.\n\nTo address this gap, we introduce PersONAL , the first embodied AI benchmark for personalized, user-centric navigation and personalized object grounding (Figure 1).\nAgents must interpret user-specific queries and either navigate to or retrieve the location of objects associated with particular individuals (e.g., “Navigate to Carl’s backpack”).\nEach episode includes a textual scene description specifying object attributes and ownership (e.g., “the upper kitchen cabinet belongs to Linda”), followed by a personalized query.\nUnlike prior work, we also define a grounding task which acts as an embodied memory challenge, requiring agents to recall and localize targets using internal maps.\n\nPersONAL supports two evaluation modes: (i) Personalized Active Navigation (PAN), the agent must navigate an unknown environment to find the target object, and (ii) Personalized Object Grounding (POG), where the goal is to localize objects within a pre-mapped environment.\nWe release a dataset of over 2,000 curated evaluation episodes across three difficulty levels (easy, medium, hard), capturing increasing complexity in human–object associationsWe evaluate the proposed benchmark with several zero-shot, state-of-the-art navigation baselines and introduce a simple zero-shot method for the grounding task, demonstrating in both settings that a substantial performance gap persists between current Embodied AI agents and human capabilities.\n\nIn summary, our main contributions are:\n\nWe present PersONAL , a comprehensive Embodied AI benchmark specifically designed to evaluate embodied personalization, incorporating user-centric queries and object–ownership semantics.\n\nWe release a dataset of 2,000 high-quality episodes sampled from over 30 realistic household environments, divided into three difficulty levels (easy, medium, and hard).\n\nWe provide empirical analyses with state-of-the-art zero-shot baselines, showcasing limitations and possible future research directions toward human-level personalized navigation in Embodied AI.\n\nWe argue that personalization cannot be achieved through repeated LLM-based interactions alone.\nInstead, agents must maintain and recall user-specific preferences internally—ideally through long-term memory and continual learning—an ability crucial in complex households settings where ownership is stable but scenes continually evolve.\n\n1. We present PersONAL , a comprehensive Embodied AI benchmark specifically designed to evaluate embodied personalization, incorporating user-centric queries and object–ownership semantics.\n\n2. We release a dataset of 2,000 high-quality episodes sampled from over 30 realistic household environments, divided into three difficulty levels (easy, medium, and hard).\n\n3. We provide empirical analyses with state-of-the-art zero-shot baselines, showcasing limitations and possible future research directions toward human-level personalized navigation in Embodied AI.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在真实人类中心场景中实现个性化的具身智能代理。  \n2. 现有的个性化视觉语言模型在动态环境中的应用局限性。  \n3. 如何有效地评估和提升具身智能代理的个性化导航能力。  \n\n【用了什么创新方法】  \n本研究提出了PersONAL，一个综合性的基准，专注于个性化的具身智能代理。该基准包含2000个高质量的评估情节，涵盖30多个真实家庭环境，代理需根据用户特定的自然语言查询进行导航和物体定位。研究展示了当前具身智能代理在个性化导航任务中的性能差距，并指出了未来研究方向，强调了内部记忆和持续学习在复杂家庭环境中的重要性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving",
            "authors": "Carlo Bosio,Greg Woelki,Noureldin Hendy,Nicholas Roy,Byungsoo Kim",
            "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19789",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19789",
            "arxiv_html_link": "https://arxiv.org/html/2509.19789v1",
            "abstract": "Human drivers focus only on a handful of agents at any one time. On the other hand, autonomous driving systems process complex scenes with numerous agents, regardless of whether they are pedestrians on a crosswalk or vehicles parked on the side of the road.\nWhile attention mechanisms offer an implicit way to reduce the input to the elements that affect decisions, existing attention mechanisms for capturing agent interactions are quadratic, and generally computationally expensive.\nWe propose RDAR, a strategy to learn per-agent relevance — how much each agent influences the behavior of the controlled vehicle — by identifying which agents can be excluded from the input to a pre-trained behavior model.\nWe formulate the masking procedure as a Markov Decision Process where the action consists of a binary mask indicating agent selection.\nWe evaluate RDAR on a large-scale driving dataset, and demonstrate its ability to learn an accurate numerical measure of relevance by achieving comparable driving performance, in terms of overall progress, safety and performance, while processing significantly fewer agents compared to a state of the art behavior model.",
            "introduction": "Humans, when driving, do not pay equal attention to all agents around them (e.g., other vehicles, pedestrians).\nTransfomer-based attention models offer the promise of attending only to relevant components of the input, but existing attention models are typically quadratic in the size of the input space. Driving models encounter hundreds of input tokens, leading to substantial computational complexity and latency Harmel et al. (2023); Huang et al. (2024); Baniodeh et al. (2025).\n\nIn autonomous driving, there is a tension between the limited available compute resources and the desire to take advantage of scaling laws, large models, and test-time compute. Having access to numerical per-agent relevance scores would not only improve the interpretability of large driving models, but also allow compute resources to be prioritized for the features that are most important.\nIn fact, when agents and other scene elements are represented explicitly as tokens, reasoning about interactions between these tokens (typically through self-attention or graph neural network operations) is quadratic and difficult to reduce using low-rank or other approximations that work well for long-sequence data. Reducing the number of tokens under consideration provides quadratic improvements in FLOPs used.\n\nIn this work, we introduce RDAR (Reward-Driven Agent Relevance), through which we quantify agent relevance through a learned approach. The basic intuition is that if an agent is not relevant towards the driving decisions of the controlled vehicle, then its absence would not change the controlled vehicle’s driving behavior significantly. Thus, we quantify per-agent relevance by learning which agents can be masked out from the controlled vehicle’s planner input while maintaining a good driving behavior.\nWe formulate agent selection as a reinforcement learning (RL) problem where an action is a binary mask indicating which agents to include in the driving policy input, and which not to.\nThe RDAR scoring policy is trained in the loop with a frozen, pre-trained driving policy and a simulator. At each time step, based on the relevance scores, an agent mask is fed to the driving policy, making the controlled vehicle blind to the lower score agents. As it will be clear from the following sections, this is not a binary classification problem over agents due to the underlying system dynamics (e.g., not observing an agent now could lead to a collision later) and to the unavailability of ground truth labels.\nSome examples of relevance (color-coded) computed by our method are shown in Fig. 1.\nOur main contributions are:\n\nA novel reinforcement learning formulation for agent relevance estimation;\n\nA sampling-based mechanism for agent selection that enables efficient training and inference;\n\nA comprehensive evaluation showing that we can maintain driving performance while processing only a handful of surrounding agents.\n\n1. A novel reinforcement learning formulation for agent relevance estimation;\n\n2. A sampling-based mechanism for agent selection that enables efficient training and inference;\n\n3. A comprehensive evaluation showing that we can maintain driving performance while processing only a handful of surrounding agents.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效评估每个代理对自动驾驶行为的影响？  \n2. 如何减少输入代理的数量以降低计算复杂性？  \n3. 如何在保持驾驶性能的同时提高模型的可解释性？  \n\n【用了什么创新方法】  \n提出了RDAR（Reward-Driven Agent Relevance），通过强化学习方法量化每个代理的相关性，形成一个二进制掩码来选择输入代理。该方法在保持良好驾驶行为的同时，显著减少了处理的代理数量，从而降低了计算复杂性。评估表明，RDAR在大规模驾驶数据集上表现出与最先进的行为模型相当的驾驶性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "VIMD: Monocular Visual-Inertial Motion and Depth Estimation",
            "authors": "Saimouli Katragadda,Guoquan Huang",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19713",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19713",
            "arxiv_html_link": "https://arxiv.org/html/2509.19713v1",
            "abstract": "Accurate and efficient dense metric depth estimation is crucial for 3D visual perception in robotics and XR.\nIn this paper, we develop a monocular visual-inertial motion and depth (VIMD) learning framework to estimate dense metric depth by leveraging accurate and efficient MSCKF-based monocular visual-inertial motion tracking.\nAt the core the proposed VIMD is to exploit multi-view information to iteratively refine per-pixel scale,\ninstead of globally fitting an invariant affine model as in the prior work.\nThe VIMD framework is highly modular, making it compatible with a variety of existing depth estimation backbones.\nWe conduct extensive evaluations on the TartanAir and VOID datasets and demonstrate its zero-shot generalization capabilities on the AR Table dataset.\nOur results show that VIMD achieves exceptional accuracy and robustness, even with extremely sparse points—as few as 10-20 metric depth points per image.\nThis makes the proposed VIMD a practical solution for deployment in resource-constrained settings,\nwhile its robust performance and strong generalization capabilities offer significant potential across a wide range of scenarios.",
            "introduction": "For applications like robotics and extended reality (XR), accurate and efficient metric dense depth estimation is critical for 3D visual perception, which is essential for tasks such as obstacle avoidance and motion planning.\nMonocular methods, which estimate depth from a single RGB image, are particularly appealing because they use a compact, inexpensive, and common camera. However, purely monocular vision suffers from an inherent scale ambiguity: it can capture the relative shape of a scene but cannot determine the true distances to objects.\nKnowing exactly how far an obstacle is from the camera can be the difference between safe passage and collision.\nIntegrating inertial data can (partially) resolve this ambiguity.\nAs most mobile devices and robots are already equipped with an IMU,\nvisual–inertial odometry (VIO) or SLAM prevails but typically yields only sparse metric depth from a small set of tracked landmarks [1],\nwhich alone cannot provide the dense metric depth map required in safety-critical operations.\n\nAlthough recent advances in monocular depth prediction have achieved high-quality relative depth estimation [3, 4], these methods still lack the ability to produce outputs with absolute metric scale.\nA common approach assumes an invariant affinity model and aligns the predicted depth to the sparse VIO depth via per-frame least-squares fitting for a global scale and offset (e.g., see [5]).\nHowever, as shown in Fig. 1, the fitted scale and offset vary significantly across frames—particularly the offset, which exhibits much larger fluctuations than the scale.\nThis temporal instability indicates that the predicted relative depths are not truly affine with respect to the metric depths; that is, they cannot be accurately modeled by a single global scale and offset per frame without unmodeled errors.\nBesides the unreliable per-frame global fitting, the optimization may become poorly conditioned when sparse points are unevenly distributed or noisy, leading to sensitivity in the fitted parameters and potential error amplification in low-density regions.\nIt has been empirically observed that predicting offset along with scale can hurt depth accuracy [6].\n\nFrom Fig. 1, it is clear that the offset exhibits much higher variance across frames than the scale, implying that offsets may also vary more significantly across pixels within a single frame (due to factors like viewpoint changes, occlusions, or non-uniform scene structure), thus making them harder to learn reliably at the per-pixel level compared to the more stable scale.\nMotivated by this observation, we propose to exploit multi-view constraints and learn to optimize iteratively a per-pixel scale, formulated as learning approach that moves beyond the global fitting affine parameters.\nIn particular, the proposed monocular Visual-Inertial Motion and Depth (VIMD)\nleverages an accurate and efficient MSCKF-based visual-inertial motion estimation module to form multi-view geometric constraints\nand then refines and predicts dense depth and its uncertainty.\nAs a result, the proposed VIMD improves accuracy and robustness in low-density depth regions.\nFig. 2 shows some exemplar performance of the proposed VIMD in both outdoor and indoor scenes, producing high-quality dense metric depths.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在单目视觉中准确高效地估计稠密的度量深度。  \n2. 解决单目视觉方法中的尺度模糊问题，以提高深度估计的准确性和鲁棒性。  \n3. 如何利用视觉惯性数据来增强深度估计的性能，尤其是在稀疏点情况下。  \n\n【用了什么创新方法】  \n提出了一种单目视觉-惯性运动和深度(VIMD)学习框架，通过利用基于MSCKF的视觉惯性运动跟踪，迭代优化每个像素的尺度，克服了传统方法中全局拟合不稳定的问题。该框架兼容多种深度估计模型，并在TartanAir和VOID数据集上进行了广泛评估，展示了其在资源受限环境中的实用性和强大的零-shot泛化能力。VIMD在极少的深度点（仅10-20个）情况下，仍能实现卓越的准确性和鲁棒性，适用于多种场景。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar",
            "authors": "William L. Muckelroy III,Mohammed Alsakabi,John M. Dolan,Ozan K. Tonguz",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19644",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19644",
            "arxiv_html_link": "https://arxiv.org/html/2509.19644v1",
            "abstract": "LiDAR’s dense, sharp point cloud (PC) representations of the surrounding environment enable accurate perception and significantly improve road safety by offering greater scene awareness and understanding. However, LiDAR’s high cost continues to restrict the broad adoption of high-level Autonomous Driving (AD) systems in commercially available vehicles. Prior research has shown progress towards circumventing the need for LiDAR by training a neural network, using LiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds using only 4D Radars. One of the best examples is a neural network created to train a more efficient radar target detector with a modular 2D convolutional neural network (CNN) backbone and a temporal coherence network at its core that uses the RaDelft dataset for training [1]. In this work, we investigate the impact of higher-capacity segmentation backbones on the quality of the produced point clouds. Our results show that while very high-capacity models may actually hurt performance, an optimal segmentation backbone can provide a 23.7% improvement over the state-of-the-art (SOTA).",
            "introduction": "There are numerous reasons why widespread adoption of commercially available autonomous vehicles (AV) and commercially viable autonomous driving (AD) has not occurred, one of which is the cost-prohibitive sensor suites they employ. These systems are designed to significantly improve road safety through their greater awareness and understanding of the scene, allowing them to interact with the world and react to environmental disturbances in a safe manner. However, an essential component of these systems is LiDAR because of the sharp, dense point cloud (PC) representations of the environment they can produce. These PCs are often used with arrays of radars and cameras to detect people, cars, and other objects as the ego vehicle traverses the world. Of these three primary sensors, we see that LiDARs take a significant stake in the overall cost of the sensor suite in AVs, ranging from $4,000 for mid-range LiDARs to $70,000 for long-range high-end LiDARs like that used in the KITTI dataset [2][3]. However, 4D Radar is a relatively new emerging sensor in the AD space that offers promising performance at lower costs, as seen in Table I.\n\nPrevious research has shown that one can build a system that has the potential to replace LiDAR by leveraging concepts from Deep Learning (DL). By training a neural network using LiDAR PCs as ground truth (GT), one can produce LiDAR-like 3D PCs using only 4D radars [1][6][7][8][5][4]. An example of such a network is the neural network proposed in [1] and trained on the RaDelft dataset as a more efficient radar target detector. Using a modular and straightforward 2D convolutional neural network (CNN) backbone for segmentation and a temporal coherence network enabled the authors of [1] to achieve significant improvements in bidirectional chamfer distance (BCD) when compared to other methods like various forms of Constant False Alarm Rate (CFAR) (i.e., CA-CFAR, SOCA-CFAR, GOCA-CFAR, OS-CFAR, ML-CFAR, etc.) [9][10][11].\n\nOur work further investigates the impact of the 2D segmentation backbone and the number of 3D convolutional layers in the temporal coherence network on the overall performance of the predicted point clouds. This allows us to gain insight into how optimizations of the overall architecture can further minimize BCD and increase the viability of this technique as a potential replacement for LiDAR.\n\nThe remainder of this paper is organized as follows. Section II presents the background of the problem investigated and different definitions of key terms used throughout the paper. Section III outlines the methods used in this work for experimentation and how we evaluate the performance of such experiments. Section IV presents the primary findings of our work. Finally, Section V summarizes the discussions that have been held throughout this paper, along with our intentions to further improve the metrics and results in the future.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用4D雷达替代高成本的LiDAR进行3D点云预测。  \n2. 2D分割骨干网络对生成点云质量的影响。  \n3. 高容量模型在点云生成中的性能表现。  \n\n【用了什么创新方法】  \n本研究通过训练神经网络，使用LiDAR点云作为真实值，探索了不同2D分割骨干网络对生成点云质量的影响。重点在于优化网络架构，包括2D CNN骨干和时间一致性网络的3D卷积层数量。结果表明，适当的分割骨干网络可以在性能上比现有技术提高23.7%。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation",
            "authors": "Ramy ElMallah,Krish Chhajer,Chi-Guhn Lee",
            "subjects": "Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "Accepted to the CoRL 2025 Eval&Deploy Workshop",
            "pdf_link": "https://arxiv.org/pdf/2509.19524",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19524",
            "arxiv_html_link": "https://arxiv.org/html/2509.19524v1",
            "abstract": "Robot learning papers typically report a single binary success rate (SR), which obscures where a policy succeeds or fails along a multi-step manipulation task. We argue that subgoal-level reporting should become routine: for each trajectory, a vector of per-subgoal SRs that makes partial competence visible (e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware plug-in evaluation framework that utilizes vision–language models (VLMs) as automated judges of subgoal outcomes from recorded images or videos. Rather than proposing new benchmarks or APIs, our contribution is to outline design principles for a scalable, community-driven open-source project. In StepEval, the primary artifact for policy evaluation is the per-subgoal SR vector; however, other quantities (e.g., latency or cost estimates) are also considered for framework-optimization diagnostics to help the community tune evaluation efficiency and accuracy when ground-truth subgoal success labels are available. We discuss how such a framework can remain model-agnostic, support single- or multi-view inputs, and be lightweight enough to adopt across labs. The intended contribution is a shared direction: a minimal, extensible seed that invites open-source contributions, so that scoring the steps, not just the final goal, becomes a standard and reproducible practice.",
            "introduction": "Robotic manipulation research largely evaluates policies with a single binary metric (success or failure). While simple to report, success rate (SR) provides a general, non-granular view of performance [1, 2]. A policy either achieves the task or not, with no insight into partial progress or which sub-task caused the failure. This is especially problematic for long-horizon tasks composed of multiple sequential subgoals (e.g., grasp, lift, place in a pick-and-place task). A low success rate might indicate failure, but it does not reveal where the policy struggled. As a motivating example, Kress-Gazit et al. [1] describe a pancake-flipping task consisting of several stages (picking up spatulas, flipping the pancake, then plating it). One policy in their study had only 17% overall success, yet it completed the first two subgoals 100% of the time; it consistently grasped the spatulas and flipped the pancake, failing only at the final plating step. If judged solely by overall SR, this policy would be deemed poor, obscuring the fact that it mastered over half the task. Granular evaluation is essential: without it, researchers cannot pinpoint failure modes (e.g., was it the grasp or the pour that failed?), making improvements guesswork.\n\nThe need for fine-grained, nuanced evaluation in robot learning has been increasingly recognized. Recent best-practice guidelines explicitly recommend reporting subgoal completion metrics and failure mode analyses alongside overall success [1, 3, 4]. However, adopting such practices in everyday research is challenging: manually labeling each sub-step success for dozens or hundreds of experiments is labor-intensive and subjective. Some works have automated subgoal checks in simulation (where the state is known) or relied on human labeling for physical trials. Vision-language models (VLMs) offer a promising middle ground, a way to automatically classify outcomes from visual data. Indeed, initial attempts have been made to use learned classifiers [5, 6] or VLMs [7] to detect task failures.\n\nOur goal is to reframe evaluation practice by advocating a subgoal-first view of manipulation metrics. We present StepEval as a blueprint for a cost-aware, plug-in framework that treats the per-subgoal success-rate (SR) vector as the primary artifact for policy evaluation. In this view, a VLM acts as a black-box automated judge that maps recorded imagery to subgoal outcomes. We articulate design principles for a community-built, scalable open-source project. All other quantities (e.g., latency, token/image cost, or confusion matrices based on ground-truth subgoal labels) are framed as framework-optimization diagnostics that help tune evaluation efficiency and accuracy when ground-truth labels are available, rather than as core evaluation metrics.\n\nWe contribute by articulating the following pillars for subgoal-based evaluation:\n\nSubgoal-first evaluation perspective. We formalize the evaluation target as a trajectory-level vector 𝐲∈{0,1}n\\mathbf{y}\\in\\{0,1\\}^{n} of per-subgoal SRs and argue that this granular object, rather than a single binary task SR, should become the standard reported outcome for manipulation studies.\n\nModel-agnostic judging concept. We propose using a VLM as a judge to infer subgoal outcomes from RGB image frames (single- or multi-view) without instrumenting policies or environments, keeping the framework post-hoc and lightweight. The design is intentionally policy- and model-agnostic, with the aim of making scoring the steps a routine, reproducible practice across labs.\n\nSeparation of evaluation vs. framework optimization metrics. We distinguish the primary evaluation artifact (per-subgoal SR vector) from optional diagnostics (e.g., cost/latency; confusion matrices based on ground-truth subgoal labels; or optional prompt-optimization procedures). The latter are intended only to help the community tune efficiency and accuracy when labels exist, not to redefine the core evaluation target.\n\n1. Subgoal-first evaluation perspective. We formalize the evaluation target as a trajectory-level vector 𝐲∈{0,1}n\\mathbf{y}\\in\\{0,1\\}^{n} of per-subgoal SRs and argue that this granular object, rather than a single binary task SR, should become the standard reported outcome for manipulation studies.\n\n2. Model-agnostic judging concept. We propose using a VLM as a judge to infer subgoal outcomes from RGB image frames (single- or multi-view) without instrumenting policies or environments, keeping the framework post-hoc and lightweight. The design is intentionally policy- and model-agnostic, with the aim of making scoring the steps a routine, reproducible practice across labs.\n\n3. Separation of evaluation vs. framework optimization metrics. We distinguish the primary evaluation artifact (per-subgoal SR vector) from optional diagnostics (e.g., cost/latency; confusion matrices based on ground-truth subgoal labels; or optional prompt-optimization procedures). The latter are intended only to help the community tune efficiency and accuracy when labels exist, not to redefine the core evaluation target.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有的机器人学习评估方法仅使用单一的成功率（SR），缺乏对多步骤任务的细粒度分析。  \n2. 研究者无法准确识别失败模式，导致改进过程变得困难和不确定。  \n3. 手动标注每个子步骤的成功与否既费时又主观，亟需自动化解决方案。  \n\n【用了什么创新方法】  \n提出了一种名为StepEval的框架，利用视觉-语言模型（VLM）自动评估机器人操作中的子目标成功率（SR）。该框架以每个子目标的SR向量作为主要评估指标，旨在提供细粒度的任务完成情况反馈。通过将VLM作为黑箱评判者，StepEval能够从记录的图像中推断出子目标结果，且不需要对政策或环境进行额外的仪器化。该方法保持轻量级和模型无关，促进了社区的开放源代码贡献，推动了细化评估的标准化实践。最终，StepEval为机器人操作的评估提供了更高效和准确的途径。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Robust Near-Optimal Nonlinear Target Enclosing Guidance",
            "authors": "Abhinav Sinha,Rohit V. Nanavati",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO); Optimization and Control (math.OC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19477",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19477",
            "arxiv_html_link": "https://arxiv.org/html/2509.19477v1",
            "abstract": "This paper proposes a nonlinear optimal guidance law that enables a pursuer to enclose a target within arbitrary geometric patterns, which extends beyond conventional circular encirclement. The design operates using only relative state measurements and formulates a target enclosing guidance law in which the vehicle’s lateral acceleration serves as the steering control, making it well-suited for aerial vehicles with turning constraints. Our approach generalizes and extends existing guidance strategies that are limited to target encirclement and provides a degree of optimality. At the same time, the exact information of the target’s maneuver is unnecessary during the design. The guidance law is developed within the framework of a state-dependent Riccati equation (SDRE), thereby providing a systematic way to handle nonlinear dynamics through a pseudo-linear representation to design locally optimal feedback guidance commands through state-dependent weighting matrices. While SDRE ensures near-optimal performance in the absence of strong disturbances, we further augment the design to incorporate an integral sliding mode manifold to compensate when disturbances push the system away from the nominal trajectory, and demonstrate that the design provides flexibility in a sense that the (possibly time-varying) stand-off curvature could also be treated as unknown. Simulations demonstrate the efficacy of the proposed approach.",
            "introduction": "Driven by increasing autonomy requirements, unmanned aerial vehicles (UAVs) have become integral to a variety of mission-critical operations such as area coverage, surveillance, environmental monitoring, aerial defense, reconnaissance, and search and rescue, e.g., [1, 2, 3, 4]. A common control objective in these applications involves regulating the motion of a UAV designated as the pursuer to maintain a prescribed geometric configuration or trajectory with respect to a reference entity, termed the target. The target may correspond to another vehicle, a stationary beacon, or a dynamically evolving point of interest. This objective, referred to as target enclosing, entails enforcing spatial constraints on the relative position of the pursuer with respect to the target, and is central to both civilian and military guidance and coordination strategies.\n\nOne of the earliest investigations into target enclosing using teams of mobile robots was presented in [5], where the agents were modeled under the assumption of holonomic dynamics. Subsequent works extended this framework, leveraging the relative simplicity of control synthesis afforded by holonomic vehicle models. However, in practical scenarios, the dynamics of mobile platforms are more accurately represented by non-holonomic constraints, which significantly complicate both analysis and control design. Target enclosing under non-holonomic vehicle dynamics has thus been examined in a more realistic setting in works such as [6, 7], often within the broader context of distributed formation control. Within this domain, a task of interest is maintaining circular motion around a target (circumnavigation). This specific formation behavior has attracted substantial research attention due to its relevance in surveillance, tracking, and protection missions, as evidenced by a wide body of literature (e.g., [8, 9, 10, 11, 12]).\n\nOne approach is to leverage vector fields defined around the desired trajectory or geometric pattern to regulate the vehicle’s heading and generate convergence behavior. In [13], the authors employed a Lyapunov-based vector field to coordinate the circumnavigation of a stationary target at multiple altitudes using a fleet of UAVs. The work in [14] extended this methodology to account for external disturbances, proposing a coordinated standoff tracking strategy in the presence of wind. In contrast to purely reactive vector field approaches, the work in [15] presented a nonlinear model predictive control framework for dual-vehicle coordinated standoff tracking, optimizing performance over a finite prediction horizon. An alternative formulation in [11] utilized spherical pendulum-inspired dynamics, wherein backstepping and Lyapunov-based techniques were applied to regulate position and velocity tracking errors for robust guidance law synthesis. Beyond vector field and predictive control approaches, several recent strategies have explored guidance law design under partial or minimal state information, often motivated by scenarios with limited sensing or communication capabilities. These methods typically employ either relative range and or bearing measurements (see, for example, [12, 10, 16, 17, 8]).\n\nDespite such solutions, several key challenges remain unaddressed. This work advances the state of the art in target-enclosing guidance by addressing limitations of existing methods, which we summarize below:\n\nMany of the aforementioned methods assume slow or negligible target motion, which may become limited under adversarial scenarios if the target moves aggressively. The proposed framework explicitly accommodates a general maneuvering target without requiring knowledge of its guidance law or future maneuvers.\n\nWhile previous studies rely on curvature information of the enclosing geometry, our formulation has the flexibility to eliminate this requirement, thereby broadening applicability to arbitrary geometric patterns provided the prescribed range-parameterized curve is at least ℂ2\\mathbb{C}^{2}.\n\nWe introduce a robust nonlinear optimal guidance design for this problem class by integrating an SDRE framework with a supertwisting sliding mode controller defined over an integral sliding manifold. This hybrid design leverages the optimality of SDRE while inheriting robustness and finite-time convergence properties from sliding mode control, where the reaching phase is absent.\n\n1. Many of the aforementioned methods assume slow or negligible target motion, which may become limited under adversarial scenarios if the target moves aggressively. The proposed framework explicitly accommodates a general maneuvering target without requiring knowledge of its guidance law or future maneuvers.\n\n2. While previous studies rely on curvature information of the enclosing geometry, our formulation has the flexibility to eliminate this requirement, thereby broadening applicability to arbitrary geometric patterns provided the prescribed range-parameterized curve is at least ℂ2\\mathbb{C}^{2}.\n\n3. We introduce a robust nonlinear optimal guidance design for this problem class by integrating an SDRE framework with a supertwisting sliding mode controller defined over an integral sliding manifold. This hybrid design leverages the optimality of SDRE while inheriting robustness and finite-time convergence properties from sliding mode control, where the reaching phase is absent.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有方法假设目标运动缓慢，无法应对快速移动目标的情况。  \n2. 许多研究依赖于包围几何形状的曲率信息，限制了应用范围。  \n3. 如何设计一个鲁棒的非线性最优引导法则以应对复杂动态环境。  \n\n【用了什么创新方法】  \n本研究提出了一种非线性最优引导法则，能够在任意几何模式下包围目标，扩展了传统的圆形包围方法。该方法基于状态依赖里卡提方程(SDRE)框架，结合超扭滑模控制器，确保在存在扰动时的鲁棒性和有限时间收敛性。通过仿真验证了该方法的有效性，显示出其在动态环境中对目标的灵活适应能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Learning from Observation: A Survey of Recent Advances",
            "authors": "Returaj Burnwal,Hriday Mehta,Nirav Pravinbhai Bhatt,Balaraman Ravindran",
            "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO); Machine Learning (stat.ML)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19379",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19379",
            "arxiv_html_link": "https://arxiv.org/html/2509.19379v1",
            "abstract": "Imitation Learning (IL) algorithms offer an efficient way to train an agent by mimicking an expert’s behavior without requiring a reward function. IL algorithms often necessitate access to state and action information from expert demonstrations. Although expert actions can provide detailed guidance, requiring such action information may prove impractical for real-world applications where expert actions are difficult to obtain. To address this limitation, the concept of learning from observation (LfO) or state-only imitation learning (SOIL) has recently gained attention, wherein the imitator only has access to expert state visitation information. In this paper, we present a framework for LfO and use it to survey and classify existing LfO methods in terms of their trajectory construction, assumptions and algorithm’s design choices. This survey also draws connections between several related fields like offline RL, model-based RL and hierarchical RL. Finally, we use our framework to identify open problems and suggest future research directions.",
            "introduction": "Imitation learning, often studied as part of reinforcement learning (RL), has emerged as a promising paradigm for training autonomous agents to perform tasks effectively by mimicking demonstrator behavior (Hussein et al., 2017a). In contrast to RL, which relies on environment reward, IL has the distinct advantage of learning solely from expert guidance, which alleviates the need to design a reward function. This feature proves particularly beneficial in complex environments where crafting a suitable reward function can be challenging. Standard methods developed in this framework often require access to demonstrations, in the form of state-action (st,at)(s_{t},a_{t}) pairs. While expert actions can provide detailed guidance, requiring such action information may be restrictive in practical scenarios. For instance, a human expert may demonstrate the object-manipulation task to a robot by manually moving the robot’s arm. The robot can record both its joint angles (state) and joint torques (action) induced by the human expert. Collecting such state-action pair demonstrations can be challenging or, in some cases, impossible; such as environments with limited access to physical demonstrators.\n\nLearning from demonstration (LfD) aims to replicate the expert’s actions for a given state; this introduces a constraint that both the expert and the imitator must share the same dynamics model: specifically, they must have identical action spaces and the exact next-state transition probabilities for all feasible state-action pairs. This assumption brings severe limitations, as imagine that a robot with a low-speed limit imitates another robot that moves fast; then, using the LfD framework, a slow robot cannot learn to mimic the behavior of the fast-moving robot. The LfD framework does not support transfer learning across dynamically different agents (Liu et al., 2020a).\n\nWhile IL research is inspired by the way humans learn from observations of another human, there exists a notable difference in the imitation process employed by humans. Unlike LfD, which often relies on demonstrator’s action information to guide behavior, humans typically imitate without requiring explicit action information. A more general framework, known as learning from observation (LfO) or state-only imitation learning (SOIL), involves an expert communicating solely the state sequence information using raw sequences of true agent states, or partial observations like images or videos. In comparison to LfD, LfO aligns more closely with the natural way humans learn to imitate in the physical world, a phenomenon referred to as observational learning in psychology (Bandura and Walters, 1977). Moreover, the LfO framework can support transfer learning across dynamically different agents, as it does not rely on demonstrator’s action information.\n\nThe recent works on LfO have demonstrated significant success in both simulated environments and real-world tasks, however the literature lacks a systematic review of this field. Though a novel taxonomy for LfO was proposed by Torabi et al. (2019b), we found that the recent LfO algorithms either do not neatly fit or cannot be categorised based on the established taxonomy (see section 9 for a detailed discussion of related work). Hence, this article presents a survey of the literature based on the fundamental questions on trajectory dataset construction and algorithm’s design choices.\n\nOverview\n\nThe goal of this article is to present a comprehensive overview of learning from observation and to provide an over-arching framework to formalize this class of methods. We aim to define classification criteria based on the trajectory data set’s construction and algorithms’ design choices. We address the following fundamental questions to structure our classification:\n\nWho qualifies as an expert? An expert can be a human, a distinct dynamical agent from the imitator, or the same dynamical agent as the imitator. The formulation of the algorithm varies depending on the identity of the expert with respect to the imitator. (Section 3.1)\n\nHow are expert trajectories collected? These trajectories can be collected in two primary ways: i) in a first-person viewpoint, where the state trajectories are collected from the agent-centric viewpoint. Collecting first-person demonstrations can be challenging or, in some instances, impossible. A more natural way would be to collect demonstrations from a ii) third-person viewpoint. (Section: 3.2)\n\nWhat are the different trajectory datasets? In section 3.3, we survey different LfO algorithms based on the trajectory dataset they use to learn from expert demonstrations. A trajectory dataset can contain demonstrations from a single or multiple experts. Additionally, we provide a discussion of how the trajectory dataset varies depending on whether the algorithm operates online or offline.\n\nHow are algorithms formulated to facilitate learning from state-only demonstrations? In section 4, we define and discuss our framework for classifying LfO approaches based on algorithm design choices.\n\nIn the next section, we will provide the necessary background for LfO algorithms. Section 3, 4 forms the survey’s core and will provide a comprehensive overview of the recent LfO methodologies. In Section 5, we discuss how these algorithms are used to address learning across different viewpoints and non-identical agents. In Section 6, we discuss the datasets, benchmarks and algorithms used in LfO research. Following that, in Section 7, we will discuss the connections between LfO algorithms and other related fields in RL. Subsequently, in Section 8, we will use our framework to identify open problems and discuss future directions. Lastly, we will conclude the article by discussing related surveys in Section 9 and summarize this article in Section 10.\n\n1. Who qualifies as an expert? An expert can be a human, a distinct dynamical agent from the imitator, or the same dynamical agent as the imitator. The formulation of the algorithm varies depending on the identity of the expert with respect to the imitator. (Section 3.1)\n\n2. How are expert trajectories collected? These trajectories can be collected in two primary ways: i) in a first-person viewpoint, where the state trajectories are collected from the agent-centric viewpoint. Collecting first-person demonstrations can be challenging or, in some instances, impossible. A more natural way would be to collect demonstrations from a ii) third-person viewpoint. (Section: 3.2)\n\n3. What are the different trajectory datasets? In section 3.3, we survey different LfO algorithms based on the trajectory dataset they use to learn from expert demonstrations. A trajectory dataset can contain demonstrations from a single or multiple experts. Additionally, we provide a discussion of how the trajectory dataset varies depending on whether the algorithm operates online or offline.\n\n4. How are algorithms formulated to facilitate learning from state-only demonstrations? In section 4, we define and discuss our framework for classifying LfO approaches based on algorithm design choices.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在缺乏专家动作信息的情况下进行模仿学习？  \n2. 如何分类和评估现有的学习观察（LfO）方法？  \n3. LfO方法如何支持不同动态代理之间的迁移学习？  \n4. 当前LfO领域存在哪些开放问题和未来研究方向？  \n\n【用了什么创新方法】  \n本文提出了一个框架用于学习观察（LfO），并对现有的LfO方法进行分类，重点关注轨迹构建、假设和算法设计选择。通过对不同专家身份、轨迹收集方式和数据集的分析，本文系统性地整理了LfO领域的研究进展。此外，文章还探讨了LfO与其他强化学习相关领域的联系，并识别了当前研究中的开放问题和未来的研究方向。通过这些分析，本文为LfO的研究提供了一个全面的视角和结构化的分类标准。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Scensory: Automated Real-Time Fungal Identification and Spatial Mapping",
            "authors": "Yanbaihui Liu,Erica Babusci,Claudia K. Gunsch,Boyuan Chen",
            "subjects": "Signal Processing (eess.SP); Robotics (cs.RO)",
            "comment": "Our project website is at:this http URL",
            "pdf_link": "https://arxiv.org/pdf/2509.19318",
            "code": "http://generalroboticslab.com/Scensory",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19318",
            "arxiv_html_link": "https://arxiv.org/html/2509.19318v1",
            "abstract": "Indoor fungal contamination poses significant risks to public health, yet existing detection methods are slow, costly, and lack spatial resolution. Conventional approaches rely on laboratory analysis or high-concentration sampling, making them unsuitable for real-time monitoring and scalable deployment. We introduce Scensory, a robot-enabled olfactory system that simultaneously identifies fungal species and localizes their spatial origin using affordable volatile organic compound (VOC) sensor arrays and deep learning. Our key idea is that temporal VOC dynamics encode both chemical and spatial signatures, which we decode through neural architectures trained on robot-automated data collection. We demonstrate two operational modes: a passive multi-array configuration for environmental monitoring, and a mobile single-array configuration for active source tracking. Across five fungal species, our system achieves up to 89.85% accuracy in species detection and 87.31% accuracy in localization under ambient conditions, where each prediction only takes 3–7 s sensor inputs. Additionally, by computationally analyzing model behavior, we can uncover key biochemical signatures without additional laboratory experiments. Our approach enables real-time, spatially aware fungal monitoring and establishes a scalable and affordable framework for autonomous environmental sensing.",
            "introduction": "Indoor fungal contamination poses significant risks to human health and building safety [1]. Mold thrives in damp and poorly ventilated environments, especially in older homes and infrastructure [2]. Exposure to airborne fungal emissions has been linked to asthma, allergies, and other respiratory illnesses, particularly in children and people with weakened immune systems [3, 4]. Yet current detection methods are slow, invasive, and expensive. They require laboratory processing and offer limited information about the spatial distribution of contamination. The lack of scalable, real-time, and spatially aware detection systems continues to hinder timely intervention and effective mitigation of fungal exposure risks.\n\nTraditional fungal detection methods are constrained by high cost, long processing time, and technical complexity. Culture-based assays aim to grow fungal colonies from surface or air samples and identify species through morphological or physiological features [5, 6]. While widely used, these approaches require incubation periods that can delay results, and the subsequent identification process is often manual, requiring expert interpretation or additional laboratory analysis.\nMicroscopic identification of spores and hyphae requires expert interpretation, offers limited species resolution, and lacks sensitivity at low contamination levels [7]. Molecular diagnostics such as qPCR and ITS-based sequencing improves sensitivity and specificity [8], but require sample preprocessing and specialized laboratory infrastructure, and can be confounded by DNA from nonviable or nonpathogenic organisms [9]. While accurate, these methods are impractical for routine monitoring or rapid field deployment.\n\nAs a non-invasive alternative, volatile organic compound (VOC) analysis has gained attention. Fungi emit diverse VOCs during metabolism, including alcohols, ketones, and sulfur-based compounds, that serve as chemical fingerprints [10, 11, 12]. Electronic nose (eNose) systems [13] use arrays of cross-sensitive sensors to capture these emissions, increasingly paired with machine learning classifiers. Recent advances have enabled real-time fungal detection across food, agriculture, and environmental applications [14, 15, 16, 17]. With appropriate training, ML-enhanced eNoses have achieved species-level accuracy exceeding 90% in controlled settings [18, 19]. However, these systems remain spatially blind and typically require sealed chambers, pre-concentration, or extended exposure times [20]. They cannot localize fungal sources in ambient real-world environments [21].\n\nIn parallel, mobile robots and autonomous agents have made progress in gas plume tracking and odor source localization under turbulent or outdoor conditions [22, 23, 24, 25]. These strategies leverage visible or high-concentration plumes and rely on structured airflow dynamics [26, 27], making them unsuitable for detecting the subtle, low-density, and highly variable emissions characteristic of indoor fungal growth.\n\nDespite substantial advances, existing technologies can either identify fungal species without spatial resolution, or they can localize odor sources but cannot handle complex and low-level VOC signatures from fungi. Such a disconnect highlights a critical gap: we lack a scalable and real-time approach that can simultaneously determine “what” fungal species are present and “where” they are distributed under natural and low concentration conditions.\n\nWe present Scensory, a robot-enabled chemical sensing system that performs simultaneous species identification and source localization using affordable VOC sensor arrays and deep learning. The key insight is that complex temporal VOC dynamics encode both chemical identity and spatial information, which can be decoded using neural networks. We demonstrate this system in two operational modes: a passive multi-array configuration for environmental monitoring, and a mobile single-array configuration for real-world deployments where only limited resources are available, or rapid, mobile, or cost-effective solutions are required. Furthermore, our modeling framework allows us to infer which sensor inputs are most biochemically relevant for species identification.\n\nTo enable scalable model training, we automate high-throughput data collection using a robotic arm that samples VOCs under various species and spatial arrangements. Across five phylogenetically diverse fungal species, our system achieves up to 89.85% accuracy in species classification and 87.31% spatial localization accuracy under ambient conditions, where each prediction only takes 3–7 s sensor inputs. We further demonstrate the potential of our approach through real-world deployment on a mobile robot in a residential environment. Our results establish a scalable and practical framework for autonomous real-time fungal monitoring that bridges chemical perception and spatial inference, paving a way to advance the state of the art in environmental sensing, health diagnostics, and autonomous robotic inspection.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有室内真菌检测方法速度慢、成本高且缺乏空间分辨率。  \n2. 传统方法无法实时监测和定位真菌源，影响及时干预和有效减轻风险。  \n3. 现有技术无法同时识别真菌种类和其空间分布。  \n\n【用了什么创新方法】  \n我们提出了Scensory，一个机器人辅助的化学传感系统，能够同时进行真菌种类识别和源定位。该系统利用经济实惠的挥发性有机化合物（VOC）传感器阵列和深度学习，解码复杂的VOC动态信号。通过自动化的高通量数据收集，我们的系统在五种真菌物种上实现了高达89.85%的种类分类准确率和87.31%的空间定位准确率。该方法在真实环境中展示了其可扩展性和实用性，为自主实时真菌监测奠定了基础。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        }
    ]
}