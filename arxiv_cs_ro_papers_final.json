{
    "2025-09-24": [
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies",
            "authors": "Lars Ankile,Zhenyu Jiang,Rocky Duan,Guanya Shi,Pieter Abbeel,Anusha Nagabandi",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19301",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19301",
            "arxiv_html_link": "https://arxiv.org/html/2509.19301v1",
            "abstract": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor control policies. However, these approaches are limited by the quality of human demonstrations, the manual effort required for data collection, and the diminishing returns from increasing offline data.\nIn comparison, reinforcement learning (RL) trains an agent through autonomous interaction with the environment and has shown remarkable success in various domains. Still, training RL policies directly on real-world robots remains challenging due to sample inefficiency, safety concerns, and the difficulty of learning from sparse rewards for long-horizon tasks, especially for high-degree-of-freedom (DoF) systems.\nWe present a recipe that combines the benefits of BC and RL through a residual learning framework. Our approach leverages BC policies as black-box bases and learns lightweight per-step residual corrections via sample-efficient off-policy RL.\nWe demonstrate that our method requires only sparse binary reward signals and can effectively improve manipulation policies on high-degree-of-freedom (DoF) systems in both simulation and the real world. In particular, we demonstrate, to the best of our knowledge, the first successful real-world RL training on a humanoid robot with dexterous hands.\nOur results demonstrate state-of-the-art performance in various vision-based tasks, pointing towards a practical pathway for deploying RL in the real world.",
            "introduction": "Enabling robots to learn and improve directly in their deployment environments remains a fundamental challenge in robotics. Recently, significant progress has been made in training visuomotor control policies in the real world with behavior cloning (BC) from human demonstrations [1, 2, 3, 4, 5, 6, 7, 8, 9]. However, this success requires significant infrastructure, as well as numerous hours of manual and cumbersome data collection.\nEven if unlimited data could be collected for every task, not only is human teleoperator performance generally suboptimal, but there is also emerging evidence that policy performance saturates with increasing demonstrations [10, 6, 11, 12, 13].\n\nReinforcement learning (RL) offers a complementary paradigm: agents learn autonomously through trial and error in the environment. Deep RL has shown great success in various domains [14, 15, 16, 17, 18, 19, 20, 21], including in-hand manipulation [22, 23] and locomotion [24, 25, 26, 27]. However, strong RL performance generally requires large amounts of data from online interactions, so its application has been mainly in simulation [28, 29] since real-world data are expensive and potentially unsafe to gather in large amounts.\n\nA natural direction to improve BC policies is to leverage online RL [12, 30, 31, 32], combining the strengths of each: BC policies provide a strong prior that can regularize exploration in the RL process, while online RL enhances policy performance by learning from interactions with the environment. However, modern BC architectures are typically deep models with tens of millions to billions of parameters that utilize action chunking or diffusion-based approaches, which can make it challenging to apply RL methods directly to optimize the policy. A simple yet powerful recipe that avoids several of the above issues is residual RL [33, 34, 35, 36, 12, 32, 31], where RL is applied not to learn a full policy, but only to learn corrective terms on top of a fixed base controller. Previous work has demonstrated that residual RL can indeed enhance the reliability of a pre-trained policy. Still, it has so far been limited to learning in simulation [12, 32, 31] or demonstrating results in simple or constrained settings [33, 34, 35, 36]; Applications to high-DoF systems learning directly in the real world are still lacking.\n\nIn this work, we present an off-policy residual fine-tuning (ResFiT) approach that utilizes online RL to enhance BC policies. By treating the base policy as a black box and learning a per-step residual correction that is independent of chunk size and policy parameterization, we sidestep the challenges of directly optimizing huge base policies. By carefully designing our off-policy recipe, we make the RL process sample efficient enough to scale to high-DoF bimanual systems, require only sparse binary reward signals, and be safe enough to deploy in the real world. We demonstrate robust performance on sparse-reward, long-horizon, vision-based tasks, showing that our approach achieves state-of-the-art performance for a range of tasks in simulation. We also investigate each design decision in our recipe. To the best of our knowledge, we provide the first demonstration of RL on a humanoid robot with five-fingered hands, trained entirely in the real world.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在保留行为克隆（BC）基线的同时通过离线/在线强化学习实现对复杂高自由度系统的高效微调与改进\n2. 如何实现对高DoF系统的安全、样本高效的Residual RL，以最少数据和稀疏奖励实现现实世界鲁棒性\n3. 如何在不直接优化大型基准策略的前提下，对其进行逐步残差修正以提升 visuomotor 控制性能\n4. 如何将离线BC与在线RL结合，解决数据质量、数据收集成本、以及现实世界训练的安全性挑战\n5. 如何在真实 humanoid/五指手等高复杂度机械臂上实现端到端的实证RL训练与评估\n\n【用了什么创新的方案】\n核心解决方案：提出离线行为克隆基线上的残差强化学习（ResFiT），将基线策略视为黑盒，通过学习逐步的每步残差信号来实现修正；使用面向离线/在线混合的高效RL配方，确保在高DoF、稀疏二值奖励下也能实现样本友好、真实世界的训练，并在仿真与现实中获得前所未有的鲁棒性与性能提升；通过残差学习避免直接优化庞大基线策略，降低参数化、数据需求和安全风险；首次在真实 humanoid/五指手上实现全现实世界的RL微调。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration",
            "authors": "Yang Jin,Jun Lv,Han Xue,Wendi Chen,Chuan Wen,Cewu Lu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19292",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19292",
            "arxiv_html_link": "https://arxiv.org/html/2509.19292v1",
            "abstract": "Intelligent agents progress by continually refining their capabilities through actively exploring environments. Yet robot policies often lack sufficient exploration capability due to action mode collapse. Existing methods that encourage exploration typically rely on random perturbations, which are unsafe and induce unstable, erratic behaviors, thereby limiting their effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a framework that enhances policy exploration and improvement in robotic manipulation. SOE learns a compact latent representation of task-relevant factors and constrains exploration to the manifold of valid actions, ensuring safety, diversity, and effectiveness. It can be seamlessly integrated with arbitrary policy models as a plug-in module, augmenting exploration without degrading the base policy performance. Moreover, the structured latent space enables human-guided exploration, further improving efficiency and controllability. Extensive experiments in both simulation and real-world tasks demonstrate that SOE consistently outperforms prior methods, achieving higher task success rates, smoother and safer exploration, and superior sample efficiency. These results establish on-manifold exploration as a principled approach to sample-efficient policy self-improvement.",
            "introduction": "“We want AI agents that can discover like we can, not which contain what we have discovered.”\n            — Richard Sutton, The Bitter Lesson\n\nIn recent years, data-driven robot learning [10, 53, 5, 7, 25] has attracted considerable attention, particularly for its potential to enhance robotic manipulation capabilities through large-scale data collection and training. By modeling visuomotor behaviors with neural networks, these approaches allow robot policies to learn from expert demonstrations and achieve near-human performance across a variety of tasks.\n\nDespite these advances, most existing methods still rely heavily on human teleoperation for data acquisition [53, 13] and policy refinement [30, 31], which presents several challenges. A primary concern is the high cost of teleoperation, as it typically requires skilled operators and specialized equipment, thereby limiting the scalability of data collection. More critically, teleoperated demonstrations often fail to cover the diverse scenarios a robot could encounter in the real world, resulting in distributional bias [52] and compounding error [39]. The problem is further exacerbated by the fact that human operators may act based on contextual cues inaccessible to robot sensors. Robots, on the other hand, may internalize human habits rather than task-relevant behaviors. As a result, simply scaling up teleoperated data is not the optimal path toward improving policy performance.\n\nInstead of passively imitating human-provided behavior, a line of research addresses this challenge by enabling robot policy self-improvement [6, 23, 35, 32]—actively exploring the environment to collect diverse experience and leveraging that experience to refine policies. Under this paradigm, robots can autonomously discover novel behaviors that go beyond the coverage of human demonstrations. By iteratively practicing the learned behaviors, they also develop a deeper understanding of the natural variability in their actions, ultimately leading to a more robust and resilient policy.\n\nThe key to sample-efficient robot policy self-improvement lies in effective exploration. Prior work [3, 23] has shown that imitation-learned policies often overfit demonstrations, collapse into single-modal motions, and fail to produce diverse behaviors. Without proper exploration, these policies tend to repeat failed behaviors, limiting their ability to discover improved solutions. While random exploration strategies can occasionally yield novel behaviors [29], they are generally ineffective in high-dimensional action spaces [28] and can pose safety risks in real-world deployment [16], causing potential hardware damage. This necessitates a more structured approach to exploration—one that ensures safety and effectiveness without sacrificing the diversity of experiences.\n\nTo this end, we propose SOE, a novel framework for Sample-Efficient Robot Policy Self-improvement via On-Manifold Exploration. The core idea of our method is to ensure that exploration remains constrained to the manifold of valid actions—critical for both safety and effectiveness. Prior works often perturb the action space directly [29] or inject random noise [23], leading to temporally inconsistent and unsafe behaviors, particularly under “action chunking” representations [53]. In contrast, we perform exploration in a compact latent space learned through a variational information bottleneck (VIB). The latent representation in this space preserves only task-essential information in observation while discarding irrelevant details, ensuring exploration remains structured and efficient. As illustrated in Fig. 1, by operating on this latent representation, our framework enables effective on-manifold exploration and more robust policy improvement. Furthermore, we demonstrate that in the latent space, action chunks are naturally disentangled into distinct modes. Leveraging this property, we achieve controllable exploration, which allows users to guide exploration toward preferred directions, thereby enhancing interpretability and further boosting sample efficiency. Implemented as a plug-in module, our approach can be seamlessly integrated with existing imitation learning algorithms and jointly optimized, without any degradation in their performance.\n\nTo evaluate the effectiveness of our method, we conduct extensive experiments across a variety of robot manipulation tasks in both simulation and real world. The results show that SOE consistently outperforms prior exploration methods in effectiveness, motion smoothness, and sample efficiency. With just one round of policy self-improvement, our method achieves substantial gains over the base policy, including an average relative improvement of 50.8% on real-world tasks. Additional experiments in simulation and ablation studies further confirm multi-round performance improvements and the contribution of each component in our framework. Collectively, these findings demonstrate that on-manifold exploration provides a structured, safe, and effective approach to sample-efficient robot policy self-improvement.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在高维机器人动作空间中实现安全、有效且样本高效的策略自我改进中的探索？\n2. 如何通过潜在表征约束探索到行动流形以避免无效或危险的随机扰动？\n3. 如何实现对探索的可控性与可解释性，同时保持对既有模仿学习的无干扰性？\n4. 如何在仿真与真实世界任务中提升任务成功率、平滑性和样本效率？\n\n【用了什么创新的方案】\nSOE在任务相关因素的紧凑潜在表示上进行探索，利用变分信息瓶颈学习一个仅保留任务本质信息的潜在空间，并在该潜在流形上进行探索以保持动作的有效性和安全性。它将探索与现有策略模型无缝对接，可作为插件模块嵌入到任意模仿学习框架中；潜在空间还使动作块在不同模式上解耦，从而实现可控探索，并支持人工引导以提高效率。实验结果表明在仿真和真实任务中，SOE实现更高的成功率、更平滑且更安全的探索以及更强的样本效率。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces",
            "authors": "Kuanqi Cai,Chunfeng Wang,Zeqi Li,Haowen Yao,Weinan Chen,Luis Figueredo,Aude Billard,Arash Ajoudani",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19261",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19261",
            "arxiv_html_link": "https://arxiv.org/html/2509.19261v1",
            "abstract": "Robotic manipulation in dynamic environments often requires seamless transitions between different grasp types to maintain stability and efficiency. However, achieving smooth and adaptive grasp transitions remains a challenge, particularly when dealing with external forces and complex motion constraints. Existing grasp transition strategies often fail to account for varying external forces and do not optimize motion performance effectively. In this work, we propose an Imitation-Guided Bimanual Planning Framework that integrates efficient grasp transition strategies and motion performance optimization to enhance stability and dexterity in robotic manipulation. Our approach introduces Strategies for Sampling Stable Intersections in Grasp Manifolds for seamless transitions between uni-manual and bi-manual grasps, reducing computational costs and regrasping inefficiencies. Additionally, a Hierarchical Dual-Stage Motion Architecture combines an Imitation Learning-based Global Path Generator with a Quadratic Programming-driven Local Planner to ensure real-time motion feasibility, obstacle avoidance, and superior manipulability. The proposed method is evaluated through a series of force-intensive tasks, demonstrating significant improvements in grasp transition efficiency and motion performance.\nA video demonstrating our simulation results can be viewed at https://youtu.be/3DhbUsv4eDo.",
            "introduction": "Robotic manipulation in dynamic forceful operations—such as collaborative cutting or drilling—demands real-time adaptation to varying external forces that critically affect grasp stability. Consider a human-robot woodworking scenario (Fig. 1) where the robot must continuously adjust between uni-manual and bi-manual grasps to counteract changing cutting and drilling forces. This fundamental requirement exposes two unresolved challenges in existing methods: efficient grasp transitions by minimizing execution time and arm movement and motion performance awareness, as crucial metrics like manipulability and joint limits essential for control safety are often overlooked. To bridge this gap, we propose an imitation-guided planning framework that integrates efficient grasp transitions with motion performance constraints, ensuring both stability and dexterity in forceful tasks.\n\nMulti-step manipulation planners have long tackled regrasping and grasping transitions [1]. Traditional grasping involves transporting an object by repeatedly releasing and regrasping it as needed [2]. Conventional regrasp planners rely on a supporting surface for single-arm manipulation [3, 4], while recent research extends these strategies to dual-arm scenarios [5, 6, 7].\nHowever, existing methods do not explicitly account for dynamic external forces, which vary over time, nor do they optimize regrasp transitions during forceful interactions. Studies in forceful human-robot collaboration [8, 9, 10] focus on regulating contact forces but assume a fixed or pre-determined grasp. The key challenge remains: determining where and how a robot should grasp for stability and when to transition seamlessly under complex external forces. Recent works [11, 12, 13] have made progress, but achieving stable grasps that withstand varying forces while ensuring efficient planning, manipulability, and dexterity remains difficult.\nThis paper addresses two key challenges in forceful robotic manipulation: efficient grasp transitions and motion performance optimization, proposing a novel framework to overcome them.\n\nEfficient Grasp Transitions. Previous methods [11, 13] mostly rely on random sampling-based planners for grasp transitions, often resulting in high computational costs and unstable changes. To reduce task execution time and minimize the movement distance of the robot arm, we introduce Strategies for Sampling Stable Intersections in Grasp Manifolds for seamless uni-manual and bi-manual transitions. Our Directional Gradient-Based Resampling locally adjusts the unimanual manipulator along the negative gradient, ensuring stability while maintaining a secure unimanual grasp and minimizing movement. For tasks with multiple grasp changes, Multi-Grasp Transition Check (MTC) identifies a shared intermediate configuration, reducing redundant regrasping. To further boost efficiency, we propose a Hierarchical Dual-Stage Motion Architecture, combining an Imitation Learning-based Global Path Generator with a QP-driven local planner for real-time motion optimization and obstacle avoidance, enabling faster collision-free path generation than sampling-based methods.\n\nMotion Performance Optimization.\nIn forceful operations, robotic stability is determined by three motion performance factors: manipulability, dexterity, and joint limits, which ensure kinematic feasibility under varying forces. However, many recent methods [14, 15] tend to overlook these critical aspects.\nOur framework optimizes motion performance at both the grasp configuration level and during execution. To enhance manipulability and avoid kinematic limitations, we introduce a Motion Performance Map that encodes feasibility, manipulability, and joint limit proximity. This map guides grasp sampling toward optimal workspace regions, improving selection efficiency. During execution, we enforce manipulability constraints within the QP framework, ensuring control authority over the end-effector while avoiding singularities and joint limits. This real-time optimization enables stable, collision-free motion.\n\nBy integrating these advancements, our framework enhances grasp transition efficiency while ensuring superior motion performance in both grasp selection and execution, effectively addressing key limitations in existing forceful robotic planning approaches.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在动态外部力下实现稳定而高效的双手抓持与无缝转换（uni-manual到bi-manual）？\n2. 如何在抓取转移中兼顾运动性能（可控性、可控度、关节极限）并降低计算成本？\n3. 如何在强力操作场景中实时规划可行路径并避免碰撞，同时优化抓取点选择？\n\n【用了什么创新的方案】\n策略性在抓取流形中采样稳定交点以实现单臂到双臂的平滑转变，并用方向梯度重采样局部调整单臂抓持以保持稳定性。提出多抓取转变检查以寻找共用中间配置，降低冗余重新抓取。建立分层双阶段运动架构：基于模仿学习的全局路径生成器+QP驱动的局部规划器实现实时、避障且高可 manipulability 的路径规划。引入运动性能地图（可行性、可操作性、关节极限接近度）以引导抓取选择并在执行中通过QP约束实现对端执行器的控制权和避免奇异点。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Proactive-reactive detection and mitigation of intermittent faults in robot swarms",
            "authors": "Sinan Oğuz,Emanuele Garone,Marco Dorigo,Mary Katherine Heinrich",
            "subjects": "Robotics (cs.RO); Multiagent Systems (cs.MA); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19246",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19246",
            "arxiv_html_link": "https://arxiv.org/html/2509.19246v1",
            "abstract": "Intermittent faults are transient errors that sporadically appear and disappear. Although intermittent faults pose substantial challenges to reliability and coordination, existing studies of fault tolerance in robot swarms focus instead on permanent faults. One reason for this is that intermittent faults are prohibitively difficult to detect in the fully self-organized ad-hoc networks typical of robot swarms, as their network topologies are transient and often unpredictable. However, in the recently introduced self-organizing nervous systems (SoNS) approach, robot swarms are able to self-organize persistent network structures for the first time, easing the problem of detecting intermittent faults. To address intermittent faults in robot swarms that have persistent networks, we propose a novel proactive–reactive strategy to detection and mitigation, based on self-organized backup layers and distributed consensus in a multiplex network. Proactively, the robots self-organize dynamic backup paths before faults occur, adapting to changes in the primary network topology and the robots’ relative positions. Reactively, robots use one-shot likelihood ratio tests to compare information received along different paths in the multiplex network, enabling early fault detection. Upon detection, communication is temporarily rerouted in a self-organized way, until the detected fault resolves.\nWe validate the approach in representative scenarios of faulty positional data occurring during formation control, demonstrating that intermittent faults are prevented from disrupting convergence to desired formations, with high fault detection accuracy and low rates of false positives.",
            "introduction": "Reliability in networked systems requires consistently accurate information exchange among components, often under dynamic and uncertain conditions [1, 2]. If communication links fail or become unreliable during multi-hop communication, system convergence and performance guarantees can be compromised [3, 4, 5, 6]. In self-organized robot swarms, this challenge is exacerbated by asynchronous ad-hoc communication and decentralized coordination of actuation and decision making. Robots in a self-organized swarm rely solely on local information and communication with nearby robots, without any estimation of the global state of the swarm or its environment, often leading to prolonged convergence times and vulnerability to the spread of incorrect information [7]. Frequent communication between robots can cause faulty information to spread quickly and potentially degrade overall swarm performance or lead to permanent failures.\n\nSelf-organized robot swarms exhibit some inherent fault tolerance, through redundancy and a lack of single points of failure [8, 9].\nHowever, many fault types are not mitigated by this passive tolerance and instead require dedicated mechanisms for detection and mitigation [10, 11, 12, 13]. Somewhat counter-intuitively, self-organized robot swarms are inherently much more tolerant to complete robot failures than to partial ones [14]. For example, a single robot producing faulty or malicious information has been shown to be capable of severe disruption to overall swarm behavior [14, 15]. Faulty robots can also physically obstruct the rest of the swarm, and this interference can paradoxically be worsened by the redundancy that provides swarms with some types of inherent fault tolerance [16].\n\nOther faults to which self-organized robot swarms are vulnerable and which require dedicated mechanisms for detection and mitigation are intermittent faults (IFs). IFs are temporary faults that can appear, disappear, and reappear [17], potentially caused by communication interference, sensor malfunctions, or software bugs [18]. IFs are difficult to detect and diagnose due to their transience [19] and can cause significant disruptions without leaving an easily detectable trace [18].\nA representative example involves intermittent GPS signal degradation in cluttered environments, which can induce sporadic localization errors. These errors propagate through decentralized state estimation protocols, gradually undermining coordination mechanisms without generating explicit failure indicators.\nIn real applications, e.g., in robot swarms deployed in inaccessible or dangerous environments [20, 12], the consequences of IFs to mission performance and to safety can be severe and in some cases could be irreversible. Detecting and resolving IFs before they escalate is key to minimizing disruption: early detection can prevent cascading failures leading to erroneous execution of tasks and can prevent culmination in permanent failures, either of individual robots or the swarm as a whole [21].\n\nIFs are difficult to detect in robot swarms with fully self-organized ad-hoc networks, because the network topology is transient and often unpredictable. IFs are much more straightforward to detect in fully centralized systems and in networks with static structures, for example in sensor networks [22, 23, 24]. However, for multi-robot systems, full centralization and fully static networks also present downsides, such as single points of failure and limited scalability.\n\nOur recently introduced self-organizing nervous systems (SoNS) [25] approach combines aspects of centralization and decentralization through self-organized hierarchy. Using the SoNS approach, robot swarms are coordinated via temporary logical networks that are hierarchical and culminate in a dynamic “brain” robot (i.e., leader), but which are not imposed from the outside, being instead established and maintained in a self-organized manner. This provides robot swarms with persistent and predictable network structures that are more amenable to detecting IFs, without introducing any single points of failure.\nIn short, the SoNS approach allows, for the first time, to apply centralized fault detection and mitigation strategies to robot swarms without sacrificing their oft-cited benefits of scalability, flexibility, and a lack of single points of failure.\n\nSwarm robotics usually studies passive tolerance to permanent faults [26]—that is, faults such as electromechanical failures that will remain unless they are actively repaired.\nWhen relying on passive fault tolerance, studies have usually demonstrated that a swarm continues its mission after some or many robots have failed, either by continuing with fewer robots  [10, 27, 28] or by replacing/repairing the failed robots without pausing the mission [29, 30, 31, 25].\n\nSwarm robotics studies that focus specifically on fault tolerance do not typically rely on passive tolerance, instead developing dedicated mechanisms to handle permanent faults.\nThe majority of these methods detect and react to permanent electromechanical failures after they have occurred [21, 32], often relying on time-out mechanisms in which a robot is considered non-operational if it does not respond to a message within a certain time. Existing methods for detecting permanent faults include LED synchronization [29], simulation comparison [33], shared sensor data analysis [28], and behavioral feature vectors (BFVs) [11].\nThese methods often focus on detection, assuming that once a fault is detected, a repair or other intervention is possible during normal operation (e.g., [34, 29, 21]). Although such repairs might be unrealistic in inaccessible, hazardous, or congested environments [35, 21, 12], future methods for autonomous repair could be developed to complement detection. In short, the existing reactive methods can be considered effective for many types of permanent faults [10].\nHowever, the above-mentioned detection approaches are unlikely to be applicable to the transience of IFs and their long response times [36] would likely be too slow for the early detection and recovery that IFs require. Methods to detect and repair IFs in robot swarms still need to be developed.\n\nTo the best of our knowledge, there are no existing swarm robotics methods focused on IF detection and recovery. Strategies developed for IFs in other types of systems, such as model-based analysis (e.g., discrete-event-system models [37], causal models [38]) and quantitative analysis (e.g., parameter estimation [39], geometric approaches [40], Kalman-like filtering [41]), provide valuable insights but primarily target single-unit systems with static and known system models [24, 42], which is incompatible with self-organized systems such as robot swarms. Likewise, IF strategies developed for sensor networks [22, 23] typically use fully centralized architectures to correct information transmission and reception [24], and are therefore incompatible with self-organized systems.\n\nFurthermore, although fully centralized monitoring is highly effective for detecting and correcting IFs, it can present problems of inflexibility, limited scalability, and single points of failure (e.g., at the point where monitoring is centralized). Fully self-organized approaches, by contrast, would be highly flexible and offer greater scalability and a lack of single points of failure, but would present problems of limited accuracy and potentially slow reaction times.\nIn this paper, we aim to combine elements of each system type to get the benefits of both. Using our proposed proactive–reactive approach, robots can monitor each other using self-organizing hierarchy, detecting IFs accurately and remedying them proactively.\n\nTo demonstrate our proposed proactive–reactive approach, we use the SoNS concept of self-organizing hierarchy in a robot swarm, which has been shown to incorporate temporarily centralized structures into an otherwise self-organized robot swarm without introducing single points of failure or inherently limiting scalability [25, 30, 43, 44, 45, 46]. We build on our recent theoretical foundations for self-organizing hierarchical frameworks: hierarchical Henneberg construction (HHC) [47]. In our previous work [47], we demonstrated HHC for key self-reconfiguration problems (framework merging, robot departure, and framework splitting), derived the mathematical conditions of these problems, and developed algorithms that preserve rigidity and hierarchy using only local information.\n\nIn the remainder of this paper, we assume all graphs are constructed using these already demonstrated HHC algorithms, and refer to such graphs as HHC-constructed graphs. See Appendix A for details on how HHC and SoNS are related.\n\nIn fault tolerance for multi-robot systems, both proactive and reactive mechanisms are important [48].\nIn this paper, we propose a novel proactive–reactive method to detect and mitigate IFs in robot swarms.\nIn the proposed proactive–reactive method, the robots first use distributed consensus to preemptively self-organize dynamic backup communication paths before IFs are detected. Then, the robots compare information received via primary and backup paths to detect IFs, using a one-shot likelihood ratio test. When IFs are detected, the robots react by rerouting communication through the dynamic backup paths. In this paper, we apply the proposed proactive–reactive method to a scenario of intermittently faulty relative positional information within multi-robot formations that have a hierarchical structure towards a fault-free leader, and demonstrate that the method mitigates IFs and robots are able to continue with the desired formations.\n\nThe main technical contributions of this paper can be summarized as follows:\n\nWe address a current gap in robot swarm networking, specifically how to establish back-up communication paths for leader–follower formation control in a self-organized robot swarm. We address this gap by extending the biased minimum consensus (BMC) [49] protocol for shortest path planning in static graphs. We introduce the adaptive biased minimum consensus (ABMC) protocol for dynamic graphs—addressing time-varying topologies, node neighborhoods, and costs. We demonstrate that our ABMC protocol addresses the minimum-cost path problem, with two objectives integrated into a single cost function: to minimize the number of hops to the destination (the leader robot) and to minimize the degree of network congestion (by minimizing the occurrence of parallel edges).We provide the mathematical properties and stability analysis of the ABMC protocol as a distributed consensus mechanism in dynamic graphs with piecewise constancy, including providing the necessary and sufficient conditions to uniquely determine an equilibrium point representing a minimum-cost backup path.\n\nWe address a current gap in robot swarm fault tolerance, specifically tolerance against intermittent faults (IFs). We address this gap by proposing a novel proactive–reactive fault-tolerance strategy for detection and mitigation of IFs in robot swarms. Our proposed strategy uses the ABMC protocol to construct backup network layers and combines it with a distributed likelihood ratio (LR) protocol to dynamically reroute traffic in the constructed multiplex network. We propose the mathematical conditions and design the distributed algorithms for backup layer construction and for execution of the proactive–reactive strategy for IF detection and mitigation. We also provide the time and space complexity and efficiency properties of both distributed algorithms. Finally, we demonstrate the proactive–reactive fault-tolerance strategy in formations of 20 robots with moving leaders.\n\nThe rest of the paper is organized as follows. In Sec. II, the foundational concepts regarding hierarchical frameworks are presented, along with the existing BMC protocol. In Sec. III, we formulate three key problems addressed in this paper: construction of dynamic minimum-cost backup paths, detection of IFs using the constructed backup paths, and mitigation of the detected IFs using the constructed backup paths. The first problem is addressed in Secs. IV and V, and the second and third problem are addressed in Sec. VI. Finally, in Sec. VII we validate our contributions in experiments of representative scenarios. The conclusions are summarized in Sec. VIII.",
            "llm_summary": "【关注的是什么问题】\n1. Intermittent faults (IFs) in self-organized robot swarms and their impact on formation convergence and reliability\n2. 如何在自组织机器人群体中检测与缓解IFs，兼顾可扩展性与无单点故障\n3. 在动态拓扑中实现备份路径的自组织构建与快速故障检测\n4. 将集中式检测策略与去中心化群体控制结合的主动-被动（proactive–reactive）两层防护\n\n【用了什么创新的方案】\n核心解决方案：提出主动-被动（proactive–reactive）IF检测与缓解框架，利用自组织层次化的备份通信路径实现多路径冗余；通过自组织的ABMC协议在动态图构建最小代价备份路径，并使用分布式一次性似然比检验比较主路径与备份路径的信息以实现早期IF检测；IF检测后，故事通路通过自组织方式重新路由通信直至故障消失；在20机器人队形实验中验证方法提升鲁棒性与收敛性。并将ABMC与LR检验结合，给出时空复杂度分析与稳定性条件。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap",
            "authors": "Tianyu Wu,Xudong Han,Haoran Sun,Zishang Zhang,Bangchao Huang,Chaoyang Song,Fang Wan",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 4 figures, accepted to Data@CoRL2025 Workshop",
            "pdf_link": "https://arxiv.org/pdf/2509.19169",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19169",
            "arxiv_html_link": "https://arxiv.org/html/2509.19169v1",
            "abstract": "The transfer of manipulation skills from human demonstration to robotic execution is often hindered by a “domain gap” in sensing and morphology. This paper introduces MagiClaw, a versatile two-finger end-effector designed to bridge this gap. MagiClaw functions interchangeably as both a handheld tool for intuitive data collection and a robotic end-effector for policy deployment, ensuring hardware consistency and reliability. Each finger incorporates a Soft Polyhedral Network (SPN) with an embedded camera, enabling vision-based estimation of 6-DoF forces and contact deformation. This proprioceptive data is fused with exteroceptive environmental sensing from an integrated iPhone, which provides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS application, MagiClaw streams synchronized, multi-modal data for real-time teleoperation, offline policy learning, and immersive control via mixed-reality interfaces. We demonstrate how this unified system architecture lowers the barrier to collecting high-fidelity, contact-rich datasets and accelerates the development of generalizable manipulation policies. Please refer to the iOS app at https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.",
            "introduction": "The success of modern robot learning paradigms, from Learning from Demonstration (LfD) [1, 2] to offline reinforcement learning, is fundamentally dependent on the quality and richness of the underlying data [3]. For contact-rich manipulation tasks, robust policies require more than just kinematic trajectories; they demand a holistic understanding of interaction forces, tactile feedback, and environmental context [4, 5]. Consider a human deftly handling a delicate object: the action is a symphony of precise motion, modulated forces, and continuous tactile adjustments [6]. Replicating such skills requires capturing this multi-modal information stream in its entirety.\n\nHowever, existing data collection methodologies present significant challenges. First, they often rely on a patchwork of disparate, expensive sensors—such as external motion capture systems, wrist-mounted force/torque sensors, and complex tactile skins [7, 8]—resulting in cumbersome and costly setups. This high barrier to entry limits the scale and diversity of data collection efforts [9]. Second, and more critically, a persistent domain gap exists between the human demonstrator and the robotic learner [10]. Data is often collected using one set of hardware (e.g., an instrumented glove) and deployed on a robot with entirely different sensor suites and end-effector morphology. This mismatch necessitates complex domain adaptation techniques and is a primary reason why policies trained on demonstration data often fail to generalize to physical hardware [11].\n\nTo address these challenges, we present MagiClaw, a unified hardware platform designed to seamlessly bridge the gap from human demonstration to robotic deployment. MagiClaw is a dual-purpose, two-fingered gripper that merges three key innovations:\n\nUnified Hardware Form Factor: The exact same MagiClaw device can be used as a hand-held tool for human demonstration or mounted on a robot arm for autonomous execution. This hardware consistency minimizes the sensor and morphological domain gap, facilitating direct policy transfer.\n\nVision-Based Proprioceptive Fingertips: Each finger integrates a Soft Polyhedral Network (SPN) [12] with an embedded miniature camera. This novel design enables visuotactile perception, inferring 6-DoF forces, torque, and high-resolution contact deformation from the distortion of the internal lattice structure, thereby obviating the need for costly external force sensors.\n\nIntegrated Multi-Modal Exteroception: An attached iPhone leverages its powerful sensor suite (LiDAR, RGB cameras, IMU) and ARKit framework [13] to provide synchronized, rich environmental context, including gripper pose, depth maps, and high-resolution video.\n\nOur primary contribution is an integrated system that fundamentally streamlines the collection of holistic, contact-centric data for robot learning. By fusing proprioceptive force/tactile data from the fingertips with exteroceptive visual and spatial data from a commodity smartphone, MagiClaw offers a low-cost, powerful, and user-friendly solution for both teleoperation and autonomous policy development. We posit that by democratizing access to such high-fidelity, multi-modal data, MagiClaw can serve as a catalyst for developing more robust and generalizable manipulation skills, advancing the pursuit of universal action embodiment in robotics.",
            "llm_summary": "【关注的是什么问题】\n1. 领域差距导致的人类示范到机器人执行的迁移困难（≤40词）\n2. 需要高质量多模态数据但现有传感器系统昂贵、零散、耦合度高的问题（≤40词）\n3. 如何在同一硬件上实现手持示范与机器人执行之间的无缝切换（≤40词）\n4. 如何通过 visuotactile 与环境感知实现对力、接触变形的高保真推断（≤40词）\n5. 如何降低数据采集成本并提高数据多样性以提升策略泛化能力（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：MagiClaw 将两指软聚合网格内嵌相机实现视觉-本体感知的六自由度力与接触变形估计；每指作為 SPN 传感器；通过 iPhone 的 LiDAR、RGB、深度数据进行同步的外感知；手持与机器人端可互换的统一硬件形式；自带 iOS 应用实现实时远控、离线学习数据流；降低数据采集成本、提升多模态数据质量与一致性。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination",
            "authors": "Mark Gonzales,Ethan Oh,Joseph Moore",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 Pages, 7 Figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19168",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19168",
            "arxiv_html_link": "https://arxiv.org/html/2509.19168v1",
            "abstract": "In this paper, we present a receding-horizon, sampling-based planner capable of reasoning over multimodal policy distributions. By using the cross-entropy method to optimize a multimodal policy under a common cost function, our approach increases robustness against local minima and promotes effective exploration of the solution space. We show that our approach naturally extends to multi-robot collision-free planning, enables agents to share diverse candidate policies to avoid deadlocks, and allows teams to minimize a global objective without incurring the computational complexity of centralized optimization. Numerical simulations demonstrate that employing multiple modes significantly improves success rates in trap environments and in multi-robot collision avoidance. Hardware experiments further validate the approach’s real-time feasibility and practical performance.",
            "introduction": "Local minima pose a fundamental challenge for finite-horizon, gradient-based planning approaches. In multi-robot scenarios, local minima can arise not only from the environment but also from dynamic factors, such as the changing trajectories of teammates, which may inadvertently block or cut off routes that would otherwise be viable. These pitfalls often cause robots to become stuck, find suboptimal solutions, or fail to coordinate effectively in complex environments.\n\nSampling-based planners, such as Model Predictive Path Integral (MPPI) [1] and Cross-Entropy Method (CEM) [2, 3], attempt to improve the trajectory cost by stochastically sampling and evaluating trajectories in the cost landscape. In practice, these methods utilize hyperparameters, such as sampling variance, number of samples, and the horizon length, to adapt the exploration to the environment. However, both MPPI and CEM typically sample trajectories around the prior best policy, leading to a concentration of samples in a narrow region of the solution space. This localized search impedes the planner’s ability to effectively navigate around traps or escape from local minima once they occur, especially in environments with challenging topology. As a result, the planner can become stuck in suboptimal regions, regardless of the variance or adaptation strategy.\n\nIn multi-robot systems, the difficulty is exacerbated by the need for robots to coordinate planned trajectories. Centralized control approaches [4, 5, 6, 7] can, in principle, achieve globally optimal coordination; however, they suffer from scalability issues and high computational costs as the team size increases. Distributed methods, while scalable, often require robots to individually select their optimal trajectory, subsequently negotiating with teammates to reach a feasible consensus. When each robot contributes only a single candidate trajectory, the team risks deadlock or persistent local minima, as a lack of trajectory diversity reduces the likelihood of discovering collision-free, cooperative maneuvers, especially when teammates dynamically update their plans or block each other’s routes in real-time.\n\nTo overcome these limitations, we introduce a multimodal sampling and clustering framework that maintains multiple policy candidates for each robot, thereby increasing diversity and robustness against local minima in both environmental and collaborative planning contexts.\n\nOur contributions are:\n\nA cross-entropy planning approach capable of preserving multiple policy modes for increased planning robustness.\n\nA multi-robot coordination framework that enables reasoning about sets of candidate policies to avoid local minima and deadlocks more reliably.\n\n1. A cross-entropy planning approach capable of preserving multiple policy modes for increased planning robustness.\n\n2. A multi-robot coordination framework that enables reasoning about sets of candidate policies to avoid local minima and deadlocks more reliably.",
            "llm_summary": "【关注的是什么问题】\n1. 本地最优陷阱与多模态政策在单机器人导航中的鲁棒性（≤40词）\n2. 多机器人协同中的死锁、协调与全局目标优化的可扩展性（≤40词）\n3. 受限搜索空间中多模态策略对避障与探索的提升（≤40词）\n\n【用了什么创新的方案】\n- 引入跨熵法的多模态策略规划，保留多条候选策略以提升对局部极小值的鲁棒性。\n- 基于采样与聚类的框架，在多机器人场景中共享多策略集合，避免死锁并实现分布式协同。\n- 通过 receding-horizon 与多模态分布采样实现对全局成本的高效优化，降低集中化计算开销。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer",
            "authors": "Kangmin Kim,Seunghyeok Back,Geonhyup Lee,Sangbeom Lee,Sangjun Noh,Kyoobin Lee",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 5 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19142",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19142",
            "arxiv_html_link": "https://arxiv.org/html/2509.19142v1",
            "abstract": "Bimanual grasping is essential for robots to handle large and complex objects. However, existing methods either focus solely on single-arm grasping or employ separate grasp generation and bimanual evaluation stages, leading to coordination problems including collision risks and unbalanced force distribution. To address these limitations, we propose BiGraspFormer, a unified end-to-end transformer framework that directly generates coordinated bimanual grasps from object point clouds. Our key idea is the Single-Guided Bimanual (SGB) strategy, which first generates diverse single grasp candidates using a transformer decoder, then leverages their learned features through specialized attention mechanisms to jointly predict bimanual poses and quality scores. This conditioning strategy reduces the complexity of the 12-DoF search space while ensuring coordinated bimanual manipulation. Comprehensive simulation experiments and real-world validation demonstrate that BiGraspFormer consistently outperforms existing methods while maintaining efficient inference speed (<<0.05s), confirming the effectiveness of our framework. Code and supplementary materials are available at https://sites.google.com/bigraspformer",
            "introduction": "Bimanual grasping enables robots to manipulate large, heavy, or unwieldy objects beyond single-arm capabilities, making it essential for tasks such as lifting furniture, carrying long boards, or moving large boxes [1, 2]. However, most robotic grasping research has focused on single-arm systems, primarily on learning to detect 6-DoF grasp poses from point clouds [3, 4, 5, 6, 7, 8]. While effective for single-arm tasks, these approaches cannot be directly extended to bimanual scenarios. First, bimanual grasping expands the action space to 12-DoF, doubling the computational complexity. Second, it introduces new challenges, including collision avoidance, balanced force/torque distribution, and dual-arm coordination for post-grasp manipulation.\n\nFor bimanual grasping, only a few methods have been proposed so far. The DA2 dataset [9] introduced the first benchmark by extending single-arm datasets [10, 11, 3] with dual-arm-specific metrics such as force closure, dexterity, and torque balance [9, 12]. However, most existing approaches adopt modular architectures that separate grasp generation and evaluation. For example, Dual-PointNetGPD [9] evaluates the quality of grasp pairs from given candidates, requiring external single-arm grasp generators. Similarly, CGDF [13] directly generates bimanual grasps but lacks integrated quality prediction, instead relying on additional scoring modules or heuristic pairing strategies [14, 15]. As a result, current methods yield limited diversity, poor coordination, and high computation due to modular pipelines.\n\nIn this paper, we propose BiGraspFormer, the first unified end-to-end framework that directly generates coordinated bimanual grasps from object point clouds (Fig. 1). The key insight is that single-grasp features can effectively guide bimanual grasp generation, rather than treating dual-arm coordination as two independent problems. BiGraspFormer introduces a novel Single-Guided Bimanual (SGB) strategy: it first generates diverse single-arm grasp candidates, then leverages their learned features through specialized attention mechanisms to jointly predict bimanual poses and quality scores. This unified approach eliminates separate modules and explicitly models coordination between grasps, enabling stable and efficient dual-arm manipulation. Comprehensive experiments in both simulation and real-world environments demonstrate that BiGraspFormer achieves superior success, diversity, and speed compared to existing methods.\n\nOur contributions are summarized as follows:\n\nWe propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\nWe propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\nWe introduce the Single-Guided Bimanual (SGB) strategy, which leverages single-arm grasp features to guide bimanual generation, reducing computational complexity and enhancing dual-arm coordination.\n\nWe achieve state-of-the-art bimanual grasping performance while maintaining fast inference suitable for real-world deployment.\n\n1. We propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\n2. We introduce the Single-Guided Bimanual (SGB) strategy, which leverages single-arm grasp features to guide bimanual generation, reducing computational complexity and enhancing dual-arm coordination.\n\n3. We achieve state-of-the-art bimanual grasping performance while maintaining fast inference suitable for real-world deployment.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在点云上直接生成协调的双臂抓取姿态（12-DoF）以实现稳定的双臂物体操作\n2. 现有方法的模块化导致双臂协作差、碰撞风险与力分配不均等问题\n3. 如何在端到端框架中有效建模双臂协调并提高推理速度\n\n【用了什么创新的方案】\nSingle-Guided Bimanual (SGB) 策略：先通过变换器解码器生成多样的单臂抓取候选，再利用这些单臂抓取的特征，通过专门的注意力机制共同预测双臂抓取的姿态与质量分数，从而端到端地产生协调的双臂抓取。该统一框架 eliminates 分离的抓取生成与评估模块，直接从对象点云进行协调抓取的生成与评分。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation",
            "authors": "Sarvesh Prajapati,Ananya Trivedi,Nathaniel Hanson,Bruce Maxwell,Taskin Padir",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 10 figures, submitted to Robotic Computing & Communication",
            "pdf_link": "https://arxiv.org/pdf/2509.19105",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19105",
            "arxiv_html_link": "https://arxiv.org/html/2509.19105v1",
            "abstract": "Successful navigation in outdoor environments requires accurate prediction of the physical interactions between the robot and the terrain. To this end, several methods rely on geometric or semantic labels to classify traversable surfaces. However, such labels cannot distinguish visually similar surfaces that differ in material properties. Spectral sensors enable inference of material composition from surface reflectance measured across multiple wavelength bands. Although spectral sensing is gaining traction in robotics, widespread deployment remains constrained by the need for custom hardware integration, high sensor costs, and compute-intensive processing pipelines. In this paper, we present RGB Image to Spectral Signature Neural Network (RS-Net), a deep neural network designed to bridge the gap between the accessibility of RGB sensing and the rich material information provided by spectral data. RS-Net predicts spectral signatures from RGB patches, which we map to terrain labels and friction coefficients. The resulting terrain classifications are integrated into a sampling-based motion planner for a wheeled robot operating in outdoor environments. Likewise, the friction estimates are incorporated into a contact-force–based MPC for a quadruped robot navigating slippery surfaces. Thus, we introduce a framework that learns the task-relevant physical property once during training and thereafter relies solely on RGB sensing at test time. The code is available at https://github.com/prajapatisarvesh/RS-Net.",
            "introduction": "Autonomous robots are increasingly deployed in everyday settings, ranging from self-driving taxis [1] and search-and-rescue missions [2, 3] to wildfire prevention [4, 5]. In such unstructured environments, reliable autonomy demands more than obstacle avoidance. It requires precise reasoning about how terrain properties influence motion. For example, vehicles must modulate braking on icy roads, and off-road platforms should bypass dense swamps to avoid entrapment. These scenarios show that perception must move beyond geometry and semantics toward reliable estimates of robot–terrain interactions.\n\nSeveral off-road motion planning pipelines use RGB cameras to identify terrain from images [6, 7, 8]. In some cases, visually similar surfaces with different physical properties, such as ice on asphalt, may be mislabeled, leading to invalid traversability cost maps. Depth cameras and LiDAR are often used to estimate the ease of motion over a surface [9, 10]. However, the robot must first drive the terrain to create a dataset, which risks hardware damage and necessitates tuning specific to the operating site.\n\nIn contrast, spectral sensors offer a non-invasive way to estimate material properties. This is accomplished by leveraging distinct patterns of light absorption and reflection, known as spectral signatures, to characterize underlying material composition. These capabilities are finding use in robotics applications such as wildfire risk monitoring [3], manipulation [11, 12], and exploration [13]. By mapping spectral signatures to physical quantities such as moisture content, rigidity, or surface type, the same sensing stack can be repurposed across robots and environments with minimal changes to the processing pipeline. However, challenges such as custom mounts, calibration requirements, large datasets, and high sensor costs currently limit deployment at scale.\n\nRGB cameras are inexpensive, widely available, and already standard in robotic perception pipelines such as object detection [14] and tracking [15]. Compared to hyperspectral systems, they are cheaper, lighter, and more power efficient, which simplifies integration on mobile platforms. Advances in deep learning [16] now allow RGB imagery to approximate measurements traditionally obtained from more information-dense sensors.\n\nMotivated by this, we seek to retain the deployment advantages of RGB cameras while recovering spectral sensor features. We introduce RGB Image to Spectral Signature Neural Network or RS-Net, a deep neural network architecture trained on spectral data collected from diverse materials. It maps RGB image patches to their corresponding spectral signatures. These estimates are passed to a lightweight feedforward neural network whose weights are fine-tuned for the target physical property. We retrain this network once per task, enabling the same neural network architecture to perform terrain classification and friction estimation. Our entire inference pipeline runs at approximately 5 Hz, making it suitable for real-time robotic applications. Fig. LABEL:fig:paper_intro outlines the proposed architecture.\n\nWe validate our method in both simulation and hardware experiments. The terrain classification is used in a sampling-based motion planner for outdoor navigation of a skid-steer robot. Similarly, the friction estimates are integrated into a model predictive control (MPC) scheme for a quadrupedal robot operating on slippery surfaces. Finally, we also discuss how the proposed approach generalizes to other robots and additional physical properties relevant to off-road planning.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在不增加硬件成本的情况下，通过RGB图像推断材料光谱特征以更准确地评估地形与摩擦\n2. how to bridge RGB sensing with spectral properties to improve terrain traversability prediction and motion planning\n3. 将光谱特征转化为可用于采样式路径规划和MPC的物理属性\n4. 实时性与通用性：在多种环境与机器人上实现低成本、快速推断的框架\n\n【用了什么创新的方案】\nRS-Net 将 RGB patch 映射到光谱签名，在此基础上训练一个轻量前馈网络输出地形标签和摩擦系数；整个管线在测试时仅需 RGB，推理约 5 Hz；通过在训练阶段明确学习任务相关的物理属性，使同一架构可一次性对不同任务进行微调并发布到实际机器人上。代码开放，支持仿真与硬件实验中的地形导航和滑移表面的 MPC/规划集成。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation",
            "authors": "Hongli Xu,Lei Zhang,Xiaoyue Hu,Boyang Zhong,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "project website:this https URL, 11 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.19102",
            "code": "https://sites.google.com/view/funcanon",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19102",
            "arxiv_html_link": "https://arxiv.org/html/2509.19102v1",
            "abstract": "General-purpose robotic skills from end-to-end demonstrations often leads to task-specific policies that fail to generalize beyond the training distribution.\nTherefore, we introduce FunCanon, a framework that converts long-horizon manipulation tasks into sequences of action chunks, each defined by an actor, verb, and object.\nThese chunks focus policy learning on the actions themselves, rather than isolated tasks, enabling compositionality and reuse. To make policies pose-aware and category-general, we perform functional object canonicalization for functional alignment and automatic manipulation trajectory transfer, mapping objects into shared functional frames using affordance cues from large vision–language models.\nAn object-centric and action-centric diffusion policy FuncDiffuser trained on this aligned data naturally respects object affordances and poses, simplifying learning and improving generalization ability.\nExperiments on simulated and real-world benchmarks demonstrate category-level generalization, cross-task behavior reuse, and robust sim-to-real deployment, showing that functional canonicalization provides a strong inductive bias for scalable imitation learning in complex manipulation domains.\nDetails of the demo and supplemental material are available on our project website\n https://sites.google.com/view/funcanon.",
            "introduction": "As robots transition from controlled laboratory settings to unstructured real-world environments, developing robust and generalizable manipulation policies becomes increasingly critical. A fundamental challenge is enabling agents to generalize across unseen objects, diverse poses, and varying tasks — a capability that remains elusive for current imitation learning approaches.\n\nImitation learning methods based on RGB images [1] or point clouds [2] often suffer from limited precision and generalization due to viewpoint sensitivity, noisy observations, and redundant scene encodings. In contrast, 3D scene representations have shown promise in improving generalization [3]. To further address these challenges, object-centric representations, which focus on structured, object-level information such as 6D poses [4, 5] and scene flow [6], have gained significant attention. SPOT [4] demonstrates that SE(3) pose diffusion policy can improve cross-embodiment generalization, even when trained solely on passive human videos.\nExisting object-centric approaches [4] often depend on instance-specific, goal-conditioned trajectories, limiting generalization across categories and tasks. We attribute this to viewing manipulation as monolithic programs rather than modular, reusable behaviors.\nIn the field of computer vision, prior work, such as UAD [7] and Object Canonicalization [8], mainly targets improving visual representations and semantic understanding. Related efforts have also investigated category-level affordance pose estimation [9]. However, these methods have not explored how such representations can be leveraged for improving category-level alignment of robotic manipulation data, or augmenting manipulation trajectories.\n\nKey open questions remain: how can generalized representations be leveraged to synthesize diverse manipulation data, model long-horizon tasks, and train robust, generalizable manipulation policies?\n\nTo address these challenges, we propose FunCanon, a framework that models manipulation as compositions of reusable action primitives—such as pouring, grasping, or inserting—defined over functionally aligned bi-object interactions. By leveraging affordance cues from large vision-language models (VLMs), FunCanon canonicalizes semantically related objects (e.g., kettles and pitchers) into shared functional frames. This functional alignment enables the automatic manipulation trajectory transfer and the training of pose-aware, object-centric diffusion policies that focus solely on interaction dynamics, decoupled from specific object identities, camera viewpoints, or task semantics. To achieve robust and generalizable manipulation, we first decompose long-horizon tasks into meaningful action chunks, each specifying an actor, an action, and an object. This segmentation is performed by a large multimodal language model (MLLM), such as GPT-4o, in combination with large vision models (LVM) that extract affordance cues to guide chunking based on functional relevance. Next, we integrate these affordance cues with precise object pose estimates to perform functional alignment, canonicalizing objects into shared functional frames. This process identifies action-related affordance regions and aligns bi-object poses, producing a semantically grounded representation of manipulation interactions. Leveraging this functional alignment, automatic trajectory transfer method is proposed to augment training data on RLBench base tasks to increase data diversity and functional coverage. During policy training and inference, our object-centric diffusion policy receives inputs encoding both affordances, poses of bi-object pairs, point clouds and action verb and estimate actions.\nOur main contributions are:\n\nIntroducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\nIntroducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\nLeveraging large vision-language models for affordance-driven functional canonicalization for functional alignment and automatic trajectory transfer, enabling pose-aware and category-generalizable policy learning.\n\nDeveloping an object-centric diffusion policy trained on functionally aligned data, achieving both instance-level and category-level generalization and robust sim-to-real transfer.\n\n1. Introducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\n2. Leveraging large vision-language models for affordance-driven functional canonicalization for functional alignment and automatic trajectory transfer, enabling pose-aware and category-generalizable policy learning.\n\n3. Developing an object-centric diffusion policy trained on functionally aligned data, achieving both instance-level and category-level generalization and robust sim-to-real transfer.",
            "llm_summary": "【关注的是什么问题】\n1. 如何将长时程操纵任务分解为可重复使用的动作原语以提高泛化性\n2. 如何利用功能对齐和对象-功能框架实现从示例到通用策略的迁移\n3. 如何在无关对象身份、视角和任务语义的前提下实现姿态感知的策略学习\n4. 如何通过对齐数据训练对象中心的扩散策略以增强实例级和类别级泛化能力\n\n【用了什么创新的方案】\n- 将长时程操作分解为由行为者-动作-对象组成的可重用动作原语，并在功能对齐的双对象框架内进行分段\n- 通过大视觉语言模型的 affordance 提供和功能 canonicalization 将相关对象映射到共享功能框架，实现自动轨迹迁移\n- 基于对齐数据训练对象中心的扩散策略 FuncDiffuser，使策略在对象姿态、可供性和交互动力学上具备姿态感知能力并实现良好跨域泛化\n- 结合多模态分段（MLLM/ LVM）与功能对齐，进行自动化的轨迹迁移和增强数据多样性\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation",
            "authors": "Zhennan Jiang,Kai Liu,Yuxin Qin,Shuai Tian,Yupeng Zheng,Mingcai Zhou,Chao Yu,Haoran Li,Dongbin Zhao",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19080",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19080",
            "arxiv_html_link": "https://arxiv.org/html/2509.19080v1",
            "abstract": "Robotic manipulation policies are commonly initialized through imitation learning, but their performance is limited by the scarcity and narrow coverage of expert data. Reinforcement learning can refine polices to alleviate this limitation, yet real-robot training is costly and unsafe, while training in simulators suffers from the sim-to-real gap. Recent advances in generative models have demonstrated remarkable capabilities in real-world simulation, with diffusion models in particular excelling at generation. This raises the question of how diffusion model-based world models can be combined to enhance pre-trained policies in robotic manipulation. In this work, we propose World4RL, a framework that employs diffusion-based world models as high-fidelity simulators to refine pre-trained policies entirely in imagined environments for robotic manipulation. Unlike prior works that primarily employ world models for planning, our framework enables direct end-to-end policy optimization. World4RL is designed around two principles: pre-training a diffusion world model that captures diverse dynamics on multi-task datasets and refining policies entirely within a frozen world model to avoid online real-world interactions. We further design a two-hot action encoding scheme tailored for robotic manipulation and adopt diffusion backbones to improve modeling fidelity. Extensive simulation and real-world experiments demonstrate that World4RL provides high-fidelity environment modeling and enables consistent policy refinement, yielding significantly higher success rates compared to imitation learning and other baselines. More visualization results are available at https://world4rl.github.io/.",
            "introduction": "Despite recent progress in robotic manipulation, the field still faces critical challenges for practical deployment. Imitation learning is widely used to bootstrap policies from demonstrations, but its effectiveness is constrained by the inconsistency[1] and limited diversity[2, 3, 4] of available datasets. Although offline reinforcement learning (RL) can extract better policies from imperfect data, its susceptibility to overestimation[5] still makes it difficult to work effectively with limited datasets. Online RL offers a natural way to refine such pre-trained policies through interaction. However, real-robot RL, while capable of overcoming dataset limitations, suffers from high interaction costs and significant safety risks that hinder large-scale training. Training in simulation avoids these risks but inevitably introduces discrepancies from real-world physics, leading to a persistent sim-to-real gap[6].\n\nIn recent years, generative models have achieved remarkable progress in the visual domain[7], with diffusion models[8] demonstrating particularly strong performance in image[9] and video generation[10, 11]. Such generative capacity opens new opportunities for modeling complex and dynamic environments, offering a promising path toward learnable world simulators that provide realistic yet flexible environments for RL training in robotic manipulation.\n\nBuilding on this idea, we introduce World4RL, a framework that systematically integrates diffusion world models into RL for robotic manipulation. World4RL follows a two-stage paradigm: we first pre-train a diffusion world model on multi-task datasets to capture diverse dynamics, and then refine policies entirely within the frozen model to avoid costly and unsafe online interactions. Serving as a high-fidelity simulator, the world model is composed of a diffusion transition model that predicts future observations conditioned on current observations and actions, and a reward classifier that provides sparse success signals, enabling policy optimization without real-world rollouts.\n\nThis design of framework contrasts with prior approaches such as IRASim[12] and NWM[13], which primarily use generative video models for planning at test time rather than for direct policy training. A closer line of work, DiWA[14], also employs world models for policy learning. However, it relies on recurrent state-space models (RSSM[15]), which lead to blurry generations and compounding rollout errors. In contrast, World4RL leverages diffusion backbones that generate sharper and temporally coherent rollouts, thereby supporting effective end-to-end reinforcement learning.\n\nTo further adapt world models to robotic manipulation, which involves high-dimensional action spaces and complex environment interactions compared to navigation[13] and games[16], we investigate two critical design choices: a two-hot action encoding[17] scheme that provides an efficient representation of continuous actions while enabling lossless reconstruction, thereby serving as a robust bridge between the RL agent and the world model, and diffusion backbone architectures that determine the fidelity and consistency of predictions. These considerations are essential for enabling diffusion world models to serve not only as visual predictors but also as reliable simulators for policy training. To this end, our work makes the following key contributions.\n\nWe propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\nWe propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\nTo improve modeling fidelity and enable more effective policy refinement, we design a two-hot action encoding tailored for robotic manipulation and adopt a diffusion backbone as the world model.\n\nWe validate the effectiveness of World4RL through extensive experiments, showing that it consistently outperforms competitive baselines and significantly enhances policy refinement, improving success rates by 16% and 25% in simulation and real-robot experiments, respectively.\n\n1. We propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\n2. To improve modeling fidelity and enable more effective policy refinement, we design a two-hot action encoding tailored for robotic manipulation and adopt a diffusion backbone as the world model.\n\n3. We validate the effectiveness of World4RL through extensive experiments, showing that it consistently outperforms competitive baselines and significantly enhances policy refinement, improving success rates by 16% and 25% in simulation and real-robot experiments, respectively.",
            "llm_summary": "【关注的是什么问题】\n1. 如何利用扩散模型构建高保真世界模型以提升机器人操控策略的学习效率与安全性（≤40词）\n2. 如何在不在线真实交互的前提下，通过在冻结的扩散世界模型中对策略进行端到端优化来实现改进（≤40词）\n3. 如何设计适合高维持续动作空间的两热编码以及扩散骨干网络以提升生成的时序一致性与预测保真度（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出 World4RL，将扩散式世界模型作为高保真仿真器用于策略的离线强化学习训练；先在多任务数据集上预训练扩散世界模型，再在冻结的模型中进行端到端策略优化；使用两热动作编码和扩散骨干来提升建模保真度与鲁棒性；通过扩散转移模型预测未来观测并结合奖励分类器提供稀疏成功信号，实现无真实滚动的策略 refinement。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions",
            "authors": "Laura Connolly,Aravind S. Kumar,Kapi Ketan Mehta,Lidia Al-Zogbi,Peter Kazanzides,Parvin Mousavi,Gabor Fichtinger,Axel Krieger,Junichi Tokuda,Russell H. Taylor,Simon Leonard,Anton Deguet",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19076",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19076",
            "arxiv_html_link": "https://arxiv.org/html/2509.19076v1",
            "abstract": "Image-guided robotic interventions involve the use of medical imaging in tandem with robotics. SlicerROS2 is a software module that combines 3D Slicer and robot operating system (ROS) in pursuit of a standard integration approach for medical robotics research. The first release of SlicerROS2 demonstrated the feasibility of using the C++ API from 3D Slicer and ROS to load and visualize robots in real time. Since this initial release, we’ve rewritten and redesigned the module to offer greater modularity, access to low-level features, access to 3D Slicer’s Python API, and better data transfer protocols. In this paper, we introduce this new design as well as four applications that leverage the core functionalities of SlicerROS2 in realistic image-guided robotics scenarios.",
            "introduction": "Medical robotics is an evolving and rapidly growing research field with the potential to transform standard clinical practice. It is possible that robots will one day transcend human capabilities while offering higher efficiency, lower costs, improved training outcomes and better safety [1]. The advancement of image-guided robotics in particular, which are systems that rely on both medical imaging and robotics, is critical for achieving this potential. This is because image-guided robots can be used to fuse preoperative and intraoperative realities [2].\n\nThere are several combinations of imaging modalities and robotic systems have been explored in this capacity. For example, the SpineBot uses computed tomography (CT) imaging to help define the trajectory of pedicle screws, and robotics to guide the surgeon through those trajectories [3], [4]. Another example is the MrBot, which was designed to help perform percutaneous needle interventions within the confines of a magnetic resonance imaging (MRI) scanner [5]. Similarly, the Artemis robot was designed to facilitate transrectal prostate biopsy under ultrasound guidance with MRI fusion [6]. These are just a few examples of the numerous procedures and therapies where the use of image-guidance in tandem with robotics has been investigated. More recently, advancements in image-guided robotics have enabled: navigation of catheters into blood vessels with magnetic continuum devices [7], autonomous needle steering for lung biopsy [8] and teleoperated neurovascular interventions [9].\n\nDespite this extensive investigation, there are only a few areas of intervention where image-guided robotic systems that have achieved widespread adoption and financial commercial success such as robotic bronchoscopy, radiation oncology and neurosurgery [2] [10]. One contributor to this slow growth and adoption is the lack of a common integration approach. For any image-guided robotic system, integration of the imaging modality and the robot is the most important factor for usability. However, several companies and research systems take their own unique approach to integration. This results in device-specific software, expensive research licenses, incompatible communication protocols, and overall, a high barrier to entry to develop such systems. Considering these challenges and their potential threat to continued development, it is imperative to provide a common integration scheme for image-guided robotics. We hypothesize that this will prevent re-engineering and promote reproducibility across different clinical applications.\n\nFrom a development perspective, image-guided therapy (IGT) platforms and medical robotics platforms are often separated. In the realm of IGT, the open-source medical imaging platform, 3D Slicer is the most commonly used research platform [11] [12]. With over one million downloads and an active research and support community, 3D Slicer is used for segmentation, virtual reality, image analysis, artificial intelligence, and several other applications [13]. Several research platforms have been enabled by or added to 3D Slicer such as SlicerIGT [12], SlicerVR [14], MONAI label [15], and Total Segmentator [16]. There are also open-source platforms like OpenIGTLink and the PLUS toolkit that allow users to interface commercial hardware with 3D Slicer to build complete IGT systems [17] [18]. As a result of these efforts, 3D Slicer is considered the de-facto open source software for developing navigated, image-guided interventions.\n\nIn the field of medical robotics, robot operating system (ROS), an open-source middleware designed to support robotics development, is the predominant framework. ROS is a modular development framework that provides tools for autonomous navigation, simulation, visualization, and control [19]. Like 3D Slicer, ROS has a very active research community that is constantly contributing to the platform. For medical robotics specifically, several research tools like the Computer Integrated Surgical Systems Surgical Assistant Workstation (CISST-SAW) libraries [20] and the Asynchronous Multi-Body Framework (AMBF) [21] support ROS. The da Vinci Research Kit (dVRK), a popular open-source medical robot [22], also supports ROS for software development. Furthermore, many commercially available robots provide a ROS interface off the shelf.\n\nIn an effort to pursue a common integration scheme for image-guided robotics research, we decided to bridge these two ecosystems. As evidenced by the numerous published papers that employ both 3D Slicer and ROS (71 papers available on Google Scholar using keywords “3D Slicer” AND “robot operating system”), there is also demand from the community for this integration. Previous attempts to bridge 3D Slicer and ROS such as the ROS-IGTL bridge [23], custom applications for specific robots [24] [25], and our initial offering of SlicerROS2 [26] have fallen short of meeting all of the needs of a common integration tool. These needs include greater usability by providing access to low-level features, robust data transfer protocols that support commonly used message types, thorough documentation, and maintainability [27]. We have since redesigned SlicerROS2 to further support image-guided robotics research considering these requirements. The details of this new design are described in the following sections. The contributions of this paper are: 1) A newly designed research module for efficient data transfer between 3D Slicer and ROS 2 and 2) four relevant applications that demonstrate how it can be used for rapid research prototyping.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在图像引导的机器人干预中实现 3D Slicer 与 ROS2 的高效、通用集成\n2. 提高跨平台数据传输的鲁棒性与对低级功能的可访问性\n3. 快速原型化研究所需的模块化、可维护的工具体系\n4. 在医学机器人研究中促进可重复性与协同开发的标准化方案\n【用了什么创新的方案】\n核心解决方案：重新设计的 SlicerROS2 模块实现更高模块化和低级功能访问，整合 3D Slicer 的 Python 与 C++ API，改进数据传输协议，支撑多种消息类型，并提供四个实际应用场景以展示核心功能在图像引导机器人中的应用与原型化能力\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation",
            "authors": "Geonhyup Lee,Yeongjin Lee,Kangmin Kim,Seongju Lee,Sangjun Noh,Seunghyeok Back,Kyoobin Lee",
            "subjects": "Robotics (cs.RO)",
            "comment": "9 pages, 9 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19047",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19047",
            "arxiv_html_link": "https://arxiv.org/html/2509.19047v1",
            "abstract": "Contact-rich manipulation tasks such as precision assembly require precise control of interaction forces, yet existing imitation learning methods rely mainly on vision-only demonstrations. We propose ManipForce, a handheld system designed to capture high-frequency force–torque (F/T) and RGB data during natural human demonstrations for contact-rich manipulation. Building on these demonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT). FMT encodes asynchronous RGB and F/T signals using frequency- and modality-aware embeddings and fuses them via bi-directional cross-attention within a transformer diffusion policy. Through extensive experiments on six real-world contact-rich manipulation tasks—such as gear assembly, box flipping, and battery insertion—FMT trained on ManipForce demonstrations achieves robust performance with an average success rate of 83% across all tasks, substantially outperforming RGB-only baselines. Ablation and sampling-frequency analyses further confirm that incorporating high-frequency F/T data and cross-modal integration improves policy performance, especially in tasks demanding high precision and stable contact.\nHardware, software, and video demos are available at: https://sites.google.com/view/manipforce/홈.",
            "introduction": "Contact-rich manipulation tasks such as precise assembly [1, 2, 3, 4], battery disassembly [5], and non-prehensile handling [6] require high precision and force-aware manipulation. Humans naturally perceive contact forces and their subtle changes when assembling parts, adjusting their strategies accordingly. Yet most robotic approaches rely solely on visual demonstrations, missing the rich F/T information humans provide.\n\nRecent advances in imitation learning [7, 8, 9] have demonstrated strong potential for dexterous and contact-rich manipulation by learning directly from human demonstrations.\nHowever, these methods still rely on high-quality demonstration data, which is costly and difficult to collect for fine-grained interactions.\nHand-held data collection systems such as UMI [10] have been proposed to address this challenge by enabling natural human demonstrations without the expertise requirements and remote-control limitations of teleoperation.\nWhile effective for simplifying demonstration collection, UMI does not capture force–torque (F/T) information, which is essential for accurately modeling contact behaviors.\nMore recent work [11] combines visual and F/T data but relies on point clouds to represent the scene, which introduces complex setup requirements and fundamentally limits the ability to perceive small objects and fine clearances essential for contact-rich manipulation.\nFurthermore, from a learning perspective, this approach down-samples high-frequency F/T signals to match the image frame rate, losing rich temporal information necessary for modeling contact dynamics.\n\nTo address these limitations, we introduce ManipForce a handheld system for simultaneous RGB–F/T data collection during natural human demonstrations, and the Frequency-Aware Multimodal Transformer (FMT), which learns robust policies from the collected data for diverse, precise, and contact-rich manipulation tasks.\n\nManipForce consists of a dual handheld camera setup with a wrist-mounted F/T sensor to capture both visual and high-frequency force signals during human-guided demonstrations.\nThis configuration enables robust perception of small objects, tight clearances, and fine-grained contacts, allowing collected demonstrations to transfer directly to robotic execution.\nWe replace SLAM-based wrist tracking with 3D ArUco marker pose estimation to maintain accuracy during close-contact interactions without environmental dependencies, and apply tool gravity compensation to ensure precise and interaction-focused F/T measurements.\nWe propose the FMT, which learns from asynchronous RGB (30 Hz) and F/T (>200 Hz) signals using a Transformer-based Diffusion Policy [7] architecture.\nTo exploit the higher-frequency force signals relative to images, the model tokenizes both RGB and F/T inputs using learnable frequency and modality embeddings.\nThis design enables the model to effectively handle heterogeneous modalities with asynchronous sampling rates.\nIn addition, bi-directional cross-attention modules fuse complementary information across modalities.\nWe evaluate our approach on six contact-rich manipulation tasks spanning precision assembly, non-prehensile manipulation, and complex disassembly, and observe significant performance gains over RGB-only baselines.\nAblation studies further confirm that high-frequency F/T sensing, unified positional embeddings, and bi-directional cross-attention each make complementary contributions to robust multimodal policy learning.\n\nOur main contributions are:\n\nWe introduce ManipForce, a handheld RGB–F/T data collection system enabling diverse and fine-grained contact-rich manipulation demonstrations.\n\nWe propose FMT, which handles inputs with asynchronous sampling rates through frequency-aware multimodal representation learning and cross-attention within a Transformer architecture, enabling robust policy learning for contact-rich manipulation.\n\nWe demonstrate robust performance on diverse contact-rich manipulation tasks—including gear assembly, plug insertion, battery disassembly, and lid operations—consistently outperforming RGB-only baselines.\n\n1. We introduce ManipForce, a handheld RGB–F/T data collection system enabling diverse and fine-grained contact-rich manipulation demonstrations.\n\n2. We propose FMT, which handles inputs with asynchronous sampling rates through frequency-aware multimodal representation learning and cross-attention within a Transformer architecture, enabling robust policy learning for contact-rich manipulation.\n\n3. We demonstrate robust performance on diverse contact-rich manipulation tasks—including gear assembly, plug insertion, battery disassembly, and lid operations—consistently outperforming RGB-only baselines.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在接触丰富的操纵任务中同时利用高频力矩信息与RGB视觉实现鲁棒仿生策略\n2. 如何在异步采样率下融合多模态数据以提升策略学习的准确性与稳定性\n3. 如何减少对复杂场景感知的依赖并实现自然人类演示的高效数据收集\n\n【用了什么创新的方案】\nManipForce 提供可手持的 RGB–F/T 数据采集系统，结合高频力矩与 RGB 数据；FMT 使用频率感知的多模态表示对异步 RGB(30 Hz) 与 F/T(>200 Hz) 信号进行编码，采用双向交叉注意力在 Transformer diffusion 策略中进行跨模态融合，显著提升对接触力与微小几何的建模能力；并通过 3D ArUco 标定与工具重力补偿实现近距离接触的高精度演示对齐与采样。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors",
            "authors": "Qingzheng Cong,Steven Oh,Wen Fan,Shan Luo,Kaspar Althoefer,Dandan Zhang",
            "subjects": "Robotics (cs.RO)",
            "comment": "14 pages, 8 figures. Equal contribution: Qingzheng Cong, Steven Oh, Wen Fan. Corresponding author: Dandan Zhang (d.zhang17@imperial.this http URL). Additional resources atthis http URL",
            "pdf_link": "https://arxiv.org/pdf/2509.19037",
            "code": "http://stevenoh2003.github.io/TacEva/, http://ac.uk",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19037",
            "arxiv_html_link": "https://arxiv.org/html/2509.19037v1",
            "abstract": "Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because of the high spatial resolution they offer and their relatively low manufacturing costs. However, variations in their sensing mechanisms, structural dimension, and other parameters lead to significant performance disparities between existing VBTSs. This makes it challenging to optimize them for specific tasks, as both the initial choice and subsequent fine-tuning are hindered by the lack of standardized metrics. To address this issue, TacEva is introduced as a comprehensive evaluation framework for the quantitative analysis of VBTS performance. The framework defines a set of performance metrics that capture key characteristics in typical application scenarios. For each metric, a structured experimental pipeline is designed to ensure consistent and repeatable quantification. The framework is applied to multiple VBTSs with distinct sensing mechanisms, and the results demonstrate its ability to provide a thorough evaluation of each design and quantitative indicators for each performance dimension. This enables researchers to pre-select the most appropriate VBTS on a task by task basis, while also offering performance-guided insights into the optimization of VBTS design. A list of existing VBTS evaluation methods and additional evaluations can be found on our website:\nhttps://stevenoh2003.github.io/TacEva/.",
            "introduction": "Robots have yet to attain the level of manipulative dexterity exhibited by humans, a challenge rooted in the difficulty of accurately acquiring detailed contact information in physical environments [1]. Tactile sensing has therefore become indispensable for delicate and precise robotic manipulation in embodied intelligence systems [2]. A notable development in this domain has been the rise of vision-based tactile sensors (VBTSs) [3]. These sensors employ high-resolution cameras to capture detailed contact surface information, thereby integrating seamlessly with computer vision and image-based deep learning methods. Nevertheless, among VBTSs, we see a wide variety of architectures, structural dimensions, and fabrication techniques, depending on the exact nature of the application requirements [4].\n\nThe rapid development of VBTSs has created an urgent need for standardized performance evaluation. Selecting an appropriate VBTS for a specific task scenario remains challenging, as distinct sensor designs offer substantial variation in performance. A universal evaluation protocol would therefore facilitate fair comparison across sensor designs, support informed selection, and guide design optimization. However, a significant gap persists in the field: no standardized framework currently exists for VBTS evaluation, and the inconsistency of current metrics limits objective, reproducible, and comprehensive cross-sensor assessment. This challenge is further compounded by the inherently multi-modal nature of tactile sensing [5], which necessitates coordinated evaluation across multiple performance dimensions.\n\nThe application scenarios for VBTSs are inherently diverse. This makes it difficult to establish universal, broadly applicable evaluation metrics that remain meaningful across the field. Further complexity arises from the fact that VBTSs are commonly fabricated using silicone elastomers, introducing additional considerations related to soft-material mechanics, optics, and imaging. Owing to differences in their underlying sensing mechanisms, each VBTS design inevitably exhibits its own profile of strengths and weaknesses. As a result, certain positive performance characteristics will typically tend to be emphasized, while weaker ones may be under-reported. This is likely to complicate informed selection, making it more challenging for prospective users.\n\nTo address these challenges, we propose TacEva, a systematic evaluation framework that integrates performance quantification with a structured and reproducible assessment pipeline. TacEva is designed to provide consistency and comparability across VBTS designs, enabling practitioners to make evidence-based decisions while obtaining a holistic understanding of sensor performance (see Fig. 1). It also offers sensor developers clear guidance for targeted optimization during the design process. By defining and standardizing a comprehensive set of performance metrics and evaluation protocols, TacEva aims to provide a unified and transparent characterization of VBTS designs, thereby facilitating objective comparison, reliable selection, and more informed innovation for future sensor development.",
            "llm_summary": "【关注的是什么问题】\n1. 缺乏标准化评估框架导致VBTSs在不同设计间难以公平比较与任务级优化（≤40词）\n2. 多模态触觉传感的评估指标缺乏统一、可重复的量化流程（≤40词）\n3. 不同VBTS sensing机制、结构与材料差异造成性能特征分布不一致，难以选型与设计优化（≤40词）\n4. 缺少可跨传感器的全面综合性能指标与任务驱动的性能指示（≤40词）\n\n【用了什么创新的方案】\nTacEva 提供一个综合的性能评估框架，定义一套标准化、可重复的评测指标和实验流水线，覆盖典型应用场景的关键性能维度，并可对多种VBTS 进行横向比较与任务驱动的选型与优化指导。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion",
            "authors": "Shuai Liu,Meng Cheng Lau",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "11 pages, 5 figures, 1 table, Computational Science Graduate Project",
            "pdf_link": "https://arxiv.org/pdf/2509.19023",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19023",
            "arxiv_html_link": "https://arxiv.org/html/2509.19023v1",
            "abstract": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a two-stage reinforcement learning framework for humanoid walking that requires no motion capture data or elaborate reward shaping. In the first stage, a compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via Proximal Policy Optimization. This generates energy-efficient gait templates. In the second stage, those dynamically consistent trajectories guide a full-body policy trained with Soft Actor–Critic augmented by an adversarial discriminator, ensuring the student’s five-dimensional gait feature distribution matches the ROM’s demonstrations. Experiments at 1 m/s and 4 m/s show that ROM-GRL produces stable, symmetric gaits with substantially lower tracking error than a pure-reward baseline. By distilling lightweight ROM guidance into high-dimensional policies, ROM-GRL bridges the gap between reward-only and imitation-based locomotion methods, enabling versatile, naturalistic humanoid behaviors without any human demonstrations.",
            "introduction": "Achieving natural humanoid locomotion is a longstanding goal in both robotics and computer animation. From bipedal robots that can walk and run with human-like grace, to virtual characters that move realistically in games, the ability to synthesize lifelike walking gaits remains a persistent challenge. Traditional model-based control has produced impressive feats, but often requires painstaking design and does not always capture the natural motion nuances of human walking Kuindersma et al. (2016). In recent years, reinforcement learning (RL) has emerged as a promising data-driven paradigm for developing locomotion controllers Radosavovic et al. (2024). RL allows simulated humanoids to learn complex gait behaviors through trial-and-error, offering the potential to discover agile and robust walking strategies that would be difficult to manually design.\n\nReinforcement learning (RL) methods for natural locomotion generally fall into two main paradigms aimed at producing lifelike gait behaviors.\n\nA purely objective‑driven RL policy can discover stable walking gaits by optimizing energy and stability rewards. These reference-free methods train locomotion policies from scratch by optimizing carefully crafted reward functions, without any motion capture examples. The reward terms are designed to encourage physically plausible and human-like traits, such as forward speed with energy efficiency, maintaining center-of-mass stability and upright posture, periodic foot contact patterns, and symmetric gait cycles between left and right legs. By rewarding such objectives, controllers can spontaneously develop stable walking gaits that emerge naturally from learning. Notably, researchers have demonstrated that symmetric bipedal walking can arise solely from reward design and curriculum training, without any reference motions Yu et al. (2018). Indeed, recent work showed that an RL policy trained with biomechanically inspired rewards could produce natural walking behaviors purely through self-exploration Peng et al. (2025). The appeal of this approach is that it does not require any pre-recorded motions – the agent invents its own walking cycle. However, designing the reward function is notoriously difficult: the policy’s behavior is highly sensitive to the choice and weighting of reward terms, often requiring elaborate hand-tuning and expertise. Even with careful tuning, purely objective-driven policies may develop subtle artifacts or unnatural quirks since there is no direct template of “human” motion to imitate. In summary, while motion-free RL can yield impressively natural gaits under the right conditions, it faces challenges in reward engineering and consistency of motion style.\n\nAlternatively, imitation‑driven RL uses human mocap examples to ensure motion realism but at the cost of dataset dependency. To directly ensure natural movement quality, a dominant approach is to imitate example locomotion trajectories from motion capture data. In this paradigm, the RL agent is guided by reference motions of humans (or animals) and receives rewards for matching the reference pose and velocity at each time step, or uses adversarial critics to judge realism against a motion dataset Peng et al. (2018, 2021). DeepMimic pioneered this line of work by showing that standard RL algorithms can learn robust control policies capable of imitating a broad range of example motion clips from a motion capture library Peng et al. (2018). By combining motion imitation objectives with task goals, DeepMimic enabled physically simulated characters to reproduce dynamic skills (flips, spins, walks) with high fidelity to the mocap examples. Following this, numerous studies have pushed the state of the art in imitation-based locomotion. For instance, ASE (Adversarial Skill Embeddings) uses large unstructured motion datasets to train latent skill models via adversarial imitation, yielding a repertoire of reusable behaviors that look remarkably life-like Peng et al. (2022). Another example is CALM (Conditional Adversarial Latent Models), which learns a rich latent representation of human movement through imitation learning, capturing the complexity and diversity of human motion while allowing direct user control over the character’s style and direction Tessler et al. (2023). Most recently, the concept of Behavioral Foundation Models have emerged – here a policy is pre-trained on massive collections of motion data (in an unsupervised manner) to serve as a generalist locomotion model. These foundation models, once trained on unlabeled motion trajectories, can be prompted to perform new tasks in a zero-shot fashion, while retaining human-like gait qualities Tirinzoni et al. (2025a). Motion imitation approaches thus achieve state-of-the-art realism in simulated walking; policies closely mimic human kinematics and can produce motions nearly indistinguishable from motion capture. The downside, however, is their heavy reliance on curated motion data – one must have access to large datasets of reference gaits, and the learned skills are inevitably tied to the distribution of motions in those datasets.\n\nDespite these advances, achieving natural, human‑like locomotion without any motion capture data remains an open challenge. Purely objective‑driven methods often fail to reproduce the fluidity and subtle timing of human gait, while imitation‑based approaches are fundamentally constrained by the availability and diversity of mocap archives.\n\nTo bridge this gap, we introduce a reduced‑order model–guided reinforcement learning(ROM-GRL) framework. It leverages a simplified locomotion model as a stand‑in for motion capture, providing high‑level gait guidance to the RL agent without any human demonstrations. Our framework unfolds in two stages. In the first stage, we train a lightweight teacher model to generate efficient, dynamically consistent gait templates that capture the essence of natural walking. In the second stage, we distill these templates into a full-body controller by rewarding adherence to the teacher’s motion distribution, ensuring smooth, human-like locomotion. By separating high-level gait planning from detailed control, we achieve a single walking policy that is both robust and naturally fluid, all without relying on motion capture data or intricate reward design.\n\nIn summary, our approach marries the insights from model-based gait generation with the flexibility of reinforcement learning. By using a ROM to guide RL instead of direct motion capture, we maintain a purely physics-driven training regime while still inducing realistic movement patterns. The proposed framework demonstrates that natural humanoid locomotion can emerge without demonstrations, closing the gap between reward engineering and motion imitation. We validate that our ROM-guided RL method produces walking controllers that are stable across different speeds and exhibit natural gait symmetry and fluidity comparable to reward-based policies, suggesting a novel solution to produce life-like humanoid locomotion in the absence of motion data.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在没有运动捕捉数据的情况下实现自然、稳定的人形机器人行走\n2. 如何在强化学习中兼顾高层步态规划与全身控制的协同学习\n3. 如何避免繁琐的奖励设计而仍获得自然的步态与对称性\n4. 如何将简化的ROM释放为对高维策略的有效引导\n\n【用了什么创新的方案】\n两阶段ROM-GRL：先用4-DOF简化模型通过PPO生成高效、动态一致的步态模板；再用这些模板对全身策略进行蒸馏，使策略通过对ROM分布的遵循实现自然、对称的行走，并结合带对抗判别的Soft Actor–Critic以匹配ROM示范分布，且无任何人类示范数据。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
            "authors": "Dapeng Zhang,Jin Sun,Chenghui Hu,Xiaoyan Wu,Zhenlong Yuan,Rui Zhou,Fei Shen,Qingguo Zhou",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19012",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19012",
            "arxiv_html_link": "https://arxiv.org/html/2509.19012v1",
            "abstract": "The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.",
            "introduction": "Robotics has long been a prominent area of scientific research. Historically, robots primarily relied on pre-programmed instructions and engineered control policies to perform task decomposition and execution. These methods were commonly applied to simple, repetitive tasks, such as factory assembly lines and logistics sorting. In recent years, the rapid advancement of artificial intelligence has enabled researchers to exploit the feature extraction and trajectory prediction capabilities of deep learning across diverse modalities, including images, text, and point clouds. By integrating techniques such as perception, detection, tracking, and localization, researchers have decomposed robotic tasks into multiple stages to meet execution requirements, thereby advancing the development of embodied intelligence and autonomous driving. However, most of these robots still operate as isolated agents, designed for specific tasks and lacking effective interaction with humans and external environment.\n\nTo address these limitations, researchers have begun exploring the incorporation of large language models (LLMs) and vision language models (VLMs) to enable more accurate and flexible robotic manipulation. Modern robotic manipulation methods [1, 2] typically leverage vision language generative paradigms (e.g., autoregressive models [3, 4, 5, 6] or diffusion models [7]), combined with large-scale datasets [8] and advanced fine-tuning strategies. We refer to these as VLA foundation models, which have substantially improved the quality of robotic manipulations. Fine-grained action control over generated content provides users with greater flexibility, unlocking the practical potential of VLA for task execution.\n\nDespite their promise, reviews of pure VLA methods remain scarce. Existing surveys either focus on taxonomy over VLM foundational models or provide broad overviews of robotic manipulation as a whole. Firstly, VLA methods represent a nascent field in robotics, with no established methodological landscape or consensus taxonomy, making it challenging to systematically summarize these approaches. Secondly, current reviews either classify VLA approaches based on differences in foundational models or present a comprehensive analysis of robotic applications across the entire history of the field, often emphasizing traditional methods at the expense of emerging techniques. While these reviews offer valuable insights, they provide only cursory examinations of robotic models or concentrate primarily on foundational models, leaving a significant gap in the literature regarding pure VLA methods.\n\nIn this paper, we investigate VLA methods and associated resources, providing a focused and comprehensive review of existing approaches. Our goal is to present a clear taxonomy, systematically summarize VLA research, and elucidate the development trajectory of this rapidly evolving field. After a brief overview of LLMs and VLMs, we focus on the policy strategies of VLA models, highlighting the unique contributions and distinctive features of previous studies. We classify VLA approaches into 4 categories: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods, and provide a detailed analysis of their motivations, core strategies, and mechanisms. As shown in Fig. 2, we present a VLA skeleton of these methods.\nWe examine application domains, including robotic arms, quadruped robots, humanoids, and wheeled robots (autonomous vehicles), offering a comprehensive assessment of VLA deployment across diverse scenarios. Given the strong dependence of VLA models on datasets and simulation platforms, we provide a concise overview of these resources. Finally, based on the current VLA landscape, we identify key challenges and outline future research directions—including data limitations, inference speed, and safety—to accelerate the advancement of VLA models and generalizable robotics.\n\nThe overall structure of this survey is illustrated in Fig. 1. First, Section 2 provides an overview of the background for VLA research. Section 3 presents the existing VLA approaches in the robotics field. Section 4 introduces the datasets and benchmarks employed by VLA approaches. Sections 5 and 6 discuss simulation platforms and robotic hardware. Section 7 further discusses the challenges and future directions for VLA-based robotic methods. Finally, we summarize the paper and provide our perspective on future developments.\n\nIn summary, our contributions are as follows:\n\nWe present the well-structured taxonomy of pure VLA methods, classifying approaches based on their action-generation strategies. This facilitates understanding of existing methods and highlights core challenges in the field.\n\nThe survey emphasizes the defining characteristics and methodological innovations of each category and technique, providing a clear perspective on current approaches.\n\nWe provide a comprehensive overview of associated resources (datasets, benchmarks and simulation platforms) for training and evaluating VLA models.\n\nWe investigate the practical impact of VLA in robotics, identify key limitations of existing techniques, and propose potential avenues for further exploration.\n\n1. We present the well-structured taxonomy of pure VLA methods, classifying approaches based on their action-generation strategies. This facilitates understanding of existing methods and highlights core challenges in the field.\n\n2. The survey emphasizes the defining characteristics and methodological innovations of each category and technique, providing a clear perspective on current approaches.\n\n3. We provide a comprehensive overview of associated resources (datasets, benchmarks and simulation platforms) for training and evaluating VLA models.\n\n4. We investigate the practical impact of VLA in robotics, identify key limitations of existing techniques, and propose potential avenues for further exploration.",
            "llm_summary": "【关注的是什么问题】\n1. 如何对纯VLA方法进行清晰分类与系统综述，以揭示其核心动机与实现机制？（≤40词）\n2. VLA模型在机器人执行中的生成策略、数据/基准资源、仿真平台与应用域的整合与挑战是什么？（≤40词）\n3. 现有VLA方法的局限性、安全性与可扩展性如何影响通用机器人能力的提升？（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出基于动作生成策略的纯VLA方法分型（自回归、扩散、强化学习、混合、专用），并对各类方法的动机、核心策略与实现机制进行系统比较；梳理数据集、基准与仿真平台；并给出未来方向与挑战。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Category-Level Object Shape and Pose Estimation in Less Than a Millisecond",
            "authors": "Lorenzo Shaikewitz,Tim Nguyen,Luca Carlone",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18979",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18979",
            "arxiv_html_link": "https://arxiv.org/html/2509.18979v1",
            "abstract": "Object shape and pose estimation is a foundational robotics problem, supporting tasks from manipulation to scene understanding and navigation. We present a fast local solver for shape and pose estimation which requires only category-level object priors and admits an efficient certificate of global optimality. Given an RGB-D image of an object, we use a learned front-end to detect sparse, category-level semantic keypoints on the target object. We represent the target object’s unknown shape using a linear active shape model and pose a maximum a posteriori optimization problem to solve for position, orientation, and shape simultaneously. Expressed in unit quaternions, this problem admits first-order optimality conditions in the form of an eigenvalue problem with eigenvector nonlinearities. Our primary contribution is to solve this problem efficiently with self-consistent field iteration, which only requires computing a 4×44\\times 4 matrix and finding its minimum eigenvalue-vector pair at each iterate. Solving a linear system for the corresponding Lagrange multipliers gives a simple global optimality certificate. One iteration of our solver runs in about 100 microseconds, enabling fast outlier rejection. We test our method on synthetic data and a variety of real-world settings, including two public datasets and a drone tracking scenario. Code is released at https://github.com/MIT-SPARK/Fast-ShapeAndPose.",
            "introduction": "A diverse set of robotics applications benefits from object shape and pose estimation. Autonomous cars, for example, need to locate obstacles and other cars [1], while household manipulators need to locate objects to interact with [2].\nIn many of these applications the object shape is not known exactly but its category is available (e.g., from a semantic segmentation method). We consider this setting and derive a shape and pose estimator using category-level priors.\n\nThe work of Shi et al. [3] established a certifiably optimal approach for category-level shape and pose estimation using a semidefinite relaxation. We consider a similar problem setup but emphasize both speed and certifiability. A fast estimator allows fast reaction to new inputs, performance with limited compute, and comprehensive outlier rejection [4, 5]. Certifiability provides an a posteriori guarantee that the estimate returned is statistically optimal. When the certificate fails, the user can decide to trust the output, try a different initialization, or acquire a new batch of measurements.\n\nOur algorithm relies on the eigenvalue structure of the first-order optimality conditions written in the quaternion representation of rotations. It returns local solutions which are often globally optimal. To verify this, we introduce a fast global optimality certifier based on Lagrangian duality. Specifically, our contributions are:\n\nA fast local solver for category-level shape and pose estimation\nusing self-consistent field iteration [6].\n\nA fast a posteriori certificate of global optimality for our local solutions.\n\nExperimental evaluation of runtime and accuracy on synthetic data, a drone tracking scenario, and two large-scale datasets.\n\nThe remainder of the paper is organized as follows. We begin with a literature review (Section II) and quaternion preliminaries (Section III). Then, we give the problem formulation in Section IV and reformulate it with quaternions in Section V.\nTo solve the nonlinear eigenproblem, we use self-consistent field iteration for local solutions and SDP optimality conditions to certify global optimality in Section VI. In Section VII, we show our method is significantly faster than other local solvers and learned baselines.\n\n1. A fast local solver for category-level shape and pose estimation\nusing self-consistent field iteration [6].\n\n2. A fast a posteriori certificate of global optimality for our local solutions.\n\n3. Experimental evaluation of runtime and accuracy on synthetic data, a drone tracking scenario, and two large-scale datasets.",
            "llm_summary": "【关注的是什么问题】\n1. 在低计算资源下，基于类别级先验进行对象形状与姿态的快速估计\n2. 给定RGB-D，如何通过稀疏语义关键点实现高效、可证伪的全局最优性口径\n3. 如何在四元数表示下将非线性优化转化为可迭代求解并给出全局最优性证据\n\n【用了什么创新的方案】\n核心解决方案：提出基于自洽场迭代的快速局部求解器，用线性主动形状模型表示未知形状，构造最大后验优化以同时估计位置、姿态和形状；在四元数表示下，将一阶最优性条件转化为特征值问题，通过迭代求解一个4×44×4的矩阵并获取最小特征值-向量对；通过拉格朗日乘子求解线性系统，提供快速的全局最优性证书（可后验判断是否可信）。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation",
            "authors": "Minoo Dolatabadi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18954",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18954",
            "arxiv_html_link": "https://arxiv.org/html/2509.18954v1",
            "abstract": "LiDAR-based localization and SLAM often rely on iterative matching algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align sensor data with pre-existing maps or previous scans. However, ICP is prone to errors in featureless environments and dynamic scenes, leading to inaccurate pose estimation. Accurately predicting the uncertainty associated with ICP is crucial for robust state estimation but remains challenging, as existing approaches often rely on handcrafted models or simplified assumptions. Moreover, a few deep learning-based methods for localizability estimation either depend on a pre-built map—which may not always be available—or provide a binary classification of localizable versus non-localizable, which fails to properly model uncertainty.\nIn this work, we propose a data-driven framework that leverages deep learning to estimate the registration error covariance of ICP before matching, even in the absence of a reference map. By associating each LiDAR scan with a reliable 6-DoF error covariance estimate, our method enables seamless integration of ICP within Kalman filtering, enhancing localization accuracy and robustness. Extensive experiments on the KITTI dataset demonstrate the effectiveness of our approach, showing that it accurately predicts covariance and, when applied to localization using a pre-built map or SLAM, reduces localization errors and improves robustness.",
            "introduction": "In recent years, autonomous vehicles have become an integral component of intelligent transportation systems, driving continuous research to push the boundaries of their capabilities. A key requirement for autonomous driving is achieving precise self-localization at the centimeter level [1]. The Global Navigation Satellite System (GNSS) is a cost-effective and widely used method for vehicle localization. Although GNSS delivers reliable positioning in open-sky environments, its accuracy is significantly compromised in urban settings due to factors such as signal blockage, non-line-of-sight (NLOS) conditions, and multipath effects [2]. To address these challenges, vision-based approaches—particularly those leveraging LiDAR—have been proposed as alternative or complementary solutions [3]. In these systems, map matching techniques like the Iterative Closest Point (ICP) algorithm are frequently employed to align sensor data with pre-existing maps, thereby enhancing localization precision [4].\n\nHowever, vision-based localization methods—like other relative localization techniques—are susceptible to error accumulation, where minor errors can progressively lead to significant drift over time. This phenomenon is observed in both Simultaneous Localization and Mapping (SLAM) and pre-built map-based localization approaches, although it tends to be more pronounced in SLAM [5]. In the context of ICP, previous work has demonstrated that factors such as featureless environments (e.g., tunnels) and the presence of dynamic objects can adversely affect the matching accuracy [6].\n\nState estimation methods, such as Kalman filtering, are commonly employed to mitigate these problems. Yet, these techniques depend on an accurate error model (often represented in the simplest form by an error covariance matrix), which is challenging to determine for matching algorithms\n[7].\n\nTo address this limitation, several studies have proposed data-driven approaches to predict ICP error, either using classification-based methods [8] or by directly estimating the covariance [7].\n\nSpecifically, we introduce a data-driven framework that predicts the full six-degree-of-freedom (6-DoF) ICP registration covariance from a single LiDAR scan prior to correspondence search and ICP refinement. The output is a symmetric positive-definite (SPD) 6×66\\times 6 covariance on S​E​(3)SE(3) suitable for probabilistic fusion. An overview of the pipeline is shown in Fig. 1.\n\nContributions.\n\nPre-ICP, per-scan S​E​(3)SE(3) covariance. We predict the full 6×66\\times 6 registration covariance directly from a single LiDAR scan, capturing translation–rotation coupling before ICP is run. Per-scan training targets are obtained by estimating empirical covariances from Monte Carlo ICP under randomized initializations.\n\nKalman fusion and evaluation. Each scan is paired with a reliable 6-DoF covariance, used as the measurement noise in a standard Kalman filter. This improves pose accuracy over fixed or heuristic covariances on the evaluated sequences.\n\nPracticality. The model does not require a pre-built map at inference, supporting both SLAM and map-based operation.\n\n1. Pre-ICP, per-scan S​E​(3)SE(3) covariance. We predict the full 6×66\\times 6 registration covariance directly from a single LiDAR scan, capturing translation–rotation coupling before ICP is run. Per-scan training targets are obtained by estimating empirical covariances from Monte Carlo ICP under randomized initializations.\n\n2. Kalman fusion and evaluation. Each scan is paired with a reliable 6-DoF covariance, used as the measurement noise in a standard Kalman filter. This improves pose accuracy over fixed or heuristic covariances on the evaluated sequences.\n\n3. Practicality. The model does not require a pre-built map at inference, supporting both SLAM and map-based operation.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在 ICP 匹配前直接估计六自由度（6-DoF）注册协方差以提升 LiDAR 基于定位的鲁棒性\n2. 缺少地图时如何进行无地图依赖的协方差预测，并实现与卡尔曼滤波的无缝融合\n3. 面对 featureless 或动态环境，如何更准确地量化 ICP 的不确定性并减少定位误差\n4. 将深度学习预测的协方差用于先验状态估计，从而提升 SLAM/基于地图的定位的鲁棒性\n\n【用了什么创新的方案】\nPre-ICP 直接从单帧 LiDAR 预测对称正定的 6×6 协方差矩阵，捕捉平移-旋转耦合；以蒙特卡洛 ICP 在随机初始值下得到的经验协方差作为训练目标。将该协方差作为卡尔曼滤波的测量噪声，实现 ICP 前后的一体化鲁棒定位；方法在推断阶段不依赖预构建地图，支持 SLAM 与地图匹配场景。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations",
            "authors": "Hanqing Liu,Jiahuan Long,Junqi Wu,Jiacheng Hou,Huili Tang,Tingsong Jiang,Weien Zhou,Wen Yao",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18953",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18953",
            "arxiv_html_link": "https://arxiv.org/html/2509.18953v1",
            "abstract": "Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60%, with object transformations causing up to 97.8% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.",
            "introduction": "Vision-Language-Action (VLA) models represent a paradigm shift in robotic manipulation, integrating visual perception, language understanding, and action generation into unified end-to-end systems [1]. Recent deployments across manufacturing [2], healthcare [3], and service robotics [4, 5] demonstrate their transformative potential. However, in real-world deployments, VLA models inevitably face challenging physical variations, such as spatial transformations, illumination variations, and visual disruptions, which can dramatically alter robot behavior without being immediately detectable, posing significant safety risks. Therefore, it is crucial to investigate VLA robustness across various physical conditions systematically.\n\nExisting research has explored the robustness of VLA-based robotic systems through approaches like adversarial patches [6], which generate localized perturbations via gradient-based white-box attacks to achieve visual interference. However, these methods suffer from critical limitations: they violate physical plausibility constraints and fail to capture the rich spectrum of real-world physical variations. Moreover, their reliance on gradient access restricts applicability to black-box deployment scenarios. Based on these limitations, we aim to generate more diverse and realistic physical variations for comprehensively evaluating VLA robustness, while two key challenges must be addressed: (1) How to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility? (2) How to discover worst-case scenarios without prohibitive real-world data collection costs efficiently?\n\nTo address these challenges, we propose Eva-VLA, a unified framework for evaluating vision-language-action models’ robustness. Our key innovation lies in transforming discrete physical variations into continuous optimization problems. First, as shown in Fig. 1, we decompose real-world variations into three distinct domains: object 3D transformations parameterized with rotation angles(α\\alpha, β\\beta, γ\\gamma), illumination variations defined by point light parameters including position(xx, yy), radius(σ\\sigma), intensity(II), and adversarial patch placement specified by (Δ​x\\Delta x, Δ​y\\Delta y). This parameterization enables systematic exploration of the variation space while maintaining physical plausibility through explicit constraints. Second, to overcome the black-box nature of VLA models and non-differentiable simulation environments, we employ Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [7], a gradient-free optimization algorithm, to efficiently discover worst-case scenarios by iteratively optimizing physical variations parameters. This approach enables comprehensive vulnerability assessment without requiring model gradients or expensive real-world data collection.\n\nOur main contributions are as follows: ❶ To the best of our knowledge, we are the first to decompose real-world physical variations into three key domains—object 3D transformation, illumination variations, and adversarial patches—enabling a comprehensive evaluation of VLA robustness under these physical variations. ❷ We propose Eva-VLA, a novel framework that transforms discrete physical variations into continuous parameter optimization. By leveraging a simulator environment that allows us to reset to the same conditions, we ensure the repeatability and reliability of the evaluation process, which enables efficient exploration of worst-case scenarios without the need for expensive real-world data collection. ❸ Through extensive experiments on state-of-the-art model OpenVLA across multiple benchmarks, we expose significant fragility in current VLA systems, with failure rates exceeding 60% across all variation categories, with object transformations causing up to 97.8% failure in long-horizon tasks. These findings provide crucial insights for developing more robust VLA architectures and underscore the urgent need for improved robustness training methodologies.",
            "llm_summary": "【关注的是什么问题】\n1. 如何系统评估 Vision-Language-Action (VLA) 模型在真实世界物理变异下的鲁棒性（robustness）  \n2. 如何在不依赖大量真实数据的情况下发现最坏场景（worst-case scenarios）并保证评估可重复性  \n3. 如何将离散物理变异转化为连续优化问题以便可控探索  \n4. 如何覆盖三大变异域（object 3D transformation、illumination、adversarial patches）的全面评估  \n【用了什么创新的方案】\n核心方案：提出 Eva-VLA 框架，将离散物理变异转化为连续参数优化问题；将变异分解为 object 3D transformations、illumination variations、adversarial patches 三大域，采用 CMA-ES（协方差矩阵适应进化策略）进行黑盒优化以高效发现 worst-case 场景，并在可复现的仿真环境中重置到相同条件进行评估，从而在无需梯度信息或大量真实数据的情况下系统评估 VLA 的鲁棒性。通过对 OpenVLA 在多基准上的大规模实验，揭示了超过 60% 的失败率，且对象变换在长时任务中可达 97.8% 的失败率，强调了现实部署中的鲁棒性缺口与改进必要性。  \n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands",
            "authors": "Yanyuan Qiao,Kieran Gilday,Yutong Xie,Josie Hughes",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18937",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18937",
            "arxiv_html_link": "https://arxiv.org/html/2509.18937v1",
            "abstract": "Designing robotic hand morphologies for diverse manipulation tasks requires balancing dexterity, manufacturability, and task-specific functionality. While open-source frameworks and parametric tools support reproducible design, they still rely on expert heuristics and manual tuning. Automated methods using optimization are often compute-intensive, simulation-dependent, and rarely target dexterous hands. Large language models (LLMs), with their broad knowledge of human-object interactions and strong generative capabilities, offer a promising alternative for zero-shot design reasoning. In this paper, we present Lang2Morph, a language-driven pipeline for robotic hand design. It uses LLMs to translate natural-language task descriptions into symbolic structures and OPH-compatible parameters, enabling 3D-printable task-specific morphologies. The pipeline consists of: (i) Morphology Design, which maps tasks into semantic tags, structural grammars, and OPH-compatible parameters; and (ii) Selection and Refinement, which evaluates design candidates based on semantic alignment and size compatibility, and optionally applies LLM-guided refinement when needed. We evaluate Lang2Morph across varied tasks, and results show that our approach can generate diverse, task-relevant morphologies. To our knowledge, this is the first attempt to develop an LLM-based framework for task-conditioned robotic hand design.",
            "introduction": "Designing the morphology and structure of a robotic hand for a specific application is a fundamental yet long-standing challenge in robotics.\nUnlike the design of general-purpose manipulators, for example anthropomorphic robotic hands, task-specific robotic hands must balance dexterity and generality with optimization for a specific task, whilst remaining manufacturable [1, 2].\nThe design of dexterous, yet tasks-specific hands requires a full understanding of the task, and also fabrication constraints. For example, whilst increasing finger count or joint complexity improves capability but complicates fabrication and control.\nAs a result, tasks specific hand design has typically relied on expert-driven heuristics and iterative prototyping, which are time-consuming and hard to scale.\n\nThe development of parametric hand designs can provide a design space for task-specific hand morphology design.\nOpen-source hardware projects such as the Yale OpenHand Project [3, 4] have released reproducible underactuated hand designs, lowering the barrier for building functional prototypes. Parametric frameworks like the Open Parametric Hand (OPH) [5] further expose a structured design space with tunable parameters, enabling systematic exploration of hand morphologies.\nHowever, both approaches still depend on human designers to interpret tasks, identify likely grasp types, and manually adjust parameters. Without automated mapping from task requirements to design instantiations, scaling to diverse, task-specific morphologies remains difficult.\n\nIn parallel, the robotics community has explored automated robot morphology optimization and generation through evolutionary optimization, grammar-based synthesis, and differentiable pipelines [6, 7, 8, 9].\nThese works demonstrate that morphology can indeed be generated algorithmically, but they are often computationally expensive, require carefully crafted objective functions, and are typically coupled to physics-based simulators and rely on the curation of algorithmic representation of a fitness function or objective.\nMoreover, they have rarely targeted dexterous hand design, where functional requirements such as fingertip precision, lateral pinching, or stabilizing support are used to determine the resulting morphology.\nThus, these approaches lacks generality and relies on expert identification of key functional metrics or objective functions.\n\nRecently, large language models (LLMs) have been explored in robotics, with applications in navigation and manipulation where language is grounded into action policies [10, 11].\nWhilst these efforts focus mainly on control and planning,\nLLMs are not only capable of semantic reasoning but also encode broad background knowledge across biology, engineering, and everyday practice, which traditional optimization-based methods typically lack.\nThis combination makes them promising candidates for reasoning about form and functionality in design.\nSome recent works have adapted LLMs for computer-aided design (CAD) [12, 13], translating text into parametric part models.\nYet these methods remain restricted to geometry generation and do not address morphology design, where task semantics such as lateral pinching or stabilizing support directly dictate structural choices.\nTo the best of our knowledge, the use of LLMs for robotic hand morphology design remains unexplored, motivating our study.\n\nIn this paper, we propose Lang2Morph, a language-driven pipeline for robotic hand morphology generation. Our key insight is that LLMs are well suited to reasoning about task semantics and mapping them into symbolic and geometric design representations. Unlike prior methods that rely on expert-driven heuristics or costly physics-based simulation, Lang2Morph leverages LLM reasoning and semantic feedback to achieve scalable, task-conditioned morphology generation, as illustrated in Fig. 1, where a user-provided instruction is mapped into a design rationale and a CAD-ready morphology.\n\nLang2Morph builds upon the Open Parametric Hand (OPH) framework [5], which defines a structured and fabricable design space for robotic hands. OPH supports single-piece 3D printing, allowing generated designs to be directly manufactured without additional assembly or simulation. This enables our method to output physically realizable morphologies from natural language instructions.\n\nSpecifically, the pipeline comprises two major modules: (i) LLM-powered Morphology Design, which performs dual-level task analysis (semantic and structural), followed by geometry parameterization and constraint-aware validation to produce OPH-compatible parameters; and (ii) LLM-Guided Selection and Refinement, which ranks rendered variants based on semantic alignment and size compatibility, and optionally provides design refinements through feedback.\nTogether, these components form an end-to-end pipeline that generates fabricable, task-adaptive morphologies directly from natural language task description.\n\nOur main contributions are as follows:\n\nWe present Lang2Morph, a novel framework that generates robotic hand morphologies from natural-language instructions using large language models.\n\nWe design a two-stage pipeline that combines symbolic grammar generation, geometric parameterization, and semantic feedback for task-adaptive hand design.\n\nWe explore the use of LLMs for early-stage robot morphology generation, offering a flexible alternative to expert tuning.\n\nWe evaluate our method on a range of manipulation tasks, demonstrating improved design validity, diversity, and semantic alignment.\n\n1. We present Lang2Morph, a novel framework that generates robotic hand morphologies from natural-language instructions using large language models.\n\n2. We design a two-stage pipeline that combines symbolic grammar generation, geometric parameterization, and semantic feedback for task-adaptive hand design.\n\n3. We explore the use of LLMs for early-stage robot morphology generation, offering a flexible alternative to expert tuning.\n\n4. We evaluate our method on a range of manipulation tasks, demonstrating improved design validity, diversity, and semantic alignment.",
            "llm_summary": "【关注的是什么问题】\n1. 如何从自然语言任务描述自动生成可直接制造的任务适应性机器人手形态\n2. 在确保可制造性与任务相关性的同时，减少对人工经验启发式设计的依赖\n3. 面向 dexterous 手掌的任务特定形态设计的高效自动化路径\n4. 将语言模型用于高层语义分析到几何参数化、结构约束的端到端设计\n\n【用了什么创新的方案】\nLang2Morph 使用大语言模型进行双层任务分析（语义与结构），将自然语言指令映射到 OPH 兼容的参数化几何；结合符号语法生成与几何参数化，通过语义对齐和尺寸兼容性评估进行候选选择与 refinement；实现从文本到可直接 3D 打印的任务特定形态的端到端流程，且无需仿真驱动。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation",
            "authors": "Masato Kobayashi,Thanpimon Buamanee",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18865",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18865",
            "arxiv_html_link": "https://arxiv.org/html/2509.18865v1",
            "abstract": "We propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral control-based imitation learning to handle more than one task within a single model. Conventional bilateral control methods exploit joint angle, velocity, torque, and vision for precise manipulation but require task-specific models, limiting their generality.\nBi-VLA overcomes this limitation by utilizing robot joint angle, velocity, and torque data from leader-follower bilateral control with visual features and natural language instructions through SigLIP and FiLM-based fusion.\nWe validated Bi-VLA on two task types: one requiring supplementary language cues and another distinguishable solely by vision. Real-robot experiments showed that Bi-VLA successfully interprets vision-language combinations and improves task success rates compared to conventional bilateral control-based imitation learning.\nOur Bi-VLA addresses the single-task limitation of prior bilateral approaches and provides empirical evidence that combining vision and language significantly enhances versatility.\nExperimental results validate the effectiveness of Bi-VLA in real-world tasks.\nFor additional material, please visit the website: https://mertcookimg.github.io/bi-vla/",
            "introduction": "Robotic manipulation is increasingly important in human-centered applications such as cooking, eldercare, and interactive service robots [1, 2, 3, 4].\nUnlike traditional industrial robots that excel at repetitive and pre-programmed routines, service and collaborative robots must adapt to dynamic environments and interact with objects of diverse shapes, sizes, and material properties [5, 6]. Achieving such adaptability requires learning frameworks capable of acquiring human-like manipulation strategies [7].\n\nImitation learning (IL) has emerged as a promising approach for transferring human manipulation skills directly to robots [8].\nLeader-follower teleoperation has become a common pipeline for collecting demonstrations. For example, ALOHA and Mobile ALOHA use position-based unilateral control to gather diverse datasets that enable a wide range of manipulation [9, 10].\nAlthough effective for kinematics-driven tasks, such unilateral control omits force feedback, which limits robustness in contact-rich interactions.\n\nBilateral control-based imitation learning addresses these limitations by exchanging both position and force information between the demonstrator and robot [11, 12].\nBilateral control allows demonstrators to feel contact forces directly, yielding richer demonstrations and improving generalization across objects with different hardness and weights.\n\nBuilding on this foundation, recent methods have explored combining bilateral control with modern architectures.\nBilateral Control-Based Imitation Learning via Action Chunking with Transformers (Bi-ACT) [13] integrates bilateral control with visual observations via Transformers, yielding improved manipulation accuracy.\nHowever, it remains restricted to single-task settings, limiting its practicality in dynamic environments where multiple tasks must be handled seamlessly.\nIn parallel, Bilateral Control-Based Imitation Learning via Natural Language and Action Chunking with Transformers (Bi-LAT) [14] introduced natural language instructions into bilateral control-based imitation learning, demonstrating effective force modulation in manipulation.\nWhile this work highlights the promise of language integration, it focuses on regulating applied force and does not address the challenge of enabling a single model to adapt to multiple task contexts.\n\nIn this paper, we propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that unifies robot joint angle, velocity, and torque data from bilateral control with visual and language features, as shown in Fig. 1.\nBy leveraging SigLIP-based text embeddings [15] and FiLM-based EfficientNet feature fusion [16], Bi-VLA learns a shared representation of vision and language aligned with robot state information. Unlike prior bilateral frameworks, Bi-VLA is designed to handle multiple tasks within a single model, enabling flexible task switching without retraining or manual model selection.\n\nThe main contributions of this paper are summarized as follows:\n\nWe propose Bi-VLA, the bilateral control-based imitation learning framework that fuses vision and language features into a unified representation.\n\nWe demonstrate that Bi-VLA enables a single model to perform multiple tasks, overcoming the single-task limitation inherent in prior bilateral control-based imitation learning approaches.\n\nWe validate Bi-VLA through real-robot experiments on two distinct tasks, showing improved performance and adaptability compared to conventional methods.\n\n1. We propose Bi-VLA, the bilateral control-based imitation learning framework that fuses vision and language features into a unified representation.\n\n2. We demonstrate that Bi-VLA enables a single model to perform multiple tasks, overcoming the single-task limitation inherent in prior bilateral control-based imitation learning approaches.\n\n3. We validate Bi-VLA through real-robot experiments on two distinct tasks, showing improved performance and adaptability compared to conventional methods.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在单一模型中实现多任务的双边控制模仿学习与任务切换（≤40词）\n2. 如何将视觉与语言信息与机器人关节角度、速度、力矩数据融合以提升操控鲁棒性（≤40词）\n3. 如何在现实机器人环境中验证视觉-语言融合对多任务泛化的提升（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：在双边控制基础上，结合 SigLIP 文本嵌入和 FiLM 机制对 EfficientNet 提取的视觉特征进行融合，形成一个统一的视觉-语言-状态表示；通过该表示实现单模型处理多任务、并通过任务切换无需重新训练；在两类任务与真实机器人上验证性能提升。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation",
            "authors": "Suzannah Wistreich,Baiyu Shi,Stephen Tian,Samuel Clarke,Michael Nath,Chengyi Xu,Zhenan Bao,Jiajun Wu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "Accepted to CoRL 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18830",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18830",
            "arxiv_html_link": "https://arxiv.org/html/2509.18830v1",
            "abstract": "Human skin provides a rich tactile sensing stream, localizing intentional and unintentional contact events over a large and contoured region.\nReplicating these tactile sensing capabilities for dexterous robotic manipulation systems remains a longstanding challenge.\nIn this work, we take a step towards this goal by introducing DexSkin. DexSkin is a soft, conformable capacitive electronic skin that enables sensitive, localized, and calibratable tactile sensing, and can be tailored to varying geometries.\nWe demonstrate its efficacy for learning downstream robotic manipulation by sensorizing a pair of parallel jaw gripper fingers, providing tactile coverage across almost the entire finger surfaces.\nWe empirically evaluate DexSkin’s capabilities in learning challenging manipulation tasks that require sensing coverage across the entire surface of the fingers, such as reorienting objects in hand and wrapping elastic bands around boxes, in a learning-from-demonstration framework. We then show that, critically for data-driven approaches, DexSkin can be calibrated to enable model transfer across sensor instances, and demonstrate its applicability to online reinforcement learning on real robots.\nOur results highlight DexSkin’s suitability and practicality for learning real-world, contact-rich manipulation.\nPlease see our project webpage for videos and visualizations: https://dex-skin.github.io/.",
            "introduction": "Tactile feedback is essential for robust and dexterous manipulation in natural and artificial systems. In humans, mechanoreceptors within the skin provide a rich sensory stream that guides tasks ranging from handling delicate objects to using tools with force [1]. This tactile feedback enables more precise reactive control than can be achieved with human and proprioceptive feedback alone [2].\n\nEmulating this tactile sensitivity in robotic systems has long been a challenge, starting from sensing hardware. Research efforts typically focus on rigid or partially flexible sensors that offer low spatial coverage, limited adaptability, and poor conformability to complex surfaces. In contrast, everyday tasks such as rotating a key in-hand or picking up delicate berries require sensing coverage in multiple contact regions.\nSimilarly, wrapping an elastic band around an object requires a sensor to detect, distinguish, and localize dynamic contact events on all surfaces of a human hand or end effector accurately, applying the appropriate force without allowing the band to slip.\n\nLearning-based approaches offer a general way to harness tactile sensing information. However, data-driven systems pose requirements beyond sensing coverage. Seemingly small distribution shifts in sensor readings from wear or replacement can render previously trained models unusable, thus, output signals must be replicable across sensor hardware instances. Additionally, for real-world online learning, sensors must remain consistent, durable, and precise under repeated stresses during trial-and-error interaction, and output interpretable signals amenable to reward or cost specification.\n\nTo address these challenges, we introduce a novel soft tactile skin named DexSkin that is particularly suitable for robotic learning applications. DexSkin can be conformably integrated onto robotic end-effectors with unparalleled spatial coverage.\nIt is based on a capacitive mechanism, and features high sensitivity and robustness under repeated interactions.\nBecause each of the dozens of taxels on the skin are individually addressable, DexSkin can localize and characterize simultaneous contacts from distinct regions.\nIt can also be calibrated to provide consistent readings across distinct sensor units, enabling re-use of learned networks. At the same time, it can withstand deformations encountered during typical dexterous tasks such as pinching, twisting, and bending.\n\nIn this work, we introduce the DexSkin framework and its fabrication, as well as a representative integration with a soft cylindrical robotic fingertip that sensorizes the distal dome of the finger and 294∘ of the circumference. Then, we evaluate DexSkin’s applicability to robot learning. First, we test whether DexSkin’s coverage and tailorability expands the range of learnable manipulation tasks. Then, we evaluate its calibration performance, which are critical for working with and transferring learned tactile models. Finally, we demonstrate its suitability for online robotic learning settings by performing real-world reinforcement learning for a delicate object picking task. The results highlight DexSkin’s robust applicability to a wide range of robotic tasks and morphologies, and its particular practicality for robot learning researchers and practitioners.",
            "llm_summary": "【关注的是什么问题】\n1. 需要大面积、可 conform 的触觉传感覆盖以实现高鲁棒的接触丰富操控\n2. 如何在多传感单元上获得高灵敏、可标定、跨传感器的一致性读取\n3. 在 learns-from-demonstration 与 在线强化学习 场景中实现对不同几何和材料的可转移性\n4. 传感硬件对姿态、变形的耐久性及对数据分布偏移的鲁棒性\n\n【用了什么创新的方案】\nDexSkin 是一种软性、可 conform 的电容式电子皮肤，具有大量独立可寻址的触觉单元（taxels），可覆盖手指表面且对变形鲁棒；通过逐个触觉单元标定实现跨传感器的一致性，使已学习的模型可在不同传感器实例间转移；支持对多点同时接触的定位与表面广覆盖的感知，适用于从演示学习到在线强化学习的任务。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations",
            "authors": "Lukas Zanger,Bastian Lampe,Lennart Reiher,Lutz Eckstein",
            "subjects": "Robotics (cs.RO); Multiagent Systems (cs.MA); Software Engineering (cs.SE)",
            "comment": "7 pages, 2 figures, 2 tables; Accepted to be published as part of the 2025 IEEE International Conference on Intelligent Transportation Systems (ITSC 2025), Gold Coast, Australia, November 18-21, 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18793",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18793",
            "arxiv_html_link": "https://arxiv.org/html/2509.18793v1",
            "abstract": "Vehicles are becoming increasingly automated and interconnected, enabling the formation of cooperative intelligent transport systems (C-ITS) and the use of offboard services. As a result, cloud-native techniques, such as microservices and container orchestration, play an increasingly important role in their operation.\nHowever, orchestrating applications in a large-scale C-ITS poses unique challenges due to the dynamic nature of the environment and the need for efficient resource utilization.\nIn this paper, we present a demand-driven application management approach that leverages cloud-native techniques – specifically Kubernetes – to address these challenges. Taking into account the demands originating from different entities within the C-ITS, the approach enables the automation of processes, such as deployment, reconfiguration, update, upgrade, and scaling of microservices.\nExecuting these processes on demand can, for example, reduce computing resource consumption and network traffic.\nA demand may include a request for provisioning an external supporting service, such as a collective environment model.\nThe approach handles changing and new demands by dynamically reconciling them through our proposed application management framework built on Kubernetes and the Robot Operating System (ROS 2).\nWe demonstrate the operation of our framework in the C-ITS use case of collective environment perception and make the source code of the prototypical framework publicly available at https://github.com/ika-rwth-aachen/application_manager.",
            "introduction": "In future cooperative intelligent transport systems (C-ITS), various entities, such as vehicles equipped with driving automation systems, sensor-equipped roadside infrastructure units, edge/cloud servers, and control centers, will be connected, exchange data, and may offer computational resources.\nThese advancements enable new applications – such as collective environment perception, cooperative decision-making, computation offloading, and intelligent traffic management – that can contribute to improved comfort and safety for road users [1], [2], [3].\nNot only cloud and edge servers but also vehicles and roadside units can be part of a distributed computing system.\nHowever, these applications may also introduce complexity that is difficult to manage. The dynamic nature of C-ITS, the presence of resource-constrained entities, and the strict requirements for safety and security pose unique challenges.\n\nCloud-native techniques provide a promising foundation for the development and operation of scalable applications in dynamic environments. Such techniques involve paradigms like containerization, microservice architectures, and container orchestration. They enable loosely coupled systems which are manageable and resilient [4]. Said techniques and paradigms have the potential to contribute to the advancement of C-ITS.\n\nKubernetes has evolved as the de facto standard for orchestrating containerized applications in distributed systems. It is open-source and widely adopted by software companies worldwide [5].\nNevertheless, Kubernetes lacks methods that are domain-specific, e.g., to C-ITS, considering that specific tasks like the deployment of required applications are only needed at certain times or may depend on the specific content of data exchanged in the C-ITS.\nWe have developed the approach RobotKube [6] to extend the regular capabilities of Kubernetes. RobotKube comprises software components designed to automate the identification of requirements and the formulation of specific Kubernetes workloads.\nThese components include the event detector and the application manager.\n\nIn this paper, we propose a demand-driven application management approach and present the methodology behind the application manager as part of an application management framework.\nThis methodology integrates seamlessly into the RobotKube architecture and complements parts of RobotKube which were not detailed yet.\nThe application management framework – comprising the application manager and a set of custom operators – addresses the orchestration challenges in C-ITS through a demand-driven approach.\nIn this context, applications are deployed, reconfigured, scaled, and updated based on the current demands of C-ITS entities.\n\nWith our work, we make the following main contributions:\n\nPresentation of the methodology for demand-driven application management allowing to deploy, reconfigure, update, upgrade, and scale applications based on demands of entities in a C-ITS.\n\nPrototypical implementation of the application manager and the custom operators in an application management framework based on Kubernetes and ROS 2.\n\nDemonstration and evaluation of the capabilities of the application management framework in the complex C-ITS use case of collective environment perception involving various C-ITS entities.\n\nOpen source code publication of the application management framework and the demonstration use case allowing for reproducibility and extensibility.\n\nComplementation of RobotKube [6] by providing the concrete methodology of the application manager.\n\n1. Presentation of the methodology for demand-driven application management allowing to deploy, reconfigure, update, upgrade, and scale applications based on demands of entities in a C-ITS.\n\n2. Prototypical implementation of the application manager and the custom operators in an application management framework based on Kubernetes and ROS 2.\n\n3. Demonstration and evaluation of the capabilities of the application management framework in the complex C-ITS use case of collective environment perception involving various C-ITS entities.\n\n4. Open source code publication of the application management framework and the demonstration use case allowing for reproducibility and extensibility.\n\n5. Complementation of RobotKube [6] by providing the concrete methodology of the application manager.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在动态且资源受限的C-ITS环境中对应用进行按需驱动的管理与部署（≤40词）\n2. 如何将云原生技术（Kubernetes、ROS 2、微服务）整合用于C-ITS中的应用编排与再配置（≤40词）\n3. 如何通过需求感知实现应用的部署、重配置、更新、升级与扩缩以优化资源与网络开销（≤40词）\n【用了什么创新的方案】\n核心解决方案：提出基于需求驱动的应用管理框架，结合 Kubernetes 与 ROS 2，开发应用管理器与自定义 Operator，用事件检测器识别需求并动态编排、部署、重配置、升级和扩缩，支持对外部服务等需求的 provisioning；框架作为 RobotKube 的扩展，公开源码以便复现与扩展。\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Human-Interpretable Uncertainty Explanations for Point Cloud Registration",
            "authors": "Johannes A. Gaus,Loris Schneider,Yitian Shi,Jongseok Lee,Rania Rayyes,Rudolph Triebel",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18786",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18786",
            "arxiv_html_link": "https://arxiv.org/html/2509.18786v1",
            "abstract": "In this paper, we address the point cloud registration problem, where well-known methods like ICP fail under uncertainty arising from sensor noise, pose‐estimation errors, and partial overlap due to occlusion. We develop a novel approach, Gaussian Process Concept Attribution (GP-CA), which not only quantifies registration uncertainty but also explains it by attributing uncertainty to well-known sources of errors in registration problems. Our approach leverages active learning to discover new uncertainty sources in the wild by querying informative instances. We validate GP-CA on three publicly available datasets and in our real-world robot experiment. Extensive ablations substantiate our design choices. Our approach outperforms other state-of-the-art methods in terms of runtime, high sample-efficiency with active learning, and high accuracy. Our real-world experiment clearly demonstrates its applicability. Our video also demonstrates that GP-CA enables effective failure-recovery behaviors, yielding more robust robotic perception.",
            "introduction": "Point cloud registration refers to the problem of estimating a relative transformation between two sets of 3D points [1]. This problem is essential in many robotic perception tasks, such as simultaneous localization and mapping (SLAM) [2], 3D reconstruction [3], and 6-DoF object pose estimation [4], to name a few. A widely used approach for point cloud registration is Iterative Closest Point (ICP) [5].\nHowever, in ICP, well-known causes\nof uncertainty are\nsensor noise, poor initialization of the optimization process of the relative transformation between the two point clouds, and insufficient overlap between the two point clouds (e.g., due to occlusions), which can make ICP inaccurate or unreliable in practice  [6]. Hence, several researchers attempt to improve the robustness of the registration process [7].\n\nAmong others, several probabilistic approaches were proposed to quantify uncertainty in point cloud registration [6, 8, 9, 10, 11]. The underlying idea is that uncertainty estimates provide insight into the reliability of the obtained results, enabling the identification and rejection of unreliable registrations. Moreover, information about uncertainty is often used in many downstream tasks, including sensor fusion, state estimation, and 3D reconstruction [6]. To this end, a variety of tools have been developed to quantify uncertainty in point cloud registration, ranging from closed-form Gaussian solvers [11] to particle-based methods [6].\n\nWhile quantifying uncertainty helps to improve the registration, in practice, the magnitude of uncertainty is rarely sufficient in practical robotic perception tasks, as it does not reveal why registration failed or how to recover from it. Distinct failure causes — such as sensor noise, poor initialization, or occlusion [6] — require different actions to recover from failure.\nIn this work, we propose a novel approach, Gaussian Process Concept Attribution (GP-CA). A major difference to existing work is that, our approach not only quantifies uncertainty, but also explains uncertainty in a human-interpretable manner. Our GP-CA integrates active learning, learned point-cloud representations, and a Gaussian Process classifier to provide concept-level uncertainty attribution. The active learning allows fast adaptation to new uncertainty sources.\nIn preliminary work [12], the explainability method SHAP was used to analyze the influence of different uncertainty sources on ICP-based registration. However, it cannot identify which uncertainty source is present in a registration and depends on manual control over uncertainty sources and heavy computation, limiting applicability and hindering its feasibility for robotic decision-making.\n\nGP-CA attributes uncertainty to semantically meaningful concepts that can be used later to enable robots to select targeted recovery actions. An example is depicted in Fig. 1, where the robot is tasked to perform a 6D pose estimation of objects using the ICP algorithm. Initially, an object of interest (the bottle) is occluded by another object, which causes the ICP algorithm to fail. Using our method, the occlusion is identified as the cause of failure, thereby enabling targeted recovery actions (e.g., changing the viewpoint).\nHence, our main idea of explaining uncertainty in point cloud registration can improve the robustness of the perception tasks.\n\nWhen registration fails or reports high uncertainty, GP-CA embeds the ICP-aligned source point cloud using a learned representation. Then, a multi-class Gaussian Process classifier (GPC) maps these representations to confidence scores over different concepts. In this way, we attribute the uncertainty to specific concepts, thereby identifying its source. The concepts are predefined by sets of examples. Still, a robot may encounter unknown sources of uncertainty during its operations. Therefore, GP-CA is equipped with an active learning mechanism [13, 14], which can learn a new concept by querying the user for the label and choosing the most informative data to learn from. Across experiments on the LINEMOD [15], the YCB [16], and the Coffee Cup [17] dataset and a custom real-world RGB-D dataset from our laboratory, we validate GP-CA and provide extensive ablation studies. We further propose how GP-CA can enable a more robust robotic perception by executing a set of failure recovery actions associated with uncertainty explanations.\n\nIn summary, our main contributions are as follows.\n\nWe propose a novel approach, GP-CA, which enables human-interpretable uncertainty explanations in point cloud registration. To the best of our knowledge, this is the first work to show how explainability can advance point cloud registration for real-world applications\n\nWe extend our approach with active learning to adapt and integrate new concepts.\n\nWe validate the GP-CA design through an extensive ablation study\n\nWe demonstrate clearly the runtime-efficiency, the high accuracy, and the sample-efficiency of our method, with SOTA baseline comparison across four datasets.\n\nWe propose how GP-CA can be employed for recovery actions in a real-world robot setting.\n\n1. We propose a novel approach, GP-CA, which enables human-interpretable uncertainty explanations in point cloud registration. To the best of our knowledge, this is the first work to show how explainability can advance point cloud registration for real-world applications\n\n2. We extend our approach with active learning to adapt and integrate new concepts.\n\n3. We validate the GP-CA design through an extensive ablation study\n\n4. We demonstrate clearly the runtime-efficiency, the high accuracy, and the sample-efficiency of our method, with SOTA baseline comparison across four datasets.\n\n5. We propose how GP-CA can be employed for recovery actions in a real-world robot setting.",
            "llm_summary": "【关注的是什么问题】\n1. 点云配准中的不确定性量化与原因解释（≤40词）\n2. 如何将不确定性解释转化为人类可理解的概念级因果归因（≤40词）\n3. 在实际机器人场景中实现快速、鲁棒的失败恢复策略（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出高斯过程概念归因GP-CA，结合点云嵌入、GPC分类器实现对不确定性的概念级归因，并通过主动学习发现新不确定源；可用于在ICP失败时给出针对性的恢复操作，提升运行时鲁棒性与效率。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models",
            "authors": "Shijia Ge,Yinxin Zhang,Shuzhao Xie,Weixiang Zhang,Mingcai Zhou,Zhi Wang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "submitted to AAAI 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.18778",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18778",
            "arxiv_html_link": "https://arxiv.org/html/2509.18778v1",
            "abstract": "Visual imitation learning frameworks allow robots to learn manipulation skills from expert demonstrations. While existing approaches mainly focus on policy design, they often neglect the structure and capacity of visual encoders—limiting spatial understanding and generalization. Inspired by biological vision systems, which rely on both visual and proprioceptive cues for robust control, we propose VGGT-DP, a visuomotor policy framework that integrates geometric priors from a pretrained 3D perception model with proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer (VGGT) as the visual encoder and introduce a proprioception-guided visual learning strategy to align perception with internal robot states, improving spatial grounding and closed-loop control. To reduce inference latency, we design a frame-wise token reuse mechanism that compacts multi-view tokens into an efficient spatial representation. We further apply random token pruning to enhance policy robustness and reduce overfitting. Experiments on challenging MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines such as DP and DP3, particularly in precision-critical and long-horizon scenarios.",
            "introduction": "未获取到引言",
            "llm_summary": "【关注的是什么问题】\n1. 如何将视觉编码器的结构与能力用于提升机器人视觉-动作嵌入的泛化和精确控制\n2. 如何融合预训练3D感知模型的几何先验与机器人本体的 proprioceptive 信息以改善空间定位与闭环控制\n3. 如何降低多视角视觉输入的推理时延并提升鲁棒性\n4. 如何通过 token 重用和随机剪枝提升效率与抗过拟合能力\n【用了什么创新的方案】\n- 以 Visual Geometry Grounded Transformer (VGGT) 作为视觉编码器，将几何先验整合到 visuomotor 策略中\n- 引入 proprioception-guided visual learning，使感知对齐内部状态并增强空间着陆与闭环控制\n- 设计 frame-wise token reuse 机制，将多视图 token 压缩为高效的空间表示以降低推理延迟\n- 采用随机 token pruning 增强策略鲁棒性并抑制过拟合\n- 在 MetaWorld 任务上对比 DP、DP3，显示显著提升，尤其在精度关键与长时程场景\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning",
            "authors": "Omar Rayyan,John Abanes,Mahmoud Hafez,Anthony Tzes,Fares Abu-Dakka",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "For project website and videos, see httpsthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18757",
            "code": "https://mv-umi.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18757",
            "arxiv_html_link": "https://arxiv.org/html/2509.18757v1",
            "abstract": "Recent advances in imitation learning have shown great promise for developing robust robot manipulation policies from demonstrations. However, this promise is contingent on the availability of diverse, high-quality datasets, which are not only challenging and costly to collect but are often constrained to a specific robot embodiment. Portable handheld grippers have recently emerged as intuitive and scalable alternatives to traditional robotic teleoperation methods for data collection. However, their reliance solely on first-person view wrist-mounted cameras often creates limitations in capturing sufficient scene contexts. In this paper, we present MV-UMI (Multi-View Universal Manipulation Interface), a framework that integrates a third-person perspective with the egocentric camera to overcome this limitation. This integration mitigates domain shifts between human demonstration and robot deployment, preserving the cross-embodiment advantages of handheld data-collection devices. Our experimental results, including an ablation study, demonstrate that our MV-UMI framework improves performance in sub-tasks requiring broad scene understanding by approximately 47% across 3 tasks, confirming the effectiveness of our approach in expanding the range of feasible manipulation tasks that can be learned using handheld gripper systems, without compromising the cross-embodiment advantages inherent to such systems.\nVideos can be found here: https://mv-umi.github.io",
            "introduction": "Imitation Learning (IL) provides a compelling pathway toward acquiring general robot policies capable of performing long-horizon tasks across diverse environments. This approach, particularly through supervised methods like Behavioral Cloning (BC), enables robots to acquire complex behaviors by learning to imitate human-directed actions in response to observations. Recent advances in architectures that better model this mapping [1, 2, 3, 4], coupled with enhancements in embodiments and hardware integrations [5, 6], have made this route increasingly convincing.\n\nRecent studies on data scaling laws in imitation learning [7] show that robot policy performance follows training scenario diversity, emphasizing the need for extensive and diverse data for robust policies. Data collection typically lies between two extremes. On one end, robot teleoperation enables the acquisition of high-quality, precise data with minimal embodiment discrepancies. However, this method is time-consuming and costly, as it requires an actively operated robot. On the other end, the internet is replete with videos of humans performing various tasks. However, substantial effort is required to establish structured explicit mappings between observed states and actions from these videos.\n\nAs a middle-ground, portable handheld grippers [8, 9, 10, 11, 12] have emerged as inexpensive and intuitive to use data collection devices. By relying exclusively on a wrist-mounted camera, they enable non-experts to record demonstrations without the need for a robotic manipulator. While this egocentric viewpoint minimizes visual discrepancies between training and deployment, resulting in cross-embodiment policies, it demands that the robot maintain a longer memory context to recall scene elements that move out of the constraining wrist view.\n\nIn this work, we propose a novel framework that augments the conventional wrist-mounted camera in handheld gripper systems with a third-person camera viewpoint, without incurring distributional shifts. We achieve this by performing real-time masking of the human demonstrator in the third-person video stream, effectively removing the operator’s presence from the training data. As a result, the model benefits from a broader view of the environment, while relying less on memory for scene remembrance. A side benefit we find of this masking is its removal of correlations between the demonstrator’s motions and the gripper’s actions, encouraging the policy to focus on task-relevant cues such as the manipulated objects rather than overfitting to human-specific signals. We also utilize a custom-made three-jaw gripper for some of the tasks that require greater dexterity in this work. This design allows for greater payload weight in comparison to other hand-held devices, at the cost of its volume. Schematics and instructions to reproduce the hardware are open-sourced separately. The hardware aspect is not a key focus in this paper.\n\nSummary of Contributions:\n\nMulti-View Cross-Embodiment Framework: MV-UMI fuses wrist-mounted and third-person views using SAM-2 segmentation and inpainting to eliminate domain shift, boosting performance in context-dependent tasks by 47%.\n\nEnd-to-End Open-Source System: Complete pipeline, including hardware design, data collection, training code, and deployment tools, is publicly released to advance cross-embodiment manipulation research, https://mv-umi.github.io.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在跨 embodiment 的示范学习中利用多视角来缓解域间差异（跨人体/设备）的影响\n2. 如何在手持抓取设备的数据集上融合第一视角和第三视角以提升上下文理解能力\n3. 如何通过去除示范者在第三视角中的影像来减少示范者动作与抓手动作的相关性\n4. 如何在不增加分布漂移的前提下扩展数据视角以提升对场景理解的鲁棒性\n5. 如何在跨视角学习中保持对任务相关 cues 的聚焦，避免过拟合人类信号\n\n【用了什么创新的方案】\n核心解决方案：在手持夹具的第一视角与第三视角之间进行实时多视图融合，利用 SAM-2 对人类示范者进行分割并在第三视角视频中进行去人化处理，从而获得更广的环境观察并降低域偏移；结合自定义三撬式抓手以提高耐受载荷，输出端到端的开放源码数据收集、训练与部署管线；通过掩码化第三视角实现对示范者的去域化，减少示范者动作与抓取动作的相关性，提升跨嵌入学习的任务自适应性。实验结果显示在需要广域场景理解的子任务上约提升47%。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation",
            "authors": "Nishant Doshi,Amey Sutvani,Sanket Gujar",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18734",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18734",
            "arxiv_html_link": "https://arxiv.org/html/2509.18734v1",
            "abstract": "One of the challenges faced by Autonomous Aerial\nVehicles is reliable navigation through urban environments.\nFactors like reduction in precision of Global Positioning System\n(GPS), narrow spaces and dynamically moving obstacles make the\npath planning of an aerial robot a complicated task. One of the\nskills required for the agent to effectively navigate through such an\nenvironment is to develop an ability to avoid collisions using\ninformation from onboard depth sensors. In this paper, we propose\nReinforcement Learning of a virtual quadcopter robot agent\nequipped with a Depth Camera to navigate through a simulated\nurban environment.",
            "introduction": "In recent years, Quadcopters have been extensively used\nfor civilian task like object tracking, disaster rescue, wildlife\nprotection and asset localization. It presents interesting\napplication avenues especially in tasks such as automated mail\ndelivery system, fire protection and disaster management.\nHowever, quadcopter navigation through urban environments\nis a complex task because of frequent dynamic obstacles\n(Humans, Posters, etc.). Also, the GPS navigation system can\nperform poorly when surrounded by tall buildings in urban\nenvironment, dilating the precision of the 3D position fix. It\nbecomes more dangerous when the quadcopter is flying\nthrough tight spaces and is uncertain of its position, increasing\nthe chances of collision. The quadcopter also needs to take\nsmart action after detecting dynamic obstacles (Humans,\nVehicles, animals, traffic signals etc.) during navigation in\nruntime in urban environment. Traditionally, obstacle\navoidance techniques have been designed as end point solution\nin an aerial robot navigation. One of the promising approach\nfor this problem is deep reinforcement learning. In this paper a\nsimple model is developed for the task of detecting and\navoiding common civilian obstacles encountered by a\nquadcopter while navigating a path in an urban environment.\n\nFrom the reinforcement learning view, the main challenge\nhere is that, the policy should update itself during runtime for\nstochastic obstacles detected in the environment and take the\noptimal action accordingly. Also, the navigation problem has\nsparse distributed reward in state space which is a challenge for\nlearning the shortest distance.\n\nThe objective of this project is to train a quadcopter to\nnavigate without hitting obstacles and taking a shortest path\naround through a high-rise urban environment where stochastic\nand dynamic obstacles are frequent.\n\nThe organization of the paper is as follows: Section I\nprovides a general introduction to the challenges for\nquadcopter urban navigation. Section II provides a\nprerequisites required to understand the experiments. Section\nIII defines the problem outlining the agent used and the\nenvironment. Section IV gives a brief description about the\nAirSim simulator, while section V defines the solution\napproaches for the problem defined. Section VI describes the\nexperiments and training and testing arena used. Section VII\ndiscusses the results for the experiments, while section VIII\ndescribes the future attempt that can be made and section IX\ndescribes the challenges faced during the experiments.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在城市环境中实现四旋翼无人机的可靠导航。  \n2. 如何处理动态障碍物对无人机路径规划的影响。  \n3. 如何在稀疏奖励环境中优化深度强化学习策略。  \n\n【用了什么创新的方案】  \n本研究提出了一种基于深度强化学习的虚拟四旋翼机器人代理，利用深度摄像头在模拟城市环境中进行导航。该方法通过实时更新策略来应对环境中的随机障碍物，并优化路径选择以避免碰撞，同时解决了稀疏奖励的问题。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Query-Centric Diffusion Policy for Generalizable Robotic Assembly",
            "authors": "Ziyi Xu,Haohong Lin,Shiqi Liu,Ding Zhao",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "8 pages, 7 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.18686",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18686",
            "arxiv_html_link": "https://arxiv.org/html/2509.18686v1",
            "abstract": "The robotic assembly task poses a key challenge in building generalist robots due to the intrinsic complexity of part interactions and the sensitivity to noise perturbations in contact-rich settings. The assembly agent is typically designed in a hierarchical manner: high-level multi-part reasoning and low-level precise control. However, implementing such a hierarchical policy is challenging in practice due to the mismatch between high-level skill queries and low-level execution. To address this, we propose the Query-centric Diffusion Policy (QDP), a hierarchical framework that bridges high-level planning and low-level control by utilizing queries comprising objects, contact points, and skill information. QDP introduces a query-centric mechanism that identifies task-relevant components and uses them to guide low-level policies, leveraging point cloud observations to improve the policy’s robustness. We conduct comprehensive experiments on the FurnitureBench in both simulation and real-world settings, demonstrating improved performance in skill precision and long-horizon success rate. In the challenging insertion and screwing tasks, QDP improves the skill-wise success rate by over 50% compared to baselines without structured queries.",
            "introduction": "Contact-rich manipulation has been widely recognized as a critical task when building generalist intelligent robots [1, 2, 3]. In this field, robot assembly [4, 5] stands out because it requires policies that are both precise and versatile to control the robot arm and interact with multiple objects.\nDespite the access to some offline demonstration data from human priors, robotic assembly poses two key challenges: inter-part relational reasoning and intra-part precise control in the online deployment. As is visualized in Figure 1, the first challenge arises from the long-horizon, multi-part nature of the task, which demands accurate prediction of the next skill based on current observations, as well as identifying the correct objects for interaction. The second challenge becomes especially difficult in contact-rich scenarios where successful assembly hinges on precise object alignment. Even minor noise or occlusions in raw sensory observations can completely fail the sim-to-real transfer of low-level non-prehensile control policies.\n\nVarious methods have been explored to address these challenges. One line of work focuses on high-level reasoning by leveraging cross-modality affordance-based approaches [6], graph-based key point reasoning [7], or skill-based retrieval [8, 9]. However, these methods rely heavily on heuristic-based low-level controllers and predefined skill libraries, thus limiting their adaptability and precision.\nMeanwhile, recent advances in robot learning have significantly improved the precision and adaptability of low-level policies, thanks to (i) powerful imitation learning backbones such as diffusion models [10, 11, 12] and transformer-based architectures [13, 14], (ii) enhanced learning regimes like residual policies [15] and safe failure prevention methods [16], and (iii) the integration of sensor modalities beyond vision and proprioception, such as tactile sensing [17, 9] or point clouds [18] for improved contact modeling.\nStill, transferring these policies from simulation to the real world remains challenging due to the inherent difficulty of accurately simulating contact dynamics.\n\nIn addition to the individual challenges of high-level and low-level policy design, integrating these two levels in a hierarchical framework introduces further complications. For instance, high-level policies may mis-specify objects or skills, causing the low-level policy to rely on the false queries that include irrelevant factors such as background pixels or non-impactful objects, ultimately leading to failure in real-world environments. Hence, it is crucial to establish a parsimonious proxy between high-level and low-level modules.\n\nTo address these challenges and establish a robust interface between high-level and low-level modules, we propose Query-centric Diffusion Policy (QDP). Our framework leverages powerful pre-trained foundation models to extract high-level information by specifying both the requisite skills and target objects as a query. This query then serves as a powerful precondition to guide point cloud-based low-level control, forcing the robot agent to focus on the current task-relevant components when generating the action chunks under different contexts.\nOur contributions are threefold:\n\nWe introduce QDP, a query-centric diffusion policy framework that selects task-relevant queries for guiding low-level policies, enabling accurate skill selection and precise object interaction.\n\nWe introduce QDP, a query-centric diffusion policy framework that selects task-relevant queries for guiding low-level policies, enabling accurate skill selection and precise object interaction.\n\nWe propose a query-conditioned policy learning scheme to model the complex geometry captured by point cloud observations, improving precision and facilitating sim-to-real transfer.\n\nWe demonstrate the effectiveness of our approach on FurnitureBench in both simulation and real-world settings, showcasing robust performance under object misalignment and human intervention.\n\n1. We introduce QDP, a query-centric diffusion policy framework that selects task-relevant queries for guiding low-level policies, enabling accurate skill selection and precise object interaction.\n\n2. We propose a query-conditioned policy learning scheme to model the complex geometry captured by point cloud observations, improving precision and facilitating sim-to-real transfer.\n\n3. We demonstrate the effectiveness of our approach on FurnitureBench in both simulation and real-world settings, showcasing robust performance under object misalignment and human intervention.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂的机器人组装任务中实现高层次的多部件推理与低层次的精确控制。  \n2. 如何解决在接触丰富的环境中，低层次控制策略的噪声敏感性和仿真到现实的转移问题。  \n3. 如何有效整合高层和低层策略，以避免错误的对象或技能指定导致的失败。  \n\n【用了什么创新的方案】  \n提出了Query-centric Diffusion Policy (QDP)框架，通过查询机制选择任务相关的组件，指导低层策略的执行。QDP利用点云观察来增强政策的鲁棒性，改善技能选择的准确性和物体交互的精确性。该框架在FurnitureBench上进行了全面实验，展示了在技能精度和长时间成功率上的显著提升。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space",
            "authors": "Sangjun Noh,Dongwoo Nam,Kangmin Kim,Geonhyup Lee,Yeonguk Yu,Raeyoung Kang,Kyoobin Lee",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "7 main scripts + 2 reference pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18676",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18676",
            "arxiv_html_link": "https://arxiv.org/html/2509.18676v1",
            "abstract": "Learning robust visuomotor policies that generalize across diverse objects and interaction dynamics remains a central challenge in robotic manipulation. Most existing approaches rely on direct observation-to-action mappings or compress perceptual inputs into global or object-centric features, which often overlook localized motion cues critical for precise and contact-rich manipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework that leverages scene-level 3D flow as a structured intermediate representation to capture fine-grained local motion cues. Our approach predicts the temporal trajectories of sampled query points and conditions action generation on these interaction-aware flows, implemented jointly within a unified diffusion architecture. This design grounds manipulation in localized dynamics while enabling the policy to reason about broader scene-level consequences of actions. Extensive experiments on the MetaWorld benchmark show that 3D FDP achieves state-of-the-art performance across 50 tasks, particularly excelling on medium and hard settings. Beyond simulation, we validate our method on eight real-robot tasks, where it consistently outperforms prior baselines in contact-rich and non-prehensile scenarios. These results highlight 3D flow as a powerful structural prior for learning generalizable visuomotor policies, supporting the development of more robust and versatile robotic manipulation. Robot demonstrations, additional results, and code can be found at https://sites.google.com/view/3d-fdp.",
            "introduction": "Learning robust manipulation skills in unstructured environments is a fundamental challenge in robotics. Policies deployed in such settings must generalize across diverse object geometries, appearances, dynamics, and contextual variations. Visual imitation learning offers a scalable alternative by enabling robots to acquire visuomotor skills from expert demonstrations without requiring task-specific reward functions [1, 2]. Recent approaches [3, 4, 5, 6, 7] often employ end-to-end architectures that map perception directly to control, leveraging transformers whose attention mechanisms excel at capturing global context and integrating multiple modalities.\n\nMore recently, diffusion models have shown significant promise in this domain [8, 4]. By formulating action generation as an iterative denoising process, they provide stable training and support complex, multimodal action distributions. When combined with structured inputs such as point clouds, these models achieve strong performance across diverse manipulation tasks [9, 10, 11]. Despite these advances, most existing methods still follow a direct observation-to-action mapping, often compressing perceptual inputs into global or object-centric features. Such representations can overlook localized motion cues that are essential for precise and contact-rich manipulation. While some works introduce intermediate representations, approaches based on object poses [12, 13] can struggle to capture scene-level dynamics, and those predicting future frames [14, 15, 16, 17, 18, 19] are often computationally intensive.\n\nTo address these limitations, we introduce the 3D Flow Diffusion Policy (3D FDP), an architecture that explicitly models local interaction dynamics through an intermediate representation of 3D scene flow. Our method predicts the temporal trajectories of sampled 3D query points and uses this predicted flow to infer actions. Unlike prior approaches [12, 20] that rely on object-level representations, our model learns both flow prediction and action generation jointly within a unified diffusion framework (Fig. 1). This integrated design offers two key advantages. First, 3D flow captures fine-grained correlations between the robot gripper and manipulated objects, providing spatial grounding that supports contact-aware behavior. Second, by modeling how local motion propagates across the scene, the policy can reason about the downstream effects of manipulation, such as how inserting a book might disturb surrounding objects on a cluttered shelf. These capabilities improve generalization across both geometric and dynamic variations.\n\nWe evaluate 3D FDP on the MetaWorld benchmark [21] and a set of real-world manipulation tasks. On MetaWorld, our approach achieves state-of-the-art performance across all difficulty levels, with particularly strong improvements on medium and hard tasks. In real-robot experiments involving diverse physical interactions, 3D FDP demonstrates capabilities beyond baseline policies, successfully handling contact-rich and non-prehensile tasks where previous methods fail. These results demonstrate that 3D scene flow provides an effective inductive bias for learning generalizable visuomotor policies. Our main contributions are as follows:\n\nWe introduce a scene-level 3D flow representation that captures interaction-aware motion throughout the scene.\n\nWe develop a unified diffusion-based architecture that jointly learns 3D flow prediction and action generation in an end-to-end manner.\n\nWe demonstrate that 3D FDP achieves state-of-the-art results on 50 MetaWorld tasks and outperforms prior baselines in real-world manipulation scenarios.\n\nWe provide a comprehensive analysis that demonstrates the effectiveness of our 3D flow representation, investigating key aspects such as conditioning strategies and query point density.\n\n1. We introduce a scene-level 3D flow representation that captures interaction-aware motion throughout the scene.\n\n2. We develop a unified diffusion-based architecture that jointly learns 3D flow prediction and action generation in an end-to-end manner.\n\n3. We demonstrate that 3D FDP achieves state-of-the-art results on 50 MetaWorld tasks and outperforms prior baselines in real-world manipulation scenarios.\n\n4. We provide a comprehensive analysis that demonstrates the effectiveness of our 3D flow representation, investigating key aspects such as conditioning strategies and query point density.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何学习能够在多样化对象和交互动态中泛化的稳健视觉运动策略。  \n2. 现有方法如何忽视局部运动线索，影响接触丰富的操控精度。  \n3. 如何有效地结合3D场景流与动作生成以改善操控性能。  \n\n【用了什么创新的方案】  \n我们提出了3D Flow Diffusion Policy (3D FDP)，一种新颖的框架，通过场景级3D流作为结构化中间表示，捕捉细粒度的局部运动线索。该方法预测采样查询点的时间轨迹，并基于这些交互感知流条件生成动作，所有这些都在统一的扩散架构中共同实现。此设计使操控 grounded 在局部动态上，同时使策略能够推理动作的更广泛场景级后果。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout",
            "authors": "Kaixin Chai,Hyunjun Lee,Joseph J. Lim",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18671",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18671",
            "arxiv_html_link": "https://arxiv.org/html/2509.18671v1",
            "abstract": "In mobile manipulation, the manipulation policy has strong preferences for initial poses where it is executed. However, the navigation module focuses solely on reaching the task area, without considering which initial pose is preferable for downstream manipulation.\nTo address this misalignment, we introduce N2M, a transition module that guides the robot to a preferable initial pose after reaching the task area, thereby substantially improving task success rates. N2M features five key advantages: (1) reliance solely on ego-centric observation without requiring global or historical information; (2) real-time adaptation to environmental changes; (3) reliable prediction with high viewpoint robustness; (4) broad applicability across diverse tasks, manipulation policies, and robot hardware; and (5) remarkable data efficiency and generalizability.\nWe demonstrate the effectiveness of N2M through extensive simulation and real-world experiments.\nIn the PnPCounterToCab task, N2M improves the averaged success rate from 3% with the reachability-based baseline to 54%.\nFurthermore, in the Toybox Handover task, N2M provides reliable predictions even in unseen environments with only 15 data samples, showing remarkable data efficiency and generalizability.\nProject website: https://clvrai.github.io/N2M/",
            "introduction": "Mobile manipulators, which integrate mobility and environmental interaction capabilities, hold significant promise for a wide range of real-world applications.\nBy leveraging scene understanding Rana et al. (2023); Hughes et al. (2022); Rosinol et al. (2020) and navigation modules Zheng et al. (2025); Chai et al. (2024); Chang et al. (2023), these robots can reach the task area based on the task descriptions, and subsequently accomplish the task by executing pre-trained manipulation policies Fu et al. (2024); Chi et al. (2024); Black et al. (2024).\n\nHowever, existing works mainly focus on enhancing navigation and manipulation independently, while not giving sufficient attention to the interplay between them.\nIn this paper, we identify an inherent misalignment between navigation and manipulation, which significantly reduces the task success rate.\nSpecifically, due to factors such as joint limitation and training data distribution, the performance of the manipulation policy is sensitive to the initial pose from which execution begins.\nMeanwhile, navigation merely focuses on guiding the robot to task areas without considering which initial pose is preferable for executing the manipulation policy.\n\nThe most direct solution would be to develop an end-to-end model handling both navigation and manipulation Yang et al. (2024), thereby avoiding challenges in inter-module coordination. However, due to the inherent complexity of both navigation and manipulation, the design, training, and data collection for such end-to-end models remain an open problem.\nAn alternative approach within modular frameworks is to enhance the robustness of the manipulation policy. However, visuomotor policies are sensitive to viewpoint changes Heo et al. (2023), necessitating data collection from various initial poses throughout the task area Gu et al. (2022), which is costly.\n\nIn this paper, we propose a simple but effective transition module, named N2M (Navigation-to-Manipulation), serving as a bridge between navigation and manipulation. As depicted in Fig. 1, after reaching the task area, the robot is transferred from the end pose of navigation to an initial pose that is preferable for executing the manipulation policy, thereby improving the task success rate.\n\nWe identify five fundamental challenges for bridging navigation and manipulation, and propose our corresponding solutions.\n\nAdaptability to non-static environments.\nThe environments are typically non-static, requiring predictions to adapt to environmental changes. To support this, N2M predicts the preferable initial pose from the ego-centric RGB point cloud with a single forward pass. This efficient design enables N2M to generate real-time predictions in dynamic environments, as demonstrated in Section 5.2.\n\nMulti-modality of preferable initial poses.\nMultiple preferable initial poses may exist within the task area. Consequently, predicting a single pose is insufficient, as it can cause the model to learn an interpolation between viable poses Bishop (1994), which may not be preferable to execute manipulation policies. To address this multi-modality, N2M predicts the distribution of preferable initial poses, which is represented with a Gaussian Mixture Model (GMM) Bahl et al. (2023).\n\nCriterion of preferable initial poses.\nManipulation performance depends on multiple factors: policy architecture, training data distribution, robot configuration, task, and environment. Rather than attempting to model these complex relationships, we directly evaluate the pose through policy rollouts.\nDuring data collection, we position the robot at various poses and execute the manipulation policy, and successful execution indicates a preferable initial pose.\nLearning initial pose preferences directly from policy rollouts ensures that N2M’s predictions align with the policy’s actual performance while simultaneously enabling broad applicability across diverse policies, tasks, and robot hardware, as shown in Sections 4.2, 5.1, and 5.2.\n\nViewpoint Robustness.\nSince the robot navigation end poses can be anywhere within the task area, N2M needs to provide reliable predictions at various viewpoints. To achieve this, we augment N2M’s training data from multiple viewpoints. Experiments in Sections 4 and 5 demonstrate that N2M reliably predicts preferable initial poses across the whole task area. Interestingly, we note that our proposed data augmentation approach also significantly improves data efficiency and generalizability. We will further analyze the reason behind these benefits in Section 6.\n\nData Efficiency.\nCollecting rollouts requires substantial time and human effort, as each rollout must be monitored and manually labeled with success or failure.\nWe incorporate two main strategies to make N2M data-efficient:\nFirst, we design the module to directly predict the initial pose distribution, rather than low-level action Lee et al. (2019);\nSecond, we augment the dataset through viewpoint rendering to increase its coverage and diversity. In Sections 4.3, 4.4, and 5, we demonstrate that N2M has remarkable data efficiency and generalizability.\n\nOur contributions are as follows:\n\nFirst, we identify a critical misalignment between navigation and manipulation modules and introduce N2M, which predicts preferable initial poses on the fly from ego-centric observations.\n\nSecond, we propose learning policy initial pose preferences from rollouts, without making additional assumptions about tasks, policies, and hardware, thereby endowing N2M with broad applicability.\n\nThird, we achieve remarkable data efficiency and generalizability through a novel data augmentation approach combined with our carefully designed input-output architecture.\n\nFinally, we conduct extensive experiments validating the effectiveness of our proposed N2M module across various settings and release our code to facilitate community exploration.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有导航和操作模块之间的协调不足，导致任务成功率低。  \n2. 操作策略对初始姿态的敏感性未被导航模块考虑。  \n3. 数据收集和训练过程中的高成本和低效率问题。  \n\n【用了什么创新的方案】  \nN2M（Navigation-to-Manipulation）模块作为导航与操作之间的桥梁，实时预测可执行操作的初始姿态。它依赖于自我中心的RGB点云进行高效预测，采用高斯混合模型表示多种可行姿态，直接从策略回放中学习初始姿态偏好，显著提高数据效率和泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Distributionally Robust Safe Motion Planning with Contextual Information",
            "authors": "Kaizer Rahaman,Simran Kumari,Ashish R. Hota",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18666",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18666",
            "arxiv_html_link": "https://arxiv.org/html/2509.18666v1",
            "abstract": "We present a distributionally robust approach for collision avoidance by incorporating contextual information. Specifically, we embed the conditional distribution of future trajectory of the obstacle conditioned on the motion of the ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional kernel mean embedding operator. Then, we define an ambiguity set containing all distributions whose embedding in the RKHS is within a certain distance from the empirical estimate of conditional mean embedding learnt from past data. Consequently, a distributionally robust collision avoidance constraint is formulated, and included in the receding horizon based motion planning formulation of the ego agent. Simulation results show that the proposed approach is more successful in avoiding collision compared to approaches that do not include contextual information and/or distributional robustness in their formulation in several challenging scenarios.",
            "introduction": "Safe motion planning in uncertain and dynamic environments is a fundamental challenge in the field of robotics and autonomous systems [1, 2]. Past works have proposed a plethora of approaches, including sampling-based methods [3], collision avoidance using velocity obstacles [4], and optimization-based techniques [5, 6]. In recent years, optimal control with a receding horizon implementation, popularly known as model predictive control (MPC), have gained popularity as it enables the designer to systematically include the effects of robot dynamics, constraints on robot states and inputs as well as collision avoidance constraints while planning the motion of the robot.\n\nWhile early works on MPC based motion planning focused on avoiding static obstacles [7, 8, 6], more recent works have considered dynamic obstacles in the robust and stochastic MPC frameworks. While robust MPC tends to result in conservative solutions, stochastic MPC techniques formulate collision avoidance conditions in terms of probabilistic constraints or via suitable risk measures [9]. However, the safety guarantees provided by the above techniques may not hold when the probability distribution of the future obstacle position is unknown and/or time-varying.\n\nConsequently, several recent works have proposed distributionally robust motion planning techniques where the stochastic collision avoidance constraints hold for an entire family of distributions (or ambiguity set) that are close (in terms of the Wasserstein distance) to the empirical distribution constructed from the observed data [10, 11, 12, 13, 14]. However, not all available past data are equally relevant at a given context or scenario. In particular, when we consider multiple agents that share the same environment, and are in the vicinity of each other, the future position of the obstacle is a function of the future position of the ego agent. Therefore, in order to obtain more accurate solutions, it is essential to consider ambiguity sets that depend on the actions of the ego agent and other available contextual information. Despite its significance, there is no prior work which considers decision-dependent ambiguity sets or any other contextual information while formulating the distributionally robust safe motion planning problem.\n\nA few recent works have explicitly included the behavior of the ego vehicle for trajectory prediction. Specifically, [15] investigated the impact of ego vehicle planning on nearby vehicles’ trajectories using an LSTM-based encoder combined with a convolutional social pooling module. The authors in [16] modeled each vehicle as a wave characterized by amplitude and phase, proposing that wave-pooling better captures dynamic states and high-order interactions through wave superposition. More recently, [17] provided a detailed comparison of several contextual trajectory prediction techniques. However, the robustness of neural network based trajectory prediction to distribution shifts is not adequately explored in the literature. Neural network based techniques also do not provide any rigorous (e.g., finite-sample) guarantees on the probability of collision among vehicles.\n\nIn this paper, we aim to fill this research gap. First, we compute the empirical estimate of the conditional kernel mean embedding (CKME) [18] of the (conditional) distribution of the future obstacle position as a function of the (i.e., conditioned on the) current states and the predicted trajectory of the ego agent. Then, we formulate an optimal control problem that embeds the ego-conditioned predicted trajectory in the constraints. In particular, the CKME operator provides a closed-form expression of the future trajectory of other vehicles as a function of the states and inputs of the ego vehicle, i.e., the decision variables of the optimal control problem. In order to robustify against distribution shifts, the conditional value at risk of the collision avoidance constraint is required to hold for all distributions whose mean embeddings are within a specified maximum-mean discrepancy (MMD) distance from the empirical estimate of the CKME; a similar approach was recently examined in [19] in the context of optimal control. Another recent paper [20] explored RKHS to select probable trajectories and adopted a sampling-based optimization approach, which is distinct from our approach. We present a tractable approximation of the above constraints following the reformulations developed in [21, 22] such that off-the-shelf nonlinear optimization solvers, such as IPOPT, can be deployed to solve the optimal control problem.111Neural network based predictors do not provide a simple closed form expression of the predicted trajectories as a function of the input applied to the ego vehicle. Therefore, computing optimal control inputs with predicted trajectories used in the constraints is not straightforward.  We provide detailed simulation results involving an autonomous ground vehicle moving on a road with obstacle agent(s), and show that the proposed approach successfully avoids collision in several challenging scenarios.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在不确定和动态环境中实现安全的运动规划。  \n2. 如何有效利用上下文信息来增强碰撞避免的鲁棒性。  \n3. 如何处理未来障碍物位置的概率分布未知或时变的情况。  \n\n【用了什么创新的方案】  \n本文提出了一种基于条件核均值嵌入（CKME）的分布鲁棒运动规划方法，通过将障碍物未来轨迹的条件分布嵌入到再生核希尔伯特空间（RKHS）中，构建了一个依赖于自我代理动作的模糊集合。通过优化控制问题，确保在所有与CKME的经验估计保持在特定距离内的分布下，碰撞避免约束的条件值风险得以满足。该方法在多个挑战场景中展现出优越的碰撞避免能力。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer",
            "authors": "Yarden As,Chengrui Qu,Benjamin Unger,Dongho Kang,Max van der Hart,Laixi Shi,Stelian Coros,Adam Wierman,Andreas Krause",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18648",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18648",
            "arxiv_html_link": "https://arxiv.org/html/2509.18648v1",
            "abstract": "Safety remains a major concern for deploying reinforcement learning (RL) in real-world applications. Simulators provide safe, scalable training environments, but the inevitable sim-to-real gap introduces additional safety concerns, as policies must satisfy constraints in real-world conditions that differ from simulation. To address this challenge, robust safe RL techniques offer principled methods, but are often incompatible with standard scalable training pipelines. In contrast, domain randomization, a simple and popular sim-to-real technique, stands out as a promising alternative, although it often results in unsafe behaviors in practice. We present SPiDR, short for Sim-to-real via Pessimistic Domain Randomization—a scalable algorithm with provable guarantees for safe sim-to-real transfer. SPiDR uses domain randomization to incorporate the uncertainty about the sim-to-real gap into the safety constraints, making it versatile and highly compatible with existing training pipelines. Through extensive experiments on sim-to-sim benchmarks and two distinct real-world robotic platforms, we demonstrate that SPiDR effectively ensures safety despite the sim-to-real gap while maintaining strong performance.",
            "introduction": "Reinforcement learning (RL) has made significant strides in recent years, demonstrating remarkable progress across a range of domains. These include achieving superhuman capabilities in games (Mnih et al., 2015; Silver et al., 2016), fine-tuning large language models (Ouyang et al., 2022), advancing applications in healthcare (Fox et al., 2020; Zhu et al., 2020), robotics (Lee et al., 2020; Degrave et al., 2022; Lin et al., 2025) and autonomous driving (Cusumano-Towner et al., 2025; Cornelisse et al., 2025).\nYet despite these achievements, ensuring safety and preventing harmful behaviors remains a critical challenge and a prerequisite for unlocking the full potential of RL as a ubiquitous element in everyday life (Amodei et al., 2016; Gu et al., 2022).\n\nThe use of simulators has been a key component behind the success of many of the mentioned applications  (Visentin et al., 2014; Makoviychuk et al., 2021; Degrave et al., 2022; Kazemkhani et al., 2024). Training in simulation allows agents to learn from unsafe interactions, which in reality would lead to catastrophic outcomes. In addition, learning complex behaviors fully online can be prohibitively time-consuming. Modern simulators accelerate training, reducing hours of real-world experience to minutes on consumer-grade GPUs (Rudin et al., 2022). However, while being a major driver in the development of the above examples, even state-of-the-art simulators often fall short in precisely mirroring the real-world. Indeed, “all models are wrong” (Box, 1976)—the so-called sim-to-real gap can make simulation-trained policies violate real-world constraints, which can be particularly dangerous in high-stakes settings where safety must be guaranteed on first contact.\n\nExisting literature to address this challenge often relies on tools from robust optimization (Queeney and Benosman, 2024; Kitamura et al., 2024; Zhang et al., 2024). While being theoretically grounded, such methods typically require practitioners to significantly alter their existing training pipelines, rendering them less prevalent in practice. In contrast, due to its simplicity, domain randomization has become the de facto tool for sim-to-real transfer (Tobin et al., 2017; Peng et al., 2018; Lee et al., 2020; Degrave et al., 2022). Despite its success, in problems that require adherence to safety constraints, domain randomization lacks safety guarantees and often fails to satisfy the constraints in practice (cf. Queeney and Benosman, 2024, and Figure˜3). Therefore, a method that provably guarantees safe sim-to-real transfer, while being highly compatible with standard training practices, is still missing.\n\nIn this work, we address this gap by presenting a simple method that builds on domain randomization while ensuring safety under sim-to-real transfer. We theoretically show that unsafe transfer can be associated with large uncertainty about the sim-to-real gap, quantified as the disagreement among next-state predictions from domain-randomized dynamics models. This key idea is illustrated in Figure˜1, where spikes in uncertainty (e.g. at t=4.6t=4.6 and t=5.3t=5.3) coincide with unstable or unsafe behaviors, such as stumbling or flipping. Motivated by this insight, we propose to penalize the cost with the uncertainty to achieve safe sim-to-real transfer, leading to the design of SPiDR. Notably, SPiDR seamlessly integrates with state-of-the-art RL algorithms (Schulman et al., 2017; Haarnoja et al., 2019), delivering strong empirical performance on both in simulated and real-world safe RL tasks while ensuring constraint satisfaction, even under severe model mismatch.\n\nWe address an important challenge to real-world adoption of RL: zero-shot safe sim-to-real transfer, where an agent must learn a safe and effective policy using only simulated interactions. We propose SPiDR, a practical algorithm with formal safety guarantees that integrates easily into popular sim-to-real pipelines.\n\nWe address an important challenge to real-world adoption of RL: zero-shot safe sim-to-real transfer, where an agent must learn a safe and effective policy using only simulated interactions. We propose SPiDR, a practical algorithm with formal safety guarantees that integrates easily into popular sim-to-real pipelines.\n\nWe validate SPiDR on two real-world robotic platforms, where it achieves zero-shot constraint satisfaction, substantially outperforming other baselines in terms of safety and performance. These results provide empirical evidence that our theoretical guarantees translate to the real-world, suggesting that SPiDR can be safely used in real-world deployment.\n\nFinally, we extensively evaluate SPiDR on well established simulated continuous control benchmarks, including the RWRL benchmark (Dulac-Arnold et al., 2020), Safety Gym (Ray et al., 2019) and RaceCar environments (Kabzan et al., 2020), where SPiDR consistently satisfies safety constraints while achieving strong task performance.\n\n1. We address an important challenge to real-world adoption of RL: zero-shot safe sim-to-real transfer, where an agent must learn a safe and effective policy using only simulated interactions. We propose SPiDR, a practical algorithm with formal safety guarantees that integrates easily into popular sim-to-real pipelines.\n\n2. We validate SPiDR on two real-world robotic platforms, where it achieves zero-shot constraint satisfaction, substantially outperforming other baselines in terms of safety and performance. These results provide empirical evidence that our theoretical guarantees translate to the real-world, suggesting that SPiDR can be safely used in real-world deployment.\n\n3. Finally, we extensively evaluate SPiDR on well established simulated continuous control benchmarks, including the RWRL benchmark (Dulac-Arnold et al., 2020), Safety Gym (Ray et al., 2019) and RaceCar environments (Kabzan et al., 2020), where SPiDR consistently satisfies safety constraints while achieving strong task performance.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在sim-to-real转移中确保安全性，尤其是零-shot情况下。  \n2. 现有的安全强化学习方法在实际应用中往往不兼容标准训练流程。  \n3. 域随机化在满足安全约束方面的不足。  \n\n【用了什么创新的方案】  \nSPiDR（Sim-to-real via Pessimistic Domain Randomization）是一种新颖的算法，通过将对sim-to-real差距的不确定性纳入安全约束，确保在模拟到现实的转移中实现安全性。该方法与现有的强化学习算法无缝集成，能够在模拟和现实世界中有效地满足安全约束，同时保持强大的任务性能。SPiDR的设计使其在保持高兼容性的同时，提供了形式化的安全保证。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Do You Need Proprioceptive States in Visuomotor Policies?",
            "authors": "Juntu Zhao,Wenbo Lu,Di Zhang,Yufeng Liu,Yushen Liang,Tianluo Zhang,Yifeng Cao,Junyuan Xie,Yingdong Hu,Shengjie Wang,Junliang Guo,Dequan Wang,Yang Gao",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18644",
            "code": "https://statefreepolicy.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18644",
            "arxiv_html_link": "https://arxiv.org/html/2509.18644v1",
            "abstract": "Imitation-learning–based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0% to 85% in height generalization and from 6% to 64% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment.",
            "introduction": "Imitation-learning-based visuomotor policies [1, 2, 3, 4, 5] have been widely used in robotic manipulation.\nLeveraging large-scale demonstration datasets [6, 7, 8, 9] and fine-tuning powerful pre-trained policies have enabled robots to achieve remarkable performance across diverse real-world tasks.\n\nFor precise and reliable control, these visuomotor policies typically incorporate not only visual observations of the task environment but also proprioceptive state (hereafter referred to as state) inputs [10, 3, 11], such as end-effector poses and joint angles. The state inputs provide compact and accurate information about the robot’s configuration, but they also make the policy prone to overfitting by simply memorizing the training trajectories.\nTherefore it severely limits spatial generalization [12, 13, 14] if the training data lacks diversity [15].\nIn today’s context, where collecting demonstration data with wide state coverage (i.e., diverse spatial locations of task-relevant objects) is prohibitively expensive, this has become a critical bottleneck for the development of visuomotor policies.\n\nIn this study, we propose to completely remove the state input in visuomotor policies to enhance their spatial generalization ability, hereafter referred to as “State-free Policies.”\nThis design is built upon two conditions:\n\nRelative end-effector (EEF) action space [16]: The visuomotor policies predict relative displacements of the end-effector based on the current observation. Among different action spaces, the relative EEF action space most naturally supports the generalization of policies.\n\nFull task observation: Another key condition for effective State-free Policies is to ensure sufficient task-relevant visual information, which we term “full task observation”. This enables visuomotor policies to fully “see” the task-relevant objects in the task.\n\nThis mechanism of State-free Policies forces the policy to develop a deeper understanding of the task environment rather than simply memorizing the trajectories, thereby enabling State-free Policies to achieve advantages that state-based policies cannot provide:\n\nSpatial Generalization: Since State-free Policies do not rely on state inputs, they avoid overfitting to the training trajectories.\nTherefore, they exhibit strong height and horizontal generalization abilities, where height refers to variations of the task-relevant object’s location in the vertical direction, and horizontal refers to variations of the object’s location in the 2D plane.\n\nData efficiency: Even in in-domain settings, state-based policies require diverse demonstrations to avoid overfitting to specific trajectories. In contrast, removing the state input eliminates this dependence on trajectory diversity, allowing State-free Policies to be fine-tuned with less demonstration data. This reduces the cost of data collection, which is often a major bottleneck in deploying real-world robots.\n\nCross-embodiment adaptation: Since State-free Policies rely only on visual inputs and predict actions in the relative EEF space, they exhibit stronger cross-embodiment adaptation ability than state-based policies. They do not require additional adaptation to different state spaces, so the same task can be easily adapted to new embodiments with fewer fine-tuning steps.\n\nCross-embodiment adaptation: Since State-free Policies rely only on visual inputs and predict actions in the relative EEF space, they exhibit stronger cross-embodiment adaptation ability than state-based policies. They do not require additional adaptation to different state spaces, so the same task can be easily adapted to new embodiments with fewer fine-tuning steps.\n\nWe have conducted extensive experiments across a diverse range of tasks, robot embodiments, and policy architectures.\nIn both real-world and simulation environments, State-free Policies achieve comparably great in-domain performance to state-based policies.\nMost importantly, when trained on strictly collected real-world demonstration data\n(i.e., the task-relevant object location has a constrained initial distribution range),\nState-free Policies exhibit significantly stronger spatial generalization ability than state-based policies.\nFor further benefits, e.g., data efficiency and cross-embodiment adaptation ability, they also demonstrate obvious advantages over state-based policies, highlighting their potential for scalable and practical deployment in real-world robotic systems.\n\n1. Relative end-effector (EEF) action space [16]: The visuomotor policies predict relative displacements of the end-effector based on the current observation. Among different action spaces, the relative EEF action space most naturally supports the generalization of policies.\n\n2. Full task observation: Another key condition for effective State-free Policies is to ensure sufficient task-relevant visual information, which we term “full task observation”. This enables visuomotor policies to fully “see” the task-relevant objects in the task.\n\n1. Spatial Generalization: Since State-free Policies do not rely on state inputs, they avoid overfitting to the training trajectories.\nTherefore, they exhibit strong height and horizontal generalization abilities, where height refers to variations of the task-relevant object’s location in the vertical direction, and horizontal refers to variations of the object’s location in the 2D plane.\n\n2. Data efficiency: Even in in-domain settings, state-based policies require diverse demonstrations to avoid overfitting to specific trajectories. In contrast, removing the state input eliminates this dependence on trajectory diversity, allowing State-free Policies to be fine-tuned with less demonstration data. This reduces the cost of data collection, which is often a major bottleneck in deploying real-world robots.\n\n1. Cross-embodiment adaptation: Since State-free Policies rely only on visual inputs and predict actions in the relative EEF space, they exhibit stronger cross-embodiment adaptation ability than state-based policies. They do not require additional adaptation to different state spaces, so the same task can be easily adapted to new embodiments with fewer fine-tuning steps.",
            "llm_summary": "【关注的是什么问题】  \n1. 过度依赖proprioceptive状态输入导致策略对训练轨迹的过拟合。  \n2. 现有的visuomotor策略在空间泛化能力上存在严重限制。  \n3. 数据收集的多样性不足成为开发visuomotor策略的瓶颈。  \n\n【用了什么创新的方案】  \n提出了一种“无状态策略”（State-free Policy），完全去除proprioceptive状态输入，仅基于视觉观察预测动作。这种策略在相对末端执行器动作空间中构建，确保充分的任务相关视觉信息，从而增强空间泛化能力。实验结果表明，该策略在多种真实世界任务中表现出显著的空间泛化能力、数据效率和跨躯体适应性，提升了实际部署的可行性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments",
            "authors": "Yuan Zhou,Jialiang Hou,Guangtong Xu,Fei Gao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18636",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18636",
            "arxiv_html_link": "https://arxiv.org/html/2509.18636v1",
            "abstract": "Formation maintenance with varying number of drones in narrow environments hinders the convergence of planning to the desired configurations. To address this challenge, this paper proposes a formation planning method guided by Deformable Virtual Structures (DVS) with continuous spatiotemporal transformation. Firstly, to satisfy swarm safety distance and preserve formation shape filling integrity for irregular formation geometries, we employ Lloyd algorithm for uniform P​A¯\\underline{PA}rtitioning and Hungarian algorithm for A​S¯\\underline{AS}signment (PAAS) in DVS. Subsequently, a spatiotemporal trajectory involving DVS is planned using primitive-based path search and nonlinear trajectory optimization. The DVS trajectory achieves adaptive transitions with respect to a varying number of drones while ensuring adaptability to narrow environments through affine transformation. Finally, each agent conducts distributed trajectory planning guided by desired spatiotemporal positions within the DVS, while incorporating collision avoidance and dynamic feasibility requirements. Our method enables up to 15% of swarm numbers to join or leave in cluttered environments while rapidly restoring the desired formation shape in simulation. Compared to cutting-edge formation planning method, we demonstrate rapid formation recovery capacity and environmental adaptability. Real-world experiments validate the effectiveness and resilience of our formation planning method.",
            "introduction": "In recent years, formation flight becomes the foundation requirement for aerial swarms in practical applications, such as collaborative exploration [1], light show [2], search and rescue [3]. For large-scale swarms [4], [5], formation inevitably encounters agent loss in narrow environments [6, 7, 8]. Naturally, the integration of new members can restore the impaired formation and enhance the efficiency of mission execution [9], [10]. Therefore, dynamic adaptation of formation reconfiguration for changes in the number of drones is essential to ensure operational resilience under unforeseen disturbance. Besides, formation needs to experience adaptively overall shape transformations to navigate through narrow environments and maintain its configuration as much as possible to keep resilience [11, 12, 13, 14].\n\nWhen the number of drones in formations varies, the abrupt variation of formation cooperative constraints tends to degrade the feasibility of trajectory optimization. In narrow environments, the inherent conflict between formation maintenance and collision avoidance also deteriorates the feasibility, resulting in no solution for the classical planning method without global formation adjustment mechanism.\n\nNumerous works have demonstrated formation systems with swarm number variations.\nArtificial Potential Fields (APF) [4], [8], [15] are commonly used to guide the formation generation with variable swarm numbers. This approach eliminates the need for explicit assignment of desired positions, enabling adaptive number variation of formation. However, the slow convergence rate and inadequate swarm collision avoidance hinder its applicability in real-time formation planning. Although explicit position and assignment during number variations enable rapid formation recovery [16], [17], [18], they often fail to guarantee robust performance in narrow environments, where collision avoidance remains challenging during formation recovery.\nAdaptive deformation of the formation shape is widely adopted for safe and rapid navigation through obstacle environments [11], [13]. Nevertheless, distributed cooperative planning is insufficient without effective guidance for formation in narrow environments. Consequently, adjusting the overall deformation of the swarm provides an effective resolution [11]. However, this category of approaches is limited by either locally reactive mechanisms [13] and simply adjusts the local target that hinders formation guidance and environmental adaptability [11], or an excessive pursuit of collision-free passage that severely compromises formation maintenance integrity while neglecting post-maneuver recovery capability. These methods often struggle to rapidly restore desired formations in cluttered environments. Furthermore, most studies of formation planning do not address adaptive adjustment with variations in swarm number [11], [19], [20].\n\nTo achieve real-time formation flight with a variable number of drones, we propose a number adaptive formation planning system. According to the desired formation shape, we employ explicit partitioning and assignment to designate formation positions for the swarm. We focus on designing a Deformable Virtual Structures (DVS) trajectory with spatiotemporal motion and deformation that can effectively guide formation recovery and maintenance. Specifically, we introduce virtual rigid structures with two additional degrees of freedom, including scaling and affine transformation in DVS. The concept of virtual structures has been applied in multiple fields, but predominantly uses rigid virtual structures which are unable to adaptively deform [21, 22, 23]. In this work, we design a spatiotemporal trajectory optimization method for DVS, which utilizes compact piecewise polynomial representations in Cartesian and deformable spaces to formulate nonlinear optimization problems. This enables adaptive adjustment of the scale of formations according to swarm number variations, while permitting appropriate affine deformation of the virtual structures to facilitate the formations to navigate in narrow environments. Integrating PAAS and DVS with spatiotemporal transformation, we designate this framework as Deformation Guidance (DG), and agents perform distributed trajectory planning under the guidance of DG. Extensive simulation and real-world experiments demonstrate that our method supports variable number formation planning while achieving a high success rate in navigating through narrow environments.\n\nThe contributions of this paper are as follows:\n\nTo satisfy swarm safety distance and preserve shape filling integrity for irregular formation geometries, we implement the Lloyd algorithm for uniform partitioning in formation planning with variable number, with assignment via the Hungarian algorithm. We experimentally verified that the computational efficiency of PAAS is sufficient to achieve real-time planning.\n\nTo satisfy swarm safety distance and preserve shape filling integrity for irregular formation geometries, we implement the Lloyd algorithm for uniform partitioning in formation planning with variable number, with assignment via the Hungarian algorithm. We experimentally verified that the computational efficiency of PAAS is sufficient to achieve real-time planning.\n\nWe propose a spatiotemporal trajectory optimization method for DVS with scaling and affine transformation to guide formation planning in narrow environments. We employ a primitive path search to efficiently obtain high-dimensional DVS paths as initial values for DVS trajectory optimization, which can be transformed into unconstrained optimization problems for real-time solution.\n\nWe incorporate DVS-based spatiotemporal formation constraint in distributed trajectory optimization to achieve rapid formation recovery and enhanced environmental adaptability. Extensive experiments and benchmarks validate the efficiency and advantages of our system.",
            "llm_summary": "【关注的是什么问题】  \n1. 变动数量的无人机在狭窄环境中的编队维护问题。  \n2. 传统规划方法在动态调整编队形状和避免碰撞方面的局限性。  \n3. 如何实现实时的编队飞行以适应无人机数量的变化。  \n\n【用了什么创新的方案】  \n本文提出了一种基于可变形虚拟结构（DVS）的编队规划方法，通过连续的时空变换来指导编队恢复和维护。采用Lloyd算法进行均匀划分，并结合匈牙利算法进行分配，以满足安全距离和形状完整性要求。设计了一种具有缩放和仿射变换能力的DVS轨迹优化方法，能够在狭窄环境中有效导航。通过将DVS与时空变换相结合，支持动态的编队调整，确保在复杂环境中快速恢复所需的编队形状。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training",
            "authors": "Shuo Cheng,Liqian Ma,Zhenyang Chen,Ajay Mandlekar,Caelan Garrett,Danfei Xu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18631",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18631",
            "arxiv_html_link": "https://arxiv.org/html/2509.18631v1",
            "abstract": "Behavior cloning has shown promise for robot manipulation, but real-world demonstrations are costly to acquire at scale. While simulated data offers a scalable alternative, particularly with advances in automated demonstration generation, transferring policies to the real world is hampered by various simulation and real domain gaps. In this work, we propose a unified sim-and-real co-training framework for learning generalizable manipulation policies that primarily leverages simulation and only requires a few real-world demonstrations. Central to our approach is learning a domain-invariant, task-relevant feature space. Our key insight is that aligning the joint distributions of observations and their corresponding actions across domains provides a richer signal than aligning observations (marginals) alone. We achieve this by embedding an Optimal Transport (OT)-inspired loss within the co-training framework, and extend this to an Unbalanced OT framework to handle the imbalance between abundant simulation data and limited real-world examples. We validate our method on challenging manipulation tasks, showing it can leverage abundant simulation data to\nachieve up to a 30% improvement in the real-world success rate\nand even generalize to scenarios seen only in simulation.",
            "introduction": "Behavior cloning pomerleau1988alvinn  is a promising approach for acquiring robot manipulation skills directly in the real world, due to its simplicity and effectiveness in mimicking expert demonstrations robomimic2021 ; florence2022implicit . However, achieving robust and generalizable performance requires collecting large-scale datasets khazatsky2024droid ; open_x_embodiment_rt_x_2023  across diverse environments, object configurations, and tasks. This data collection process is labor-intensive, time-consuming, and costly, posing significant challenges to scalability in real-world applications.\n\nRecently, with rapid advancements in physics simulators Genesis ; Xiang_2020_SAPIEN , procedural scene generation raistrick2024infinigen ; deitke2022️ , and motion synthesis techniques mandlekar2023mimicgen ; cheng2023nod , there has been growing interest in leveraging simulation as an alternative source of training data. These simulation-based approaches enable scalable and controllable data generation, allowing for diverse and abundant supervision at a fraction of the real-world cost. However, transferring policies trained in simulation to the physical world remains a non-trivial challenge due to sim-to-real gap—the discrepancies between the simulated and real-world environments that a policy encounters during execution. These differences can manifest in various forms, such as variations in visual appearance, sensor noise, and action dynamics andrychowicz2020learning ; tobin2017domain . In particular, learning visuomotor control policies that remain robust under changing perceptual conditions during real-world deployment continues to be an open area of research.\n\nCommon strategies to bridge this domain gap include domain randomization andrychowicz2020learning ; tobin2017domain  and data augmentation hansen2020self ; yarats2021mastering , though these often require careful tuning. Domain adaptation (DA) techniques aim to explicitly align distributions, either at pixel bousmalis2017unsupervised ; james2019sim  or feature levels tzeng2014deep ; long2015learning ; zhao2019learning . However, many feature-level methods align only marginal observation distributions (e.g., MMD tzeng2014deep ; long2015learning ), which can be insufficient for fine-grained manipulation alignment as it may not preserve action-relevant relationships across domains. More recently, sim-and-real co-training—simply training a single policy on mixed data from both domains wei2025empirical ; maddukuri2025sim —has shown surprising effectiveness. We argue that while beneficial for data diversity, such co-training approaches typically lack explicit constraints for feature space alignment across domains, potentially hindering optimal transfer and generalization because they don’t enforce a consistent mapping of task-relevant structures.\n\nWe present a unified sim-and-real co-training framework that explicitly learns a shared latent space where observations from simulation and the real world are aligned and preserve action-relevant information. Our key insight is that aligning the joint distributions of observations and their corresponding actions or task-relevant states across domains provides a direct signal for learning transferable features.\nConcretely, we leverage Optimal Transport (OT) courty2016optimal  as an alignment objective to learn representations where the geometric relationships crucial for action prediction are consistent, irrespective of whether the input comes from simulation or the real world. Further more, to robustly handle the data imbalance in co-training with abundant simulation data and limited real-world data, we further extend to an Unbalanced OT (UOT) formulation fatras2021unbalanced ; chizat2018scaling  and develop a temporally-aware sampling strategy to improve domain alignment learning in a mini-batch OT setting.\n\nOur contributions are: (1) a sim-and-real co-training framework that learns a domain-invariant yet task-salient latent space to improve real-world performance with abundant simulation data, (2) an Unbalanced Optimal Transport framework and temporally-aware sampling strategy to mitigate data imbalance and improve alignment quality in mini-batch OT training, (3) comprehensive experiments using both image and point-cloud modalities, evaluating sim-to-sim and sim-to-real transfer across diverse manipulation tasks, demonstrating up to a 30% average success rate improvement and achieving generalization to real-world scenarios for which the training data only appears in simulation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效地将模拟数据中的策略转移到真实世界中。  \n2. 如何处理模拟与真实世界之间的域间差距。  \n3. 如何在有限的真实世界示例下利用丰富的模拟数据进行训练。  \n\n【用了什么创新的方案】  \n提出了一种统一的sim-and-real共训练框架，通过学习一个域不变且任务相关的特征空间，来提高机器人操作策略的可迁移性。核心思想是对观察和对应动作的联合分布进行对齐，利用Optimal Transport (OT)作为对齐目标，确保无论输入来自模拟还是现实，几何关系在动作预测中保持一致。此外，扩展到Unbalanced OT框架以处理数据不平衡，并开发了时间感知采样策略，以改善小批量OT训练中的域对齐学习。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving",
            "authors": "Jay Patrikar,Apoorva Sharma,Sushant Veer,Boyi Li,Sebastian Scherer,Marco Pavone",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "8 pages, 5 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.18626",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18626",
            "arxiv_html_link": "https://arxiv.org/html/2509.18626v1",
            "abstract": "Learning-based autonomous driving systems are trained mostly on incident-free data, offering little guidance near safety–performance boundaries. Real crash reports contain precisely the contrastive evidence needed, but they are hard to use: narratives are unstructured, third-person, and poorly grounded to sensor views. We address these challenges by normalizing crash narratives to ego-centric language and converting both logs and crashes into a unified scene–action representation suitable for retrieval. At decision time, our system adjudicates proposed actions by retrieving relevant precedents from this unified index; an agentic counterfactual extension proposes plausible alternatives, retrieves for each, and reasons across outcomes before deciding. On a nuScenes benchmark, precedent retrieval substantially improves calibration, with recall on contextually preferred actions rising from 24% to 53%. The counterfactual variant preserves these gains while sharpening decisions near risk.",
            "introduction": "End-to-end learning-based autonomous vehicle (AV) systems are trained primarily through imitation learning on positive, incident-free driving data [1, 2].\nThis data is typically collected by expert human drivers driving sensor-instrumented vehicles in a variety of driving scenarios, resulting in a dataset pairing the sensor observations that the AV will encounter with the action that the human driver chose in that moment.\nWhile this data helps define “good” driving that an AV should imitate, it does not provide direct supervision of what behaviors are to be avoided.\nSome have aimed to address this gap through auxiliary reward functions defining a rules-based definition of risky driving [3], but such rules can be challenging to specify: Risk is difficult to quantify due to uncertainty over other road user’s behaviors. Moreover, competent driving requires appropriately managing the risk that is taken on to make progress; remaining stopped is the safest policy, but not competent driving behavior.\n\nInstead, in this work, we consider an alternative data-driven approach to provide negative supervision for AV decision making. Specifically, we explore the use of crash reports as a complementary source of driving knowledge.\nAgencies such as the National Highway Traffic Safety Administration (NHTSA) collect structured narrative accounts of real-world accidents, including the actions taken and the conditions under which failures occurred. While these reports lack the rich multimodal data of first-person human-driven AV logs, they contain valuable causal and contextual information that can support counterfactual reasoning. While these reports can’t directly be used in policy training, recent advances in vision-language models (VLMs) capable of reasoning across sensor and text domains offer a compelling avenue for bringing such valuable sources of negative data into AV decision making.\n\nIn this paper, we study how negative data influences VLM reasoning capabilities in AV decision making tasks by developing a retrieval-augmented-generation (RAG) pipeline for AV safety adjudication. Specifically our contributions are as follows:\n(i) a GraphRAG [4] style retrieval pipeline for both positive and negative driving precedent, using a unified structured language representation for both sensor-domain positive data and language-domain negative data; (ii) an agentic extension which uses additional test-time compute to reason about counterfactuals prior to making a safety judgment; (iii) evaluation of both approaches in terms of alignment with human judgement on the safety of possible actions in driving scenarios, showing the impact of negative crash report data on VLM decision making capability.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用负数据（如事故报告）来改善自动驾驶系统的决策能力。  \n2. 如何将非结构化的事故叙述转换为可用于自动驾驶的结构化信息。  \n3. 如何在决策时使用检索机制来提高安全性判断的准确性。  \n\n【用了什么创新的方案】  \n本研究提出了一种基于检索增强生成（RAG）管道的方法，利用事故报告作为负监督数据来提升自动驾驶决策的安全性。通过将正负驾驶先例统一为结构化语言表示，构建了一个GraphRAG风格的检索管道。同时，增加了一个代理扩展，在决策前进行反事实推理，以提高与人类判断的一致性，并评估了负数据对视觉语言模型（VLM）决策能力的影响。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones",
            "authors": "Maximilian Adang,JunEn Low,Ola Shorinwa,Mac Schwager",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18610",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18610",
            "arxiv_html_link": "https://arxiv.org/html/2509.18610v1",
            "abstract": "Large vision-language models have driven remarkable progress in open-vocabulary robot policies, e.g., generalist robot manipulation policies, that enable robots to complete complex tasks specified in natural language. Despite these successes, open-vocabulary autonomous drone navigation remains an unsolved challenge due to the scarcity of large-scale demonstrations, real-time control demands of drones for stabilization, and lack of reliable external pose estimation modules.\nIn this work,\nwe present SINGER for language-guided autonomous drone navigation in the open world using only onboard sensing and compute. To train robust, open-vocabulary navigation policies, SINGER leverages three central components: (i) a photorealistic language-embedded flight simulator with minimal sim-to-real gap using Gaussian Splatting for efficient data generation, (ii) an RRT-inspired multi-trajectory generation expert for collision-free navigation demonstrations, and these are used to train (iii) a lightweight end-to-end visuomotor policy for real-time closed-loop control.\nThrough extensive hardware flight experiments, we demonstrate superior zero-shot sim-to-real transfer of our policy to unseen environments and unseen language-conditioned goal objects. When trained on ∼\\sim700k-1M observation action pairs of language conditioned visuomotor data and deployed on hardware, SINGER outperforms a velocity-controlled semantic guidance baseline by reaching the query 23.33%\\mathbf{23.33\\%} more on average, and maintains the query in the field of view 16.67%\\mathbf{16.67\\%} more on average, with 𝟏𝟎%\\mathbf{10\\%} fewer collisions.",
            "introduction": "Everyday, humans demonstrate notable semantic and physical understanding of their environments. For example, given a task to go to a specified location, a person relatively easily transforms the language instruction into a physical goal location using semantic cues and navigates to the desired location, safely avoiding collisions.\nAlthough autonomous drones excel at agile flight, they are often limited to controlled environments with pre-specified goal locations. In this work, we ask the question: “Can we train a vision-language drone navigation policy to reach previously unseen goal objects in a previously unseen environment using only on board sensing and compute?”\n\nAdvances in diffusion policies [1] and vision-language-action (VLA) models [2, 3] have led to significant research breakthroughs in robot policy learning from expert demonstration via imitation, particularly in robot manipulation.\nSpecifically, leveraging imitation learning on large-scale robot manipulation datasets [4, 5], state-of-the-art policies endow robots with the requisite task understanding and planning capabilities necessary to perform complex tasks entirely from task descriptions provided in natural language, e.g., to “pick up the apple and place it on a plate.”\nHowever, this paradigm has been largely unsuccessful in drone navigation, due to scarcity of large-scale drone navigation datasets, and effective semantic distillation methods for open-world drone navigation. This is exacerbated by inherent challenges in collecting large quantities of high quality visuomotor data on highly dynamic and naturally unstable drones.\n\nTo address the data scarcity challenge, prior work [6, 7] trains visuomotor policies for drone navigation in simulation, but the effectiveness of the resulting policies are often limited by the non-negligible sim-to-real gap. SOUS-VIDE [8] introduces FiGS, a high-fidelity Gaussian-Splatting-based drone simulator to narrow the sim-to-real gap for stronger real-world transfer; however, FiGS lacks the semantic knowledge required for open-world drone navigation, limiting its deployment to only environments and trajectories seen during training.\n\nIn this paper, we introduce SINGER (Semantic In-situ Navigation and Guidance for Embodied Robots), a pipeline for training language-conditioned drone navigation policies addressing the aforementioned limitations. SINGER consists of three central components: (i) a semantics-rich photorealistic flight simulator based on 3D Gaussian Splatting for efficient data generation with expert demonstrations,\n(ii) a high-level rapidly exploring random trees (RRT*) based planner that efficiently computes spatially spanning collision-free paths to a language-specified goal by time-inverting an expanded tree, and\n(iii) a robust low-level visuomotor policy that tracks the resulting high-level plans with real-time feedback.\nWith these components, SINGER trains a lightweight viusal policy that runs onboard a drone in real-time for online navigation given a natural-language goal object.\n\nTo build an effective flight simulator, we blend the high-fidelity scene-reconstruction capabilities of Gaussian Splatting [9] with the generalizable open-world vision-language semantic features computed by CLIP [10], achieving minimal sim-to-real gap.\nThis core design choice underpins SINGER’s strong zero-shot generalization capabilities to unseen tasks and environments at inference time. In particular, by abstracting goal specification to a semantic (vision-language) space, SINGER effectively aligns a small dataset of synthetic expert trajectories with a broad set of tasks, yielding a data-efficient training scheme for robust visuomotor policies. We augment this training approach with domain randomization for added robustness.\n\nAt deployment, we inference CLIPSeg [11] to produce open-vocabulary semantic images of the environment as conditioning inputs, processed by an end-to-end visuomotor drone policy for low-level drone commands.\n\nThrough our experiments, we show that SINGER outperforms baseline methods in achieving sub-meter proximity to goal by 23.33%23.33\\% with 10%10\\% less collisions and keeping the query in the field of view 16.67%16.67\\% more often without relying on external pose estimation or map-based navigation methods.\n\nWe summarize our contributions as follows:\n\nWe introduce a high-fidelity drone simulator for efficient imitation learning in language-specified drone navigation problems built on language embedded Gaussian Splatting.\n\nWe design a RRT* trajectory planner that efficiently finds thousands of collision-free feasible trajectories across multiple Gaussian Splatting scenes and multiple semantic classes, used to produce large quantities of data for training a generalist policy.\n\nWe present a real-time, lightweight, low-level visual policy architecture for language guided drone navigation using onboard sensing and compute.\n\nUsing these components, we train robust visuomotor policies for drone guidance given a natural language goal specification that generalizes to never before seen environments and semantic queries.\n\n1. We introduce a high-fidelity drone simulator for efficient imitation learning in language-specified drone navigation problems built on language embedded Gaussian Splatting.\n\n2. We design a RRT* trajectory planner that efficiently finds thousands of collision-free feasible trajectories across multiple Gaussian Splatting scenes and multiple semantic classes, used to produce large quantities of data for training a generalist policy.\n\n3. We present a real-time, lightweight, low-level visual policy architecture for language guided drone navigation using onboard sensing and compute.\n\n4. Using these components, we train robust visuomotor policies for drone guidance given a natural language goal specification that generalizes to never before seen environments and semantic queries.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现无人机在开放环境中的语言引导导航？  \n2. 如何解决无人机导航中的数据稀缺和sim-to-real差距问题？  \n3. 如何在无人机上实现实时的低级视觉控制策略？  \n\n【用了什么创新的方案】  \nSINGER提出了一种新的无人机导航策略，通过三个核心组件实现语言引导的自主导航：首先，利用基于3D Gaussian Splatting的高保真飞行模拟器生成高效的专家演示数据；其次，设计了一种基于RRT*的多轨迹生成规划器，以确保碰撞自由的导航；最后，开发了一种轻量级的端到端视觉运动策略，实现实时闭环控制。该方法通过在未见环境和目标对象上的实验，展示了优越的零-shot sim-to-real迁移能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving",
            "authors": "Chengran Yuan,Zijian Lu,Zhanqi Zhang,Yimin Zhao,Zefan Huang,Shuo Sun,Jiawei Sun,Jiahui Li,Christina Dao Wen Lee,Dongen Li,Marcelo H. Ang Jr",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18609",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18609",
            "arxiv_html_link": "https://arxiv.org/html/2509.18609v1",
            "abstract": "End-to-end motion planning is promising for simplifying complex autonomous driving pipelines. However, challenges such as scene understanding and effective prediction for decision-making continue to present substantial obstacles to its large-scale deployment. In this paper, we present PIE, a pioneering framework that integrates advanced perception, reasoning, and intention modeling to dynamically capture interactions between the ego vehicle and surrounding agents. It incorporates a bidirectional Mamba fusion that addresses data compression losses in multimodal fusion of camera and LiDAR inputs, alongside a novel reasoning-enhanced decoder integrating Mamba and Mixture-of-Experts to facilitate scene-compliant anchor selection and optimize adaptive trajectory inference. PIE adopts an action-motion interaction module to effectively utilize state predictions of surrounding agents to refine ego planning. The proposed framework is thoroughly validated on the NAVSIM benchmark. PIE, without using any ensemble and data augmentation techniques, achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of prior state-of-the-art methods. Comprehensive quantitative and qualitative analyses demonstrate that PIE is capable of reliably generating feasible and high-quality ego trajectories.",
            "introduction": "End-to-end motion planning has emerged as a promising paradigm for general robotic systems, including autonomous vehicles (AVs). This data-driven approach has the potential to enable AVs to handle complex and previously unseen scenarios, a capability that becomes increasingly critical as urban environments grow denser and more intricate. By leveraging sensor data directly, end-to-end methods [1, 2, 3] aim to consolidate the traditionally segmented autonomy pipeline—encompassing perception, prediction, and planning—into a single, cohesive framework.\n\nDespite the promising performance of end-to-end methods, several key challenges remain. First, fusing multimodal data (e.g., image and LiDAR inputs) often leads to compression-induced losses when reducing historical information or fusing features from different sources. Second, while data-driven approaches offer the potential for enhancing environmental understanding, the complexity of real-world driving requires more sophisticated models that are capable of both reasoning and dynamically adjusting their strategies. Third, incorporating the predictions of other traffic participants into the end-to-end planning pipeline often introduces substantial computational overhead. Developing efficient methods to seamlessly integrate these predictions into the planning process remains an open challenge, presenting considerable opportunities for further advancements.\n\nTo address these issues, we present PIE, an encoder-decoder framework designed to model the interaction between the action of ego vehicle and the motion of nearby agents and to enable more nuanced reasoning about the driving environment. Our approach mitigates data loss and integrates prediction and planning effectively. The contributions of this work are threefold:\n\nBidirectional Mamba Fusion We introduce a bidirectional Mamba fusion that effectively improves the multimodal data fusion between camera and LiDAR. A notable improvement of 1.9 PDM score can be achieved by merely employing this fusion approach based on the Transfuser backbone.\n\nBidirectional Mamba Fusion We introduce a bidirectional Mamba fusion that effectively improves the multimodal data fusion between camera and LiDAR. A notable improvement of 1.9 PDM score can be achieved by merely employing this fusion approach based on the Transfuser backbone.\n\nReasoning-Enhanced Decoder To improve scene reasoning in complex driving scenarios, we design an efficient decoder integrating the MoE, harnessing Mamba to enhance trajectory generation.\n\nAction-Motion Interaction We propose an action-motion interaction module via a shared cross-attention that directly integrates the velocity predictions of surrounding agents into ego action to model the dynamic interactions between traffic users.\n\nOur approach surpasses the previous state-of-the-art DiffusionDrive [4] by achieving an 88.9 PDM score and 85.6 EPDM score on the NAVSIM navtest split, demonstrating the superiority and effectiveness of the proposed modules.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效融合多模态数据（如相机和LiDAR）以减少压缩损失？  \n2. 如何在复杂的驾驶场景中实现更高效的推理和动态策略调整？  \n3. 如何将其他交通参与者的预测无缝整合进端到端规划流程？  \n\n【用了什么创新的方案】  \n核心解决方案：本文提出了PIE框架，通过引入双向Mamba融合技术来改善相机和LiDAR数据的多模态融合，减少数据压缩损失。设计了一个集成Mixture-of-Experts的推理增强解码器，以提高复杂驾驶场景的推理能力。同时，提出了一个动作-运动交互模块，通过共享的交叉注意力机制，将周围代理的速度预测直接整合到自我动作中，从而更好地建模交通用户之间的动态交互。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning",
            "authors": "Ana Luiza Mineiro,Francisco Affonso,Marcelo Becker",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Accepted to the 22nd International Conference on Advanced Robotics (ICAR 2025). 7 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18608",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18608",
            "arxiv_html_link": "https://arxiv.org/html/2509.18608v1",
            "abstract": "Reliable navigation in under-canopy agricultural environments remains a challenge due to GNSS unreliability, cluttered rows, and variable lighting. To address these limitations, we present an end-to-end learning-based navigation system that maps raw 3D LiDAR data directly to control commands using a deep reinforcement learning policy trained entirely in simulation. Our method includes a voxel-based downsampling strategy that reduces LiDAR input size by 95.83%, enabling efficient policy learning without relying on labeled datasets or manually designed control interfaces. The policy was validated in simulation, achieving a 100% success rate in straight-row plantations and showing a gradual decline in performance as row curvature increased, tested across varying sinusoidal frequencies and amplitudes.",
            "introduction": "Autonomous robots have seen significant growth in modern agriculture, particularly for under-canopy tasks such as plant phenotyping, crop row harvesting, and disease scouting. These applications require platforms that are not only compact and agile but also capable of accurately navigating between dense crop rows (Fig. 1) [1]. However, reliable navigation in such environments remains an active area of research due to several challenges, including clutter and occlusions caused by narrow row spacing and the high visual variability introduced by different plant growth stages [2].\n\nTo enable decision-making systems capable of navigating through plantations in under-canopy environments, robots typically rely on exteroceptive sensors [3]. This choice is driven by the unreliability of GNSS-based localization in such environments, where signal degradation is common due to foliage occlusion [4]. As a result, there is a need for local perception strategies that allow the robot to infer navigation information from its immediate surroundings.\n\nRecent methods have adopted learning-based approaches that aim to extract row-following features directly from curated and labeled datasets, using cameras or LiDAR sensors as the primary source of perception data. On one hand, cameras provide rich semantic and textural information; however, their performance often degrades in low-light conditions, which are common in dense canopy environments [5, 6]. On the other hand, LiDAR offers accurate depth measurements and is unaffected by lighting variations, making it more robust in shaded or poorly lit areas [7, 8]. Additionally, while a sim-to-real gap exists for LiDAR data, the structural consistency of point clouds tends to generalize well across training scenarios. This characteristic allows learning approaches based on point cloud data to effectively leverage large-scale simulated datasets [9].\n\nIn addition, most of these methods typically divide the navigation task into separate modules (e.g., perception, locomotion controller), which introduces challenges in designing effective interfaces between them [10]. As a result, important environmental features may be lost or overly simplified when translated into reference values for the controller. Combined with the high cost and impracticality of collecting labeled data across diverse agricultural conditions, these limitations hinder generalization and reduce overall system robustness.\n\nFor instance, to address the limitations of curated datasets and the challenge of designing interfaces between navigation modules, deep reinforcement learning (RL) can be explored. This approach enables agents to learn end-to-end policies by mapping perceptual observations directly to navigation actions through interaction with the environment. The control policy is optimized via trial-and-error, guided by reward functions that encode the desired task [11, 12].\n\nIn this paper, we present an end-to-end row-following navigation system for under-canopy agricultural environments that maps raw 3D LiDAR data directly to control commands using a RL policy trained in simulation. Our approach avoids the need for manually designed interfaces between navigation modules and does not rely on expert-labeled datasets. Additionally, we propose a LiDAR downsampling technique that transforms raw point clouds into compact row maps, significantly reducing the observation space while preserving essential structural features for learning.\n\nThe key contributions of this work are:\n\nAn end-to-end learning system for crop row-following using only raw 3D LiDAR data.\n\nA voxel-based downsampling technique to convert LiDAR point clouds into compact 2D row maps.\n\n1. An end-to-end learning system for crop row-following using only raw 3D LiDAR data.\n\n2. A voxel-based downsampling technique to convert LiDAR point clouds into compact 2D row maps.",
            "llm_summary": "【关注的是什么问题】  \n1. 在农业环境中，如何实现可靠的导航以应对GNSS不可靠性和环境复杂性？  \n2. 如何有效利用3D LiDAR数据进行自主导航而不依赖于标注数据或手动设计的控制接口？  \n3. 如何通过深度强化学习优化导航策略以提高系统的鲁棒性和通用性？  \n\n【用了什么创新的方案】  \n本研究提出了一种端到端的学习系统，利用原始3D LiDAR数据直接映射到控制命令，采用深度强化学习策略在模拟环境中训练。通过引入体素下采样技术，将LiDAR点云转换为紧凑的2D行地图，显著减少了观察空间，同时保留了学习所需的结构特征，从而避免了手动设计模块接口的需求。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
            "authors": "Neel P. Bhatt,Yunhao Yang,Rohan Siva,Pranay Samineni,Daniel Milan,Zhangyang Wang,Ufuk Topcu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Systems and Control (eess.SY)",
            "comment": "Codebase, datasets, and videos for VLN-Zero are available at:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18592",
            "code": "https://vln-zero.github.io/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18592",
            "arxiv_html_link": "https://arxiv.org/html/2509.18592v1",
            "abstract": "Rapid adaptation in unseen environments is essential for scalable real-world autonomy, yet existing approaches rely on exhaustive exploration or rigid navigation policies that fail to generalize. We present VLN-Zero, a two-phase vision-language navigation framework that leverages vision-language models to efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic navigation. In the exploration phase, structured prompts guide VLM-based search toward informative and diverse trajectories, yielding compact scene graph representations. In the deployment phase, a neurosymbolic planner reasons over the scene graph and environmental observations to generate executable plans, while a cache-enabled execution module accelerates adaptation by reusing previously computed task–location trajectories. By combining rapid exploration, symbolic reasoning, and cache-enabled execution, the proposed framework overcomes the computational inefficiency and poor generalization of prior vision-language navigation methods, enabling robust and scalable decision-making in unseen environments. VLN-Zero achieves 2x higher success rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned baselines, and reaches goal locations in half the time with 55% fewer VLM calls on average compared to state-of-the-art models across diverse environments.††The full codebase, datasets, and videos for VLN-Zero are available at https://vln-zero.github.io/.",
            "introduction": "Deploying autonomous agents in new environments remains a fundamental challenge: policies trained in one setting often fail in another due to novel layouts, obstacles, or constraints, and consequently require fine-tuning or multi-shot inference.\nFor example, a robot trained to navigate one office building may struggle in a different building unless retrained, a process that is both slow and impractical for rapid deployment.\n\nThe central problem is twofold: (i) How can an agent efficiently construct a representation of an unseen environment, such as a scene graph, without exhaustive search?\n(ii) Given this representation, how can the agent efficiently generate constraint-satisfying plans in real time without fine-tuning or multi-shot inference?\n\nExisting approaches, ranging from frontier-based exploration to reinforcement learning with fixed policies, struggle with either computational inefficiency, lack of generalization, or both. Vision-language navigation models (VLNs) offer promise; however, current approaches suffer from slow, exhaustive exploration, weak task decomposition, and high training and query cost.\n\nWe argue that addressing these limitations requires rethinking the interaction between perception, symbolic reasoning, and policy adaptation. Specifically, agents must be able to (i) rapidly acquire symbolic representations of their environment to minimize exploration cost, and (ii) leverage these representations for efficient navigation in new environments without retraining or extensive fine-tuning.\n\nTo address these challenges, we introduce VLN-Zero, a two-phase zero-shot framework that combines vision-language model (VLM) guided exploration with neurosymbolic navigation. In the exploration phase, the agent interacts with the environment using structured and compositional task prompts, guiding exploration toward informative and diverse trajectories to construct a compact scene graph with semantic area labels. In the deployment phase, a neurosymbolic planner reasons over this scene graph, environmental observations to generate executable plans, eliminating reliance on fixed policies. To further improve scalability, we propose a cache-enabled execution procedure that stores previously computed task–location trajectories for reuse, accelerating both exploration and deployment.\n\nIn summary, VLN-Zero offers three key contributions:\n\nVLM-guided rapid exploration: We design structured, compositional prompts that steer a VLN agent to propose exploration actions while incrementally constructing compact symbolic scene graphs. This enables coverage of novel environments within a time- and compute-constrained exploration budget while avoiding unsafe behaviors.\n\nZero-shot neurosymbolic navigation: We introduce a planner that reasons jointly over\nscene graphs, task prompts, and real-time observations, transforming free-form natural language\ninstructions into constraint-satisfying action sequences without fine-tuning or multi-shot inference.\n\nCache-enabled execution for fast adaptation: We develop a trajectory-level caching mechanism\nthat stores validated task–location pairs, allowing the system to reuse previously computed plans.\nThis reduces redundant VLM queries to minimize execution time, cost, and compute demands which accelerates real-world deployment.\n\n1. VLM-guided rapid exploration: We design structured, compositional prompts that steer a VLN agent to propose exploration actions while incrementally constructing compact symbolic scene graphs. This enables coverage of novel environments within a time- and compute-constrained exploration budget while avoiding unsafe behaviors.\n\n2. Zero-shot neurosymbolic navigation: We introduce a planner that reasons jointly over\nscene graphs, task prompts, and real-time observations, transforming free-form natural language\ninstructions into constraint-satisfying action sequences without fine-tuning or multi-shot inference.\n\n3. Cache-enabled execution for fast adaptation: We develop a trajectory-level caching mechanism\nthat stores validated task–location pairs, allowing the system to reuse previously computed plans.\nThis reduces redundant VLM queries to minimize execution time, cost, and compute demands which accelerates real-world deployment.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在未见环境中高效构建场景图表示，而不依赖于耗时的探索？  \n2. 如何在实时生成约束满足的导航计划，而无需进行微调或多次推理？  \n\n【用了什么创新的方案】  \n核心解决方案：VLN-Zero是一个两阶段的零-shot框架，结合了基于视觉语言模型（VLM）的快速探索和神经符号导航。在探索阶段，使用结构化提示引导VLM进行有效的探索，构建紧凑的场景图。在部署阶段，神经符号规划器基于场景图和环境观察生成可执行计划，同时通过缓存机制加速适应，重用先前计算的任务-位置轨迹，从而提高决策效率和可扩展性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA",
            "authors": "Zeyi Kang(1),Liang He(2),Yanxin Zhang(3),Zuheng Ming(4),Kaixing Zhao(5) ((1) Northwestern Polytechnical University, (2) University Sorbonne Paris Nord)",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18576",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18576",
            "arxiv_html_link": "https://arxiv.org/html/2509.18576v1",
            "abstract": "Multimodal semantic learning plays a critical role in embodied intelligence, especially when robots perceive their surroundings, understand human instructions, and make intelligent decisions. However, the field faces technical challenges such as effective fusion of heterogeneous data and computational efficiency in resource-constrained environments. To address these challenges, this study proposes the lightweight LCMF cascaded attention framework, introducing a multi-level cross-modal parameter sharing mechanism into the Mamba module. By integrating the advantages of Cross-Attention and Selective parameter-sharing State Space Models (SSMs), the framework achieves efficient fusion of heterogeneous modalities and semantic complementary alignment. Experimental results show that LCMF surpasses existing multimodal baselines with an accuracy of 74.29% in VQA tasks and achieves competitive mid-tier performance within the distribution cluster of Large Language Model Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a 4.35-fold reduction in FLOPs relative to the average of comparable baselines while using only 166.51M parameters (image-text) and 219M parameters (video-text), providing an efficient solution for Human-Robot Interaction (HRI) applications in resource-constrained scenarios with strong multimodal decision generalization capabilities.",
            "introduction": "In contemporary research, powerful multimodal understanding capabilities have emerged as the foundational element for enabling robotic perception, cognition, and interaction within complex dynamic environments[1]. In the domain of embodied intelligence, Vision-Language Pre-training (VLP) [2] has advanced into a critical technological paradigm for the development of sophisticated intelligent robotic systems, offering substantial support for the realization of more intelligent HRI [3]. Concurrently, to address the challenges inherent in multimodal learning, such as the scarcity of labeled data and the prohibitive costs of annotation, self-supervised learning [4] has garnered considerable attention and research focus. More precisely, by setting multi-task optimization objectives (multimodal masked modeling, contrastive learning, etc.) [5], these methods provide possibilities for common robotic tasks, such as environmental understanding [6], decision-making (Visual Question Answering (VQA) [7], Embodied Question Answering (EQA) [8]) or even more advanced cross-modal general understanding [9].\n\nHowever, embodied intelligence [10] still faces numerous challenges that limit learning capabilities in visual-language decision tasks. At the semantic understanding level, current models [11, 12, 13] struggle to reconstruct fine-grained mask labels, resulting in an information gap between local features, mask features, and global scene understanding. In addition, the efficiency problem of long sequence modeling cannot be ignored, as the Transformer architecture’s computational complexity grows quadratically when processing large-scale sequence data [14, 15], making it difficult to achieve optimal trade-offs between cross-modal understanding performance and hardware efficiency.\n\nIn response to the above challenges, this paper proposes the lightweight LCMF architecture, which achieves high-quality multimodal understanding and inference acceleration on low-computation robotic platforms. LCMF uses a semantic diffusion mechanism [11] to address the information gap in multi-scale visual semantics and enhance the ability to model fine-grained masked information. For cross-modal interaction, Cross-Modality Mamba (CMM) extends the Mamba state-space model to the multimodal domain, achieving comprehensive optimization in hardware awareness, time efficiency, and lightweight design. At the level of multimodal semantic fusion, Enhanced Mamba Fusion (EMF) introduces efficient semantic bridging mechanisms and fine-grained feature modulation techniques, enabling the effective integration of heterogeneous modality semantics.\n\nIn summary, our contributions include:\n\n1) CMM implements multi-level sharing of state space parameters and parallel modeling of multimodal long sequence semantics, achieving linear computational complexity and inference acceleration.\n\n2) LCMF has implemented a lightweight Mamba-Transformer (Selective SSMs-Attention) architecture in the fields of unimodal feature extraction, multimodal (image, text, video) interaction, and multimodal fusion.\n\n3) Under a significantly reduced parameter scale compared to existing multimodal baselines and LLM Agents, LCMF achieves improved computational efficiency while maintaining strong performance on downstream tasks such as VQA and EQA, demonstrating its effectiveness in efficient multimodal modeling.\n\nThe rest of this article is organized as follows.\n\nSection II reviews research on Mamba variant architecture, VQA, EQA.\nSection III details the LCMF model architecture, pretraining, fine-tuning, and evaluation methods.\nSection IV describes the experimental setup, performance evaluation, and ablation experiments on specific downstream tasks.\nFinally, Section V concludes this article.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效融合异构数据以提升机器人在复杂环境中的理解能力？  \n2. 如何在资源受限的环境中实现高效的多模态学习和决策？  \n3. 当前模型在视觉语言决策任务中面临的学习能力限制是什么？  \n\n【用了什么创新的方案】  \n本研究提出了轻量级的LCMF框架，通过引入多级跨模态参数共享机制，结合Cross-Attention和选择性状态空间模型（SSMs），实现了异构模态的高效融合和语义互补对齐。该框架在保持较低计算复杂度的同时，显著提升了多模态理解能力，并在视觉问答（VQA）和视频问答（EQA）任务中表现出色，展示了其在资源受限场景下的有效性和强大的多模态决策泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Spatial Envelope MPC: High Performance Driving without a Reference",
            "authors": "Siyuan Yu,Congkai Shen,Yufei Xi,James Dallas,Michael Thompson,John Subosits,Hiroshi Yasuda,Tulga Ersal",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18506",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18506",
            "arxiv_html_link": "https://arxiv.org/html/2509.18506v1",
            "abstract": "This paper presents a novel envelope-based model predictive control (MPC) framework designed to enable autonomous vehicles to handle high-performance driving across a wide range of scenarios without a predefined reference.\nIn high-performance autonomous driving, safe operation at the vehicle’s dynamic limits requires a real-time planning and control framework capable of accounting for key vehicle dynamics and environmental constraints when following a predefined reference trajectory is suboptimal or even infeasible. State-of-the-art planning and control frameworks, however, are predominantly reference-based, which limits their performance in such situations.\nTo address this gap, this work first introduces a computationally efficient vehicle dynamics model tailored for optimization-based control and a continuously differentiable mathematical formulation that accurately captures the entire drivable envelope.\nThis novel model and formulation allow for the direct integration of dynamic feasibility and safety constraints into a unified planning and control framework, thereby removing the necessity for pre-defined references.\nThe challenge of envelope planning, which refers to maximally approximating the safe drivable area, is tackled by combining reinforcement learning with optimization techniques.\nThe framework is validated through both simulations and real-world experiments, demonstrating its high performance across a variety of tasks, including racing, emergency collision avoidance and off-road navigation.\nThese results highlight the framework’s scalability and broad applicability across a diverse set of scenarios.",
            "introduction": "High-performance autonomous driving technology has advanced rapidly and significantly in the past decade [1].\nThe grand opening of the first autonomous racing competition has thrust autonomous driving technology into the spotlight, showcasing performance that rivals expert human drivers in tasks demanding high levels of precision and speed [2]. Recent developments in autonomous driving have enabled reliable performance in advanced collision avoidance capabilities [3, 4, 5], complex drifting maneuvers [6, 7, 8], and challenging off-road navigation [9, 10, 11].\nThese advancements have greatly expanded the potential applications of autonomous vehicles across various fields. To execute these extreme maneuvers safely, the vehicle must operate at the limits of its capabilities.\nHowever, this is a challenge, because even a small deviation from the desired trajectory can result in catastrophic outcomes.\n\nIn this regard, the autonomous system should be designed to effectively utilize all feasible operational regions including the limits, ensuring performance is not unnecessarily sacrificed for safety via overly conservative constraints.\nFor instance, every inch of the race track is critical for drivers to adjust their strategies and enhance their performance. In addition, the algorithm’s design should ensure scalability when applied to a diverse set of scenarios with varying levels of complexity.\nIn this context, the term ‘scalability’ refers to the algorithm’s ability to handle an increasing amount of tasks or scenarios without a significant compromise in performance or without significant redesign.\nIt is also desirable that the system be capable of generating optimal trajectories online in real-time without having to depend on a predefined reference, because deviations from the original plan may render the predefined reference suboptimal or even infeasible.\n\nHowever, as the literature review in Sec. II reveals, existing methods typically rely on predefined references and therefore either limit vehicle performance to the quality of that reference or do not provide optimal and scalable solutions in performance-demanding scenarios.\n\nTo address this gap, this paper presents a novel, spatial envelope model predictive control (MPC) framework for reference-free high performance driving.\nThe proposed framework builds upon a new, computationally efficient vehicle dynamics model tailored for closed-loop optimization based planning and control, capturing the essential dynamics required for aggressive maneuvers.\nA twice continuously differentiable mathematical formulation of the entire driving envelope is introduced to conservatively estimate the drivable region to be used in MPC.\nThis enables the MPC to break from the restrictive constant-speed assumption previously used in spatial envelope MPC [12, 13, 14, 5], and instead optimize speed, as well, to maximize performance while maintaining safety.\n\nTo the authors’ knowledge, this is the first published MPC algorithm that is experimentally validated for safe and effective high-performance driving at the handling limits in a fully reference-free setting.\nThe algorithm is validated across a wide range of scenarios, including racing, off-road navigation and emergency collision avoidance, demonstrating both generality and real-world applicability.\n\nFinally, a new spatial envelope planning technique is introduced to further enhance applicability. A hybrid approach that combines optimization-based formulation with reinforcement learning is developed to segment the drivable area into blocks, enabling scalable planning in complex environments.\n\nThe original contributions are summarized as follows:\n\nA validated 3-DoF single-track dynamic model that accounts for longitudinal load transfer and the friction circle limit, while remaining computationally efficient in a fully reference-free setting.\n\nA validated 3-DoF single-track dynamic model that accounts for longitudinal load transfer and the friction circle limit, while remaining computationally efficient in a fully reference-free setting.\n\nA hard constraint formulation to mathematically express the spatial envelope with guaranteed conservativeness.\n\nA real-time Model Predictive Control (MPC) formulation that leverages the first two contributions to optimize vehicle trajectories online without any predefined path.\n\nA reinforcement learning approach to design a set of blocks to approximate arbitrary shapes of spatial envelopes in real time.\n\nValidation of the proposed MPC formulation in racing, emergency collision avoidance and off-road environments.\n\nThe rest of the paper is organized as follows. Sec. II reviews the relevant literature. Sec. III-A describes the 3 DoF single-track vehicle dynamics. Sec. III-B describes the MPC formulation including the conservative spatial envelope constraints. Sec. III-C describes the real-time spatial envelope planner. Sec. IV describes the model fidelity test. The results and discussion of the proposed spatial envelope MPC are presented in Sec. IV.\nFrom Sec. V-A to E, the simulation and experimental results of spatial envelope MPC are conducted and analyzed in multiple scenarios.\nIn Sec. III-F, the proposed spatial envelope planning technique is presented and discussed.\nFinally, Sec. VI concludes the study.",
            "llm_summary": "【关注的是什么问题】  \n1. 高性能自动驾驶中缺乏有效的参考轨迹规划与控制方法。  \n2. 现有方法过于依赖预定义参考，限制了车辆性能。  \n3. 如何在复杂环境中实现实时、无参考的轨迹优化。  \n\n【用了什么创新的方案】  \n本研究提出了一种新颖的空间包络模型预测控制（MPC）框架，旨在实现高性能的无参考自动驾驶。该框架结合了高效的车辆动力学模型和连续可微的数学公式，能够准确捕捉可行驶区域。通过将动态可行性和安全约束直接整合到规划和控制框架中，消除了对预定义参考的依赖。此外，采用强化学习与优化技术相结合的方法，能够在复杂环境中进行可扩展的规划。该框架在多种场景下进行了验证，包括赛车、紧急避障和越野导航，展示了其广泛的适用性和高性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain",
            "authors": "Junnosuke Kamohara,Feiyang Wu,Chinmayee Wamorkar,Seth Hutchinson,Ye Zhao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18466",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18466",
            "arxiv_html_link": "https://arxiv.org/html/2509.18466v1",
            "abstract": "Model predictive control (MPC) has demonstrated effectiveness for humanoid bipedal locomotion; however, its applicability in challenging environments, such as rough and slippery terrain, is limited by the difficulty of modeling terrain interactions.\nIn contrast, reinforcement learning (RL) has achieved notable success in training robust locomotion policies over diverse terrain, yet it lacks guarantees of constraint satisfaction and often requires substantial reward shaping.\nRecent efforts in combining MPC and RL have shown promise of taking the best of both worlds, but they are primarily restricted to flat terrain or quadrupedal robots.",
            "introduction": "Legged locomotion conventionally employs model-based controllers (MBCs), particularly Model Predictive Control (MPC), due to their optimization-based constraint satisfaction [1, 2].\nWhile whole-body dynamics models [3] are more accurate,\nresearchers use simplified models [4, 5, 6, 7] for computational efficiency and consequently suffer from model mismatch due to simplification of the dynamics.\nAs a result, simplified models exhibit poorer tracking accuracy and instability, particularly during contact [8].\nAdditionally, MPC with simplified dynamics usually requires predefined contact sequence and swing leg trajectory, which limits its adaptivity to diverse terrains.\nOverall, the deterministic but inaccurate dynamic model and manual constraint design of MPC restrict its robustness and versatility, limiting its applicability to diverse terrains in the real world.\n\nIn contrast, learning-based controls (LBC), exemplified by Reinforcement Learning (RL) methods, have gained wide attention for their robustness and agility [9, 10, 11, 12, 13].\nBy training policies parameterized by neural networks, RL policies can achieve zero-shot transfer from simulation to reality.\nHowever, training robust policies requires substantial environmental interactions and extensive reward shaping.\nFurthermore, RL policies lack explicit constraint satisfaction because of the absence of explicit constraints.\n\nMotivated by the unique advantages of both sides, recent years have witnessed a surge of methods combining model-based and learning-based approaches, leveraging the safety offered by MPC’s explicit constraints as well as powerful reactive behaviors offered by RL [14, 15].\nIn legged robotics, there are two main threads of combination.\nThe first thread uses MPC within a policy.\nRecent works either adopt a hierarchical architecture, where RL parametrizes MPC’s components, including system dynamics, center of mass reference trajectory, and gait frequency [16, 8, 17, 18]; or follows a parallel architecture, where RL policies refine MPC outputs by adding corrective actions such as footholds and joint commands [19, 20, 21].\nAnother thread uses MPC as an expert policy, training the policy through behavior cloning or RL with imitation loss to increase sample efficiency and motion accuracy [22, 13, 23, 24].\nEach of these designs carries trade-offs:\nMPC as an expert improves training efficiency by imitating MPC motions, yet it incurs significant computational overhead during training due to repeated optimization solves, making training in parallelized RL environments particularly challenging [24].\nWhile parallel architectures offer flexibility by directly augmenting MPC outputs, they raise safety concerns since the RL policy bypasses feasibility constraints from optimization.\nHierarchical architectures, in contrast, preserve the optimization structure and computational complexity, as the policy is evaluated before solving the optimization problem.\nThis ensures the feasibility and constraint satisfaction within the optimization framework.\n\nDespite these advances, most combined approaches for bipedal locomotion remain limited to flat terrain, as prior works primarily emphasize improving tracking accuracy rather than adaptability [19, 21], leaving integration of MPC and RL for rough-terrain-adaptive locomotion unexplored.\nIn this work, we aim to enhance the adaptability of humanoid locomotion, enabling responsive and robust behaviors in the face of terrain disturbances.\nWe leverage a hierarchical method that augments MPC via RL by incorporating rich whole-body information into the simplified system model, adjusting the gait frequency to modulate step length, and modifying the swing foot trajectory to improve robustness against challenging terrain.\nWe focus on addressing the limitations of MPC with simplified dynamics: model mismatch, predefined swing leg curve, and static gait frequency.\nThe RL policy learns residual dynamics through whole-body dynamics simulation, as well as swing leg curve parameters, including apex height and control points, and dynamic gait frequency within one locomotion cycle.\n\nThese learned adaptations enable reactive behaviors, including recovery from foot entrapment and severe slippage.\nWe implement our method on bipedal locomotion tasks with the HECTOR robot [25] in NVIDIA IsaacLab, a state-of-the-art GPU-accelerated simulator [26].\nOur framework significantly improves robustness against disturbances on diverse terrains, including slippery surfaces, stairs, and stepping stones.\nAdditionally, we conduct ablation studies on the three residual modules to analyze the contribution of each component.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高双足机器人在复杂地形上的适应性和鲁棒性。  \n2. 现有的模型预测控制（MPC）在动态建模和适应性方面的局限性。  \n3. 强化学习（RL）在约束满足和训练效率方面的不足。  \n\n【用了什么创新的方案】  \n本研究提出了一种层次化方法，通过将强化学习（RL）与模型预测控制（MPC）相结合，增强了双足机器人在复杂地形上的适应性。该方法利用丰富的全身信息来调整简化系统模型，学习残余动态和摆腿曲线参数，并动态调整步频，从而提高在滑面、楼梯和踏石等多样地形上的鲁棒性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task",
            "authors": "Jannick van Buuren,Roberto Giglio,Loris Roveda,Luka Peternel",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18463",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18463",
            "arxiv_html_link": "https://arxiv.org/html/2509.18463v1",
            "abstract": "This paper explores how deliberate mutations of reward function in reinforcement learning can produce diversified skill variations in robotic manipulation tasks, examined with a liquid pouring use case. To this end, we developed a new reward function mutation framework that is based on applying Gaussian noise to the weights of the different terms in the reward function. Inspired by the cost-benefit tradeoff model from human motor control, we designed the reward function with the following key terms: accuracy, time, and effort. The study was performed in a simulation environment created in NVIDIA Isaac Sim, and the setup included Franka Emika Panda robotic arm holding a glass with a liquid that needed to be poured into a container. The reinforcement learning algorithm was based on Proximal Policy Optimization. We systematically explored how different configurations of mutated weights in the rewards function would affect the learned policy. The resulting policies exhibit a wide range of behaviours: from variations in execution of the originally intended pouring task to novel skills useful for unexpected tasks, such as container rim cleaning, liquid mixing, and watering. This approach offers promising directions for robotic systems to perform diversified learning of specific tasks, while also potentially deriving meaningful skills for future tasks.",
            "introduction": "For robots to successfully operate in unstructured and unpredictable real-world environments, they need the ability to constantly adapt and learn many tasks. One way to do this is to learn from human demonstration [1]. However, human involvement can be costly, and humans are not always available to correct or teach robots new skills. Indeed, an alternative is reinforcement learning (RL) that allows the robots to autonomously acquire new skills through trial‑and‑error interaction with their environment [2]. The robot is given an objective function (typically from a human), which then guides its autonomous exploration to obtain a policy of how to perform a given task. At each timestep, the robot observes the current state, executes an action according to its policy, and receives a scalar reward. Over many episodes, it refines its policy to maximise the expected sum of discounted rewards, thereby acquiring skills optimised for long‑term success.\n\nRL has been successfully applied to robots to solve a diverse range of tasks, ranging from pick-and-place actions [3, 4], object lifting [5], to assembly [6], as well as play ball-in-the-cup game [7], table tennis [8], and air hockey [9]. Within this context, the liquid pouring task [10, 11, 12, 13] stands out as a particularly compelling benchmark for investigating the role of reward function design in shaping learned behaviours. Unlike binary success criteria seen in stacking or placement tasks, pouring involves balancing multiple continuous objectives, such as avoiding spillage, reducing effort, and maximising efficiency, making it highly sensitive to how learning is incentivised.\n\nRL has achieved impressive results in specific robotic tasks with well-crafted and tailored reward functions. This typically results in good skills specialised for the given task, but lacks generalisation capabilities when new tasks arise. Learning new tasks is typically relatively long and sample-inefficient, especially in complex tasks without prior knowledge and where rewards are sparse or delayed. Rather than relying solely on environmental feedback, an agent can benefit from understanding the reward logic itself, such as temporal dependencies, conditional sequences, or subgoals—thereby improving learning efficiency and policy quality [14, 15, 16]. By leveraging structured reward representations, agents can more effectively sequence and reuse behaviours, enabling them to adjust previously learned skills to new or modified tasks. This structured approach facilitates faster adaptation, as agents can generalise from prior experience rather than starting from scratch each time [17].\n\nAnother approach to reduce learning time and improve generalisability is to utilise direct prior task knowledge from models or human demonstrations [18, 2]. In that way, an agent already has a rough policy, which then only needs to be refined and optimised for the given specifics of the robot and the environment. However, resulting policies that are not considered optimal for the given specific task are often discarded. We argue that such “failed” or “suboptimal” policies should not be discarded, since such skill variations resulting from various mutations might be useful starting points for learning new tasks.\n\nMutation of policy in robot learning when subject to physical interaction with humans and unpredictable environments has been observed in [19]. A follow-up study [20] investigated how the policy mutations occur during the learning and what kind of variations of skill emerge in a sawing task. The study concluded that certain policy variations may not be optimal or suitable for the original task, but can be useful for optimising some other parameters/tasks. This highlighted the potential for diversification of skills and the importance of not discarding the policies that appear to be suboptimal for the current task, as they might be a good starting point for unforeseen new tasks. However, the work so far relied on mutations from a passive environment, while intentional mutations with a systematic mechanism are still missing.\n\nTo address this challenge, we introduce a reward mutation framework that treats the reward function as a tunable mechanism for active skill diversification (Fig. 1). While online reward-shaping has been investigated for improvement of sample efficiency and optimisation of a specific task in [21], differently, the proposed approach mutates the reward function to discover new skills. As a starting point, we study a liquid pouring task performed by a Franka Emika Panda robotic manipulator in the NVIDIA Isaac Sim simulation environment, where the reward is composed of three weighted terms: pouring accuracy, time spent, and effort spent. The goal of the study is to systematically explore how the mutation of the weights of these reward terms induces the emergence of diverse policies. We perform training of agents with Proximal Policy Optimization (PPO) [22] under 25 distinct reward configurations. Some of the emerging policies are useful for executing the original task in different ways (fast and slow), some are identified to be useful for unforeseen other tasks (container rim cleaning, liquid mixing, watering), and some are not useful for any identifiable tasks.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何通过变异奖励函数来实现机器人技能的多样化。  \n2. 现有的强化学习方法在新任务学习中的效率和泛化能力不足。  \n3. 如何有效利用“次优”策略作为新任务学习的起点。  \n\n【用了什么创新的方案】  \n本研究提出了一种奖励变异框架，通过对奖励函数中不同权重施加高斯噪声，系统性地探索了不同奖励配置对学习策略的影响。该方法允许机器人在液体倒入任务中产生多样化的技能，从而不仅优化原有任务的执行方式，还为意外任务（如容器边缘清洁、液体混合和浇水）提供了有用的技能。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems",
            "authors": "Haeyoon Han,Mahdi Taheri,Soon-Jo Chung,Fred Y. Hadaegh",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18460",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18460",
            "arxiv_html_link": "https://arxiv.org/html/2509.18460v1",
            "abstract": "Perception systems provide a rich understanding of the environment for autonomous systems, shaping decisions in all downstream modules. Hence, accurate detection and isolation of faults in perception systems is important. Faults in perception systems pose particular challenges: faults are often tied to the perceptual context of the environment, and errors in their multi-stage pipelines can propagate across modules. To address this, we adopt a counterfactual reasoning approach to propose a framework for fault detection and isolation (FDI) in perception systems. As opposed to relying on physical redundancy (i.e., having extra sensors), our approach utilizes analytical redundancy with counterfactual reasoning to construct perception reliability tests as causal outcomes influenced by system states and fault scenarios. Counterfactual reasoning generates reliability test results under hypothesized faults to update the belief over fault hypotheses. We derive both passive and active FDI methods. While the passive FDI can be achieved by belief updates, the active FDI approach is defined as a causal bandit problem, where we utilize Monte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find control inputs that maximize a detection and isolation metric, designated as Effective Information (EI). The mentioned metric quantifies the informativeness of control inputs for FDI. We demonstrate the approach in a robot exploration scenario, where a space robot performing vision-based navigation actively adjusts its attitude to increase EI and correctly isolate faults caused by sensor damage, dynamic scenes, and perceptual degradation.",
            "introduction": "Autonomous systems such as self-driving cars, unmanned aerial vehicles (UAV), and autonomous robots rely on perception systems to convert heterogeneous sensor measurements into a coherent representation of their surrounding environment [1]. The role of the perception system is to provide accurate and timely information on objects, terrain, and the surrounding environment so that higher-level modules in an autonomous system (e.g., localization, motion planning, and control) can guarantee safety and achieve mission objectives [2]. The combination of utilizing heterogeneous sensors (e.g., LiDAR, radar, cameras) and deep learning-based algorithms has led to recent advances in perception-based control. However, this has also resulted in an increased level of complexity in perception systems, which makes detecting their faults and algorithmic errors challenging [3, 4]. Considering the importance of a perception system in the guidance and control of an autonomous system, perception faults can result in the complete loss of a mission. For instance, on 6 June 2025, the Japanese lunar lander Resilience (Hakuto-R Mission 2) had a hard landing during its final descent on the Moon when its laser range finder began outputting erroneous altitude values in the last few kilometers before touchdown [5]. This highlights the need for accurate monitoring systems that can address the problem of fault detection and isolation (FDI) in perception systems.\n\nThe method presented in this paper can handle a broad range of fault and failure types, including both physical malfunctions and algorithmic errors in perception systems that cause deviations from their intended functionality. On the physical side, sensors can suffer calibration shifts, temporary occlusions, and environmental interference [7]. At the algorithmic level, deep neural networks (DNN) can misclassify objects due to distribution shifts (i.e., out-of-distribution inputs), and multi-sensor fusion can become erroneous due to calibration issues [8, 9, 10]. Moreover, faults that occur at an early stage of a perception system’s pipeline propagate through it and do not remain isolated [11]. Hence, FDI methodologies that rely on physical redundancy may not be sufficient [12]. Thus, one needs to study and investigate FDI methodologies based on the available analytical redundancy in perception systems. Once a certain fault is detected and isolated, a fault recovery control can be implemented.\n\nThe faults that occur in Simultaneous Localization and Mapping (SLAM) and Visual Inertial Odometry (VIO) systems are sensor faults [3, 7], tracking failures [15, 16], data association failures [17, 18], and filtering inconsistency problems [19]. Sensor faults are caused by hardware damage or software malfunction. Faults in front-end modules, such as tracking and data association failures, are often caused by visually deprived conditions (i.e., textureless surfaces and repetitive patterns), dynamic scenes (i.e., aggressive camera motion), and undesirable lighting conditions (i.e., high-contrast images). Lastly, the filtering inconsistency problems, a type of fault in back-end modules, result from large inter-frame transformations that trigger the accumulation of linearization errors.\n\nThe work in [8] compares perception outputs with a predefined fault threshold for runtime monitoring. Additionally, [3] developed fault diagnostic graphs to associate errors with individual perception module outputs, as evaluated by diagnostic tests. Although these works enable FDI, they rely on having redundant sensors, which can be costly. To enhance the robustness of SLAM [20] developed image quality metrics to select confident features or scenes. Similarly, feature quality metrics that assess keypoint co-visibility between frames [15, 21, 3] and the dynamic scene metrics that leverage vehicle velocity [15], optical flow [22], and image sharpness [23, 24] have been proposed.\n\nWe define perception reliability tests for various fault modes to capture differences between fault-free and fault-induced behaviors. We utilize the structural causal model (SCM) formalism of Pearl [25] and its operational rules for interventions and counterfactual queries, where we treat each hypothesized fault mode as an intervention on the perception pipeline. We then introduce and define an information-theoretic metric based on the Kullback–Leibler (KL) divergence between the reliability test results and those from a baseline fault-free case to measure the detectability and isolability of the hypothesized faults. This metric, designated as Effective Information (EI), captures how control inputs influence the reliability test results by affecting the autonomous system’s state. To the best of our knowledge, this is the first work that studies the FDI as a counterfactual reasoning problem for a closed-loop autonomous system and also connects the informativeness of control inputs to the detection and isolation of the hypothesized faults. Finally, we show that finding the control input that helps maximizing the EI leads to having a causal bandit problem [26], where each action arm corresponds to an intervention on the control input that improves our FDI accuracy. A Monte-Carlo Tree Search (MCTS) approach with Upper Confidence Bound (UCB) [27, 28] that penalizes large deviations from primary mission objectives (e.g., tracking a trajectory) is employed to solve the mentioned causal bandit problem.\n\nThe main contributions of this paper are as follows.\n\nWe exploit analytical redundancy of the perception system and actively use control inputs for FDI by applying the do-operator from causal inference. This is achieved via a counterfactual reasoning approach, where it is analyzed how control inputs affect reliability test outcomes under various fault hypotheses. A quantitative detection and isolation metric measuring the informativeness of each control input for FDI is introduced.\n\nWe formulate the problem of selecting control inputs for FDI as a causal bandit problem. Using a MCTS strategy with UCB, we maximize a weighted reward function that prioritizes inputs informative about the most likely fault modes. In addition, our reward function penalizes large deviations from the desired trajectory of the system.\n\nOur FDI method uses the distribution of reliability test results under various fault modes and accounts for the uncertainty inherent in the perception system’s outputs. Thus, our method encodes more information than mean value and threshold-based FDI methods, which only reflect the central tendency of a distribution.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在感知系统中准确检测和隔离故障。  \n2. 传统的物理冗余方法在故障检测中的局限性。  \n3. 如何利用反事实推理提高故障检测和隔离的有效性。  \n\n【用了什么创新的方案】  \n本研究提出了一种基于反事实推理的故障检测和隔离（FDI）框架，利用分析冗余而非物理冗余来构建感知可靠性测试。通过生成假设故障下的可靠性测试结果，更新对故障假设的信念。我们将FDI问题建模为因果赌博问题，使用蒙特卡洛树搜索（MCTS）和上置信界（UCB）策略来最大化有效信息（EI），从而优化控制输入以提高故障检测和隔离的准确性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands",
            "authors": "Yunshuang Li,Yiyang Ling,Gaurav S. Sukhatme,Daniel Seita",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18455",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18455",
            "arxiv_html_link": "https://arxiv.org/html/2509.18455v1",
            "abstract": "Nonprehensile manipulation, such as pushing and pulling, enables robots to move, align, or reposition objects that may be difficult to grasp due to their geometry, size, or relationship to the robot or the environment. Much of the existing work in nonprehensile manipulation relies on parallel-jaw grippers or tools such as rods and spatulas. In contrast, multi-fingered dexterous hands offer richer contact modes and versatility for handling diverse objects to provide stable support over the objects, which compensates for the difficulty of modeling the dynamics of nonprehensile manipulation.\nTherefore, we propose Geometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile manipulation with dexterous robotic hands. We study pushing and pulling by framing the problem as synthesizing and learning pre-contact dexterous hand poses that lead to effective manipulation. We generate diverse hand poses via contact-guided sampling, filter them using physics simulation, and train a diffusion model conditioned on object geometry to predict viable poses.\nAt test time, we sample hand poses and use standard motion planners to select and execute pushing and pulling actions.\nWe perform 840 real-world experiments with an Allegro Hand, comparing our method to baselines. The results indicate that GD2P offers a scalable route for training dexterous nonprehensile manipulation policies. We further demonstrate GD2P on a LEAP Hand, highlighting its applicability to different hand morphologies. Our pre-trained models and dataset, including 1.3 million hand poses across 2.3k objects, will be open-source to facilitate further research.\nOur project website is available at: geodex2p.github.io.",
            "introduction": "Nonprehensile actions are fundamental to how humans and robots interact with the physical world [4, 5, 6, 7].\nThese actions permit the manipulation of objects that may be too large, heavy, or geometrically complex to grasp directly.\nWhile there has been tremendous progress in nonprehensile robot manipulation [8, 9, 10, 11, 12], most work uses simple end-effectors such as parallel-jaw grippers, rods [13, 14], or spatulas [15]. In contrast, multi-fingered hands with high degrees-of-freedom (DOF) such as the Allegro Hand or LEAP Hand [16] enable contact patterns that can be especially useful for stabilizing complex, awkward, or top-heavy objects, or for coordinating contact across multiple objects, compensating for the challenges of modeling nonprehensile manipulation dynamics.\nHowever, despite their promise and recent progress [17] [18], leveraging high-DOF hands for nonprehensile manipulation remains relatively underexplored due to the challenges of modeling hand-object relationships and planning feasible contact-rich motions.\n\nIn this paper, we study pushing and pulling objects using the 4-finger, 16-DOF Allegro and LEAP Hands. We select pushing and pulling as representative tasks of nonprehensile manipulation because they are more commonly used for manipulating general daily objects and are\nmore amenable to scaling.\nOur insight is to recast this problem into one of synthesizing effective pre-contact hand poses, an approach inspired by recent success in generating large-scale datasets for dexterous manipulation [19, 20, 21, 22, 23, 24].\nWe propose a scalable pipeline for generating hand poses for pushing and pulling objects. This involves contact-guided optimization and validation via GPU-accelerated physics simulation with IsaacGym [25].\nThese filtered hand poses are then used to train a generative diffusion policy conditioned on object geometry, represented using basis point sets [1].\n\nAt test time, we use visual data to reconstruct an object mesh in physics simulation. The trained diffusion policy uses this mesh to generate diverse hand poses for pushing or pulling. We then validate the resulting hand poses in simulation, and execute the best-performing action in the real world.\nWe call this pipeline Geometry-aware Dexterous Pushing and Pulling (GD2P) with multi-fingered hands.\nFigure LABEL:fig:pull shows several real-world examples where the hand pose differs depending on object geometry. Overall, our experimental results across diverse daily objects demonstrate that GD2P is a promising approach for generalizable object pushing and pulling. It outperforms alternative methods such as querying the nearest hand pose in our data or using a fixed spatula-like hand pose, highlighting the need for a diffusion model to generate diverse hand poses.\n\nTo summarize, the contributions of this paper include:\n\nA scalable pipeline for generating and filtering dexterous hand poses for nonprehensile pushing and pulling.\n\nA diffusion model for geometry-conditioned hand pose prediction for nonprehensile pushing and pulling.\n\nA motion planning framework to execute these poses in the real world, with results across 840 trials showing that GD2P outperforms alternative methods.\n\nA dataset of 1.3 million hand poses for nonprehensile pushing and pulling across 2.3k objects with corresponding canonical point cloud observations.\n\n1. A scalable pipeline for generating and filtering dexterous hand poses for nonprehensile pushing and pulling.\n\n2. A diffusion model for geometry-conditioned hand pose prediction for nonprehensile pushing and pulling.\n\n3. A motion planning framework to execute these poses in the real world, with results across 840 trials showing that GD2P outperforms alternative methods.\n\n4. A dataset of 1.3 million hand poses for nonprehensile pushing and pulling across 2.3k objects with corresponding canonical point cloud observations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用多指灵巧手进行非抓取操作（如推拉）以处理复杂物体。  \n2. 如何生成有效的手部预接触姿势以提高非抓取操作的成功率。  \n3. 如何在真实环境中执行基于物体几何的手部姿势以实现有效的推拉操作。  \n\n【用了什么创新的方案】  \n提出了一种几何感知的灵巧推拉（GD2P）方法，通过接触引导采样生成多样的手部姿势，并利用物理仿真进行过滤。训练一个条件于物体几何的扩散模型来预测可行的手部姿势。测试时，使用标准运动规划器选择并执行推拉动作。该方法在840次真实实验中表现优于基线，展示了其在不同手形态上的适用性，并提供了130万个手部姿势的数据集以促进后续研究。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction",
            "authors": "Rishabh Madan,Jiawei Lin,Mahika Goel,Angchen Xie,Xiaoyu Liang,Marcus Lee,Justin Guo,Pranav N. Thakkar,Rohan Banerjee,Jose Barreiros,Kate Tsui,Tom Silver,Tapomayukh Bhattacharjee",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Conference on Robot Learning (CoRL)",
            "pdf_link": "https://arxiv.org/pdf/2509.18447",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18447",
            "arxiv_html_link": "https://arxiv.org/html/2509.18447v1",
            "abstract": "Physical human–robot interaction (pHRI) requires robots to adapt to individual contact preferences, such as where and how much force is applied. Identifying preferences is difficult for a single contact; with whole-arm interaction involving multiple simultaneous contacts between the robot and human, the challenge is greater because different body parts can impose incompatible force requirements. In caregiving tasks, where contact is frequent and varied, such conflicts are unavoidable. With multiple preferences across multiple contacts, no single solution can satisfy all objectives–trade-offs are inherent, making prioritization essential. We present PrioriTouch, a framework for ranking and executing control objectives across multiple contacts. PrioriTouch can prioritize from a general collection of controllers, making it applicable not only to caregiving scenarios such as bed bathing and dressing but also to broader multi-contact settings. Our method combines a novel learning-to-rank approach with hierarchical operational space control, leveraging simulation-in-the-loop rollouts for data-efficient and safe exploration. We conduct a user study on physical assistance preferences, derive personalized comfort thresholds, and incorporate them into PrioriTouch. We evaluate PrioriTouch through extensive simulation and real-world experiments, demonstrating its ability to adapt to user contact preferences, maintain task performance, and enhance safety and comfort. Website: https://emprise.cs.cornell.edu/prioritouch.",
            "introduction": "Physical human–robot interaction (pHRI) requires physical contact. Contact is not uniform: individuals have distinct preferences for acceptable forces and contact locations [1, 2, 3, 4, 5]. For pHRI to be safe and effective, robots must personalize their behavior, and a critical aspect of personalization is contact preferences. Even for a single contact, identifying and respecting these preferences while ensuring task success is challenging. Many physical robot caregiving tasks, such as bathing [6, 7], dressing [8, 9], and transferring [10], require whole‑arm pHRI [11], where multiple segments of the robot arm simultaneously touch the human body. For example, during bed bathing (Fig. 1), the robot may need to reach over a user to wipe the upper arm while maintaining comfortable forces on the torso and shoulder. Although whole‑arm manipulation expands workspace and improves maneuverability, it also exacerbates conflicts: different body parts can impose incompatible force requirements, and no single policy can satisfy all objectives.\n\nTo bootstrap personalization, we elicit population‑level contact preferences offline and use them to seed a conservative base policy. However, a one‑size‑fits‑all policy is insufficient: (i) stated preferences can diverge from realized comfort under true contact (pressure/shear, approach, speed, duration); and (ii) preferences are context‑dependent and time‑varying (posture, clothing, fatigue). Therefore, online interaction is necessary to accommodate individual preferences. Experimenting directly with the user is risky and inefficient because each update can involve repeated physical contact and multiple feedback exchanges. This increases the user’s cognitive workload, prolongs the interaction, and may cause discomfort when forces are suboptimal or excessive.\n\nWe introduce PrioriTouch, a framework that casts contact preference learning as a learning‑to‑rank problem over control objectives. Given a reference trajectory produced by a high‑level policy (e.g., a contact‑aware planner generating end‑effector or joint‑space paths), PrioriTouch instantiates pose‑tracking and force‑regulation objectives from the current contact state. We develop LinUCB‑Rank, a contextual bandit that learns a priority policy; H‑OSC [12] then executes this ordering as a null space hierarchy, translating high‑level preferences into low‑level control. We initialize the policy with conservative priors derived from population-level user-study statistics. During interaction, LinUCB-Rank adapts the ordering online using sparse user feedback while safely refining the policy via simulation-in-the-loop learning before deploying it in real-world interactions. The framework is controller-agnostic: it can rank heterogeneous objectives, enabling principled trade-offs across simultaneous objectives.\n\nWe evaluate PrioriTouch across simulated and real-world environments, progressively increasing in complexity and realism. First, we design a simplified simulation scenario with predefined contacts and a static end-effector pose to isolate and specifically assess LinUCB-Rank’s ability to learn user contact preferences. Second, we demonstrate PrioriTouch in a simulated caregiving scenario involving robot-assisted bed bathing, requiring whole-arm contact to safely wipe a user’s limbs. Third, we showcase our approach’s capability in intricate multi-contact scenarios through a real-world 3D goal-reaching maze with multiple vertical cylinders representing distinct body-part contact preferences. Finally, we validate PrioriTouch’s practical feasibility by performing a realistic caregiving task in a user study with human subjects.\n\nOur contributions are summarized as follows:\n\nWe propose PrioriTouch, a framework that formulates contact preference learning as a ranking problem over control objectives and executes the learned priority ordering as a null space hierarchy via H-OSC for whole-arm pHRI.\n\nWe introduce LinUCB‑Rank, a contextual bandit that learns priority orderings from sparse user feedback while accounting for inter‑objective coupling in hierarchical control.\n\nWe enable safe and data-efficient learning through simulation-in-the-loop validation, where candidate priority updates are tested in a digital twin before real-world deployment.\n\nWe conduct a user study to inform realistic models of contact preferences for robot-initiated touch, which we leverage to simulate authentic user feedback in our evaluation.\n\nWe evaluate PrioriTouch through extensive simulation, real-world experiments, and a realistic caregiving user study, demonstrating effective adaptation to individual contact preferences without compromising task performance or comfort.\n\nOur framework integrates user contact preference learning with low-level control by parameterizing operational space control using the outputs of a learned ranking policy. This structured integration ensures that high-level feedback is directly translated into low-level force regulation and pose tracking, effectively bridging the gap between user preferences and robot control.\n\n1. We propose PrioriTouch, a framework that formulates contact preference learning as a ranking problem over control objectives and executes the learned priority ordering as a null space hierarchy via H-OSC for whole-arm pHRI.\n\n2. We introduce LinUCB‑Rank, a contextual bandit that learns priority orderings from sparse user feedback while accounting for inter‑objective coupling in hierarchical control.\n\n3. We enable safe and data-efficient learning through simulation-in-the-loop validation, where candidate priority updates are tested in a digital twin before real-world deployment.\n\n4. We conduct a user study to inform realistic models of contact preferences for robot-initiated touch, which we leverage to simulate authentic user feedback in our evaluation.\n\n5. We evaluate PrioriTouch through extensive simulation, real-world experiments, and a realistic caregiving user study, demonstrating effective adaptation to individual contact preferences without compromising task performance or comfort.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在物理人机交互中适应个体的接触偏好。  \n2. 在多接触场景中，如何处理不同身体部位的力要求冲突。  \n3. 如何高效地学习和调整接触偏好以确保安全和舒适。  \n\n【用了什么创新的方案】  \nPrioriTouch框架将接触偏好学习视为控制目标的排名问题，通过LinUCB-Rank上下文赌博机从稀疏用户反馈中学习优先级排序，并利用H-OSC以空心空间层次结构执行该排序。该方法结合了模拟环中的验证，确保在真实世界中的安全部署，同时通过用户研究获得个性化的接触阈值，以增强适应性和任务性能。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Latent Action Pretraining Through World Modeling",
            "authors": "Bahey Tharwat,Yara Nasser,Ali Abouzeid,Ian Reid",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18428",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18428",
            "arxiv_html_link": "https://arxiv.org/html/2509.18428v1",
            "abstract": "Vision-Language-Action (VLA) models have gained popularity for learning robotic manipulation tasks that follow language instructions. State-of-the-art VLAs, such as OpenVLA and π0\\pi_{0}, were trained on large-scale, manually labeled action datasets collected through teleoperation. More recent approaches, including LAPA and villa-X, introduce latent action representations that enable unsupervised pretraining on unlabeled datasets by modeling abstract visual changes between frames. Although these methods have shown strong results, their large model sizes make deployment in real-world settings challenging.\nIn this work, we propose LAWM, a model-agnostic framework to pretrain imitation learning models in a self-supervised way, by learning latent action representations from unlabeled video data through world modeling. These videos can be sourced from robot recordings or videos of humans performing actions with everyday objects. Our framework is designed to be effective for transferring across tasks, environments, and embodiments. It outperforms models trained with ground-truth robotics actions and similar pretraining methods on the LIBERO benchmark and real-world setup, while being significantly more efficient and practical for real-world settings.",
            "introduction": "Self-supervised learning has been a key enabler of recent breakthroughs in Large Language Models (LLMs) such as ChatGPT [1] and Gemini [2], where models learn from large amounts of text on the Internet. Inspired by this success, the robotics community is now ready for its own transformative moment, where we can build systems that learn action representations directly from raw, unstructured video data, rather than relying on curated action labels.\n\nMost current approaches to robot learning are heavily based on supervised learning frameworks. Methods like imitation learning and VLA models, including OpenVLA [3] and π0\\pi_{0} [4], require paired image action datasets often obtained through teleoperation. These action annotations are expensive to collect, difficult to scale, and prone to bias, limiting the generalizability of these systems across tasks, environments, and embodiments.\n\nIn this work, we introduce LAWM, a Latent Action pretraining framework through World Modeling that aims to overcome these limitations by combining an Imitation Learning Model with a World Model. Our objective, as shown in Fig. LABEL:fig:first-page, is to learn action representations from both robot-collected and human demonstration videos in a fully self-supervised way. These learned representations serve as action priors that can be effectively leveraged during finetuning on downstream tasks. The proposed framework, illustrated in Figure 2, is designed to be model-agnostic, meaning that it does not depend on any specific architecture for the imitation learning model or the world model. This flexibility allows for the integration of a variety of different models.\nOur pipeline follows a two-stage steps. The first stage is an end-to-end pretraining in a self-supervised way, with the learning signal derived from predicting the next image in a video sequence. The inputs to the system consist of: (i) an image frame from a human or robot performing a manipulation task, and (ii) a natural language instruction describing the goal of the task. The imitation learning model takes these inputs to produce action chunks representations. These representations of nn actions are paired with the current image frame and the next n−1n-1 frames, then fed into the world model to generate the next image frames that would result from executing the action in the current environment. The second stage is a finetuning stage, where labeled data are used to finetune only the imitation learning model to downstream tasks. During this phase, the world model is no longer used. The learned imitation learning model is now equipped with a robust prior from large-scale unlabeled videos and can be finetuned efficiently.\n\nWe summarize our main contributions and findings below:\n\nWe propose LAWM, a model-agnostic framework, to learn action chunk representations for imitation learning models from both robot and human videos without action labels.\n\nOur experiments show that our framework can learn superior action priors from human demonstrations and robotic manipulation videos without using ground-truth action labels, compared to supervised pretraining.\n\nWe demonstrate that our framework with small models such as BAKU [5] and Dreamerv3 [6] outperforms similar methods with large models such as villa-X [7] on the LIBERO benchmark  [8].\n\n1. We propose LAWM, a model-agnostic framework, to learn action chunk representations for imitation learning models from both robot and human videos without action labels.\n\n2. Our experiments show that our framework can learn superior action priors from human demonstrations and robotic manipulation videos without using ground-truth action labels, compared to supervised pretraining.\n\n3. We demonstrate that our framework with small models such as BAKU [5] and Dreamerv3 [6] outperforms similar methods with large models such as villa-X [7] on the LIBERO benchmark  [8].",
            "llm_summary": "【关注的是什么问题】  \n1. 当前机器人学习方法依赖于昂贵的监督学习框架，限制了系统的可扩展性。  \n2. 大规模标注数据集的收集困难且容易产生偏差，影响模型的泛化能力。  \n3. 现有方法在真实世界应用中面临模型规模过大和部署效率低的问题。  \n\n【用了什么创新的方案】  \n提出LAWM框架，通过世界建模在自监督方式下学习潜在动作表示，利用机器人和人类视频数据，无需动作标签。该框架采用两阶段流程：第一阶段进行自监督预训练，第二阶段在有标签数据上微调模仿学习模型。LAWM在LIBERO基准测试中表现优于依赖真实动作标签的模型，且在真实环境中更高效、实用。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections",
            "authors": "Navya Tiwari,Joseph Vazhaeparampil,Victoria Preston",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
            "comment": "6 pages, 5 figures. Accepted as a poster at Northeast Robotics Colloquium (NERC 2025). Extended abstract",
            "pdf_link": "https://arxiv.org/pdf/2509.18407",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18407",
            "arxiv_html_link": "https://arxiv.org/html/2509.18407v1",
            "abstract": "未获取到摘要",
            "introduction": "Intersections account for nearly 40% of U.S. crashes [1], with many occurring at uncontrolled or partially controlled locations [2]. Ambiguous right of way, compounded by occlusions, non-compliant drivers, and limited sensing leave drivers uncertain how to act [3]. Addressing these challenges requires assistive technology that reduces driver uncertainty and improves awareness. We propose an Advanced Driver Assistance System (ADAS) that fuses sensor data, interprets intersection context, and applies uncertainty-aware frameworks to recommend safe actions at uncontrolled intersections. We pose three research questions (RQs):\n\nHow can the accuracy of ego- and external-vehicle state estimation be improved while constraining uncertainty through frustum-based fusion of camera and lidar data, given real-time, computationally limited resources?\n\nHow can the accuracy of ego- and external-vehicle state estimation be improved while constraining uncertainty through frustum-based fusion of camera and lidar data, given real-time, computationally limited resources?\n\nTo what extent can a driver-assist system enable safe navigation of uncontrolled intersections by efficiently interpreting intersection context (lane markings, stop signs, traffic flow patterns, and pedestrian presence) under partial observability?\n\nHow effectively can different decision-making frameworks handle uncertainty at uncontrolled intersections, and what trade-offs emerge between safety, efficiency, and computational feasibility in real-time deployment?\n\nHere, we present an initial analysis of RQ3 in synthetic uncontrolled intersections. We show that probabilistic planners, particularly POMCP (Partially Observable Monte Carlo Planning) [7] and DESPOT (Determinized Sparse Partially Observable Tree) [15], outperform deterministic approaches in predicting the intent of other drivers and selecting collision-free actions, while maintaining safety under complex, partially observable scenarios. Continued work will integrate Sensor Fusion (RQ1) and Environment Perception (RQ2) modules for end-to-end, real-time navigation under realistic traffic and environmental conditions.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高自我和外部车辆状态估计的准确性，同时在实时计算资源有限的情况下约束不确定性？  \n2. 如何在部分可观测的情况下高效解读交叉口环境，以实现安全导航？  \n3. 不同决策框架在处理不确定性时的有效性如何，以及在实时部署中安全性、效率和计算可行性之间的权衡是什么？  \n\n【用了什么创新的方案】  \n提出了一种先进的驾驶辅助系统（ADAS），通过融合传感器数据、解释交叉口上下文，并应用不确定性感知框架，推荐在无人控制交叉口的安全行动。初步分析表明，概率规划方法（如POMCP和DESPOT）在预测其他驾驶员意图和选择无碰撞行动方面优于确定性方法，同时在复杂的部分可观测场景中保持安全。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation",
            "authors": "Rajitha de Silva,Jonathan Cox,James R. Heselden,Marija Popovic,Cesar Cadena,Riccardo Polvara",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "Sumbitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.18342",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18342",
            "arxiv_html_link": "https://arxiv.org/html/2509.18342v1",
            "abstract": "Accurate localisation is critical for mobile robots in structured outdoor environments, yet LiDAR-based methods often fail in vineyards due to repetitive row geometry and perceptual aliasing. We propose a semantic particle filter that incorporates stable object-level detections, specifically vine trunks and support poles into the likelihood estimation process. Detected landmarks are projected into a bird’s eye view and fused with LiDAR scans to generate semantic observations. A key innovation is the use of semantic walls, which connect adjacent landmarks into pseudo-structural constraints that mitigate row aliasing. To maintain global consistency in headland regions where semantics are sparse, we introduce a noisy GPS prior that adaptively supports the filter. Experiments in a real vineyard demonstrate that our approach maintains localisation within the correct row, recovers from deviations where AMCL fails, and outperforms vision-based SLAM methods such as RTAB-Map.",
            "introduction": "Accurate localisation is a critical component of mobile robot navigation in outdoor environments [1]. Among the various approaches, LiDAR-based localisation remains widely adopted due to its reliable and precise perception of geometric structure. However, these methods rely solely on scene geometry, which can be problematic in outdoor agricultural settings like vineyards, where repetitive and ambiguous structures are common [2]. In such environments, incorporating semantic information complements the geometric structure offering a promising alternative to enhance localisation performance [3].\n\nIn this paper, we tackle the challenge of semantic ambiguity in geometry-based localisation within vineyard environments. The repetitive structure of vineyard rows often induces perceptual aliasing in LiDAR range data, resulting in localisation drift and errors. To overcome this limitation, we exploit semantically meaningful landmarks, specifically vine trunks and support poles whose distinctive spatial distributions provide stronger discriminative cues. Our approach detects these semantic objects and estimates their relative positions from RGB-D imagery, which are then projected onto the LiDAR frame. This enables a semantic-LiDAR particle filter that offers a robust alternative to conventional localisation methods.\n\nTraditional particle filters, such as Adaptive Monte Carlo Localisation (AMCL) [4], estimate a robot’s pose by evaluating the geometric consistency between sensor observations and a known map, an approach that has proven highly effective in structured indoor and urban settings where distinctive geometric features are abundant. Vineyards, however, present a markedly different challenge: their long, repetitive rows induce strong perceptual aliasing, while unstable elements such as foliage and grape clusters provide little reliability for long-term localisation [5]. We contend that robust localisation in such environments requires moving beyond raw geometry and explicitly exploiting semantics. Our key insight is that vine trunks and support poles serve as stable, distinctive landmarks whose consistent spatial distribution across rows can disambiguate pose estimates. Moreover, we introduce the concept of semantic walls, where the space between consecutive landmarks is modelled as a pseudo-structural boundary. This transforms sparse semantic detections into continuous row-level constraints, creating a representation that is far more robust to vineyard aliasing and seasonal variation. Together, these ideas lay the foundation for a semantic-LiDAR particle filter that redefines localisation in repetitive agricultural environments, as illustrated in Fig. 1.\n\nThe main contributions of this paper are threefold:\n(i) the design of a semantic particle filter that integrates object-level detections of stable vineyard landmarks (trunks and support poles) with a 2D semantic map, enabling robust localisation in highly repetitive environments;\n(ii) the introduction of the semantic walls concept, which augments sparse landmark detections by modelling pseudo-structural boundaries between adjacent landmarks, thereby strengthening row-level constraints and mitigating perceptual aliasing; and\n(iii) a systematic evaluation against established baselines, demonstrating the trade-offs in accuracy, robustness, and sensor requirements.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在重复结构的环境中实现准确的移动机器人定位。  \n2. 如何克服LiDAR方法在葡萄园中因感知别名而导致的定位漂移和错误。  \n3. 如何利用语义信息增强几何结构以提高定位性能。  \n\n【用了什么创新的方案】  \n提出了一种语义粒子滤波器，将稳定的对象级检测（如葡萄藤干和支撑杆）融入到定位过程中，通过将检测到的地标投影到鸟瞰图并与LiDAR扫描融合生成语义观测。此外，引入了语义墙的概念，将相邻地标连接成伪结构约束，以减轻行别名问题，并在语义稀疏的区域引入噪声GPS先验以维持全局一致性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020",
            "authors": "Marsette Vona",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18330",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18330",
            "arxiv_html_link": "https://arxiv.org/html/2509.18330v1",
            "abstract": "The Landform contextual mesh fuses 2D and 3D data from up to thousands of Mars 2020 rover images, along with orbital elevation and color maps from Mars Reconnaissance Orbiter, into an interactive 3D terrain visualization. Contextual meshes are built automatically for each rover location during mission ground data system processing, and are made available to mission scientists for tactical and strategic planning in the Advanced Science Targeting Tool for Robotic Operations (ASTTRO). A subset of them are also deployed to the ”Explore with Perseverance” public access website.",
            "introduction": "Like its predecessors including Spirit, Opportunity, and Curiosity, the Mars 2020 Perseverance rover carries a suite of stereo cameras to image the surrounding terrain [1, 2, 3]. Data from those instruments is used for multiple purposes including both on-board and ground-based navigation, engineering operations, and science analysis. An important use case is tactical and strategic science planning, where teams of mission scientists use the imagery to select areas of interest and plan subsequent observations.\n\nStereo vision produces a “tactical wedge” 3D terrain mesh for each stereo image pair, so-called because often a radial panorama of such wedges is acquired from a single rover location using the pan/tilt mast. Of course, only a fraction of nearby terrain is included since the cameras have limited fields of view, effective resolution decreases with distance, the rover occludes areas underneath itself, nearby rocks and hills create self-occlusions in the terrain, and stereo reconstruction fails in areas with insufficient texture. Nevertheless, viewing such panoramas of tactical wedges in 2D and 3D (latter also called the tactical mesh) has been a standard approach for science planning on Mars 2020 and its predecessors. Figure 1(a) shows such a view for site 40, drive 132 of the Perseverance rover, acquired on sols 821–832111One sol is the equivalent of a day on Mars..\n\nAn alternative to the tactical mesh is to use a portion of a digital elevation map (DEM) derived from orbital observations, as shown in Figure 1(b). The Mars 2020 mission typically uses colored DEM data from the HiRISE instrument on the Mars Reconnaissance Orbiter [4]. This orbital mesh can cover a much larger extent—up to 10s of km—and typically has no gaps in its coverage. However, at a typical resolution of 1 elevation sample (and 16 color samples) per square meter, it’s much coarser than the tactical mesh, which can have sub-millimeter resolution near the rover.\n\nIn this paper we introduce the contextual mesh, which we have developed to fuse up to thousands of images from in-situ stereo cameras together with orbital DEM data into a single 3D scene, shown in Figure 1(c) and Figure 2. The contextual mesh is produced by Landform, a subsystem within the Mars 2020 ground data system (M20 GDS), and typically viewed in the Advanced Science Targeting Tool for Robotic Operations (ASTTRO) collaborative web application [5, 6], also part of the M20 GDS [7].\n\nWhereas the tactical mesh offers the highest fidelity local terrain reconstruction, and the orbital mesh the longest range reconstruction, the intention of the contextual mesh is to provide spatial awareness. It is typically visualized from a first-person navigable 3D point of view in ASTTRO, showing not only local terrain features such as sand, pebbles, rocks, ridges, and hills, but also distant landmarks on the horizon. ASTTRO also displays a 3D model of the Perseverance rover on the terrain as it was posed at the corresponding time in the mission.\n\nEach contextual mesh is comprised of two tilesets in the open-standard 3DTiles format [8]. One tileset contains the terrain itself, typically extending to a 1km square with a 100m square central detail area. The other tileset is a hemispherical representation of the surrounding horizon and sky, enabling visualization of distant features potentially many kilometers away. ASTTRO displays both of these simultaneously so that users can see the context of local terrain features relative to both nearby hills and to the horizon.\n\nThe 3DTiles format enables data to be progressively streamed to distributed users in the web-based ASTTRO client. Only the subset of tile data required depending on the user’s current viewpoint is transferred and rendered, enabling dynamic level-of-detail, fast load times, and deployment to resource limited clients.\n\nLike the products of many data fusion and reconstruction algorithms, e.g. computed tomography, and considering that the input data contains noise and outliers, the contextual mesh may contain some artifacts. For example\n\nboundaries between areas reconstructed primarily from surface vs orbital data may have some discontinuities\n\noutlier images with extreme brightness variations may not be completely blended\n\nreconstructed geometry may have “island” topological artifacts due to noise and residual misalignment in the input data.\n\nThe Landform contextual mesh differs from many other photogrammetry and terrain fusion systems not only because it combines both surface and orbital data and has a sky sphere, but also in that it is entirely automated, whereas most other systems require some human intervention. It heavily leverages properties of Mars mission datasets, including pose priors from rover navigation, calibrated stereo camera data, and co-registered orbital data. These enable automated processing while maintaining reasonable quality relative to manual and semi-manual approaches.\n\nThe Landform codebase will soon be released as open source. And, throughout the mission, a selection of contextual meshes have been made publicly available for interactive viewing at the “Explore with Perseverance” website [9].\n\nIn this paper we summarize the research context of related terrain fusion approaches, describe the novel algorithms we developed to implement the contextual mesh system in Landform, and present examples of contextual mesh data products.\n\n1. boundaries between areas reconstructed primarily from surface vs orbital data may have some discontinuities\n\n2. outlier images with extreme brightness variations may not be completely blended\n\n3. reconstructed geometry may have “island” topological artifacts due to noise and residual misalignment in the input data.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效融合火星探测器的2D和3D数据以生成高质量的地形可视化。  \n2. 在自动化处理过程中如何保持数据融合的质量，减少人为干预。  \n3. 如何在动态环境中实现3D场景的实时可视化和交互。  \n\n【用了什么创新的方案】  \n本文提出了一种名为“Landform contextual mesh”的新方法，通过自动融合来自火星2020探测器的数千张立体图像与轨道数字高程图（DEM）数据，生成一个交互式的3D地形可视化。该方法利用了火星任务数据集的特性，自动处理并生成高保真度的地形重建，提供了局部和远程地形特征的空间意识。此外，使用3DTiles格式实现数据的渐进式流式传输，支持动态细节级别和快速加载时间。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Haptic Communication in Human-Human and Human-Robot Co-Manipulation",
            "authors": "Katherine H. Allen,Chris Rogers,Elaine S. Short",
            "subjects": "Robotics (cs.RO)",
            "comment": "9 pages, 18 figures, ROMAN 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18327",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18327",
            "arxiv_html_link": "https://arxiv.org/html/2509.18327v1",
            "abstract": "When a human dyad jointly manipulates an object, they must communicate about their intended motion plans. Some of that collaboration is achieved through the motion of the manipulated object itself, which we call “haptic communication.” In this work, we captured the motion of human-human dyads moving an object together with one participant leading a motion plan about which the follower is uninformed. We then captured the same human participants manipulating the same object with a robot collaborator. By tracking the motion of the shared object using a low-cost IMU, we can directly compare human-human shared manipulation to the motion of those same participants interacting with the robot. Intra-study and post-study questionnaires provided participant feedback on the collaborations, indicating that the human-human collaborations are significantly more fluent, and analysis of the IMU data indicates that it captures objective differences in the motion profiles of the conditions. The differences in objective and subjective measures of accuracy and fluency between the human-human and human-robot trials motivate future research into improving robot assistants for physical tasks by enabling them to send and receive anthropomorphic haptic signals.",
            "introduction": "In physical collaboration tasks like carrying a couch or moving a table, haptic signals are an important channel of communication between participants to coordinate the group action. In human-human interactions, the communication and interpretation of these signals is primarily subconscious, but prior research suggests that they may enable more efficient human-robot collaboration [1]. In order for robots to participate in this haptic conversation, we need to develop a more robust understanding of how haptic communication occurs in both human-human and human-robot interaction. This knowledge can then be used to develop models for interpreting haptic intent, provide robots with comprehensible and predictable behavior, and avoid unwanted oscillations in collaborative manipulation.\n\nIn this paper, we present a study of haptic interaction, without use of visual or auditory signaling, during the collaborative manipulation of a shared object. We compare human-human and human-robot dyads to test whether there are observable differences in the subjective fluency of human-human and human-robot dyads, and whether these correlate with changes in the character of acceleration profiles of the co-manipulated objects. We additionally collect data on human perceptions of robot collaborators to identify potential co-variables in subjective fluency.\n\nWe conducted a user study with 34 participants. In the study, two agents collaborated to move a shared object, with one participant designated as the motion leader and one as a follower. Each participant acted in one these roles, and interacted with both another human participant and a mobile manipulator robot. We collected measures of subjective and objective measures of task fluency, as well as video and IMU recordings. We found that the acceleration data from an IMU mounted on the shared object changes more smoothly in human-human dyads than in human-robot dyads, with more fluent collaborations having smaller accelerations overall and smaller changes in acceleration during the task. We further find that common objective measures of collaboration fluency (e.g. task duration) do not correlate linearly with subjective fluency measures, and propose alternate measures based on our data. This work contributes to our understanding of the differences and similarities in current human-human and human-robot haptic communication during collaborative manipulation, and provides insights that can inform future methods for autonomous haptic signaling by robots.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何理解人类之间的触觉通信在共同操控中的作用？  \n2. 人类与机器人之间的触觉通信是否存在显著差异？  \n3. 如何提高机器人在物理任务中的协作能力？  \n\n【用了什么创新的方案】  \n本研究通过用户研究比较了人类与人类和人类与机器人在共同操控共享物体时的触觉交互。使用低成本IMU捕捉物体运动，分析了不同条件下的加速度数据，发现人类之间的协作流畅性显著高于人机协作。研究还探讨了主观流畅性与客观加速度特征之间的关系，为未来机器人自主触觉信号的研究提供了见解。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Fine-Tuning Robot Policies While Maintaining User Privacy",
            "authors": "Benjamin A. Christie,Sagar Parekh,Dylan P. Losey",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18311",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18311",
            "arxiv_html_link": "https://arxiv.org/html/2509.18311v1",
            "abstract": "Recent works introduce general-purpose robot policies.\nThese policies provide a strong prior over how robots should behave — e.g., how a robot arm should manipulate food items.\nBut in order for robots to match an individual person’s needs, users typically fine-tune these generalized policies — e.g., showing the robot arm how to make their own preferred dinners.\nImportantly, during the process of personalizing robots, end-users leak data about their preferences, habits, and styles (e.g., the foods they prefer to eat).\nOther agents can simply roll-out the fine-tuned policy and see these personally-trained behaviors.\nThis leads to a fundamental challenge: how can we develop robots that personalize actions while keeping learning private from external agents?\nWe here explore this emerging topic in human-robot interaction and develop PRoP, a model-agnostic framework for personalized and private robot policies.\nOur core idea is to equip each user with a unique key; this key is then used to mathematically transform the weights of the robot’s network.\nWith the correct key, the robot’s policy switches to match that user’s preferences — but with incorrect keys, the robot reverts to its baseline behaviors.\nWe show the general applicability of our method across multiple model types in imitation learning, reinforcement learning, and classification tasks.\nPRoP is practically advantageous because it retains the architecture and behaviors of the original policy, and experimentally outperforms existing encoder-based approaches.\nSee videos and code here: https://prop-icra26.github.io",
            "introduction": "Generalist policies enable robots to learn multiple tasks [1, 2].\nSo far these methods have traditionally been used in research labs and factories.\nBut we envision a future where robots enter domestic settings for assisting humans [3].\nFor example, consider a robot that is developed to help in a kitchen.\nThis robot will have some initial policy π0\\pi_{0} that users may want to finetune to match their own preferences and requirements.\nFor instance, perhaps the robot knows how to make a hamburger, but individual users prefer different ingredients, condiments, or even specific ways of stacking the burger.\nThis finetuning raises privacy concerns: the manufacturers can share the users’ data collected during finetuning with third-parties.\nConsequently, there is increasing demand for exploring new avenues to maintain the privacy and transparency of robotic agents [4].\nFollowing this, we come to a fundamental scientific question: how do we make systems that can learn and adapt to individual end-users, while still maintaining those user’s privacy?\n\nPrivacy in machine learning has traditionally been examined from two perspectives.\nFirst is data privacy, which concerns safeguarding the sensitive information of individuals represented in the dataset [5, 6, 7, 8].\nSecond is model privacy, which focuses on protecting the learned parameters of a neural network through techniques such as encryption or differentially-private learning [9, 10, 11, 12].\nIn this work, we adopt a third perspective with respect to robot learning: ensuring that a trained, personalized robot does not leak user preference information to other users.\nReturning to our example, privacy in this context means that the robot can be finetuned to learn your preferred way of making a burger while preventing unauthorized users from accessing those preferences even if they have access to the trained model.\nIn practice, this can be difficult to achieve because — if someone has access to the finetuned model — they can roll-out this model and infer the previous user’s preference by watching the robot actions.\nSo how do we safeguard privacy of user preferences?\nOur insight is that:\n\nConcretely, we leverage keys (Figure 1).\nA key is any feature that is unique to the user such as facial structures, vocal patterns, or a textual password.\nWhen finetuning the robot under out approach, a user combines their unique key with the intermediate features of the network and trains it to output their personalized actions.\nThis unique mechanism for personalizing robots safeguards user privacy since preference information remains inaccessible to anyone who does not have the user’s key.\nWithout careful design, keys may unintentionally cause the robot to forget its general-purpose policy.\nBut our technical approach avoids this pitfall — and preserves the initial model architecture — by leveraging the key to perform mathematical operations on the intermediate weights.\nOur proposed mechanism is not tied to a specific network architecture or application as we later demonstrate in our experiments with visual data, imitation learning, MLP classifiers, and reinforcement learning.\nIndeed, as shown in our experiments on robot arms, users can finetune the robot to make their desired hamburger without losing the robot’s previously learned behaviors, and without exposing their preference to other agents.\nWe see this work as a step towards safe and personalized human-robot interaction.\n\nOverall, we make the following contributions:\n\nKey-based Personalization of Robot Policies.\nWe present a formulation for key-based personalization of robot control policies.\nUnder this formalism, the robot learns to personalize to new users’ specifications while retaining its original, general behavior.\nThis formalism is nontrivial to implement in a learning algorithm, since the original and conditional policies operate in different domains, i.e., adding a key as the input requires changing the size of the pre-trained architecture.\nInstead, we use keys to transform the intermediate features of the pre-trained policy, circumventing the need for changing the architecture size.\n\nPersonalized and Private Robot Policies.\nWe present our implementation of the aforementioned key-based personalization with privacy guarantees.\nOur method, PRoP (Personalized and Private Robot Policies) retains the original network architecture, exhibits behavior of the original robot policy for unprivileged users, and personalizes to specific users through a privacy-oriented mechanism.\nImportantly, PRoP extends to arbitrary learning rules and architectures that enables simple, end-to-end training of the model.\n\nReal-world Validation and Empirically Verified Robustness.\nWe empirically test the performance of PRoP in a collection of controlled simulations and real-world studies, including Imitation Learning, Reinforcement Learning, Image Classification, and Task Allocation.\nWe further extend PRoP to more complex settings, such as language prose personalization and key-based obfuscation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在个性化机器人行为的同时保护用户隐私？  \n2. 现有的机器人政策如何防止用户偏好信息泄露？  \n3. 如何在保持原有政策架构的情况下实现个性化？  \n\n【用了什么创新的方案】  \n本研究提出了一种名为PRoP的模型无关框架，通过为每个用户分配一个唯一的密钥来实现个性化和隐私保护。该密钥用于对机器人的网络权重进行数学变换，使得机器人可以根据用户的偏好进行调整，而未授权用户则无法访问这些偏好信息。此外，该方法保留了原始政策的架构和行为，并在多种学习任务中表现优于现有方法。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies",
            "authors": "Jesse Zhang,Marius Memmel,Kevin Kim,Dieter Fox,Jesse Thomason,Fabio Ramos,Erdem Bıyık,Abhishek Gupta,Anqi Li",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "11 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18282",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18282",
            "arxiv_html_link": "https://arxiv.org/html/2509.18282v1",
            "abstract": "Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: (1) end-effector paths specifying what actions to take, and (2) task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4×\\times real-world improvement for a 3D policy trained only in simulation, and 2–3.5×\\times gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need—where, what, and how.\nWebsite at https://peek-robot.github.io.",
            "introduction": "Imagine walking through a crowded store when your child suddenly cries out, “I want the Labubu!”\nThough you’ve never heard the word before, context clues guide your eyes to the fuzzy toy on the shelf, and you effortlessly weave through the crowd to grab it.\nWhat makes this possible is not raw perception ability, but the ability to interpret ambiguous instructions and distill them into just the right cues—where to focus, what actions to take, and how to perform these actions at the low level. Similarly, if given where to focus and what motions to take, a robot manipulation policy should be able to achieve the visual robustness and semantic generalization necessary for open-world deployment by focusing only on how to perform actions.\n\nA common tactic for training manipulation policies is through imitation learning of human-collected robotics data [1, 2, 3, 4], which attempts to learn the where, what, and how all at the same time.\nYet their performance degrades on novel objects, clutter, or semantic variations [5, 6], since the policy alone bears the burden of handling task, semantic, and visual complexity.\nSuch failures often entangle the axes of where, what, and how—for example, grasping a distractor simultaneously reflects misplaced attention, an incorrect object choice, and a wrong motion.\n\nOur key idea is to offload high-level reasoning to vision-language models (VLMs), which can excel at semantic and visual generalization [7, 8], leaving the policy to determine how low-level behavior should be executed. Instead of forcing the policy to directly parse raw images and instructions, a high-level VLM modulates the input representation to the low-level policy by providing: (1) a path that encodes what the policy should do, and (2) masks showing where to attend. By “absorbing” semantic and visual variation, the VLM provides the policy a simplified, annotated “peek” of the scene that gives the what and the where, while the policy only needs to learn how to perform the low-level actions. This intermediate representation helps policy execution inherit many of the VLM’s semantic and visual generalization capabilities. Our VLM-modulated representation is naturally policy-agnostic, allowing it to be applied to arbitrary image-input robot manipulation policies, including state-of-the-art RGB and 3D manipulation policies [9, 1, 3].\n\nTo concretely instantiate this insight into a practical algorithm, we introduce PEEK (Policy-agnostic Extraction of Essential Keypoints), which proposes a unified, point-based intermediate representation that trains VLMs to predict what policies should do and where to focus on. Specifically, we propose to finetune pretrained VLMs [10] to predict a sequence of points corresponding to (1) a path that guides the robot end-effector in what actions to take and (2) a set of task-relevant masking points that show the policy where to focus on (see Figure 1). During low-level visuomotor policy training and inference, we modulate the policy’s image observations by directly drawing these VLM-predicted paths and masks onto the image, allowing the policy to simply focus on how to act, rather than learning all three simultaneously. Doing so significantly bolsters policy generalization, combining the generality of high-level VLM predictions with the precision of low-level policy learning. In this paper, we instantiate a full-stack implementation of PEEK, from devising a scalable data annotation scheme that enables large-scale VLM finetuning on robotic datasets to representation-modulated training of low-level robot policies from simulation and real world data.\n\nIn 535 real-world evaluations across 17 task variations, we demonstrate that PEEK consistently boosts zero-shot policy generalization: a 3D policy (3DDA [9]) trained only in simulation achieves 41.4×\\times higher success in the real world when guided by PEEK, and both large-scale vision-language-action models (π0\\pi_{0} [3]) and small transformer-based policies [1] see 2–3.5×\\times success rate improvements. These results demonstrate the power of using high-level VLMs to absorb task complexity, providing low-level policies with exactly the minimal cues they need for generalizable manipulation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高机器人操作策略的零-shot泛化能力。  \n2. 如何有效地将高层次推理任务分配给视觉-语言模型（VLMs）。  \n3. 如何简化机器人策略的输入表示以增强其性能。  \n\n【用了什么创新的方案】  \nPEEK（Policy-agnostic Extraction of Essential Keypoints）提出了一种统一的基于点的中间表示，旨在通过微调视觉-语言模型（VLMs）来预测机器人操作所需的路径和关注区域。这种表示直接叠加在机器人观察上，使得策略可以专注于如何执行操作，而不是同时学习“在哪里”、“做什么”和“如何做”。通过引入自动标注管道，PEEK能够在多个机器人数据集上生成标注数据，从而提升零-shot泛化能力，显著提高了在真实世界中的成功率。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "A Fast Initialization Method for Neural Network Controllers: A Case Study of Image-based Visual Servoing Control for the multicopter Interception",
            "authors": "Chenxu Ke,Congling Tian,Kaichen Xu,Ye Li,Lingcong Bao",
            "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19110",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19110",
            "arxiv_html_link": "https://arxiv.org/html/2509.19110v1",
            "abstract": "Reinforcement learning-based controller design methods often require substantial data in the initial training phase. Moreover, the training process tends to exhibit strong randomness and slow convergence. It often requires considerable time or high computational resources.\nAnother class of learning-based method incorporates Lyapunov stability theory to obtain a control policy with stability guarantees. However, these methods generally require an initially stable neural network control policy at the beginning of training.\nEvidently, a stable neural network controller can not only serve as an initial policy for reinforcement learning, allowing the training to focus on improving controller performance, but also act as an initial state for learning-based Lyapunov control methods.\nAlthough stable controllers can be designed using traditional control theory, designers still need to have a great deal of control design knowledge to address increasingly complicated control problems.\nThe proposed neural network rapid initialization method in this paper achieves the initial training of the neural network control policy by constructing datasets that conform to the stability conditions based on the system model.\nFurthermore, using the image-based visual servoing control for multicopter interception as a case study, simulations and experiments were conducted to validate the effectiveness and practical performance of the proposed method.\nIn the experiment, the trained control policy attains a final interception velocity of 15 m/s.",
            "introduction": "In recent years, an increasing number of studies have utilized learning-based methods to address control issues[1]. There are two types of learning-based methods for control problem.\nOne is the reinforcement learning (RL) method, and the other is the learning-based Lyapunov control (LLC) method.\nIn contrast to conventional control methods, RL techniques engage with the environment via trial and error to identify optimal strategies and may complete intricate tasks without dependence on exact models of the controlled entities.\nRL, akin to control systems, functions through feedback mechanisms.\nWhile RL largely uses input to refine its decision-making processes, control systems focus on achieving predetermined targets mainly by using static controller techniques during operation.\nThe training process of RL can be unstable and unsafety[2], especially in safety-critical situations like the unmanned aerial vehicle (UAV) visual servoing control.\nIn order to ensure that the trained policy can be applied in practice, the datasets used in the training process should encompass the Region of Interest (RoI)[3; 4], which is exceedingly challenging before obtaining a available control policy.\n\nThe Lyapunov stability method provides a definitive analytical and design framework in control theory, especially for nonlinear systems [5].\nNumerous research studies have recently integrated Lyapunov stability approaches into learning-based control, referred to as Lyapunov function learning, thereby providing formal stability guarantees for deep neural network policies.\nIn the studies [6; 7], the Lyapunov function is utilized as a critic function to assess policies performance.\nIn [8] and [9], Lyapunov functions are integrated into optimization frameworks to guarantee system stability.\nThe Lyapunov stability condition is incorporated into the reward design in [10] and [11].\nReferences [10] and [11] develop the target control policy by incorporating a Lyapunov function into the reward design.\nThe research in [12] proposes learning the Lyapunov function and its derivative (referred to as the D-function) from expert demonstration data while adhering to stability constraints, thus facilitating the development of a control policy that inherently ensures Lyapunov stability.\nNote that uniformly sampled data is necessary for this approach.\nOtherwise, the D-function employed may not accurately represent the actual system model.\nWhile these methods offer formal stability guarantees for the target policy and yield favorable outcomes, they depend on the posteriori expert controllers or trajectories and are not suitable for the original design of the control policy.\n\nThe posteriori expert controllers are also used as the initial policy of RL to circumvent the drawback of slow convergence at the beginning of the training.\nAlthough the conventional control theory can be applied for the controller designing, the rich experience of that is also important to solve a complex control problem.\nTherefore, this paper proposed an initial policy training method that involves constructing datasets that meet stability requirements and then training a neural network control policy based on the datasets.\nMoreover, acquiring a group of datasets without a stable control policy is exceedingly challenging.\nConversely, without a controller, acquiring the model of controlled objects may be more attainable than gathering data.\nUtilizing the datasets produced by mathematical models that adhere to Lyapunov stability, an untrained neural network may be directly developed into a control policy, circumventing the conventional control design and debugging procedures.\nThe trained neural network control policy can be enhanced further by RL methods or the LLC method that needs an initial stable control policy [2].\nIn this paper, the case of image-based visual servoing control for the multicopter interception is adopted to demonstrate the effectiveness of the proposed method.\nIn the experiment, a final flight speed of up to 15 m/s was achieved.\nNoted that the purpose of this method is to obtain a usable control policy with mediocre performance at least, and the optimization of the control policy still needs to be accomplished through RL and LLC methods.\n\nThe paper is organized as follows: Section II outlines the coordinate systems and mathematical models employed. Section III presents the rapid methodology for training neural network policies, applies it to the design of multicopter interception control, and validates the stability of the trained policies by the almost Lyapunov condition[13]. Section IV presents the experimental result of the trained policies implemented on quadrotor platforms.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何快速初始化神经网络控制器以提高训练效率？  \n2. 如何确保在强化学习和Lyapunov控制中使用的控制策略具备稳定性？  \n\n【用了什么创新的方案】  \n提出了一种快速初始化方法，通过构建符合稳定性条件的数据集来实现神经网络控制策略的初始训练。该方法避免了传统控制设计的复杂性，使得未训练的神经网络可以直接发展为控制策略，从而提高训练效率并确保稳定性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Guaranteed Robust Nonlinear MPC via Disturbance Feedback",
            "authors": "Antoine P. Leeman,Johannes Köhler,Melanie N. Zeilinger",
            "subjects": "Optimization and Control (math.OC); Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "Code:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18760",
            "code": "https://github.com/antoineleeman/robust-nonlinear-mpc",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18760",
            "arxiv_html_link": "https://arxiv.org/html/2509.18760v1",
            "abstract": "Robots must satisfy safety-critical state and input constraints despite disturbances and model mismatch.\nWe introduce a robust model predictive control formulation that is fast, scalable, and compatible with real-time implementation.\nOur formulation guarantees robust constraint satisfaction, input-to-state stability and recursive feasibility.\nThe key idea is to decompose the uncertain nonlinear system into (i) a nominal nonlinear dynamic model, (ii) disturbance-feedback controllers, and (iii) bounds on the model error. These components are optimized jointly using sequential convex programming.\nThe resulting convex subproblems are solved efficiently using a recent disturbance-feedback MPC solver.\nThe approach is validated across multiple dynamics, including a rocket-landing problem with steerable thrust. An open-source implementation is available at https://github.com/antoineleeman/robust-nonlinear-mpc.",
            "introduction": "Autonomous robots, whether agile drones, wheeled machines, or (autonomous) spacecrafts, must operate in dynamic and uncertain environments while satisfying strict safety and performance requirements [1].\nIn addition, model mismatch arises naturally due to many factors, such as wind gusts, actuators misalignments, or unmodelled frictions.\nIn robotics applications, disturbances such as wind gusts, actuator misalignments, or unmodeled friction are typically handled by introducing ad hoc safety margins in the control design, resulting in slower motions, reduced maneuverability, and under-utilization of the system’s capabilities.\n\nReinforcement learning, often with domain randomization, has recently shown success in achieving robust sim2real performance[2], particularly in contact-rich tasks. While learned policies can be executed in real time, training requires extensive offline computation, careful reward design, and heuristics to ensure convergence.\n\nIn contrast, trajectory-optimization methods enforce constraint satisfaction for nonlinear dynamics and are widely used in practice as a model-based control technique based on sequential convex programming [3].\nHowever, these methods typically do not ensure safety or stability in the presence of disturbances, which is critical for real-world deployment.\n\nIn this paper, we introduce a scalable robust model predictive control formulation for nonlinear systems that is safe-by-design.\nRobust model predictive control commonly accounts for disturbances by predicting a set containing all possible future states [4].\nTo reduce conservatism, these robust predictions are based on closed-loop predictions and a corresponding feedback law is typically optimized offline, e.g., using contraction metrics [5, 6].\nHowever, fixing the feedback a priori can limit closed-loop performance and the offline computations also limit scalability.\nRobust model predictive control approaches that optimize feedback laws to reduce conservatism have been proposed in [7, 8], which rely on (conservative) sequential over-approximations of the robust predictions.\nIn contrast, the disturbance feedback MPC [9] framework framework (also known as system level synthesis [10]) provides an exact characterization of the robust prediction for linear time-varying systems, thereby avoiding this compounding effect. Recent extensions [11, 12] further enable its application to nonlinear systems.\nWhile these formulations improve performance compared to fixed policy approaches, they generally do not provide guarantees of recursive feasibility or stability. However, such guarantees are crucial, since loss of feasibility at any step can cause safety constraint violations.\nContribution: Building on the nonlinear SLS formulation in [11],\nwe propose a fast and scalable robust model predictive control formulation for nonlinear systems with robust closed-loop guarantees.\nOur approach jointly optimizes the nominal nonlinear trajectory, a disturbance-feedback controller, and an upper bound on the prediction error.\n\nFormal guarantees are provided, i.e., robust constraint satisfaction, recursive feasibility (Thm. 1), and input-to-state stability (Thm. 2). Recursive feasibility is ensured by a novel treatment of the mismatch with respect to the nonlinear nominal prediction.\n\nFormal guarantees are provided, i.e., robust constraint satisfaction, recursive feasibility (Thm. 1), and input-to-state stability (Thm. 2). Recursive feasibility is ensured by a novel treatment of the mismatch with respect to the nonlinear nominal prediction.\n\nAn efficient sequential convex programming algorithm tailored to the robust MPC formulation is provided to enable real-time deployment. Each iteration consists of solving a nominal trajectory optimization with a quadratic program, updating a disturbance-feedback controller via Riccati recursions, and evaluating Jacobians of the nonlinear dynamics. The design is general and the provided code can be directly applied to systems with large state and input dimensions and long prediction horizons.\n\nReal-time feasibility (computation times) is demonstrated across different dynamics, including a quadcopter and a rocket landing. Robust performance is validated on the rocket-landing problem with steerable thrust including actuator dynamics, illustrated in Fig. 1, demonstrating robust constraint satisfaction with an average total latency of 19.7 [ms] per iteration.\nA comparison to a soft-constrained MPC baseline highlights increased safety and stability of the proposed approach.\n\nNotation:\nFor vectors or matrices aa and bb with the same number of rows, we denote their horizontal concatenation by [a,b][a,~b].\nWe denote stacked vectors or matrices by (a,b)=[a⊤,b⊤]⊤\\left(a,b\\right)=[a^{\\top},~b^{\\top}]^{\\top}. For a vector r∈ℝnr\\in\\mathbb{R}^{n}, we denote its ithi^{\\text{th}} component by rir_{i}.\nFor a sequence of matrices Mk,j∈ℝp×qM_{k,j}\\in\\mathbb{R}^{p\\times q}, indexed by k>j≥0k>j\\geq 0,\nwe define the shorthand horizontal concatenation M(k):=[Mk,k−1,Mk,k−2,…,Mk,0]∈ℝp×k​q.M_{(k)}\\vcentcolon=[M_{k,k-1},~M_{k,k-2},~\\dots,~M_{k,0}]\\in\\mathbb{R}^{p\\times kq}.\nFor a vector v∈ℝnv\\in\\mathbb{R}^{n}, we write its 1-norm as ‖v‖1=|v1|+…+|vn|\\|v\\|_{1}=|v_{1}|+\\ldots+|v_{n}| and its infinity norm as ‖v‖∞=maxi=1,…,n⁡|vi|\\|v\\|_{\\infty}=\\max_{i=1,\\ldots,n}|v_{i}|.\nFor a matrix M∈ℝm×nM\\in\\mathbb{R}^{m\\times n}, the matrix infinity norm is ‖M‖∞=maxi​∑j|Mi​j|\\|M\\|_{\\infty}=\\max_{i}\\sum_{j}|M_{ij}|.\nWe denote sets with calligraphic letters, e.g., 𝒲⊆ℝn\\mathcal{W}\\subseteq\\mathbb{R}^{n}.\nLet ℬm\\mathcal{B}^{m} be the unit ball defined by ℬm:={d∈ℝm|‖d‖∞≤1}\\mathcal{B}^{m}\\vcentcolon=\\{d\\in\\mathbb{R}^{m}|~\\|d\\|_{\\infty}\\leq 1\\}. The Minkowski sum of two sets 𝒜,𝒟⊆ℝn\\mathcal{A},\\mathcal{D}\\subseteq\\mathbb{R}^{n} is defined as 𝒜⊕𝒟:={a+d|a∈𝒜,d∈𝒟}\\mathcal{A}\\oplus\\mathcal{D}\\vcentcolon=\\{a+d|a\\in\\mathcal{A},d\\in\\mathcal{D}\\}.\nFor a vector-valued function f:ℝn→ℝqf:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{q}, we denote the Jacobian by ∂ϕ∂x|x∈ℝq×n\\frac{\\partial\\phi}{\\partial x}|_{x}\\in\\mathbb{R}^{q\\times n}. For the ithi^{\\text{th}} component (f)i(f)_{i}, we denote its Hessian by\n∂2(f)i∂x2|x∈ℝn×n\\frac{\\partial^{2}(f)_{i}}{\\partial x^{2}}|_{x}\\;\\in\\;\\mathbb{R}^{n\\times n}.\n\n1. Formal guarantees are provided, i.e., robust constraint satisfaction, recursive feasibility (Thm. 1), and input-to-state stability (Thm. 2). Recursive feasibility is ensured by a novel treatment of the mismatch with respect to the nonlinear nominal prediction.\n\n2. An efficient sequential convex programming algorithm tailored to the robust MPC formulation is provided to enable real-time deployment. Each iteration consists of solving a nominal trajectory optimization with a quadratic program, updating a disturbance-feedback controller via Riccati recursions, and evaluating Jacobians of the nonlinear dynamics. The design is general and the provided code can be directly applied to systems with large state and input dimensions and long prediction horizons.\n\n3. Real-time feasibility (computation times) is demonstrated across different dynamics, including a quadcopter and a rocket landing. Robust performance is validated on the rocket-landing problem with steerable thrust including actuator dynamics, illustrated in Fig. 1, demonstrating robust constraint satisfaction with an average total latency of 19.7 [ms] per iteration.\nA comparison to a soft-constrained MPC baseline highlights increased safety and stability of the proposed approach.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在存在扰动和模型不匹配的情况下确保机器人控制的安全性和稳定性。  \n2. 如何实现快速、可扩展的非线性模型预测控制，以满足实时实施的需求。  \n\n【用了什么创新的方案】  \n提出了一种鲁棒模型预测控制（MPC）方法，通过将不确定的非线性系统分解为名义非线性动态模型、扰动反馈控制器和模型误差界限，联合优化这些组件以确保鲁棒约束满足、输入到状态的稳定性和递归可行性。采用高效的顺序凸编程算法实现实时部署，验证了在不同动态下的实时可行性，特别是在火箭着陆问题中展示了鲁棒性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "An Extended Kalman Filter for Systems with Infinite-Dimensional Measurements",
            "authors": "Maxwell M. Varley,Timothy L. Molloy,Girish N. Nair",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO)",
            "comment": "8 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18749",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18749",
            "arxiv_html_link": "https://arxiv.org/html/2509.18749v1",
            "abstract": "This article examines state estimation in discrete-time nonlinear stochastic systems with finite-dimensional states and infinite-dimensional measurements, motivated by real-world applications such as vision-based localization and tracking.\nWe develop an extended Kalman filter (EKF) for real-time state estimation, with the measurement noise\nmodeled as an infinite-dimensional random field.\nWhen applied to vision-based state estimation, the measurement Jacobians required to implement the EKF are shown to correspond to image gradients.\nThis result provides a novel system-theoretic justification for the use of image gradients as features for vision-based state estimation, contrasting with their (often heuristic) introduction in many computer-vision pipelines.\nWe demonstrate the practical utility of the EKF on a public real-world dataset involving the localization of an aerial drone using video from a downward-facing monocular camera.\nThe EKF is shown to outperform VINS-MONO, an established visual-inertial odometry algorithm, in some cases achieving mean squared error reductions of up to an order of magnitude.",
            "introduction": "In this paper we focus on state estimation for systems with finite-dimensional states and infinite-dimensional measurements.\nThis focus is motivated by vision-based state estimation, control, and localization problems that arise across robotics [1, 2, 3] and control [4, 5, 6].\nIn such problems, the measurements take the form of images with dimensions determined by the camera’s resolution (i.e., number of pixels), while the underlying state of interest (e.g., position and orientation) is typically relatively low-dimensional.\nWith modern cameras offering increasingly high-resolution images, the emergent challenge in many of these problems is how best to estimate a low-dimensional state with arbitrarily high-dimensional measurements.\nTraditional approaches from computer vision and robotics for processing high-dimensional measurements rely on (spatial) feature extraction [3].\nHowever, such approaches may fail to exploit the dynamics and uncertainty of the state estimates in determining which features to extract or how to weight them in computing a state estimate.\nIn this paper, we therefore take a different approach by formulating an extended Kalman filter (EKF) capable of processing an entire (infinite-dimensional) image domain, and assigning dynamic weights (via gains) to every pixel based on its contribution to state estimates.\n\nAlthough Kalman filters and their nonlinear variants, such as the EKF, have long been used for estimation in robotics and control, the vast majority of vision-based filters are feature-based [7, 8, 6, 1, 3].\nThe use of these filters thus typically involves first reducing images to a sparse set of extracted keypoints before applying standard Kalman filter techniques.\nIn contrast, our novel EKF is capable of operating directly on dense image data (i.e., pixel intensities) directly in real-time, avoiding the need to extract features, and preserving the image structure.\nA key insight that allows the formulation of our filter, is the modeling of the measurement noise as an infinite-dimensional random field. This allows us to construct a continuous image-domain measurement model that naturally integrates with the structure of an infinite-dimensional EKF. In doing so, we are able to derive a system-theoretic justification for using image gradients in the filter update step. These image gradients are usually introduced heuristically in computer-vision pipelines (cf. [3]), but here emerge from the principles of the filter design itself.\n\nA number of different approaches have been employed in early works to derive the Kalman filter for distributed parameter systems [9, 10, 11, 12], although none of these works examined systems with finite-dimensional states and infinite-dimensional measurements as presented here. A survey contextualizing the methods and results of these early derivations is given in [13] and a modern, comprehensive examination of control and estimation of distributed parameter systems is given in [14]. In the case of nonlinear distributed parameter systems, the EKF is generally utilized either by reducing the dimensionality of the underlying system before designing the estimator (the early lumping approach) [15], or designing an infinite-dimensional distributed parameter EKF and using some discretization scheme for real-world implementation (the late lumping approach) [16].\n\nThis article makes the following key contributions, extending the optimal linear filter work in [17, 18].\nFirstly, the optimal linear filter originally derived in that prior work is generalized to construct an EKF for systems with finite-dimensional states and infinite-dimensional measurements, and with both nonlinear state dynamics and nonlinear measurement equations.\nWe provide a derivation of this EKF and establish and interpret the measurement Jacobians that arise within it, with the latter relating to image gradients in the case of image measurements.\nWe verify the efficacy of the EKF for vision-based state estimation on a real-world dataset.\nSpecifically, the filter estimates the position, velocity, acceleration, and yaw of an aerial drone equipped with an Inertial Measurement Unit (IMU) as well as optical cameras providing measurements in the form of grayscale downward-facing images. The estimates are evaluated against the ground truth included in the dataset, and the results are compared with the performance of the well-established monocular visual-inertial odometry algorithm VINS-MONO [2], showing that our filter generally achieves superior or comparable state estimation performance.\n\nThis article is structured as follows. Section II presents the notational definitions used throughout this work. Section III will define the system model that we will analyze, as well as the assumptions used throughout. Section IV will describe a linearization of the previously described system model, in preparation for an application of the optimal linear filter derived in [18] to this linearized system. Section V will present the filter procedure and two results, Proposition V.1 and Proposition V.2, which help to simplify the implementation and reduce the computational complexity of our algorithm. Section VI will give details pertaining to the dataset, as well as how the filter is implemented on the data within, and our chosen measures of filter performance. Using these performance metrics, Section VII demonstrates an empirical verification of the proposed filter and compares the results with those of VINS-MONO.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在有限维状态和无限维测量的系统中进行状态估计。  \n2. 传统特征提取方法在处理高维测量时的局限性。  \n3. 如何实时处理整个图像域以提高状态估计的准确性。  \n\n【用了什么创新的方案】  \n本文提出了一种扩展卡尔曼滤波器（EKF），能够直接在密集图像数据上进行实时状态估计，而无需提取特征。通过将测量噪声建模为无限维随机场，EKF能够动态地为每个像素分配权重，从而更好地利用图像梯度进行状态更新。这种方法提供了对图像梯度作为特征使用的系统理论依据，并在实际应用中表现出优于现有视觉惯性里程计算法的性能。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Dual Iterative Learning Control for Multiple-Input Multiple-Output Dynamics with Validation in Robotic Systems",
            "authors": "Jan-Hendrik Ewering,Alessandro Papa,Simon F.G. Ehlers,Thomas Seel,Michael Meindl",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO)",
            "comment": "11 pages, 4 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.18723",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18723",
            "arxiv_html_link": "https://arxiv.org/html/2509.18723v1",
            "abstract": "Solving motion tasks autonomously and accurately is a core ability for intelligent real-world systems.\nTo achieve genuine autonomy across multiple systems and tasks, key challenges include coping with unknown dynamics and overcoming the need for manual parameter tuning, which is especially crucial in complex  Multiple-Input Multiple-Output (MIMO) systems.",
            "introduction": "Accurate reference tracking is a critical control capability for a wide range of real-world applications, from industrial manufacturing to service robotics and biomedical systems [1, 2, 3], which often involve complex MIMO system dynamics.\nFor these systems to be effective and user-friendly, they must be capable of self-reliantly adapting to new tasks and environments. In other words, it is required to learn to perform reference tracking autonomously.\nThis capability is crucial to eliminate the need for expert-provided model information or time-consuming manual tuning.\n\nIn repetitive settings,  Iterative Learning Control (ILC) is an established method that enables highly accurate reference tracking, given a reference trajectory [4, 5, 6].\nHowever, satisfactory learning performance is almost always dependent on human expert knowledge, such as access to model information or the manual tuning of algorithmic (hyper) parameters.\nThe necessary manual tuning effort is typically even more aggravated in MIMO systems due to cross-coupling effects and scale variations between different inputs and outputs [7].\nMoreover, learning performance is often system- or reference-specific, which hinders genuine autonomous deployment across numerous systems and tasks.\n\nHence, an ILC method must possess the following three characteristics in order to enable real-world systems to autonomously learn to solve reference tracking tasks.\nFirst, the ILC method must neither require prior model information nor the manual tuning of parameters to enable autonomous application.\nSecond, the ILC method has to be applicable to MIMO dynamics, as these are often present in realistic settings.\nAnd third, the ILC method should be validated – ideally on multiple – real-world systems.\nBased on these criteria, we continue to review the state of research in ILC.\n\nFirst, there exists an extensive class of so-called  Model-Based Iterative Learning Control (MB-ILC) methods that can yield remarkable reference tracking performance in various real-world applications.\nFor example,  Norm-Optimal Iterative Learning Control (NO-ILC) schemes have been applied to gantry robots [8] and stroke rehabilitation [9], and NO-ILC can readily be applied to MIMO dynamics [10, 11].\nSimilarly,  Frequency-Domain Iterative Learning Control (FD-ILC) has been successfully applied to real-world systems with MIMO dynamics such as a marine vibrator [12] or nano-positioning systems [13].\nDespite these achievements, the aforementioned and other MB-ILC methods are limited because they require prior model information and typically involve manual tuning of learning parameters, which hinders their autonomous application.\n\nTo overcome the need for prior model information, so-called  Data-Driven Iterative Learning Control (DD-ILC) methods have been developed.\nA common approach is to use the input/output trajectory pairs from previous trials to estimate the gradient of the tracking errors with respect to the input trajectory to update the latter [14, 15, 16].\nOn the other hand, several DD-ILC schemes use experimental data to estimate a model of the plant dynamics and combine this plant approximation with well-known MB-ILC methods [11, 17].\nFor instance, some approaches use  Recursive Least Squares (RLS) to estimate a model of the system dynamics and combine it with NO-ILC or adaptive ILC [18, 19, 20].\nOther approaches combine  Iterative Learning Identification (ILI) and ILC [21, 22] to iteratively learn a model and input trajectory without prior model information.\nFD-ILC has been combined with an iterative learning approach for MIMO dynamics using a pseudo-inversion approach [23].\nNotably, there are approaches that iteratively learn a dynamic linearization of the plant dynamics, which can be utilized in a NO-ILC update law [24, 25, 26, 27, 28].\nWhat is common among all of these methods is that they overcome the need for prior model information, and many of them are applicable to MIMO dynamics.\nHowever, most of these methods have not been validated in real-world experiments, and all of these methods require the manual tuning of learning parameters, which precludes the autonomous application of the learning methods.\n\nTo overcome the need for prior model information and manual parameter tuning, MB-ILC has been combined with repeated model learning using Gaussian processes and self-parametrization schemes [29, 30, 31], and some of the methods have been validated on different real-world systems [29].\nHowever, these approaches are limited in terms of their applicability and validation in MIMO dynamics.\nWe, hence, conclude that there is no DD-ILC method that is autonomous in the sense that it neither requires prior model information nor manual parameter tuning, is applicable to MIMO dynamics, and has been validated on multiple—possibly real-world—systems.\n\nTo address these three issues, we propose a novel MIMO DILC framework that builds on previous results [32] and enables autonomous learning of reference tracking tasks in real-world systems with MIMO dynamics.\nSpecifically, the contributions of this paper are threefold:\n\nFirst, a novel DILC scheme for simultaneous model and control learning in MIMO systems, while requiring neither prior model information nor manual parameter tuning. It exploits a novel iterative learning paradigm that generalizes ILC approaches for iterative model learning, thus enabling the learning of system models using established ILC methods. The algorithmic architecture is illustrated in Figure 1.\n\nSecond, a theoretical analysis providing convergence conditions of the proposed algorithm under mild assumptions. We emphasize that iterative model learning in complex MIMO systems poses significant challenges, such as an overparametrized model, for which we present novel analysis to prove convergence.\n\nThird, an extensive empirical validation with two real-world MIMO systems and a six-degree-of-freedom industrial robot simulation. We demonstrate, in contrast to the vast majority of existing works, the truly autonomous learning capabilities of DILC without any model information or human tuning effort. To the best of our knowledge, this is the first time that a DD-ILC method has solved different reference tracking tasks in multiple real-world systems with MIMO dynamics, without requiring prior model information or manual parameter tuning. We highlight that DILC solves many reference tracking tasks within 1010-2020 trials and learns even complex motions in less than 100100 iterations.\n\nThis paper is structured as follows.\nWe formally define the considered problem in Section II and introduce preliminaries on ILC in Section III.\nThe proposed method and its theoretical properties are detailed in Section IV.\nThe simulative and experimental results are presented in Section V.\nFinally, we conclude the paper in Section VI.\n\nNotation: We denote the set of real numbers by ℝ\\mathbb{R}, the set of natural numbers by ℕ\\mathbb{N}, the set of all natural numbers greater than or equal to a∈ℕa\\in\\mathbb{N} by ℕ≥a\\mathbb{N}_{\\geq a}, and the set of natural numbers in the interval [a,b]⊂ℕ[a,b]\\subset\\mathbb{N} by ℕ[a,b]\\mathbb{N}_{[a,b]}.\nWe denote vectors (matrices) by lower-case (upper-case) letters in bold, e. g., 𝐯∈ℝN\\mathbf{v}\\in\\mathbb{R}^{N} (𝐀∈ℝN×N\\mathbf{A}\\in\\mathbb{R}^{N\\times N}).\nIf not explicitly stated, all vectors are column vectors, and by writing [𝐯]i[\\mathbf{v}]_{i}, we refer to the ii-th entry of 𝐯\\mathbf{v}.\nBy writing [𝐀]i,j[\\mathbf{A}]_{i,j}, we refer to the ii-th entry of the jj-th column of 𝐀\\mathbf{A}.\nTo vectorize 𝐀\\mathbf{A}, we write vec​(𝐀)\\mathrm{vec}(\\mathbf{A}).\nThe Euclidean norm of a vector 𝐯\\mathbf{v} is denoted by ‖𝐯‖\\left\\lVert\\mathbf{v}\\right\\rVert, and the induced Euclidean norm of a matrix 𝐀\\mathbf{A} is denoted by ‖𝐀‖\\left\\lVert\\mathbf{A}\\right\\rVert.\nThe weighted norm with respect to a positive definite matrix 𝐖≻0\\mathbf{W}\\succ 0 with 𝐖=𝐖⊤\\mathbf{W}=\\mathbf{W}^{\\top} is denoted by ‖𝐯‖𝐖=𝐯⊤​𝐖𝐯\\left\\lVert\\mathbf{v}\\right\\rVert_{\\mathbf{W}}=\\sqrt{\\mathbf{v}^{\\top}\\mathbf{W}\\mathbf{v}}.\nWe denote the identity matrix of size N×NN\\times N by 𝐈N\\mathbf{I}_{N}, and the zero matrix of suitable dimension by 𝟎\\mathbf{0}.\nThe Kronecker product of two matrices 𝐀\\mathbf{A} and 𝐁\\mathbf{B} is 𝐀⊗𝐁\\mathbf{A}\\otimes\\mathbf{B}.\nWe recall that a function α:ℝ≥0→ℝ≥0\\alpha:\\mathbb{R}_{\\geq 0}\\rightarrow\\mathbb{R}_{\\geq 0} is of class 𝒦\\mathscr{K} if it is continuous, strictly increasing, and satisfies α​(0)=0\\alpha(0)=0.\nBy ℒ\\mathscr{L}, we refer to the class of functions θ:ℝ≥0→ℝ≥0\\theta:\\mathbb{R}_{\\geq 0}\\rightarrow\\mathbb{R}_{\\geq 0} that are continuous, non-increasing, and satisfy lims→∞θ​(s)=0\\lim_{s\\rightarrow\\infty}\\theta(s)=0, and by 𝒦​ℒ\\mathscr{K}\\negthinspace\\negthinspace\\mathscr{L} to the class of functions β:ℝ≥0×ℝ≥0→ℝ≥0\\beta:\\mathbb{R}_{\\geq 0}\\times\\mathbb{R}_{\\geq 0}\\rightarrow\\mathbb{R}_{\\geq 0} with β​(⋅,s)∈𝒦\\beta(\\cdot,s)\\in\\mathscr{K} and β​(r,⋅)∈ℒ\\beta(r,\\cdot)\\in\\mathscr{L} for any fixed s∈ℝ≥0s\\in\\mathbb{R}_{\\geq 0} and r∈ℝ≥0r\\in\\mathbb{R}_{\\geq 0}, respectively.\nLast, we denote the space of all block-lower-triangular Toeplitz matrices of dimension NN with sub-matrices, ∀n∈ℕ[1,N]\\forall n\\in\\mathbb{N}_{[1,N]}, 𝐓¯n∈ℝL×M\\bar{\\mathbf{T}}_{n}\\in\\mathbb{R}^{L\\times M}, by 𝒯NL,M\\mathcal{T}^{L,M}_{N}, that is,\n\n1. First, a novel DILC scheme for simultaneous model and control learning in MIMO systems, while requiring neither prior model information nor manual parameter tuning. It exploits a novel iterative learning paradigm that generalizes ILC approaches for iterative model learning, thus enabling the learning of system models using established ILC methods. The algorithmic architecture is illustrated in Figure 1.\n\n2. Second, a theoretical analysis providing convergence conditions of the proposed algorithm under mild assumptions. We emphasize that iterative model learning in complex MIMO systems poses significant challenges, such as an overparametrized model, for which we present novel analysis to prove convergence.\n\n3. Third, an extensive empirical validation with two real-world MIMO systems and a six-degree-of-freedom industrial robot simulation. We demonstrate, in contrast to the vast majority of existing works, the truly autonomous learning capabilities of DILC without any model information or human tuning effort. To the best of our knowledge, this is the first time that a DD-ILC method has solved different reference tracking tasks in multiple real-world systems with MIMO dynamics, without requiring prior model information or manual parameter tuning. We highlight that DILC solves many reference tracking tasks within 1010-2020 trials and learns even complex motions in less than 100100 iterations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂的MIMO系统中实现自主的参考跟踪任务？  \n2. 如何消除对先前模型信息和手动参数调整的需求？  \n3. 如何在多个真实世界系统中验证学习方法的有效性？  \n\n【用了什么创新的方案】  \n提出了一种新的MIMO DILC框架，能够在不需要先前模型信息或手动参数调整的情况下，实现对参考跟踪任务的自主学习。该框架结合了迭代学习控制（ILC）方法，提供了理论分析以证明算法的收敛性，并通过在两个真实世界的MIMO系统和一个六自由度工业机器人仿真中的广泛验证，展示了其真正的自主学习能力。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction",
            "authors": "Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO); Image and Video Processing (eess.IV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18566",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18566",
            "arxiv_html_link": "https://arxiv.org/html/2509.18566v1",
            "abstract": "Reconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond temporal resolution, making them a superior sensing choice for dynamic human reconstruction. Accordingly, we present a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. Specifically, a unified set of 3D Gaussians carries a learnable semantic attribute; only Gaussians classified as human undergo deformation for animation, while scene Gaussians stay static. To combat blur, we propose an event-guided loss that matches simulated brightness changes between consecutive renderings with the event stream, improving local fidelity in fast-moving regions. Our approach removes the need for external human masks and simplifies managing separate Gaussian sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.",
            "introduction": "Human reconstruction from monocular videos is a critical task in computer vision and graphics, with applications spanning virtual reality [1], augmented reality [2], and film production [3]. Recent neural rendering advancements, including Neural Radiance Fields (NeRFs)[4] and 3D Gaussian Splatting (3DGS)[5], enable highly-fidelity, photorealistic 3D reconstruction. Building on this, various 3D human reconstruction methods have emerged. Examples include 3DGS-Avatar [6] and ASH [7], which focus on animatable avatars, and HUGS [8], which reconstructs human and scene simultaneously using separate Gaussian sets.\n\nDespite these promising results, existing methods still face significant challenges.\nFirst, most approaches require an external human mask, necessitating a prior segmentation step that can introduce artifacts.\nSecond, rapid human motion in frame-based camera captures often leads to motion blur, deteriorating image quality.\nWhile some methods attempt to deblur RGB images or integrate event data for reconstruction, their generalizability is limited.\nExFMan [9] is a notable exception that leverages event data for dynamic human reconstruction but lacks static scene modeling.\n\nTo address these challenges, we introduce a unified framework for reconstructing animatable humans and static scenes from a monocular event camera (Fig. 1).\nUnlike HUGS [8], which uses separate Gaussian sets, our method encodes both human and scene in a single set of 3D Gaussians with semantic attributes, refined during training via rendering feedback.\nFurthermore, synthetic events generated from rendered images are aligned with real event streams, providing supervision that alleviates motion blur.\n\nWe evaluate our method on two newly created datasets, ZJU-MoCap-Blur and MMHPSD-Blur, generated by simulating motion blur to test performance under challenging conditions.\nExperiments show that our unified human-scene reconstruction framework surpasses the state-of-the-art HUGS [8], with notable gains on ZJU-MoCap-Blur: +19.5%19.5\\% PSNR, +3.95%3.95\\% SSIM, and –32.5%32.5\\% LPIPS.\nIn summary, our main contributions are:\n\nA novel framework for unified human and scene reconstruction using a single semantically attributed set of 3D Gaussians.\n\nThe integration of event data to mitigate motion blur and enhance the reconstruction quality of fast-moving subjects.\n\nAn extensive evaluation on self-generated motion-blurred datasets that demonstrates state-of-the-art performance in challenging high-speed scenarios.\n\n1. A novel framework for unified human and scene reconstruction using a single semantically attributed set of 3D Gaussians.\n\n2. The integration of event data to mitigate motion blur and enhance the reconstruction quality of fast-moving subjects.\n\n3. An extensive evaluation on self-generated motion-blurred datasets that demonstrates state-of-the-art performance in challenging high-speed scenarios.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何从单目视频中重建动态人类与静态场景，尤其是在快速运动下。  \n2. 现有方法依赖外部人类掩码，增加了处理复杂性与潜在伪影。  \n3. RGB帧在快速运动中容易产生运动模糊，影响重建质量。  \n\n【用了什么创新的方案】  \n提出了一种新颖的事件引导的人类-场景重建框架，通过单一的3D高斯点集联合建模动态人类和静态场景。该方法利用事件数据来减轻运动模糊，并通过事件引导损失提高快速运动区域的重建质量。此外，框架简化了高斯点集的管理，消除了对外部人类掩码的需求。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Policy Gradient with Self-Attention for Model-Free Distributed Nonlinear Multi-Agent Games",
            "authors": "Eduardo Sebastián,Maitrayee Keskar,Eeman Iqbal,Eduardo Montijano,Carlos Sagüés,Nikolay Atanasov",
            "subjects": "Systems and Control (eess.SY); Multiagent Systems (cs.MA); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18371",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18371",
            "arxiv_html_link": "https://arxiv.org/html/2509.18371v1",
            "abstract": "Multi-agent games in dynamic nonlinear settings are challenging due to the time-varying interactions among the agents and the non-stationarity of the (potential) Nash equilibria. In this paper we consider model-free games, where agent transitions and costs are observed without knowledge of the transition and cost functions that generate them. We propose a policy gradient approach to learn distributed policies that follow the communication structure in multi-team games, with multiple agents per team. Our formulation is inspired by the structure of distributed policies in linear quadratic games, which take the form of time-varying linear feedback gains. In the nonlinear case, we model the policies as nonlinear feedback gains, parameterized by self-attention layers to account for the time-varying multi-agent communication topology. We demonstrate that our distributed policy gradient approach achieves strong performance in several settings, including distributed linear and nonlinear regulation, and simulated and real multi-robot pursuit-and-evasion games.",
            "introduction": "Multi-robot problems encompass a variety of expected behaviors [1, 2, 3, 4], including cooperative, conflicting or competitive actions. For instance, in a perimeter-defense setting [5, 6], multiple teams must coordinate to effectively defend a region from potential attackers (Fig. 1). These kinds of problems can be formulated as multi-team dynamic games [7, 8, 9, 10, 11], where each multi-agent team is viewed as a player with specific goals and constraints, and where agents interact with teammates (intra-team interactions) and agents on other teams (inter-team interactions). These settings are typically nonlinear and dynamic, requiring complex interactions that evolve with time as a function of how the agents play the game. These challenging features are specially relevant when we seek distributed policies subject to the communication constraints imposed by the topology of the teams; and in the absence of a mathematical description of the game dynamics and costs, demanding model-free approaches that only rely on transition and cost samples to assess the performance of the teams. Inspired by distributed policies in linear quadratic games, we present a novel policy gradient approach to learn distributed policies for nonlinear dynamic games that are both effective and scalable.\n\nModel-based methods for nonlinear multi-agent dynamic games rely on iterative linearization of the system dynamics and quadratic approximation of the game cost [12, 13, 14]. This allows for fast computation with guarantees of convergence to a saddle configuration [15] but imposes a centralized calculation that limits the applicability in distributed settings, where the agent communication is restricted according to a graph topology. To overcome such limitations, it is possible to restrict the class of nonlinear multi-agent dynamic games to potential games [16, 17, 18], where it is assumed that a potential function exists such that the relative incentives in modifying one agent’s policy is equal to the difference in value of the potential function. Under this constraint, it is possible to derive algorithms that compute open-loop optimal trajectories for the agents under centralized [19] or distributed topological constraints [20, 17]. However, open-loop policies lack robustness and require knowledge on how the multi-agent topology will evolve with time. In contrast, we propose a novel policy parameterization that is distributed by construction and does not require network topology prediction. In all previous cases, a model of the system dynamics and the structure of the cost function is needed to compute the actions. Instead, to address general nonlinear multi-agent dynamic games, we propose a model-free policy gradient approach that relies only on transition and cost samples.\n\nModel-free solutions for games are limited due to the non-stationarity of the Nash equilibria (if one exists) [21, 22]. Traditional approaches either focus on providing theoretical guarantees of convergence or addressing practical settings assuming the existence of such Nash equilibria. An instance of the former is [23], where distributed linear quadratic regulators are learned assuming that the sequence of graphs representing the communication structure of the game is known. From a different perspective, when the linear cost function is known and the strategies of all players are available, the problem can be posed as a multi-team distributed optimization program [24, 25]. In practical settings, existing solutions rely on multi-agent reinforcement learning algorithms [26] that consider independent heterogeneous agents to apply policy gradient methods [27, 28, 29, 30]. In this work, we bring together the benefits of both alternatives by proposing a self-attention-based policy parameterization built from first principles and which enforces distributed execution constraints. The distributed policy is trained using a policy gradient learning method that considers, simultaneously, the policies of all the agents from all different teams, addressing the non-stationarity of the game in practice.\n\nOur main contribution is a policy gradient method for learning distributed policies in model-free nonlinear multi-agent dynamic games (Sec. II). Our approach uses a nonlinear feedback gain formulation of the agent policies, parameterized using self-attention layers (Sec. III). The use of self-attention enables to enforce intra- and inter-team graph constraints, handling time-varying communication and achieving invariance with respect to the total number of agents. Furthermore, a neural network parameterization of the policies motivates the use of a policy gradient method to learn effective and scalable policies in the model-free setting.\nThe method also allows to learn heterogeneous policies per team, such that the teams adjust to specific goals. We demonstrate that our method applies broadly, from linear quadratic settings under topology constraints to multi-agent reinforcement learning in competitive games with simulated and real robots (Sec. IV).",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在动态非线性多智能体游戏中学习分布式策略。  \n2. 如何处理多智能体之间的时间变化通信拓扑。  \n3. 如何在缺乏系统动态和成本函数模型的情况下实现模型无关的学习。  \n\n【用了什么创新的方案】  \n提出了一种基于自注意力机制的策略梯度方法，用于学习动态非线性多智能体游戏中的分布式策略。该方法通过非线性反馈增益的参数化，能够处理时间变化的多智能体通信结构，并且不需要对网络拓扑进行预测。通过使用神经网络参数化，方法能够有效地学习异质策略，适应不同团队的具体目标，并在多种设置中展示了良好的性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata",
            "authors": "Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "Accepted at NeurIPS 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18350",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18350",
            "arxiv_html_link": "https://arxiv.org/html/2509.18350v1",
            "abstract": "Accurate visual localization from aerial views is a fundamental problem with applications in mapping, large-area inspection, and search-and-rescue operations. In many scenarios, these systems require high-precision localization while operating with limited resources (e.g., no internet connection or GNSS/GPS support), making large image databases or heavy 3D models impractical. Surprisingly, little attention has been given to leveraging orthographic geodata as an alternative paradigm, which is lightweight and increasingly available through free releases by governmental authorities (e.g., the European Union). To fill this gap, we propose OrthoLoC, the first large-scale dataset comprising 16,425 UAV images from Germany and the United States with multiple modalities. The dataset addresses domain shifts between UAV imagery and geospatial data. Its paired structure enables fair benchmarking of existing solutions by decoupling image retrieval from feature matching, allowing isolated evaluation of localization and calibration performance. Through comprehensive evaluation, we examine the impact of domain shifts, data resolutions, and covisibility on localization accuracy. Finally, we introduce a refinement technique called AdHoP, which can be integrated with any feature matcher, improving matching by up to 95% and reducing translation error by up to 63%. The dataset and code are available at: https://deepscenario.github.io/OrthoLoC.",
            "introduction": "Visual localization for Unmanned Aerial Vehicles is essential for digital-twin modeling [60, 74], surveillance [29], search-and-rescue [51], and infrastructure inspection [34], yet faces unique challenges not addressed by ground-level localization systems. While ground-level approaches [56, 71, 70] benefit from similar viewpoints between images [59, 49, 57], aerial applications encounter dramatic perspective differences and require scalability over large areas [69, 72].\n\nCurrent UAV localization algorithms rely on retrieving the closest match from a database of posed images [72, 77], which is inaccurate, or on 3D models of the scene [69, 66], which are memory and computationally expensive. In limited resources settings, as it is often the case for connectivity-limited environments, this can result in accuracy degradation. Recent approaches like LoDLoc [78] improve storage efficiency by using  Level-of-Detail (LoD) but still assume unchanged environments, perform poorly in building-sparse areas such as highways, and its initialization depends on positioning sensors.\n\nIn contrast, a compelling solution involves geodata, such as orthographic aerial views (Digital Orthophotos) and elevation maps (Digital Surface Models). These provide a reliable, lightweight source for localizing UAV images, as shown in Figure˜1. Such data is increasingly accessible through free releases from European government geoportals [46, 17], and where public access is limited, can be synthesized using photogrammetric tools [20]. Geodata are scalable and better suited for low-resource settings. For example, covering an area of approximately 0.265 km² would require a 3D model of around 8 GB [69], whereas geodata requires about 30 times less memory. Surprisingly, no existing UAV localization approach seems to fully leverage these data sources. We believe this is mainly due to the absence of aligned cross-domain datasets and the lack of full-pose paired large-scale benchmarks specifically designed for localization using these types of geodata.\n\nTo fill this gap, we capture and release the  Orthographic Aerial Localization and Calibration Dataset (OrthoLoC). It comprises 5 main modalities such as UAV imagery, DOPs, DSMs, 3D point maps, and 3D meshes with a total of 16.4K images captured in 47 regions in 19 cities across 2 countries. Our dataset is the first to offer three key advantages: (1) paired UAV-geodata structure that decouples pose estimation from image retrieval, eliminating confounding error sources in the evaluations; (2) precise 6-DoF poses obtained through multi-view georeferenced photogrammetric reconstruction; and (3) additional reference data sources to increase the domain gaps in the dataset.\n\nWe have evaluated state-of-the-art methods on this novel localization and calibration task in a comprehensive benchmark. Additionally, we introduce a method-agnostic refinement technique called  Adaptive Homography Preconditioning (AdHoP) that further improves localization and calibration accuracy. The technique exploits the uniform structure of DOPs to perform homography-based warping by assuming quasi-planar surfaces common in built environments.\n\nOur evaluation reveals several insights. First, state-of-the-art matching algorithms can generalize to aerial perspectives but struggle with the substantial domain gap between perspective UAV imagery and orthographic reference data. Second, our AdHoP technique significantly reduces the perspective disparity, improving all metrics across the tested methods, particularly achieving up to 95% and 63% enhancements in matching and translation accuracy, respectively. Third, camera calibration in aerial settings presents unique challenges due to fundamental geometric ambiguities that affect parameters estimation. Finally, reference data characteristics including domain shifts, data resolutions, and covisibility. significantly impact localization performance, with higher resolution geodata providing improvement in accuracy.\n\nThe main contributions of this paper are: (1) OrthoLoC, the first UAV dataset providing alignment with geodata across multiple modalities and locations; (2) a unified benchmarking framework for UAV localization and calibration that integrates with state-of-the-art matching algorithms and includes our AdHoP technique for addressing perspective disparities; and (3) benchmarking results for camera localization and calibration and an analysis of performance factors including cross-domain challenges, data resolution effects, and covisibility.",
            "llm_summary": "【关注的是什么问题】  \n1. UAV视觉定位的准确性和资源限制下的挑战  \n2. 现有UAV定位算法对地理数据的利用不足  \n3. 跨域数据集缺乏导致的评估困难  \n\n【用了什么创新的方案】  \n本研究提出了OrthoLoC数据集，包含16,425幅UAV图像及多种地理数据，旨在解决UAV图像与地理数据之间的领域转移问题。通过配对结构，该数据集能够独立评估定位和校准性能。此外，提出的AdHoP技术可与任何特征匹配器集成，显著提高匹配精度和降低平移误差。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought",
            "authors": "Yu Ti Huang",
            "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18200",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18200",
            "arxiv_html_link": "https://arxiv.org/html/2509.18200v1",
            "abstract": "Conversational agents must translate egocentric utterances (e.g., “on my right”) into allocentric orientations (N/E/S/W). This challenge is particularly critical in indoor or complex facilities where GPS signals are weak and detailed maps are unavailable. While chain-of-thought (CoT) prompting has advanced reasoning in language and vision tasks, its application to multimodal spatial orientation remains underexplored. We introduce Conversational Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese conversational navigation projected from real-world environments, addressing egocentric-to-allocentric reasoning in non-English and ASR-transcribed scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: (1) extracting spatial relations, (2) mapping coordinates to absolute directions, and (3) inferring user orientation. A curriculum learning strategy progressively builds these capabilities on Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of resource-constrained settings. Experiments show that MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. Moreover, MCoT demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching. The model also maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity. These findings highlight the potential of structured MCoT spatial reasoning as a path toward interpretable and resource-efficient embodied navigation. Code and data are available at https://github.com/yu-ti-huang/Conversational-Orientation-Reasoning.",
            "introduction": "Humans naturally describe spatial environments in egocentric (agent-centric) terms (e.g., “The exit is on my right”), whereas navigation systems typically operate on allocentric (world-centric) orientations such as north, south, east, and west. Conversational navigation has emerged as a promising paradigm that enables users to specify goals through dialogue, offering a natural and human-centered means of guidance in unfamiliar environments Sundar et al. (2024); Sheshadri & Hara (2024); Kaniwa et al. (2024); Liu et al. (2024); Levi & Kadar (2025). However, the crucial problem of grounding egocentric language into allocentric orientation remains underexplored. Current approaches typically assume access to GPS, detailed maps, or fixed global frames de Vries et al. (2018); Chen et al. (2020), and have concentrated primarily on English-based scenarios. Recent progress has also relied heavily on large-scale models Ghosh et al. (2024); Tang et al. (2023), which show strong reasoning abilities but demand substantial computational resources, hindering deployment in resource-constrained settings such as mobile navigation and edge devices.\n\nResearch in embodied AI and MCoT has advanced vision-language navigation and action planning Mu et al. (2023); Sun et al. (2024); Liu et al. (2025); Shen et al. (2025); Pareek et al. (2024), but orientation reasoning from natural language has been largely overlooked. These approaches typically assume that the agent’s orientation is already known or operate on high-level action spaces rather than inferring fundamental spatial relationships de Vries et al. (2018); Chen et al. (2020). Meanwhile, large audio-language models (LALMs) Zhang et al. (2023); Xie & Wu (2024); Fu et al. (2025); Défossez et al. (2024) have advanced speech understanding and dialogue Tang et al. (2024); Gong et al. (2023); Ghosh et al. (2024); Kong et al. (2024), yet their reasoning abilities remain limited to perception-level tasks such as transcription or summarization yu Huang et al. (2024); Yang et al. (2024); Wang et al. (2025); Shi et al. (2025). While recent efforts like Audio-CoT Ma et al. (2025) show promise for enhanced speech-based reasoning, the challenge of transforming egocentric spatial descriptions into allocentric orientation inference remains unaddressed.\n\nTo address this gap, we introduce Conversational Orientation Reasoning (COR), a new benchmark for egocentric-to-allocentric orientation reasoning in Traditional Chinese conversational navigation. COR is derived from real-world urban transportation environments in Taiwan, projected into structured grid representations. Unlike prior studies that rely on vision or raw audio, COR combines ASR-transcribed egocentric language with structured landmark coordinates, evaluating both clean text and ASR transcripts to simulate realistic recognition errors. COR addresses the lack of non-English benchmarks in multimodal spatial reasoning, particularly under noisy ASR conditions.\n\nOur study is guided by three research questions:\n\nRQ1 (Effectiveness): How effective is multimodal CoT prompting for orientation reasoning compared to unimodal and unstructured baselines?\n\nRQ2 (Component analysis): What are the contributions of ASR preprocessing, multimodal fusion, and structured CoT steps?\n\nRQ3 (Robustness and generalization): How robust is the approach to linguistic variation, and how well does it generalize across different spatial domains?\n\nOur contributions are as follows:\n\nTask and benchmark. We introduce the COR benchmark for egocentric-to-allocentric orientation reasoning, combining ASR-transcribed speech with landmark coordinates.\n\nFramework. We develop a multimodal CoT framework with a structured three-step reasoning process that integrates noisy transcripts with spatial signals for orientation inference.\n\nEvaluation. We provide extensive experiments in Traditional Chinese, demonstrating effectiveness, component contributions, and robustness validation across linguistic variation, cross-domain generalization, and referential ambiguity beyond English-centric research.\n\n1. RQ1 (Effectiveness): How effective is multimodal CoT prompting for orientation reasoning compared to unimodal and unstructured baselines?\n\n2. RQ2 (Component analysis): What are the contributions of ASR preprocessing, multimodal fusion, and structured CoT steps?\n\n3. RQ3 (Robustness and generalization): How robust is the approach to linguistic variation, and how well does it generalize across different spatial domains?",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将自我中心的语言描述转换为世界中心的方向推理？  \n2. 在缺乏GPS和详细地图的情况下，如何实现室内导航的有效性？  \n3. 如何在多模态环境中处理ASR识别错误和语言变化？  \n\n【用了什么创新的方案】  \n我们提出了对话方向推理（COR），这是一个新的基准，旨在解决自我中心到世界中心的方向推理问题。我们开发了一个多模态链式思维（MCoT）框架，通过结构化的三步推理过程，将ASR转录的语音与地标坐标相结合，提取空间关系、映射坐标到绝对方向，并推断用户的方向。该框架在资源受限的环境中表现出色，能够有效应对噪声条件下的对话和跨领域评估。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation",
            "authors": "Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin",
            "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18198",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18198",
            "arxiv_html_link": "https://arxiv.org/html/2509.18198v1",
            "abstract": "Autonomous systems have advanced significantly, but challenges persist in accident-prone environments where robust decision-making is crucial. A single vehicle’s limited sensor range and obstructed views increase the likelihood of accidents. Multi-vehicle connected systems and multi-modal approaches, leveraging RGB images and LiDAR point clouds, have emerged as promising solutions. However, existing methods often assume the availability of all data modalities and connected vehicles during both training and testing, which is impractical due to potential sensor failures or missing connected vehicles. To address these challenges, we introduce a novel framework MMCD (Multi-Modal Collaborative Decision-making) for connected autonomy. Our framework fuses multi-modal observations from ego and collaborative vehicles to enhance decision-making under challenging conditions. To ensure robust performance when certain data modalities are unavailable during testing, we propose an approach based on cross-modal knowledge distillation with a teacher-student model structure. The teacher model is trained with multiple data modalities, while the student model is designed to operate effectively with reduced modalities. In experiments on connected autonomous driving with ground vehicles and aerial-ground vehicles collaboration, our method improves driving safety by up to 20.7%{\\it 20.7}\\%, surpassing the best-existing baseline in detecting potential accidents and making safe driving decisions. More information can be found on our website https://ruiiu.github.io/mmcd.",
            "introduction": "Autonomous technology has rapidly evolved over the past few decades, with advancements in perception [1, 2, 3, 4, 5], decision-making [6, 7, 8], and control systems [9, 10]. However, the deployment of autonomous vehicles still face challenges, particularly in accident-prone scenarios. These scenarios demand high robustness and reliability, as any failure in decision-making could have severe consequences. A single vehicle navigating these scenarios is prone to have accidents due to occlusions and limited sensor range. One promising solution to mitigate these risks is to have multi-vehicle connected systems [11, 12, 13]. By sharing information, vehicles can expand their field of view and reduce the chances of accidents. Another promising direction is the use of multi-modal data [14, 15, 16, 17], such as RGB images and LiDAR point clouds, to enhance the perception and decision-making capabilities of autonomous systems. Recent works have combined these two paradigms to develop multi-vehicle, multi-modal systems [18, 19, 20, 21], leveraging both connectivity and diverse sensor data to further improve autonomous driving performance.\n\nHowever, existing works often assume that the ego vehicle has consistent access to all sensors and connected vehicles during both training and testing. For example, methods utilizing both RGB and LiDAR data for training [14, 15, 22] assume the availability of both modalities during testing. This is not always realistic; for instance, LiDAR sensors may malfunction or become unavailable during testing, leaving only RGB data accessible, or some connected vehicles may not be able to share data, as shown in Fig. 1. Additionally, cost efficiency is a crucial consideration, as LiDAR sensors are more expensive than RGB cameras. Reducing the reliance on LiDAR sensors while still achieving high performance with RGB-only models during testing presents a more cost-effective solution.\n\nTo address these challenges, we introduce a novel multi-modal collaborative decision-making framework for connected autonomy, enabling the ego vehicle to make informed decisions by leveraging shared multi-modal data from collaborative vehicles. To handle scenarios where certain data modalities are missing during testing, we propose an approach based on knowledge distillation (KD) with a teacher-student model structure. Our multi-modal framework serves as the teacher model, trained with multiple data modalities (e.g., RGB and LiDAR), while the student model operates with reduced modalities (e.g., RGB). The knowledge distillation process ensures the student model maintains robust performance even with missing modalities during test time.\n\nIn summary, the main contributions of this paper are:\n\nWe introduce MMCD, a novel multi-modal collaborative decision-making framework for connected autonomy. Our approach fuses single or multi-modal observations provided by ego or connected vehicles in a principled way to make decisions for the ego vehicle in accident-prone scenarios. Our method improves the driving safety by up to 20.7%\\bf 20.7\\% in experiments on connected autonomous driving with ground vehicles and aerial-ground vehicles collaboration, outperforming the best-existing baseline.\n\nWe propose a cross-modal knowledge distillation-based approach for MMCD. Our model is trained with multi-modal cues (e.g., LiDAR and RGB) from connected vehicles but executes using single-modality observations (e.g., RGB). This design ensures robust performance in the presence of missing modalities during testing.\n\n1. We introduce MMCD, a novel multi-modal collaborative decision-making framework for connected autonomy. Our approach fuses single or multi-modal observations provided by ego or connected vehicles in a principled way to make decisions for the ego vehicle in accident-prone scenarios. Our method improves the driving safety by up to 20.7%\\bf 20.7\\% in experiments on connected autonomous driving with ground vehicles and aerial-ground vehicles collaboration, outperforming the best-existing baseline.\n\n2. We propose a cross-modal knowledge distillation-based approach for MMCD. Our model is trained with multi-modal cues (e.g., LiDAR and RGB) from connected vehicles but executes using single-modality observations (e.g., RGB). This design ensures robust performance in the presence of missing modalities during testing.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在事故多发环境中实现鲁棒的决策制定？  \n2. 如何处理测试时缺失数据模态的情况？  \n\n【用了什么创新的方案】  \n提出了MMCD（Multi-Modal Collaborative Decision-making）框架，通过融合来自自我和协作车辆的多模态观察，增强决策能力。采用基于知识蒸馏的教师-学生模型结构，教师模型使用多模态数据进行训练，而学生模型则在测试时能够有效地使用减少的模态，从而确保在某些数据模态缺失时仍能保持鲁棒性能。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        }
    ],
    "2025-09-25": [
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation",
            "authors": "Shaofeng Yin,Yanjie Ze,Hong-Xing Yu,C. Karen Liu,Jiajun Wu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "Website:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.20322",
            "code": "https://visualmimic.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20322",
            "arxiv_html_link": "https://arxiv.org/html/2509.20322v1",
            "abstract": "Humanoid loco-manipulation in unstructured environments demands tight integration of egocentric perception and whole-body control. However, existing approaches either depend on external motion capture systems or fail to generalize across diverse tasks. We introduce VisualMimic, a visual sim-to-real framework that unifies egocentric vision with hierarchical whole-body control for humanoid robots. VisualMimic combines a task-agnostic low-level keypoint tracker—trained from human motion data via a teacher-student scheme—with a task-specific high-level policy that generates keypoint commands from visual and proprioceptive input. To ensure stable training, we inject noise into the low-level policy and clip high-level actions using human motion statistics. VisualMimic enables zero-shot transfer of visuomotor policies trained in simulation to real humanoid robots, accomplishing a wide range of loco-manipulation tasks such as box lifting, pushing, football dribbling, and kicking. Beyond controlled laboratory settings, our policies also generalize robustly to outdoor environments.\nVideos are available at: visualmimic.github.io",
            "introduction": "How do humans manage to push a box that is too heavy to move with only their arms? We start with vision perception to localize the box and rely on visual feedback to guide our interaction with the box. To generate sufficient force, we might bend down and push with our hands, lean in with the strength of our arms and shoulders, or even nudge the box forward with our feet. In such cases, every part of the body can be brought into play to accomplish the task. These strategies underscore two fundamental aspects of human loco-manipulation: egocentric visual perception and whole-body dexterity.\n\nEquipping humanoid robots with such human-like object interaction abilities has been a long-standing challenge. Current approaches can be categorized into three main paradigms based on tasks: First, locomotion-focused methods [1, 2] that excel at terrain traversal but do not address object interaction. Second, approaches that rely on external motion capture systems [3, 4] for object state estimation, restricting their deployment to controlled laboratory environments. Third, vision-based methods for object interaction, which follow two distinct paths: 1) imitation learning approaches [5, 6, 7] that train visuomotor policies via human demonstrations, which are constrained by the scarcity of large-scale demonstration data and result in limited generalization capabilities; and 2) sim-to-real reinforcement learning (RL) methods [8, 9] that offer greater robustness and generalizability; however, vision-based RL is currently limited to simple environmental interactions such as sitting [8] and stair climbing [8, 9], falling significantly short of human-level object interaction abilities, due to the large exploration and action space of humanoid robots.\n\nWe aim to take one step forward on the pathway of sim-to-real RL for visual humanoid-object interaction. To make sim-to-real RL generalize better, we adopt a hierarchical design comprising low-level and high-level policies. In such a hierarchical framework, the task-agnostic low-level policy takes care of balanced control and tracks the command sent by the high-level policy, and the task-specific high-level policy generates simplified tracking commands conditioning on egocentric vision input. This design enables more effective task-specific training. We formulate the command interface as body keypoints (root, hands, feet, head) to ensure both compactness and expressiveness.\n\nTo obtain a low-level keypoint tracker that performs human-like behaviors while tracking commands, we curate human motion data and supervise the tracker via motion imitation rewards. However, because keypoint commands alone do not capture the entirety of human motion, we observe that the keypoint tracker can track target keypoints while not perfectly producing human-like behaviors. To address this problem, we adopt a teacher–student training scheme: 1) We first train a motion tracker with full access to current and future whole-body motions, thereby capable of precisely following human reference motions; 2) We then distill this motion tracker into a keypoint tracker that operates on simplified keypoint commands. By doing so, our keypoint tracker captures human motion behaviors while still maintaining a compact command space. Notably, our keypoint tracker is task-agnostic and shared across tasks once trained.\n\nBuilt upon this general keypoint tracker, we train a high-level keypoint generator via sim-to-real RL. Directly training polices via visual RL significantly slows down the training and leads to non-optimal solutions. Therefore, we also apply a teacher–student scheme: 1) We first train a state-based policy with privileged access to object states, enabling them to solve tasks effectively; 2) We then distill the state-based policy into the visuomotor policy that rely solely on egocentric vision and robot proprioception, making it ready for real-world deployment without external object state estimation. To address the large visual sim-to-real gap (Fig. 8), we apply heavy masking to depth images in simulation, approximating real-world sensor noise.\n\nDue to the exploration nature of RL, we find that the high-level policy training is not stable when the high-level policies explore the action space that is beyond the human motion space (HMS) present in training motion datasets. We adopt two strategies to alleviate this problem: 1) injecting noise during training the low-level policy to help it adapt to potentially noisy commands from the high-level policy, and 2) clipping actions from the high-level policy to keep them within the feasible HMS.\n\nThe resulting framework, VisualMimic, enables us to obtain robust and generalizable visuomotor policies that can zero-shot transfer to the real robot, across a broad range of humanoid loco-manipulation tasks, with relatively simple task-specific reward design and without requiring paired human-object motion data. For real-world experiments (Fig. 4 and Fig. 3), we show that our humanoid robot can 1) lift a 0.5-kilogram box to a height of 1 meter, 2) push a very large box (similar height as the robot and weight 3.8 kilograms) straight and steady with its whole body, 3) dribble a football with the fluency of an experienced player, and 4) kick a box forward with alternating feet. Notably, we also show that our visuomotor policies achieve stable performance in outdoor scenarios, showing strong robustness to real-world variability such as lighting changes and uneven ground.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现人形机器人在非结构化环境中的物体交互与运动控制。  \n2. 现有方法依赖于外部运动捕捉系统，限制了其在多样化任务中的泛化能力。  \n3. 如何通过视觉反馈和全身控制来提升人形机器人的物体操控能力。  \n\n【用了什么创新方法】  \nVisualMimic提出了一种视觉sim-to-real框架，结合了任务无关的低级关键点跟踪器和任务特定的高级策略。低级策略通过教师-学生方案从人类运动数据中训练，确保稳定训练并适应高层策略的指令。通过注入噪声和剪辑高层动作，VisualMimic实现了在多种人形运动操控任务中的零-shot转移，表现出强大的鲁棒性和泛化能力，能够在真实环境中稳定执行如箱子搬运、足球运球等任务。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies",
            "authors": "Remo Steiner,Alexander Millane,David Tingdahl,Clemens Volk,Vikram Ramasamy,Xinjie Yao,Peter Du,Soha Pouya,Shiwei Sheng",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted to CoRL 2025 Workshop RemembeRL",
            "pdf_link": "https://arxiv.org/pdf/2509.20297",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20297",
            "arxiv_html_link": "https://arxiv.org/html/2509.20297v1",
            "abstract": "End-to-end learning of robot control policies, structured as neural networks, has emerged as a promising approach to robotic manipulation. To complete many common tasks, relevant objects are required to pass in and out of a robot’s field of view.\nIn these settings, spatial memory - the ability to remember the spatial composition of the scene - is an important competency.\nHowever, building such mechanisms into robot learning systems remains an open research problem.\nWe introduce mindmap (Spatial Memory in Deep Feature Maps for 3D Action Policies), a 3D diffusion policy that generates robot trajectories based on a semantic 3D reconstruction of the environment.\nWe show in simulation experiments that our approach is effective at solving tasks where state-of-the-art approaches without memory mechanisms struggle.\nWe release our reconstruction system111github.com/nvidia-isaac/nvblox, training code222github.com/NVlabs/nvblox_mindmap, and evaluation tasks22footnotemark: 2 to spur research in this direction.",
            "introduction": "Designing generalist robot manipulation policies remains a holy grail of robotics.\nSuch policies would perform manipulation tasks with a high level of competence and be instructed to do so in natural language.\nRecent advances in deep learning, vision, and natural language processing have, for the first time, brought this goal within reach; however, significant challenges remain.\n\nExisting approaches to developing learned manipulation policies generally aim to learn a mapping from sensor observations to robot control signals [2, 3, 4, 5].\nThese models typically employ transformer-based architectures to process image and proprioceptive inputs to generate control signals.\nSuch methods have shown an impressive ability to complete language-guided manipulation tasks.\nOne limitation of several leading approaches, however, is that the generation of output signals is conditioned on current visual observations only.\nSuch approaches lack spatial memory - the ability to remember the spatial and semantic composition of the scene (see [6] for a taxonomy of robot memory).\nThis leads to surprising limitations to their capabilities.\nAlthough some methods incorporate temporal information by maintaining a temporal window of past images, these approaches have drawbacks of their own (see Section 2).\n\nIn this work, we introduce mindmap, an approach that combines a diffusion policy with a metric-semantic 3D reconstruction of the scene.\nmindmap generates trajectories of 3D end-effector poses in the reconstructed space.\nThis approach allows the policy to generate actions that depend on parts of the scene that are outside of the camera’s current  Field of View (FOV).\nOur experiments show that, on tasks requiring spatial memory, mindmap is effective in completing tasks on which several current approaches struggle.\n\nContributions: \nIn this paper, we contribute tools for extending 3D manipulation policies with spatial memory.\nIn particular, we release metric-semantic mapping333nvidia-isaac.github.io/nvblox/pages/torch_examples_deep_features in nvblox [7], our GPU-accelerated reconstruction library11footnotemark: 1, in addition to our training code22footnotemark: 2, and simulation environments22footnotemark: 2 for testing spatial memory.\nWe demonstrate the efficacy of these tools by extending a state-of-the-art 3D diffusion policy [1].\nWe show that by making changes to the architecture and training, the policy’s performance, on challenging tasks that require spatial memory, is significantly improved.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在机器人控制策略中整合空间记忆以提高任务完成能力。  \n2. 现有方法在处理需要空间记忆的任务时表现不佳。  \n3. 如何利用3D语义重建来生成有效的机器人轨迹。  \n\n【用了什么创新方法】  \n本研究提出了mindmap，一个结合扩散策略与度量-语义3D重建的框架。该方法生成的3D末端执行器姿态轨迹能够依赖于当前视野外的场景部分，从而有效解决了需要空间记忆的任务。实验结果表明，mindmap在这些任务上的表现显著优于现有方法，展示了其在机器人操控中的潜力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video",
            "authors": "Georgios Tziafas,Jiayun Zhang,Hamidreza Kasaei",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20286",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20286",
            "arxiv_html_link": "https://arxiv.org/html/2509.20286v1",
            "abstract": "Learning visuomotor policies from expert demonstrations is an important frontier in modern robotics research, however, most popular methods require copious efforts for collecting teleoperation data and struggle to generalize out-of-distribution. Scaling data collection has been explored through leveraging human videos, as well as demonstration augmentation techniques. The latter approach typically requires expensive simulation rollouts and trains policies with synthetic image data, therefore introducing a sim-to-real gap. In parallel, alternative state representations such as keypoints have shown great promise for category-level generalization. In this work, we bring these avenues together in a unified framework: PAD (Parse-Augment-Distill), for learning generalizable bimanual policies from a single human video. Our method relies on three steps: (a) parsing a human video demo into a robot-executable keypoint-action trajectory, (b) employing bimanual task-and-motion-planning to augment the demonstration at scale without simulators, and (c) distilling the augmented trajectories into a keypoint-conditioned policy. Empirically, we showcase that PAD outperforms state-of-the-art bimanual demonstration augmentation works relying on image policies with simulation rollouts, both in terms of success rate and sample/cost efficiency.\nWe deploy our framework in six diverse real-world bimanual tasks such as pouring drinks, cleaning trash and opening containers, producing one-shot policies that generalize in unseen spatial arrangements, object instances and background distractors.\nSupplementary material can be found in the project webpage https://gtziafas.github.io/PAD_project/.",
            "introduction": "Visuomotor policy learning for robot manipulation has seen great success in recent years [1, 2, 3, 4, 5], yet it typically demands costly and time-consuming data collection from expert demonstrators.\nThis data-hungriness stems from the different required axes of generalization: a competent policy must generalize in unseen object arrangements (spatial) and object instances (object), as well as be robust to environmental conditions such as scene background, camera placement etc. (background).\nAs a result, most common policies struggle to generalize in out-of-distribution scenarios where corresponding data has not been collected.\nA recent methodology to tackle this data scarcity is to tap into the vast repository of videos available in the web, showcasing humans interacting with objects in diverse scenarios [6, 7, 8, 9, 10].\nHere the main challenge is bridging the embodiment gap between humans and robot morphologies [11, 12, 13].\nAlternatively, a recent line of works aims at dealing with spatial generalization by augmenting a small number of source demos with structured, object-centric task-and-motion planning (TAMP) procedures [14, 15, 16, 17].\nHowever, most works train image policies that require calibrated digital twins and expensive on-robot rollouts to generate the augmentations, therefore introducing a visual sim-to-real gap, while still struggling with object and background generalization.\nWhen it comes to bimanual manipulation, additional considerations related to arm collaboration strategies for different task scenarios further complicate data collection / generation.\n\nIn this work we wish to tackle these challenges by proposing PAD (Parse-Augment-Distill), a unified framework for learning bimanual visuomotor policies from a single human video demonstration.\nOur framework works in three steps (see Fig. 1): (a) parsing the video into robot-executable data, (b) augmenting the data in a simulation-free fashion and, (c) distilling the augmented data into a closed-loop policy.\n\nIn our work, we explicitly seek spatial, object and background generalization.\nTo accommodate this, we utilize 3D keypoint coordinates as state representations for our trained policy, which offers three important advantages:\nFirst, keypoints abstract the visual scene into a low-dimensional geometric representation, which is task-specific and decoupled from object semantics, and therefore has empirically shown to aid in sample-efficiency and robustness to background noise [18, 13, 19].\nSecond, keypoints facilitate category-level object generalization, inherited by the open-world capabilities of pretrained vision models for identifying semantic correspondences [20, 21, 22].\nFinally, 3D point states enable efficient spatial augmentations, as keypoint coordinates can be computed on-the-fly through 3D rigid geometry assumptions [23].\nThis alleviates the need for a digital twin and expensive simulation rollouts, which would be required by a typical image policy to obtain image observations [14, 15, 16, 17].\nIn turn, this significantly improves data collection time and bridges the sim-to-real gap introduced by simulators.\n\nConcretely, in PAD we introduce a general TAMP framework for spatial demo augmentations, specialized for bimanual manipulation.\nTo that end, we introduce bimanual task templates, symbolic representations that declare information about each arm’s object assignments, involved contacts and requirements for arm synchronization, while abstracting away the specific semantics of the task.\nWe particularly focus on handling issues related to bimanual manipulation, such as out-of-range arm-object assignments and re-synchronization between the arms during motion planning, which are missing from previous works in bimanual demo augmentation [17].\nOur augmentation framework is general, cost-efficient and embodiment-agnostic, as it uses human video as the source demo that can be mapped to any given morphology.\nFinally, we use prescribed 3D keypoints as our state representation instead of RGB images or point-clouds, and accompany them with augmentations that aid the policy in object generalization.\nTo distill the augmented data, we introduce Kp-RDT, an adapted version of RDT [4] for learning bimanual diffusion policies with keypoint conditioning.\n\nEmpirically, we show that our framework outperforms state-of-the-art bimanual demo augmentation methods [17] in four simulation tasks from the DexMimicGen robosuite benchmark [24], both in terms of success rates, as well as sample-efficiency and data generation time.\nWe further apply our framework in six diverse real-world tasks and show that PAD obtains policies that generalize to unseen spatial arrangements, object instances and background scene noise, while doing so from a single human demonstration.\n\nIn summary, our contributions with this work are threefold:\n\nWe introduce PAD, a unified framework for generalizable bimanual policy learning from a human video.\n\nWe propose a general bimanual TAMP framework for spatial demo augmentations, applicable to a wide variety of manipulation skills and arm-coordination strategies, as well as open-ended object categories.\n\nWe perform extensive robot experiments in 10 tasks, 4 in simulation and 6 with hardware, demonstrating significant gains compared to previous works in terms of success rates and sample/cost efficiency, as well as strong generalization in real-world tasks.\n\n1. We introduce PAD, a unified framework for generalizable bimanual policy learning from a human video.\n\n2. We propose a general bimanual TAMP framework for spatial demo augmentations, applicable to a wide variety of manipulation skills and arm-coordination strategies, as well as open-ended object categories.\n\n3. We perform extensive robot experiments in 10 tasks, 4 in simulation and 6 with hardware, demonstrating significant gains compared to previous works in terms of success rates and sample/cost efficiency, as well as strong generalization in real-world tasks.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何从单个视频中学习可泛化的双手视觉运动策略。  \n2. 如何解决数据收集的高成本和时间消耗问题。  \n3. 如何实现空间、对象和背景的泛化能力。  \n\n【用了什么创新方法】  \n本研究提出了PAD（Parse-Augment-Distill）框架，通过三个步骤实现从单个视频学习双手策略：解析视频为机器人可执行的关键点-动作轨迹，利用无模拟器的任务与运动规划进行演示数据增强，最后将增强的数据蒸馏为关键点条件政策。实验结果表明，PAD在成功率和样本/成本效率上优于现有的双手演示增强方法，能够在六个真实世界任务中生成可泛化的策略，适应未见的空间排列和背景干扰。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "HL-IK: A Lightweight Implementation of Human-Like Inverse Kinematics in Humanoid Arms",
            "authors": "Bingjie Chen,Zihan Wang,Zhe Han,Guoping Pan,Yi Cheng,Houde Liu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20263",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20263",
            "arxiv_html_link": "https://arxiv.org/html/2509.20263v1",
            "abstract": "Traditional IK methods for redundant humanoid manipulators emphasize end-effector (EE) tracking, frequently producing configurations that are valid mechanically but not human-like. We present Human-Like Inverse Kinematics (HL-IK), a lightweight IK framework that preserves EE tracking while shaping whole-arm configurations to appear human-like—without full-body sensing at runtime. The key idea is a learned elbow prior: using large-scale human motion data retargeted to the robot, we train a FiLM-modulated spatio-temporal attention network (FiSTA) to predict the next-step elbow pose from the EE target and a short history of EE–elbow states. This prediction is incorporated as a small residual alongside EE and smoothness terms in a standard Levenberg–Marquardt optimizer, making HL-IK a drop-in addition to numerical IK stacks. Over 183k simulation steps, HL-IK reduces arm-similarity position and direction error by 30.6% and 35.4% on average, and by 42.2% and 47.4% on the most challenging trajectories. Hardware teleoperation on a robot distinct from simulation further confirms the gains in anthropomorphism. HL-IK is simple to integrate, adaptable across platforms via our pipeline, and adds minimal computation, enabling human-like motions for humanoid robots. Project page: https://hl-ik.github.io/",
            "introduction": "A robotic arm can be defined as a series of links connected together by joints [1]. Inverse kinematics (IK) is a fundamental problem in such robotics, traditionally formulated to compute joint configurations that achieve a specified end-effector (EE) pose. For industrial manipulators, this formulation is often sufficient, since the primary objective is to place the tool center point at the desired location with high precision. Classical IK solvers—whether based on closed-form derivations [2, 3, 4, 5], numerical iterations [6, 7], or optimization frameworks [8, 9, 10] — focus almost exclusively on EE tracking.\n\nFor redundant robotic arms, the inverse solution to a given EE pose is often not unique, with infinitely many possible configurations [11, 12]. When only the EE pose is constrained, the intermediate joints remain underdetermined [13], which can lead to solutions that are mechanically valid but visually unnatural and non-human-like. In scenarios such as humanoid robot teleoperation [14, 15, 16], beyond accurate EE tracking, we also aim for the robot’s overall arm configuration to closely resemble that of the human arm, thereby achieving a higher level of anthropomorphism. Existing methods [17, 18] often rely on external cameras to capture human body keypoints and align them with robot joints to improve configuration similarity. Yet, such approaches not only require additional perception inputs but also typically do not treat EE tracking as the primary constraint, and thus cannot be regarded as strict IK solutions. Therefore, our goal is to develop a system that, given only the desired EE pose as input (as in traditional IK), not only ensures precise EE tracking but also achieves close similarity between the human and robot arm configurations.\n\nTo realize this goal, we first model the human arm as a four-point, three-segment kinematic chain comprising the shoulder, elbow, wrist, and fingertips (the human EE) [19]. For a fixed EE pose, the dominant redundancy manifests as the elbow “swivel” about the shoulder–wrist axis. Aligning the elbow pose effectively sets the arm plane and the forearm pointing direction, thereby resolving the main ambiguity and yielding anthropomorphic configurations without sacrificing EE accuracy. In other words, once the elbow is aligned, the overall arm configuration becomes perceptually natural and significantly more similar to that of a human. Furthermore, given a desired EE pose, determining a prior elbow pose that best reflects the natural human form becomes a central aspect of our approach. In summary, the primary contributions of this paper are:\n\n1) Human-like data acquisition framework: We propose an automatic EE–elbow data collection scheme based on large-scale human motion trajectory datasets, which can be readily adapted to different robots.\n\n2) Elbow prediction network: We design a FiLM-modulated Spatio-Temporal Attention Network (FiSTA) that uses only a partial history of EE and elbow frames to predict the desired human-like elbow pose for a given EE target.\n\n3) Comprehensive validation: The effectiveness of our approach is validated both in simulation and on real hardware, as well as across different robotic configurations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在保证末端执行器（EE）跟踪精度的同时，实现类人臂的运动配置？  \n2. 现有的逆向运动学（IK）方法如何导致机械有效但视觉上不自然的解决方案？  \n3. 如何在没有全身传感器的情况下，实现类人运动的机器人臂？  \n\n【用了什么创新方法】  \n本研究提出了一种轻量级的类人逆向运动学（HL-IK）框架，通过学习的肘部先验来改善机械臂的运动配置。使用大规模人类运动数据训练的FiLM调制时空注意力网络（FiSTA）预测肘部姿态，并将其作为小残差与EE和光滑性项结合，应用于标准的Levenberg-Marquardt优化器中。实验结果显示，HL-IK在183k次仿真步骤中，平均减少了30.6%和35.4%的臂部相似性位置和方向误差，并在最具挑战性的轨迹上分别减少了42.2%和47.4%。此外，硬件遥操作验证了其在类人运动方面的提升，表明HL-IK易于集成并适应不同平台。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving",
            "authors": "Jinhao Chai,Anqing Jiang,Hao Jiang,Shiyi Mu,Zichong Gu,Shugong Xu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "IWACIII 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.20253",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20253",
            "arxiv_html_link": "https://arxiv.org/html/2509.20253v1",
            "abstract": "End-to-end multi-modal planning has become a transformative paradigm in autonomous driving, effectively addressing behavioral multi-modality and the generalization challenge in long-tail scenarios. We propose AnchDrive, a framework for end-to-end driving that effectively bootstraps a diffusion policy to mitigate the high computational cost of traditional generative models. Rather than denoising from pure noise, AnchDrive initializes its planner with a rich set of hybrid trajectory anchors. These anchors are derived from two complementary sources: a static vocabulary of general driving priors and a set of dynamic, context-aware trajectories. The dynamic trajectories are decoded in real-time by a Transformer that processes dense and sparse perceptual features. The diffusion model then learns to refine these anchors by predicting a distribution of trajectory offsets, enabling fine-grained refinement. This anchor-based bootstrapping design allows for efficient generation of diverse, high-quality trajectories. Experiments on the NAVSIM benchmark confirm that AnchDrive sets a new state-of-the-art and shows strong generalizability.",
            "introduction": "End-to-end autonomous driving algorithms have gained substantial attention in recent years owing to their superior scalability and adaptability over traditional rule-based motion planning approaches. By learning control signals directly from raw sensor data—such as camera images or LiDAR point clouds—these methods bypass the complexity of modular design pipelines, mitigate the accumulation of perception errors, and enhance overall system consistency and robustness. Earlier end-to-end planners, including UniAD[1], VAD[2], and Transfuser[3], relied on ego queries to regress single-modal trajectories, while more recent approaches such as SparseDrive[4] explored sparse perception modules in combination with parallel motion planners. Nevertheless, in complex traffic conditions—such as intersections or high-speed lane changes—potential vehicle behaviors can be highly ambiguous and diverse. Ignoring the inherent uncertainty in driving behavior and the multi-modal decision-making requirements imposed by environmental perception often leads to overconfident or outright failed predictions when relying on a single predicted trajectory.\n\nRecent research has therefore begun to incorporate multi-modal modeling strategies, producing multiple trajectory proposals consistent with current scene constraints to improve decision coverage. Methods such as VADv2[5] and Hydra-MDP[6] achieve this by using predefined discrete trajectory sets. While this increases coverage to some extent, the reliance on fixed trajectory sets inherently discretizes what is fundamentally a continuous control process, thus constraining expressiveness and flexibility.\n\nDiffusion models have emerged as a promising alternative, offering generative and adaptive capabilities well suited for multi-modal trajectory planning. They enable direct sampling from the high-dimensional joint distribution of the ego vehicle and surrounding agents’ trajectories, and have demonstrated strong modeling capacity in high-dimensional continuous control spaces—evidenced by successes in domains such as image synthesis and robotic motion planning. Their ability to naturally model conditional distributions makes it straightforward to integrate key contextual inputs, including trajectory history, map semantics, and ego objectives, thereby improving both consistency and contextual relevance in policy generation. Moreover, their controllable test-time sampling allows for incorporating additional constraints without retraining, unlike many Transformer-based architectures.\n\nDespite improvements such as DDIM[7] for accelerating sampling, conventional diffusion models require numerous iterative denoising steps, resulting in high computational and latency costs at inference. To address this, prior work has shown that initializing the generation process from non-standard noise distributions can shorten the sampling path by leveraging prior information. Building on this idea, DiffusionDrive[8] proposed a truncated diffusion strategy that anchors the process to a fixed set of trajectory anchors, enabling sampling to begin from intermediate states and thus reducing the number of required iterations. However, such fixed anchor sets lack the flexibility to adapt to scenarios demanding dynamically generated anchors.\n\nWe address this limitation with AnchDrive, a novel end-to-end multi-modal autonomous driving framework. AnchDrive employs a multi-head trajectory decoder to dynamically generate a set of dynamic trajectory anchors informed by scene perception, capturing behavioral diversity under local environmental conditions. Simultaneously, we construct a broad-coverage static anchor set from large-scale human driving data, providing cross-domain behavioral priors. These dynamic anchors provide context-aware guidance tailored to the immediate scene, while the static anchor set mitigates overfitting to training distributions and improves generalization to unseen environments. By leveraging this hybrid anchor set, our diffusion-based planner can produce high-quality and diverse predictions within a reduced number of denoising steps.\n\nWe evaluate AnchDrive in closed-loop settings on the Navsim-v2[9] simulation platform, which features reactive background traffic agents and high-fidelity synthetic multi-view imagery. Experiments on a navtest set show that AnchDrive achieves 85.5 EPDMS, indicating robust and contextually appropriate behavior generation in complex driving scenarios.\n\nOur key contributions are as follows:\n\nWe propose AnchDrive, an end-to-end autonomous driving framework that employs a truncated diffusion process initialized from a hybrid set of trajectory anchors. This approach, which integrates both dynamic and static anchors, significantly improves initial trajectory quality and enables robust planning. We validate its effectiveness on the challenging Navsim-v2[9] benchmark.\n\nWe propose AnchDrive, an end-to-end autonomous driving framework that employs a truncated diffusion process initialized from a hybrid set of trajectory anchors. This approach, which integrates both dynamic and static anchors, significantly improves initial trajectory quality and enables robust planning. We validate its effectiveness on the challenging Navsim-v2[9] benchmark.\n\nWe design a hybrid perception model with dense and sparse branches. The dense branch builds a bird’s-eye-view (BEV) representation for the planner’s primary input, while the sparse branch extracts instance-level cues—such as detected obstacles, lane boundaries, centerlines, and stop lines—to enhance the planner’s understanding of obstacles and road geometry.\n\n1. We propose AnchDrive, an end-to-end autonomous driving framework that employs a truncated diffusion process initialized from a hybrid set of trajectory anchors. This approach, which integrates both dynamic and static anchors, significantly improves initial trajectory quality and enables robust planning. We validate its effectiveness on the challenging Navsim-v2[9] benchmark.\n\n2. We design a hybrid perception model with dense and sparse branches. The dense branch builds a bird’s-eye-view (BEV) representation for the planner’s primary input, while the sparse branch extracts instance-level cues—such as detected obstacles, lane boundaries, centerlines, and stop lines—to enhance the planner’s understanding of obstacles and road geometry.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂交通条件下生成多模态的驾驶轨迹以应对行为的不确定性。  \n2. 如何降低传统生成模型在推理时的计算成本和延迟。  \n3. 如何结合动态和静态轨迹锚点以提高轨迹生成的质量和多样性。  \n\n【用了什么创新方法】  \nAnchDrive框架通过引入混合轨迹锚点，采用截断扩散过程来初始化生成模型。动态轨迹锚点由Transformer实时解码，结合静态锚点提供广泛的行为先验。该方法显著提高了初始轨迹的质量，并在Navsim-v2基准测试中表现出强大的泛化能力，达到了85.5 EPDMS，展示了在复杂驾驶场景中的稳健和上下文适应行为生成能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Techno-Economic analysis for Smart Hangar inspection operations through Sensing and Localisation at scale",
            "authors": "Angelos Plastropoulos,Nicolas P. Avdelidis,Argyrios Zolotas",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20229",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20229",
            "arxiv_html_link": "https://arxiv.org/html/2509.20229v1",
            "abstract": "The accuracy, resilience, and affordability of localisation are fundamental to autonomous robotic inspection within aircraft maintenance and overhaul (MRO) hangars. Hangars typically feature tall ceilings and are often made of materials such as metal. Due to its nature, it is considered a GPS-denied environment, with extensive multipath effects and stringent operational constraints that collectively create a uniquely challenging environment. This persistent gap highlights the need for domain-specific comparative studies, including rigorous cost, accuracy, and integration assessments, to inform a reliable and scalable deployment of a localisation system in the Smart Hangar. This paper presents the first techno-economic roadmap that benchmarks motion capture (MoCap), ultra-wideband (UWB), and a ceiling-mounted camera network across three operational scenarios: robot localisation, asset tracking, and surface defect detection within a 40 × 50 m hangar bay. A dual-layer optimisation for camera selection and positioning framework is introduced, which couples market-based camera-lens selection with an optimisation solver, producing camera layouts that minimise hardware while meeting accuracy targets. The roadmap equips MRO planners with an actionable method to balance accuracy, coverage, and budget, demonstrating that an optimised vision architecture has the potential to unlock robust and cost-effective sensing for next-generation Smart Hangars.",
            "introduction": "Accurate localisation is fundamental to enabling autonomous robotic inspection in MRO hangars, where metallic structures, extensive multipath effects, and strict operational constraints define a unique and challenging environment [1]. Existing localisation technologies, including infrared MoCap, UWB real-time location systems, and camera-based or Simultaneous Localisation and Mapping (SLAM) approaches, offer different trade-offs in terms of achievable accuracy, infrastructure complexity, cost, and robustness to occlusion and interference [2]. Sensor fusion frameworks that combine vision, inertial, and UWB data can improve robustness and deliver cost–accuracy trade-offs, as demonstrated in large-scale warehouse deployments. However, their performance and economic viability remain highly dependent on environment-specific factors [3]. In addition, this study also treats localisation as the backbone for real-time asset monitoring, allowing ground support equipment, tooling, and spares to be tracked seamlessly in the bay. On the opposite scale, artefact localisation is addressed, with an exploration of how sensing can facilitate the identification of surface defects and other critical features on the airframe. The framing of these macroscopic and microscopic needs together sets the stage for the optimisation framework, comparative experiments, and cost analyses developed in the remainder of the paper.\n\nDespite technical advances, a notable lack of comprehensive, real-world techno-economic analyses remains, specifically focused on aircraft hangar deployments. The available literature provides only partial benchmarking or component-level comparisons for individual or hybrid localisation modalities [4, 5], with little empirical evidence on their robustness to the full spectrum of hangar-specific challenges such as dynamic occlusion, specular reflections, and integration with existing maintenance workflows. To date, no studies have presented a holistic side-by-side evaluation of MoCap, UWB, and vision-based solutions in an operational metallic aircraft hangar context. This persistent gap underscores an urgent need for domain-specific comparative studies, including rigorous assessments of cost, accuracy, and integration, to inform the reliable and scalable deployment of robotic inspection systems in aviation environments. The economic impact of daily maintenance practices has begun to be quantified by recent hangar-focused studies; for instance, significant rework costs in narrow-body bays can be avoided by improving technicians’ awareness of composite repair, as demonstrated by Jong et al. [6]. Across the timeline, Moenck et al. [7] outline how the forthcoming Industry 5.0 automation could reshape the trade-offs of labour hours and logistics on large MRO campuses. At the same time, the classic aerodynamic analysis of the enclosed engine test hangars by Wallis and Ruglen still provides a valuable historical baseline for energy throughput economics [8].\n\nIn summary, this study offers five significant contributions: (i) it presents the inaugural techno-economic roadmap that evaluates MoCap, UWB, and ceiling-camera vision in parallel for full-scale aircraft hangars; (ii) it introduces a dual-layer optimisation framework that combines market-driven camera-lens selection with a Mixed-Integer Linear Programming-based set-cover placement, resulting in the minimal number of cameras needed; (iii) it supplies quantified design-to-cost case studies converting three typical MRO tasks into specific bills of materials and cost estimates; (iv) determines the optimal balance for defect-detection accuracy, illustrating how ceiling cameras and drone close-ups converge at various defect sizes; and (v) provides the first cost/accuracy comparison between camera localisation and commercial UWB/MoCap systems for a conventional 40 × 50 m bay. Collectively, these contributions deliver an actionable and comprehensive methodology for MRO decision-makers to select, size, and cost localisation and inspection systems within large hangars.\n\nInspired by the aviation industry’s shift towards Industry 5.0, which sees mobile robots and AI-based decision support systems taking on routine maintenance duties, the hangar should transition from a passive shelter to a dynamic sensing platform. The end goal is a ceiling infrastructure dense enough to localise robots, track assets, and even surface defects in real-time but lean enough to be economically retrofitted into legacy bays. Against this backdrop, the remainder of the paper is organised as follows. Section 2 reviews the state-of-the-art in MoCap, UWB and ceiling-camera vision, clarifying their respective accuracy, cost, and integration trade-offs. Section 3 introduces a dual-layer optimisation framework that first selects a market-ready camera–lens pair and then solves a set-cover problem to minimise hardware while meeting resolution targets. Section 4 translates those algorithms into three design-to-cost frameworks: robot localisation, asset tracking, and defect detection, each with a detailed bill of materials for practitioner guidance. It also presents MoCap and UWB implementation options for benchmarking.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在GPS-denied环境中实现准确、可靠的机器人定位和检测？  \n2. 如何在航空维护和检修（MRO）环境中平衡成本、准确性和集成性？  \n3. 当前的定位技术在复杂环境中的经济可行性和性能如何？  \n\n【用了什么创新方法】  \n本文提出了首个针对飞机机库的技术经济路线图，比较了运动捕捉（MoCap）、超宽带（UWB）和天花板摄像头网络在机器人定位、资产跟踪和表面缺陷检测中的应用。引入了一个双层优化框架，结合市场驱动的摄像头镜头选择与混合整数线性规划的设置覆盖问题，最小化所需硬件数量，同时满足准确性目标。研究表明，优化的视觉架构能够实现强大且具有成本效益的传感能力，为下一代智能机库的部署提供了可行的方法。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "A Biomimetic Vertebraic Soft Robotic Tail for High-Speed, High-Force Dynamic Maneuvering",
            "authors": "Sicong Liu,Jianhui Liu,Fang Chen,Wenjian Yang,Juan Yi,Yu Zheng,Zheng Wang,Wanchao Chi,Chaoyang Song",
            "subjects": "Robotics (cs.RO)",
            "comment": ". Submitted Under Review",
            "pdf_link": "https://arxiv.org/pdf/2509.20219",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20219",
            "arxiv_html_link": "https://arxiv.org/html/2509.20219v1",
            "abstract": "Robotic tails can enhance the stability and maneuverability of mobile robots, but current designs face a trade-off between the power of rigid systems and the safety of soft ones. Rigid tails generate large inertial effects but pose risks in unstructured environments, while soft tails lack sufficient speed and force. We present a Biomimetic Vertebraic Soft Robotic (BVSR) tail that resolves this challenge through a compliant pneumatic body reinforced by a passively jointed vertebral column inspired by musculoskeletal structures. This hybrid design decouples load-bearing and actuation, enabling high-pressure actuation (up to 6 bar) for superior dynamics while preserving compliance. A dedicated kinematic and dynamic model incorporating vertebral constraints is developed and validated experimentally. The BVSR tail achieves angular velocities above 670°/s and generates inertial forces and torques up to 5.58 N and 1.21 Nm, indicating over 200% improvement compared to non-vertebraic designs. Demonstrations on rapid cart stabilization, obstacle negotiation, high-speed steering, and quadruped integration confirm its versatility and practical utility for agile robotic platforms.",
            "introduction": "The tail is a masterful evolutionary solution for dynamic locomotion, enabling animals to achieve remarkable feats of stability and agility [1]. The functional utility of this appendage is rooted in the principles of classical mechanics, particularly the conservation of angular momentum. This is often accomplished through a process known as inertial adjustment, where rapid, controlled movements of the tail generate reaction forces and torques that are imparted onto the main body to regulate its orientation and momentum in real time. For instance, cheetahs, during their high-speed bounding gait, use their long, muscular tails as aerodynamic rudders and inertial counterweights to modulate yaw and roll, allowing for exceptionally sharp turns and stable braking [2, 3]. In the aerial realm, animals like geckos and lizards execute rapid mid-air self-righting maneuvers by swinging their tails, inducing a counter-rotation in their bodies to ensure a safe landing orientation [4, 5]. Even during terrestrial or arboreal locomotion, animals from kangaroos to squirrels leverage their tails for a spectrum of dynamic tasks, from providing a “fifth leg” for postural stability to recovering from unexpected falls and slips [6, 7]. These biological archetypes, which demonstrate a sophisticated functional integration of sensing, neural control, and musculoskeletal actuation, provide a rich foundation for designing robotic tails that augment the dynamic performance of mobile robots [8, 9].\n\nInspired by these natural mechanisms, roboticists have long sought to replicate their function to enhance the agility and robustness of mobile systems. Early pioneering work included the Uniroo, a monopedal hopping robot that employed a simple tail for pitching stabilization [10]. Subsequent research in this domain has primarily focused on rigid, articulated tails, which typically manifest as pendulum-like mechanisms. These designs range from single-degree-of-freedom (DOF) systems for planar regulation in the pitch, yaw, or roll axes [2, 11, 12, 13, 14, 3, 15, 16, 17], to 2-DOF mechanisms that provide more generalized spatial control [18, 19]. The high stiffness and well-defined kinematics of these rigid systems allow them to generate significant and predictable inertial effects at high speeds. However, their fundamental design presents a critical set of trade-offs. The primary limitation is their lack of compliance, which restricts their utility in unstructured environments and poses significant safety risks for any physical human-robot interaction [20]. Furthermore, the high reflected inertia of a rigid tail necessitates large, powerful actuators, which adds considerable mass and increases power consumption. This demand for high-bandwidth torque control to manage large inertial loads and mitigate potentially destabilizing impact forces adds significant complexity to the control system.\n\nTo address the inherent safety and compliance limitations of rigid systems, the field of soft robotics presents a compelling alternative paradigm. The intrinsic compliance of soft robots, derived from their deformable materials and structures, offers inherent safety, adaptability to uncertain environments, and robustness to physical impacts [21, 22, 23, 24, 25]. This has motivated the development of a new class of soft robotic tails, including hyper-redundant continuum structures for inertial adjustment [26], aquatic robots with flexible tails that emulate fish locomotion for propulsion and maneuvering [27], and even novel wearable tails for human balance assistance [28]. However, while these pioneering systems validate the potential of soft appendages, they also reveal a persistent performance gap. This deficit is rooted in the fundamental properties of the soft materials themselves, which typically exhibit low stiffness and significant viscoelasticity, leading to challenges in generating sufficient force at high frequencies. The low actuation authority of most soft systems, combined with the immense difficulty of accurately modeling and controlling their near-infinite degrees of freedom, limits their ability to produce the rapid, high-magnitude accelerations necessary for effective dynamic regulation of a large robotic platform. Consequently, the field is faced with a critical unmet need: a robotic appendage that unites the raw inertial authority of rigid systems with the inherent safety and adaptability of soft structures. The absence of such a system currently precludes the deployment of agile, dynamic robots in unstructured, human-centric environments.\n\nThis paper introduces a Biomimetic Vertebraic Soft Robotic (BVSR) tail, as shown in Fig. 1, specifically designed to address the trade-offs above by establishing a novel hybrid design approach. Our method involves the functional integration of a compliant pneumatic body with an internal, passively jointed vertebral column. This central element performs a crucial dual role: it acts as a structural backbone that bears the tensile loads from high-pressure actuation, allowing the system to generate large forces without material failure, and it serves as a kinematic constraint that reduces the complex, high-dimensional deformation of the soft body into a predictable, low-dimensional bending motion. This architectural choice makes the system’s modeling and control more tractable while enabling a level of dynamic performance previously unattainable in soft robotic appendages. The primary contributions of this work are threefold:\n\nThe formulation and physical realization of a vertebraic soft robotic design principle, wherein a passive kinematic constraint enables high-pressure (6 Bar) actuation in an otherwise compliant structure to achieve superior angular velocity and inertial output.\n\nA comprehensive Euler-Lagrange dynamic model that, among the first in this class of robots, explicitly incorporates the kinematic constraints imposed by an internal vertebral structure, demonstrating high fidelity between theoretical predictions and experimental results.\n\nRigorous experimental validation of the tail’s performance envelope and a demonstration of its functional efficacy in dynamic tasks, including inertial assistance for a wheeled mobile robot and successful integration with a quadrupedal platform, validating its versatility.\n\nThe remainder of this paper is organized as follows. Section 2 describes the concept and modeling of the BVSR tail. Section 3 presents its physical implementation and characterization. Section 4 details the experimental validation and functional demonstrations. Section 5 discusses the implications of our findings, limitations, and comparisons to the state of the art. Finally, Section 6 concludes the paper and discusses future work.\n\n1. The formulation and physical realization of a vertebraic soft robotic design principle, wherein a passive kinematic constraint enables high-pressure (6 Bar) actuation in an otherwise compliant structure to achieve superior angular velocity and inertial output.\n\n2. A comprehensive Euler-Lagrange dynamic model that, among the first in this class of robots, explicitly incorporates the kinematic constraints imposed by an internal vertebral structure, demonstrating high fidelity between theoretical predictions and experimental results.\n\n3. Rigorous experimental validation of the tail’s performance envelope and a demonstration of its functional efficacy in dynamic tasks, including inertial assistance for a wheeled mobile robot and successful integration with a quadrupedal platform, validating its versatility.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何设计一种既能提供高速度和高力量又具备安全性的机器人尾部？  \n2. 现有的刚性和柔性尾部设计之间的性能差距如何弥补？  \n3. 如何有效建模和控制具有高维自由度的软体机器人？  \n\n【用了什么创新方法】  \n本研究提出了一种生物仿生的脊椎软体机器人尾部（BVSR），结合了顺应性气动体和被动关节脊柱，解决了刚性和柔性系统之间的权衡。该设计通过高压驱动（最高6 Bar）实现了超过670°/s的角速度和5.58 N的惯性力，显示出200%以上的性能提升。通过建立和验证包含脊柱约束的动态模型，实验结果表明该尾部在动态任务中的有效性，成功集成于四足机器人，展示了其多功能性和实用性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving",
            "authors": "Pengxiang Li,Yinan Zheng,Yue Wang,Huimin Wang,Hang Zhao,Jingjing Liu,Xianyuan Zhan,Kun Zhan,Xianpeng Lang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20109",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20109",
            "arxiv_html_link": "https://arxiv.org/html/2509.20109v1",
            "abstract": "End-to-End (E2E) solutions have emerged as a mainstream approach for autonomous driving systems, with Vision-Language-Action (VLA) models representing a new paradigm that leverages pre-trained multimodal knowledge from Vision-Language Models (VLMs) to interpret and interact with complex real-world environments. However, these methods remain constrained by the limitations of imitation learning, which struggles to inherently encode physical rules during training. Existing approaches often rely on complex rule-based post-refinement, employ reinforcement learning that remains largely limited to simulation, or utilize diffusion guidance that requires computationally expensive gradient calculations. To address these challenges, we introduce ReflectDrive, a novel learning-based framework that integrates a reflection mechanism for safe trajectory generation via discrete diffusion. We first discretize the two-dimensional driving space to construct an action codebook, enabling the use of pre-trained Diffusion Language Models for planning tasks through fine-tuning. Central to our approach is a safety-aware reflection mechanism that performs iterative self-correction without gradient computation. Our method begins with goal-conditioned trajectory generation to model multi-modal driving behaviors. Based on this, we apply local search methods to identify unsafe tokens and determine feasible solutions, which then serve as safe anchors for inpainting-based regeneration. Evaluated on the NAVSIM benchmark, ReflectDrive demonstrates significant advantages in safety-critical trajectory generation, offering a scalable and reliable solution for autonomous driving systems.",
            "introduction": "Autonomous driving (AD) is guiding the transportation industry toward a safer and more efficient future (Tampuu et al., 2020). Within this trend, End-to-End (E2E) systems (Hu et al., 2023; Chen et al., 2023) have emerged as the mainstream alternative to traditional modular designs (Bansal et al., 2018), which are prone to error accumulation between interdependent modules. They have also largely replaced rule-based methods (Fan et al., 2018; Treiber et al., 2000) that demand extensive human engineering effort. Meanwhile, Vision-Language-Action (VLA) models (Kim et al., 2024; Hwang et al., 2024) offer a new solution by incorporating pre-trained knowledge from Vision-Language Models (VLMs) (Hurst et al., 2024; Bai et al., 2025). Equipped with enhanced generalization capabilities, VLA models can interpret visual scenes and understand human instructions to directly output planning trajectories, thereby improving adaptability in challenging situations.\n\nHowever, eixsting learning-based methods does not resolve the core challenge in imitation learning-based driving systems. Specifically, behavior cloning fails to inherently encode inviolable physical rules, such as collision avoidance or adherence to drivable areas (Lu et al., 2023). As a result, a generated trajectory may be highly probable under the model’s distribution yet still violate critical safety constraints. Consequently, existing deployed solutions often rely on significant human priors, such as trajectory anchors (Li et al., 2024) or rule-based generated paths (Dauner et al., 2023). These priors offer a reliable initial solution for the learning system, but they also necessitate substantial post-processing, particularly in complex scenarios. Concurrently, more advanced solutions are emerging. Some methods integrate reinforcement learning (Kaelbling et al., 1996; Kendall et al., 2019; Jaeger et al., 2025; Cusumano-Towner et al., 2025) with human-designed reward functions to enhance causal reasoning. However, most existing studies remain confined to the simulation level. From a deployment perspective, these approaches typically require unsafe online rollouts and suffer from training instability, especially in large-scale models (Zheng et al., 2024). Although guidance mechanisms in diffusion models provide a promising alternative by enabling controllable generation during inference (Zheng et al., 2025; Jiang et al., 2023; Zhong et al., 2023), they often experience slow sampling speeds due to gradient computations and are highly sensitive to parameter tuning, which can lead to numerical instability.\n\nTo address these challenges, we pioneer the use of discrete diffusion (Austin et al., 2021) for planning to meet the demand for verifiable and controllable E2E driving systems. A key advantage of this approach is its operation in a discrete action space, which facilitates the seamless incorporation of critical safety constraints through search, masking, and sampling techniques during trajectory generation. This results in a hybrid framework in which learned behaviors can be rigorously guided by prior knowledge, shifting away from black-box planning toward trustworthy and interpretable decision-making. Inspired by these insights, we propose ReflectDrive, a novel learning-based framework that integrates a reflection mechanism for safe trajectory generation via discrete diffusion. Specifically, we first discretize the two-dimensional driving space to construct a action codebook, enabling the representation of vehicle trajectories through discrete codebook embeddings. This representation allows us to leverage a pre-trained Diffusion Language Models (DLMs) (You et al., 2025; Nie et al., 2025) for planning tasks via fine-tuning. The approach facilitates parallel decoding and bidirectional feature fusion within a unified architecture that supports scalable training. Based on this fine-tuned model, our reflection mechanism begins with goal-conditioned generation, where the goal point guides the generation process to capture diverse multi-modal driving behaviors. Furthermore, the framework integrates safety metrics to evaluate the generated multi-modal trajectories. For unsafe waypoints, we perform a local search to identify a feasible solution, which then serves as a safe anchor token for trajectory inpainting. The entire process operates without gradient computation, enabling parallel generation and the injection of safety constraints during trajectory regeneration. Evaluations on the real-world autonomous driving benchmark NAVSIM (Dauner et al., 2024) demonstrate the feasibility of employing discrete diffusion for trajectory generation. Equipped with our reflection mechanism, ReflectDrive achieves near human-level closed-loop performance. Our contributions are summarized as follows:\n\nWe pioneer the application of discrete diffusion for E2E autonomous driving trajectory generation and integrate it into a VLA model for scalable training.\n\nWe introduce reflection mechanism, a novel inference-time guidance framework specifically designed for the denoising process in discrete diffusion, integrating external safety validation with efficient discrete token optimization.\n\nWe evaluate our method on real-world driving benchmarks, proving that the framework can enforce hard safety constraints without compromising behavioral coherence.\n\n1. We pioneer the application of discrete diffusion for E2E autonomous driving trajectory generation and integrate it into a VLA model for scalable training.\n\n2. We introduce reflection mechanism, a novel inference-time guidance framework specifically designed for the denoising process in discrete diffusion, integrating external safety validation with efficient discrete token optimization.\n\n3. We evaluate our method on real-world driving benchmarks, proving that the framework can enforce hard safety constraints without compromising behavioral coherence.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在自主驾驶中有效地生成安全的轨迹以满足物理规则？  \n2. 现有的模仿学习方法如何克服对复杂场景的依赖和不稳定性？  \n3. 如何在不依赖梯度计算的情况下实现高效的轨迹生成？  \n\n【用了什么创新方法】  \n本研究提出了ReflectDrive，一个基于离散扩散的学习框架，集成了反射机制以实现安全的轨迹生成。首先，我们将二维驾驶空间离散化，构建动作代码本，并通过微调利用预训练的扩散语言模型进行规划任务。核心是一个安全意识的反射机制，能够在不进行梯度计算的情况下进行迭代自我修正。通过目标条件的轨迹生成，我们能够捕捉多模态驾驶行为，并通过局部搜索识别不安全的轨迹点，确保生成的轨迹符合安全约束。评估结果表明，ReflectDrive在NAVSIM基准上表现出显著的安全性优势，提供了一种可扩展且可靠的自主驾驶解决方案。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Hybrid Safety Verification of Multi-Agent Systems using $ψ$-Weighted CBFs and PAC Guarantees",
            "authors": "Venkat Margapuri,Garik Kazanjian,Naren Kosaraju",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20093",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20093",
            "arxiv_html_link": "https://arxiv.org/html/2509.20093v1",
            "abstract": "This study proposes a hybrid safety verification framework for closed-loop multi-agent systems under bounded stochastic disturbances. The proposed approach augments control barrier functions with a novel ψ\\psi-weighted formulation that encodes directional control alignment between agents into the safety constraints. Deterministic admissibility is combined with empirical validation via Monte Carlo rollouts, and a PAC-style guarantee is derived based on margin-aware safety violations to provide a probabilistic safety certificate. The results from the experiments conducted under different bounded stochastic disturbances validate the feasibility of the proposed approach.",
            "introduction": "Safety within multi-agent systems is essential for real-world applications such as autonomous driving [1, 2] and robotic swarm deployments in agriculture [3, 4], manufacturing [5, 6], and search and rescue operations [7], where agents must navigate safely through their environment. Safety in a stochastic multi-agent dynamical system requires that all agent trajectories remain within a predefined safe set under specified control inputs and time horizons.\nTraditional approaches include reachability-based formulations [8, 9], where a Hamilton-Jacobi partial differential equation is solved to characterize the backward-reachable set that avoids unsafe regions. However, these methods are computationally expensive and scale poorly to high-dimensional or multi-agent systems. More recently, barrier certificates [10, 11] and control barrier functions (CBFs) [12, 13] have emerged as tractable alternatives for certifying safety. By enforcing forward invariance of a safe set via control-affine constraints, CBFs offer real-time safety guarantees under deterministic assumptions. Yet, such guarantees may fail in the presence of noise or unmodeled disturbances. While stochastic CBF variants address this, they often rely on strong distributional assumptions or chance-constrained formulations.\n\nTo bridge the gap, this work introduces a hybrid safety verification framework that unifies ψ\\psi-weighted CBFs for forward invariance with finite-sample probably approximately correct (PAC)-style guarantees for margin-aware safety under bounded stochastic disturbances, where ψ\\psi is a term inspired by quantum walk dynamics [14] to promote pairwise safety among different agents. Rather than assuming complete knowledge of the noise distributions, the proposed method combines deterministic admissibility with empirical validation via Monte Carlo rollouts under bounded stochasticity, yielding a distribution-free safety certificate with high-probability guarantees. The proposed method is feasible in multi-agent applications where uncertainty is prevalent and exact noise modeling is infeasible.",
            "llm_summary": "【关注的是什么问题】  \n1. 多智能体系统在随机干扰下的安全性验证问题。  \n2. 现有方法在高维或多智能体系统中的计算复杂性和可扩展性不足。  \n3. 如何在不完全知识的情况下提供概率安全证书。  \n\n【用了什么创新方法】  \n本研究提出了一种混合安全验证框架，结合了ψ加权控制障碍函数（CBFs）与基于有限样本的PAC风格保证。该方法通过引入量子行走动态启发的ψ项，促进不同智能体间的方向控制一致性。通过Monte Carlo回放进行经验验证，结合确定性可接受性，最终实现了在有界随机干扰下的分布无关安全证书。实验结果表明，该方法在多智能体应用中具有良好的可行性和高概率保证。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "C-3TO: Continuous 3D Trajectory Optimization on Neural Euclidean Signed Distance Fields",
            "authors": "Guillermo Gil(1),Jose Antonio Cobano(1),Luis Merino(1),Fernando Caballero(1) ((1) Service Robotics Laboratory, Universidad Pablo de Olavide, Seville, Spain)",
            "subjects": "Robotics (cs.RO)",
            "comment": "submitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.20084",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20084",
            "arxiv_html_link": "https://arxiv.org/html/2509.20084v1",
            "abstract": "This paper introduces a novel framework for continuous 3D trajectory optimization in cluttered environments, leveraging online neural Euclidean Signed Distance Fields (ESDFs). Unlike prior approaches that rely on discretized ESDF grids with interpolation, our method directly optimizes smooth trajectories represented by fifth-order polynomials over a continuous neural ESDF, ensuring precise gradient information throughout the entire trajectory. The framework integrates a two-stage nonlinear optimization pipeline that balances efficiency, safety and smoothness. Experimental results demonstrate that C-3TO produces collision-aware and dynamically feasible trajectories. Moreover, its flexibility in defining local window sizes and optimization parameters enables straightforward adaptation to diverse user’s needs without compromising performance. By combining continuous trajectory parameterization with a continuously updated neural ESDF, C-3TO establishes a robust and generalizable foundation for safe and efficient local replanning in aerial robotics. The source code is open source and can be found at: https://anonymous.4open.science/r/icra2026_neural_trajectory_planner_C3TO_anon/",
            "introduction": "Aerial robots have become increasingly popular for a wide range of real-world applications due to their ability to perform hazardous tasks more efficiently and, most importantly, more safely than humans [1][2]. Fast trajectory replanning remains a critical area of research, particularly in dynamic and unstructured environments. Equally important is maintaining a continuously updated representation of the drone’s surroundings, which is essential for generating continuous, safe, and smooth 3D local trajectories in real time. This paper presents a framework for planning a continuous local trajectory on an online, neurally-generated, distance field.\n\nChosing an adequate map representation is key. Having an efficient calculation of the free space and the direction to the closest obstacle are some of those desirable features for such representation. Euclidean Signed Distance Fields (ESDFs) have become increasingly popular as a method for representing and modeling robot surroundings for planning purposes [3, 4] and present many very useful properties that are desirable for planning: it is continuous, differentiable everywhere except at the cut-locus, its gradient is Lipschitz-continuous everywhere except at the cut-locus, and the norm of its spatial gradient is one [5]. Representations of ESDFs are typically based on discrete voxel grids or neural networks. Discrete methods for online ESDF representation, such as Voxblox [6], FIESTA [3], and Voxfield [7], have gained popularity, but require interpolation to produce continuous ESDF values. In contrast, neural networks can represent ESDFs in a continuous manner [8]. HIO-SDF [9] introduced an incremental, online, and global ESDF model represented by a Sinusoidal Representation Neural Network (SIREN) [10]. Compared to iSDF [8], HIO-SDF can capture finer details, producing smoother surfaces and incorporating more geometric information throughout the environment.\n\nRegarding path and trajectory planning for drones, the traditional approaches are sampling-based or searching-based planners. They can generate optimal paths, but overlook path safety, which makes them undesirable for real-world operations in occluded spaces without post-processing. There are several state-of-the-art entries showing that ESDFs can be very convenient and useful tools for path planning methods [4, 11, 12, 13]. Heuristic search planners, by integrating ESDFs and leveraging their properties, can inherently address the safety problem and have demonstrated the capability to compute feasible, safe, and fast paths. However, these discretized paths, defined by a sequence of intermediate waypoints, are neither continuous nor smooth, and do not take into account kinematic or dynamic constraints [14].\nTrajectory planning through non-linear optimization is the next step, as it results in trajectories that can comply to complex restrictions, which can be assessed in the form of cost functions and can be easily customized depending on the needs of the user. Although trajectory replanning has been widely investigated, most methods depend on discretized ESDFs with interpolation, limiting gradient accuracy and trajectory quality.\n\nThis work focuses on performing local continuous 3D trajectory planning using non-linear optimization directly on an online generated neural ESDF. We present a framework that starts by building the drone’s environment representation using 3D LIDAR measurements to train a SIREN-like network, based on the network described in [9]. The framework then leverages the properties of the neural representation to perform trajectory optimization on that online ESDF, taking into account distance to obstacles as an indication of safety in addition to other restrictions.\n\nThe main contribution of the proposed framework is to maintain a level of computational optimization sufficient for it to be suitable for use in trajectory replanning, while highlighting its robustness and flexibility compared to existing approaches. The novelty of the framework lies in the optimization of continuous trajectories over continuous ESDFs that are updated online.\n\nWe have organized this paper into six sections. Section II describes the current state of the art. Section III provides an overview of the framework implemented. Section IV provides a detailed description of the trajectory planner on the neural ESDF. The experimental validation can be found in Section V and conclusions are presented in Section VI.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在动态和复杂环境中进行连续的3D轨迹优化？  \n2. 如何利用神经欧几里得有符号距离场（ESDF）提高轨迹规划的安全性和效率？  \n3. 如何实现对障碍物距离的实时更新以优化轨迹？  \n\n【用了什么创新方法】  \n本研究提出了一种新颖的框架C-3TO，通过在线神经ESDF实现连续的3D轨迹优化。该方法使用第五阶多项式表示平滑轨迹，并通过两阶段非线性优化管道，平衡效率、安全性和光滑性。实验结果表明，C-3TO能够生成碰撞感知和动态可行的轨迹，并且其灵活性使其能够适应不同用户需求而不影响性能。通过将连续轨迹参数化与持续更新的神经ESDF相结合，C-3TO为安全高效的局部重规划奠定了稳健且可推广的基础。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots",
            "authors": "Surov Maksim",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20082",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20082",
            "arxiv_html_link": "https://arxiv.org/html/2509.20082v1",
            "abstract": "This paper presents a control methodology for achieving orbital stabilization\nwith simultaneous time synchronization of periodic trajectories in\nunderactuated robotic systems. The proposed approach extends the classical\ntransverse linearization framework to explicitly incorporate time-desynchronization\ndynamics. To stabilize the resulting extended transverse dynamics,\nwe employ a combination of time-varying LQR and sliding-mode control.\nThe theoretical results are validated experimentally through the implementation\nof both centralized and decentralized control strategies on a group\nof six Butterfly robots.",
            "introduction": "The problem of trajectory tracking for underactuated robots has been\naddressed in a series of publications [1, 2, 3, 4, 5, 6, 7].\nMost of these works focus on designing control algorithms for orbital\nstabilization, where the system state converges to a reference periodic\ntrajectory up to a phase shift. Formulating the control objective\nin this way has enabled the development of algorithms that have demonstrated\neffectiveness in real-world applications [8, 9, 10, 11].\nHowever, in some practical scenarios, orbital asymptotic stability\nalone may be insufficient. For example, in cooperative or synchronized\ntasks involving multiple underactuated robots, it may be necessary\nto ensure asymptotic stability of the full state rather than only\nthe orbit itself, particularly when the robots share the same clock.\n\nA straightforward method for tracking a reference trajectory is based\non linearization of the tracking error dynamics, followed by the design\nof an LQR for the resulting linear time-varying (LTV) system. This\nmethod is described in Chapter 12 of [12] and has been\nshown to achieve asymptotic stability for small tracking errors, as\ndemonstrated in experiments with a triple pendulum on a cart [13].\nCompared to orbital stabilization methods, this approach is sensitive\nto initial time shifts, and the control system may lose stability\nif the robot becomes desynchronized.\n\nAlternative approaches for synchronization of closed orbits in underactuated\nrobots involve modifications of orbital tracking algorithms to ensure\nsynchronization between robots in a group [2, 14, 15, 16, 17].\nFor example, in [2], the authors employ transverse\nlinearization of the dynamics of a group of three robots to design\na centralized control law that achieves synchronization. In [17]\norbital stabilization together with synchronization is attained using\nthe dynamic virtual holonomic constraints approach. In [15],\nthe authors propose an ad-hoc modification of the transverse–linearization\napproach, and demonstrate its effectiveness experimentally on the\nsynchronization of two real robots.\n\nOur method for orbital stabilization with simultaneous time synchronization\nalso represents a modification of orbital stabiliation feedback. It\nbuilds on the transverse linearization framework [18, 2, 1].\nFor a given periodic trajectory, we augment the transverse dynamics\nwith the dynamics of robot desynchronization, defined as the difference\nbetween the physical time and the reference time corresponding to\nthe “closest” point on the trajectory. As we show, the linearization\nof this extended transverse dynamics takes the form of an LTV system,\nwhich can be stabilized using a combination of LQR and sliding-mode\ncontrol, similarly to [7]. The resulting feedback\nlaw naturally decomposes into an orbital stabilization component and\na synchronization component. The synchronization term is bounded,\nwith the desynchronization variable entering through the signum function.\nThis structure allows the control law to preserve the benefits of\norbital stabilization while providing bounded corrective actions,\neven for large desynchronizations.\n\nThe remainder of the paper is organized as follows. Section II\nformulates the problem of periodic trajectory tracking for a class\nof nonlinear control systems. Section III\nbriefly reviews the orbital-stabilization algorithm based on the transverse\nlinearization approach. The main results are presented in Section IV,\nwhere the transverse dynamics are extended with a desynchronization\nvariable and two methods for extended dynamics stabilization are proposed.\nThe first method applies an LQR design to linearization of the extended\ntransverse dynamics, while the second employs a sliding-mode control\nmethodology. Section V reports experimental\nresults obtained on a group of six butterfly robots [26].\nConcluding remarks are provided in Section VI.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在欠驱动机器人中实现轨迹的轨道稳定性与时间同步。  \n2. 现有方法在处理多机器人同步任务时的局限性。  \n3. 需要在轨道稳定性之外确保全状态的渐近稳定性。  \n\n【用了什么创新方法】  \n本研究提出了一种控制方法，通过扩展经典的横向线性化框架，结合时间变化的LQR和滑模控制，实现欠驱动机器人周期轨迹的轨道稳定性与时间同步。通过将机器人去同步化动态纳入考虑，形成了一个线性时变系统，从而能够同时处理轨道稳定性和同步性。实验结果表明，该方法在六个蝴蝶机器人上有效验证了理论成果，展示了良好的稳定性和同步性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping",
            "authors": "Jose E. Maese,Luis Merino,Fernando Caballero",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20081",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20081",
            "arxiv_html_link": "https://arxiv.org/html/2509.20081v1",
            "abstract": "This paper presents a high-efficiency, CPU-only volumetric mapping framework based on a Truncated Signed Distance Field (TSDF). The system incrementally fuses raw LiDAR point-cloud data into a voxel grid using a directional bitmask-based integration scheme, producing dense and consistent TSDF representations suitable for real-time 3D reconstruction. A key feature of the approach is that the processing time per point-cloud remains constant, regardless of the voxel grid resolution, enabling high resolution mapping without sacrificing runtime performance. In contrast to most recent TSDF/ESDF methods that rely on GPU acceleration, our method operates entirely on CPU, achieving competitive results in speed. Experiments on real-world open datasets demonstrate that the generated maps attain accuracy on par with contemporary mapping techniques. The source code is publicly available at https://github.com/robotics-upo/DB-TSDF",
            "introduction": "Volumetric mapping is a fundamental capability in mobile robotics, supporting tasks such as collision avoidance, motion planning, and the construction of consistent world models under real-time constraints. Point clouds and occupancy grids remain widely used on CPU-only platforms, as their simple data structures allow efficient processing without specialized hardware. However, they are prone to aliasing at high resolutions and often produce geometric artifacts that hinder downstream processing. Truncated Signed Distance Fields (TSDFs) address these limitations by storing per-voxel distances to the nearest surface and providing smooth proximity information. Despite their advantages, many existing TSDF and ESDF (Euclidean Signed Distance Fields) pipelines rely heavily on GPU acceleration or exhibit computational costs on the CPU that grow unfavorably with map resolution and update rate.\n\nThis work introduces DB-TSDF, a mapping method that integrates TSDFs using a directional bitmask representation specifically designed for fast operation on a discrete voxel grid using only the CPU. Each voxel encodes a compact 32-bit distance mask, a sign flag, and a hit counter. For each LiDAR return, the system selects a precomputed, direction-dependent kernel applied over a fixed neighborhood. A single bitwise AND operation per voxel updates the mask, while a directional shadow mechanism assigns occupied or free-space evidence. Since the kernel size remains constant, the integration time per scan is bounded and remains largely unaffected by the total grid dimensions. Increasing resolution increases memory usage but does not compromise real-time performance. The implementation is fully parallelized using multi-threading and relies exclusively on integer operations.\n\nDB-TSDF builds upon the Truncated Distance Field mapping backend initially developed for the D-LIO framework [1], which was primarily focused on localization. In this work, the mapping formulation is expanded and refined: the field representation is extended from unsigned to signed distances, directional evidence accumulation is introduced, and the memory layout is optimized for improved cache efficiency. These changes result in a high-resolution mapping method capable of maintaining stable runtimes on CPU-constrained platforms and delivering robust performance even in feature-sparse environments.\n\nThe method is evaluated on public LiDAR datasets, with results reported in terms of geometric accuracy, runtime, and the trade-off between resolution, and update latency. The experiments show that DB-TSDF achieves mapping quality comparable to established volumetric approaches while maintaining competitive performance on CPU at resolutions typically requiring GPU acceleration. An example of such high-resolution reconstruction is shown in Figure 1, generated from the Newer College dataset.\n\nThe main contributions of this work are: (i) a TSDF integration scheme based on directional kernels and bitmask distance encoding, implemented entirely on CPU; (ii) a mapping approach with constant per-scan computational cost that is independent of the global grid size; (iii) a signed and memory-efficient voxel structure designed for high-resolution mapping and ROS2 integration; and (iv) an experimental evaluation that quantifies the method’s accuracy, speed, and memory requirements.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在CPU上实现高效的体积映射，避免GPU加速的依赖。  \n2. 如何在高分辨率下保持实时性能，同时减少计算成本和内存使用。  \n3. 如何利用方向性位掩码提高TSDF的集成效率和准确性。  \n\n【用了什么创新方法】  \n本研究提出了一种基于方向位掩码的TSDF集成方案，专门设计用于在离散体素网格上快速操作。每个体素编码了一个紧凑的32位距离掩码、符号标志和命中计数器。通过预计算的方向依赖内核，系统对每个LiDAR返回进行处理，使用位与操作更新掩码，确保每次扫描的集成时间保持恒定。该方法在公共LiDAR数据集上的实验表明，DB-TSDF在几何准确性和运行速度上与现有的体积映射方法相当，同时在CPU平台上实现了高分辨率映射，展示了其在特征稀疏环境中的稳健性能。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning",
            "authors": "Xun Li,Rodrigo Santa Cruz,Mingze Xi,Hu Zhang,Madhawa Perera,Ziwei Wang,Ahalya Ravendran,Brandon J. Matthews,Feng Xu,Matt Adcock,Dadong Wang,Jiajun Liu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20077",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20077",
            "arxiv_html_link": "https://arxiv.org/html/2509.20077v1",
            "abstract": "To enable robots to comprehend high-level human instructions and perform complex tasks, a key challenge lies in achieving comprehensive scene understanding: interpreting and interacting with the 3D environment in a meaningful way. This requires a smart map that fuses accurate geometric structure with rich, human-understandable semantics.\nTo address this, we introduce the 3D Queryable Scene Representation (3D QSR), a novel framework built on multimedia data that unifies three complementary 3D representations: (1) 3D-consistent novel view rendering and segmentation from panoptic reconstruction, (2) precise geometry from 3D point clouds, and (3) structured, scalable organization via 3D scene graphs. Built on an object-centric design, the framework integrates with large vision-language models to enable semantic queryability by linking multimodal object embeddings, and supporting object-level retrieval of geometric, visual, and semantic information. The retrieved data are then loaded into a robotic task planner for downstream execution.",
            "introduction": "For robots to perform complex tasks in 3D environments under human instruction, they must relate high-level semantics in natural language commands to actual content in their surrounding environment. Even a simple instruction such as “Robot, I’m thirsty.” demands the ability to infer intent, locate relevant items (e.g., a water bottle), assess affordances, and plan a path for retrieval. Although trivial for humans, the tasks are exceptionally challenging for robots, as they must simultaneously reason about the spatial structure and semantic meaning of the environment based on human queries. While it is essential to enhance the robot’s intelligence for navigating and manipulating complex environments, it is equally important to make the environment more understandable. We address this dual necessity by introducing the concept of a queryable 3D scene representation (QSR), which embeds intelligence directly into the scene. This enables both robots and humans to interact with their surroundings in a more collaborative, context-aware, and semantically grounded manner.\n\nTraditional 3D maps for robotic systems, such as voxel-based occupancy grids, point clouds, and mesh models (Fredriksson et al., 2024; Liu, 2015; Chen et al., 2021; Bandyopadhyay et al., 2024), are predominantly geometric and often constructed using SLAM algorithms (Taheri and Xia, 2021; Ramezani et al., 2022). However, these representations lack the semantic information necessary for understanding and interacting with the scene. Semantic understanding, on the other hand, is typically derived from 2D object detection/segmentation models. The central challenge is aligning 2D semantics with 3D geometry to form a unified representation that enables complex reasoning and interaction. Moreover, human queries often span multiple levels of granularity and conceptual domains (e.g., “a pillow with a tree pattern”), requiring far richer semantics than conventional models can provide.\nFinally, human understanding of environments is inherently structural, involving hierarchical organisation and inter-object relationships. Capturing and reflecting the structural organisation in the map is essential for enhancing analytical capabilities and enabling more intuitive interaction with complex environments.\n\nTo address these challenges, we introduce 3D QSR, a scene-understanding multimodal framework built using multimedia data. It combines state-of-the-art 3D reconstruction techniques, such as NeRF (Mildenhall et al., 2021) and point clouds, with advanced semantic understanding through panoptic segmentation and vision-language embeddings. We also incorporate a 3D scene graph as an abstract layer, providing a structured, explicit, and lightweight representation enriched with object properties and inter-object relationships. Unlike single-modality systems, 3D QSR supports object-level queries involving location, appearance, function, and relational context, significantly enhancing scene understanding and interaction.\nWith a Large Language Model (LLM), our framework supports advanced language querying and reasoning grounded in QSR content. We demonstrate the capability of this representation through various downstream robotic task planning scenarios simulated in Unity (Juliani, 2018) using the Replica dataset (Straub et al., 2019). In summary, the 3D QSR framework provides:\n\nUnified alignment of semantic, geometric, and structural information, enabling robots to reason over spatial and semantic context simultaneously.\n\nUnified alignment of semantic, geometric, and structural information, enabling robots to reason over spatial and semantic context simultaneously.\n\nNatural language-driven interaction, supporting intuitive query-answering for object retrieval.\n\nComprehensive support for robotic task planning, such as autonomous navigation, object retrieval, and adaptive decision-making in complex scenarios.\n\n1. Unified alignment of semantic, geometric, and structural information, enabling robots to reason over spatial and semantic context simultaneously.\n\n2. Natural language-driven interaction, supporting intuitive query-answering for object retrieval.\n\n3. Comprehensive support for robotic task planning, such as autonomous navigation, object retrieval, and adaptive decision-making in complex scenarios.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现机器人对3D环境的全面理解，以执行复杂任务。  \n2. 如何将2D语义信息与3D几何信息对齐，形成统一的表示。  \n3. 如何支持自然语言驱动的交互，以增强机器人与环境的互动能力。  \n\n【用了什么创新方法】  \n提出了3D Queryable Scene Representation (3D QSR)框架，该框架结合了3D重建技术、全景分割和视觉-语言嵌入，形成了一个多模态的场景理解系统。通过3D场景图的引入，3D QSR能够支持对象级查询，增强场景理解和交互能力。该框架通过与大型语言模型的结合，实现了基于自然语言的查询和推理，展示了在复杂场景下的机器人任务规划能力。实验结果表明，3D QSR显著提高了机器人在复杂环境中的导航、物体检索和决策能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "LLM Trainer: Automated Robotic Data Generating via Demonstration Augmentation using LLMs",
            "authors": "Abraham George,Amir Barati Farimani",
            "subjects": "Robotics (cs.RO)",
            "comment": ". Submitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.20070",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20070",
            "arxiv_html_link": "https://arxiv.org/html/2509.20070v1",
            "abstract": "We present LLM Trainer, a fully automated pipeline that leverages the world knowledge of Large Language Models (LLMs) to transform a small number of human demonstrations (as few as one) into a large robot dataset for imitation learning. Our approach decomposes demonstration generation into two steps: (1) offline demonstration annotation that extracts keyframes, salient objects, and pose–object relations; and (2) online keypose retargeting that adapts those keyframes to a new scene, given an initial observation. Using these modified keypoints, our system warps the original demonstration to generate a new trajectory, which is then executed, and the resulting demo, if successful, is saved. Because the annotation is reusable across scenes, we use Thompson sampling to optimize the annotation, significantly improving generation success rate. We evaluate our method on a range of tasks, and find that our data annotation method consistently outperforms expert-engineered baselines. We further show an ensemble policy that combines the optimized LLM feed-forward plan with a learned feedback imitation learning controller. Finally, we demonstrate hardware feasibility on a Franka Emika Panda robot. For additional materials and demonstration videos, please see the project website: https://sites.google.com/andrew.cmu.edu/llm-trainer",
            "introduction": "Recent advances in Large Language Models (LLMs) have revolutionized the field of robot learning, with applications ranging from task planning [1], to tool use in long horizon tasks [2], to deformable object manipulation [3].\nAt the core of these works is the LLM’s broad base of world knowledge, gathered from training on internet-scale data, which allows these agents to be extremely generalizable. In this work, we seek to leverage the world knowledge of LLMs to fully automate demonstration generation through human demo augmentation. To do this, we employ a similar pipeline as [4] and [5] for data generation: first, identify key robot poses in a demonstration, then generate a new environment and modify the key poses based on an initial observation, and finally, use the new key poses to warp the demonstration trajectory, resulting in a new trajectory, which is rolled out in the new environment. However, unlike prior works which rely on human annotation and hard-coded methods to identify key poses and modify them in response to the new environments [4, 5, 6], our system seeks to fully automate this process by leveraging large language models (LLMs). An outline of our method can be seen in Fig. 1.\n\nOur method for LLM-based data generation has two main steps: First, the LLM annotates the human demonstration, identifying keyframes (timesteps that are important inflection points for the task), listing relevant objects at each keyframe, and explaining the relationship between the robot and these objects. Second, the LLM uses this annotation, along with an initial observation of a newly initialized scene, to determine how the robot’s pose should be adjusted at each keypoint. Because the first step of this process does not require information from the new scene, we can reuse these annotations, saving compute cost and opening the door for optimization. By employing a multi-armed bandit-based method, we are able to optimize the demo annotation step, improving data generation success rate by 2-3 times.\n\nOnce the data generation process is complete, we can use the generated data to train imitation learning agents. Additionally, thanks to our annotation optimization process, we develop a highly effective LLM-based feed-forward policy during data generation. In addition to serving as a viable agent on its own, we show that this feed-forward policy, when combined with the feedback agent, can form an effective ensembled policy, combining the long-horizon planning and generalizability of LLMs with the feedback control of imitation learning.\n\nThis work has three main contributions:\n\nAn LLM-based data generation method which can autonomously generate data using only a single, unannoted demonstration and a short (one sentence) description of the task.\n\nA multi-armed bandit-based optimization method which significantly improves demo generation success rate, allowing our method to outperform baselines that rely on expert annotations.\n\nAn ensembling strategy to combine a learned IL policy with the optimized LLM-based feedforward controller developed during data collection.\n\n1. An LLM-based data generation method which can autonomously generate data using only a single, unannoted demonstration and a short (one sentence) description of the task.\n\n2. A multi-armed bandit-based optimization method which significantly improves demo generation success rate, allowing our method to outperform baselines that rely on expert annotations.\n\n3. An ensembling strategy to combine a learned IL policy with the optimized LLM-based feedforward controller developed during data collection.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用LLMs自动生成机器人模仿学习所需的数据？  \n2. 如何优化演示生成过程以提高成功率？  \n3. 如何将LLM生成的策略与反馈控制策略结合以提升性能？  \n\n【用了什么创新方法】  \n本研究提出了LLM Trainer，一个自动化的数据生成管道，利用大型语言模型（LLMs）将少量人类演示转化为大量机器人数据。该方法分为两个步骤：首先，通过LLM进行离线演示注释，提取关键帧、显著对象和姿态-对象关系；其次，进行在线关键姿态重定向，根据初始观察调整关键帧。通过使用Thompson采样优化注释过程，成功率显著提高。实验结果表明，该方法在多项任务中优于专家设计的基线，并展示了将优化的LLM前馈计划与学习的反馈模仿学习控制器结合的有效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping",
            "authors": "Yinzhao Dong,Ji Ma,Liu Zhao,Wanyue Li,Peng Lu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20036",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20036",
            "arxiv_html_link": "https://arxiv.org/html/2509.20036v1",
            "abstract": "Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have demonstrated impressive performance on challenging terrains, allowing robots to execute complex skills such as climbing, running, and jumping. However, existing blind locomotion controllers often struggle to ensure safety\nand efficient traversal through risky gap terrains, which are typically highly complex, requiring robots to perceive terrain information and select appropriate footholds during locomotion accurately. Meanwhile, existing perception-based controllers still present several practical limitations, including a complex multi-sensor deployment system and expensive computing resource requirements. This paper proposes a DRL controller named MAstering Risky Gap Terrains (MARG), which integrates terrain maps and proprioception to dynamically adjust the action and enhance the robot’s stability in these tasks. During the training phase, our controller accelerates policy optimization by selectively incorporating privileged information (e.g., center of mass, friction coefficients) that are available in simulation but unmeasurable directly in real-world deployments due to sensor limitations. We also designed three foot-related rewards to encourage the robot to explore safe footholds. More importantly, a terrain map generation (TMG) model is proposed to reduce the drift existing in mapping and provide accurate terrain maps using only one LiDAR, providing a foundation for zero-shot transfer of the learned policy. The experimental results indicate that MARG maintains stability in various risky terrain tasks.",
            "introduction": "Legged robots have significantly advanced locomotion capabilities, demonstrating impressive skills across various movement modes, such as climbing stairs [1, 2], descending ramps [3], high-speed running [4], parkour [5], bipedal locomotion [6], and backflipping [7]. These abilities enable robots to perform well in continuous and highly challenging terrains, including rugged mountain paths, narrow passages, stairwells, slippery or unstable surfaces, etc. However, existing blind locomotion controllers often struggle to overcome risky gap terrains due to shortcomings in ensuring the safety and balance of quadruped robots.\n\nRisky gap terrains exhibit numerous complex characteristics, imposing nearly stringent demands on robots regarding footholds and balance capabilities during locomotion. As shown in Fig. 1 (a), robots must not only strive to maintain the stability of their center of gravity on a narrow single-plank bridge but also respond in real time to potential lateral disturbances. Once a robot makes errors during locomotion, such as slipping or shifting its center of gravity, it may quickly step on the air or lose stability, leading to a fall and potentially causing severe damage to the robot. When traversing balance beams, the quadruped robot must not only accurately perceive terrain information such as height variations, gap width, and edges, but also select appropriate landing footholds and timing for exertion based on its locomotion capabilities and current state to avoid missteps, as shown in Figs. 1 (b-c).\n\nThe majority of existing quadrupedal locomotion controllers are blind, which means that they do not utilize perception sensors like cameras and LiDARs [8, 9, 10, 11, 12]. It is nearly impossible for these controllers to traverse risky terrains as shown in Fig. 1. Recently, perception sensors have been used to obtain an elevation map of the environment [13, 1]. However, they do not take risky terrains into consideration. Only a few studies consider risky terrains, and they either rely on multiple sensors [14], which significantly increases the complexity of hardware deployment, or use motion capture systems to obtain prior information about the terrain [15]. In this paper, we only use one sensor to construct a robot-centered map and do not rely on motion capture systems.\n\nExisting model-based controllers rely on precise modeling of robots to calculate the optimal joint torques or footholds required for locomotion. For example, Singh et al. [16] compute second-order derivatives of rigid-body inverse and forward dynamics, achieving significant speed-ups over automatic differentiation in optimization-driven robot control. The CAFE-MPC framework [17] employs a cascaded-fidelity model predictive control scheme paired with a tuning-free whole-body controller, enabling quadruped robots to execute agile maneuvers without manual parameter tuning. Meduri et al. [18] splits the nonlinear MPC problem into biconvex centroidal dynamics and full-body kinematics, enabling real-time generation of dynamic whole-body motions for legged robots. These models can generate accurate control commands, enabling the robot to achieve stable and efficient locomotion in an ideal simulation and simple terrains [19]. However, uncertainty factors in real-world environments, such as terrain irregularity, changing friction, and external disturbances, present significant challenges to model-based methods. These factors are difficult to accurately incorporate into models, leading to potential mismatches between the model and the real world. Even slight discrepancies can cause robot locomotion failures, especially in risky gap terrains.\n\nTo address these challenges, researchers [20, 21] have attempted to simplify the dynamics model by utilizing Nonlinear Model Predictive Control (NMPC) to enhance the locomotion of robots in complex and dynamic environments. Yin et al. [22] propose an optimization algorithm to improve the robot’s locomotion performance by transforming the discrete terrain height map into a continuous cost map to adjust the footholds dynamically. [23] proposes a novel control system that integrates adaptive control into a force-based control system for legged robots, enabling them to dynamically locomotion on uneven terrains. However, the computational complexity and slow convergence rates limit the applicability of robots in dynamic environments.\n\nIn addition, studies [24, 25] are also exploring the use of multiple sensors to enhance the accuracy and reliability of the model. Alongside the robot’s inertial measurement unit (IMU), external perception devices such as depth cameras and LiDARs are employed to gather environmental information, including terrain width, height, and edge shape, and integrate this information into the dynamic model to assist robot control [26]. The synchronization of sensor data, the design of fusion algorithms for different sensor inputs, and the computational burden of data processing will further adversely affect the real-time control performance of robots.\n\nModel-free methods, such as deep reinforcement learning (DRL), have shown promise in enabling legged robots to adapt to complex terrains without relying on precise dynamic models. These methods focus on training robots to learn optimal policies through trial and error, allowing them to manage uncertainties and dynamic changes in their environment effectively [27]. The blind locomotion controllers [2, 10] have shown impressive progress in enabling robots to traverse challenging continuous terrains. However, these controllers often struggle in risky terrains due to the absence of environmental perception.\n\nIntegrating data from other external sensors, such as depth cameras, motion capture, etc, into the DRL framework is an effective way to help robots comprehensively understand their surrounding environment. Pioneering works [28, 5, 29] utilize deep learning models, such as GRU [30] and LSTM [31], to process depth images and extract terrain features, including height, slope, and distribution. Robots can successfully perform high-difficulty parkour tasks by incorporating these terrain factors into their decision-making processes. Challenges such as lighting changes, occlusion issues, high dimensionality, and complexity of images [32] may lead to inaccurate terrain feature extraction or high computational complexity during the training process, ultimately affecting the real-time performance of the robot [33]. Meanwhile, [34] and [15] use motion capture and an offline map to derive the height map around the robot’s feet, which limits the practical applicability of this algorithm.\n\nTo obtain more accurate terrain information in the real world, previous DRL controllers [13, 35, 14] utilize multiple depth cameras or LiDARs simultaneously for elevation mapping, which can significantly enhance the accuracy of terrain representation. However, this approach increases the complexity of hardware deployment, as it requires sophisticated processing capabilities to handle the data from multiple sensors. Additionally, existing localization technologies  [36, 37, 38, 39] heavily rely on the pose estimation of floating bases within the global frame. Any inaccuracy in this estimation may lead to map drift, thereby affecting the movement of legged robots in risky terrains. Thus, designing safe and reliable controllers, developing efficient algorithms to simplify deployment processes, and obtaining precise terrain maps remain challenging in risky gaps tasks.\n\nIn summary, we propose a DRL controller for quadrupedal locomotion—MAstering Risky Gap (MARG)—which integrates terrain maps, privileged information, and proprioceptive into the policy to enhance the locomotion performance of quadrupedal robots in risky terrains. The key contributions of this work can be listed as follows:\n\nWe propose a safe and robust robot controller for locomotion, which can predict the body velocity and the contact state of feet on each step, significantly enhancing the robot’s stability in risky gap terrains.\n\nFor risky tasks, we have designed three foot-related rewards: feet air time, feet stumble, and feet center, which promote the policy to explore safe footholds, enhancing the safety of movement.\n\nWe propose a terrain map generation model that uses a single LiDAR to obtain the robot-centered height map. Our method minimizes drift compared to the traditional localization approaches while achieving zero-shot transfer capability and optimal computational efficiency.\n\nThe MARG controller empowers quadruped robots to adeptly handle risky gap terrains in the real world, including 65 cm large gaps, 18 cm narrow single-plank bridges, and balance beams with varying sizes, heights, and inclinations.\n\n1. We propose a safe and robust robot controller for locomotion, which can predict the body velocity and the contact state of feet on each step, significantly enhancing the robot’s stability in risky gap terrains.\n\n2. For risky tasks, we have designed three foot-related rewards: feet air time, feet stumble, and feet center, which promote the policy to explore safe footholds, enhancing the safety of movement.\n\n3. We propose a terrain map generation model that uses a single LiDAR to obtain the robot-centered height map. Our method minimizes drift compared to the traditional localization approaches while achieving zero-shot transfer capability and optimal computational efficiency.\n\n4. The MARG controller empowers quadruped robots to adeptly handle risky gap terrains in the real world, including 65 cm large gaps, 18 cm narrow single-plank bridges, and balance beams with varying sizes, heights, and inclinations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高四足机器人在复杂和危险的间隙地形中的稳定性和安全性。  \n2. 现有的盲目运动控制器在感知和选择合适的落脚点方面的局限性。  \n3. 如何减少传统定位方法中的地图漂移并实现零-shot转移能力。  \n\n【用了什么创新方法】  \n本文提出了一种名为MAstering Risky Gap Terrains (MARG)的深度强化学习控制器，集成了地形图和本体感知，以动态调整行动并增强机器人在危险地形中的稳定性。通过选择性地整合特权信息（如重心和摩擦系数），加速了策略优化。设计了三种与脚相关的奖励机制，以鼓励机器人探索安全的落脚点。此外，提出了一种地形图生成模型，仅使用一个LiDAR来生成准确的地形图，从而减少了映射中的漂移。实验结果表明，MARG在各种危险地形任务中保持了稳定性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "An effective control of large systems of active particles: An application to evacuation problem",
            "authors": "Albina Klepach,Egor E. Nuzhin,Alexey A. Tsukanov,Nikolay V. Brilliantov",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19972",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19972",
            "arxiv_html_link": "https://arxiv.org/html/2509.19972v1",
            "abstract": "Manipulation of large systems of active particles is a serious challenge across diverse domains, including crowd management, control of robotic swarms, and coordinated material transport. The development of advanced control strategies for complex scenarios is hindered, however, by the lack of scalability and robustness of the existing methods, in particular, due to the need of an individual control for each agent.\nOne possible solution involves controlling a system through a leader or a group of leaders, which other agents tend to follow. Using such an approach we develop an effective control strategy for a leader, combining reinforcement learning (RL) with artificial forces acting on the system. To describe the guidance of active particles by a leader we\nintroduce the generalized Vicsek model.\nThis novel method is then applied to the problem of\nthe effective evacuation by a robot-rescuer (leader) of large groups of people from hazardous places. We demonstrate, that while a straightforward application of RL yields suboptimal results, even for advanced\narchitectures, our approach provides a robust and efficient evacuation strategy.\nThe source code supporting this study is publicly available at: https://github.com/cinemere/evacuation.",
            "introduction": "The manipulation of large systems of active particles, especially controlling their collective behavior, has become a fundamental problem in recent decades, initiated by emerging new areas of application.\nThe examples, across diverse domains,\ninclude crowd management [1, 2], controlling of robotic swarms\n[3], coordinated material transport [4], etc. The concept of active matter [5] offers an appropriate framework for modeling such ensembles,\ncomprised of agents that consume energy to move, interact with their environment, and adjust their direction of motion subject to external signals [6, 7, 8, 9].\nSynthetic microswimmers, swarming robots, colonies of bacteria, fish schools, flocks of birds, groups of humans [10, 9] – all these systems of living and non-living agents represent an active matter.\n\nThe primary goals of controlling active particle systems include navigating them in complex environments and supporting synchronized collective behavior [11, 12, 13]. This can be addressed by classical optimal control theory, which requires a complete knowledge of the environment and dynamics (similar to Zermelo’s navigation problem [14]), or by reinforcement learning (RL), where an agent learns strategies through trial-and-error [15, 16]. The latter is particularly applicable when an agent has only partial, local information or when the impact of noise is significant. A key limitation of such control problems\nis that their solution relies on the individual manipulation of each participating particle. For ensembles comprising hundreds of agents a straightforward application of RL, with individual-level control, becomes computationally intractable.\n\nTo overcome this limitation, a common strategy involves guiding a group through a leader or multiple leaders, when active particles just follow the leader(s) [17, 18, 19, 20]. This leader-follower paradigm significantly simplifies the control problem by focusing strategies solely on the leader(s). Nevertheless, complexities persist, particularly when managing multiple groups of different size and location, or when groups cannot be manipulated simultaneously or possess non-coinciding goals [21, 22]. Among\nimportant examples of such challenging problems is an effective guidance of large groups of people, especially – their evacuation from hazardous places. Here a leader (rescuer)\nhas the goal to evacuate all people\nguiding them to the exit(s) in shortest time.\nGiven the inherent randomness and non-negligible noise within these systems, reinforcement learning (RL) seems to be the most\nsuitable approach. However, a straightforward application of standard RL\nis neither computationally efficient nor effective in achieving the ultimate goal.\n\nTo this end, we propose an application of auxiliary (artificial)\n“pseudo-gravitational” forces acting on the system, as a part of the environment; this helps the leader to find the most effective guidance strategy.\nSuch a combination of RL with artificial forces [23] results in a very effective method to control large ensembles of active particles.\nTo implement our new method for the evacuation problem, we utilize the generalized Vicsek model. This model takes into account\nnot only interactions between active particles\n(as in the conventional Vicsek model [5]), but also between particles and the leader.\n\nThe rest of the paper is organized as follows. In the next Sec. II we formulate the problem of an effective evacuation from a hazardous place by an informed rescuer-leader. In Sec. III we describe the methods for training optimal evacuation policy and propose an approach based on “pseudo-gravitational” forces, viz the new method of RL with artificial fields. In Sec. IV we report the results of our numerical experiments. Finally, in Sec. V, we summarize our findings.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效控制大规模活性粒子系统以实现集体行为的协调。  \n2. 如何在复杂环境中进行有效的人员疏散，特别是在危险场所的疏散。  \n3. 现有方法在处理多个群体时的可扩展性和鲁棒性问题。  \n\n【用了什么创新方法】  \n本研究提出了一种结合强化学习（RL）与人工“伪引力”力的控制策略，通过引导者控制活性粒子群体的行为。利用广义Vicsek模型，研究者能够有效地指导大规模人群的疏散。实验结果表明，尽管传统RL方法效果不佳，采用新方法后，疏散策略表现出显著的鲁棒性和效率。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Generalist Robot Manipulation beyond Action Labeled Data",
            "authors": "Alexander Spiridonov,Jan-Nico Zaech,Nikolay Nikolov,Luc Van Gool,Danda Pani Paudel",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted at Conference on Robot Learning 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.19958",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19958",
            "arxiv_html_link": "https://arxiv.org/html/2509.19958v1",
            "abstract": "Recent advances in generalist robot manipulation leverage pre-trained Vision–Language Models (VLMs) and large-scale robot demonstrations to tackle diverse tasks in a zero-shot manner. A key challenge remains: scaling high-quality, action-labeled robot demonstration data, which existing methods rely on for robustness and generalization. To address this, we propose a method that benefits from videos without action labels—featuring humans and/or robots in action—enhancing open-vocabulary performance and enabling data-efficient learning of new tasks. Our method extracts dense, dynamic 3D point clouds at the hand or gripper location and uses a proposed 3D dynamics predictor for self-supervision. This predictor is then tuned to an action predictor using a smaller labeled dataset for action alignment. We show that our method not only learns from unlabeled human and robot demonstrations—improving downstream generalist robot policies—but also enables robots to learn new tasks without action labels (i.e., out-of-action generalization) in both real-world and simulated settings.",
            "introduction": "Robust zero-shot manipulation across diverse tasks and environments is one of the biggest bottlenecks towards truly autonomous robots. Inspired by the open-world reasoning capabilities of Large-Language (LLM) and Vision-Language Models (VLM), Vision-Language-Action (VLA) models have emerged for generalist robot manipulation. Approaching this challenge, VLAs extend the semantic reasoning abilities of VLMs with embodied understanding and adapt them for robotic control by training on large datasets of teleoperated robot demonstrations [1, 2, 3, 4, 5, 6, 7]. This has led to impressive progress in learning robust manipulation policies. However, most success is centered around in-domain settings, and performance quickly degrades as the tasks move outside the training distribution. While collecting yet larger robot datasets seems straightforward, it remains unclear what resources would be required to achieve generalist manipulation.\n\nMultimodal training with videos of human demonstrations is a promising alternative to prohibitively expensive robot demonstrations. Such videos contain valuable spatiotemporal information highly relevant to learning robotic control, are readily available at internet scale, and provide diverse tasks and environments. However, learning from human demonstration datasets comes with a range of challenges; Videos provide no direct action labels supervision, exhibit human-to-robot domain gaps, and include redundant or distracting features irrelevant to robotic control.\n\nBeing a fundamental challenge, learning motion priors from humans and unlabeled data has been widely explored. Yet existing work remains confined to specialist, small-scale policies. Some focus on visual representations [8, 9, 10, 11], not considering unseen motions. Others predict visual plans that require a bespoke inverse-dynamics model for execution [12, 13, 14, 15, 16, 17]. Another line retargets human hands to robot grippers [18, 19, 20, 21, 22], but suffers from a large domain gap.\n\nIn this work, we bridge this gap and present MotoVLA, a generalist robot manipulation policy that enables new tasks from human and robot videos without action labels. To achieve this, we propose a VLA model and two-stage training approach using a combination of large-scale labeled and unlabeled\n111Unlabeled refers to non-action-labeled, as action labels are the main challenge in acquiring manipulation data.\nhuman and robot videos.\nIn the first training stage, a dynamic point cloud predictor is trained on the unlabeled data, which establishes a common embodiment-agnostic action representation. Since the dynamic point cloud strongly correlates with the end-effector actions up to hand-eye calibration, the second stage training of an action expert on action-labeled data is simplified. This natural correspondence between dynamic point clouds and 3D robot actions makes our approach particularly effective for learning from unlabeled data. An overview of our method is shown in Figure 1.\nIn summary, our contributions are:\n\nMotoVLA, the first end-to-end VLA model that allows the use of unlabeled data for learning motion priors required for the generalist robot manipulation.\n\nA two-stage training approach enabling the use of dynamic point clouds as a common embodiment-agnostic representation, which is both scalable and intuitive.\n\nExtensive real and simulated evaluations of our model for in-domain, out-of-domain, and transfer learning tasks, demonstrating the effective use of unlabeled data by our model.\n\n1. MotoVLA, the first end-to-end VLA model that allows the use of unlabeled data for learning motion priors required for the generalist robot manipulation.\n\n2. A two-stage training approach enabling the use of dynamic point clouds as a common embodiment-agnostic representation, which is both scalable and intuitive.\n\n3. Extensive real and simulated evaluations of our model for in-domain, out-of-domain, and transfer learning tasks, demonstrating the effective use of unlabeled data by our model.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在缺乏高质量、标注动作的数据情况下实现通用机器人操作？  \n2. 如何利用无标签视频数据提升机器人在多样任务中的表现？  \n3. 如何解决人类与机器人之间的领域差距以实现有效学习？  \n\n【用了什么创新方法】  \n提出了MotoVLA，一个通用机器人操作策略，利用无标签的人类和机器人视频数据进行学习。方法包括一个两阶段的训练流程：第一阶段在无标签数据上训练动态点云预测器，建立通用的动作表示；第二阶段在小规模标注数据上训练动作专家，实现动作对齐。通过这种方式，MotoVLA能够有效地从无标签数据中学习运动先验，提升机器人在多种任务中的表现，尤其是在真实和模拟环境中的迁移学习任务。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation",
            "authors": "Pinhao Song,Yurui Du,Ophelie Saussus,Sofie De Schrijver,Irene Caprara,Peter Janssen,Renaud Detry",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19954",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19954",
            "arxiv_html_link": "https://arxiv.org/html/2509.19954v1",
            "abstract": "We propose a probabilistic shared-control solution for navigation, called Robot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe, effective assistance in human–robot interaction. RT-V2 jointly models a user’s long-term behavioral patterns and their noisy, low-dimensional control signals by combining a prior intent model with a posterior update that accounts for real-time user input and environmental context. The prior captures the multimodal and history-dependent nature of user intent using recurrent neural networks and conditional variational autoencoders, while the posterior integrates this with uncertain user commands to infer desired actions. We conduct extensive experiments to validate RT-V2 across synthetic benchmarks, human–computer interaction studies with keyboard input, and brain–machine interface experiments with non-human primates. Results show that RT-V2 outperforms the state of the art in intent estimation, provides safe and efficient navigation support, and adequately balances user autonomy with assistive intervention. By unifying probabilistic modeling, reinforcement learning, and safe optimization, RT-V2 offers a principled and generalizable approach to shared control for diverse assistive technologies. Code will be available in https://mousecpn.github.io/RTV2_page/.",
            "introduction": "Shared control is a collaborative approach between a human operator and a robot, designed to reduce operator workload and facilitate the more efficient and safer completion of complex tasks with the robot. This approach is widely used in various fields, such as subsea maintenance, surgery, driving, and assistive devices. In the context of assistive devices, it enables individuals with disabilities to regain autonomy through technologies like robotic wheelchairs and manipulators. The key challenge here is that while robots have many degrees of freedom, the input devices available to disabled users are typically low-DoF and noisy due to the nature of their disabilities. Examples include chin joysticks (Rulik et al., 2022) and neural implants (Hochberg et al., 2012). Using these devices to control a wheelchair or robot arm can be slow, tiring, and prone to errors. Shared control addresses these issues by identifying the user’s intent and facilitating smoother, more effortless goal achievement. To enhance this process, shared control often utilizes additional sources of information to interpret user input in context. For instance, cameras can provide images of the surrounding environment to aid in this interpretation.\n\nAccurately assisting and executing a user’s desired action requires understanding their intent.\nPredicting user intent is particularly challenging due to three defining characteristics:\n(i) Multi-modal: a user may approach a goal through different sub-optimal paths;\n(ii) Non-Markovian: past experiences continue to influence current actions (e.g., a previous car accident may make a driver more cautious);\n(iii) Non-stationary: user performance fluctuates, improving when they are focused and declining when they are fatigued.\nThis complexity creates a dynamic interplay between the user’s and the assistive controller’s authority. Users often require more control to effectively convey their intent, while assistive controllers may need increased authority to reduce the user’s effort. This tension gives rise to two critical dilemmas frequently faced by assistive controllers:\n\n(i) Action–noise dilemma:\nAn assistive controller must execute the user’s intended actions while filtering out noise from the user interface. However, this task is complicated by the challenge of distinguishing noise from the effect of the three defining characteristics of user intent listed above.\nOver-reliance on the assistive controller may reduce noise effectively but risks suppressing genuine changes in the user’s intent. Conversely, relying more on the user preserves their autonomy but fails to alleviate their control burden or input noise. For example, as illustrated in Fig. 1 (a), at the current timestep, the assistive controller predicts an action toward goal 1, while the user issues a command toward goal 2. Treating the user’s command as noise and filtering it out may result in the user reaching an unintended goal if the command represents their true intent. On the other hand, fully relying on the user’s command offers no assistance if the command is indeed noise.\nSome approaches attempt to address this dilemma heuristically (Demeester et al., 2008; Song et al., 2024a). By introducing a disagreement threshold, the system differentiates between intended actions and noise, returning control to the user when the threshold is exceeded. However, this method does not fundamentally solve the problem, as there is no guarantee that a user command exceeding the threshold represents true intent. Additionally, the process of setting such a threshold lacks theoretical justification.\nTo fully resolve the action–noise dilemma, the assistive controller must adaptively blend actions by considering the uncertainties of both the user and the environment.\n\n(ii) Disagreement Dilemma: A common approach to shared control involves linearly blending the user’s commands with those of the controller, facilitated by an arbitrator (Dragan and Srinivasa, 2013; Song et al., 2024a; Maeda, 2022). However, as Trautman points out (Trautman, 2015), tasks such as collision avoidance often allow for multiple equally optimal trajectories due to the multi-modal nature of human intent. The user may select any of these trajectories, which can differ from the controller’s prediction.\nWhen the user disagrees with the assistive controller at a given timestep, blending a safe user command with a safe controller-proposed action may unintentionally result in an unsafe shared action. For example, as shown in Fig. 1 (b), the assistive controller predicts an action to the left, while the user commands movement to the right. A linear blend of these two actions could lead to a collision with the obstacle.\nExisting approaches, such as those based on probabilistic models (Trautman, 2015), constraint-based shared control (Iregui et al., 2021), and model predictive control (MPC) (Lu et al., 2019), attempt to address this issue by implicitly blending policies. However, these methods often reduce to linear blending, limiting their ability to fully resolve the disagreement dilemma.\nTo effectively tackle this challenge, the assistive controller should adopt a multi-modal blending strategy. For instance, it could generate multiple trajectory proposals and blend the user’s command with the proposal most aligned with their intent, ensuring both safety and responsiveness.\n\nIn this paper, we propose an assistive controller named Robot Trajectron V2 (RT-V2) for navigation tasks. RT-V2 is designed within a Bayesian framework, as:\n\nIn summary:\n\nOur paper proposes Robot Trajectron V2 (RT-V2), a Bayesian-based assistive controller designed for navigation tasks. RT-V2 models user behavior using a prior trained on past data and a posterior that adapts to real-time user commands, addressing both action-noise and disagreement dilemmas.\n\nThe prior model is built using a recurrent neural network and a conditional variational autoencoder (CVAE), enabling it to capture multi-modal and non-Markovian aspects of human intent. This enhances the controller’s ability to accurately interpret and blend user commands in a dynamic shared control setting.\n\nTo overcome causal confusion in imitation learning, we introduce Imagined Rollout Reinforcement Learning, where RT-V2 simulates future interactions to receive reward signals and refine its autonomous navigation capabilities.\n\nA sampling-based trajectory optimization method with safety constraints is employed to ensure the controller’s actions are collision-free.\n\nThe novel contributions of the paper are:\n\nA shared-control model grounded in a probabilistic formulation of the intention prior and posterior, and their acquisition from data via a combination of imitation learning, reinforcement learning, and sampling-based optimization.\n\nExtensive experiments show that RT-V2 achieves high accuracy in intent estimation and safe, efficient navigation. Tests with human users (keyboard interface) and monkey users (BMI interface) demonstrate its effectiveness in optimizing shared autonomy with respect to agreeability, safety, and efficiency.\n\n1. Our paper proposes Robot Trajectron V2 (RT-V2), a Bayesian-based assistive controller designed for navigation tasks. RT-V2 models user behavior using a prior trained on past data and a posterior that adapts to real-time user commands, addressing both action-noise and disagreement dilemmas.\n\n2. The prior model is built using a recurrent neural network and a conditional variational autoencoder (CVAE), enabling it to capture multi-modal and non-Markovian aspects of human intent. This enhances the controller’s ability to accurately interpret and blend user commands in a dynamic shared control setting.\n\n3. To overcome causal confusion in imitation learning, we introduce Imagined Rollout Reinforcement Learning, where RT-V2 simulates future interactions to receive reward signals and refine its autonomous navigation capabilities.\n\n4. A sampling-based trajectory optimization method with safety constraints is employed to ensure the controller’s actions are collision-free.\n\n1. A shared-control model grounded in a probabilistic formulation of the intention prior and posterior, and their acquisition from data via a combination of imitation learning, reinforcement learning, and sampling-based optimization.\n\n2. Extensive experiments show that RT-V2 achieves high accuracy in intent estimation and safe, efficient navigation. Tests with human users (keyboard interface) and monkey users (BMI interface) demonstrate its effectiveness in optimizing shared autonomy with respect to agreeability, safety, and efficiency.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何准确预测用户意图以实现安全有效的导航支持。  \n2. 如何解决助理控制器在用户命令和控制器建议之间的行动噪声和不一致性困境。  \n\n【用了什么创新方法】  \n本研究提出了Robot Trajectron V2 (RT-V2)，一个基于贝叶斯框架的助理控制器，旨在解决导航任务中的用户意图建模问题。RT-V2结合了先验模型和后验更新，利用递归神经网络和条件变分自编码器捕捉用户的多模态和非马尔可夫特性。通过引入想象滚动强化学习，RT-V2能够模拟未来交互以优化自主导航能力。此外，采用基于采样的轨迹优化方法确保安全性。实验结果表明，RT-V2在意图估计和安全导航方面优于现有方法，展示了其在共享自主性优化中的有效性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference",
            "authors": "Zijun Che,Yinghong Zhang,Shengyi Liang,Boyu Zhou,Jun Ma,Jinni Zhou",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19916",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19916",
            "arxiv_html_link": "https://arxiv.org/html/2509.19916v1",
            "abstract": "Autonomous exploration in structured and complex indoor environments remains a challenging task, as existing methods often struggle to appropriately model unobserved space and plan globally efficient paths.\nTo address these limitations, we propose GUIDE, a novel exploration framework that synergistically combines global graph inference with diffusion-based decision-making.\nWe introduce a region-evaluation global graph representation that integrates both observed environmental data and predictions of unexplored areas, enhanced by a region-level evaluation mechanism to prioritize reliable structural inferences while discounting uncertain predictions.\nBuilding upon this enriched representation, a diffusion policy network generates stable, foresighted action sequences with significantly reduced denoising steps. Extensive simulations and real-world deployments demonstrate that GUIDE consistently outperforms state-of-the-art methods, achieving up to 18.3% faster coverage completion and a 34.9% reduction in redundant movements.",
            "introduction": "Autonomous exploration remains a cornerstone of modern robotics research, with pivotal applications in scenarios where unknown environment coverage is critical: environmental monitoring, warehouse logistics, and search-and-rescue operations [1, 2]. A defining challenge in these tasks is efficiently covering all reachable areas under stringent constraints—limited time, finite energy, and constrained computational resources. Despite decades of progress, existing exploration strategies still struggle to appropriately model unobserved space and plan globally efficient paths.\n\nCurrent exploration methodologies can be broadly categorized into model-based and learning-based approaches, each exhibiting fundamental limitations in addressing the global coverage challenge. Early model-based techniques, including frontier-based methods [3, 4] and sampling-based exploration strategies [5], rely exclusively on observed map information to determine exploration directions. While these approaches demonstrate reasonable performance in structured environments, their inherent myopia toward unobserved areas frequently results in redundant revisits, inefficient path planning, and suboptimal coverage—particularly in environments with complex topologies. Coverage path-based methods [6, 7] attempt to address this limitation by incorporating explicit coverage objectives; however, their reliance on uniform grid decomposition implicitly assumes environmental regularity, leading to performance degradation in spaces with irregular layouts or varying structural complexity.\n\nMore recently, learning-based approaches have emerged as promising alternatives, yet they too face significant challenges in achieving comprehensive spatial understanding. The first category employs neural networks to directly map observed environments to exploration actions [8, 9, 10]. Although these methods improve adaptability to specific environments, they fundamentally operate with limited information—encoding only observed areas while remaining agnostic to the structure of unknown spaces. This inherent constraint severely limits their capacity to achieve globally efficient exploration, often requiring extensive training across diverse environments to achieve moderate performance. The second category explicitly predicts unobserved areas and associated information gain [11, 12]. While conceptually promising, these approaches typically utilize predicted maps only for local planning rather than incorporating them into a comprehensive global planning framework, thereby failing to fully leverage the predictive information for long-horizon path optimization.\n\nThese limitations collectively highlight a critical research gap: the absence of a unified framework that effectively integrates predictions of unknown areas with globally optimized exploration planning. Specifically, existing methods lack mechanisms to (1) construct a comprehensive environmental representation that coherently combines observed information with predictions of unexplored areas, (2) leverage credible predictions to guide exploration decisions, and (3) generate stable, long-horizon trajectories that maximize coverage efficiency while minimizing redundant movements.\n\nTo address these challenges, we propose GUIDE, a novel exploration framework that synergistically combines global graph inference with diffusion-based decision-making. At its core, GUIDE constructs a region-evaluation global graph representation that integrates both observed environmental data and predictions of unexplored areas. This representation is enhanced through a region-level evaluation mechanism that prioritizes significant regional structural inferences, effectively creating an informative yet compact environmental model that prioritizes credible structural inferences while appropriately discounting uncertain predictions. Building upon this enriched representation, GUIDE employs a diffusion policy network that generates stable, foresighted action sequences with significantly reduced denoising steps compared to conventional approaches—enabling efficient long-horizon planning that effectively balances immediate information gain with comprehensive coverage objectives. The reduced computational overhead ensures real-time responsiveness—a critical advantage for resource-constrained robotic platforms.\n\nWe rigorously evaluate GUIDE across diverse simulation environments with varying structural complexities and through real-world deployments on physical robotic platforms. Quantitative results demonstrate consistent improvements over state-of-the-art methods, with our approach achieving up to 18.3% faster coverage completion and a 34.9% reduction in redundant movements across benchmark environments. Qualitative analysis further showcases GUIDE’s superior capability in structural inference and adaptive exploration behavior.\nThe main contributions of this work are threefold:\n\n1) We introduce a region-evaluation global graph inference module that constructs a unified environmental representation by integrating observed information with predictions of unexplored areas. It incorporates a novel region-evaluation mechanism that assesses the reliability and decision relevance of predicted areas, enabling robust and reliable exploration planning under uncertainty.\n\n2) We develop a diffusion-based decision-making framework that leverages the global graph representation to generate stable, long-horizon exploration trajectories, significantly reducing the computational burden of conventional diffusion policies while producing foresighted and efficient exploration paths.\n\n3) We conduct comprehensive evaluations across multiple simulation environments and real-world scenarios, demonstrating GUIDE’s superior performance in both structural inference accuracy and exploration efficiency metrics, establishing a new benchmark for autonomous exploration systems.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效建模未观察空间以实现自主探索？  \n2. 现有方法在复杂环境中的路径规划效率不足。  \n3. 缺乏统一框架整合已知与未知区域的预测信息。  \n\n【用了什么创新方法】  \n本研究提出了GUIDE框架，结合全球图推理与扩散决策。首先，构建了一个区域评估的全球图表示，整合已观察环境数据与未探索区域的预测。通过区域级评估机制，优先考虑可靠的结构推断。然后，利用扩散策略网络生成稳定的长远行动序列，显著减少去噪步骤。实验结果显示，GUIDE在覆盖完成速度上提高了18.3%，并减少了34.9%的冗余移动，展现出优越的探索效率和结构推断能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "D3Grasp: Diverse and Deformable Dexterous Grasping for General Objects",
            "authors": "Keyu Wang,Bingcong Lu,Zhengxue Cheng,Hengdi Zhang,Li Song",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19892",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19892",
            "arxiv_html_link": "https://arxiv.org/html/2509.19892v1",
            "abstract": "Achieving diverse and stable dexterous grasping for general and deformable objects remains a fundamental challenge in robotics, due to high-dimensional action spaces and uncertainty in perception. In this paper, we present D3Grasp, a multimodal perception-guided reinforcement learning framework designed to enable Diverse and Deformable Dexterous Grasping. We firstly introduce a unified multimodal representation that integrates visual and tactile perception to robustly grasp common objects with diverse properties. Second, we propose an asymmetric reinforcement learning architecture that exploits privileged information during training while preserving deployment realism, enhancing both generalization and sample efficiency.\nThird, we meticulously design a training strategy to synthesize contact-rich, penetration-free, and kinematically feasible grasps with enhanced adaptability to deformable and contact-sensitive objects.\nExtensive evaluations confirm that D3Grasp delivers highly robust performance across large-scale and diverse object categories, and substantially advances the state of the art in dexterous grasping for deformable and compliant objects, even under perceptual uncertainty and real-world disturbances. D3Grasp achieves an average success rate of 95.1% in real-world trials—outperforming prior methods on both rigid and deformable objects benchmarks.",
            "introduction": "Dexterous robotic hands, with their human-like kinematic structures and multi-finger adaptability, hold transformative potential across industrial assembly, elderly care, and hazardous material handling. Recent advances in hardware design, exemplified by Shadow Dexterous Hand Robot (2025), Allegro Hand Hand (2025), and Paxini Dexhand13 Paxini (2025), have enabled 16+ degree-of-freedom (DoF) manipulation capabilities approaching human-level dexterity. However, two fundamental challenges persist in bridging this mechanical potential to real-world applications Xiao et al. (2025); An et al. (2025): multimodal perception integration and data-efficient policy learning, particularly for long-horizon manipulation tasks.\nContemporary robotic manipulation systems primarily depend on single sensing modality, each with inherent limitations: vision enables global localization but struggles with transparency or occlusion; tactile sensing offers precise contact feedback, yet lacks global awareness; proprioception monitors internal states but provides minimal environmental understanding. Hybrid architectures, such as visual-tactile fusion networks  Li et al. (2024b); Akinola et al. (2024); Dave et al. (2024); Ferrandis et al. (2024); Jin et al. (2023); Parsons et al. (2022), attempt to address these constraints through direct sensor concatenation. However, this approach induces a high-dimensional observation space, hindering policy convergence Tao et al. (2024). Crucially, fixed fusion weights cannot adapt to the varying sensory dominance across manipulation phases Li et al. (2022a); Wang et al. (2025), often resulting in conflicting signals that degrade control stability Akinola et al. (2024).\n\nFor data-efficient policy learning, modern simulation platforms such as IsaacSim Mittal et al. (2023), PyBullet Coumans and Bai (2016–2021), Genesis Authors (2024), Robotwin Mu et al. (2025) allow safe parallelized reinforcement learning (RL) exploration Li (2017). However, sim2real transfer is fundamentally hindered by sparse rewards and exponential exploration complexity in long-horizon tasks Wang et al. (2022), and catastrophic error propagation across sequential subtasks owing to compounding inaccuracies. Consequently, data-driven approaches leveraging imitation learning (IL) Hussein et al. (2017) and policy distillation (PD) Rusu et al. (2015) are gaining traction for improved sample efficiency Mandlekar et al. (2021), although scaling high-quality teleoperation data remains prohibitively expensive due to human-robot morphological differences Darvish et al. (2023). While integrated RL/IL/PD strategies Zhang et al. (2025a); Wan et al. (2023) mitigate data costs, they often overlook the critical influence of diverse object properties and grasp configurations in sim2real deployment.\n\nTo overcome these limitations, we introduce a multimodal learning framework for dexterous manipulation. Our primary contributions are threefold: (1) We develop a tactile-based multimodal perception representation capable of maximally leveraging environmental information and proprioception, while dynamically selecting optimal contact force outputs based on object material texture. (2) We construct an asymmetric actor-critic (AAC) network architecture utilizing privileged information; this framework employs simulated privileged data (e.g., deformable object deformation states) for policy value estimation within simulation, enabling optimal control mode selection and reducing the excessive reliance on perceptual precision in contact-intensive operations. (3) We propose a hybrid training paradigm that incorporates multiple category of objects and defines task-specific grasping postures, enabling the agent to acquire enhanced generalization capabilities.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现对一般和可变形物体的多样化和稳定的灵巧抓取。  \n2. 如何有效整合多模态感知以提高抓取性能。  \n3. 如何在长时间操作任务中实现数据高效的策略学习。  \n\n【用了什么创新方法】  \n本研究提出了D3Grasp，一个多模态感知引导的强化学习框架，旨在实现多样化和可变形的灵巧抓取。首先，构建了一个统一的多模态表示，集成视觉和触觉感知，以增强对不同物体的抓取能力。其次，设计了一个不对称的强化学习架构，利用特权信息进行训练，同时保持部署的现实性，从而提高了泛化能力和样本效率。最后，开发了一种训练策略，合成接触丰富、无穿透和运动学可行的抓取，增强了对可变形和接触敏感物体的适应性。D3Grasp在真实世界试验中取得了95.1%的平均成功率，显著超越了先前方法。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "SAGE:State-Aware Guided End-to-End Policy for Multi-Stage Sequential Tasks via Hidden Markov Decision Process",
            "authors": "BinXu Wu,TengFei Zhang,Chen Yang,JiaHao Wen,HaoCheng Li,JingTian Ma,Zhen Chen,JingYuan Wang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19853",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19853",
            "arxiv_html_link": "https://arxiv.org/html/2509.19853v1",
            "abstract": "Multi-stage sequential (MSS) robotic manipulation tasks are prevalent and crucial in robotics. They often involve state ambiguity, where visually similar observations correspond to different actions. We present SAGE, a state-aware guided imitation learning framework that models tasks as a Hidden Markov Decision Process (HMDP) to explicitly capture latent task stages and resolve ambiguity. We instantiate the HMDP with a state transition network that infers hidden states, and a state-aware action policy that conditions on both observations and hidden states to produce actions, thereby enabling disambiguation across task stages. To reduce manual annotation effort, we propose a semi-automatic labeling pipeline combining active learning and soft label interpolation. In real-world experiments across multiple complex MSS tasks with state ambiguity, SAGE achieved 100% task success under the standard evaluation protocol, markedly surpassing the baselines. Ablation studies further show that such performance can be maintained with manual labeling for only about 13% of the states, indicating its strong effectiveness.",
            "introduction": "Robotic manipulation tasks have attracted significant attention due to their broad applications. Vision-based strategies have been widely adopted [1], and have demonstrated remarkable performance across a variety of real-world scenarios [2, 3, 4, 5, 6]. However, a particular class of tasks—Multi-Stage Sequential (MSS) tasks—introduces distinctive challenges to vision-based policies. MSS tasks are characterized by a sequence of interdependent stages that must be executed in a prescribed temporal order, often requiring the policy to perform long-horizon reasoning, retain contextual information from prior steps, and ensure coherent progression across successive stages.\n\nIn many MSS tasks, conventional vision-based policies struggle in scenarios involving state ambiguity. In such cases, visually similar observations may correspond to different actions, resulting in ambiguity during action selection. An illustrative case is the Push Buttons task shown in Fig. 1. The visual observations at stages 1-1, 2-1, and 3-1 are nearly indistinguishable; however, the correct action—pressing the yellow, pink, or blue button—requires knowledge of the current task stage to be correctly determined. This requires the policy to map similar observations to distinct actions, a phenomenon we refer to as State Ambiguity. Similar challenges also arise in other real-world contexts, such as assessing whether a container has been filled in a warehouse packaging task, or judging whether a cloth is wet or dry during household cleaning. These examples highlight the inherent difficulty of resolving state ambiguity when relying solely on visual input.\n\nTo handle state ambiguity in MSS tasks, existing methods mainly fall into two categories: Memory-based approaches and hierarchical task decomposition. (1) In robotic manipulation, memory-based methods use models like recurrent neural networks [7], attention mechanisms [8], Transformer-XL [9] to capture historical context, as incorporating earlier observations can help distinguish visually similar states that lead to ambiguity. Although these methods are flexible, they often struggle with redundant information, high computational cost, and difficulties in deciding how much history to retain. (2)Hierarchical approaches, on the other hand, structure the policy into multiple levels of controllers. A high-level controller manages stage transitions, while low-level policies are responsible for executing specific actions [10, 11, 12, 13]. While this structure helps reduce ambiguity, designing the high-level controller typically requires extensive manual effort. For example, some methods use a nine-layer decision tree to handle transitions [11]. Moreover, since low-level modules directly execute action primitives, they are often constrained to be simple and modular, which limits flexibility. Transitions between primitives further introduce delays, reducing execution efficiency and stability in real-world scenarios.\n\nTo address the issue of State Ambiguity, we propose SAGE, a State-Aware Guided End-to-End Imitation Learning framework based on the Hidden Markov Decision Process (HMDP). Specifically, in Section III, we provide a theoretical analysis. The key idea is to treat observations as partial manifestations of a latent environment state and to explicitly model this state as a hidden variable. With this formulation, the HMDP can distinguish between different underlying physical states that share similar visual appearances, thereby resolving ambiguity. The HMDP consists of two components: hidden state estimation and decision-making agent. In Section IV, guided by the theoretical formulation, we implement these components as two neural networks. The state transition network infers hidden states from the current observation together with the previously estimated state, while the state-aware action policy generates actions by conditioning jointly on visual observations and the inferred states. We integrate them into an End-to-End architecture and train the framework using actions from expert demonstrations and human-labeled states as supervision. Furthermore, as detailed in Section V, to reduce the manual annotation cost of state labels required by supervised learning, we propose a semi-automatic labeling pipeline that integrates active learning with soft label interpolation. It substantially reduces manual labeling effort by annotating only a small subset of representative segments and automatically labeling the remaining data.\n\nExtensive real-world experiments on three MSS tasks, all of which involve state ambiguity, were conducted to evaluate the effectiveness of SAGE. The results show that it achieves up to a 100% stage success rate under standard evaluation settings, significantly outperforming competitive baselines. Our method remains robust under visually distracting conditions and continuous execution, completing 50-step sequences without error. Ablation studies show that our semi-automatic annotation strategy in SAGE achieves full success with only 13% of episodes manually labeled, demonstrating efficient annotation. These results validate the effectiveness and generality of our approach in tackling real-world state ambiguity.\n\nTherefore, we propose a unified imitation learning framework. Our main contributions are as follows:\n\nTo the best of our knowledge, we are the first to formulate MSS tasks with state ambiguity as a Hidden Markov Decision Process, offering a principled framework to address this challenge.\n\nWe realize the HMDP formulation through two dedicated neural modules for hidden state inference and state-aware action generation, which are jointly integrated into an End-to-End training pipeline.\n\nWe propose a semi-automatic state annotation strategy that substantially reduces the human effort required to provide state supervision signals for the HMDP.\n\nWe conduct extensive real-world experiments on multiple MSS tasks with state ambiguity, demonstrating the superior performance and robustness of SAGE.\n\n1. To the best of our knowledge, we are the first to formulate MSS tasks with state ambiguity as a Hidden Markov Decision Process, offering a principled framework to address this challenge.\n\n2. We realize the HMDP formulation through two dedicated neural modules for hidden state inference and state-aware action generation, which are jointly integrated into an End-to-End training pipeline.\n\n3. We propose a semi-automatic state annotation strategy that substantially reduces the human effort required to provide state supervision signals for the HMDP.\n\n4. We conduct extensive real-world experiments on multiple MSS tasks with state ambiguity, demonstrating the superior performance and robustness of SAGE.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何处理多阶段顺序（MSS）任务中的状态模糊性问题。  \n2. 如何通过隐马尔可夫决策过程（HMDP）建模MSS任务以捕捉潜在任务阶段。  \n3. 如何减少对手动标注的依赖，提高状态标注的效率。  \n\n【用了什么创新方法】  \n提出了一种名为SAGE的状态感知引导模仿学习框架，通过隐马尔可夫决策过程（HMDP）建模MSS任务，显式捕捉潜在状态并解决状态模糊性。该方法结合了状态转移网络和状态感知动作策略，能够根据观察和隐状态生成动作。通过引入半自动标注管道，结合主动学习和软标签插值，显著降低了手动标注的工作量。实验结果显示，SAGE在多个复杂MSS任务中实现了100%的任务成功率，远超基线方法，并且在仅标注13%状态的情况下仍能保持高效性能。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "DynaFlow: Dynamics-embedded Flow Matching for Physically Consistent Motion Generation from State-only Demonstrations",
            "authors": "Sowoo Lee,Dongyun Kang,Jaehyun Park,Hae-Won Park",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19804",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19804",
            "arxiv_html_link": "https://arxiv.org/html/2509.19804v1",
            "abstract": "This paper introduces DynaFlow, a novel framework that embeds a differentiable simulator directly into a flow matching model. By generating trajectories in the action space and mapping them to dynamically feasible state trajectories via the simulator, DynaFlow ensures all outputs are physically consistent by construction. This end-to-end differentiable architecture enables training on state-only demonstrations, allowing the model to simultaneously generate physically consistent state trajectories while inferring the underlying action sequences required to produce them.\nWe demonstrate the effectiveness of our approach through quantitative evaluations and showcase its real-world applicability by deploying the generated actions onto a physical Go1 quadruped robot.\nThe robot successfully reproduces diverse gait present in the dataset, executes long-horizon motions in open-loop control and translates infeasible kinematic demonstrations into dynamically executable, stylistic behaviors. These hardware experiments validate that DynaFlow produces deployable, highly effective motions on real-world hardware from state-only demonstrations, effectively bridging the gap between kinematic data and real-world execution.",
            "introduction": "Generative models, such as Diffusion Models and Flow Matching, have recently achieved unprecedented success across various domains, including image[1, 2, 3], audio[4], and text generation[5]. They have demonstrated a remarkable ability to learn intricate data distributions from large-scale datasets, producing highly natural and diverse outputs. Inspired by this success, these models are increasingly being recognized as powerful tools for generating complex motion trajectories in fields like robotics and computer graphics[6, 7, 8].\nIndeed, their application to kinematic motion generation has seen significant progress, largely driven by the increasing availability of state demonstration data from sources such as motion capture and raw video.\n\nHowever, directly applying these generative models to character animation and robot control presents significant challenges.\nA primary limitation is the lack of physical consistency.\nMost generative models learn a statistical approximation of the data distribution from a finite set of examples, rather than the underlying physical principles governing the data. Consequently, there is no guarantee that the generated outputs will adhere to intrinsic physical principles or dynamic constraints.\nThis means that when generating novel behaviors, the resulting motions can be either physically implausible, with artifacts like ground penetration, character floating, and foot sliding, or dynamically inconsistent, making them unsuitable to execute in the physical world.\nThe issue becomes particularly pronounced when models are trained on datasets with inherent physical inconsistencies, such as those sourced from motion capture or generated through kinematic retargeting.\n\nAnother major hurdle is the scarcity of action-labeled data. While state trajectories can often be obtained from motion capture or other sources, the corresponding action sequences (e.g., joint torques or motor commands) are rarely available and are costly to collect, typically being hardware-specific. Prior diffusion-based control approaches can be categorized into two principal ways. One option is to directly train a policy to predict actions from true action data, but this is rarely viable given the limited availability of true action data. Alternatively, a hierarchical framework can be employed, where the diffusion model generates desired state trajectories that are then executed by a low-level tracking controller. However, this hierarchical strategy has its own drawback: the tracking controller usually requires extensive fine-tuning for each new motion, and ensuring robustness against the distributional gap between planned trajectories and the controller remains a significant challenge [9].\n\nTo overcome these limitations, we propose DynaFlow, a novel framework that guarantees physically consistent motion generation by embedding dynamics directly into the generation process, while simultaneously inferring actions from state-only demonstration data.\nThe core idea of DynaFlow is to integrate a differentiable simulator at the output of the flow matching prediction module. This simulator layer acts as a mapping from the space of action trajectories to the space of dynamically feasible state trajectories, ensuring that the model’s output strictly adheres to the laws of physics by construction. Furthermore, its differentiable nature allows the entire model to be trained end-to-end. During this process, the model naturally discovers the action trajectory required to reconstruct a given state trajectory in a dynamically consistent manner, even without explicit action labels.\n\nWe conduct a series of experiments to validate the effectiveness of DynaFlow. Our quantitative analysis compares DynaFlow against several baselines, evaluating both dynamic feasibility and distributional similarity on two distinct datasets: a rich, strictly feasible dataset of quadruped locomotion and a challenging single-trajectory dataset from retargeted motion capture.\nOur results demonstrate that DynaFlow consistently generates strictly feasible trajectories, even when trained on a physically inconsistent dataset, while remaining competitive in distributional similarity. To showcase its real-world applicability, we deploy action trajectories generated by DynaFlow on a physical Go1 quadruped robot.\nThe robot successfully reproduces diverse gaits observed in the training data and executes long-horizon motions with high accuracy in challenging open-loop experiments, validating the precision and coherence of the generated actions.\nFurthermore, we demonstrate its ability to translate infeasible retargeted motions into dynamically executable and stylistic behaviors on hardware, bridging the gap between kinematic demonstration and real-world execution.\n\nThe main contributions of this paper are as follows:\n\nDynamics-embedded generative modeling:\nWe propose DynaFlow, a novel generative model that embeds a differentiable simulator into a flow matching framework to guarantee all generated trajectories are strictly dynamically consistent by construction.\n\nLearning from state-only data:\nOur method overcomes the common challenge of unavailable action data by leveraging analytical gradients, learning action directly from state-only demonstrations.\n\nReal-world validation:\nWe demonstrate the practical viability of our approach through successful hardware deployment of the generated motion on a physical quadruped robot, bridging the gap between motion generation and real-world execution.\n\n1. Dynamics-embedded generative modeling:\nWe propose DynaFlow, a novel generative model that embeds a differentiable simulator into a flow matching framework to guarantee all generated trajectories are strictly dynamically consistent by construction.\n\n2. Learning from state-only data:\nOur method overcomes the common challenge of unavailable action data by leveraging analytical gradients, learning action directly from state-only demonstrations.\n\n3. Real-world validation:\nWe demonstrate the practical viability of our approach through successful hardware deployment of the generated motion on a physical quadruped robot, bridging the gap between motion generation and real-world execution.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何确保生成的运动轨迹在物理上是一致的？  \n2. 如何从仅有的状态演示数据中推断出相应的动作序列？  \n3. 如何将生成的动作有效地应用于现实世界的机器人？  \n\n【用了什么创新方法】  \nDynaFlow框架通过将可微分模拟器嵌入流匹配模型中，确保生成的运动轨迹在物理上始终一致。该方法允许从状态演示数据中直接学习动作序列，克服了缺乏动作标注数据的挑战。通过一系列实验，DynaFlow在动态可行性和分布相似性方面表现出色，并成功在物理四足机器人上部署生成的动作，验证了其在真实环境中的有效性和精确性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training",
            "authors": "Rushuai Yang,Hangxing Wei,Ran Zhang,Zhiyuan Feng,Xiaoyu Chen,Tong Li,Chuheng Zhang,Li Zhao,Jiang Bian,Xiu Su,Yi Chen",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19752",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19752",
            "arxiv_html_link": "https://arxiv.org/html/2509.19752v1",
            "abstract": "Vision-language-action (VLA) models have shown strong generalization across tasks and embodiments; however, their reliance on large-scale human demonstrations limits their scalability owing to the cost and effort of manual data collection. Reinforcement learning (RL) offers a potential alternative to generate demonstrations autonomously, yet conventional RL algorithms often struggle on long-horizon manipulation tasks with sparse rewards. In this paper, we propose a modified diffusion policy optimization algorithm to generate high-quality and low-variance trajectories, which contributes to a diffusion RL-powered VLA training pipeline. Our algorithm benefits from not only the high expressiveness of diffusion models to explore complex and diverse behaviors but also the implicit regularization of the iterative denoising process to yield smooth and consistent demonstrations. We evaluate our approach on the LIBERO benchmark, which includes 130 long-horizon manipulation tasks, and show that the generated trajectories are smoother and more consistent than both human demonstrations and those from standard Gaussian RL policies. Further, training a VLA model exclusively on the diffusion RL-generated data achieves an average success rate of 81.9%, which outperforms the model trained on human data by +5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight our diffusion RL as an effective alternative for generating abundant, high-quality, and low-variance demonstrations for VLA models.",
            "introduction": "Vision-language-action (VLA) is a promising model toward general-purpose robots capable of generalizing across a wide array of manipulation tasks [1, 2, 3]. However, this paradigm is critically dependent on massive datasets of human demonstrations, such as the Open X-Embodiment dataset [4]. The process of collecting this data via manual teleoperation is notoriously expensive and labor-intensive. The reliance on manual data collection fundamentally caps the scalability of VLA models, presenting a major bottleneck to further progress.\n\nReinforcement learning (RL) has emerged as a powerful paradigm for enabling robots to acquire sophisticated physical skills directly through environmental interaction.\nThe fundamental strength of reinforcement learning stems from its trial-and-error process: by optimizing for a reward signal, an agent can autonomously discover highly effective and efficient strategies that often surpass what can be learned by simply mimicking human demonstrations.\nHowever, a significant limitation of this approach is that the resulting policies are often highly specialized.\nA policy trained to excel under one specific set of conditions typically struggles to adapt or generalize its skills when faced with new task variations or different environmental setups [5].\nHowever, making a general RL algorithm effective enough to generate high-quality data across diverse, complex manipulation tasks is challenging.\nThe long-horizon, sparse-reward tasks prevalent in benchmarks like LIBERO [6] expose critical weaknesses of conventional RL algorithms, often leading to unstable learning process or high-variance and suboptimal trajectories [7].\n\nTo this end, we propose a general framework that utilizes a modified diffusion policy optimization algorithm for diffusion RL-powered data generation.\nWe find that diffusion offer a superior alternative for this problem.\nFirst, diffusion policy provide good expressiveness to fit complex expert distribution.\nCompared with Gaussian RL, diffusion-based RL provides more space for RL exploration when interacting with environment.\nSecond, the inherent structure of the iterative denoising process acts as a powerful implicit regularizer on the action space, The model is trained to predict the noise for the entire action chunk at every step of the denoising process. This forces the model to learn the underlying structure of smooth, physically plausible motions. A single, jerky movement in the final action would require a very specific and complex sequence of denoising steps, which is less likely to be learned than a smooth, coherent refinement process. This naturally encourages the generation of temporally smooth, low-variance motion. We further enhance this process with a stabilized fine-tuning regimen, incorporating modifications to the architecture and training strategies to ensure robust performance across the 130 challenging tasks in LIBERO. This property allows our RL agent to explore more effectively and converge to near-optimal and low-variance policies.\n\nOur experiments yield a clear message:\nA VLA model trained exclusively on our RL-generated data consistently and significantly surpasses the ones trained on human data and Gaussian RL, both on in-distribution tasks and in challenging OOD generalization. A quantitative analysis reveals the mechanism behind this success: Our generated trajectories are smoother and less variable, providing a more stable learning signal for VLA training.\nOur contributions are threefold:\n\nA diffusion RL-powered VLA training pipeline for autonomously generating high-quality and low-variance data for VLA training, including validated effective modifications on the model architecture and training strategies.\n\nCompelling empirical evidence on the 130 complex manipulation tasks of the LIBERO benchmark shows that our synthetic data provides superior training signal to human demonstrations, significantly improving both the in-distribution success rates and out-of-distribution generalization of VLA models.\n\nAn in-depth quantitative analysis that relates trajectory-level properties (e.g., efficiency, smoothness, and consistency) with the performance of fine-tuned VLA, providing a clear explanation for why optimized data is more effective.\n\n1. A diffusion RL-powered VLA training pipeline for autonomously generating high-quality and low-variance data for VLA training, including validated effective modifications on the model architecture and training strategies.\n\n2. Compelling empirical evidence on the 130 complex manipulation tasks of the LIBERO benchmark shows that our synthetic data provides superior training signal to human demonstrations, significantly improving both the in-distribution success rates and out-of-distribution generalization of VLA models.\n\n3. An in-depth quantitative analysis that relates trajectory-level properties (e.g., efficiency, smoothness, and consistency) with the performance of fine-tuned VLA, providing a clear explanation for why optimized data is more effective.",
            "llm_summary": "【关注的是什么问题】  \n1. VLA模型依赖于大量人工示范数据，限制了其可扩展性。  \n2. 传统的强化学习算法在长时间跨度的稀疏奖励任务中表现不佳。  \n3. 如何生成高质量、低方差的演示数据以提高VLA训练效果。  \n\n【用了什么创新方法】  \n提出了一种改进的扩散策略优化算法，以生成高质量、低方差的轨迹，构建了一个基于扩散强化学习的VLA训练管道。该算法利用扩散模型的高表达能力探索复杂行为，并通过迭代去噪过程的隐式正则化生成平滑、一致的演示。实验结果表明，使用扩散RL生成的数据训练的VLA模型在130个长时间跨度的操作任务中，成功率达到81.9%，超越了基于人工数据和标准高斯RL生成数据的模型，展示了扩散RL在生成丰富、高质量演示数据方面的有效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Trajectory Planning Using Safe Ellipsoidal Corridors as Projections of Orthogonal Trust Regions",
            "authors": "Akshay Jaitly,Jon Arrizabalaga,Guanrui Li",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19734",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19734",
            "arxiv_html_link": "https://arxiv.org/html/2509.19734v1",
            "abstract": "Planning collision free trajectories in complex environments remains a core challenge in robotics. Existing corridor based planners which rely on decomposition of the free space into collision free subsets scale poorly with environmental complexity and require explicit allocations of time windows to trajectory segments. We introduce a new trajectory parameterization that represents trajectories in a nonconvex collision free corridor as being in a convex cartesian product of balls. This parameterization allows us to decouple problem size from geometric complexity of the solution and naturally avoids explicit time allocation by allowing trajectories to evolve continuously inside ellipsoidal corridors. Building on this representation, we formulate the Orthogonal Trust Region Problem (Orth-TRP), a specialized convex program with separable block constraints, and develop a solver that exploits this parallel structure and the unique structure of each parallel subproblem for efficient optimization. Experiments on a quadrotor trajectory planning benchmark show that our approach produces smoother trajectories and lower runtimes than state-of-the-art corridor based planners, especially in highly complicated environments.",
            "introduction": "Collision free trajectory optimization is a core challenge in robotics, useful in mobile robot navigation and mobile manipulation [1, 2, 3]. By mathematically defining sets of feasible trajectories that a system can undergo, and assigning a cost to each element of the set, trajectory optimization can be posed as a mathematical programming problem. Several methods exist to characterize these sets to facilitate efficient searches for optimal feasible trajectories.\n\nConventional sampling based planners [4, 5, 6] build tree or graph like structures in the configuration space, with nodes corresponding to collision free states and edges representing locally feasible motions. Paths through these structures define potential trajectories, which can then be searched efficiently using graph search or shortest path algorithms. On the other hand, some optimization based approaches [7, 8, 9, 10, 11, 12] impose constraints on robot’s states (enforcing that a barrier between collision objects is respected) to define sets of feasible solutions, then minimize a cost function within this nonconvex feasible set. These enable powerful nonconvex optimization solvers for trajectory optimization. In more complicated scenarios, with larger dimensional spaces or with numerous obstacles, the above methods inevitably face the curse of dimensionality.\n\nRecent work has introduced convex approximations of free space through “safe corridors”, using either polytopic or ellipsoidal regions [13, 14, 15, 16]. In these approaches, a trajectory is divided into segments, and each segment is constrained to lie within a corresponding convex subset of the obstacle free space at a specific point in time. By enforcing convex constraints on the robot’s configuration, these methods convert a difficult global planning problem into convex optimization, which can be solved using efficient convex programming techniques [1, 15, 16].\n\nHowever, a key challenge in these methods is allocating time across the convex regions. Each region requires a specified time interval, and if this interval is too short, the system must accelerate sharply to meet continuity constraints, while overly long intervals produce inefficient motions. As the number of convex subsets increases, these timing decisions become increasingly difficult to manage, often introducing sensitivity and limiting the scalability of convex corridor methods.\n\n[17] introduced Differentiable Collision Free Parametric Corridors which model free space as a smooth nonconvex corridor, made of a continuously deforming convex set rather than discrete segments, offering a more unified description of collision free regions. Building on this idea, we view these nonconvex corridors as time-varying projections of orthogonal trust regions in a higher dimensional parameter space. By lifting trajectories into a space where feasible solutions form a cartesian product of high dimensional balls, each point in this lifted space naturally corresponds to a collision free trajectory within the corridor. This decouples problem size from environmental complexity while allowing representations of paths in the safe corridor as points in the lifted space.\n\nThis representation results in a favorable convex feasible set for trajectory optimization. We pose the resulting problem as the Orthogonal Trust Region Problem (Orth-TRP), which can be expressed as a collection of interconnected trust region subproblems (TRPs). Because each block of variables has its own seperable constraint, the Orth-TRP naturally supports a parallelizable algorithm, where each block can be updated efficiently using trust region steps that resemble simple one dimensional line searches.\n\nIn summary, our contributions are:\n\nWe develop a convex representation of sets of trajectories within a nonconvex set of configurations using a product of multiple high dimensional balls, decoupling the solution complexity from the size of the optimization problem.\n\nWe develop a convex representation of sets of trajectories within a nonconvex set of configurations using a product of multiple high dimensional balls, decoupling the solution complexity from the size of the optimization problem.\n\nWe create a solver that exploits both, the parallelizable structure and the unique Trust-Region-like structure of our resulting problem to solve quadratically constrained quadratic optimization problems where constraints are enforced on separable variables.\n\nTaken together, these components create a scalable and geometrically intuitive framework for collision-free trajectory optimization in complex environments. Our experiments show that this approach consistently outperforms state-of-the-art implementations on challenging benchmarks. In direct comparisons with the method of [15], even when using a powerful solver like OSQP [18], our method solves long-horizon planning problems faster while producing comparable or smoother results. To the best of our knowledge, this is the first continuous parameterization of trajectories that enables optimization within a collision-free corridor where problem size and runtime are agnostic to horizon length and solution complexity. We achieve this by combining a new convex programming formulation with a solver that fully exploits the problem’s separable trust-region structure, eliminating the scaling and time-allocation issues inherent to existing corridor-based approaches and enabling both speed and robustness at scale.\n\n1. We develop a convex representation of sets of trajectories within a nonconvex set of configurations using a product of multiple high dimensional balls, decoupling the solution complexity from the size of the optimization problem.\n\n2. We create a solver that exploits both, the parallelizable structure and the unique Trust-Region-like structure of our resulting problem to solve quadratically constrained quadratic optimization problems where constraints are enforced on separable variables.",
            "llm_summary": "【关注的是什么问题】  \n1. 复杂环境中的无碰撞轨迹规划仍然是机器人技术中的核心挑战。  \n2. 现有基于走廊的规划方法在环境复杂性增加时表现不佳，且需要显式的时间分配。  \n3. 如何有效地优化非凸走廊中的轨迹，同时避免时间分配问题。  \n\n【用了什么创新方法】  \n本研究提出了一种新的轨迹参数化方法，通过将轨迹表示为高维球体的笛卡尔积，形成非凸走廊的凸表示。这种方法使得问题规模与解决方案的几何复杂性解耦，并允许轨迹在椭圆走廊内连续演变。我们将其形式化为正交信任区域问题（Orth-TRP），并开发了一种利用该问题并行结构的求解器。实验结果表明，该方法在复杂环境中产生了更平滑的轨迹，并且运行时间低于现有的走廊规划方法，尤其在高复杂度环境中表现优越。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Simultaneous estimation of contact position and tool shape with high-dimensional parameters using force measurements and particle filtering",
            "authors": "Kyo Kutsuzawa,Mitsuhiro Hayashibe",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted to The International Journal of Robotics Research (IJRR)",
            "pdf_link": "https://arxiv.org/pdf/2509.19732",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19732",
            "arxiv_html_link": "https://arxiv.org/html/2509.19732v1",
            "abstract": "Estimating the contact state between a grasped tool and the environment is essential for performing contact tasks such as assembly and object manipulation.\nForce signals are valuable for estimating the contact state, as they can be utilized even when the contact location is obscured by the tool.\nPrevious studies proposed methods for estimating contact positions using force/torque signals; however, most methods require the geometry of the tool surface to be known.\nAlthough several studies have proposed methods that do not require the tool shape, these methods require considerable time for estimation or are limited to tools with low-dimensional shape parameters.\nHere, we propose a method for simultaneously estimating the contact position and tool shape, where the tool shape is represented by a grid, which is high-dimensional (more than 1000 dimensional).\nThe proposed method uses a particle filter in which each particle has individual tool shape parameters, thereby to avoid directly handling a high-dimensional parameter space.\nThe proposed method is evaluated through simulations and experiments using tools with curved shapes on a plane.\nConsequently, the proposed method can estimate the shape of the tool simultaneously with the contact positions, making the contact-position estimation more accurate.",
            "introduction": "When robots manipulate objects or use them as tools, they often need to recognize the contact states between the grasped objects/tools and the environment.\nFor instance, when inserting a key into a keyhole, the robot must know the contact position and conditions of the key and lock.\nIn addition, when cutting the bone-in meat with a knife, the robot must detect where the knife contacts with the bone.\nIn such situations, robots need to detect contact states indirectly because tools usually have no sensors.\nMoreover, because tools hide the contact location, robots must estimate the contact information from force signals instead of vision.\n\nConventional methods for contact-position estimation from force signals require the shape and position of the tools.\nA technique for contact-position estimation (Salisbury, 1984) often requires the shape of the tool surface to be known.\nHowever, shape measurements using cameras generally exhibit large errors in the depth direction and are sensitive to occlusion, reflection, and transparency.\nAlthough there are several methods for contact-position estimation without shape information (Tsuji et al., 2017; Koike et al., 2017), the estimation is slow, and these methods require that contact force constantly fluctuates during estimation.\nAs those drawbacks are unavoidable unless using shape information, it is beneficial to estimate tool shape for estimating the contact position.\nAdditionally, the shape of the object/tool is necessary to assemble tasks and plan a control strategy (von Drigalski et al., 2020).\n\nFor tools made of transparent or reflective materials, it would be helpful to be able to estimate the tool shape from force signals instead of vision.\nRecently, a method that simultaneously estimates the contact position and tool shape from force signals was proposed (Kutsuzawa et al., 2020).\nThis method gradually estimates the contact position and tool shape under uncertainty using an unscented particle filter (UPF) (van der Merwe et al., 2000a, b).\nHowever, it requires the tool shape to be expressed using a small number of parameters.\nIt is practically impossible to apply that method to general shapes because the dimensionality of the tool-shape parameters becomes high, which requires an exponential number of particles for a reliable estimation.\nThere is another method that can detect the contact position while estimating the tool shape of voxels from force measurements (Bimbo et al., 2022), but this method requires the geometry of the environment being static.\n\nHere, we propose a method to estimate the tool shape with a large number of parameters from force signals while simultaneously estimating the contact position, based on the Rao–Blackwellized particle filter (RBPF) (Murphy, 1999) used in SLAM (Murphy, 1999; Grisetti et al., 2005, 2007).\nThe conventional method (Kutsuzawa et al., 2020) is affected by the curse of dimensionality owing to the application of a particle filter to high-dimensional parameters.\nBy contrast, the proposed method avoids this issue by associating shape information with individual particles rather than scattering particles into the shape-parameter space.\nThus, the proposed method enables the tool shape to be expressed using a high-dimensional grid representation (voxels or pixels).\nIn addition, in contrast with Bimbo et al. (2022), the proposed method does not need any assumption of environment geometry.\nTherefore, the proposed method is available even for the contact object in the environment being deformable and movable, e.g., a human touching the tool.\nIn this study, we address the simple case of a curved object on a plane.\nAlthough simple, this setup can demonstrate the effectiveness of the proposed method because the challenge of this study is the high-dimensional shape parameter.\nThe contributions of this study are listed as follows:\n\nThis study proposes a force-signal-based estimation method for tool shapes represented by high-dimensional variables (grid representation) without any assumptions of environment geometry.\n\nWe formulated a probabilistic model for contact-position and tool-shape estimation from force signals.\n\nThis study also proposes a method for updating the tool-shape parameters represented by a grid using the estimated contact position and measurements.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何同时估计接触位置和高维工具形状。  \n2. 现有方法在高维参数空间处理时的效率问题。  \n3. 在缺乏工具几何信息的情况下进行接触状态估计的挑战。  \n\n【用了什么创新方法】  \n提出了一种基于力信号的估计方法，通过粒子滤波（RBPF）同时估计接触位置和工具形状，工具形状用高维网格表示。该方法避免了直接处理高维参数空间的问题，通过将形状信息与个别粒子关联，显著提高了接触位置的估计准确性。实验结果表明，该方法在处理曲面工具时表现出色，能够在不依赖环境几何假设的情况下进行有效估计。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Towards Autonomous Robotic Electrosurgery via Thermal Imaging",
            "authors": "Naveed D. Riaziat,Joseph Chen,Axel Krieger,Jeremy D. Brown",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted for publication in the proceedings of the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)",
            "pdf_link": "https://arxiv.org/pdf/2509.19725",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19725",
            "arxiv_html_link": "https://arxiv.org/html/2509.19725v1",
            "abstract": "Electrosurgery is a surgical technique that can improve tissue cutting by reducing cutting force and bleeding. However, electrosurgery adds a risk of thermal injury to surrounding tissue. Expert surgeons estimate desirable cutting velocities based on experience but have no quantifiable reference to indicate if a particular velocity is optimal. Furthermore, prior demonstrations of autonomous electrosurgery have primarily used constant tool velocity, which is not robust to changes in electrosurgical tissue characteristics, power settings, or tool type. Thermal imaging feedback provides information that can be used to reduce thermal injury while balancing cutting force by controlling tool velocity. We introduce Thermography for Electrosurgical Rate Modulation via Optimization (ThERMO) to autonomously reduce thermal injury while balancing cutting force by intelligently controlling tool velocity. We demonstrate ThERMO in tissue phantoms and compare its performance to the constant velocity approach. Overall, ThERMO improves cut success rate by a factor of three and can reduce peak cutting force by a factor of two. ThERMO responds to varying environmental disturbances, reduces damage to tissue, and completes cutting tasks that would otherwise result in catastrophic failure for the constant velocity approach.",
            "introduction": "Electrosurgery is a surgical technique for cutting tissue using energy from a high-frequency voltage source. Eight in ten surgical procedures use electrosurgery, often to remove diseased tissue [1]. Monopolar electrosurgery uses a grounding pad to dissipate current through the body from a tool, directly heating the local tissue. However, electrosurgery can significantly damage nearby healthy tissue as well. Excess thermal damage can adversely affect surgery outcomes. Surgeons try to avoid generating thermal damage by modulating cut speed while ensuring cut accuracy.\n\nWhen using electrosurgery to remove tissue, surgeons aim to spare as much healthy tissue as possible. Since signs of excess thermal energy are often not obvious until irreversible damage has been done, surgeons minimize dwell time by moving faster, as fast motions reduce the heat deposited in a specific area. However, fast motion comes at the cost of decreased cut accuracy and increased cutting force, which can also damage tissue. The ideal electrosurgical cut technique creates a small denatured tissue zone ahead of the tool tip that reduces mechanical stiffness and, thus, the required cut force. However, as the tool speed increases, this denatured tissue zone becomes smaller and eventually disappears, allowing tissue to accumulate on the tool tip and increasing the force needed to cut. This results in increased tissue damage, bulk tissue deformation, and lowered cut quality. Therefore, the optimal cutting velocity should maintain a balanced speed to reduce thermal damage and minimize excess force on the tissue.\n\nWhile there have been no prior demonstrations of techniques for optimizing cut velocity in electrosurgery, there have been recent investigations into sensor-based automation for controlling thermal damage. Bao and Mazumder showed that thermal imaging can measure the denaturation zone and control it to a specific size using a novel computer-controllable electro electrosurgical unit (ESU) [2]. This method, however, contrasts with the fixed power level typically used by surgeons. El-Kebir et al. used thermal sensing and data-driven models to control the thermal damage boundary along a cut by pausing at discrete decision points [3]. Unfortunately, neither approach simultaneously considers cut deformation and thermal damage to optimize the cut velocity.\n\nThe same velocity modulation problem exists in autonomous robotic electrosurgery, which has had recent success in medical robotics research. While these autonomous approaches promise improved cut accuracy and decreased surgeon error, they have largely avoided real-time velocity optimization. Opfermann et al.  [4] demonstrated a visual servoing approach for electrosurgical cutting on the Smart Tissue Autonomous Robot (STAR) [5]. Saeidi et al. similarly showed that STAR could perform tissue-cutting tasks using predetermined cut depth, power, and speed [6]. Ge et al. demonstrated tumor detection and excision using a suction gripper [7]. Each approach relies on a predetermined cut velocity, chosen based on clinical observation or simulation. Researchers have generally opted to move slowly to avoid excess tissue deformation. However, these approaches fail to account for the excess heat damage caused by slow cutting velocities.\n\nHere, we introduce ThERMO. ThERMO uses thermal imaging to determine the optimal cut velocity by 1) dually identifying thermal and mechanical parameters online via a “Truncated Unscented Kalman Filter” (TUKF) and 2) minimizing a parametrized cost function of denaturation width and cutting force. Thermal measurements inform the adaptation of thermal and mechanical parameters, which are applied to generic thermal and mechanical models to maximize cut accuracy and minimize thermal damage with respect to cut velocity. Collectively, we contribute first steps towards 1) infrared (IR)-camera-based tissue denaturation and force measurement method with accompanying validation, 2) a combined adaptive identification and optimization-based approach to control velocity based on thermal spread and cutting force, and 3) rigorous comparisons to the current cutting-edge autonomous electrosurgery approach.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在电外科手术中优化切割速度以减少热损伤？  \n2. 现有电外科技术未能实时优化切割速度，导致组织损伤和切割质量下降。  \n3. 传统的电外科方法依赖于固定的切割速度，缺乏对环境变化的适应性。  \n\n【用了什么创新方法】  \n本研究提出了“热成像电外科速率调制优化”（ThERMO）方法，通过热成像反馈实时调整工具速度，以减少热损伤并平衡切割力。ThERMO使用“截断无味卡尔曼滤波器”（TUKF）在线识别热和机械参数，并最小化与组织变性宽度和切割力相关的参数化成本函数。实验结果表明，ThERMO在组织模型中将切割成功率提高了三倍，并将峰值切割力降低了一半，能够有效应对环境干扰，减少组织损伤，完成传统固定速度方法无法实现的切割任务。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies",
            "authors": "Liquan Wang,Jiangjie Bian,Eric Heiden,Animesh Garg",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19712",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19712",
            "arxiv_html_link": "https://arxiv.org/html/2509.19712v1",
            "abstract": "Robotic manipulation tasks involving cutting deformable objects remain challenging due to complex topological behaviors, difficulties in perceiving dense object states, and the lack of efficient evaluation methods for cutting outcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for multi-step robotic cutting tasks, that integrates cutting environment and generalized policy learning. TopoCut is built upon three core components: (1) We introduce a high-fidelity simulation environment based on a particle-based elastoplastic solver with compliant von Mises constitutive models, augmented by a novel damage-driven topology discovery mechanism that enables accurate tracking of multiple cutting pieces.\n(2) We develop a comprehensive reward design that integrates the topology discovery with a pose-invariant spectral reward model based on Laplace–Beltrami eigenanalysis, facilitating consistent and robust assessment of cutting quality. (3) We propose an integrated policy learning pipeline, where a dynamics-informed perception module predicts topological evolution and produces particle-wise, topology-aware embeddings to support PDDP—Particle-based Score-Entropy Discrete Diffusion Policy—for goal-conditioned policy learning.\nExtensive experiments demonstrate that TopoCut supports trajectory generation, scalable learning, precise evaluation, and strong generalization across diverse object geometries, scales, poses, and cutting goals. Project page: https://topocut.github.io/.",
            "introduction": "Robotic manipulation involving the cutting of deformable objects plays a critical role across diverse domains such as food processing, medical surgery, and manufacturing. Many real-world tasks require not just a single cut, but a sequence of cutting actions to segment objects into complex or structured shapes. From slicing ingredients into uniform pieces in culinary automation, to performing multi-incision procedures in robotic surgery, and executing multi-pass segmentation in industrial workflows, multi-step cutting is essential for achieving fine-grained precision. The ability to reliably plan and execute these sequential cutting operations significantly enhances efficiency, safety, and quality in autonomous systems.\n\nDespite recent progress in robotic cutting of single-material deformable objects with fixed trajectories [1, 2, 3], goal-conditioned multi-step cutting of complex deformable geometries remains a major challenge. Deformable objects often fail to separate cleanly after each cut, making outcome evaluation ambiguous [1]. Existing evaluation metrics are sensitive to pose variations and typically require explicit alignment [3]. Furthermore, dense topological changes resulting from sequential cuts are difficult to perceive from sparse or noisy observations [4, 3], hindering the effectiveness of policy learning in such settings.\n\nTo address these challenges, we introduce TopoCut, a unified framework for multi-step robotic cutting that combines high-fidelity simulation, robust evaluation, and goal-conditioned policy learning. At its core, TopoCut features a particle-based elastoplastic simulator equipped with a novel damage-driven topology discovery mechanism that enables precise tracking of multiple cutting-induced topological changes. We further design a pose-invariant spectral reward based on Laplace–Beltrami eigenanalysis to evaluate cutting outcomes consistently across varying object geometries and poses. Finally, we propose a learning pipeline that leverages a dynamics-informed perception module to produce topology-aware, particle-wise embeddings—explicitly designed to operate on sparse visual input, making it suitable for real-world robotic settings—and supports PDDP, a discrete diffusion policy model for scalable and generalizable multi-step cutting.\n\nOur contributions are organized into three core components:\n\nHigh-fidelity Simulation and Topology Discovery: We develop a robust simulation environment utilizing a novel particle-based elastoplastic solver with compliant von Mises constitutive models, coupled with an advanced particle-based topology discovery method to precisely capture and track topological changes during cutting.\n\nPose-invariant Spectral Reward: We introduce a novel reward formulation integrating the real-time topology discovery with a spectral reward function based on Laplace–Beltrami eigenanalysis, enabling consistent, pose-invariant evaluation of cutting outcomes across arbitrary object poses.\n\nDynamics-informed Policy Learning: We propose a goal-conditioned policy learning framework that employs dynamics-informed perception modules to predict topology evolution and generate particle-level, topology-aware embeddings. These embeddings support conditional score-based discrete diffusion models, enhancing the robustness and generalizability of the learned cutting strategies.\n\n1. High-fidelity Simulation and Topology Discovery: We develop a robust simulation environment utilizing a novel particle-based elastoplastic solver with compliant von Mises constitutive models, coupled with an advanced particle-based topology discovery method to precisely capture and track topological changes during cutting.\n\n2. Pose-invariant Spectral Reward: We introduce a novel reward formulation integrating the real-time topology discovery with a spectral reward function based on Laplace–Beltrami eigenanalysis, enabling consistent, pose-invariant evaluation of cutting outcomes across arbitrary object poses.\n\n3. Dynamics-informed Policy Learning: We propose a goal-conditioned policy learning framework that employs dynamics-informed perception modules to predict topology evolution and generate particle-level, topology-aware embeddings. These embeddings support conditional score-based discrete diffusion models, enhancing the robustness and generalizability of the learned cutting strategies.",
            "llm_summary": "【关注的是什么问题】  \n1. 多步切割任务中的复杂拓扑行为如何有效处理。  \n2. 如何在稀疏观测下准确评估切割结果。  \n3. 现有的切割策略学习方法在复杂可变形几何体上的局限性。  \n\n【用了什么创新方法】  \nTopoCut框架结合了高保真模拟、稳健评估和目标条件策略学习。首先，开发了一种基于粒子弹塑性求解器的模拟环境，配合损伤驱动的拓扑发现机制，精确跟踪切割引起的拓扑变化。其次，设计了一种基于拉普拉斯-贝尔特拉米特征值分析的姿态不变谱奖励，确保在不同物体几何和姿态下的一致评估。最后，提出了一种动态感知模块，生成粒子级拓扑感知嵌入，支持条件分数基础离散扩散策略（PDDP），增强了切割策略的稳健性和泛化能力。实验结果表明，TopoCut在轨迹生成、可扩展学习、精确评估和强泛化能力方面表现出色。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks",
            "authors": "Noah Geiger,Tamim Asfour,Neville Hogan,Johannes Lachner",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19696",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19696",
            "arxiv_html_link": "https://arxiv.org/html/2509.19696v1",
            "abstract": "Learning methods excel at motion generation in the information domain but are not primarily designed for physical interaction in the energy domain. Impedance Control shapes physical interaction but requires task-aware tuning by selecting feasible impedance parameters. We present Diffusion-Based Impedance Learning, a framework that combines both domains. A Transformer-based Diffusion Model with cross-attention to external wrenches reconstructs a simulated Zero-Force Trajectory (sZFT). This captures both translational and rotational task-space behavior. For rotations, we introduce a novel SLERP-based quaternion noise scheduler that ensures geometric consistency. The reconstructed sZFT is then passed to an energy-based estimator that updates stiffness and damping parameters. A directional rule is applied that reduces impedance along non-task axes while preserving rigidity along task directions.\nTraining data were collected for a parkour scenario and robotic-assisted therapy tasks using teleoperation with Apple Vision Pro. With only tens of thousands of samples, the model achieved sub-millimeter positional accuracy and sub-degree rotational accuracy. Its compact model size enabled real-time torque control and autonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller achieved smooth parkour traversal within force and velocity limits and 30/30 success rates for cylindrical, square, and star peg insertions without any peg-specific demonstrations in the training data set.\nAll code for the Transformer-based Diffusion Model, the robot controller, and the Apple Vision Pro telemanipulation framework is publicly available. These results mark an important step towards Physical AI, fusing model-based control for physical interaction with learning-based methods for trajectory generation.",
            "introduction": "Robotic behavior emerges at the interface of two fundamentally different domains. Motion planning belongs to the information domain, where learning-based methods have recently shown remarkable progress. In contrast, physical interaction is governed by the energy domain, and model-based Impedance Control has been widely adopted to guarantee stability [1] and safety [2]. Robots that manipulate in unstructured environments must bridge these domains: motions must be inferred as information, yet executed through stable, energy-consistent exchange with the environment [3]. This requirement is critical in assembly, rehabilitation, and other contact-rich tasks where visual feedback is limited and success depends on regulating interaction rather than simply following a path.\n\nWhile Impedance Control provides the framework for shaping robot–environment interaction [4], its performance critically depends on selecting appropriate stiffness and damping parameters (which may need to be adapted online) [5]. Too much stiffness can lead to jamming, too little can prevent task execution.\n\nRecent advances in contact-rich manipulation can be divided into two main strands. Model-based approaches incorporate explicit contact dynamics like friction cones, complementarity constraints, and mixed-integer formulations [6, 7, 8, 9, 10, 11, 12, 13, 14]. These methods produce behavior consistent with physical contact but face challenges: high computational cost, parameter sensitivity, and limited robustness in unstructured settings. Learning-based approaches, in contrast, operate in the information domain. Reinforcement learning and sampling-based optimization [15, 16, 17, 18], along with more recent generative models such as diffusion policies [19], flow-matching policies [20, 21], and Transformer-based policy models [22, 23], excel at motion generation across tasks and embodiments [24]. However, execution in these frameworks often relies on velocity-based or fixed-gain PD controllers. While fixed-gain PD controllers can avoid hardware damage, apparent compliance is only a byproduct of low control gains. In unstructured environments, where vision may be unreliable or occluded, explicit impedance regulation is essential to control interaction.\n\nThe Adaptive Compliance Policy of [25] is a first step toward combining generative models with compliance adaptation, but it is formulated within an admittance-control framework. Admittance control is known to struggle with transitions into and out of contact [4, 26], making it unsuitable for many assembly tasks such as peg-in-hole insertion. To our knowledge, no approach has successfully combined learning-based trajectory generation (information domain) with impedance adaptation (energy domain).\n\nThis paper addresses that gap. We present Diffusion-Based Impedance Learning, a framework that unifies generative modeling with energy-consistent Impedance Control. A conditional Transformer-based Diffusion Model reconstructs simulated Zero-Force Trajectories111The Zero-Force Trajectory (ZFT), introduced by Hogan [5], refers to the commanded equilibrium motion in the unconstrained case: the unique end-effector pose at which the interaction wrench vanishes. from contact-perturbed displacement and external wrench signals. The sZFT serves as a reconstructed equilibrium used to modulate task-space impedance. A directional adaptation scheme adapts stiffness primarily along non-task-relevant directions and preserves rigidity where needed for execution. In this way, our approach combines the strengths of learning-based methods (motion generation in the information domain) with those of model-based methods (impedance regulation in the energy domain).\n\nWe validate the framework on a KUKA LBR iiwa in two contact-rich scenarios: parkour-style obstacle traversal and multi-geometry peg-in-hole insertion. Training data were collected through telemanipulation, using Apple Vision Pro (AVP) equipped with a markerless pose tracking framework [27] integrated into the robot controller. Both tasks highlight the shortcomings of fixed impedance and simple adaptation schemes, which either jam at obstacles or fail to achieve insertion. In contrast, Diffusion-Based Impedance Learning achieves consistent success. The outcomes of Table I preview the central result of this work: bridging the information and energy domains is key to robust manipulation in unstructured environments.\n\nAll code for the Transformer-based Diffusion Model, the robot controller,\nand the Apple Vision Pro telemanipulation framework is available on our\nGitHub repository222https://github.com/StrokeAIRobotics/DiffusionBasedImpedanceLearning. Demonstration videos of all\nexperiments can be found on the project website333https://strokeairobotics.github.io/DiffusionBasedImpedanceLearning.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效结合学习方法与阻抗控制以实现物理交互？  \n2. 如何在接触丰富的任务中实现稳定的运动生成和阻抗调节？  \n3. 如何在不确定环境中提高机器人操作的成功率和鲁棒性？  \n\n【用了什么创新方法】  \n提出了一种Diffusion-Based Impedance Learning框架，结合了生成建模与能量一致的阻抗控制。通过条件Transformer-based Diffusion Model重建模拟的零力轨迹（sZFT），并采用方向适应方案调节任务空间的阻抗。该方法在KUKA LBR iiwa机器人上进行验证，展示了在公园风格障碍物穿越和多几何体插入任务中的成功率，达到了亚毫米的位置精度和亚度的旋转精度。结果表明，信息域与能量域的结合是实现不确定环境中稳健操作的关键。\n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Formal Safety Verification and Refinement for Generative Motion Planners via Certified Local Stabilization",
            "authors": "Devesh Nath,Haoran Yin,Glen Chou",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG); Systems and Control (eess.SY); Optimization and Control (math.OC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19688",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19688",
            "arxiv_html_link": "https://arxiv.org/html/2509.19688v1",
            "abstract": "We present a method for formal safety verification of learning-based generative motion planners. Generative motion planners (GMPs) offer advantages over traditional planners, but verifying the safety and dynamic feasibility of their outputs is difficult since neural network verification (NNV) tools scale only to a few hundred neurons, while GMPs often contain millions. To preserve GMP expressiveness while enabling verification, our key insight is to imitate the GMP by stabilizing references sampled from the GMP with a small neural tracking controller and then applying NNV to the closed-loop dynamics. This yields reachable sets that rigorously certify closed-loop safety, while the controller enforces dynamic feasibility. Building on this, we construct a library of verified GMP references and deploy them online in a way that imitates the original GMP distribution whenever it is safe to do so, improving safety without retraining. We evaluate across diverse planners, including diffusion, flow matching, and vision-language models, improving safety in simulation (on ground robots and quadcopters) and on hardware (differential-drive robot).",
            "introduction": "Motion planning has been transformed by generative models like diffusion and conditional flow matching (CFM) [1, 2], which learn multimodal trajectory distributions and enable generative motion planners (GMPs) that produce diverse plans from inputs like language or images [3, 4, 5, 6].\nHowever, ensuring that GMP-generated trajectories satisfy safety and dynamic feasibility is difficult: GMPs often contain millions of parameters, making neural network verification (NNV) [7] intractable, limiting their use in safety-critical settings [6]. NNV provides hard guarantees via set-based reachability but only scales to controllers with a few hundred neurons [8, 9, 10, 11]. More scalable statistical methods [12, 13, 3, 14] yield weaker probabilistic guarantees or require prohibitive samples over long horizons. Thus, existing work trades off between expressive large models lacking hard guarantees and small verifiable models unable to capture complex behaviors.\n\nTo bridge this gap, we propose SaGe-MP (Safe Generative Motion Planning), a method that provides hard safety and dynamic feasibility guarantees for GMP-generated motion plans. Our key insight is that while NNV tools cannot directly certify the GMP, they can certify a small neural tracking controller that locally stabilizes the system around GMP-sampled references.\nReachability analysis of the resulting closed-loop system yields hard assurances of safety and dynamic feasibility over a continuum of inputs. Here, the GMP acts only as an open-loop plan generator, while verification is performed on the closed-loop dynamics induced by tracking a fixed GMP plan, resulting in a smaller computational graph that makes NNV tractable. By tracking GMP references under the true dynamics, the controller also projects potentially dynamically-infeasible GMP plans onto feasible trajectories. To preserve the original GMP behavior if possible, we develop a trajectory-library approach: multiple GMP references are sampled offline, certified as safely trackable via NNV, and deployed online in a way that mimics the potentially multimodal GMP output. In this sense, our method is a lightweight GMP refinement that enhances safety and dynamic feasibility without costly GMP retraining. Our contributions are:\n\nAn NNV-based method for formal safety verification of large GMPs that decouples trajectory generation from neural feedback loop verification, preserving planner expressiveness while providing hard safety guarantees.\n\nAn NNV-based method for formal safety verification of large GMPs that decouples trajectory generation from neural feedback loop verification, preserving planner expressiveness while providing hard safety guarantees.\n\nA method to certify a neural trajectory tracking controller that stabilizes dynamically-infeasible GMP references, producing safe, feasible trajectories.\n\nA trajectory-library method that stores certified-safe GMP plans and executes them online in a way that preserves the original behavior whenever safe. We prove sample complexity bounds for a target imitation error.\n\nExtensive simulation and real-world validation demonstrating safe stabilization of references from diverse generative models (diffusion, flow matching, VLMs, neural ODEs) on challenging nonlinear systems (e.g., 12D quadcopter, and learned NN dynamics).",
            "llm_summary": "【关注的是什么问题】  \n1. 如何确保生成运动规划器（GMP）生成的轨迹满足安全性和动态可行性？  \n2. 如何在不重新训练GMP的情况下提高其安全性？  \n\n【用了什么创新方法】  \n提出了一种名为SaGe-MP的方法，通过使用小型神经跟踪控制器对GMP生成的参考轨迹进行局部稳定化，从而实现对GMP的形式安全验证。该方法将轨迹生成与神经反馈回路验证解耦，允许使用神经网络验证（NNV）工具对闭环动态进行可行性和安全性分析。通过构建一个经过验证的轨迹库，能够在安全时在线执行这些轨迹，保持GMP的原始行为。该方法在多种生成模型上进行了广泛的仿真和实际验证，展示了在复杂非线性系统（如12维四旋翼和学习的神经网络动态）上的安全稳定化效果。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains",
            "authors": "Dongzhe Zheng,Wenjie Mei",
            "subjects": "Robotics (cs.RO); Dynamical Systems (math.DS)",
            "comment": "Accepted by NeurIPS 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.19672",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19672",
            "arxiv_html_link": "https://arxiv.org/html/2509.19672v1",
            "abstract": "Stochastic optimal control methods often struggle in complex non-convex landscapes, frequently becoming trapped in local optima due to their inability to learn from historical trajectory data. This paper introduces Memory-Augmented Potential Field Theory, a unified mathematical framework that integrates historical experience into stochastic optimal control. Our approach dynamically constructs memory-based potential fields that identify and encode key topological features of the state space, enabling controllers to automatically learn from past experiences and adapt their optimization strategy. We provide a theoretical analysis showing that memory-augmented potential fields possess non-convex escape properties, asymptotic convergence characteristics, and computational efficiency. We implement this theoretical framework in a Memory-Augmented Model Predictive Path Integral (MPPI) controller that demonstrates significantly improved performance in challenging non-convex environments. The framework represents a generalizable approach to experience-based learning within control systems (especially robotic dynamics), enhancing their ability to navigate complex state spaces without requiring specialized domain knowledge or extensive offline training.",
            "introduction": "Stochastic optimal control has proven highly effective for handling nonlinear systems and uncertain environments, finding widespread application in robotics, reinforcement learning, and complex system control. Among these approaches, Model Predictive Path Integral (MPPI) control stands out for its ability to handle continuous state-action spaces through stochastic sampling and exponentially weighted averaging. However, these methods still face significant theoretical and practical challenges when confronting highly non-convex value function landscapes.\n\nFrom an optimization perspective, stochastic optimal control problems can be viewed as trajectory optimization over a value function landscape. When this landscape exhibits complex non-convex characteristics, optimization processes may become trapped in local optima, unable to reach global solutions. While introducing noise sampling (as in MPPI’s random perturbations) can somewhat mitigate this issue, significantly non-convex features often lead to inefficient sampling or control instability when noise is simply increased.\n\nFrom a dynamical systems perspective, non-convex value functions correspond to systems with multiple attractors and unstable equilibrium points. Control algorithms need to identify these features and, when necessary, guide the system across energy barriers to escape suboptimal attractor regions. Traditional stochastic control methods have limited capabilities in this regard, as they lack awareness and memory of the state space’s topological structure.\n\nTraditional stochastic optimal controllers lack memory—operating solely on current states without learning from past trajectories. This design means controllers might repeatedly fall into the same suboptimal regions, failing to extract experience from previous \"failures.\" In contrast, advanced cognitive systems (like humans) dynamically adjust decision strategies based on prior experience when exploring complex environments.\n\nThis paper addresses a fundamental question: How can we integrate \"memory\" mechanisms into stochastic optimal control frameworks, enabling controllers to automatically learn state space topological features from historical trajectories and adjust optimization strategies accordingly? We introduce Memory-Augmented Potential Field Theory, integrating historical state experience into stochastic optimal control through dynamic potential fields that automatically identify and encode topological features of the state space during execution. These fields act as correction terms to reshape the value function landscape, enabling adaptive navigation of non-convex optimization problems. Our framework provides: 1) automatic detection and encoding of problematic regions like local minima and low-gradient areas, 2) dynamic reshaping of value functions for efficient escape from suboptimal attractors, 3) theoretical guarantees for convergence to global optima, and 4) significant performance improvements in complex control tasks without requiring extensive offline training.\n\nOur approach uniquely integrates memory mechanisms with dynamical systems theory and stochastic optimal control, analyzing memory’s impact on non-convex optimization topologically. Beyond simply storing experiences, our method automatically identifies key state space features and dynamically reshapes value function landscapes, enabling \"meta-optimization\" capabilities. The code has been anonymized and is available at https://anonymous.4open.science/r/MA_MPPI-6555.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将“记忆”机制整合到随机最优控制框架中，以便自动学习状态空间的拓扑特征？  \n2. 如何在复杂的非凸环境中提高控制器的优化策略和性能？  \n\n【用了什么创新方法】  \n本研究提出了记忆增强潜力场理论，通过动态构建基于记忆的潜力场，自动识别和编码状态空间的关键拓扑特征。该方法允许控制器从历史轨迹中学习，调整优化策略。理论分析表明，记忆增强潜力场具有非凸逃逸特性和渐近收敛性。通过实现记忆增强模型预测路径积分（MPPI）控制器，在复杂非凸环境中显著提高了性能，展示了经验学习在控制系统中的广泛适用性。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "RoboSSM: Scalable In-context Imitation Learning via State-Space Models",
            "authors": "Youngju Yoo,Jiaheng Hu,Yifeng Zhu,Bo Liu,Qiang Liu,Roberto Martín-Martín,Peter Stone",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19658",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19658",
            "arxiv_html_link": "https://arxiv.org/html/2509.19658v1",
            "abstract": "In-context imitation learning (ICIL) enables robots to learn tasks from prompts consisting of just a handful of demonstrations. By eliminating the need for parameter updates at deployment time, this paradigm supports few-shot adaptation to novel tasks.\nHowever, recent ICIL methods rely on Transformers, which have computational limitations and tend to underperform when handling longer prompts than those seen during training.\nIn this work, we introduce RoboSSM, a scalable recipe for in-context imitation learning based on state-space models (SSM).\nSpecifically, RoboSSM replaces Transformers with Longhorn – a state-of-the-art SSM that provides linear-time inference and strong extrapolation capabilities, making it well-suited for long-context prompts.\nWe evaluate our approach on the LIBERO benchmark and compare it against strong Transformer-based ICIL baselines.\nExperiments show that RoboSSM extrapolates effectively to varying numbers of in-context demonstrations, yields high performance on unseen tasks, and remains robust in long-horizon scenarios.\nThese results highlight the potential of SSMs as an efficient and scalable backbone for ICIL.\nOur code is available at https://github.com/youngjuY/RoboSSM.",
            "introduction": "Imitation Learning (IL) is a powerful framework that enables robots to learn behaviors from demonstrations without explicit programming or reward design [1, 2].\nWhile IL has achieved notable success in manipulation and navigation tasks, a key limitation of conventional imitation learning lies in its restricted adaptation capability, particularly when faced with new tasks.\nEven with models trained on large multi-task datasets [3, 4, 5, 6], adapting to novel tasks still requires collecting a large amount of task-specific data and retraining, which can be computationally costly and often unstable [7, 8].\nTo address this challenge, In-Context Imitation Learning (ICIL) introduces a new paradigm, inspired by the success of large language models (LLMs) [9, 10, 11] in adapting to unseen language tasks through few-shot learning [12].\nICIL integrates the concept of prompting into imitation learning [13, 14, 15, 16, 12, 17, 18, 19], allowing the model to infer and perform tasks based on a prompt composed of demonstrations, with no post-demonstration training.\n\nGiven that ICIL formulates imitation learning as a sequence modeling problem, recent ICIL approaches have naturally adopted Transformer-based models as their primary architecture [17, 19, 18].\nAlthough Transformers are the dominant architecture for sequence modeling [20], their time complexity scales quadratically with sequence length, and they struggle to extrapolate beyond training lengths [21, 22].\nFor ICIL to handle long prompts efficiently at test time, it is essential to adopt alternatives to Transformers that enhance scalability with input length.\n\nIn this paper, we introduce RoboSSM, a scalable in-context learning framework that replaces Transformers with state-space models (SSMs).\nSpecifically, RoboSSM utilizes Longhorn [23], a state-of-the-art SSM with linear inference time and strong extrapolation capability for long-context sequences.\nLeveraging these properties, RoboSSM can process substantially longer prompts at test time compared to previous Transformer-based ICIL methods.\nWe also investigate adapting Longhorn to ICIL via β\\beta-scaling ablations, which encourage the model to attend to demonstration prompts.\n\nOn the LIBERO [24] benchmark, RoboSSM uniquely benefits from using more in-context examples, maintaining high success rates on unseen tasks when trained with only a few demonstrations.\nFor instance, on the task pick up the plate and place it in the tray, where the plate object was unseen during training, RoboSSM achieves its highest performance when prompted with 32 demonstrations, despite being trained on only two.\nFurthermore, our framework performs well on unseen long-horizon tasks, which we simulate by repeating frames in the demonstrations to create time-dilated scenarios.\nConsequently, RoboSSM handles test-time demonstration prompts up to 16 times longer than those seen in training while maintaining linear inference time, whereas Transformer-based ICIL methods sharply degrade once the test prompt exceeds the training length.\nThese findings confirm that RoboSSM establishes a scalable in-context imitation learning framework by effectively leveraging long-range contextual information.\nOur project video is available at RoboSSM-video.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高机器人在新任务上的适应能力，特别是在少量示范的情况下。  \n2. 传统的Transformer模型在处理长上下文提示时的计算限制和性能不足。  \n3. 如何在不进行参数更新的情况下实现高效的在-context模仿学习。  \n\n【用了什么创新方法】  \nRoboSSM引入了一种基于状态空间模型（SSM）的可扩展在-context模仿学习框架，替代了计算复杂度高的Transformer。使用Longhorn SSM，RoboSSM实现了线性推理时间和强大的外推能力，能够有效处理长上下文提示。实验表明，RoboSSM在LIBERO基准测试中表现优异，能够在仅用少量示范的情况下，成功适应未见过的任务，并在长时间范围任务中保持高性能。具体而言，在未见物体的任务中，RoboSSM在使用32个示范时表现最佳，且能够处理比训练时长16倍的测试提示。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Minimalistic Autonomous Stack for High-Speed Time-Trial Racing",
            "authors": "Mahmoud Ali,Hassan Jardali,Youwei Yu,Durgakant Pushp,Lantao Liu",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "The data associated with this paper is available atthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.19636",
            "code": "https://doi.org/10.5281/zenodo.17187680",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19636",
            "arxiv_html_link": "https://arxiv.org/html/2509.19636v1",
            "abstract": "Autonomous racing has seen significant advancements, driven by competitions such as the Indy Autonomous Challenge (IAC) and the Abu Dhabi Autonomous Racing League (A2RL). However, developing an autonomous racing stack for a full-scale car is often constrained by limited access to dedicated test tracks, restricting opportunities for real-world validation.\nWhile previous work typically requires extended development cycles and significant track time, this paper introduces a minimalistic autonomous racing stack for high-speed time-trial racing that emphasizes rapid deployment and efficient system integration with minimal on-track testing.\nThe proposed stack was validated on real speedways, achieving a top speed of 206 km h−1206\\text{\\,}\\mathrm{km}\\text{\\,}{\\mathrm{h}}^{-1} within just 11 hours’ practice run on the track with 325 km325\\text{\\,}\\mathrm{km} in total. Additionally, we present the system performance analysis, including tracking accuracy, vehicle dynamics, and safety considerations, offering insights for teams seeking to rapidly develop and deploy an autonomous racing stack with limited track access.",
            "introduction": "Autonomous racing has gained significant traction in recent years, advancing both research and real-world deployment in high-speed autonomy. Competitions such as\nIAC [1] and A2RL [2] provide a platform for testing cutting-edge autonomous systems in extreme conditions. These events have driven advancements in perception, planning, and control algorithms [3, 4, 5, 6], leading to fully autonomous race cars competing at 290 km h−1290\\text{\\,}\\mathrm{km}\\text{\\,}{\\mathrm{h}}^{-1} [7].\nDespite these achievements, developing an autonomous racing stack for full-scale vehicles remains a resource-intensive endeavor due to the limited availability of dedicated racetracks for testing. Prior work has demonstrated impressive results but often relies on years of development and extensive track-testing time, making rapid deployment difficult for new teams.\n\nTo address this challenge, we present a minimalistic autonomous racing stack designed for high-speed time-trial racing with a focus on single-car speed performance and rapid deployment.\nOur approach strategically maximizes track time utilization, enabling a fully functional autonomy stack with minimal on-track testing.\nThe proposed system was implemented on the IAC AV-24 race car [8] and validated on real speedways, achieving a top speed of 206 km h−1206\\text{\\,}\\mathrm{km}\\text{\\,}{\\mathrm{h}}^{-1} with only 11 track hours and 325 km325\\text{\\,}\\mathrm{km} of practice runs.\nThe key contributions of this work include:\nI) A minimalistic autonomous racing stack tailored for high-speed solo racing.\nII) A system integration of the proposed stack into the AV-2424 racing car.\nIII) An evaluation of the proposed stack\nwith emphasis on controllers performance and dynamics analysis.\nIV) A discussion of the safety measures incorporated into the system, along with an analysis of failure cases.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在有限的赛道测试条件下快速开发和部署自主赛车系统？  \n2. 如何提高自主赛车在高速度下的性能和安全性？  \n\n【用了什么创新方法】  \n本研究提出了一种简约的自主赛车堆栈，专注于高速度单车时间试验，旨在通过最大化赛道时间利用率来实现快速部署。该系统在真实赛道上进行了验证，仅用11小时的练习运行便达到了206 km/h的最高速度，总计325 km的测试里程。研究还分析了系统性能，包括跟踪精度、车辆动态和安全性，为寻求快速开发和部署自主赛车的团队提供了宝贵的见解。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data",
            "authors": "Ryan Punamiya,Dhruv Patel,Patcharapong Aphiwetsa,Pranav Kuppili,Lawrence Y. Zhu,Simar Kareer,Judy Hoffman,Danfei Xu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "Accepted at 39th Conference on Neural Information Processing Systems (NeurIPS 2025) and Oral at Conference on Robot Learning (CoRL 2025)",
            "pdf_link": "https://arxiv.org/pdf/2509.19626",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19626",
            "arxiv_html_link": "https://arxiv.org/html/2509.19626v1",
            "abstract": "Egocentric human experience data presents a vast resource for scaling up end-to-end imitation learning for robotic manipulation. However, significant domain gaps in visual appearance, sensor modalities, and kinematics between human and robot impede knowledge transfer. This paper presents EgoBridge, a unified co-training framework that explicitly aligns the policy latent spaces between human and robot data using domain adaptation. Through a measure of discrepancy on the joint policy latent features and actions based on Optimal Transport (OT), we learn observation representations that not only align between the human and robot domain but also preserve the action-relevant information critical for policy learning.\nEgoBridge achieves a significant absolute policy success rate improvement by 44% over human-augmented cross-embodiment baselines in three real-world single-arm and bimanual manipulation tasks. EgoBridge also generalizes to new objects, scenes, and tasks seen only in human data, where baselines fail entirely. Videos and additional information can be found at https://ego-bridge.github.io/",
            "introduction": "Supervised imitation learning methods such as behavior cloning have emerged as a promising path to scaling robot performance across diverse objects, tasks, and environments. However, while large-scale models in vision and language have achieved remarkable generalization through Internet-sourced data, replicating this success in robotics remains challenging due to the labor-intensive nature of collecting teleoperated demonstrations. Deploying physical robots to many new environments to collect data with enough coverage and diversity is economically and practically intractable.\n\nIn this work, we aim to enable robots to learn from egocentric recordings of natural human behavior, collected by increasingly ubiquitous wearable devices (e.g., XR devices and smart glasses). Without a robot in the loop, such data is cheap and scalable to collect and captures natural human interactions with the world. More importantly, it reflects the embodied human experience, as it contains both observations (e.g., egocentric RGB images) and actions (e.g., hand motions). Unlike unstructured data sources such as Internet videos, the rich embodied information allows us to treat human data and robot data as equal parts in a continuous spectrum of demonstration data and potentially learn from both with a unified learning framework.\n\nHowever, the multitudes of domain gaps between human and robot pose significant challenges in designing such a framework. Human bodies and robots have different visual appearances. Even within a shared action space, kinematic differences can lead to behavior distribution shifts. Robots also have additional sensing modalities such as wrist cameras that are often missing from embodied human data. While recent works such as EgoMimic kareer2024egomimicscalingimitationlearning  have attempted to bridge the embodiment gaps with techniques such as visual masking, data normalization, and motion retargeting, such domain gaps still largely remain. More broadly, simply co-training from cross-domain data does not automatically yield effective knowledge transfer, as suggested by recent studies wei2025empirical . Such challenges prevent policies from scaling their performance primarily with human data.\n\nWe formalize the human-robot cross-embodiment learning problem as a domain adaptation problem, where human and robot data represent two labeled distributions with significant covariate shifts in observations due to embodiment gaps. Standard domain adaptation approaches often rely on global distribution alignment techniques such as adversarial training tzeng2017adversarialdiscriminativedomainadaptation  and maximum mean discrepancy minimization long2017deeptransferlearningjoint . However, they primarily address high-level tasks such as image classification and fail to preserve detailed action-relevant information—a critical requirement for robot learning where actions and observations are temporally correlated under compounding covariate shift.\n\nTo address these challenges, we propose EgoBridge, a novel domain adaptation approach that uses Optimal Transport (OT) to align latent representations from human and robot domains as part of the policy co-training objective. Unlike conventional domain alignment methods, our OT formulation explicitly exploits the inherent relationship between motion similarities in human and robot domains to form pseudo-pairs as supervision for the adaptation process. Concretely, we use the dynamic time warping (DTW) distance among human and robot motion trajectories to shape the OT ground cost. This encourages the transport map to find a minimal-cost coupling between human and robot data exhibiting similar behaviors. As such, EgoBridge aligns policy representations across domains via a differentiable OT loss (Sinkhorn distance), while preserving action-relevant information for policy learning. Importantly, we show that EgoBridge learns a shared latent representation that generalizes beyond the paired data. This enables the policy to learn behaviors observed only within the human dataset, effectively enabling the policy to scale primarily with human data.\n\nWe evaluate EgoBridge on both a reproducible simulation benchmark task and three challenging real-world manipulation tasks. Our results show that EgoBridge consistently improves policy success rates compared to human-augmented cross-embodiment baselines, for up to 44% absolute success rate improvement, and effectively transfers behaviors from diverse human demonstrations to robotic execution in tasks requiring spatial, visual, and task generalization.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效地从人类的第一人称数据中学习模仿行为以提升机器人操作能力。  \n2. 人类与机器人之间在视觉外观、传感器模态和运动学上的显著领域差距如何影响知识转移。  \n3. 现有的领域适应方法如何未能有效保留与动作相关的信息，导致机器人学习性能受限。  \n\n【用了什么创新方法】  \n本研究提出了EgoBridge，一个统一的共训练框架，通过最优传输（Optimal Transport, OT）对人类和机器人数据的策略潜在空间进行显式对齐。该方法利用动态时间规整（Dynamic Time Warping, DTW）距离来构建人类与机器人运动轨迹之间的伪配对，从而在适应过程中形成监督信号。EgoBridge通过可微分的OT损失（Sinkhorn距离）对跨领域的策略表示进行对齐，同时保留与动作相关的信息。实验结果表明，EgoBridge在三个真实世界的单臂和双臂操作任务中实现了高达44%的绝对成功率提升，并能够有效地将人类演示中的行为转移到机器人执行中，展示了其在新物体、场景和任务上的泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots",
            "authors": "Qingxi Meng,Emiliano Flores,Carlos Quintero-Peña,Peizhu Qian,Zachary Kingston,Shannan K. Hamlin,Vaibhav Unhelkar,Lydia E. Kavraki",
            "subjects": "Robotics (cs.RO)",
            "comment": "under review",
            "pdf_link": "https://arxiv.org/pdf/2509.19610",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19610",
            "arxiv_html_link": "https://arxiv.org/html/2509.19610v1",
            "abstract": "In this work, we address the problem of planning robot motions for a high-degree-of-freedom (d\n\no\nf) robot that effectively achieves a given perception task while the robot and the perception target move in a dynamic environment. Achieving navigation and perception tasks simultaneously is challenging, as these objectives often impose conflicting requirements. Existing methods that compute motion under perception constraints fail to account for obstacles, are designed for low-d\n\no\nf robots, or rely on simplified models of perception. Furthermore, in dynamic real-world environments, robots must replan and react quickly to changes and directly evaluating the quality of perception (e.g., object detection confidence) is often expensive or infeasible at runtime. This problem is especially important in human-centered environments such as homes and hospitals, where effective perception is essential for safe and reliable operation. To address these challenges, we propose a GPU-parallelized perception-score-guided probabilistic roadmap planner with a neural surrogate model (ps-prm). The planner explicitly incorporates the estimated quality of a perception task into motion planning for high-d\n\no\nf robots. Our method uses a learned model to approximate perception scores and leverages GPU parallelism to enable efficient online replanning in dynamic settings. We demonstrate that our planner, evaluated on high-d\n\no\nf robots, outperforms baseline methods in both static and dynamic environments in both simulation and real-robot experiments.",
            "introduction": "Achieving mobile manipulation (e.g., navigating to a table to grasp a tool) alongside perception tasks (e.g., tracking an object or a human) remains highly challenging. In this work, we address the problem of planning robot motions for a high-degree-of-freedom (d\n\no\nf) robot that effectively achieves a given perception task while the robot and the perception target move in a dynamic environment. A fundamental challenge emerges: the objectives of motion and perception often conflict—perception tasks impose non-trivial constraints on robot motion, as performance varies with object shape, occlusion, distance, and viewing angle [1, 2, 3]. For example, detecting human faces often requires maintaining a frontal view, while reliable object tracking typically benefits from close proximity. At the same time, motion planning for high-degree-of-freedom (d\n\no\nf) robots must follow kinematic constraints, avoid obstacles and nearby humans, and account for the complex, nonlinear relationship between the robot’s configuration and the camera pose.\n\nAs an example, consider the situation in Fig. 1. A mobile robot with wrist-mounted camera must navigate a cluttered environment while observing a monitor at the end of the hallway. The robot must navigate through clutter with a differential drive base while simultaneously maintaining tracking performance, i.e., it avoids occlusions and attains high tracking confidence.\n\nThese challenges are further compounded in dynamic environments, where objects and people move. In such settings, robots must continuously adapt to changes while preserving high perception quality [4, 5, 6, 7].\nFor example, a key motivation of this work comes from human-centered environments such as nurse training settings [8, 9], where robotic tutors are used to support skill acquisition. In such a system [10], the robot must continuously monitor a trainee to detect compliance with sterile techniques and provide timely feedback while simultaneously navigating the environment. Fig. 7 and Fig. 10 illustrate this challenge, where the robot must continuously track nurses’ faces while navigating through a cluttered clinical environment.\nHowever, directly evaluating perception performance (e.g., detection confidence) in such settings can be prohibitively expensive or even impossible. This motivates the need for efficient mechanisms to approximate perception quality and support fast, responsive replanning in high-dimensional spaces.\n\nHow do existing planning methods tackle planning for perception in high d\n\no\nf setting?\nExisting planning methods that consider both motion and perception have several limitations. Informative path planners [11, 12, 13, 14] aim to find paths that maximize information gain while respecting constraints like path length or budget, but they often become computationally expensive, especially when the problem size grows. Perception-aware path planners are typically designed for low-d\n\no\nf systems [15, 16, 17, 18, 19, 20] or rely on overly simplified perception models [21], such as keeping a point of interest centered in the field of view. Active perception approaches [22, 23, 24, 25, 26, 27, 28, 29, 30] focus on selecting discrete viewpoints to maximize scene understanding, rather than ensuring consistent perception quality along a motion trajectory. Moreover, most of these approaches are not designed to operate in dynamic environments where objects or humans may move and where continuous adaptation is needed. As a result, they lack the responsiveness required for real-time perception-aware planning in practical settings.\n\nIn this paper, we propose Perception-Score-guided Probabilistic Roadmap Planning (PS-PRM)—a GPU-parallelized, roadmap-based planner that explicitly incorporates the estimated quality of a given perception task (e.g., object detection confidence) into motion planning for high-degree-of-freedom (DOF) robots. The perception score is predicted by a neural surrogate model trained to approximate perception scores for specific tasks.\nps-prm jointly optimizes for both motion and perception by associating each configuration with an estimated perception score and using these scores to guide roadmap construction and path selection. To efficiently approximate perception quality during planning, we introduce a neural surrogate model trained on data from various objects and human targets. To account for occlusions in cluttered environments, we incorporate a ray-casting-based pipeline that dynamically adjusts perception estimates. Finally, to enable fast replanning in dynamic environments, we develop a GPU-parallelized framework that executes collision checking, forward kinematics (FK), and perception evaluation in batch. These components allow our method to scale to high-dimensional planning problems while maintaining robust perception performance in both static and dynamic scenarios.\n\nIn this work, we make the following key contributions:\n\nA sampling-based roadmap planner, ps-prm, that integrates perception quality into motion planning for high-d\n\no\nf robots.\n\nA perception score estimation pipeline that uses a neural surrogate model and ray casting to efficiently approximate detection quality while accounting for occlusions in cluttered scenes.\n\nA batch-processing infrastructure that accelerates collision checking, forward kinematics, and perception evaluation using GPU parallelism, enabling real-time replanning.\n\nExtensive simulation and real-robot experiments on three different high-d\n\no\nf robots, demonstrating that ps-prm consistently improves both perception performance and planning efficiency compared to baseline methods.\n\n1. A sampling-based roadmap planner, ps-prm, that integrates perception quality into motion planning for high-d\n\no\nf robots.\n\n2. A perception score estimation pipeline that uses a neural surrogate model and ray casting to efficiently approximate detection quality while accounting for occlusions in cluttered scenes.\n\n3. A batch-processing infrastructure that accelerates collision checking, forward kinematics, and perception evaluation using GPU parallelism, enabling real-time replanning.\n\n4. Extensive simulation and real-robot experiments on three different high-d\n\no\nf robots, demonstrating that ps-prm consistently improves both perception performance and planning efficiency compared to baseline methods.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在动态环境中为高自由度机器人规划运动和感知任务？  \n2. 现有方法在处理高自由度机器人运动和感知时的局限性是什么？  \n3. 如何有效评估感知质量以支持快速的在线重规划？  \n\n【用了什么创新方法】  \n提出了一种名为ps-prm的GPU并行化感知评分引导概率路网规划器，该方法将感知任务的质量估计纳入高自由度机器人的运动规划中。通过训练神经代理模型来近似感知评分，并利用GPU并行性实现动态环境中的高效在线重规划。实验结果表明，ps-prm在静态和动态环境中均优于基线方法，显著提高了感知性能和规划效率。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting",
            "authors": "Sander Tonkens,Nikhil Uday Shinde,Azra Begzadić,Michael C. Yip,Jorge Cortés,Sylvia L. Herbert",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "The first three authors contributed equally. This work has been accepted for publication at the Conference on Robot Learning",
            "pdf_link": "https://arxiv.org/pdf/2509.19597",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19597",
            "arxiv_html_link": "https://arxiv.org/html/2509.19597v1",
            "abstract": "The widespread deployment of autonomous systems in safety-critical environments such as urban air mobility hinges on ensuring reliable, performant, and safe operation under varying environmental conditions.\nOne such approach, value function-based safety filters, minimally modifies a nominal controller to ensure safety.\nRecent advances leverage offline learned value functions to scale these safety filters to high-dimensional systems.\nHowever, these methods assume detailed priors on all possible sources of model mismatch, in the form of disturbances in the environment – information that is rarely available in real world settings.\nEven in well-mapped environments like urban canyons or industrial sites, drones encounter complex, spatially-varying disturbances arising from payload-drone interaction, turbulent airflow, and other environmental factors.\nWe introduce space2time, which enables safe and adaptive deployment of offline-learned safety filters under unknown, spatially-varying disturbances.\nThe key idea is to reparameterize spatial variations in disturbance as temporal variations, enabling the use of precomputed value functions during online operation.\nWe validate space2time on a quadcopter through extensive simulations and hardware experiments, demonstrating significant improvement over baselines.",
            "introduction": "Autonomous systems are increasingly deployed in safety-critical environments subject to variable conditions, where ensuring reliable and safe operation is of paramount importance.\nFor instance, a drone operating in mapped environments such as urban canyons or shipyards must remain within a known safe region despite complex, spatially-varying wind disturbances.\nRather than designing a bespoke performant, yet safe, controller for each task, a more modular approach uses a safety filter.\nThese filters monitor a nominal, high-performance controller in real-time and intervene minimially-only when necessary to enforce guarantees without unduly compromising task performance [1].\nPopular approaches for constructing such filters include Control Barrier Functions (CBFs) [2] and Hamilton-Jacobi Reachability (HJR) analysis [3].\nA recent line of work merges these two paradigms, leveraging reachability-based value functions as barrier certificates to construct safety filters with formal guarantees [4, 5, 6].\n\nHowever, these methods face significant practical challenges. A primary limitation is their reliance on an accurate, pre-specified model of the system’s dynamics and its operational domain-the set of conditions, such as expected wind patterns, the system is designed to operate in.\nSecond, each method faces inherent hurdles: HJR analysis is limited by the curse of dimensionality, making it intractable for high-dimensional systems, while the systematic synthesis of a valid CBF for general nonlinear systems remains an open problem.\n\nTo overcome these practical limitations, learning-based approaches have gained prominence, seeking to approximate safety value functions or barrier certificates directly from data [7, 8, 9].\nHowever, these learned approaches often assume a static operational domain that is known beforehand.\nThis makes them brittle when faced with environmental conditions that shift during and across deployments, forcing a choice between unsafe behavior in the face of novelty or an overly conservative policy designed for the worst case [10].\n\nOffline learning of a value function for a safety filter relies on a joint system-environment model that captures the true system’s runtime behavior.\nSuch a model is infeasible in environments with spatially varying disturbances, e.g., wind in urban canyons [11, 12], which are unknown a priori and even differ across deployments.\nA compounding challenge arises because disturbance measurements are typically obtained at a slower rate than control inputs, due to practical sensing and computational constraints.\nHowever, this slower update rate means unmodeled spatial variation can cause significant changes between consecutive measurements, leading to safety violations if ignored.\nOur insight is that spatial variations in disturbance appear as temporal variations along a trajectory.\nBy learning a time-varying safety value function that explicitly accounts for disturbance evolution over time, we implicitly capture spatial variations along trajectories, enabling their use as online safety filters.\nThis work takes a step towards bridging offline-learned value functions with online adaptation in evolving operational domains.\nOur main contributions are:\n\nWe introduce a safety value function formulation that is explicitly conditioned on a disturbance’s temporal rate of change.\n\nWe introduce a safety value function formulation that is explicitly conditioned on a disturbance’s temporal rate of change.\n\nWe use this value function formulation to propose space2time.\nOur approach reparameterizes spatial variations as temporal variations in disturbance.\nThis ensures safety in the presence of unknown, spatially varying disturbances through the use of an adaptive safety filter that leverages our offline-learned value functions.\n\nWe validate space2time on a quadcopter through extensive simulations and hardware experiments demonstrating substantial improvements in safety compared to existing approaches without significantly sacrificing performance.\n\n1. We introduce a safety value function formulation that is explicitly conditioned on a disturbance’s temporal rate of change.\n\n2. We use this value function formulation to propose space2time.\nOur approach reparameterizes spatial variations as temporal variations in disturbance.\nThis ensures safety in the presence of unknown, spatially varying disturbances through the use of an adaptive safety filter that leverages our offline-learned value functions.\n\n3. We validate space2time on a quadcopter through extensive simulations and hardware experiments demonstrating substantial improvements in safety compared to existing approaches without significantly sacrificing performance.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在未知的空间变化干扰下确保自主系统的安全性和性能。  \n2. 现有的安全过滤方法依赖于准确的系统动态模型，如何克服这一限制。  \n3. 如何将离线学习的价值函数与在线适应相结合，以应对不断变化的操作环境。  \n\n【用了什么创新方法】  \n本文提出了space2time方法，通过将空间变化的干扰重新参数化为时间变化，利用离线学习的安全价值函数进行在线适应。该方法显著提高了在复杂环境中的安全性，经过广泛的仿真和硬件实验验证，结果显示与现有方法相比，安全性有显著改善，同时性能损失不大。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping",
            "authors": "Chad R. Samuelson,Abigail Austin,Seth Knoop,Blake Romrell,Gabriel R. Slade,Timothy W. McLain,Joshua G. Mangelson",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19579",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19579",
            "arxiv_html_link": "https://arxiv.org/html/2509.19579v1",
            "abstract": "Outdoor intelligent autonomous robotic operation relies on a sufficiently expressive map of the environment. Classical geometric mapping methods retain essential structural environment information, but lack a semantic understanding and organization to allow high-level robotic reasoning. 3D scene graphs (3DSGs) address this limitation by integrating geometric, topological, and semantic relationships into a multi-level graph-based map. Outdoor autonomous operations commonly rely on terrain information either due to task-dependence or the traversability of the robotic platform. We propose a novel approach that combines indoor 3DSG techniques with standard outdoor geometric mapping and terrain-aware reasoning, producing terrain-aware place nodes and hierarchically organized regions for outdoor environments. Our method generates a task-agnostic metric-semantic sparse map and constructs a 3DSG from this map for downstream planning tasks, all while remaining lightweight for autonomous robotic operation. Our thorough evaluation demonstrates our 3DSG method performs on par with state-of-the-art camera-based 3DSG methods in object retrieval and surpasses them in region classification while remaining memory efficient. We demonstrate its effectiveness in diverse robotic tasks of object retrieval and region monitoring in both simulation and real-world environments.",
            "introduction": "Autonomous robotic systems within large-scale outdoor environments have the potential to address a wide range of fundamental societal problems, including search and rescue, forest fires, food delivery, and others.\nHowever, such robotic systems require the ability to robustly and reliably localize within, map, and interpret outdoor scenes at large scales. In this context, we focus on the development of metric-semantic mapping techniques that enable large-scale autonomy in outdoor scenes.\n\nDue to its range, accuracy, and 360∘360^{\\circ} field-of-view, LiDAR has become a standard sensor for large-scale outdoor geometric mapping [1, 2, 3]. Beyond geometry, some approaches train models to semantically classify points in LiDAR scans [4, 5, 6, 7]. While promising, these models are still largely closed-set (restricted to a fixed set of semantic classes).\n\nCameras, in contrast, provide rich visual data and have achieved remarkable success in semantic scene understanding. Recent techniques enable more general open-set scene understanding and even enable grounding of visual data with natural language through vision-language models (VLMs) [8, 9, 10].\n\nOver the last several years, 3D scene graphs (3DSGs) have emerged as a structured approach to build semantically and hierarchically organized metric-semantic maps. Many 3DSG-based methods utilize VLMs and large language models (LLMs) for both scene graph construction and autonomous task planning. Most existing 3DSG methods focus on indoor environments using camera imagery and depth data to build a semantically-classified mesh that forms the base layer of the 3DSG. However, constructing mesh maps over large areas is both computationally and memory intensive. Additionally, camera-derived depth has limited range (≤20\\leq 20 meters). These both restrict indoor 3DSG techniques from scaling to large outdoor settings.\n\nIn this work, we combine indoor 3DSG techniques with geometrically robust outdoor LiDAR SLAM methods, enabling metric-semantic mapping in large-scale outdoor environments, we term our method Terra (see Fig. 1).\nWe structure the resulting map into a hierarchical scene graph specifically designed to support autonomous outdoor robotic tasks. In particular, since terrain is a key factor for outdoor navigation, we integrate a terrain layer into our 3DSG.\nThe key contributions of our paper are:\n\nA novel, memory-efficient, and task-agnostic approach for open-set metric-semantic mapping in large-scale outdoor environments,\n\nA terrain layer in the outdoor 3DSG that supports terrain-aware tasks where VLMs alone struggle,\n\nHierarchical region layers to handle multiple levels of task abstraction,\n\nAn in-depth evaluation on simulated and real world data comparing Terra with state-of-the-art (SOTA) indoor 3DSG methods.\n\nThe rest of our paper is outlined as follows. Section II provides a brief overview of research in semantic mapping and 3DSG methods. Section III gives an explanation for the Terra method. Section IV provides experiments across simulation and real-world datasets demonstrating the capabilities of our Terra method compared to SOTA methods and techniques. Section V concludes our findings and explores future work in the area of outdoor 3DSG generation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在大规模户外环境中实现有效的metric-semantic mapping？  \n2. 如何结合几何和语义信息以支持自主机器人的任务规划？  \n3. 如何处理户外环境中的地形信息以提高导航能力？  \n\n【用了什么创新方法】  \n本研究提出了一种名为Terra的创新方法，将室内3D场景图（3DSG）技术与户外几何映射和地形感知推理相结合，生成任务无关的稀疏地图。该方法通过构建层次化的场景图，集成地形层以支持户外导航任务，显著提高了区域分类的性能，并在内存效率上优于现有的相机基础3DSG方法。经过全面评估，Terra在物体检索和区域监控等多种机器人任务中表现出色，验证了其在模拟和真实环境中的有效性。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning",
            "authors": "Zachary Olkin,Kejun Li,William D. Compton,Aaron D. Ames",
            "subjects": "Robotics (cs.RO)",
            "comment": "Submitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.19573",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19573",
            "arxiv_html_link": "https://arxiv.org/html/2509.19573v1",
            "abstract": "Achieving highly dynamic behaviors on humanoid robots, such as running, requires controllers that are both robust and precise, and hence difficult to design. Classical control methods offer valuable insight into how such systems can stabilize themselves, but synthesizing real-time controllers for nonlinear and hybrid dynamics remains challenging. Recently, reinforcement learning (RL) has gained popularity for locomotion control due to its ability to handle these complex dynamics. In this work, we embed ideas from nonlinear control theory, specifically control Lyapunov functions (CLFs), along with optimized dynamic reference trajectories into the reinforcement learning training process to shape the reward. This approach, CLF-RL, eliminates the need to handcraft and tune heuristic reward terms, while simultaneously encouraging certifiable stability and providing meaningful intermediate rewards to guide learning. By grounding policy learning in dynamically feasible trajectories, we expand the robot’s dynamic capabilities and enable running that includes both flight and single support phases. The resulting policy operates reliably on a treadmill and in outdoor environments, demonstrating robustness to disturbances applied to the torso and feet. Moreover, it achieves accurate global reference tracking utilizing only on-board sensors, making a critical step toward integrating these dynamic motions into a full autonomy stack.",
            "introduction": "Humanoid running is a challenging task that involves executing highly dynamic motion on a nonlinear and hybrid system. Achieving performant and robust running demands controllers that can reject disturbances arising from model mismatch and environmental uncertainty, all while operating near the limits of the robot’s dynamic capabilities. Running inherently involves alternating between a flight phase, where both feet are off the ground, and a single-support phase, where only one foot is in contact. Effectively handling control across these hybrid domains is critical, as improper treatment can lead to instability.\n\nBipedal running has been studied for decades with early examples including the Raibert heuristic [1]. In the 2010’s, a number of planar bipeds were developed and running was achieved [2, 3, 4]. These methods fall under the category of Hybrid Zero Dynamics (HZD) [5] where offline trajectory optimization leveraging the idea of virtual constraints is used to generate a stable and periodic trajectory. Then, online, the trajectories are tracked using tools from nonlinear control theory such as feedback linearization and control Lyapunov functions (CLFs). These controllers have been shown to be certifiably stable if the convergence to the virtual constraints is sufficiently quick relative to the destabilizing effect of the foot-ground impact [6]. In general, these methods optimize for a steady-state motion (i.e. a periodic orbit) while the ability to get to the steady state motion is entirely dependent on the region of attraction of the tracking controller. Because these controllers operate only on the continuous dynamics, their capacity to generate transient and robust behaviors is inherently limited.\n\nOne of the difficulties with generating transients motions lies in the ability to reason through contact, specifically determining the subsequent contact schedule. A number of model predictive control (MPC) schemes have attempted to solve this issue through various numerical methods. Contact implicit MPC (CI-MPC) uses gradient-based optimization to implicitly yield a hybrid domain sequence, but have not yielded bipedal running and are quite computationally intensive in general [7, 8, 9]. Sample-based methods circumvent the need for an explicit domain sequence by rolling out sampled inputs and optimizing the inputs in an MPC fashion [10, 11, 12]. Yet these methods still require large amounts of on-board compute and have not yet produced humanoid running.\n\nReinforcement learning (RL) has recently emerged as a dominant approach for controlling legged robots [13, 14, 15, 16, 17]. RL methods are attractive due to their robustness, ability to generate diverse motions, lightweight on-board execution requirements, and their capacity to learn contact-rich behaviors directly from experience rather than requiring explicit contact modeling. Notably, RL has achieved bipedal running [18, 19]. However, many RL schemes require extensive hand tuning of rewards to produce a performant and robust policy. Poorly shaped rewards can lead to unstable learning, failure to achieve the desired behaviors, or prohibitively long training times. To mitigate these issues, a number of works have merged pre-computed trajectories with RL. This includes reduced-order model trajectories [20, 21] and full-order model trajectories such as from the HZD framework [22]. Yet, even in these cases, the rewards generally incentivize being close to the trajectories in an ad-hoc manner. Building on the ideas of [23], we embed a CLF tracking controller’s stability condition and Lyapunov function into the reward to provide meaningful intermediate rewards and incentivize certifiably stable behavior.\n\nMany prior works focus on bipeds, not humanoids, and have not shown running with low positional drift required for treadmill operation. Hand designed rewards are often used and the policies may fail to match the desired velocity [19]. Even when fast locomotion is achieved, a flight phase is not necessarily attained [20]. Alternative approaches use RL to imitate human motion data [24, 25, 26, 27]. Although such methods can reproduce the demonstrated motion, they have not shown the ability to produce dynamically stable steady-state motions, like running, with tracking capabilities ready for use in an autonomy stack. In contrast, our proposed method provides a principled way of synthesizing running controllers for full humanoids, not just bipeds, with minimal tuning and without relying on human data. The resulting policy produces both transient and steady state running motions with accurate position and velocity tracking.\n\nIn this paper, we develop a model-guided approach to enable running on a humanoid. We leverage multi-domain trajectory optimization, control Lyapunov functions (CLFs), and reinforcement learning (RL) to create a robust and performant controller. Trajectory optimization is used to generate nominal motions, which are then incorporated into CLF-based tracking controllers. These CLFs are embedded directly into the RL reward, eliminating the need for heuristic reward design. The resulting RL policy no longer requires trajectories or CLFs at runtime. Fig. 1 shows an overview of the framework. We demonstrate this controller on a Unitree G1 humanoid robot both on a treadmill and outdoors. The resulting policy shows accurate position and velocity tracking on the hardware, and exhibits robustness to various objects on the ground all while maintaining running speeds and achieving a full flight phase.\n\nThe rest of the paper is organized as follows: in section II the mathematical preliminaries are presented, including the hybrid system model and CLFs. Section III describes how multi-domain trajectories can be generated and embedded into the RL via CLFs. Then, section IV showcases the simulation and hardware experiments, demonstrating the performance and robustness of the policy. Lastly section V gives the paper’s conclusions.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何设计一个既稳健又精确的人形机器人跑步控制器。  \n2. 如何利用强化学习（RL）处理复杂的非线性和混合动力学。  \n3. 如何消除手工调节奖励项的需求，同时确保控制的稳定性。  \n\n【用了什么创新方法】  \n本研究提出了一种控制Lyapunov函数（CLF）引导的强化学习（CLF-RL）方法，通过将非线性控制理论的思想与优化的动态参考轨迹嵌入到强化学习训练过程中，形成了一种新的奖励机制。该方法消除了对手工设计和调节奖励项的需求，同时鼓励可证明的稳定性，并提供有意义的中间奖励以指导学习。通过基于动态可行轨迹的策略学习，扩展了机器人的动态能力，使其能够在跑步中有效地处理飞行和单支撑阶段。实验结果表明，该策略在跑步机和户外环境中均表现出可靠性，能够抵御对机器人躯干和脚部施加的干扰，并实现准确的全局参考跟踪，仅依赖于机载传感器。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action",
            "authors": "Sacha Morin,Kumaraditya Gupta,Mahtab Sandhu,Charlie Gauthier,Francesco Argenziano,Kirsty Ellis,Liam Paull",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.19571",
            "code": "https://montrealrobotics.ca/agentic-scene-policies.github.io/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19571",
            "arxiv_html_link": "https://arxiv.org/html/2509.19571v1",
            "abstract": "Executing open-ended natural language queries is a core problem in robotics. While recent advances in imitation learning and vision-language-actions models (VLAs) have enabled promising end-to-end policies, these models struggle when faced with complex instructions and new scenes. An alternative is to design an explicit scene representation as a queryable interface between the robot and the world, using query results to guide downstream motion planning. In this work, we present Agentic Scene Policies (ASP), an agentic framework that leverages the advanced semantic, spatial, and affordance-based querying capabilities of modern scene representations to implement a capable language-conditioned robot policy. ASP can execute open-vocabulary queries in a zero-shot manner by explicitly reasoning about object affordances in the case of more complex skills. Through extensive experiments, we compare ASP with VLAs on tabletop manipulation problems and showcase how ASP can tackle room-level queries through affordance-guided navigation and a scaled-up scene representation. We encourage readers to visit our project page.",
            "introduction": "Generalist language-conditioned robot policies need to manage the complex interplay between language, space, and action. Much of the recent progress on this problem has been driven by vision-language models (VLMs) trained on internet-scale data and showing strong general visual understanding in the open-world. Applying VLMs to the robotics domain has broadly followed two paradigms. In the first paradigm, VLMs can serve as backbones for end-to-end policy learning, yielding “vision-language actions” models (VLAs) that directly map sensor data and language commands to robot actions [1, 2, 3, 4, 5]. In the second paradigm, VLMs are primarily used for perception in the construction and querying of structured scene representations with advanced capabilities for object retrieval and spatial reasoning [6, 7, 8, 9, 10, 11, 12, 13, 14, 15].\n\nVLAs are increasingly showing zero-shot potential on new tasks [16, 4] but in practice still require task-specific fine-tuning to be truly proficient, which poses challenges in terms of data collection and infrastructure that limit overall deployment. For their part, scene representations preserve the generality of VLMs—they can practically represent any object—but do not offer a direct solution to the motion problem and are often constrained to navigation and pick-and-place tasks as a result [6, 17, 18].\n\nWe observe that a large number of language queries can be solved through a (potentially repeated) three-step process consisting of 1) object grounding, 2) spatial reasoning, and 3) part-level interaction. In this work, we demonstrate that state-of-the-art zero-shot performance can be achieved across a wide range of robotics tasks by implementing all three steps as scene queries. We expose querying functionalities as tools that can be freely called by a large language model (LLM) agent to execute language commands. For interaction, we design an expressive set of skill primitives supported by the strong affordance detection capabilities of VLMs. Our modular policy can map language queries (Ring the desk bell, Remove the thumbtack) to specific affordances and affordance-based skills (tip_push, pinch_pull), as well as solve a range of mobile manipulation queries. In summary, our key contributions include:\n\nAgentic Scene Policies (ASP), a language-conditioned manipulation policy that can solve a broad range of queries involving specific semantics, spatial reasoning, and affordances.\n\nAn extensive empirical comparison with leading VLAs on 15 manipulation tasks, providing a valuable data point in the ongoing debate between modular and end-to-end methods.\n\nA mobile version of ASP that tackles room-level queries through affordance-guided navigation and a scaled-up scene representation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何执行开放式自然语言查询以实现机器人操作。  \n2. 现有的视觉-语言-动作模型在复杂指令和新场景下的局限性。  \n3. 如何设计一个明确的场景表示作为机器人与世界之间的可查询接口。  \n\n【用了什么创新方法】  \n本研究提出了Agentic Scene Policies (ASP)，通过结合语义、空间和可供性查询能力，创建了一个语言条件的机器人操作政策。ASP通过三个步骤（对象定位、空间推理和部分交互）实现复杂任务的零-shot执行。实验表明，ASP在15个操作任务中表现优于现有的视觉-语言-动作模型，能够有效处理房间级查询并实现基于可供性的导航。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space",
            "authors": "Sankalp Agrawal,Junwon Seo,Kensuke Nakamura,Ran Tian,Andrea Bajcsy",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19555",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19555",
            "arxiv_html_link": "https://arxiv.org/html/2509.19555v1",
            "abstract": "Recent works have shown that foundational safe control methods, such as Hamilton–Jacobi (HJ) reachability analysis, can be applied in the latent space of world models. While this enables the synthesis of latent safety filters for hard-to-model vision-based tasks, they assume that the safety constraint is known a priori and remains fixed during deployment, limiting the safety filter’s adaptability across scenarios. To address this, we propose constraint-parameterized latent safety filters that can adapt to user-specified safety constraints at runtime. Our key idea is to define safety constraints by conditioning on an encoding of an image that represents a constraint, using a latent-space similarity measure. The notion of similarity to failure is aligned in a principled way through conformal calibration, which controls how closely the system may approach the constraint representation. The parameterized safety filter is trained entirely within the world model’s imagination, treating any image seen by the model as a potential test-time constraint, thereby enabling runtime adaptation to arbitrary safety constraints. In simulation and hardware experiments on vision-based control tasks with a Franka manipulator, we show that our method adapts at runtime by conditioning on the encoding of user-specified constraint images, without sacrificing performance. Video results can be found on the project website.",
            "introduction": "World models offer a promising paradigm for generalizing robot control to hard-to-simulate physical tasks by learning compact latent state spaces and dynamics directly from high-dimensional observations [1, 2, 3, 4]. Recent works have demonstrated that foundational safe control methods, such as Hamilton–Jacobi (HJ) reachability analysis [5, 6], can be applied directly in a world model’s latent space, enabling safety analysis directly from high-dimensional sensor inputs. By computing robot policies that anticipate and avoid future failures within the world model’s imagination, these latent safety filters can proactively steer robots away from hard-to-model constraints, such as spilling the contents of deformable bags [7] or toppling complex rigid-body structures [8].\n\nHowever, most safe control frameworks assume that the state constraints that robots should avoid are determined a priori and remain fixed during deployment [9, 6]. In practice, this assumption is overly restrictive: at deployment time, a robot may need to adapt its notion of what is a safety constraint based on changing environments or end-user requirements. For example, consider the robot manipulator in Fig. 1 that must sweep clutter from a table. In one scenario, it needs to avoid sweeping objects in a particular region (top row), but later it may be tasked with intentionally collecting objects into that same region while avoiding a different one (bottom row). This raises the central question of our work:\n\nIn this work, we design constraint-parameterized latent safety filters (called AnySafe). The core challenge with parameterizing safety constraints in the latent space is that, unlike in hand-designed state spaces, the structure needed to represent and optimize against a suite of safety constraints does not naturally emerge. In hand-designed state spaces, one can design a low-dimensional parameterization of the constraint set (e.g., a circle by its center and radius) alongside a dense distance measure for guiding policy optimization (e.g., signed distance to the constraint set); this allows for the safety filter to be effectively computed for all possible constraint variations. In latent spaces, by contrast, constraints are typically only implicitly defined by classifiers on the latent states [7, 8] which do not admit a continuous parameterization to represent diverse safety constraints nor yield a notion of proximity from a state to such constraints.\n\nWe propose three key ingredients that enable constraint-parameterization in latent safety filters. First, we specify safety constraints via a similarity measure between the embedding of a constraint image and the robot’s current latent state; this provides a dense signal of how close the policy is to failure. Then, we calibrate the resulting constraint set with conformal prediction [10, 11] to align with an end-user’s semantic notion of failure. Lastly, we train the safety filter by treating any image in the world model dataset as a possible test-time safety constraint. At runtime, we adapt the latent safety filter by conditioning it on an encoding of a user-specified constraint image, thereby adapting it to the runtime safety specification.\n\nWe evaluate our framework on vision-based safe-control tasks, including a simulated vehicle collision-avoidance domain and real-world object sweeping with a Franka manipulator. Our results highlight four key findings: (1) by parameterizing the safety filter with constraint representations, AnySafe can adapt to arbitrary constraints provided as images; (2) this adaptability does not come at the cost of performance, as for a given constraint, the parameterized safety filter achieves performance comparable to a specialized filter trained solely on that constraint;\n(3) AnySafe generalizes to constraints beyond those that specialized safety filters can model; and (4) since AnySafe learns from continuous latent similarity signals, conformal calibration allows us to control how conservatively the robot avoids specified constraints by adjusting the effective size of the failure set.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在运行时适应用户指定的安全约束以提高机器人控制的灵活性？  \n2. 如何在潜在空间中有效地参数化安全约束以支持多样化的任务？  \n3. 如何在不牺牲性能的情况下实现安全过滤器的适应性？  \n\n【用了什么创新方法】  \n本研究提出了一种名为AnySafe的约束参数化潜在安全过滤器。其核心方法包括通过相似性度量将安全约束与机器人的潜在状态进行关联，使用符合预测来校准约束集，并在训练过程中将任何图像视为潜在的测试时安全约束。在模拟和硬件实验中，AnySafe展示了其在运行时根据用户指定的约束图像进行适应的能力，且在给定约束下，其性能与专门训练的过滤器相当。此外，AnySafe能够超越专门安全过滤器的建模能力，提供更广泛的适应性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots",
            "authors": "Min Dai,Aaron D. Ames",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19545",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19545",
            "arxiv_html_link": "https://arxiv.org/html/2509.19545v1",
            "abstract": "We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots. RoMoCo’s modular architecture unifies state-of-the-art planners and whole-body locomotion controllers under a consistent API, enabling rapid prototyping and reproducible benchmarking. By leveraging reduced-order models for platform-agnostic gait generation, RoMoCo enables flexible controller design across diverse robots.\nWe demonstrate its versatility and performance through extensive simulations on the Cassie, Unitree H1, and G1 robots, and validate its real-world efficacy with hardware experiments on the Cassie and G1 humanoids.",
            "introduction": "Bipedal locomotion remains one of the central challenges in robotics, given bipeds’ high-dimensional, non-linear, hybrid, and underactuated nature. A widely adopted strategy to address this complexity is the use of reduced-order models (ROM) that capture the essential dynamics of walking while abstracting full-body details. These models, including the linear inverted pendulum (LIP) [1] and its variants [2, 3], have enabled the design of theoretically grounded and robust locomotion controllers. However, despite their success in research, deploying ROM-based planners and integrating them with whole-body controllers (WBC) remains a significant challenge, requiring expertise in contact and state estimation, robot kinematics and dynamics, and nonlinear control.\n\nIn recent years, there has been a surge of interest in learning-based locomotion, largely fueled by the release of frameworks such as IsaacLab [4] and IsaacGym [5], which provide scalable reinforcement learning (RL) environments and simulation infrastructure. These platforms, combined with open-source RL algorithms implementations such as RSL-RL [6], have lowered the barrier to entry for training locomotion policies at scale, enabling impressive demonstrations of locomotion in simulation and on hardware.\n\nIn contrast to learning-based approaches, model-based methods, though more interpretable, computationally efficient, and theoretically grounded, lack equivalent open-source support. Researchers typically face steep implementation hurdles, not only in developing reduced-order planners but also in integrating them with whole-body controllers and simulators. To date, only isolated efforts exist, such as the ALIP controller for Cassie [7] implemented in Simulink Real-Time, which, while effective, is tied to a specific robot platform and simulation ecosystem, preventing its generalization. This lack of a standardized, extensible framework limits reproducibility, comparative benchmarking, and rapid prototyping across diverse robotic platforms.\n\nA number of open-source libraries provide essential components for the planning and control of robots. FROST [8] supports trajectory optimization for bipedal robots, though it emphasizes offline trajectory generation rather than real-time controllers. Drake [9] offers a versatile platform for trajectory optimization and dynamics simulation, but its focus remains primarily on manipulation. OCS2 [10] implements efficient optimal control solvers for switched systems, but it requires significant integration effort to connect with whole-body controllers or reduced-order abstractions. Other optimal control tools, such as OpenSoT [11], Crocoddyl [12], and TSID [13], operate largely in isolation from dynamic locomotion planning. Collectively, these libraries form a rich ecosystem of building blocks; yet, researchers still assemble them manually, as there remains no unified framework that seamlessly couples reduced-order planning with whole-body control in a reproducible, extensible manner.\n\nTo address these challenges, we introduce RoMoCo—an open-source software toolbox designed to unify the development, evaluation, and deployment of ROM-based planners and WBC algorithms for bipedal and humanoid locomotion. The key contributions of this work are:\n\nA unified mathematical formulation of popular LIP-based planners (ALIP, H-LIP, MLIP, DCM), enabling their modular implementation and direct comparison.\n\nA modular software architecture that decouples planners, output mappings, whole-body controllers, and robot interfaces, allowing for rapid prototyping across different hardware.\n\nAn open-source library with integrated MuJoCo [14] simulation, demonstrated hardware deployments on multiple bipedal and humanoid platforms, and a fully available anonymized repository111https://anonymous.4open.science/r/RoMoCo-6E85 for review purposes.222The repository will be linked to a permanent public release upon acceptance.\n\nA comparative analysis of different ROM planners and whole-body controllers on the Cassie and Unitree G1, offering insights into their performance trade-offs.\n\n1. A unified mathematical formulation of popular LIP-based planners (ALIP, H-LIP, MLIP, DCM), enabling their modular implementation and direct comparison.\n\n2. A modular software architecture that decouples planners, output mappings, whole-body controllers, and robot interfaces, allowing for rapid prototyping across different hardware.\n\n3. An open-source library with integrated MuJoCo [14] simulation, demonstrated hardware deployments on multiple bipedal and humanoid platforms, and a fully available anonymized repository111https://anonymous.4open.science/r/RoMoCo-6E85 for review purposes.222The repository will be linked to a permanent public release upon acceptance.\n\n4. A comparative analysis of different ROM planners and whole-body controllers on the Cassie and Unitree G1, offering insights into their performance trade-offs.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效整合减少阶模型（ROM）与全身控制器（WBC）以实现双足和类人机器人运动控制？  \n2. 现有的开源工具缺乏统一框架，限制了不同机器人平台间的可重复性和快速原型开发。  \n\n【用了什么创新方法】  \nRoMoCo是一个开源C++工具箱，旨在统一ROM规划器和WBC算法的开发、评估和部署。它采用了模块化的软件架构，允许快速原型设计，并提供了对多种双足和类人机器人的硬件实验验证。通过比较不同的ROM规划器和全身控制器，RoMoCo展示了其在多种平台上的灵活性和性能，促进了对运动控制的深入理解。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Autonomous Elemental Characterization Enabled by a Low Cost Robotic Platform Built Upon a Generalized Software Architecture",
            "authors": "Xuan Cao,Yuxin Wu,Michael L. Whittaker",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19541",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19541",
            "arxiv_html_link": "https://arxiv.org/html/2509.19541v1",
            "abstract": "Despite the rapidly growing applications of robots in industry, the use of robots to automate tasks in scientific laboratories is less prolific due to lack of generalized methodologies and high cost of hardware.\nThis paper focuses on the automation of characterization tasks necessary for reducing cost while maintaining generalization,\nand proposes a software architecture for building robotic systems in scientific laboratory environment.\nA dual-layer (Socket.IO and ROS) action server design is the basic building block, which facilitates the implementation of a web-based front end for user-friendly operations and the use of ROS Behavior Tree for convenient task planning and execution.\nA robotic platform for automating mineral and material sample characterization is built upon the architecture, with an open source, low-cost three-axis computer numerical control gantry system serving as the main robot.\nA handheld laser induced breakdown spectroscopy (LIBS) analyzer is integrated with a 3D printed adapter, enabling automated 2D chemical mapping. We demonstrate the utility of automated chemical mapping by scanning of the surface of a spodumene-bearing pegmatite core\nsample with a 1071-point dense hyperspectral map acquired at a rate of 1520 bits per second.\nAutomated LIBS scanning enables controlled chemical quantification in the laboratory that complements field-based measurements acquired with the same handheld device, linking resource exploration and processing steps in the supply chain for lithium-based battery materials.",
            "introduction": "The rapid development of robotics in recent years has given a boost of its applications in industry, such as machine tending [1], palletizing [2], and assembly [3]. The operational stock of industrial robots worldwide increased from about 1.3 million in 2013 to 4.3 million in 2023 [4].\n\nSimilarly, robotic automation in research laboratories has become an emerging field, since “Robotics and automation can enable scientific experiments to be conducted faster, more safely, more accurately, and with greater reproducibility, allowing scientists to tackle large societal problems in domains such as health and energy on a shorter timescale” [5].\nAlthough there have been successful applications of robotic automation in laboratories [6, 7, 8, 9, 10], the use of robots to automate laboratory operations is still limited in general due to the automation gap caused by the variety of tasks and protocols [11], ultimately resulting in high costs.\n\nThis work sheds some light on the automation of characterization tasks in labs, which determine the properties, composition, and behavior of substances (e.g. spectrometry, microscopy, thermal analysis, etc.), and hence are essential in scientific research.\nOne common pattern in characterization tasks is sample-move-instrument-stay (SMIS), where a sample is placed to a specific position for an analytical instrument to start working. Automating this pattern using robots requires precise pick-and-place operations and enough degrees of freedom.\n\nBy contrast, this paper focuses on the sample-stay-instrument-move (SSIM) pattern, where an instrument is held by a robot and moved around a sample during characterization. Automating this pattern does not require pick-and-place operations since the instrument is mounted on the robot all the time. Sample standardization, such as positioning on a 2-d horizontal plane, reduces the robot’s required degrees of freedom to reach the samples, which could potentially lower the hardware cost.\n\nTowards this end, this paper introduces a robotic platform for automating SSIM characterization tasks for mineral and material samples. The platform consists of: (1) a low-cost 3D (translational movements in X, Y, and Z directions) gantry system commonly used in traditional computer numerical control (CNC) machining as the primary robot, (2) an analytical instrument mounted to the gantry system for sample characterization, and (3) a stereo camera capable of depth sensing for locating samples to be measured. All components, and samples to be measured, are placed on a benchtop. The general workflow consists of the following steps: (1) a sample location is either predefined, or else identified by the camera; (2) the gantry system takes the analytical instrument to the sample location; (3) the instrument starts characterization and collects raw data; (4) the raw data are processed and optional feedback is generated.\n\nThe core of the software is a generalized custom-designed architecture for automation systems in laboratory environments. The basis of the architecture is a dual-layer action server design for every hardware component, which monitors incoming operation requests through both Socket.IO [12] and Robot Operating System (ROS) [13] communication protocols and commands the hardware to act accordingly.\nOn top of all action servers lies a Behavior Tree (BT) [14] which orchestrates the hardware components by interacting with their action servers to automate the characterization workflow.\nA web-based front end is developed to ensure user-friendly operations of the platform, including both manual control of each individual hardware and execution of the BT.\n\nTo showcase the efficacy of the platform, we integrate a handheld laser induced breakdown\nspectroscopy (LIBS) analyzer to the gantry system and use the platform to perform dense LIBS scanning on the surface of a spodumene-bearing pegmatite core sample with 1071 measurement points, each containing optical emission spectra between 190 nm and 950 nm with 0.03 nm resolution, corresponding to 22800 data channels per measurement. The resulting 2×1072\\times 10^{7} data are automatically quantified using a custom algorithm, yielding spatially-resolved, comprehensive chemical analysis with parts-per-million levels for most chemical elements.\nThe automated LIBS scanning (1) frees researchers from tedious operations,\n(2) accelerates LIBS characterization by at least 3 times the rate of manual operations, and (3) provides crucial information about downstream processing chemistry.\n\nThis paper makes three contributions.\nFirst, a generalized software architecture for building robotic automation systems in scientific laboratory environment is proposed.\nSecond, a low-cost gantry system commonly used in CNC machining is shown to be capable of working as a robot for the automation of SMIS characterization tasks.\nThird, automated dense LIBS scanning using the developed robotic platform and automatic data reduction is achieved.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何降低科学实验室中自动化任务的成本和复杂性？  \n2. 如何实现对矿物和材料样本的自动化表征任务？  \n3. 如何设计通用的软件架构以支持实验室机器人系统的开发？  \n\n【用了什么创新方法】  \n本研究提出了一种通用的软件架构，采用双层（Socket.IO和ROS）动作服务器设计，支持实验室环境中的机器人自动化。通过集成手持激光诱导击穿光谱（LIBS）分析仪，构建了一个低成本的三轴计算机数控（CNC）龙门系统，实现了对矿物样本的自动化化学映射。该平台能够以1520比特每秒的速度扫描样本，生成1071点的密集高光谱图，显著提高了实验效率，减少了人工操作的繁琐性，并提供了重要的下游处理化学信息。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot",
            "authors": "James Avtges,Jake Ketchum,Millicent Schlafly,Helena Young,Taekyoung Kim,Allison Pinosky,Ryan L. Truby,Todd D. Murphey",
            "subjects": "Robotics (cs.RO)",
            "comment": "Published at IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.19525",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19525",
            "arxiv_html_link": "https://arxiv.org/html/2509.19525v1",
            "abstract": "Closed-loop control remains an open challenge in soft robotics. The nonlinear responses of soft actuators under dynamic loading conditions limit the use of analytic models for soft robot control. Traditional methods of controlling soft robots underutilize their configuration spaces to avoid nonlinearity, hysteresis, large deformations, and the risk of actuator damage. Furthermore, episodic data-driven control approaches such as reinforcement learning (RL) are traditionally limited by sample efficiency and inconsistency across initializations. In this work, we demonstrate RL for reliably learning control policies for dynamic balancing tasks in real-time single-shot hardware deployments. We use a deformable Stewart platform constructed using parallel, 3D-printed soft actuators based on motorized handed shearing auxetic (HSA) structures. By introducing a curriculum learning approach based on expanding neighborhoods of a known equilibrium, we achieve reliable single-deployment balancing at arbitrary coordinates. In addition to benchmarking the performance of model-based and model-free methods, we demonstrate that in a single deployment, Maximum Diffusion RL is capable of learning dynamic balancing after half of the actuators are effectively disabled, by inducing buckling and by breaking actuators with bolt cutters. Training occurs with no prior data, in as fast as 15 minutes, with performance nearly identical to the fully-intact platform. Single-shot learning on hardware facilitates soft robotic systems reliably learning in the real world and will enable more diverse and capable soft robots.",
            "introduction": "Soft robots offer the potential for improved adaptability and safety compared to their rigid counterparts due to compliance and material redundancies [1]. However, soft-actuated robots are traditionally difficult to control due to their nonlinear dynamics and high degrees of freedom [2, 3]. Soft-actuated robot dynamics—which may be impractical or impossible to model classically—are often stochastic and vary with a number of factors including strain, actuator fatigue, and even manufacturing processes [4, 5].\n\nData-driven methods such as reinforcement learning (RL) have been used to circumvent many of the modeling difficulties associated with controlling soft robots [6]. Prior applications of RL in this domain have explored various approximations of soft actuator dynamics, including piecewise constant curvature models, Cosserat rod models, and rigid NN-link pendulum approximations [7, 8, 9, 5, 10]. However, lower-dimensional approximations may oversimplify the dynamics of highly nonlinear actuators, which may be compounded by external forces and changes to the dynamics during use. Additionally, models parameterized using these approximations are almost exclusively used to train control policies in simulation, requiring dedicated approaches to address the sim-to-real gap. Existing approaches also often assume quasi-static behavior and focus on relatively simple tasks such as reaching or tracing with continuum arms.\n\nWhen learning approaches are applied to soft robot control, current works often constrain the size or dimensionality of the configuration space to regions with smaller deformations, or where their dynamics are approximately linear [11, 7, 9]. This may include directly constraining actuator outputs, choices of robot orientation, and evaluating control approaches only on undamaged, unloaded actuators.  While these techniques are effective, utilizing the full variety of soft robots’ diverse configuration spaces despite their stochastic and nonlinear dynamics that may be impossible to simulate is paramount for producing behaviors that involve buckling, large deformations, or actuator breakages. Exploiting these unique properties of soft robots has the potential to enable more diverse task learning, biomimetic behaviors, and adaptation to loading conditions or damage.\n\nOur experimental platform is a six degree-of-freedom (DoF), soft-actuated parallel mechanism with a structure similar to that of a Stewart platform [12]. The struts of the platform are motorized soft actuators based on 3D-printed Handed Shearing Auxetics (HSAs) that lengthen and shorten upon the turn of a servo motor [13, 14]. In this work, we balance a sliding puck on the platform, at both the center and at arbitrary coordinates. We utilize deep RL to train closed-loop controllers and benchmark multiple model-based and model-free RL frameworks. Our training occurs fully on-hardware without simulation or bootstrapped data.\n\nLearning in real-time on hardware has its challenges [15, 16], some unique to soft robotics. The limited lifespan of flexible, strain-dependent actuators makes sample efficiency and adaptability valuable attributes  for minimizing and adapting to actuator damage throughout training [9]. Our approach combines a classical kinematics model for a rigid Stewart platform with a deep RL controller to efficiently learn nonlinear control policies for the system. We train all of our policies using single-shot reinforcement learning, a non-episodic problem formulation for continual learning without any environment resets. Learning in a singular real-time episode may be necessary for a variety of tasks well-suited to soft robotics—such as functioning in extreme environments or human interactions—where leveraging prior data, simulations, and reset routines may be undesirable or impossible. While episodic soft robot RL has been implemented on-hardware in works such as [9, 17, 18], and reset-free hardware learning has been implemented in works such as [19, 20], to the best of our knowledge this work is the first to accomplish all of the above in a soft-actuated robot.\n\nAn additional challenge with learning a balancing task in single-shot episodes is that reliably learning control policies is not necessarily an inevitable outcome in reasonable timescales—catastrophic failure modes do exist. While the balancing puck is constrained from falling off the platform, without access to a reset routine, either learned or programmed, the puck can become stuck in a corner during training, providing little to no variance in data required for learning. This is especially  relevant when learning to balance at arbitrary coordinates on the platform, where the reward landscape is parameterized by an observable setpoint that may not be near the platform’s center. Other works conducting soft-actuated RL such as [9, 17, 8, 18] structure their experiments such that providing zero control results in an intervention-free environment reset—such as orienting a continuum arm downwards—stable states such as these do not exist in our experimental platform with an unarticulated sliding puck.\n\nTo overcome this challenge, we employ a curriculum learning approach based on expanding neighborhoods of a known equilibrium: the platform center. With this we reliably learn to balance in a single deployment, without it the task is impossible to accomplish consistently, or at all.\n\nFurthermore, we show how RL can learn to balance the puck despite major changes to the robot’s dynamics during training. We buckle half of the HSAs, introducing singularities, hysteresis, and an out-of-distribution configuration. We also damage the platform by cutting through half of the HSAs with bolt cutters. Despite these alterations, RL attains evaluation performance indistinguishable to the default case.\n\nThe contributions of this work are:\n\nDemonstrations and benchmarking of single-shot learning for dynamic tasks on a soft robot,\n\nDemonstrations and benchmarking of single-shot learning for dynamic tasks on a soft robot,\n\nIntroduction of a curriculum learning procedure to improve single-shot learning outcomes, and\n\nAn illustration of a setting where RL can accommodate changing dynamics such as buckling or breaking actuators during single-shot training.\n\n1. Demonstrations and benchmarking of single-shot learning for dynamic tasks on a soft robot,\n\n2. Introduction of a curriculum learning procedure to improve single-shot learning outcomes, and\n\n3. An illustration of a setting where RL can accommodate changing dynamics such as buckling or breaking actuators during single-shot training.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在软机器人中实现闭环控制以应对非线性动态和高自由度问题？  \n2. 如何提高强化学习在动态平衡任务中的样本效率和一致性？  \n3. 如何在没有模拟或先前数据的情况下实现实时单次学习？  \n\n【用了什么创新方法】  \n本研究采用了一种基于已知平衡点邻域扩展的课程学习方法，以提高软机器人在动态平衡任务中的单次学习效果。通过在真实硬件上进行训练，系统能够在不重置环境的情况下快速学习控制策略。即使在半数执行器失效的情况下，强化学习仍能实现与完整平台相当的性能，展示了软机器人在动态环境下的适应能力和学习效率。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Bioinspired SLAM Approach for Unmanned Surface Vehicle",
            "authors": "Fabio Coelho,Joao Victor T. Borges,Paulo Padrao,Jose Fuentes,Ramon R. Costa,Liu Hsu,Leonardo Bobadilla",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19522",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19522",
            "arxiv_html_link": "https://arxiv.org/html/2509.19522v1",
            "abstract": "This paper presents OpenRatSLAM2, a new version of OpenRatSLAM—a bioinspired SLAM framework based on computational models of the rodent hippocampus. OpenRatSLAM2 delivers low-computation-cost visual-inertial based SLAM, suitable for GPS-denied environments. Our contributions include a ROS2-based architecture, experimental results on new waterway datasets, and insights into system parameter tuning. This work represents the first known application of RatSLAM on USVs. The estimated trajectory was compared with ground truth data using the Hausdorff distance. The results show that the algorithm can generate a semimetric map with an error margin acceptable for most robotic applications.",
            "introduction": "The increasing use of unmanned surface vehicles (USVs) for scientific, military, and commercial purposes requires the development of robust navigation systems [1]. Common applications include oceanographic data collection, oil and gas exploration, environmental surveys, mine countermeasures, and surveillance [2, 3]. To autonomously perform such tasks, a mobile robot must be able to localize itself within its environment [4].\n\nCommon approaches include combining GPS with an inertial measurement unit (IMU) and Kalman filtering algorithms for state estimation in USVs [5]–[6]. However, these methods fail in GPS-denied environments where satellite signals are obstructed [7]. Moreover, GPS signals are vulnerable to various disruptions and cyberattacks, including jamming and spoofing [8].\n\nTo address these limitations, Simultaneous Localization and Mapping (SLAM) is an alternative that enables a vehicle to build a map of its surroundings while estimating its position relative to that map. Many existing SLAM implementations rely on computationally intensive sensors, such as LiDAR or depth cameras. These sensors often require high processing and storage demands, making them less suitable for real-time applications on resource-constrained platforms [9].\n\nMotivated by recent advances in neuroscience, several brain-inspired SLAM systems have been proposed [10]. A pioneering work is the RatSLAM framework, a biologically inspired SLAM algorithm based on computational models of the rodent hippocampus. RatSLAM employs a Continuous Attractor Neural Network (CANN) to construct a cognitive map of an environment using only a low-resolution monocular camera [11]. Compared to probabilistic SLAM approaches, RatSLAM offers reduced computational complexity and efficient memory usage and is well-suited for both indoor and large-scale outdoor mapping.\n\nIn recent years, several RatSLAM-based variants have been proposed [12]. For instance, [13] introduced a MATLAB-based RatSLAM implementation in a rat robot, demonstrating its capability to learn spatial layouts. However, the system’s performance was too slow for real-time operation in large environments. Another approach, OpenRatSLAM, was proposed as an open-source RatSLAM implementation based on the Robot Operating System (ROS) [14]. This version benefits from ROS’s node parallelization and modular integration with diverse robotic architectures [12].\n\nThe emergence of ROS 2 as the dominant middleware for new robotic systems has created integration challenges, as OpenRatSLAM was primarily developed for ROS 1. In this context, xRatSLAM was developed as an extensible, parallel, open-source framework implemented as a C++ library to facilitate the development and testing of RatSLAM algorithm variants [12].\n\nWhile most applications have targeted ground robots, RatSLAM-inspired algorithms have also been explored in other domains. One aerial application, NeuroSLAM, is a neuro-inspired SLAM system with four degrees of freedom (4DoF), based on computational models of 3D grid cells and multilayered head direction cells. It integrates visual and self-motion cues through a dedicated vision system [15]. In underwater environments, two RatSLAM-based systems have been developed: DolphinSLAM [16], a 3D variant, and a more recent system that implements Pose Cells using Spiking Neural Networks (SNNs) [17]. Both were developed using ROS 1 distributions, which are now deprecated and unsupported.\n\nIn summary, the contributions of this work are as follows:\n\nA new version of OpenRatSLAM, implemented using ROS 2 Rolling, referred to as OpenRatSLAM2. This version benefits from ROS 2’s advantages, including improved maintainability and easier integration with modern tools. Additionally, the communication middleware is more robust than ROS 1, providing streamlined transition from simulation to physical robot deployment;\n\nTo the best of our knowledge, this is the first application of RatSLAM to a USV;\n\nA visual-inertial dataset collected using a USV for evaluating SLAM performance in aquatic environments.\n\n1. A new version of OpenRatSLAM, implemented using ROS 2 Rolling, referred to as OpenRatSLAM2. This version benefits from ROS 2’s advantages, including improved maintainability and easier integration with modern tools. Additionally, the communication middleware is more robust than ROS 1, providing streamlined transition from simulation to physical robot deployment;\n\n2. To the best of our knowledge, this is the first application of RatSLAM to a USV;\n\n3. A visual-inertial dataset collected using a USV for evaluating SLAM performance in aquatic environments.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在GPS-denied环境中实现低计算成本的SLAM？  \n2. 如何将RatSLAM算法应用于无人水面车辆（USV）？  \n3. 如何提高SLAM系统在水域环境中的性能评估？  \n\n【用了什么创新方法】  \n本研究提出了OpenRatSLAM2，一个基于ROS 2的生物启发式SLAM框架，利用低分辨率单目相机和视觉惯性传感器进行导航。通过在水域环境中收集的数据集进行实验，结果表明该算法能够生成具有可接受误差范围的半度量地图，适合大多数机器人应用。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion",
            "authors": "Najeeb Ahmed Bhuiyan,M. Nasimul Huq,Sakib H. Chowdhury,Rahul Mangharam",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19521",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19521",
            "arxiv_html_link": "https://arxiv.org/html/2509.19521v1",
            "abstract": "Gesture-based control for mobile manipulators faces persistent challenges in reliability, efficiency, and intuitiveness. This paper presents a dual-hand gesture interface that integrates TinyML, spectral analysis, and sensor fusion within a ROS framework to address these limitations. The system uses left-hand tilt and finger flexion, captured using accelerometer and flex sensors, for mobile base navigation, while right-hand IMU signals are processed through spectral analysis and classified by a lightweight neural network. This pipeline enables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3 manipulator. By supporting simultaneous navigation and manipulation, the framework improves efficiency and coordination compared to sequential methods. Key contributions include a bimanual control architecture, real-time low-power gesture recognition, robust multimodal sensor fusion, and a scalable ROS-based implementation. The proposed approach advances Human–Robot Interaction (HRI) for industrial automation, assistive robotics, and hazardous environments, offering a cost-effective, open-source solution with strong potential for real-world deployment and further optimization.",
            "introduction": "Robot remote control underpins a wide range of transformative technologies, enabling precise operations across domains like space exploration [1, 2, 3], disaster response [4, 5], military operations [6, 7], and assistive robotics [8, 9]. By harnessing human input to guide robotic systems, this field has unlocked capabilities that extend beyond manual reach, adapting to environments where direct intervention is impractical or hazardous. Gesture-based control emerges as a natural evolution of this concept, leveraging intuitive human movements to command robots with minimal training. This approach proves particularly effective in scenarios requiring immediate, instinctual interaction, such as directing medical neuro-arms with laser scalpels for pinpoint accuracy [10] or steering assistive devices to support daily tasks. Its appeal lies in bridging the gap between human intent and robotic action, offering a direct and expressive interface.\n\nBuilding on this foundation, mobile manipulators—systems that pair a mobile base with a robotic arm—amplify the potential of gesture control, combining locomotion with manipulation for versatile applications. In industrial automation, they streamline assembly lines [11, 12]; in healthcare, they assist with patient care [13, 14]; and in search-and-rescue missions, they navigate complex terrains [15]. Their utility extends to extreme conditions: in coal mines, gesture-controlled robots dig under searing heat where workers cannot endure [16]; in bomb defusing, they shield human operators from lethal risks [17]; and in nuclear reactors, they manage radioactive waste without exposing personnel [18]. This synergy of mobility and dexterity makes mobile manipulators a cornerstone of modern robotics. However, gesture-based control, despite its promise, grapples with practical hurdles—unreliable recognition, environmental sensitivity, and resource intensity—that impede its widespread adoption, necessitating innovative solutions.\n\nThese limitations arise from fundamental flaws in existing gesture recognition methods, each posing distinct challenges to effective HRI. Vision-based systems, such as Microsoft’s Kinect or Leap Motion, depend on cameras that falter under variable lighting, occlusions, or when users move beyond a narrow range, disrupting gesture detection [19, 20]. Similarly, approaches employing Deep Neural Networks (DNNs) or Convolutional Neural Networks (CNNs) deliver high accuracy but at the cost of substantial power and computational demands, rendering them impractical for lightweight or real-time systems [21, 22]. Electromyography (EMG)-based methods offer an alternative by capturing muscle signals, with studies like one using the Myo Armband achieving 78.94% accuracy across 10 gestures [23], yet their reliability wanes as muscle fatigue degrades signal quality over time [24]. Such challenges are especially pronounced in rehabilitation and mobility assistance, where intuitive, dependable control is paramount. Stroke, a leading cause of disability per the World Stroke Organization [25], often induces hemiparesis, impairing one side of the body, while paralysis afflicts millions globally, severely restricting mobility. Early robot-assisted therapy with patient-driven devices can enhance recovery, and related efforts, such as a Robot Wheelchair (RW) using sensor-based hand gestures via gloves or handles, underscore the demand for accessible interfaces in these contexts [26].\n\nTo address these shortcomings, a more robust system is essential—one that overcomes environmental constraints, reduces resource demands, and maintains consistency across diverse users and conditions. Wearable sensor-based approaches, enhanced by TinyML and edge computing, provide a compelling solution, offering real-time, low-power performance adaptable to dynamic environments [27, 28, 29]. By integrating sensor fusion—combining data from accelerometers, IMUs, and flex sensors—these systems achieve greater accuracy and robustness, circumventing the pitfalls of vision, neural networks, and EMG [30, 31]. This paradigm shift not only improves technical feasibility but also aligns with the urgent need for intuitive HRI in rehabilitation, hazardous operations, and beyond, setting the stage for advanced robotic control frameworks.\n\nInspired by these insights, we propose a dual-hand gesture control system for a ROS-based mobile manipulator, harnessing TinyML and sensor fusion to deliver seamless, efficient operation. The left hand governs the mobile base using an Arduino Nano equipped with an accelerometer and two flex sensors—tilting to command directional movement (forward, backward, left, right) and flexing to fine-tune acceleration or deceleration. Concurrently, the right hand directs the 7-DOF Kinova Gen3 manipulator via an Arduino Nano 33 BLE Sense with TinyML and an LSM9DS1 IMU, mapping distinct gestures: “Forward-Backward” to a pickup pose, “Left-Right” to placement on the mobile base, “Flat Rectangle” to table placement, “Rectangle” to elevated placement, “Circle” to alternative motion, and “Up-Down” to homing. Built on ROS (ros2-jazzy), the system simulates the mobile base in Gazebo Harmonic and controls the manipulator via RViz with MoveIt!. Our primary contributions include:\n\nA dual-hand gesture control architecture that separates mobile base and manipulator functions, enhancing coordination and natural interaction.\n\nA TinyML-based gesture recognition system for real-time, low-power classification of complex manipulator gestures.\n\nA sensor fusion approach combining accelerometer and flex sensor data for precise mobile base control.\n\nA comprehensive ROS-based simulation framework integrating a custom mobile base and the Kinova Gen3 manipulator.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高移动操控器的手势识别的可靠性和效率。  \n2. 如何实现双手手势控制以同时导航和操作机器人。  \n3. 如何克服现有手势识别方法在环境敏感性和资源消耗上的不足。  \n\n【用了什么创新方法】  \n本文提出了一种基于TinyML和传感器融合的双手手势控制系统，利用左手的倾斜和手指弯曲控制移动底座，右手通过IMU信号进行手势识别。该系统实现了实时、低功耗的手势识别，显著提高了移动操控器的协调性和效率。通过ROS框架的集成，系统在复杂环境中表现出更高的准确性和鲁棒性，推动了人机交互在工业自动化和助理机器人中的应用。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Supercomputing for High-speed Avoidance and Reactive Planning in Robots",
            "authors": "Kieran S. Lachmansingh,José R. González-Estrada,Ryan E. Grant,Matthew K. X. J. Pan",
            "subjects": "Robotics (cs.RO); Distributed, Parallel, and Cluster Computing (cs.DC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19486",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19486",
            "arxiv_html_link": "https://arxiv.org/html/2509.19486v1",
            "abstract": "This paper presents SHARP (Supercomputing for High-speed Avoidance and Reactive Planning), a proof-of-concept study demonstrating how high-performance computing (HPC) can enable millisecond-scale responsiveness in robotic control. While modern robots face increasing demands for reactivity in human–robot shared workspaces, onboard processors are constrained by size, power, and cost. Offloading to HPC offers massive parallelism for trajectory planning, but its feasibility for real-time robotics remains uncertain due to network latency and jitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator must dodge high-speed foam projectiles. Using a parallelized multi-goal A* search implemented with MPI on both local and remote HPC clusters, the system achieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote,  300 km away), with avoidance success rates of 84% and 88%, respectively. These results show that when round-trip latency remains within the tens-of-milliseconds regime, HPC-side computation is no longer the bottleneck, enabling avoidance well below human reaction times. The SHARP results motivate hybrid control architectures: low-level reflexes remain onboard for safety, while bursty, high-throughput planning tasks are offloaded to HPC for scalability. By reporting per-stage timing and success rates, this study provides a reproducible template for assessing real-time feasibility of HPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable pathway toward dependable, reactive robots in dynamic environments.",
            "introduction": "Modern robots are increasingly expected to operate in unstructured and dynamic environments, often in close collaboration with humans. These scenarios demand not only accurate planning but also highly reactive control: a robot must perceive changes and adapt within tens of milliseconds to avoid unsafe or undesirable interactions. Traditionally, such responsiveness has relied on local computing platforms (commodity CPUs or GPUs embedded on the robot or on nearby workstations), which are constrained by size, power, and cost [1, 2]. As robots integrate more resource-intensive artificial intelligence (AI) and machine learning models, these platforms are reaching their limits, creating bottlenecks that threaten truly responsive human–robot interaction (HRI).\n\nHigh-performance computing (HPC), or supercomputing, offers an alternative. HPC systems perform billions of computations per second and excel at large-scale graph search and optimization—problems central to robotics tasks such as inverse kinematics, trajectory planning, and collision avoidance [3, 4]. In principle, HPC can deliver solutions orders of magnitude faster than local compute, enabling behaviours that are more optimal, adaptive, and reactive.\n\nThe main obstacle is latency [5]. HPC resources are typically non-local and accessed via network connections that introduce delays. In time-critical robotics, where milliseconds matter, the central question is whether the raw compute advantage of HPC can overcome communication costs and still enable real-time action [6, 7].\n\nThis paper addresses that question through SHARP—Supercomputing for High-speed Avoidance and Reactive Planning—a proof-of-concept system that evaluates whether HPC offloading can support real-time avoidance. To stress-test responsiveness, we examine a deliberately stringent scenario: a 7-DOF manipulator tasked with dodging high-speed foam projectiles. This setup serves two purposes. First, it creates a controlled, time-critical environment in which HPC planning must deliver trajectories fast enough to influence live robot behaviour. Second, it provides a proxy for broader applications such as collision avoidance in dynamic, human–robot shared workspaces, where comparable millisecond-scale reaction budgets apply.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用高性能计算（HPC）实现机器人在动态环境中的实时反应与规划？  \n2. 网络延迟是否会影响HPC在机器人控制中的实时性和有效性？  \n3. 传统本地计算平台在处理复杂机器人任务时的瓶颈是什么？  \n\n【用了什么创新方法】  \n本研究提出了SHARP系统，通过将高性能计算（HPC）应用于机器人控制，进行高速度规避和反应规划。采用并行化的多目标A*搜索算法，结合MPI在本地和远程HPC集群上进行评估。在应对高速度泡沫投射物的压力测试中，系统实现了平均规划延迟为22.9毫秒（本地）和30.0毫秒（远程），规避成功率分别为84%和88%。这些结果表明，当往返延迟保持在十毫秒以内时，HPC计算不再是瓶颈，从而使机器人能够在低于人类反应时间的情况下实现有效规避。SHARP的结果为混合控制架构提供了动力，低级反应留在机器人本体以确保安全，而高吞吐量的规划任务则可离线处理以实现可扩展性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Crater Observing Bio-inspired Rolling Articulator (COBRA)",
            "authors": "Adarsh Salagame,Henry Noyes,Alireza Ramezani,Eric Sihite,Arash Kalantari",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19473",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19473",
            "arxiv_html_link": "https://arxiv.org/html/2509.19473v1",
            "abstract": "NASA aims to establish a sustainable human basecamp on the Moon as a stepping stone for future missions to Mars and beyond. The discovery of water ice on the Moon’s craters located in permanently shadowed regions, which can provide drinking water, oxygen, and rocket fuel, is therefore of critical importance. However, current methods to access lunar ice deposits are limited. While rovers have been used to explore the lunar surface for decades, they face significant challenges in navigating harsh terrains, such as permanently shadowed craters, due to the high risk of immobilization. This report introduces COBRA (Crater Observing Bio-inspired Rolling Articulator), a multi-modal snake-style robot designed to overcome mobility challenges in Shackleton Crater’s rugged environment. COBRA combines slithering and tumbling locomotion to adapt to various crater terrains. In snake mode, it uses sidewinding to traverse flat or low inclined surfaces, while in tumbling mode, it forms a circular barrel by linking its head and tail, enabling rapid movement with minimal energy on steep slopes. Equipped with an onboard computer, stereo camera, inertial measurement unit, and joint encoders, COBRA facilitates real-time data collection and autonomous operation. This paper highlights COBRA’s robustness and efficiency in navigating extreme terrains through both simulations and experimental validation.",
            "introduction": "Robots have become essential tools in our scientific exploration of space and celestial bodies. Equipped with advanced sensors and designed to withstand severe conditions, they safely explore, sample, and analyze, yielding invaluable insights that would otherwise remain unattainable. Traditionally, robots for space exploration have relied on wheeled locomotion [1]. However, recent innovations have shifted this paradigm. Ingenuity, the Mars Helicopter [2], has successfully demonstrated the first aerial locomotion on another planet, while alternate surface mobility systems such as Dragonfly, EELS, and DuAxel [3, 4, 5] are in advanced stages of development. These solutions are typically tailored to address the locomotion challenges in specific regions of interest, such as recurring slope lineae on Mars for DuAxel or the icy moon Enceladus for EELS.\n\nHowever, some regions of interest, particularly the Moon’s permanently shadowed regions (PSRs), are still unexplored due to limitations of existing surface mobility solutions. In this work, we introduce our bio-inspired mobile robot, COBRA, Crater Observing Bio-inspired Rolling Articulator, designed to address challenges unmet by current solutions for energy-efficient exploration of steep and unknown craters in the Moon’s PSRs. As a case study, we focus on Shackleton Crater (see Fig. 2) at the lunar South Pole, a site of immense scientific potential for water ice discovery [6], but significant challenges for existing exploration methods. We present the envisioned mission scenario and the locomotion solution that led to COBRA winning the 2022 Breakthrough, Innovative, and Game-changing (BIG) Idea Challenge [7], an initiative under NASA’s Space Technology Mission Directorate’s (STMD’s) Game Changing Development Program (GCD), for alternative lunar mobility solutions. This report summarizes the design, dynamical modeling, contact-rich and optimization-based gait discovery, as well as the numerical and experimental evaluation of COBRA’s mobility in unstructured, contested environments. Specifically, the technical contributions of this work are:  (1) Hardware design, including head, tail, and body module mechanical and electronics design. (2) Design of the head-tail docking mechanism to substantiate transitions between crawling and tumbling. (3) Numerical modeling of crawling and tumbling locomotion feats based on Lagrange and mixed Hamilton-Lagrange (partitioned state-space model for tumbling) dynamics. (4) Contact-implicit optimization-based gait discovery and joint motion planning. (5) Simulation and experimental validation of hex-ring tumbling, spiral tumbling, sidewinding, lateral rolling, and vertical undulation. (6) Demonstration of field-tested locomotion modes in dusty, steep, and bumpy environments, with self-sustained dust mitigation. (7) Demonstration of full deployment from a lander, including unmanned transitions between various locomotion modes in experiments.\n\nWith the Artemis program, NASA and collaborating space agencies aim to revitalize lunar and space exploration. One Artemis objective is to create a sustainable human base camp on the Moon with the hope to then propel further missions to Mars and beyond. Accomplishing such a feat is contingent on In-Situ Resource Utilization (ISRU) on the Moon [8]. One resource of significant interest is lunar water ice, which can potentially supply drinking water, oxygen, and rocket propellant. However, the means to access the ice deposits on the Moon still require additional development [9, 10, 11]. In 2018, NASA confirmed the presence of water ice on the Moon’s poles, concentrated chiefly in PSRs [12]. The near-permanent lack of sunlight in these regions results in extremely low temperatures (as low as -238 °C) and allows for the accumulation of water ice and other volatiles [13].\nOne such PSR is Shackleton Crater.\n\nWhile the presence of water ice in Shackleton Crater is evident [6], there are no precise measurements of its quantity or chemical composition. This detailed information, along with topographic maps of crater terrain, is critical to initiate targeted mining operations for ISRU. However, acquiring these measurements requires proximate investigations in extreme lunar environments that pose significant mobility challenges to exploration platforms.\n\nHigh Porosity Regolith Surface: The surface of the lunar South Pole is characterized by high porosity regolith that poses significant locomotion challenges [14]. Due to the high porosity, traditional wheeled rovers suffer sinkage and slipping, reducing energy efficiency and increasing the risk of immobilization. The high porosity lunar dust is also abrasive and invades machinery due to its particulate nature [15].\n\nTerrain Profile: Traversing the immense slopes to reach all areas of scientific interest inside the crater also poses significant challenges. Shackleton Crater is a massive geographic feature 21 kilometers in diameter. The crater slope leading to the crater floor has an average slope of 30.5 degrees and covers a horizontal distance of approximately 8 kilometers. This steep crater slope is difficult to both ascend and descend. On the descent, there is limited traction due to the lower lunar gravity further reducing normal force, and the regolith substrate being more prone to yield, leading to slipping and sinkage.\n\nThis mobility challenge also extends to the uneven crater floor and surrounding areas. The crater slope and floor have a root mean square (RMS) surface roughness of approximately 1 meter. This makes it difficult for wheeled systems to traverse, as the height of the obstacle they can overcome is limited by their wheel diameter. Boulder fields outside the crater, the unknown mechanical properties of regolith with ice, and a lack of detailed topography of the crater floor further raise the requirements for an adaptable system.\n\nA large of number of studies have investigated the mechanics of wheeled and legged locomotion in such conditions, using both analytical terramechanics models and experimental proxies. For instance, Ishigami et al. [16] and Shrivastava et al. [17] analyze wheel-soil interactions using lunar simulants, including deformable media like poppy seeds, to understand sinkage and traction loss. Similarly, recent work by Kolvenbach et al. [18] and Karsai et al. [19] explores how legged robots navigate granular slopes, addressing challenges in slip, footing stability, and dynamic control. However, the systematic consideration of substrate phenomena–such as mass wasting, sinkage, and traction loss–during inclined traversal remains largely unexplored.\n\nLack of sunlight: Shackleton Crater is a PSR, leading to two challenges: power generation and near-absolute zero temperature. Traversal of the crater is limited due to the lack of solar power. Therefore, large distances must be covered with minimal power consumption, or systems must rely on heavy power generation mechanisms such as radioisotope thermoelectric generators (RTGs). Low temperatures interfere with mechanisms that rely on lubricants and liquid batteries. This results in less efficient power systems.\n\nTo overcome locomotion challenges in the Moon’s hostile environment, a robotic system must be designed to minimize sinkage and slippage, while incorporating features to prevent immobilization. It must traverse long distances in an energy-efficient manner while maintaining passive and active regolith mitigation strategies and be able recover from, or continue to operate with, component failures.\n\nCurrent state-of-the-art wheeled rovers [20, 21, 22, 23] shown in Fig. 1 struggle on steep inclines due to inadequate traction on the porous regolith and decreased stability from the reduced normal force. Hybrid systems such as SherpaTT [24] and Scarab [25] partially address this issue by incorporating articulated legs that can lift the wheels off the ground to reposition them. More advanced multi-modal platforms like Robosimian [26] combine wheels with fully articulated legs, enabling both wheeled and legged locomotion, which improves steep slope traversal but remains slow and energy-intensive. An alternative approach is to use tether-assisted mobility, implemented in the DuAxel system [27], where one half of the rover anchors itself while the other rappels down steep terrain using a tether. While effective in certain terrains, the scale and steepness of craters such as Shackleton limit the practicality of tethered solutions.\n\nLegged systems, such as ETH Zurich’s Spacebok [40], JPL’s Llama [38], and ANYmal [39], offer another solution by allowing precise control of foot placement, enabling stable locomotion on loose surfaces. Despite their adaptability, these systems are generally energy-intensive. To address this, recent works like SpaceHopper [37] have explored hopping-based locomotion to leverage the Moon’s low gravity for energy-efficient travel over long distances. For steeper slopes, climbing robots equipped with micro-grippers, such as JPL’s LEMUR 3 [41], have been developed, but these designs are tailored for hard, rocky surfaces and are unsuitable for the soft, granular lunar regolith.\n\nFlying systems can traverse large distances quickly, as demonstrated by Ingenuity [48] on Mars. NASA’s Dragonfly [3], the Mars Science Helicopter [29] and the Sample Return Helicopter [36] are further exploring flight-based solutions for planetary exploration. However, replicating this approach on the Moon, which lacks an atmosphere, would require the use of ion or hydrogen-based propulsion systems. These technologies are extremely energy-intensive, making frequent recharging or carrying significant fuel reserves necessary for sustained operation. Thruster-assisted multi-modal designs like LEONARDO [32], which combine hopping and flight, present a potential compromise, but their exhaust may alter the chemical composition of the lunar regolith, complicating scientific investigations. Similarly, other multi-modal systems such as the M4 [33] and JPL’s Rollocopter [34], which combine wheeled mobility and flight, would encounter similar challenges.\n\nUnconventional morphologies, such as the screw-driven snake robot EELS [4] designed for icy terrains, and micro-scale jumping robots like Frogbot [45], offer new paradigms for locomotion. To achieve safe locomotion down steep crater walls, a particularly promising approach is seen in JPL’s Hedgehog [46], a tumbling robot that uses internal flywheels to generate momentum. Tumbling locomotion takes advantage of gravity to descend slopes, eliminating the need for active actuation and resulting in highly energy-efficient traversal. Despite its long history, tumbling has seen limited application in space exploration, leaving its full potential largely unexplored. Given its simplicity and efficiency, it presents an exciting opportunity for navigating the Moon’s challenging terrain.\n\nThe merits and limitations of tumbling robots are well documented. Passive systems like NASA/JPL’s Mars Tumbleweed Rover [49] are energy-efficient, making them ideal for remote exploration where energy conservation is critical. Active rolling spherical robots, such as MIT’s Kickbot [50], with their low center of gravity and omnidirectional mobility, are robust against external perturbations and excel in tight spaces. However, tumbling robots face challenges. Passive systems often sacrifice controllability for energy efficiency, relying on their shape and external forces for maneuvering. Additionally, rolling robots typically use their entire body for locomotion, leaving no stable platform for sensors, which complicates tasks like localization and perception.\n\nEarly rolling robots like Rollo [51] and the Spherical Mobile Robot (SMR) [52] used a spring-mass system with a driving wheel to create a mass imbalance for movement, but they were unreliable, as the driving wheel often lost contact with the sphere. Furthermore, significant central weight was required to generate enough inertia for propulsion. Other notable designs include the University of Pisa’s Sphericle [53] and Spider Rolling Robot (SRR) [54]. Sphericle used a car inside a sphere to drive the structure, but relied on gravity to keep the car wheels in contact with the inside of the sphere. Large perturbations led by mobility on rough terrain could dislodge the car and incapacitate the robot.\n\nA more precise tumbling method involves shifting internal weights to control the center of mass, as seen in the University of Michigan’s Spherobot [55] and University of Tehran’s August Robot [56], though these systems are not energy-efficient due to the added weight required.\n\nA more energy-efficient means of positioning the center of gravity for tumbling is by using deformable structures. Successful examples [57, 58, 59, 60, 61] can be identified that have attempted rolling by articulated structural designs that allow such deformation. Notable examples are Ritsumeikan University’s Deformable Robot [62], and Ourobot [63] with an articulated closed-loop structure.\n\nNASA’s Hedgehog [46] is a notable example developed specifically for space applications, derived from ETH’s Cubli [47]. It combines hopping, oblique hopping, tumbling, escape motion, and yawing (pointing) to traverse asteroids and comets. While it offers robust locomotion on a flat surface, because of its simple operation principle, Hedgehog has a few serious shortcomings that make it unsuitable for long-distance mobility. First, it can get stuck in soft terrain due to its jagged shape (a cube with protusions at its corners). A tornado (fast rotations around body axes) maneuver is proposed by its designers to escape; however, any fast body movements in these conditions can aggravate the situation. Next, because of its cubic shape, Hedgehog’s tumbling can be rough compared to other smoother circular geometries. Last, Hedgehog’s actuation relies on spinning three internal flywheels with considerable inertia to produce reaction torque. This actuation can be very costly, particularly for larger versions of Hedgehog. Therefore, if large-payload systems are considered in future NASA Moon missions, Hedgehog may suffer from scalability issues.\n\nOur proposed system, COBRA, is a snake-inspired multimodal rover designed for challenging terrain exploration (Fig. 3). COBRA combines the advantages of tumbling locomotion with the morphology afforded by snake robots to effectively address the locomotion challenges of steep crater walls as well as the uneven terrain of the crater floor. COBRA is lightweight at just 7.11 kg, with a compact diameter of 10 cm and a length of 1.7 m. Its eleven actuated degrees of freedom (DOF) enable it to morph its body shape to achieve various locomotion modes that can adapt to unpredictable and rough terrain. To achieve tumbling locomotion, COBRA raises and links its head and tail modules together to transform into a wheel-like structure. We identify these two distinct configurations as (1) Snake Configuration, where the head and tail is unconnected, and (2) Hex-Ring Configuration, where the head and tail are connected to form a closed loop.\n\nIn Snake Configuration, COBRA employs sidewinding and other slithering gaits to move efficiently across flat or uphill terrain. Sidewinding is a gait used by snakes to traverse loose or slippery surfaces like sand, and is particularly relevant for traversing lunar regolith, which shares similar properties. Variations of sidewinding locomotion have been shown to be fast and efficient, able to push off rocks and other obstacles to minimize energy consumption and increase traction [64, 65, 66]. Further, the symmetric snake-like morphology makes it excellent at traversing uneven and unknown terrain by virtue of not needing to reorient itself. The system’s weight is distributed along the entire length of its body which mitigates sinkage, and also allows for a large payload capacity, which can carry sensors such as a spectrometer to determine the concentration of hydrogen in the lunar regolith. By swinging its joints back and forth, the robot can also dig into the ground, exposing scientific samples below the surface or clearing material away if the system is stuck. It can also lift sections of its body into the air to overcome obstacles, climb out of holes, and aim instruments for navigation or communication.\n\nFor tumbling down slopes, COBRA enters the Ring Configuration and shifts its center of gravity to initiate tumbling. Through manipulation of its posture within the ring configuration using its joint actuators, COBRA can actively steer itself with minimal effort, allowing it to track desired paths to reach points of interest along the slope or avoid obstacles. These capabilities are not limited to lunar exploration. As a ground based system, it is a powerful tool for exploration of similar environments on Mars, such as Valles Marineris, a canyon system that features large, sloped terrain that our system can efficiently travel down to perform in-situ measurements in channels that potentially contain water.\n\nSome examples of tumbling platforms employing active deformation have been discussed above. COBRA distinguishes itself from these platforms in two significant ways: (1) Dynamic Tumbling Locomotion: As a tumbling system, it is field-tested and capable of dynamic tumbling locomotion with steering in two dimensions (2) Multimodal Locomotion and Manipulation: COBRA is a multimodal robot that employs the Snake Configuration to perform various locomotion and manipulation tasks on flat ground. This feature extends its operational scope beyond that of typical tumbling robots, enabling it to address diverse mission requirements.\n\nPrevious studies have extensively explored snake locomotion across various robotic platforms [67, 68, 69, 70, 71, 72]. COBRA builds on these works by implementing a broad range of snake gaits, achieving untethered operation, and integrating these capabilities with tumbling locomotion. This enables COBRA to operate effectively across a wide range of environments of various slopes and roughness, making it highly suitable for the exploratory nature of the proposed task.\n\nA comparable platform of note is NASA JPL’s EELS snake robot [4], which employs a screw propulsion mechanism along the length of its body, enabling locomotion on low-friction ice surfaces such as those found on Europa. However, this mechanism is less suited for granular and rocky terrains like those encountered on the Moon. COBRA is designed with locomotion strategies tailored for high-roughness surfaces, utilizing slithering and sidewinding gaits for flat or slightly inclined terrain and tumbling locomotion for rapid movement across steep slopes. This combination makes COBRA uniquely suited for environments where rapid elevation changes and substrate variability are critical considerations, for example lunar sites such as Shackleton Crater.\n\nHere we present the mission scenario envisioned for COBRA’s operation considering the case study of Shackleton Crater. COBRA will be deployed from a Commercial Lunar Payload Services (CLPS) lander near the edge of Shackleton Crater, as illustrated in Fig. 2. Following deployment, the system will navigate to the slope of the crater using sidewinding locomotion. In this mode, segments of the robot’s body are lifted and shifted forward, while others remain in contact with the ground, minimizing shear forces at contact points and preventing slippage on the soft regolith.\n\nNext, COBRA leverages the Moon’s partial gravity to tumble down the steep slope in its wheel-like configuration. Transitioning from the snake configuration, the system shifts its internal weight onto the slope to initiate tumbling. During descent, the joints remain static, conserving energy.\n\nThe system will tumble incrementally, halting every 500 meters to disconnect its head and tail for in-situ measurements. The scientific payload, a spectrometer housed in the tail, can be positioned using COBRA’s joints. This allows the system to create a detailed hydrogen concentration map at various depths within the crater. After collecting data, COBRA transmits the results to a lunar orbiter via a radio antenna in its head module, eliminating the need for the system to climb out of the crater for data transmission.\n\nFollowing communication at the sampling location, COBRA reforms its tumbling structure, tumbles another 500 meters, and repeats the process. Upon reaching the flatter center of Shackleton Crater, it will switch to sidewinding to continue its exploration and data collection until its power is exhausted.\n\nBased on this mission scenario, we come up with four objectives that we focus on for this work. 1) Locomotion over flat or slightly sloped ground using snake-like gaits. 2) Locomotion down slopes using two tumbling configurations. 3) Multi-modal operation allowing seamless switching between the modes of locomotion with no human contact with the robot. 4) Modular design that is robust to potential failures that can occur during remote operation in a space mission.\n\nFor this prototype, we focused on developing COBRA’s locomotion capabilities and designing a robust, modular system. While not using space-grade materials, the prototype is built to withstand outdoor terrestrial conditions. The following sections detail our development and testing process towards achieving these objectives.",
            "llm_summary": "大模型总结失败",
            "llm_score": 0,
            "llm_error": "API 状态码异常：403，响应：{\"error\":{\"message\":\"免费API限制模型输入token小于4096，如有更多需求，请访问 https://api.chatanywhere.tech/#/shop 购买付费API。The number of prompt tokens for free accounts is limited to 4096. If you have additional requirements, please visit https://api.chatanywhere.tech/#/shop to purchase a premium key.(当前请求使用的ApiKey: sk-8l9****i4zt)【如果您遇到问题，欢迎加入QQ群咨询：1048463714】\",\"type\":\"chatanywhere_error\",\"param\":null,\"code\":\"403 FORBIDDEN\"}}"
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "CU-Multi: A Dataset for Multi-Robot Collaborative Perception",
            "authors": "Doncey Albin,Daniel McGann,Miles Mena,Annika Thomas,Harel Biggie,Xuefei Sun,Steve McGuire,Jonathan P. How,Christoffer Heckman",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19463",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19463",
            "arxiv_html_link": "https://arxiv.org/html/2509.19463v1",
            "abstract": "A central challenge for multi-robot systems is fusing independently gathered perception data into a unified representation. Despite progress in Collaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of dedicated multi-robot datasets. Many evaluations instead partition single-robot trajectories, a practice that may only partially reflect true multi-robot operations and, more critically, lacks standardization, leading to results that are difficult to interpret or compare across studies. While several multi-robot datasets have recently been introduced, they mostly contain short trajectories with limited inter-robot overlap and sparse intra-robot loop closures. To overcome these limitations, we introduce CU-Multi, a dataset collected over multiple days at two large outdoor sites on the University of Colorado Boulder campus. CU-Multi comprises four synchronized runs with aligned start times and controlled trajectory overlap, replicating the distinct perspectives of a robot team. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined ground-truth odometry. By combining overlap variation with dense semantic annotations, CU-Multi provides a strong foundation for reproducible evaluation in multi-robot collaborative perception tasks. The dataset, support code, and updates are available at https://arpg.github.io/cumulti.",
            "introduction": "Multi-robot systems significantly enhance capabilities across diverse domains, particularly in large-scale environments, by accelerating exploration through distributed sensing and collaborative decision-making [7]. A central challenge in realizing these advantages lies in fusing perception data collected independently by multiple robots into a unified global representation, complicated by spatial and temporal misalignment. This challenge is further compounded in many practical scenarios where external positioning methods (e.g. GPS, motion capture) are impractical, unreliable, or hazardous. Multi-robot Collaborative SLAM (C-SLAM) algorithms are typically first developed and validated offline using datasets before doing field tests in real-world conditions, where system-level issues often arise. Consequently, the availability of realistic, well-structured multi-robot datasets is essential for supporting reproducible research and bridging the gap between offline development and real-world deployment.\n\nSeveral multi-robot datasets have recently been introduced for C-SLAM verification [3, 6, 1]. These datasets include synchronized multi-agent sensor data collected across both indoor and/or outdoor environments. While these datasets represent strong starting points, there remains a need for longer-trajectory multi-robot datasets with explicitly varied overlap (see Table I). Despite recent advancements in multi-robot datasets, there remains a prevalent practice of artificially segmenting a single trajectory into multiple parts to simulate a multi-robot scenario for verification [8]. Single-robot SLAM datasets still offer a few benefits over many of the existing multi-robot datasets, such as large trajectories with multiple loop closures, as well as the availability of both camera and LiDAR semantics [8, 9]. However, without careful consideration, arbitrary segmentation may not accurately represent realistic observational overlap typically encountered in multi-robot operations [10].\n\nIn this paper, we introduce CU-Multi, a dataset designed to support the evaluation of methods relevant to multi-robot perception. These methods include C-SLAM, LiDAR and visual data association, and multi-session LiDAR-based place recognition. CU-Multi offers the following key features:\n\nA multi-robot dataset consisting of two large-scale environments, each with four robots. Our dataset contains a total of eight diverse trajectories, spanning a combined length of 16.7 km across the CU Boulder campus (Figure 1).\n\nSystematically varied trajectory overlaps and a rendezvous-based trajectory design, enabling evaluation of multi-robot perception tasks under different levels of observational redundancy and variation.\n\nRefined ground truth poses using RTK GPS, a digital elevation model, lidar-inertial SLAM, and a highly accurate scan-matching algorithm, calculated at each LiDAR timestamp.\n\nSemantic labels for all LiDAR scans, dataset tools for interacting with the data, as well as initial benchmarks on a well-known C-SLAM and LiDAR place recognition method to demonstrate dataset usability and utility of the provided tools.",
            "llm_summary": "【关注的是什么问题】  \n1. 多机器人系统在融合独立收集的感知数据时面临的挑战。  \n2. 现有多机器人数据集缺乏长轨迹和标准化，导致评估结果难以比较。  \n3. 需要一个结构良好的多机器人数据集以支持可重复的研究。  \n\n【用了什么创新方法】  \nCU-Multi数据集在两个大型户外环境中收集，包含四个同步运行的机器人，具有控制的轨迹重叠和多样化的重叠变化。数据集提供RGB-D传感、RTK GPS、语义LiDAR和精细的真实轨迹，支持多机器人协作感知任务的评估。通过丰富的语义注释和系统化的轨迹设计，CU-Multi为多机器人感知方法的评估提供了坚实的基础，并展示了其在C-SLAM和LiDAR位置识别方法上的初步基准效果。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Self-evolved Imitation Learning in Simulated World",
            "authors": "Yifan Ye,Jun Cen,Jing Chen,Zhihe Lu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19460",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19460",
            "arxiv_html_link": "https://arxiv.org/html/2509.19460v1",
            "abstract": "Imitation learning has been a trend recently, yet training a generalist agent across multiple tasks still requires large-scale expert demonstrations, which are costly and labor-intensive to collect.\nTo address the challenge of limited supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework that progressively improves a few-shot model through simulator interactions.\nThe model first attempts tasks in the simulator, from which successful trajectories are collected as new demonstrations for iterative refinement.\nTo enhance the diversity of these demonstrations, SEIL employs dual-level augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model to collaborate with the primary model, and (ii) Environment-level, introducing slight variations in initial object positions.\nWe further introduce a lightweight selector that filters complementary and informative trajectories from the generated pool to ensure demonstration quality.\nThese curated samples enable the model to achieve competitive performance with far fewer training examples.\nExtensive experiments on the LIBERO benchmark show that SEIL achieves a new state-of-the-art performance in few-shot imitation learning scenarios.\nCode is available at https://github.com/Jasper-aaa/SEIL.git.",
            "introduction": "Imitation Learning (IL) [1, 2, 3, 4] has demonstrated remarkable success across a variety of tasks by leveraging large-scale expert demonstration datasets [5].\nHowever, collecting such datasets is often time-consuming, labor-intensive [6], and in certain domains (e.g., surgical practice) can even be impractical [7].\nThis motivates the study of IL under limited supervision, which is both a more practical and challenging setting.\nHowever, learning from only a few demonstrations typically results in a substantial performance drop. For instance, in the one-shot setting, a train-from-scratch Diffusion Policy [2] achieves only 0.8% success rate, compared to 50.5% with full training.\n\nInspired by the empirical success of Reinforcement Learning (RL) [8, 9, 10, 11, 12], where agents explore through extensive trial-and-error within simulators, a natural solution is to leverage simulation environments to autonomously generate additional demonstrations for fine-tuning few-shot imitation policies.\nThat is, a policy is initially trained on a limited set of expert demonstrations and then deployed in a simulator, where it interacts with the environment and records successful trajectories based on simple success/failure feedback.\nOnce a sufficient number of demonstrations across tasks are collected, the policy is refined using standard imitation learning updates on the newly acquired data.\nHowever, this approach gives rise to two critical challenges:\n(i) How can we ensure sufficient diversity in the simulated demonstrations to support iterative learning?\n(ii) How can we identify and select the most informative samples to optimize model performance while maintaining computational efficiency?\n\nIn this paper, we investigate a few-shot imitation learning setting, address the aforementioned two key challenges and propose Self-Evolved Imitation Learning (SEIL), a novel framework that leverages simulator to enable policy self-evolution from limited expert demonstrations, often with significant improvements as shown in Figure 1.\nSEIL is built upon two core components: dual-level augmentation, comprising model-level and environment-level strategies, for generating diverse trajectories, and a sample selector tailored to identify and retain the most informative demonstrations for efficient policy refinement.\nNotably, SEIL supports multiple rounds of interaction with the simulator, enabling the policy to evolve progressively over time.\nSpecifically, for model-level augmentation, SEIL introduces an auxiliary model to conduct additional rollouts alongside the primary model.\nTo avoid incurring extra training overhead, particularly important in a multi-stage evolution setting where cost accumulates over iterations, this auxiliary policy is implemented as an Exponential Moving Average (EMA) of the main model.\nThe EMA model maintains diversity in the policy space without the need for separate training, thereby supporting efficient and scalable demonstration generation.\nIn addition to model-level augmentation, we introduce environment-level augmentation, which enables the policy to interact with the simulator under diverse initial conditions.\nThis variation in starting states enhances environmental diversity, thereby enriching the recorded demonstrations and improving the robustness of policy learning.\n\nAfter generating a pool of diverse demonstrations via dual-level augmentation, we introduce a lightweight selector to identify informative samples.\nThe efficiency nature of this selector comes from taking as input only an initial image and a trajectory, rather than full video sequences.\nTrained on few-shot data via a trajectory classification task, the selector learns category-specific feature patterns.\nOnce trained, the selector is frozen and applied to all recorded demonstrations.\nSamples with the lowest confidence, i.e., those most distinct from expert demonstrations, are selected for policy training.\n\nOur contributions are summarized as follows.\n\nWe investigate a few-shot imitation learning setting and propose Self-Evolved Imitation Learning (SEIL), a novel framework that leverages simulation environments to enable policy self-evolution from limited expert demonstrations.\n\nWe propose dual-level augmentations, i.e., model-level and environment-level augmentation, to guarantee the diversity of demonstration generation.\n\nWe propose a lightweight selector to identify the most informative demonstrations, enhancing both policy performance and training efficiency.\n\nExtensive experiments demonstrate that SEIL can effectively evolve weak few-shot trained models, achieving substantial improvements, e.g., a 217.3% performance growth over the 1-shot baseline on Libero-Long.\n\n1. We investigate a few-shot imitation learning setting and propose Self-Evolved Imitation Learning (SEIL), a novel framework that leverages simulation environments to enable policy self-evolution from limited expert demonstrations.\n\n2. We propose dual-level augmentations, i.e., model-level and environment-level augmentation, to guarantee the diversity of demonstration generation.\n\n3. We propose a lightweight selector to identify the most informative demonstrations, enhancing both policy performance and training efficiency.\n\n4. Extensive experiments demonstrate that SEIL can effectively evolve weak few-shot trained models, achieving substantial improvements, e.g., a 217.3% performance growth over the 1-shot baseline on Libero-Long.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在有限的专家演示下进行有效的模仿学习？  \n2. 如何确保模拟演示的多样性以支持迭代学习？  \n3. 如何识别和选择最具信息量的样本以优化模型性能？  \n\n【用了什么创新方法】  \n提出了自我进化模仿学习（SEIL）框架，通过模拟器交互逐步改进少量模型。采用双层增强策略，分别在模型和环境层面生成多样化轨迹，并引入轻量级选择器过滤出有用的演示样本。实验结果表明，SEIL在少样本模仿学习场景中达到了新的最先进性能，尤其在Libero-Long基准上实现了217.3%的性能增长。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation",
            "authors": "Jason Chen,I-Chun Arthur Liu,Gaurav Sukhatme,Daniel Seita",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19454",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19454",
            "arxiv_html_link": "https://arxiv.org/html/2509.19454v1",
            "abstract": "Training robust bimanual manipulation policies via imitation learning requires demonstration data with broad coverage over robot poses, contacts, and scene contexts.\nHowever, collecting diverse and precise real-world demonstrations is costly and time-consuming, which hinders scalability. Prior works have addressed this with data augmentation, typically for either eye-in-hand (wrist camera) setups with RGB inputs or for generating novel images without paired actions, leaving augmentation for eye-to-hand (third-person) RGB-D training with new action labels less explored. In this paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation (ROPA), an offline imitation learning data augmentation method that fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D observations of novel robot poses. Our approach simultaneously generates corresponding joint-space action labels while employing constrained optimization to enforce physical consistency through appropriate gripper-to-object contact constraints in bimanual scenarios. We evaluate our method on 5 simulated and 3 real-world tasks. Our results across 2625 simulation trials and 300 real-world trials demonstrate that ROPA outperforms baselines and ablations, showing its potential for scalable RGB and RGB-D data augmentation in eye-to-hand bimanual manipulation.\nOur project website is available at: https://ropaaug.github.io/.",
            "introduction": "Bimanual manipulation is critical for a wide range of daily tasks [1] such as lifting large objects [2, 3], handling deformable objects [4, 5, 6, 7], and opening containers [8, 9].\nThese activities require coordinated motion of both arms and awareness of nearby objects and surfaces. A third-person camera can capture both arms and the surrounding scene from one viewpoint, which is appealing for vision-based imitation learning [10].\nRecent imitation learning approaches show that large demonstration datasets yield increasingly general bimanual policies [11, 12, 13], but collecting sufficiently large and diverse action-labeled data is costly, which limits scalability.\n\nData augmentation has emerged as a promising strategy to address this bottleneck, particularly in single-arm manipulation, where novel views and corresponding action labels can be synthesized offline [14].\nHowever, extending these techniques to bimanual manipulation, depth images, and beyond wrist-mounted (eye-in-hand) viewpoints [15, 16], introduces new difficulties. These include enforcing scene-wide visual consistency across two robot arms, preserving action label correctness, and handling the higher degrees-of-freedom (DOF).\n\nIn this paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation (ROPA), a data augmentation method for vision-based imitation learning of bimanual manipulation. Our key insight in ROPA is to adapt Pose-Guided Person Image Synthesis techniques [17] (originally developed for human pose generation) by conditioning a diffusion model on robot joint configurations. This enables the diffusion model to yield realistic and consistent third-person views for varied robot arm poses. We maintain a tight correspondence between augmented visual observations and valid actions, so that policies trained on the augmented dataset remain executable on real hardware.\n\nWe evaluate ROPA across 5 simulated and 3 real-world bimanual manipulation tasks, based on the PerAct2 [18] benchmark in simulation and a physical bimanual UR5 setup. See Figure 1 for an example rollout. Compared to an Action Chunking with Transformers (ACT) [19] baseline trained only on raw demonstrations, data augmentation using ROPA enables ACT to achieve higher success rates across bimanual manipulation tasks that require coordinated motion and fine-grained precision.\nBy enabling third-person vision-based data augmentation for bimanual systems, ROPA scales training data and broadens coverage without extra manual data collection.\n\nThe contributions of this paper include:\n\nROPA, a novel pose-guided image synthesis method for offline data augmentation in bimanual manipulation, which supports learning from both RGB and RGB-D data.\n\nDepth image synthesis that generates depth maps consistent with the augmented joint positions and RGB images.\n\nAction-consistent augmentation that outputs images and joint-space labels, with constraints to ensure feasibility.\n\nSimulation and real-world experiments showing that bimanual policies with ROPA achieve significantly improved performance over baseline methods and ablations.\n\n1. ROPA, a novel pose-guided image synthesis method for offline data augmentation in bimanual manipulation, which supports learning from both RGB and RGB-D data.\n\n2. Depth image synthesis that generates depth maps consistent with the augmented joint positions and RGB images.\n\n3. Action-consistent augmentation that outputs images and joint-space labels, with constraints to ensure feasibility.\n\n4. Simulation and real-world experiments showing that bimanual policies with ROPA achieve significantly improved performance over baseline methods and ablations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效生成多样化的机器人姿态以增强双手操作的模仿学习数据？  \n2. 如何在数据增强过程中保持视觉一致性和动作标签的正确性？  \n3. 如何在不增加手动数据收集成本的情况下扩展训练数据的覆盖范围？  \n\n【用了什么创新方法】  \n提出了一种名为ROPA的合成机器人姿态生成方法，通过微调Stable Diffusion模型，生成第三人称RGB和RGB-D观察数据。该方法在生成图像的同时，确保动作标签的一致性，并通过约束优化来维持物理一致性，确保夹持器与物体的接触约束。经过在5个模拟和3个真实任务上的评估，ROPA在2625次模拟试验和300次真实试验中表现优于基线和消融实验，展示了其在双手操作中的数据增强潜力。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames",
            "authors": "Alessandro Saviolo,Jeffrey Mao,Giuseppe Loianno",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19452",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19452",
            "arxiv_html_link": "https://arxiv.org/html/2509.19452v1",
            "abstract": "Search and rescue operations require unmanned aerial vehicles to both traverse unknown unstructured environments at high speed and track targets once detected. Achieving both capabilities under degraded sensing and without global localization remains an open challenge. Recent works on relative navigation have shown robust tracking by anchoring planning and control to a visible detected object, but cannot address navigation when no target is in the field of view.\nWe present HUNT (High-speed UAV Navigation and Tracking), a real-time framework that unifies traversal, acquisition, and tracking within a single relative formulation. HUNT defines navigation objectives directly from onboard instantaneous observables such as attitude, altitude, and velocity, enabling reactive high-speed flight during search. Once a target is detected, the same perception–control pipeline transitions seamlessly to tracking. Outdoor experiments in dense forests, container compounds, and search-and-rescue operations with vehicles and mannequins demonstrate robust autonomy where global methods fail.\nVideo: http://bit.ly/4n5Tp5D",
            "introduction": "Unmanned Aerial Vehicles (UAVs), especially quadrotors, have become essential tools in Search-and-Rescue (SAR) operations, where their maneuverability and speed enable rapid deployment in unstructured, GPS-denied environments. These missions require two critical capabilities: the ability to safely traverse unknown terrain at high speed to search for victims or objects of interest, and the ability to reliably track and follow detected targets once they are identified. Achieving both in degraded sensing and unknown environments remains a fundamental challenge in robotics.\n\nTraditional autonomy relies on global navigation anchored by GPS, Visual-Inertial Odometry (VIO), or Simultaneous Localization and Mapping (SLAM) [1]. These methods degrade in exactly the conditions most critical to SAR: dense forests, collapsed buildings, or urban canyons where GPS is degraded and visual features are sparse, dynamic, or occluded. Their dependence on persistent landmarks and loop closures makes global pose estimation fragile, resulting in drift and unsafe behavior at high speed.\n\nAn alternative paradigm, recently advanced in prior works [2], is to abandon global consistency and instead define navigation directly in a target-relative reference frame. By continuously re-anchoring perception, planning, and control to the instantaneous detection of a moving object, UAVs can robustly pursue targets even in unstructured or degraded environments. This instantaneous relative navigation formulation has demonstrated strong performance for the tracking phase of SAR missions. However, it presumes that a target is visible from the outset and remains continuously observable—an assumption that rarely holds in practice. What is missing is a capability for safe, high-speed traversal in unknown terrain without any target directly in view, and without reverting to the fragile assumptions of traditional GPS, VIO, or SLAM.\n\nThis paper introduces HUNT, a reactive autonomy framework that unifies high-speed traversal, acquisition, and tracking under a single instantaneous relative formulation. At its core is a novel loitering mode, which defines navigation objectives solely from directly observable quantities—attitude, altitude, and instantaneous velocity—allowing the UAV to traverse unknown cluttered environments safely even in the absence of a target. When a target is detected, the same perception–control pipeline seamlessly re-anchors the reference frame to the target, turning tracking into a special case of loitering without requiring global state.\n\nSafety is enforced throughout by embedding high-order Control Barrier Functions (CBFs) directly into a Nonlinear Model Predictive Controller (NMPC), guaranteeing dynamically feasible, collision-free trajectories in real time. Robust transitions between modes are enabled by a confidence-based switching mechanism that prevents oscillations and ensures stability as detections appear and disappear.\n\nWe validate HUNT in extensive outdoor SAR-like missions spanning urban compounds, semi-structured layouts, and dense forests. Experiments include loitering flights over city blocks and forest canopy, dense-clutter traversal below canopy, and full missions where the UAV searches, acquires, and pursues static and dynamic targets such as vehicles and mannequins. Across all settings, HUNT achieves consistent high-speed performance where global methods fail.\n\nTo the best of our knowledge, HUNT is the first framework to integrate traversal, acquisition, and tracking in cluttered, GPS-denied environments under a unified instantaneous relative formulation, eliminating reliance on global pose.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在未知、无结构环境中实现无人机的高速度导航和目标跟踪。  \n2. 传统的全球导航方法在SAR任务中面临的挑战和局限性。  \n3. 在没有目标可见的情况下进行安全的高速度穿越的能力。  \n\n【用了什么创新方法】  \nHUNT框架通过将高速度穿越、目标获取和跟踪统一在一个瞬时相对参考框架下，提出了一种新颖的巡航模式。该模式仅依赖于可直接观察的量（如姿态、高度和瞬时速度）来定义导航目标，使无人机能够在没有目标的情况下安全穿越未知环境。当目标被检测到时，感知-控制管道无缝切换到跟踪模式，确保高效的目标追踪。通过将高阶控制障碍函数嵌入到非线性模型预测控制器中，HUNT保证了实时的动态可行和无碰撞轨迹。实验结果表明，HUNT在复杂的城市和森林环境中表现出一致的高速度性能，超越了传统方法的局限。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models",
            "authors": "JuanaJuana Valeria Hurtado,Rohit Mohan,Abhinav Valada",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20107",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20107",
            "arxiv_html_link": "https://arxiv.org/html/2509.20107v1",
            "abstract": "Hyperspectral imaging (HSI) captures spatial information along with dense spectral measurements across numerous narrow wavelength bands. This rich spectral content has the potential to facilitate robust robotic perception, particularly in environments with complex material compositions, varying illumination, or other visually challenging conditions. However, current HSI semantic segmentation methods underperform due to their reliance on architectures and learning frameworks optimized for RGB inputs. In this work, we propose a novel hyperspectral adapter that leverages pretrained vision foundation models to effectively learn from hyperspectral data. Our architecture incorporates a spectral transformer and a spectrum-aware spatial prior module to extract rich spatial-spectral features. Additionally, we introduce a modality-aware interaction block that facilitates effective integration of hyperspectral representations and frozen vision Transformer features through dedicated extraction and injection mechanisms.\nExtensive evaluations on three benchmark autonomous driving datasets demonstrate that our architecture achieves state-of-the-art semantic segmentation performance while directly using HSI inputs, outperforming both vision-based and hyperspectral segmentation methods. We make the code available at https://hyperspectraladapter.cs.uni-freiburg.de.",
            "introduction": "Advancing robot perception in complex environments requires an understanding of detailed scene characteristics beyond surface appearance [1]. Hyperspectral imaging (HSI), which captures reflectance information across tens to hundreds of narrow spectral bands, provides material-specific information that is significantly richer than conventional RGB imaging [2]. By encoding distinctive spectral signatures, HSI enables fine-grained discrimination of objects and materials, offering the potential to enhance perception tasks [3, 4] under challenging conditions such as varying illumination, occlusions [5], scene clutter [6], and complex material compositions. These properties make HSI particularly promising for autonomous driving, where reliable perception in diverse and visually complex environments is essential.\n\nHyperspectral semantic segmentation aims to classify each pixel of an image into predefined categories based on HSI inputs. Despite its potential, semantic segmentation with hyperspectral data remains a challenging problem. Most segmentation models are tailored for RGB images and fail to fully leverage the spectral richness of HSI. Standard vision semantic segmentation architectures, although proven powerful for extracting spatial semantics from RGB data, are not inherently equipped to model the complex inter-channel dependencies of hyperspectral inputs. As a result, models pretrained on RGB datasets often underperform when directly applied to hyperspectral inputs [7, 8, 9]. Although pseudo-RGB approaches can partially adapt existing models, they overlook the spectral diversity inherent in HSI and fail to address critical challenges such as high dimensionality, limited labeled data, and increased computational costs [2]. We illustrate the difference between these approaches in Fig. 1.\n\nVision foundation models trained on large-scale datasets generalize well across domains [10]. However, their full finetuning is computationally prohibitive, and linear probing lacks capacity for complex tasks such as hyperspectral segmentation. Adapter tuning provides an efficient alternative by inserting lightweight modules into a frozen backbone, allowing for task-specific adaptation without retraining the entire model. However, existing adapters are designed for RGB inputs and, when applied to HSI, they face initialization sensitivity and fail to capture spectral dependencies, limiting their effectiveness.\n\nTo address these limitations, we propose the HSI-Adapter, a modular architecture that enables leveraging strong pretrained vision foundation models with HSI inputs for hyperspectral semantic segmentation. Our approach introduces a spectral transformer and a spectral-enhanced spatial prior module to jointly extract rich spectral and spatial features from hyperspectral inputs. To bridge the modality gap between hyperspectral and RGB representations, we propose a modality-aware interaction block that enables effective bidirectional feature exchange between hyperspectral features and foundational vision features. This topology preserves the strengths of the pretrained foundation model while incorporating the unique characteristics of hyperspectral data. Extensive experiments on HSI-DriveV2 [11], HyperspectralCityV2.0 (HCV2) [12], and HyKo2-VIS [13] benchmark autonomous driving datasets demonstrate that our method achieves state-of-the-art performance, surpassing both RGB-based and hyperspectral segmentation approaches. In addition to quantitative gains, qualitative results show that our method generalizes well to diverse and complex driving environments, handling cluttered scenes, dense urban layouts, and visually similar material classes such as painted and unpainted metal.\n\nOur primary contributions are summarized as follows:\n\nA novel hyperspectral adapter architecture tailored for semantic segmentation using HSI inputs.\n\nA spectral transformer and spectrum-aware spatial prior module to jointly model spatial and spectral context.\n\nA modality-aware interaction block that enables effective feature fusion between hyperspectral and pretrained foundational vision representations.\n\nComprehensive experiments and ablation studies across three benchmark datasets, demonstrating the effectiveness and generalization of our approach.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效利用高光谱成像（HSI）数据进行语义分割？  \n2. 当前基于RGB的语义分割模型在处理HSI数据时表现不佳的原因是什么？  \n3. 如何在不完全重训练的情况下，适应预训练视觉基础模型以处理HSI输入？  \n\n【用了什么创新方法】  \n本研究提出了一种新颖的高光谱适配器架构，结合了光谱变换器和光谱增强空间先验模块，以联合提取高光谱输入的丰富光谱和空间特征。此外，提出的模态感知交互块实现了高光谱特征与预训练视觉特征之间的有效双向特征交换。通过在三个基准数据集上的广泛评估，证明了该方法在语义分割性能上达到了最先进的水平，超越了现有的RGB和高光谱分割方法。定量和定性结果表明，该方法在复杂的驾驶环境中表现良好，能够处理拥挤场景和视觉相似材料类别。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Embodied AI: From LLMs to World Models",
            "authors": "Tongtong Feng,Xin Wang,Yu-Gang Jiang,Wenwu Zhu",
            "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Robotics (cs.RO)",
            "comment": "Accepted by IEEE CASM",
            "pdf_link": "https://arxiv.org/pdf/2509.20021",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20021",
            "arxiv_html_link": "https://arxiv.org/html/2509.20021v1",
            "abstract": "Embodied Artificial Intelligence (AI) is an intelligent system paradigm for achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications and driving the evolution from cyberspace to physical systems. Recent breakthroughs in Large Language Models (LLMs) and World Models (WMs) have drawn significant attention for embodied AI. On the one hand, LLMs empower embodied AI via semantic reasoning and task decomposition, bringing high-level natural language instructions and low-level natural language actions into embodied cognition. On the other hand, WMs empower embodied AI by building internal representations and future predictions of the external world, facilitating physical law-compliant embodied interactions. As such, this paper comprehensively explores the literature in embodied AI from basics to advances, covering both LLM driven and WM driven works. In particular, we first present the history, key technologies, key components, and hardware systems of embodied AI, as well as discuss its development via looking from unimodal to multimodal angle. We then scrutinize the two burgeoning fields of embodied AI, i.e., embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs, meticulously delineating their indispensable roles in end-to-end embodied cognition and physical laws-driven embodied interactions. Building upon the above advances, we further share our insights on the necessity of the joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds. In addition, we examine representative applications of embodied AI, demonstrating its wide applicability in real-world scenarios. Last but not least, we point out future research directions of embodied AI that deserve further investigation.",
            "introduction": "Embodied Artificial Intelligence (AI) originated from the Embodied Turing Test by Alan Turing in 1950 [1], which is designed to explore whether agents can imitate human intelligence to achieve Artificial General Intelligence (AGI). Among them, agents that only solve abstract problems in digital world (cyberspace) are generally defined as disembodied AI, while those that also can interact with the physical world are regarded as embodied AI. Embodied AI builds on foundational insights from cognitive science and neuroscience [2, 3], which claims that intelligence emerges from the dynamic coupling of perception, cognition, and interaction. As shown in Fig. 1, embodied AI includes three key components in a closed-loop manner, i.e., 1) active perception (sensor-driven environmental observation), 2) embodied cognition (historical experience-driven cognition updating), and 3) dynamic interaction (actuator-mediated action control). Besides, hardware embodiment [4, 5, 6] is also critical due to escalating computational and energy demands, particularly under latency and power constraints of devices in real-world deployment scenarios.\n\nThe development of embodied AI has evolved from unimodal to multimodal paradigm. In early stage, embodied AI is primarily studied through focusing on individual components with single modality such as vision, language, or action, where the perception, cognition, or interaction component is driven by one sensory input [7, 8], e.g., perception tends to be dominated by the visual modality [9], cognition tends to be dominated by the language modality [10, 11], and interaction tends to be dominated by the action modality [12, 13]. Although these methods perform well within individual components, they are limited by the narrow scope of information provided by each modality and the inherent gaps between modalities across components. The continued development of embodied AI witnesses the limitations of unimodal approaches, promoting a significant shift toward integration of multiple sensory modalities [14, 15, 16]. As such, multimodal embodied AI [17, 18] naturally arises to create more adaptive, flexible, and robust agents capable of performing complex tasks in dynamic environments.\n\nLarge Language Models (LLMs) empower embodied AI via semantic reasoning [19] and task decomposition [20, 21], bringing high-level natural language instructions and low-level natural language actions into embodied cognition. Representative LLM driven works include SayCan [22], which i) provides a real-world pretrained natural language action library to constrain LLMs from proposing infeasible and contextually inappropriate actions; ii) uses LLMs to convert natural language instructions into natural language action sequences; and iii) utilizes value functions to verify the feasibility of natural language action sequences in a particular physical environment. These works suggest that LLMs are extremely useful to robots which aim at acting upon high-level, temporally extended instructions expressed in natural language. However, LLMs are only a part of the entire embodied AI system (e.g., embodied cognition), which is limited by a fixed natural language action library and a specific physical environment, making it difficult for LLM driven embodied AI to achieve adaptive expansion for new robots and environments.\n\nRecent breakthroughs in Multimodal LLMs (MLLMs) [23, 24] and World Models (WMs) [25, 26, 27] have opened up a new frontier in embodied AI research. MLLMs can act on the entire embodied AI system, bridging high-level multimodal inputting and low-level motor action sequences into end-to-end embodied applications. Semantic reasoning [28, 29, 30] leverages MLLMs’ cross-modal comprehension to interpret semantics from visual, auditory, or tactile inputs, e.g., identifying objects, inferring spatial relationships, predicting environmental dynamics. Concurrently, task decomposition [31, 32, 33] employs MLLMs’ sequential logic to break complex objectives into sub-tasks while dynamically adapting plans based on sensor feedback. However, MLLMs often fail to ground predictions in physics-compliant dynamics [34] and exhibit poor real-time adaptation [35] to environmental feedback.\n\nOn the other hand, WMs empower embodied AI by building internal representations [36, 37, 38, 39, 40] and making future predictions [41, 42, 43, 44] of the external world. Such WM driven embodied AI is able to facilitate physical law-compliant embodied interactions in dynamic environments. Internal representations compress rich sensory inputs into structured latent spaces, capturing object dynamics, physics laws, and spatial structures, as well as allowing agents to reason about “what exists” and “how things behave” in their surroundings. Simultaneously, future predictions simulate potential rewards of sequence actions across multiple time horizons aligned with physical laws, thereby preempting risky or inefficient behaviors. However, WM driven approaches struggle with open-ended semantic reasoning [45] and lack the ability of generalizable task decomposition [26] without explicit priors.\n\nBuilding upon the above advances, we further share our insights on the necessity of developing a joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds. MLLMs enable contextual task reasoning but overlook physical constraints, while WMs excel at physics-aware simulation but lack high-level semantics. The joint of MLLM and WM can bridge semantic intelligence with grounded physical interaction. For instance, EvoAgent [46] designs an autonomous-evolving agent with a joint MLLM-WM driven embodied AI architecture, which can autonomously complete various long-horizon tasks across environments through self-planning, self-reflection, and self-control, without human intervention. We believe that designing joint MLLM-WM driven embodied AI architectures will dominate next-generation embodied systems, bridging the gap between specialized AI agents and general physical intelligence.\n\nWe summarize the representative applications of embodied AI as service robotics, rescue UAVs, industrial Robots, and others etc., demonstrating its wide applicability in real-world scenarios. We also point out potential future directions of embodied AI, including but not limited to autonomous embodied AI, embodied AI hardware, and swarm embodied AI etc.\n\nAs shown in Fig. 2, the rest of this paper is organized as follows. Section II introduces the history, key technologies, key components, and hardware system of embodied AI, discussing the development of embodied AI from unimodal to multimodal angle. Section III presents embodied AI with LLMs/MLLMs, and Section IV presents embodied AI with WMs. Section V introduces our insights on designing a joint MLLM-WM driven embodied AI architecture. Section VI briefly examines applications of embodied AI. Potential future directions are discussed in Section VII.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现从LLMs到世界模型的联合驱动的具身人工智能架构？  \n2. 如何在复杂的物理环境中实现具身AI的高效任务执行？  \n3. 具身AI如何克服单一模态方法的局限性，提升多模态感知与交互能力？  \n\n【用了什么创新方法】  \n本文提出了一种联合MLLM-WM驱动的具身AI架构，通过整合多模态语言模型和世界模型，增强了具身智能的语义推理和物理交互能力。该架构能够在动态环境中实现复杂任务的自我规划、自我反思和自我控制，展现出在真实场景中广泛的应用潜力。研究表明，这种方法显著提高了具身AI在物理法则遵循下的交互能力，并推动了下一代具身系统的发展。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents",
            "authors": "Filippo Ziliotto,Jelin Raphael Akkara,Alessandro Daniele,Lamberto Ballan,Luciano Serafini,Tommaso Campari",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19843",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19843",
            "arxiv_html_link": "https://arxiv.org/html/2509.19843v1",
            "abstract": "Recent advances in Embodied AI have enabled agents to perform increasingly complex tasks and adapt to diverse environments. However, deploying such agents in realistic human-centered scenarios, such as domestic households, remains challenging, particularly due to the difficulty of modeling individual human preferences and behaviors.\nIn this work, we introduce PersONAL  (PERSonalized Object Navigation And Localization), a comprehensive benchmark designed to study personalization in Embodied AI.\nAgents must identify, retrieve, and navigate to objects associated with specific users, responding to natural-language queries such as find Lily’s backpack.\nPersONAL comprises over 2,000 high-quality episodes across 30+ photorealistic homes from the HM3D dataset. Each episode includes a natural-language scene description with explicit associations between objects and their owners, requiring agents to reason over user-specific semantics.",
            "introduction": "In recent years, Embodied AI has significantly advanced, enabling agents to perform complex tasks and interact more naturally with their environments. Modern methods combine end-to-end training with zero-shot capabilities powered by large language models (LLMs), allowing agents to answer dynamically to user input [1, 2, 3, 4, 5, 6, 7].\nYet, their application to user-centric scenarios, where agents must interpret local, implicit information not directly encoded in pretrained models, such as object ownership, remains largely unexplored.\nBridging this gap is key to deploying embodied agents in real-world environments like homes or offices.\nWhile personalized vision-language models (VLMs) [8, 9, 10] have been developed for user-specific visual grounding, they are typically limited to static, image-based contexts.\nIn contrast, embodied agents must operate in complex physical settings, reasoning and acting over time.\n\nRecently, a few works have begun to explore personalization in embodied scenarios [11, 12], but the field remains in its early stages and these works mainly focus on guiding agents using image-based queries or continuous human-robot interaction, which limits scalability and real-world applicability.\n\nTo address this gap, we introduce PersONAL , the first embodied AI benchmark for personalized, user-centric navigation and personalized object grounding (Figure 1).\nAgents must interpret user-specific queries and either navigate to or retrieve the location of objects associated with particular individuals (e.g., “Navigate to Carl’s backpack”).\nEach episode includes a textual scene description specifying object attributes and ownership (e.g., “the upper kitchen cabinet belongs to Linda”), followed by a personalized query.\nUnlike prior work, we also define a grounding task which acts as an embodied memory challenge, requiring agents to recall and localize targets using internal maps.\n\nPersONAL supports two evaluation modes: (i) Personalized Active Navigation (PAN), the agent must navigate an unknown environment to find the target object, and (ii) Personalized Object Grounding (POG), where the goal is to localize objects within a pre-mapped environment.\nWe release a dataset of over 2,000 curated evaluation episodes across three difficulty levels (easy, medium, hard), capturing increasing complexity in human–object associationsWe evaluate the proposed benchmark with several zero-shot, state-of-the-art navigation baselines and introduce a simple zero-shot method for the grounding task, demonstrating in both settings that a substantial performance gap persists between current Embodied AI agents and human capabilities.\n\nIn summary, our main contributions are:\n\nWe present PersONAL , a comprehensive Embodied AI benchmark specifically designed to evaluate embodied personalization, incorporating user-centric queries and object–ownership semantics.\n\nWe release a dataset of 2,000 high-quality episodes sampled from over 30 realistic household environments, divided into three difficulty levels (easy, medium, and hard).\n\nWe provide empirical analyses with state-of-the-art zero-shot baselines, showcasing limitations and possible future research directions toward human-level personalized navigation in Embodied AI.\n\nWe argue that personalization cannot be achieved through repeated LLM-based interactions alone.\nInstead, agents must maintain and recall user-specific preferences internally—ideally through long-term memory and continual learning—an ability crucial in complex households settings where ownership is stable but scenes continually evolve.\n\n1. We present PersONAL , a comprehensive Embodied AI benchmark specifically designed to evaluate embodied personalization, incorporating user-centric queries and object–ownership semantics.\n\n2. We release a dataset of 2,000 high-quality episodes sampled from over 30 realistic household environments, divided into three difficulty levels (easy, medium, and hard).\n\n3. We provide empirical analyses with state-of-the-art zero-shot baselines, showcasing limitations and possible future research directions toward human-level personalized navigation in Embodied AI.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在真实人类中心场景中实现个性化的具身智能代理。  \n2. 现有的个性化视觉语言模型在动态环境中的应用局限性。  \n3. 如何有效地评估和提升具身智能代理的个性化导航能力。  \n\n【用了什么创新方法】  \n本研究提出了PersONAL，一个综合性的基准，专注于个性化的具身智能代理。该基准包含2000个高质量的评估情节，涵盖30多个真实家庭环境，代理需根据用户特定的自然语言查询进行导航和物体定位。研究展示了当前具身智能代理在个性化导航任务中的性能差距，并指出了未来研究方向，强调了内部记忆和持续学习在复杂家庭环境中的重要性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving",
            "authors": "Carlo Bosio,Greg Woelki,Noureldin Hendy,Nicholas Roy,Byungsoo Kim",
            "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19789",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19789",
            "arxiv_html_link": "https://arxiv.org/html/2509.19789v1",
            "abstract": "Human drivers focus only on a handful of agents at any one time. On the other hand, autonomous driving systems process complex scenes with numerous agents, regardless of whether they are pedestrians on a crosswalk or vehicles parked on the side of the road.\nWhile attention mechanisms offer an implicit way to reduce the input to the elements that affect decisions, existing attention mechanisms for capturing agent interactions are quadratic, and generally computationally expensive.\nWe propose RDAR, a strategy to learn per-agent relevance — how much each agent influences the behavior of the controlled vehicle — by identifying which agents can be excluded from the input to a pre-trained behavior model.\nWe formulate the masking procedure as a Markov Decision Process where the action consists of a binary mask indicating agent selection.\nWe evaluate RDAR on a large-scale driving dataset, and demonstrate its ability to learn an accurate numerical measure of relevance by achieving comparable driving performance, in terms of overall progress, safety and performance, while processing significantly fewer agents compared to a state of the art behavior model.",
            "introduction": "Humans, when driving, do not pay equal attention to all agents around them (e.g., other vehicles, pedestrians).\nTransfomer-based attention models offer the promise of attending only to relevant components of the input, but existing attention models are typically quadratic in the size of the input space. Driving models encounter hundreds of input tokens, leading to substantial computational complexity and latency Harmel et al. (2023); Huang et al. (2024); Baniodeh et al. (2025).\n\nIn autonomous driving, there is a tension between the limited available compute resources and the desire to take advantage of scaling laws, large models, and test-time compute. Having access to numerical per-agent relevance scores would not only improve the interpretability of large driving models, but also allow compute resources to be prioritized for the features that are most important.\nIn fact, when agents and other scene elements are represented explicitly as tokens, reasoning about interactions between these tokens (typically through self-attention or graph neural network operations) is quadratic and difficult to reduce using low-rank or other approximations that work well for long-sequence data. Reducing the number of tokens under consideration provides quadratic improvements in FLOPs used.\n\nIn this work, we introduce RDAR (Reward-Driven Agent Relevance), through which we quantify agent relevance through a learned approach. The basic intuition is that if an agent is not relevant towards the driving decisions of the controlled vehicle, then its absence would not change the controlled vehicle’s driving behavior significantly. Thus, we quantify per-agent relevance by learning which agents can be masked out from the controlled vehicle’s planner input while maintaining a good driving behavior.\nWe formulate agent selection as a reinforcement learning (RL) problem where an action is a binary mask indicating which agents to include in the driving policy input, and which not to.\nThe RDAR scoring policy is trained in the loop with a frozen, pre-trained driving policy and a simulator. At each time step, based on the relevance scores, an agent mask is fed to the driving policy, making the controlled vehicle blind to the lower score agents. As it will be clear from the following sections, this is not a binary classification problem over agents due to the underlying system dynamics (e.g., not observing an agent now could lead to a collision later) and to the unavailability of ground truth labels.\nSome examples of relevance (color-coded) computed by our method are shown in Fig. 1.\nOur main contributions are:\n\nA novel reinforcement learning formulation for agent relevance estimation;\n\nA sampling-based mechanism for agent selection that enables efficient training and inference;\n\nA comprehensive evaluation showing that we can maintain driving performance while processing only a handful of surrounding agents.\n\n1. A novel reinforcement learning formulation for agent relevance estimation;\n\n2. A sampling-based mechanism for agent selection that enables efficient training and inference;\n\n3. A comprehensive evaluation showing that we can maintain driving performance while processing only a handful of surrounding agents.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效评估每个代理对自动驾驶行为的影响？  \n2. 如何减少输入代理的数量以降低计算复杂性？  \n3. 如何在保持驾驶性能的同时提高模型的可解释性？  \n\n【用了什么创新方法】  \n提出了RDAR（Reward-Driven Agent Relevance），通过强化学习方法量化每个代理的相关性，形成一个二进制掩码来选择输入代理。该方法在保持良好驾驶行为的同时，显著减少了处理的代理数量，从而降低了计算复杂性。评估表明，RDAR在大规模驾驶数据集上表现出与最先进的行为模型相当的驾驶性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "VIMD: Monocular Visual-Inertial Motion and Depth Estimation",
            "authors": "Saimouli Katragadda,Guoquan Huang",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19713",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19713",
            "arxiv_html_link": "https://arxiv.org/html/2509.19713v1",
            "abstract": "Accurate and efficient dense metric depth estimation is crucial for 3D visual perception in robotics and XR.\nIn this paper, we develop a monocular visual-inertial motion and depth (VIMD) learning framework to estimate dense metric depth by leveraging accurate and efficient MSCKF-based monocular visual-inertial motion tracking.\nAt the core the proposed VIMD is to exploit multi-view information to iteratively refine per-pixel scale,\ninstead of globally fitting an invariant affine model as in the prior work.\nThe VIMD framework is highly modular, making it compatible with a variety of existing depth estimation backbones.\nWe conduct extensive evaluations on the TartanAir and VOID datasets and demonstrate its zero-shot generalization capabilities on the AR Table dataset.\nOur results show that VIMD achieves exceptional accuracy and robustness, even with extremely sparse points—as few as 10-20 metric depth points per image.\nThis makes the proposed VIMD a practical solution for deployment in resource-constrained settings,\nwhile its robust performance and strong generalization capabilities offer significant potential across a wide range of scenarios.",
            "introduction": "For applications like robotics and extended reality (XR), accurate and efficient metric dense depth estimation is critical for 3D visual perception, which is essential for tasks such as obstacle avoidance and motion planning.\nMonocular methods, which estimate depth from a single RGB image, are particularly appealing because they use a compact, inexpensive, and common camera. However, purely monocular vision suffers from an inherent scale ambiguity: it can capture the relative shape of a scene but cannot determine the true distances to objects.\nKnowing exactly how far an obstacle is from the camera can be the difference between safe passage and collision.\nIntegrating inertial data can (partially) resolve this ambiguity.\nAs most mobile devices and robots are already equipped with an IMU,\nvisual–inertial odometry (VIO) or SLAM prevails but typically yields only sparse metric depth from a small set of tracked landmarks [1],\nwhich alone cannot provide the dense metric depth map required in safety-critical operations.\n\nAlthough recent advances in monocular depth prediction have achieved high-quality relative depth estimation [3, 4], these methods still lack the ability to produce outputs with absolute metric scale.\nA common approach assumes an invariant affinity model and aligns the predicted depth to the sparse VIO depth via per-frame least-squares fitting for a global scale and offset (e.g., see [5]).\nHowever, as shown in Fig. 1, the fitted scale and offset vary significantly across frames—particularly the offset, which exhibits much larger fluctuations than the scale.\nThis temporal instability indicates that the predicted relative depths are not truly affine with respect to the metric depths; that is, they cannot be accurately modeled by a single global scale and offset per frame without unmodeled errors.\nBesides the unreliable per-frame global fitting, the optimization may become poorly conditioned when sparse points are unevenly distributed or noisy, leading to sensitivity in the fitted parameters and potential error amplification in low-density regions.\nIt has been empirically observed that predicting offset along with scale can hurt depth accuracy [6].\n\nFrom Fig. 1, it is clear that the offset exhibits much higher variance across frames than the scale, implying that offsets may also vary more significantly across pixels within a single frame (due to factors like viewpoint changes, occlusions, or non-uniform scene structure), thus making them harder to learn reliably at the per-pixel level compared to the more stable scale.\nMotivated by this observation, we propose to exploit multi-view constraints and learn to optimize iteratively a per-pixel scale, formulated as learning approach that moves beyond the global fitting affine parameters.\nIn particular, the proposed monocular Visual-Inertial Motion and Depth (VIMD)\nleverages an accurate and efficient MSCKF-based visual-inertial motion estimation module to form multi-view geometric constraints\nand then refines and predicts dense depth and its uncertainty.\nAs a result, the proposed VIMD improves accuracy and robustness in low-density depth regions.\nFig. 2 shows some exemplar performance of the proposed VIMD in both outdoor and indoor scenes, producing high-quality dense metric depths.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在单目视觉中准确高效地估计稠密的度量深度。  \n2. 解决单目视觉方法中的尺度模糊问题，以提高深度估计的准确性和鲁棒性。  \n3. 如何利用视觉惯性数据来增强深度估计的性能，尤其是在稀疏点情况下。  \n\n【用了什么创新方法】  \n提出了一种单目视觉-惯性运动和深度(VIMD)学习框架，通过利用基于MSCKF的视觉惯性运动跟踪，迭代优化每个像素的尺度，克服了传统方法中全局拟合不稳定的问题。该框架兼容多种深度估计模型，并在TartanAir和VOID数据集上进行了广泛评估，展示了其在资源受限环境中的实用性和强大的零-shot泛化能力。VIMD在极少的深度点（仅10-20个）情况下，仍能实现卓越的准确性和鲁棒性，适用于多种场景。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar",
            "authors": "William L. Muckelroy III,Mohammed Alsakabi,John M. Dolan,Ozan K. Tonguz",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19644",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19644",
            "arxiv_html_link": "https://arxiv.org/html/2509.19644v1",
            "abstract": "LiDAR’s dense, sharp point cloud (PC) representations of the surrounding environment enable accurate perception and significantly improve road safety by offering greater scene awareness and understanding. However, LiDAR’s high cost continues to restrict the broad adoption of high-level Autonomous Driving (AD) systems in commercially available vehicles. Prior research has shown progress towards circumventing the need for LiDAR by training a neural network, using LiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds using only 4D Radars. One of the best examples is a neural network created to train a more efficient radar target detector with a modular 2D convolutional neural network (CNN) backbone and a temporal coherence network at its core that uses the RaDelft dataset for training [1]. In this work, we investigate the impact of higher-capacity segmentation backbones on the quality of the produced point clouds. Our results show that while very high-capacity models may actually hurt performance, an optimal segmentation backbone can provide a 23.7% improvement over the state-of-the-art (SOTA).",
            "introduction": "There are numerous reasons why widespread adoption of commercially available autonomous vehicles (AV) and commercially viable autonomous driving (AD) has not occurred, one of which is the cost-prohibitive sensor suites they employ. These systems are designed to significantly improve road safety through their greater awareness and understanding of the scene, allowing them to interact with the world and react to environmental disturbances in a safe manner. However, an essential component of these systems is LiDAR because of the sharp, dense point cloud (PC) representations of the environment they can produce. These PCs are often used with arrays of radars and cameras to detect people, cars, and other objects as the ego vehicle traverses the world. Of these three primary sensors, we see that LiDARs take a significant stake in the overall cost of the sensor suite in AVs, ranging from $4,000 for mid-range LiDARs to $70,000 for long-range high-end LiDARs like that used in the KITTI dataset [2][3]. However, 4D Radar is a relatively new emerging sensor in the AD space that offers promising performance at lower costs, as seen in Table I.\n\nPrevious research has shown that one can build a system that has the potential to replace LiDAR by leveraging concepts from Deep Learning (DL). By training a neural network using LiDAR PCs as ground truth (GT), one can produce LiDAR-like 3D PCs using only 4D radars [1][6][7][8][5][4]. An example of such a network is the neural network proposed in [1] and trained on the RaDelft dataset as a more efficient radar target detector. Using a modular and straightforward 2D convolutional neural network (CNN) backbone for segmentation and a temporal coherence network enabled the authors of [1] to achieve significant improvements in bidirectional chamfer distance (BCD) when compared to other methods like various forms of Constant False Alarm Rate (CFAR) (i.e., CA-CFAR, SOCA-CFAR, GOCA-CFAR, OS-CFAR, ML-CFAR, etc.) [9][10][11].\n\nOur work further investigates the impact of the 2D segmentation backbone and the number of 3D convolutional layers in the temporal coherence network on the overall performance of the predicted point clouds. This allows us to gain insight into how optimizations of the overall architecture can further minimize BCD and increase the viability of this technique as a potential replacement for LiDAR.\n\nThe remainder of this paper is organized as follows. Section II presents the background of the problem investigated and different definitions of key terms used throughout the paper. Section III outlines the methods used in this work for experimentation and how we evaluate the performance of such experiments. Section IV presents the primary findings of our work. Finally, Section V summarizes the discussions that have been held throughout this paper, along with our intentions to further improve the metrics and results in the future.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用4D雷达替代高成本的LiDAR进行3D点云预测。  \n2. 2D分割骨干网络对生成点云质量的影响。  \n3. 高容量模型在点云生成中的性能表现。  \n\n【用了什么创新方法】  \n本研究通过训练神经网络，使用LiDAR点云作为真实值，探索了不同2D分割骨干网络对生成点云质量的影响。重点在于优化网络架构，包括2D CNN骨干和时间一致性网络的3D卷积层数量。结果表明，适当的分割骨干网络可以在性能上比现有技术提高23.7%。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation",
            "authors": "Ramy ElMallah,Krish Chhajer,Chi-Guhn Lee",
            "subjects": "Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "Accepted to the CoRL 2025 Eval&Deploy Workshop",
            "pdf_link": "https://arxiv.org/pdf/2509.19524",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19524",
            "arxiv_html_link": "https://arxiv.org/html/2509.19524v1",
            "abstract": "Robot learning papers typically report a single binary success rate (SR), which obscures where a policy succeeds or fails along a multi-step manipulation task. We argue that subgoal-level reporting should become routine: for each trajectory, a vector of per-subgoal SRs that makes partial competence visible (e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware plug-in evaluation framework that utilizes vision–language models (VLMs) as automated judges of subgoal outcomes from recorded images or videos. Rather than proposing new benchmarks or APIs, our contribution is to outline design principles for a scalable, community-driven open-source project. In StepEval, the primary artifact for policy evaluation is the per-subgoal SR vector; however, other quantities (e.g., latency or cost estimates) are also considered for framework-optimization diagnostics to help the community tune evaluation efficiency and accuracy when ground-truth subgoal success labels are available. We discuss how such a framework can remain model-agnostic, support single- or multi-view inputs, and be lightweight enough to adopt across labs. The intended contribution is a shared direction: a minimal, extensible seed that invites open-source contributions, so that scoring the steps, not just the final goal, becomes a standard and reproducible practice.",
            "introduction": "Robotic manipulation research largely evaluates policies with a single binary metric (success or failure). While simple to report, success rate (SR) provides a general, non-granular view of performance [1, 2]. A policy either achieves the task or not, with no insight into partial progress or which sub-task caused the failure. This is especially problematic for long-horizon tasks composed of multiple sequential subgoals (e.g., grasp, lift, place in a pick-and-place task). A low success rate might indicate failure, but it does not reveal where the policy struggled. As a motivating example, Kress-Gazit et al. [1] describe a pancake-flipping task consisting of several stages (picking up spatulas, flipping the pancake, then plating it). One policy in their study had only 17% overall success, yet it completed the first two subgoals 100% of the time; it consistently grasped the spatulas and flipped the pancake, failing only at the final plating step. If judged solely by overall SR, this policy would be deemed poor, obscuring the fact that it mastered over half the task. Granular evaluation is essential: without it, researchers cannot pinpoint failure modes (e.g., was it the grasp or the pour that failed?), making improvements guesswork.\n\nThe need for fine-grained, nuanced evaluation in robot learning has been increasingly recognized. Recent best-practice guidelines explicitly recommend reporting subgoal completion metrics and failure mode analyses alongside overall success [1, 3, 4]. However, adopting such practices in everyday research is challenging: manually labeling each sub-step success for dozens or hundreds of experiments is labor-intensive and subjective. Some works have automated subgoal checks in simulation (where the state is known) or relied on human labeling for physical trials. Vision-language models (VLMs) offer a promising middle ground, a way to automatically classify outcomes from visual data. Indeed, initial attempts have been made to use learned classifiers [5, 6] or VLMs [7] to detect task failures.\n\nOur goal is to reframe evaluation practice by advocating a subgoal-first view of manipulation metrics. We present StepEval as a blueprint for a cost-aware, plug-in framework that treats the per-subgoal success-rate (SR) vector as the primary artifact for policy evaluation. In this view, a VLM acts as a black-box automated judge that maps recorded imagery to subgoal outcomes. We articulate design principles for a community-built, scalable open-source project. All other quantities (e.g., latency, token/image cost, or confusion matrices based on ground-truth subgoal labels) are framed as framework-optimization diagnostics that help tune evaluation efficiency and accuracy when ground-truth labels are available, rather than as core evaluation metrics.\n\nWe contribute by articulating the following pillars for subgoal-based evaluation:\n\nSubgoal-first evaluation perspective. We formalize the evaluation target as a trajectory-level vector 𝐲∈{0,1}n\\mathbf{y}\\in\\{0,1\\}^{n} of per-subgoal SRs and argue that this granular object, rather than a single binary task SR, should become the standard reported outcome for manipulation studies.\n\nModel-agnostic judging concept. We propose using a VLM as a judge to infer subgoal outcomes from RGB image frames (single- or multi-view) without instrumenting policies or environments, keeping the framework post-hoc and lightweight. The design is intentionally policy- and model-agnostic, with the aim of making scoring the steps a routine, reproducible practice across labs.\n\nSeparation of evaluation vs. framework optimization metrics. We distinguish the primary evaluation artifact (per-subgoal SR vector) from optional diagnostics (e.g., cost/latency; confusion matrices based on ground-truth subgoal labels; or optional prompt-optimization procedures). The latter are intended only to help the community tune efficiency and accuracy when labels exist, not to redefine the core evaluation target.\n\n1. Subgoal-first evaluation perspective. We formalize the evaluation target as a trajectory-level vector 𝐲∈{0,1}n\\mathbf{y}\\in\\{0,1\\}^{n} of per-subgoal SRs and argue that this granular object, rather than a single binary task SR, should become the standard reported outcome for manipulation studies.\n\n2. Model-agnostic judging concept. We propose using a VLM as a judge to infer subgoal outcomes from RGB image frames (single- or multi-view) without instrumenting policies or environments, keeping the framework post-hoc and lightweight. The design is intentionally policy- and model-agnostic, with the aim of making scoring the steps a routine, reproducible practice across labs.\n\n3. Separation of evaluation vs. framework optimization metrics. We distinguish the primary evaluation artifact (per-subgoal SR vector) from optional diagnostics (e.g., cost/latency; confusion matrices based on ground-truth subgoal labels; or optional prompt-optimization procedures). The latter are intended only to help the community tune efficiency and accuracy when labels exist, not to redefine the core evaluation target.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有的机器人学习评估方法仅使用单一的成功率（SR），缺乏对多步骤任务的细粒度分析。  \n2. 研究者无法准确识别失败模式，导致改进过程变得困难和不确定。  \n3. 手动标注每个子步骤的成功与否既费时又主观，亟需自动化解决方案。  \n\n【用了什么创新方法】  \n提出了一种名为StepEval的框架，利用视觉-语言模型（VLM）自动评估机器人操作中的子目标成功率（SR）。该框架以每个子目标的SR向量作为主要评估指标，旨在提供细粒度的任务完成情况反馈。通过将VLM作为黑箱评判者，StepEval能够从记录的图像中推断出子目标结果，且不需要对政策或环境进行额外的仪器化。该方法保持轻量级和模型无关，促进了社区的开放源代码贡献，推动了细化评估的标准化实践。最终，StepEval为机器人操作的评估提供了更高效和准确的途径。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Robust Near-Optimal Nonlinear Target Enclosing Guidance",
            "authors": "Abhinav Sinha,Rohit V. Nanavati",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO); Optimization and Control (math.OC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19477",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19477",
            "arxiv_html_link": "https://arxiv.org/html/2509.19477v1",
            "abstract": "This paper proposes a nonlinear optimal guidance law that enables a pursuer to enclose a target within arbitrary geometric patterns, which extends beyond conventional circular encirclement. The design operates using only relative state measurements and formulates a target enclosing guidance law in which the vehicle’s lateral acceleration serves as the steering control, making it well-suited for aerial vehicles with turning constraints. Our approach generalizes and extends existing guidance strategies that are limited to target encirclement and provides a degree of optimality. At the same time, the exact information of the target’s maneuver is unnecessary during the design. The guidance law is developed within the framework of a state-dependent Riccati equation (SDRE), thereby providing a systematic way to handle nonlinear dynamics through a pseudo-linear representation to design locally optimal feedback guidance commands through state-dependent weighting matrices. While SDRE ensures near-optimal performance in the absence of strong disturbances, we further augment the design to incorporate an integral sliding mode manifold to compensate when disturbances push the system away from the nominal trajectory, and demonstrate that the design provides flexibility in a sense that the (possibly time-varying) stand-off curvature could also be treated as unknown. Simulations demonstrate the efficacy of the proposed approach.",
            "introduction": "Driven by increasing autonomy requirements, unmanned aerial vehicles (UAVs) have become integral to a variety of mission-critical operations such as area coverage, surveillance, environmental monitoring, aerial defense, reconnaissance, and search and rescue, e.g., [1, 2, 3, 4]. A common control objective in these applications involves regulating the motion of a UAV designated as the pursuer to maintain a prescribed geometric configuration or trajectory with respect to a reference entity, termed the target. The target may correspond to another vehicle, a stationary beacon, or a dynamically evolving point of interest. This objective, referred to as target enclosing, entails enforcing spatial constraints on the relative position of the pursuer with respect to the target, and is central to both civilian and military guidance and coordination strategies.\n\nOne of the earliest investigations into target enclosing using teams of mobile robots was presented in [5], where the agents were modeled under the assumption of holonomic dynamics. Subsequent works extended this framework, leveraging the relative simplicity of control synthesis afforded by holonomic vehicle models. However, in practical scenarios, the dynamics of mobile platforms are more accurately represented by non-holonomic constraints, which significantly complicate both analysis and control design. Target enclosing under non-holonomic vehicle dynamics has thus been examined in a more realistic setting in works such as [6, 7], often within the broader context of distributed formation control. Within this domain, a task of interest is maintaining circular motion around a target (circumnavigation). This specific formation behavior has attracted substantial research attention due to its relevance in surveillance, tracking, and protection missions, as evidenced by a wide body of literature (e.g., [8, 9, 10, 11, 12]).\n\nOne approach is to leverage vector fields defined around the desired trajectory or geometric pattern to regulate the vehicle’s heading and generate convergence behavior. In [13], the authors employed a Lyapunov-based vector field to coordinate the circumnavigation of a stationary target at multiple altitudes using a fleet of UAVs. The work in [14] extended this methodology to account for external disturbances, proposing a coordinated standoff tracking strategy in the presence of wind. In contrast to purely reactive vector field approaches, the work in [15] presented a nonlinear model predictive control framework for dual-vehicle coordinated standoff tracking, optimizing performance over a finite prediction horizon. An alternative formulation in [11] utilized spherical pendulum-inspired dynamics, wherein backstepping and Lyapunov-based techniques were applied to regulate position and velocity tracking errors for robust guidance law synthesis. Beyond vector field and predictive control approaches, several recent strategies have explored guidance law design under partial or minimal state information, often motivated by scenarios with limited sensing or communication capabilities. These methods typically employ either relative range and or bearing measurements (see, for example, [12, 10, 16, 17, 8]).\n\nDespite such solutions, several key challenges remain unaddressed. This work advances the state of the art in target-enclosing guidance by addressing limitations of existing methods, which we summarize below:\n\nMany of the aforementioned methods assume slow or negligible target motion, which may become limited under adversarial scenarios if the target moves aggressively. The proposed framework explicitly accommodates a general maneuvering target without requiring knowledge of its guidance law or future maneuvers.\n\nWhile previous studies rely on curvature information of the enclosing geometry, our formulation has the flexibility to eliminate this requirement, thereby broadening applicability to arbitrary geometric patterns provided the prescribed range-parameterized curve is at least ℂ2\\mathbb{C}^{2}.\n\nWe introduce a robust nonlinear optimal guidance design for this problem class by integrating an SDRE framework with a supertwisting sliding mode controller defined over an integral sliding manifold. This hybrid design leverages the optimality of SDRE while inheriting robustness and finite-time convergence properties from sliding mode control, where the reaching phase is absent.\n\n1. Many of the aforementioned methods assume slow or negligible target motion, which may become limited under adversarial scenarios if the target moves aggressively. The proposed framework explicitly accommodates a general maneuvering target without requiring knowledge of its guidance law or future maneuvers.\n\n2. While previous studies rely on curvature information of the enclosing geometry, our formulation has the flexibility to eliminate this requirement, thereby broadening applicability to arbitrary geometric patterns provided the prescribed range-parameterized curve is at least ℂ2\\mathbb{C}^{2}.\n\n3. We introduce a robust nonlinear optimal guidance design for this problem class by integrating an SDRE framework with a supertwisting sliding mode controller defined over an integral sliding manifold. This hybrid design leverages the optimality of SDRE while inheriting robustness and finite-time convergence properties from sliding mode control, where the reaching phase is absent.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有方法假设目标运动缓慢，无法应对快速移动目标的情况。  \n2. 许多研究依赖于包围几何形状的曲率信息，限制了应用范围。  \n3. 如何设计一个鲁棒的非线性最优引导法则以应对复杂动态环境。  \n\n【用了什么创新方法】  \n本研究提出了一种非线性最优引导法则，能够在任意几何模式下包围目标，扩展了传统的圆形包围方法。该方法基于状态依赖里卡提方程(SDRE)框架，结合超扭滑模控制器，确保在存在扰动时的鲁棒性和有限时间收敛性。通过仿真验证了该方法的有效性，显示出其在动态环境中对目标的灵活适应能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Learning from Observation: A Survey of Recent Advances",
            "authors": "Returaj Burnwal,Hriday Mehta,Nirav Pravinbhai Bhatt,Balaraman Ravindran",
            "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO); Machine Learning (stat.ML)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19379",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19379",
            "arxiv_html_link": "https://arxiv.org/html/2509.19379v1",
            "abstract": "Imitation Learning (IL) algorithms offer an efficient way to train an agent by mimicking an expert’s behavior without requiring a reward function. IL algorithms often necessitate access to state and action information from expert demonstrations. Although expert actions can provide detailed guidance, requiring such action information may prove impractical for real-world applications where expert actions are difficult to obtain. To address this limitation, the concept of learning from observation (LfO) or state-only imitation learning (SOIL) has recently gained attention, wherein the imitator only has access to expert state visitation information. In this paper, we present a framework for LfO and use it to survey and classify existing LfO methods in terms of their trajectory construction, assumptions and algorithm’s design choices. This survey also draws connections between several related fields like offline RL, model-based RL and hierarchical RL. Finally, we use our framework to identify open problems and suggest future research directions.",
            "introduction": "Imitation learning, often studied as part of reinforcement learning (RL), has emerged as a promising paradigm for training autonomous agents to perform tasks effectively by mimicking demonstrator behavior (Hussein et al., 2017a). In contrast to RL, which relies on environment reward, IL has the distinct advantage of learning solely from expert guidance, which alleviates the need to design a reward function. This feature proves particularly beneficial in complex environments where crafting a suitable reward function can be challenging. Standard methods developed in this framework often require access to demonstrations, in the form of state-action (st,at)(s_{t},a_{t}) pairs. While expert actions can provide detailed guidance, requiring such action information may be restrictive in practical scenarios. For instance, a human expert may demonstrate the object-manipulation task to a robot by manually moving the robot’s arm. The robot can record both its joint angles (state) and joint torques (action) induced by the human expert. Collecting such state-action pair demonstrations can be challenging or, in some cases, impossible; such as environments with limited access to physical demonstrators.\n\nLearning from demonstration (LfD) aims to replicate the expert’s actions for a given state; this introduces a constraint that both the expert and the imitator must share the same dynamics model: specifically, they must have identical action spaces and the exact next-state transition probabilities for all feasible state-action pairs. This assumption brings severe limitations, as imagine that a robot with a low-speed limit imitates another robot that moves fast; then, using the LfD framework, a slow robot cannot learn to mimic the behavior of the fast-moving robot. The LfD framework does not support transfer learning across dynamically different agents (Liu et al., 2020a).\n\nWhile IL research is inspired by the way humans learn from observations of another human, there exists a notable difference in the imitation process employed by humans. Unlike LfD, which often relies on demonstrator’s action information to guide behavior, humans typically imitate without requiring explicit action information. A more general framework, known as learning from observation (LfO) or state-only imitation learning (SOIL), involves an expert communicating solely the state sequence information using raw sequences of true agent states, or partial observations like images or videos. In comparison to LfD, LfO aligns more closely with the natural way humans learn to imitate in the physical world, a phenomenon referred to as observational learning in psychology (Bandura and Walters, 1977). Moreover, the LfO framework can support transfer learning across dynamically different agents, as it does not rely on demonstrator’s action information.\n\nThe recent works on LfO have demonstrated significant success in both simulated environments and real-world tasks, however the literature lacks a systematic review of this field. Though a novel taxonomy for LfO was proposed by Torabi et al. (2019b), we found that the recent LfO algorithms either do not neatly fit or cannot be categorised based on the established taxonomy (see section 9 for a detailed discussion of related work). Hence, this article presents a survey of the literature based on the fundamental questions on trajectory dataset construction and algorithm’s design choices.\n\nOverview\n\nThe goal of this article is to present a comprehensive overview of learning from observation and to provide an over-arching framework to formalize this class of methods. We aim to define classification criteria based on the trajectory data set’s construction and algorithms’ design choices. We address the following fundamental questions to structure our classification:\n\nWho qualifies as an expert? An expert can be a human, a distinct dynamical agent from the imitator, or the same dynamical agent as the imitator. The formulation of the algorithm varies depending on the identity of the expert with respect to the imitator. (Section 3.1)\n\nHow are expert trajectories collected? These trajectories can be collected in two primary ways: i) in a first-person viewpoint, where the state trajectories are collected from the agent-centric viewpoint. Collecting first-person demonstrations can be challenging or, in some instances, impossible. A more natural way would be to collect demonstrations from a ii) third-person viewpoint. (Section: 3.2)\n\nWhat are the different trajectory datasets? In section 3.3, we survey different LfO algorithms based on the trajectory dataset they use to learn from expert demonstrations. A trajectory dataset can contain demonstrations from a single or multiple experts. Additionally, we provide a discussion of how the trajectory dataset varies depending on whether the algorithm operates online or offline.\n\nHow are algorithms formulated to facilitate learning from state-only demonstrations? In section 4, we define and discuss our framework for classifying LfO approaches based on algorithm design choices.\n\nIn the next section, we will provide the necessary background for LfO algorithms. Section 3, 4 forms the survey’s core and will provide a comprehensive overview of the recent LfO methodologies. In Section 5, we discuss how these algorithms are used to address learning across different viewpoints and non-identical agents. In Section 6, we discuss the datasets, benchmarks and algorithms used in LfO research. Following that, in Section 7, we will discuss the connections between LfO algorithms and other related fields in RL. Subsequently, in Section 8, we will use our framework to identify open problems and discuss future directions. Lastly, we will conclude the article by discussing related surveys in Section 9 and summarize this article in Section 10.\n\n1. Who qualifies as an expert? An expert can be a human, a distinct dynamical agent from the imitator, or the same dynamical agent as the imitator. The formulation of the algorithm varies depending on the identity of the expert with respect to the imitator. (Section 3.1)\n\n2. How are expert trajectories collected? These trajectories can be collected in two primary ways: i) in a first-person viewpoint, where the state trajectories are collected from the agent-centric viewpoint. Collecting first-person demonstrations can be challenging or, in some instances, impossible. A more natural way would be to collect demonstrations from a ii) third-person viewpoint. (Section: 3.2)\n\n3. What are the different trajectory datasets? In section 3.3, we survey different LfO algorithms based on the trajectory dataset they use to learn from expert demonstrations. A trajectory dataset can contain demonstrations from a single or multiple experts. Additionally, we provide a discussion of how the trajectory dataset varies depending on whether the algorithm operates online or offline.\n\n4. How are algorithms formulated to facilitate learning from state-only demonstrations? In section 4, we define and discuss our framework for classifying LfO approaches based on algorithm design choices.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在缺乏专家动作信息的情况下进行模仿学习？  \n2. 如何分类和评估现有的学习观察（LfO）方法？  \n3. LfO方法如何支持不同动态代理之间的迁移学习？  \n4. 当前LfO领域存在哪些开放问题和未来研究方向？  \n\n【用了什么创新方法】  \n本文提出了一个框架用于学习观察（LfO），并对现有的LfO方法进行分类，重点关注轨迹构建、假设和算法设计选择。通过对不同专家身份、轨迹收集方式和数据集的分析，本文系统性地整理了LfO领域的研究进展。此外，文章还探讨了LfO与其他强化学习相关领域的联系，并识别了当前研究中的开放问题和未来的研究方向。通过这些分析，本文为LfO的研究提供了一个全面的视角和结构化的分类标准。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-25 02:44:12",
            "title": "Scensory: Automated Real-Time Fungal Identification and Spatial Mapping",
            "authors": "Yanbaihui Liu,Erica Babusci,Claudia K. Gunsch,Boyuan Chen",
            "subjects": "Signal Processing (eess.SP); Robotics (cs.RO)",
            "comment": "Our project website is at:this http URL",
            "pdf_link": "https://arxiv.org/pdf/2509.19318",
            "code": "http://generalroboticslab.com/Scensory",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19318",
            "arxiv_html_link": "https://arxiv.org/html/2509.19318v1",
            "abstract": "Indoor fungal contamination poses significant risks to public health, yet existing detection methods are slow, costly, and lack spatial resolution. Conventional approaches rely on laboratory analysis or high-concentration sampling, making them unsuitable for real-time monitoring and scalable deployment. We introduce Scensory, a robot-enabled olfactory system that simultaneously identifies fungal species and localizes their spatial origin using affordable volatile organic compound (VOC) sensor arrays and deep learning. Our key idea is that temporal VOC dynamics encode both chemical and spatial signatures, which we decode through neural architectures trained on robot-automated data collection. We demonstrate two operational modes: a passive multi-array configuration for environmental monitoring, and a mobile single-array configuration for active source tracking. Across five fungal species, our system achieves up to 89.85% accuracy in species detection and 87.31% accuracy in localization under ambient conditions, where each prediction only takes 3–7 s sensor inputs. Additionally, by computationally analyzing model behavior, we can uncover key biochemical signatures without additional laboratory experiments. Our approach enables real-time, spatially aware fungal monitoring and establishes a scalable and affordable framework for autonomous environmental sensing.",
            "introduction": "Indoor fungal contamination poses significant risks to human health and building safety [1]. Mold thrives in damp and poorly ventilated environments, especially in older homes and infrastructure [2]. Exposure to airborne fungal emissions has been linked to asthma, allergies, and other respiratory illnesses, particularly in children and people with weakened immune systems [3, 4]. Yet current detection methods are slow, invasive, and expensive. They require laboratory processing and offer limited information about the spatial distribution of contamination. The lack of scalable, real-time, and spatially aware detection systems continues to hinder timely intervention and effective mitigation of fungal exposure risks.\n\nTraditional fungal detection methods are constrained by high cost, long processing time, and technical complexity. Culture-based assays aim to grow fungal colonies from surface or air samples and identify species through morphological or physiological features [5, 6]. While widely used, these approaches require incubation periods that can delay results, and the subsequent identification process is often manual, requiring expert interpretation or additional laboratory analysis.\nMicroscopic identification of spores and hyphae requires expert interpretation, offers limited species resolution, and lacks sensitivity at low contamination levels [7]. Molecular diagnostics such as qPCR and ITS-based sequencing improves sensitivity and specificity [8], but require sample preprocessing and specialized laboratory infrastructure, and can be confounded by DNA from nonviable or nonpathogenic organisms [9]. While accurate, these methods are impractical for routine monitoring or rapid field deployment.\n\nAs a non-invasive alternative, volatile organic compound (VOC) analysis has gained attention. Fungi emit diverse VOCs during metabolism, including alcohols, ketones, and sulfur-based compounds, that serve as chemical fingerprints [10, 11, 12]. Electronic nose (eNose) systems [13] use arrays of cross-sensitive sensors to capture these emissions, increasingly paired with machine learning classifiers. Recent advances have enabled real-time fungal detection across food, agriculture, and environmental applications [14, 15, 16, 17]. With appropriate training, ML-enhanced eNoses have achieved species-level accuracy exceeding 90% in controlled settings [18, 19]. However, these systems remain spatially blind and typically require sealed chambers, pre-concentration, or extended exposure times [20]. They cannot localize fungal sources in ambient real-world environments [21].\n\nIn parallel, mobile robots and autonomous agents have made progress in gas plume tracking and odor source localization under turbulent or outdoor conditions [22, 23, 24, 25]. These strategies leverage visible or high-concentration plumes and rely on structured airflow dynamics [26, 27], making them unsuitable for detecting the subtle, low-density, and highly variable emissions characteristic of indoor fungal growth.\n\nDespite substantial advances, existing technologies can either identify fungal species without spatial resolution, or they can localize odor sources but cannot handle complex and low-level VOC signatures from fungi. Such a disconnect highlights a critical gap: we lack a scalable and real-time approach that can simultaneously determine “what” fungal species are present and “where” they are distributed under natural and low concentration conditions.\n\nWe present Scensory, a robot-enabled chemical sensing system that performs simultaneous species identification and source localization using affordable VOC sensor arrays and deep learning. The key insight is that complex temporal VOC dynamics encode both chemical identity and spatial information, which can be decoded using neural networks. We demonstrate this system in two operational modes: a passive multi-array configuration for environmental monitoring, and a mobile single-array configuration for real-world deployments where only limited resources are available, or rapid, mobile, or cost-effective solutions are required. Furthermore, our modeling framework allows us to infer which sensor inputs are most biochemically relevant for species identification.\n\nTo enable scalable model training, we automate high-throughput data collection using a robotic arm that samples VOCs under various species and spatial arrangements. Across five phylogenetically diverse fungal species, our system achieves up to 89.85% accuracy in species classification and 87.31% spatial localization accuracy under ambient conditions, where each prediction only takes 3–7 s sensor inputs. We further demonstrate the potential of our approach through real-world deployment on a mobile robot in a residential environment. Our results establish a scalable and practical framework for autonomous real-time fungal monitoring that bridges chemical perception and spatial inference, paving a way to advance the state of the art in environmental sensing, health diagnostics, and autonomous robotic inspection.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有室内真菌检测方法速度慢、成本高且缺乏空间分辨率。  \n2. 传统方法无法实时监测和定位真菌源，影响及时干预和有效减轻风险。  \n3. 现有技术无法同时识别真菌种类和其空间分布。  \n\n【用了什么创新方法】  \n我们提出了Scensory，一个机器人辅助的化学传感系统，能够同时进行真菌种类识别和源定位。该系统利用经济实惠的挥发性有机化合物（VOC）传感器阵列和深度学习，解码复杂的VOC动态信号。通过自动化的高通量数据收集，我们的系统在五种真菌物种上实现了高达89.85%的种类分类准确率和87.31%的空间定位准确率。该方法在真实环境中展示了其可扩展性和实用性，为自主实时真菌监测奠定了基础。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        }
    ],
    "2025-09-26": [
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds",
            "authors": "Luis Augenstein,Noémie Jaquier,Tamim Asfour,Leonel Rozo",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21281",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21281",
            "arxiv_html_link": "https://arxiv.org/html/2509.21281v1",
            "abstract": "Human-like motion generation for robots often draws inspiration from biomechanical studies, which often categorize complex human motions into hierarchical taxonomies. While these taxonomies provide rich structural information about how movements relate to one another, this information is frequently overlooked in motion generation models, leading to a disconnect between the generated motions and their underlying hierarchical structure. This paper introduces the  Gaussian Process Hyperbolic Dynamical Model (GPHDM), a novel approach that learns latent representations preserving both the hierarchical structure of motions and their temporal dynamics to ensure physical consistency.\nOur model achieves this by extending the dynamics prior of the Gaussian Process Dynamical Model (GPDM) to the hyperbolic manifold and integrating it with taxonomy-aware inductive biases. Building on this geometry- and taxonomy-aware frameworks, we propose three novel mechanisms for generating motions that are both taxonomically-structured and physically-consistent: two probabilistic recursive approaches and a method based on pullback-metric geodesics. Experiments on generating realistic motion sequences on the hand grasping taxonomy show that the proposed GPHDM faithfully encodes the underlying taxonomy and temporal dynamics, and generates novel physically-consistent trajectories.",
            "introduction": "Designing robots with human-like capabilities is a long-standing goal in robotics, often drawing inspiration from biomechanics to achieve realistic and functional motions [2]. A critical aspect in this process is analyzing human movements, for which researchers often structure complex actions into hierarchical classifications known as taxonomies. These taxonomies, which categorize hand postures [3], full-body poses [4], and manipulation primitives [5], among others, carry a rich hierarchical structure that reflects the relationships among different movements. However, this crucial structural information is frequently ignored in motion generation literature. Recent works [6, 7] have shown that leveraging this structural inductive bias into motion generation models may alleviate the demand of training data, lead to human-like movements, and ultimately generate new motions that comply with the hierarchical structure of a given taxonomy.\n\nEarly approaches to generating motions within these taxonomies showed promise but did not directly leverage their hierarchical nature. For example, Romero et al. [8] used a  Gaussian Process Latent Variable Model (GPLVM) [9], where clusters corresponding to various grasp types of the GRASP taxonomy [3] emerged naturally, despite that the taxonomy’s structure was not an explicitly considered during training. They identified the potential for generating new motions via latent space interpolation as potential future work. Separately, Mandery et al. [10] abstracted the motion generation problem as a linguistic task, using probabilistic nn-gram language models to reproduce whole-body motions based on the types of whole-body poses proposed in [4]. Discrete body poses were represented by words and sequenced using sentences. However, this discrete representation struggled to capture the continuous nature of movement and overlooked the entire hierarchical structure of the associated taxonomy.\n\nTo address this gap, recent work by Jaquier et al. [6] introduced the  Gaussian Process Hyperbolic Latent Variable Model (GPHLVM), which explicitly accounts for the hierarchical structure of taxonomy data. Their key insight was to leverage hyperbolic geometry [11], a natural fit for embedding tree-like structures [12, 13, 14], to create a continuous latent representation of the hierarchically-organized taxonomy data. In the GPHLVM, high-dimensional observations (e.g., joint angles of a human body or hand) belonging to the same taxonomy node were embedded closely together, forming distinct clusters in the hyperbolic space. Crucially, they demonstrated that hyperbolic geodesics between these clusters correctly traversed intermediate clusters, mirroring the parent–child relationships of the original taxonomy. These insights confirmed that the GPHLVM could successfully learn a latent space that preserves the hierarchical structure of complex motion data.\nHowever, a significant limitation remains: While the GPHLVM can generate novel motions by decoding latent geodesics back to the high-dimensional joint space, some of the resulting motions can be physically impractical. This issue arises because the GPHLVM is trained on data concentrated within the clusters (i.e., static poses) of the taxonomy, leaving data-sparse regions in-between. In these regions, the model lacks information about valid trajectories and thus its predictions revert to non-informative mean, failing to capture the underlying dynamics of target movements.\n\nThis paper tackles this challenge. First, we propose the Gaussian Process Hyperbolic Dynamical Model (GPHDM), which learns latent representations that preserve not only the hierarchical structure of motions but also their temporal dynamics to ensure physical consistency (see Fig. LABEL:fig:teaser). To achieve this, we extend the dynamics model of the  Gaussian Process Dynamical Model (GPDM) [15] to the hyperbolic manifold and integrate it into the taxonomy-aware framework of GPHLVM. Second, we introduce three novel motion generation mechanisms that are both taxonomically-structured and physically-consistent, namely: two probabilistic recursive approaches, and a pullback-metric geodesics method. We test our approach on the hand grasping taxonomy [1], showing that the proposed GPHDM successfully preserves both the hierarchical structure and temporal dynamics of the data, while allowing us to generate novel physically-consistent motions.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将人类运动的层次结构有效融入机器人运动生成模型中。  \n2. 如何在生成运动时确保物理一致性，避免生成不切实际的运动轨迹。  \n\n【用了什么创新方法】  \n本文提出了高斯过程超曲面动力学模型（GPHDM），通过扩展高斯过程动力学模型（GPDM）到超曲面，并结合层次结构的归纳偏置，学习保留运动的层次结构和时间动态的潜在表示。提出了三种新颖的运动生成机制，包括两种概率递归方法和基于回拉度量测地线的方法。实验结果表明，GPHDM能够忠实地编码运动的层次结构和时间动态，同时生成新颖且物理一致的运动轨迹。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "\\LARGE GMP$^{3}$: Learning-Driven, Bellman-Guided Trajectory Planning for UAVs in Real-Time on SE(3)",
            "authors": "Babak Salamat,Dominik Mattern,Sebastian-Sven Olzem,Gerhard Elsbacher,Christian Seidel,Andrea M. Tonello",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21264",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21264",
            "arxiv_html_link": "https://arxiv.org/html/2509.21264v1",
            "abstract": "We propose GMP3\\text{GMP}^{3}, a multiphase global path planning framework that generates dynamically feasible three-dimensional trajectories for unmanned aerial vehicles (UAVs) operating in cluttered environments. The framework extends traditional path planning from Euclidean position spaces to the Lie group SE​(3)\\mathrm{SE}(3), allowing joint learning of translational motion and rotational dynamics. A modified Bellman-based operator is introduced to support reinforcement learning (RL) policy updates while leveraging prior trajectory information for improved convergence. GMP3\\text{GMP}^{3} is designed as a distributed framework in which agents influence each other and share policy information along the trajectory: each agent refines its assigned segment and shares with its neighbors via a consensus-based scheme, enabling cooperative policy updates and convergence toward a path shaped globally even under kinematic constraints. We also propose DroneManager, a modular ground control software that interfaces the planner with real UAV platforms via the MAVLink protocol, supporting real-time deployment and feedback. Simulation studies and indoor flight experiments validate the effectiveness of the proposed method in constrained 3D environments, demonstrating reliable obstacle avoidance and smooth, feasible trajectories across both position and orientation. The open-source implementation is available at https://github.com/Domattee/DroneManager.",
            "introduction": "Recent advances in control systems have driven the widespread adoption of unmanned vehicles—including underwater, ground, and aerial systems (UAVs)—for a variety of applications. Classical approaches to motion planning, such as sampling-based methods, trajectory interpolation [1] or control based [2], often rely on complete prior knowledge of the workspace and can struggle to scale or adapt to high dimensional or dynamic scenarios. As a result, many recent studies have focused on integrating machine learning and reinforcement leaning (RL) techniques into path planning to enable adaptability, improved performance, and broader applicability [3, 4, 5].\n\nActive Simultaneous Localization and Mapping (SLAM) methods (e.g., [6]) have explored how to acquire new information about an unknown environment, actively balancing exploration and exploitation. The relevance of these works brings attention to the necessity of policy design for navigation under limited or uncertain global knowledge. However, many active SLAM frameworks are not specifically designed for fully autonomous path planning in dynamic or cluttered spaces; they often focus on maintaining accurate localization and building maps, rather than enforcing strict dynamic feasibility or advanced multi-agent coordination.\n\nDeep neural networks have opened new avenues in path planning, as demonstrated in [7], where Motion Planning Networks leverage a learned heuristic to generate sub-optimal paths. These architectures significantly reduce execution time compared to classical methods like RRT∗ [8] or A∗ [9]. On the other hand, they may require extensive offline training data and can suffer from domain shift when deployed in previously unseen contexts or dynamic environments. Similarly, cooperative motion planning in multi-robot or congested scenarios has been addressed through congestion-aware RL-based approaches [10], which improves real-time decision-making yet often requires delicate reward tuning and can face challenges with global convergence under high robot density. In unknown and partially observable workspaces, an integral RL framework has been proposed to unify mapping, localization, and policy improvement under uncertain conditions [11], but many solutions must still combat high sample complexity and may lack strong safety guarantees. For ground vehicles operating in harsh terrains, inverse RL has shown promise in learning sophisticated cost maps directly from expert trajectories [12], although it can be sensitive to distribution mismatches between training and execution conditions. Other learning-based planners—such as those that combine global and local policies [13], exploit dynamic graph representations for online adaptation [13], or focus on UAV data gathering using time-varying Q-learning [14]—highlight the versatility of learning methods in a range of planning problems. Nevertheless, these approaches can be limited by high-dimensional action spaces, restrictive sensor assumptions, and the risk of suboptimal exploration strategies. However, developing and rigorously validating advanced control algorithms [15, 16]—and, by extension, inertial-navigation and sensor-fusion schemes [17, 18]—demands access to the full system state. This comprises not only the UAV’s translational motion (its position and the time evolution thereof) but also its rotational attitude, conventionally expressed by the pitch, roll, and yaw angles. To the best of the authors’ knowledge, no publicly available simulator can yet generate feasible representative full six–degree–of–freedom (6-DoF) trajectories that capture both translational and rotational dynamics with realistic characteristics, thereby limiting systematic controller design and benchmarking.\n\nRecently, there has also been a surge of interest in adaptive RL-based UAV planners for search and rescue [19, 20] and in hybrid optimization learning schemes for accurate multi-joint manipulator motion [21]. While these methods show improved responsiveness and adaptability, they may encounter convergence hurdles or suffer from excessive computational costs when the planning horizon and action dimensionality grow.\n\nIn recent advancements in reinforcement learning (RL) for trajectory optimization, the problem of autonomous path planning has been approached using Markov Decision Processes (MDPs) to minimize cumulative loss functions that account for trajectory smoothness, obstacle avoidance, and motion constraints. Traditional methods often rely on optimization-based approaches such as polynomial trajectory planning, which may not generalize well across dynamic environments. Instead, RL-based methods allow for adaptive learning of trajectory policies, where agents collaborate to modify and refine trajectories in real-time. Building on our previous work [22] that introduced the Global Multi-Phase Path Planning (GMP3\\text{GMP}^{3}) concept in 2D environments, this paper significantly extends the framework and presents several new contributions:\n\nExtension of the GMP3\\text{GMP}^{3} framework to the Lie group SE​(3)\\mathrm{SE}(3), enabling full six-degree-of-freedom (6-DoF) trajectory planning that incorporates both translation and rotation.\n\nWe introduce an influence-aware policy update rule that balances local adaptation, historical best performance, and global alignment.\n\nA consensus-based interaction protocol is employed to align neighboring agents through weighted policy sharing over a dynamic graph.\n\nGMP3\\text{GMP}^{3} offers a direct formulation and solution for policy optimization updates.\n\nImplementation of a modular, real-time drone control interface, DroneManager, to deploy GMP3\\text{GMP}^{3} on physical UAV platforms with minimal reconfiguration.\n\nExtensive validation in both simulation and real-world indoor experiments, demonstrating the robustness, adaptability, and practical viability of the proposed method in complex 3D environments.\n\nThe structure of the paper is as follows. Section II defines the formal problem and introduces the multi-phase trajectory learning setup in a three-dimensional environment. Section III presents the reinforcement learning-based update mechanism, including the formulation of a modified Bellman operator tailored to the trajectory optimization framework. Section IV describes five optimization strategies employed to enhance convergence speed and learning stability. Section V reports simulation results that evaluate the effectiveness of the proposed method under various scenarios. Section VI introduces the custom-developed DroneManager software, which facilitates real-time control and coordination of physical UAVs. Section VII presents the results of real-world experiments that validate the practicality and robustness of the proposed approach. The paper concludes with a summary of findings and outlines potential directions for future research.\n\nNotations: The notation ⟨X⟩\\langle X\\rangle denotes the mean of the quantity XX.\n\n1. Extension of the GMP3\\text{GMP}^{3} framework to the Lie group SE​(3)\\mathrm{SE}(3), enabling full six-degree-of-freedom (6-DoF) trajectory planning that incorporates both translation and rotation.\n\n2. We introduce an influence-aware policy update rule that balances local adaptation, historical best performance, and global alignment.\n\n3. A consensus-based interaction protocol is employed to align neighboring agents through weighted policy sharing over a dynamic graph.\n\n4. GMP3\\text{GMP}^{3} offers a direct formulation and solution for policy optimization updates.\n\n5. Implementation of a modular, real-time drone control interface, DroneManager, to deploy GMP3\\text{GMP}^{3} on physical UAV platforms with minimal reconfiguration.\n\n6. Extensive validation in both simulation and real-world indoor experiments, demonstrating the robustness, adaptability, and practical viability of the proposed method in complex 3D environments.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在动态和复杂的3D环境中为无人机生成可行的轨迹？  \n2. 如何结合强化学习和传统路径规划方法以提高路径规划的适应性和性能？  \n3. 如何实现多智能体之间的协作以优化路径规划？  \n\n【用了什么创新方法】  \n提出了GMP3框架，扩展到Lie群SE(3)，实现六自由度的轨迹规划，结合了局部适应性和全局一致性的影响感知策略更新规则。通过共识基础的交互协议，智能体之间共享策略信息，促进协作更新。实现了一个模块化的实时控制接口DroneManager，支持在真实无人机平台上的部署。经过广泛的仿真和实际飞行实验验证，GMP3展示了在复杂3D环境中的鲁棒性和适应性，能够可靠地避开障碍并生成平滑的轨迹。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "BiNoMaP: Learning Category-Level Bimanual Non-Prehensile Manipulation Primitives",
            "authors": "Huayi Zhou,Kui Jia",
            "subjects": "Robotics (cs.RO)",
            "comment": "under review",
            "pdf_link": "https://arxiv.org/pdf/2509.21256",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21256",
            "arxiv_html_link": "https://arxiv.org/html/2509.21256v1",
            "abstract": "Non-prehensile manipulation, encompassing ungraspable actions such as pushing, poking, and pivoting, represents a critical yet underexplored domain in robotics due to its contact-rich and analytically intractable nature. In this work, we revisit this problem from two novel perspectives. First, we move beyond the usual single-arm setup and the strong assumption of favorable external dexterity such as walls, ramps, or edges. Instead, we advocate a generalizable dual-arm configuration and establish a suite of Bimanual Non-prehensile Manipulation Primitives (BiNoMaP). Second, we depart from the prevailing RL-based paradigm and propose a three-stage, RL-free framework to learn non-prehensile skills. Specifically, we begin by extracting bimanual hand motion trajectories from video demonstrations. Due to visual inaccuracies and morphological gaps, these coarse trajectories are difficult to transfer directly to robotic end-effectors. To address this, we propose a geometry-aware post-optimization algorithm that refines raw motions into executable manipulation primitives that conform to specific motion patterns. Beyond instance-level reproduction, we further enable category-level generalization by parameterizing the learned primitives with object-relevant geometric attributes, particularly size, resulting in adaptable and general parameterized manipulation primitives. We validate BiNoMaP across a range of representative bimanual tasks and diverse object categories, demonstrating its effectiveness, efficiency, versatility, and superior generalization capability.",
            "introduction": "Non-prehensile manipulation refers to a class of robotic actions that do not rely on firm grasping but instead leverage physical interactions such as poking, or pivoting, or pushing to achieve manipulation goals Zhou et al. (2019); Hogan & Rodriguez (2020); Sun et al. (2020); Zhou & Held (2023); Zhang et al. (2023). These skills are not merely complementary to traditional grasp-based tasks; they are often essential in scenarios where grasping is physically infeasible or inefficient. In dual-arm robotic systems Liu et al. (2022); Wu & Kruse (2024); Yamada et al. (2025); Lu et al. (2025), non-prehensile manipulation becomes especially relevant when dealing with objects that are too fragile, too flat, or lack sufficient geometry for reliable grasping.\n\nDespite its importance, current non-prehensile manipulation faces two core bottlenecks. First, most existing works operate under the simplifying assumption of a unimanual setting, often coupled with highly structured environments Zhou et al. (2023); Cho et al. (2024); Wu et al. (2024); Lyu et al. (2025); Li et al. (2025a). To compensate for the lack of control authority, these methods rely on artificial aids such as vertical walls, inclined planes, or boundaries to stabilize and direct object motion. However, in real-world deployments, such assumptions are rarely satisfied due to environmental unpredictability or object fragility. Consider a scenario where a thin rectangular cardboard box lies flat on a tabletop—walls and ramps are unavailable, and poking the box may damage its contents. In such cases, a more general solution is to exploit bimanual coordination Krebs & Asfour (2022); Grannen et al. (2023), where one arm can serve as a stabilizing reference while the other executes the non-prehensile action. This configuration not only replaces inflexible external constraints with adaptive internal ones but also enables complex skills such as dual-arm wrapping Grotz et al. (2024); Lu et al. (2025); Zhou et al. (2025); Liu et al. (2025a) or cluttered object singulation Jiang et al. (2024b); Xu et al. (2025a), which are inaccessible to single-arm systems.\n\nThe second bottleneck lies in the heavy reliance on reinforcement learning (RL) frameworks Schulman et al. (2017); Haarnoja et al. (2018); Fujimoto et al. (2018). Most advanced approaches require constructing task-specific simulators that model manipulators, top-tables, and object dynamics, followed by lengthy policy training with dense environment interactions and carefully engineered reward functions. These RL pipelines are often sensitive to hyperparameter tuning and face substantial sim-to-real gaps due to inaccuracies in simulated physics, including mass distributions, contact dynamics, or friction coefficients. While recent works attempt to mitigate this gap via world models or differentiable simulators Lyu et al. (2025); Li et al. (2025a); Huang et al. (2025), they still inherit the inherent limitations of RL, including slow convergence and poor generalization. To the best of our knowledge, our work is the first to propose a fully RL-free paradigm for learning bimanual non-prehensile manipulation skills through imitation and geometric reasoning.\n\nTo this end, we present a three-stage paradigm that combines hardware generality with algorithmic efficiency. We employ a dual-arm setup with parallel-jaw grippers, which not only supports unimanual non-prehensile skills but also enables more complex bimanual ones. In the first stage, inspired by prior work on learning from human demonstrations Grauman et al. (2022; 2024); Chen et al. (2025); Papagiannis et al. (2025), we extract primitive bimanual motion trajectories from human hand videos for task-specific non-prehensile behaviors. Unlike grasp-based tasks, where 1–3 cm errors in hand-object alignment may be tolerable, non-prehensile tasks are extremely sensitive to deviations: even 3–5 mm misalignment can lead to premature contact loss or over-compression, causing instability or failure. To address this, our second stage introduces a geometry-aware post-optimization algorithm that leverages object shape priors to refine these noisy trajectories into smooth, task-specific motion primitives. These refined trajectories, which we term Bimanual Non-Prehensile Manipulation Primitives (BiNoMaP), exhibit high success rates and are robust to object pose variations. In the third stage, we further extend these primitives to unseen objects within the same category by parameterizing them with object-specific geometric attributes, such as the length and width of a box or the diameter of a sphere. This results in a family of Parameterized Manipulation Primitives that are adaptive and transferable across diverse category-level instances.\n\nWe extensively evaluate BiNoMaP on a diverse set of non-prehensile dual-arm skills, including poking, pivoting, pushing, and wrapping (Fig. 1). We test them on various objects with varying shapes, materials, and physical properties. To prove the effectiveness and efficiency, we compare against strong baselines Zhao et al. (2023a); Chi et al. (2023); Ze et al. (2024); Zhou et al. (2023); Cho et al. (2024); Lyu et al. (2025). Our results show that BiNoMaP consistently achieves higher success rates across tasks. Furthermore, we demonstrate how BiNoMaP can be integrated with high-level vision-language models (VLMs) Xiao et al. (2024); Ravi et al. (2025) to support advanced robotic behaviors, such as pre-grasping under ungraspable conditions, tabletop rearrangement, and error recovery—bridging the gap between low-level skills and high-level autonomy.\n\nOur main contributions are three-fold: (i) We propose the first RL-free framework for learning Bimanual Non-Prehensile Manipulation Primitives directly from human video demonstrations. (ii) We introduce a parameterization scheme that enables category-level generalization of non-prehensile skills across diverse object instances. (iii) We demonstrate the effectiveness, efficiency, versatility, and generality of BiNoMaP across a variety of tasks, objects, and strong baselines.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现双臂非抓取操作的有效学习与应用。  \n2. 现有方法依赖于强化学习，导致训练过程复杂且难以推广。  \n3. 如何从人类演示中提取并优化非抓取动作轨迹。  \n\n【用了什么创新方法】  \n本研究提出了一种三阶段的RL-free框架，首先从人类视频中提取双臂运动轨迹，然后通过几何感知后优化算法将这些轨迹精炼为可执行的非抓取操作原语（BiNoMaP），最后通过参数化对象几何属性实现类别级别的推广。该方法在多种非抓取任务中展现了高成功率和强泛化能力，超越了传统的单臂设置和强化学习方法，展示了其在复杂场景中的有效性和适应性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models",
            "authors": "Jiyeon Koo,Taewan Cho,Hyunjoon Kang,Eunseom Pyo,Tae Gyun Oh,Taeryang Kim,Andrew Jaeyong Choi",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21243",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21243",
            "arxiv_html_link": "https://arxiv.org/html/2509.21243v1",
            "abstract": "Recent Vision-Language-Action (VLA) models demonstrate remarkable generalization in robotics but are restricted by their substantial size and computational cost, limiting real-world deployment. However, conventional lightweighting methods often sacrifice critical capabilities, particularly spatial reasoning. This creates a trade-off between efficiency and performance. To address this challenge, our work reuses Register Tokens, which were introduced for artifact removal in Vision Transformers but subsequently discarded. We suppose that these tokens contain essential spatial information and propose RetoVLA, a novel architecture that reuses them directly by injecting them into the Action Expert.",
            "introduction": "By integrating vast web-scale knowledge into robotic control, Vision-Language-Action (VLA) models such as RT-2 [1] and OpenVLA [2] have demonstrated remarkable generalization in understanding complex language instructions. However, this success relies on massive, billion-parameter models, which require substantial computational costs. This fundamental issue presents a significant challenge to their practical deployment on real-world robotic platforms with limited on-board computing resources.\n\nPrevious efforts to address this efficiency problem have primarily concentrated on physically reducing the model size, as seen in approaches like SmolVLA [4]. However, this inevitably leads to a trade-off between performance and efficiency. This requires a compromise on the extensive representational power of VLMs, particularly their ability to comprehend complex spatial relationships and long-term contexts. Simply discarding information risks compromising fundamental aspects of robotic intelligence in exchange for computational gains.\n\nThis paper examines the underlying issue, seeking a solution not in the ‘deletion’ of information, but in its ‘active reuse’. Our starting point is the work of [3], which revealed that modern large-scale Vision Transformers (ViT) [5] like DINOv2 [6] inherently produce artifacts known as high-norm outlier tokens during training. These tokens typically emerge from image patches relatively low informational content, such as blank walls or the sky. The model repurposes the representational space of these patches to serve as a temporary scratchpad for storing and processing internal global information. While this is a natural learning mechanism, it has been shown to deteriorate the local information of the corresponding patch tokens, significantly impairing performance on dense prediction tasks.\n\nThe proposed solution to this issue, the Register Token [3], provides an explicit ‘scratchpad’ for the model, thereby preventing the misuse of image patch tokens and refining the attention maps. However, after absorbing these artifacts, the Register Tokens [3] have been systematically discarded in downstream tasks. This leads us to a critical question that diverges from previous work: What is the true nature of this ‘global information,’ and is it merely noise to be discarded?\n\nIn the context of robotic manipulation, we propose that this information constitutes the essential Spatial Context of a scene—encompassing the overall layout, the 3D relationships between objects, and the structure of the workspace. For a robot, this information is not a disposable asset; it is critical.\n\nBuilding on this hypothesis, we propose RetoVLA (Reusing Register Tokens [3] VLA), which achieves both efficiency and high performance through the active reuse of information. As summarized in Figure 1, our core contributions are as follows: (1) Redefining the Role of Register Tokens [3] for Spatial Context Injection: A novel VLA architecture, detailed in Figure 2, that redefines the register token [3] from a passive ‘purifier’ used for artifact removal to an active ‘spatial context provider’. We have designed a novel module to directly inject these tokens as Key-Value pairs into the Action Expert’s attention mechanism, allowing it to leverage the global spatial context of the entire scene until the final phase of action generation. (2) Analyzing the Synergy with Efficient VLAs: We analyze how register token [3] injection compensates for the information loss that occurs in efficient models like SmolVLA [4], which reduce the depth of the VLM. Our approach proves to be an effective pathway for maintaining a high level of spatial reasoning with significantly lower computational overhead. (3) Experimental Validation: Through rigorous experiments on the LIBERO simulation benchmark and with the custom-built 7-DOF robot arm we developed for this research, we demonstrate that RetoVLA significantly outperforms the baseline model, particularly on long-horizon tasks that require a complex understanding of 3D spatial structures or multi-step sequential manipulation. In our real-robot experiments, we achieved an increase in the average success rate from 50.3% to 67.4%, a 17.1%p absolute improvement.\n\nThis research reconstructs conventional design principle by re-evaluating internal information flow, enabling a new class of low-cost, high-performance robotic intelligence.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在保持高性能的同时减少Vision-Language-Action模型的计算成本？  \n2. 如何有效利用Register Tokens以增强空间推理能力而不牺牲信息？  \n3. 如何在机器人操控任务中实现复杂的3D空间理解？  \n\n【用了什么创新方法】  \n本研究提出了RetoVLA架构，通过重新利用Register Tokens作为空间上下文提供者，直接将其注入到Action Expert的注意力机制中，从而增强模型的空间推理能力。通过这一方法，我们实现了在LIBERO仿真基准和自建7-DOF机器人臂上的实验验证，结果显示RetoVLA在长时间任务中的表现显著优于基线模型，成功率从50.3%提升至67.4%。这一创新方法有效解决了在高效模型中信息损失的问题，为低成本高性能的机器人智能提供了新的设计思路。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "FSGlove: An Inertial-Based Hand Tracking System with Shape-Aware Calibration",
            "authors": "Yutong Li,Jieyi Zhang,Wenqiang Xu,Tutian Tang,Cewu Lu",
            "subjects": "Robotics (cs.RO)",
            "comment": "Presented at IROS 2025, details are available atthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.21242",
            "code": "https://fsglove.robotflow.ai",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21242",
            "arxiv_html_link": "https://arxiv.org/html/2509.21242v1",
            "abstract": "Accurate hand motion capture (MoCap) is vital for applications in robotics, virtual reality, and biomechanics, yet existing systems face limitations in capturing high-degree-of-freedom (DoF) joint kinematics and personalized hand shape. Commercial gloves offer up to 21 DoFs, which are insufficient for complex manipulations while neglecting shape variations that are critical for contact-rich tasks. We present FSGlove, an inertial-based system that simultaneously tracks up to 48 DoFs and reconstructs personalized hand shapes via DiffHCal, a novel calibration method. Each finger joint and the dorsum are equipped with IMUs, enabling high-resolution motion sensing. DiffHCal integrates with the parametric MANO model through differentiable optimization, resolving joint kinematics, shape parameters, and sensor misalignment during a single streamlined calibration. The system achieves state-of-the-art accuracy, with joint angle errors of less than 2.7∘, and outperforms commercial alternatives in shape reconstruction and contact fidelity. FSGlove’s open-source hardware and software design ensures compatibility with current VR and robotics ecosystems, while its ability to capture subtle motions (e.g., fingertip rubbing) bridges the gap between human dexterity and robotic imitation. Evaluated against Nokov optical MoCap, FSGlove advances hand tracking by unifying the kinematic and contact fidelity. Hardware design, software, and more results are available at: https://sites.google.com/view/fsglove.",
            "introduction": "The human hand, a remarkably dexterous end-effector capable of executing intricate tasks, serves as both the inspiration and benchmark for robotic hand design and manipulation research. Accurate motion capture of the hand during manipulation is critical for collecting data essential to diverse downstream applications, including hand pose estimation, teleoperation, and imitation learning. Achieving high-fidelity hand motion capture hinges on two factors: joint kinematics and hand shape modeling.\n\nCurrently, the most comprehensive commercially available hand MoCap glove [1] can capture at most 21 degrees of freedom (DoFs). However, complex manipulations, such as thumb-index fingertip rubbing, are still beyond the ability to capture, as they require additional torsional DoFs in the proximal or middle phalanges that are often overlooked (Fig. 1), but recently, some dexterous robotic hands [2] tend to support control such DoFs. Similarly, hand shape variations, driven by differences in bone length and soft-tissue composition, demand personalized models for accurate motion reconstruction, particularly during contact-rich tasks involving objects or self-interaction. Existing hand motion capture systems, whether commercial [3, 4] or research-oriented [5, 6, 7], focus primarily on joint angle estimation with limited DoFs and neglect the shape variations. This oversight hinders the transfer of in-manipulation motions to virtual avatars or anthropomorphic robotic hands, where both kinematic and contact fidelity are paramount (Fig. 1).\n\nTo bridge this gap, we present FSGlove, a novel inertial-based hand tracking system simultaneously capturing up to 48-DoF joint kinematics and personalized hand shape. Each finger joint and the dorsal region are instrumented with inertial measurement units (IMUs), enabling high-resolution motion sensing. A key innovation is DiffHCal, a shape-aware calibration method that embeds personalized hand-shape estimation into a streamlined calibration protocol. By leveraging the parametric MANO hand model [8], DiffHCal aligns captured sensor data to MANO’s joint and shape parameters through a differentiable optimization process. This framework uses a series of predefined reference poses to simultaneously resolve skeletal joint parameters, shape parameters, and manual installation errors in a single optimization process. Moreover, this process integrates seamlessly into standard glove calibration workflows, requiring no additional steps compared with commercial systems while delivering contact-consistent hand models. To democratize access, we open-source the hardware design, low-level drivers for sensor calibration, and high-level interfaces for integration with motion capture ecosystems (e.g., OptiTrack, HTC Vive), enabling plug-and-play compatibility with robotics and VR frameworks.\n\nWe evaluate FSGlove across four metrics: (1) joint angle accuracy (≤2.7∘\\leq 2.7^{\\circ}), (2) shape reconstruction precision (mean mesh error ≤3.6\\leq 3.6mm), (3) fingertip tracking error (mean positional error ≤16\\leq 16mm), and (4) hand-object interaction fidelity (mean positional error ≤20\\leq 20mm), benchmarking against commercial glove systems. Despite its low-cost design, FSGlove achieves state-of-the-art performance in shape-aware tracking, surpassing commercial alternatives (e.g., Manus Metaglove Pro, VRTRIX) in contact-rich manipulation tasks.\n\nWe summarize our contributions as follows:\n\nFSGlove, the first open-source, high-DoF (up to 48 DoFs) data glove integrating inertial sensing with shape-aware calibration.\n\nDiffHCal, a differentiable calibration framework that infers personalized hand shape during standard wear-time procedures.\n\nComprehensive validation demonstrating high-precision accuracy in joint measurement and shape reconstruction.\n\n1. FSGlove, the first open-source, high-DoF (up to 48 DoFs) data glove integrating inertial sensing with shape-aware calibration.\n\n2. DiffHCal, a differentiable calibration framework that infers personalized hand shape during standard wear-time procedures.\n\n3. Comprehensive validation demonstrating high-precision accuracy in joint measurement and shape reconstruction.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现高精度的手部运动捕捉，特别是复杂操作中的关节运动和个性化手形？  \n2. 现有手部运动捕捉系统在捕捉高自由度关节运动和形状变化方面的局限性。  \n3. 如何在不增加额外步骤的情况下，优化手部传感器的校准流程？  \n\n【用了什么创新方法】  \nFSGlove是一个基于惯性传感器的手部追踪系统，能够同时捕捉高达48个自由度的关节运动和个性化手形。其核心创新在于DiffHCal，一个形状感知的校准方法，通过可微优化将捕获的传感器数据与MANO模型的关节和形状参数对齐。该方法在标准佩戴程序中推断个性化手形，简化了校准流程。FSGlove在关节角度精度、形状重建精度、指尖跟踪误差和手-物体交互保真度等多个指标上表现出色，超越了商业手套系统，展示了其在接触丰富的操作任务中的优势。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "SEEC: Stable End-Effector Control with Model-Enhanced Residual Learning for Humanoid Loco-Manipulation",
            "authors": "Jaehwi Jang,Zhuoheng Wang,Ziyi Zhou,Feiyang Wu,Ye Zhao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21231",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21231",
            "arxiv_html_link": "https://arxiv.org/html/2509.21231v1",
            "abstract": "Arm end-effector stabilization is essential for humanoid loco-manipulation tasks, yet it remains challenging due to the high degrees of freedom and inherent dynamic instability of bipedal robot structures. Previous model-based controllers achieve precise end-effector control but rely on precise dynamics modeling and estimation, which often struggle to capture real-world factors (e.g., friction and backlash) and thus degrade in practice. On the other hand, learning-based methods can better mitigate these factors via exploration and domain randomization, and have shown potential in real-world use. However, they often overfit to training conditions, requiring retraining with the entire body, and still struggle to adapt to unseen scenarios.\nTo address these challenges, we propose a novel stable end-effector control (SEEC) framework with model-enhanced residual learning that learns to achieve precise and robust end-effector compensation for lower-body induced disturbances through model-guided reinforcement learning (RL) with a perturbation generator. This design allows the upper-body policy to achieve accurate end-effector stabilization as well as adapt to unseen locomotion controllers with no additional training.\nWe validate our framework in different simulators and transfer trained policies to the Booster T1 humanoid robot. Experiments demonstrate that our method consistently outperforms baselines and robustly handles diverse and demanding loco-manipulation tasks.",
            "introduction": "Humanoid robots promise seamless integration into human environments, where they must walk and manipulate simultaneously. From carrying objects while moving to performing collaborative tasks [1, 2, 3], this capability is fundamental for practical humanoid deployment (see the tasks shown in Fig. SEEC: Stable End-Effector Control with Model-Enhanced\nResidual Learning for Humanoid Loco-Manipulation).\nYet, achieving stable and precise control of the arm end-effector during dynamic locomotion remains an open challenge.\nEven modest base movements could induce large accelerations at the arm end-effector, causing tracking errors, destabilizing contact forces, and ultimately limiting the utility of humanoids in real-world settings.\n\nRecently, learning-based approaches [4, 5, 6, 7] have sought to achieve humanoid loco-manipulation by training end-to-end reinforcement learning (RL) policies.\nWhile effective at capturing nonlinearities and handling uncertainty,\nthese policies often rely on imitating joint or task reference trajectories [8, 9, 10], and struggle to ensure accurate end-effector stabilization.\nFor example, in HOVER [10], uncontrolled hand motions emerge as a byproduct of locomotion.\nPrior work [7] has attempted to stabilize end-effector control by directly penalizing its acceleration, but this approach heavily relies on policy optimization to “discover” the right compensation strategy.\nAdditionally, the learned behavior degenerates into static hand-holding motions, limiting general applicability.\nMoreover, when tasks require reactive whole-body coordination, instability in end-effector control is exacerbated by sudden locomotion disturbances. Conventional RL training, as in [7], fails to provide robustness under such out-of-distribution (OOD) scenarios.\n\nInspired by model-based approaches [11, 12, 13, 14, 15], which achieve precise stabilization through dynamics modeling and online estimation, we introduce SEEC: Stable End-Effector Control, a model-enhanced RL framework for humanoid loco-manipulation.\nSEEC leverages model-based expertise to provide analytic acceleration compensation signals during training. Instead of relying on naive penalization, the compensation torque from the model-based formulation is distilled into the RL policy, addressing instability in a more principled manner.\n\nFurthermore, unlike prior works that jointly train manipulation and locomotion policies, we introduce a perturbation generation strategy that exposes the upper-body policy to a wide spectrum of locomotion-induced disturbances. By modeling these disturbances as base movement patterns, the upper-body controller learns to maintain stable arm end-effector control independent of any specific locomotion policy.\nThis modular design not only improves robustness across diverse walking patterns, allowing seamless transfer across different walking patterns, including previously unseen locomotion controllers, but also facilitates integration into complex loco-manipulation tasks that demand coherent whole-body coordination.\n\nOur core contributions can be summarized as follows.\n\nWe propose a model-enhanced residual learning framework that integrates model-based expertise with learning-based adaptability, achieving precise acceleration compensation while effectively addressing model inaccuracies and parameter uncertainties.\n\nWe introduce a base-movement data generation and perturbation generation strategy that exposes the policy to a broad spectrum of locomotion-relevant disturbances during training. This enables the controller to acquire robust compensation behaviors that can transfer to unseen locomotion controllers and gaits without requiring joint re-training.\n\nWe demonstrate the first deployment of such a hybrid framework on a full humanoid Booster T1, validating it both in simulation and on the real hardware via zero-shot transfer. The system achieves more stable and precise end-effector control across a variety of loco-manipulation tasks, compared to the baselines.\n\n1. We propose a model-enhanced residual learning framework that integrates model-based expertise with learning-based adaptability, achieving precise acceleration compensation while effectively addressing model inaccuracies and parameter uncertainties.\n\n2. We introduce a base-movement data generation and perturbation generation strategy that exposes the policy to a broad spectrum of locomotion-relevant disturbances during training. This enables the controller to acquire robust compensation behaviors that can transfer to unseen locomotion controllers and gaits without requiring joint re-training.\n\n3. We demonstrate the first deployment of such a hybrid framework on a full humanoid Booster T1, validating it both in simulation and on the real hardware via zero-shot transfer. The system achieves more stable and precise end-effector control across a variety of loco-manipulation tasks, compared to the baselines.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现人形机器人在动态行走中稳定和精确的末端执行器控制？  \n2. 现有模型基控制器在真实世界中因动态建模不精确而导致的性能下降问题。  \n3. 学习基方法在面对未见场景时的适应性不足和过拟合问题。  \n\n【用了什么创新方法】  \n提出了一种模型增强的残差学习框架（SEEC），结合模型驱动的强化学习和扰动生成策略，能够在动态行走中实现精确的末端执行器补偿。通过在训练中暴露于多种运动扰动，控制器能够在未见的运动控制器上无缝转移，且无需重新训练。实验结果表明，该方法在多种复杂的行走操控任务中表现出更高的稳定性和精确性，超越了基线方法。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Next-Generation Aerial Robots -- Omniorientational Strategies: Dynamic Modeling, Control, and Comparative Analysis",
            "authors": "Ali Kafili Gavgani,Amin Talaeizadeh,Aria Alasty,Hossein Nejat Pishkenari,Esmaeil Najafi",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21210",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21210",
            "arxiv_html_link": "https://arxiv.org/html/2509.21210v1",
            "abstract": "Conventional multi-rotors are under-actuated systems, hindering them from independently controlling attitude from position. In this study, we present several distinct configurations that incorporate additional control inputs for manipulating the angles of the propeller axes. This addresses the mentioned limitations, making the systems \"omniorientational\". We comprehensively derived detailed dynamic models for all introduced configurations and validated by a methodology using Simscape Multibody simulations.\nTwo controllers are designed: a sliding mode controller for robust handling of disturbances and a novel PID-based controller with gravity compensation integrating linear and non-linear allocators, designed for computational efficiency. A custom control allocation strategy is implemented to manage the input-non-affine nature of these systems, seeking to maximize battery life by minimizing the \"Power Consumption Factor\" defined in this study. Moreover, the controllers effectively managed harsh disturbances and uncertainties.\nSimulations compare and analyze the proposed configurations and controllers, majorly considering their power consumption. Furthermore, we conduct a qualitative comparison to evaluate the impact of different types of uncertainties on the control system, highlighting areas for potential model or hardware improvements. The analysis in this study provides a roadmap for future researchers to design omniorientational drones based on their design objectives, offering practical insights into configuration selection and controller design.\nThis research aligns with the project SAC-1, one of the objectives of Sharif AgRoLab.",
            "introduction": "Multi-rotors have become highly popular due to their versatility and agility, finding use in a wide range of applications. These include package transportation [1, 2, 3], search and rescue operations [4, 5, 6, 7], agriculture [8, 9, 10, 11, 12, 13, 14], non-destructive testing [15, 16], aerial surveillance and remote sensing [17, 18, 19, 20].\n\nConventional multi-rotors (e.g., a quadrotor) are under-actuated systems, hindering them from performing complex maneuvers and independently controlling of attitude from position.\nTo overcome these limitations, Ryll et al. [21] and [22] proposed incorporating additional independent actuators to make their drone fully-actuated. Both studies utilized feedback linearization control techniques based on third-order differential equations of motion.\n\nZheng et al. [23] and Badr et al. [24] achieved full actuation in their systems using thrust-vectoring mechanisms, enabling the drone to hover with an inclined body. Allenspach et al. [25] and Kamel et al. [26] constructed hexacopters with tilting rotors, capable of tracking all six  Degrees of Freedom (DoF) independently. Kamel et al. [26] incorporated a PID controller to determine the desired force vector and Two cascade controllers to generate the desired moment vector.\nSantos et al. [27] introduced a fast control allocation method for an over-actuated quadrotor to reduce real-time computation costs. A  Proportional Derivative (PD)  Sliding Mode Controller (SMC) is proposed and simulated by Alkamachi and Erçelebi [28] to control an over-actuated quadrotor, integrating a genetic algorithm to optimally tune the controller parameters. A different over-actuated configuration is proposed by Gavgani et al. [29], introducing a SMC-based controller incorporating a none-linear mapping in control inputs to deal with control allocation problems.\n\nTalaeizadeh et al. [30] introduced a yaw-rate control to mitigate the effects of  Vortex Ring State (VRS).\nHajiAbedini et al. [31] developed a quadrotor with rotor axes tilted inwards, taking a dihedral angle to explore a different approach to overcome VRS. \nSome studies have assessed the different applications of sliding mode control [32, 33, 34].\n\nAlthough previous works have made strides in thrust-vectoring and full actuation, there remains a gap in the literature on comprehensive comparisons of omniorientational configurations and controllers. Such comparisons would allow for a nuanced understanding of how different configurations and control strategies affect energy consumption and performance across the full six-DoF.\n\nWe represent four unique drone configurations featuring rotatable propeller axes. Our target systems are \"omniorientational\", indicating their ability to independently control position and attitude across all six DoF. The terms \"hedral rotation\" and \"tilt rotation\" are used in this article to refer to two distinct methods of axis rotation. Refer to Figure 1 for a graphical representation of hedral and tilt rotations. Detailed comparative analysis examines energy efficiency, robustness, and control efficacy across configurations and controllers.\n\nThe simplified visualization of proposed configurations are represented in Figure 2 emphasizing on the physical parameters, notations and coordinate systems. They involve four  Brushless Direct Current (BLDC) motors with attached propellers along with integrated servo motors for manipulating hedral and/or tilt angles. A conceptual design for the Hedral configuration is illustrated in Figure 3.\n\nThe dynamic equations of motion for each configuration are derived comprehensively using the Newton-Euler method, validated by applying our model-based controller to CAD models in MATLAB Simscape Multibody environment.\n\nWe have developed two effective controllers: a sliding mode controller for robust handling of disturbances and a novel PID-based controller with gravity compensation integrating linear and non-linear allocators, designed for computational efficiency. A custom control allocation strategy is implemented to manage the input-non-affine nature of these systems, seeking to maximize battery life by minimizing the  Power Consumption Factor (PCF) and  Nondimensionalized Power Consumption Factor (NPCF) defined in this study.\n\nA comparison framework assessing power consumption and robustness under uncertainties and disturbances is developed in this study. This framework introduces a power factor and an Uncertainty Impact Factor (UIF) for evaluating configurations and controllers, highlighting areas for potential model or hardware improvements, and offering insights into each configuration’s practical capabilities and operational strengths.\n\nThe main contribution of this paper is exploring a comprehensive assessment of various possible configurations for omniorientational drones —systems designed to overcome the limitations of conventional multi-rotors by enabling independent control across all DoF. This enables a comparative analysis of the different configurations and controllers.\nThe unique analysis in this paper provides a roadmap for future researchers to design omniorientational drones based on their design objectives, offering practical insights into configuration selection and controller design.\n\nThis research is in alignment with the project SAC-1, one of the primary objectives of the Laboratory of Advanced Control Systems and Agricultural Robotics of Sharif University of Technology (Sharif AgRoLab), which is the development of the Sharif Agrocopter. This specialized drone is designed to navigate with ease and precision through dense garden environments abundant with trees and foliage. Its primary functions encompass conducting essential investigations, including the application of artificial intelligence for pest detection and management.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何克服传统多旋翼无人机在姿态与位置控制上的不足？  \n2. 不同的无人机配置和控制策略如何影响能量消耗和性能？  \n3. 如何设计有效的控制器以应对系统中的不确定性和干扰？  \n\n【用了什么创新方法】  \n本研究提出了几种独特的无人机配置，结合额外的控制输入以实现“全向控制”，使其能够独立控制所有六个自由度。通过使用Newton-Euler方法全面推导了动态模型，并在MATLAB Simscape Multibody环境中进行了验证。设计了两种控制器：一种滑模控制器用于强扰动处理，另一种新型PID控制器则集成了重力补偿和线性、非线性分配器，旨在提高计算效率。通过比较不同配置和控制器的能量效率、鲁棒性和控制效果，研究为未来的无人机设计提供了实用的见解和路线图。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Human-like Navigation in a World Built for Humans",
            "authors": "Bhargav Chandaka,Gloria X. Wang,Haozhe Chen,Henry Che,Albert J. Zhai,Shenlong Wang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "CoRL 2025. Project website:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.21189",
            "code": "https://reasonnav.github.io/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21189",
            "arxiv_html_link": "https://arxiv.org/html/2509.21189v1",
            "abstract": "When navigating in a man-made environment they haven’t visited before—like an office building—humans employ behaviors such as reading signs and asking others for directions.\nThese behaviors help humans reach their destinations efficiently by reducing the need to search through large areas. Existing robot navigation systems lack the ability to execute such behaviors and are thus highly inefficient at navigating within large environments. We present ReasonNav, a modular navigation system which integrates these human-like navigation skills by leveraging the reasoning capabilities of a vision-language model (VLM). We design compact input and output abstractions based on navigation landmarks, allowing the VLM to focus on language understanding and reasoning. We evaluate ReasonNav on real and simulated navigation tasks and show that the agent successfully employs higher-order reasoning to navigate efficiently in large, complex buildings. Project website: https://reasonnav.github.io/.",
            "introduction": "Imagine that you are an office worker and are asked to deliver a report to Jane Doe’s office. What steps would you take to complete this task? First, you might search in a directory to find out the building and room number for Jane Doe’s office. Then, you might look for signs that indicate the direction of that room. You can integrate the information you receive from each sign with the layout of the scene you see around you to decide where to look next. Along the way, you might ask people nearby for further clarifications.\n\nOur civilization is built to be easy for humans to navigate within. There is an abundance of knowledge-offering resources around us that we leverage to navigate the world efficiently. Directional signs are placed deliberately at junctions to eliminate the risk of going the wrong way. Room labels follow orderly patterns so that reading a few can allow one to infer the locations of other rooms. Such guidance is necessary in order to deal with the inherent uncertainty of navigation in unseen environments.\n\nExisting robot navigation systems lack the skills needed to leverage these resources and thus lose out in navigation efficiency by spending unnecessary time exploring. We call these skills, which include sign reading and asking for directions, higher-order navigation skills because they require higher-order reasoning abilities and language processing. These skills become increasingly important in larger environments, where exploring in the wrong direction can cost a massive amount of time.\n\nOur key insight is that such higher-order navigation skills can be integrated in a unified manner by taking advantage of recent advances in large vision-language models (VLMs). In this paper, we present ReasonNav, a modular system for human-like navigation that leverages the zero-shot reasoning capabilities of a VLM in an agentic manner. The system is comprised of two streams: a low-level stream that handles localization, mapping, and path planning, and a high-level stream where the VLM performs high-level planning on abstracted observation and action spaces. Specifically, we represent the environment using a memory bank of landmarks (e.g. map frontiers, doors, people, signs) with attached textual information. This simplifies both the input and output spaces for the VLM agent, allowing it to focus on higher-order reasoning.\n\nWe evaluate ReasonNav in real and simulated environments. In both cases, the robot is tasked with finding a given room in a large (unseen) building. This mimics a practical indoor delivery scenario. We show that our abstraction design allows the VLM to interpret information from signs and people and use it to guide its decision-making. We compare our full system with ablated versions and demonstrate that such higher-order navigation skills greatly impact navigation performance. Overall, the results suggest that our VLM agent framework is a promising path forward for achieving human-like navigation efficiency using higher-order reasoning skills.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在未知的人工环境中实现人类般的导航效率？  \n2. 现有机器人导航系统缺乏利用环境信息和人际互动的能力。  \n3. 如何将高阶导航技能整合进机器人导航系统以提高效率？  \n\n【用了什么创新方法】  \n提出了ReasonNav，一个模块化导航系统，利用视觉-语言模型（VLM）的推理能力，整合人类导航技能。系统分为低层流和高层流，低层流处理定位和路径规划，高层流利用VLM进行高层规划。通过使用带有文本信息的地标记忆库，简化了输入和输出空间，使VLM能够专注于高阶推理。实验证明，ReasonNav在真实和模拟环境中有效提升了导航效率，展示了高阶导航技能对机器人导航性能的重要影响。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "DAGDiff: Guiding Dual-Arm Grasp Diffusion to Stable and Collision-Free Grasps",
            "authors": "Md Faizal Karim,Vignesh Vembar,Keshab Patra,Gaurav Singh,K Madhava Krishna",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21145",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21145",
            "arxiv_html_link": "https://arxiv.org/html/2509.21145v1",
            "abstract": "Reliable dual-arm grasping is essential for manipulating large and complex objects but remains a challenging problem due to stability, collision, and generalization requirements. Prior methods typically decompose the task into two independent grasp proposals, relying on region priors or heuristics that limit generalization and provide no principled guarantee of stability. We propose DAGDiff, an end-to-end framework that directly denoises to grasp pairs in the S​E​(3)×S​E​(3)SE(3)\\times SE(3) space. Our key insight is that stability and collision can be enforced more effectively by guiding the diffusion process with classifier signals, rather than relying on explicit region detection or object priors. To this end, DAGDiff integrates geometry-, stability-, and collision-aware guidance terms that steer the generative process toward grasps that are physically valid and force-closure compliant. We comprehensively evaluate DAGDiff through analytical force-closure checks, collision analysis, and large-scale physics-based simulations, showing consistent improvements over previous work on these metrics. Finally, we demonstrate that our framework generates dual-arm grasps directly on real-world point clouds of previously unseen objects, which are executed on a heterogeneous dual-arm setup where two manipulators reliably grasp and lift them. Project Page: dag-diff.github.io/dagdiff/",
            "introduction": "Manipulating large, dual-arm relevant objects such as monitors, boxes, or chairs requires not only feasible grasps, but also reasoning about force balance and stable interaction between both arms. Imagine the task of picking up a monitor. Humans instinctively place their hands on the opposite sides of the monitor instead of grasping it at random points to balance forces and torques, ensuring stability. For robots, however, acquiring this kind of coordination is far more complex [1]. Developing this sense of dual-arm stability is essential for moving beyond single-arm grasping toward coordinated, physically robust manipulation of real-world objects [2, 3, 4, 5].\n\nWhile grasp pose generation has been explored extensively in the community, most efforts largely focus on single-arm settings. Most methods [6, 7, 8, 9, 10, 11] follow a general recipe of curating a paired dataset consisting of ground truth grasps evaluated using physics simulators, followed by training encoder-decoder style models in a supervised setting. Recently, diffusion models have emerged as powerful generative frameworks for robotic grasping [12, 13, 14, 15, 16] due to their ability to model complex multimodal distributions. This enables them to sample diverse, high-quality grasp poses either uniformly across the object or constrained to specific parts.\n\nWhile these methods have improved robustness and grasp quality on complex shapes, they are designed for single-arm grasps and lack mechanisms to ensure dual-arm stability. Moreover, extending these methods to dual-arm grasping is non-trivial, as exhaustive pair search is costly and naive single-arm extensions often yield unstable solutions [17]. Furthermore, these methods do not explicitly account for collisions and often produce grasps that intersect the object surface, a problem that becomes increasingly critical for larger and more geometrically complex shapes. A possible workaround would be to increase the resolution of the point cloud or latent representation to capture finer surface details, but this would greatly increase computation without guaranteeing collision-free grasps.\n\nIn this work, we introduce DAGDiff: Dual-Arm Grasp Diffusion, an end-to-end dual-arm grasp generation framework that leverages diffusion models guided by classifier signals. Our method extends S​E​(3)SE(3) diffusion to the dual-arm setting to generate grasp pairs that are simultaneously stable under dual-arm force closure and collision free with respect to the object’s surface.\n\nWe frame dual-arm grasp generation as the task of generating two grasps on the object point cloud, each falling in a suitable region, such that they are jointly stable and physically valid (by physically valid we mean grasps that are collision free, and make stable surface contact with the object). One of the key challenges is region selection: heuristic approaches such as choosing farthest regions [12] often fail when those regions itself are physically incompatible, while VLM-based reasoning [18] remains limited in 3D and physical understanding [19, 20]. It is further hindered by the fact that graspable regions rarely have semantic names, leaving no reliable basis for prediction. In contrast, our approach does not rely on region-specific training but instead learns suitable grasp regions implicitly from guidance signals, and we observe that it naturally discovers the right pairs of regions for stable dual-arm grasps (given in supplementary video).\nSpecifically, a force-closure module distinguishes stable from unstable grasp pairs and provides gradients that bias the diffusion process toward stability, while a collision module identifies grasp–object intersections and pushes generated grasps away from collisions. Together, these signals guide the diffusion model to diverse, stable, and physically valid dual-arm grasps.\n\nOur evaluation demonstrates the effectiveness of the proposed method in generating stable grasps within a dual-arm setup. Analytical evaluation based on dual-arm force-closure criteria [17] confirms that the generated grasp pairs satisfy fundamental stability requirements, while physics simulation-based evaluation [21] highlights the robustness of our approach across diverse objects and grasp configurations. Finally, real-world demonstrations show that our framework, trained entirely on synthetic data, transfers effectively to real point clouds, producing physically realizable dual-arm grasps on previously unseen objects like cooking utensils, buckets, drones etc as shown in Figure 1. To summarize the contributions:\n\nWe present DAGDiff, the first framework to the best of our knowledge, for dual-arm grasp generation that extends S​E​(3)SE(3) diffusion with guidance signals, enabling the synthesis of grasp pairs that are both force-closure stable and collision-free on large, geometrically complex objects.\n\nUnlike prior methods that rely on region identification using VLMs or geometric heuristics, our architecture employs geometry-, stability-, and collision-aware multi-head outputs that directly guide the diffusion process toward valid regions of the dual-arm grasp space (Figure 2).\n\nWe show substantial improvements over prior methods and adapted baselines through analytical metrics and large-scale simulations (Table I), and further validate reliable zero-shot transfer on a heterogeneous real-world dual-arm setup with real point clouds and previously unseen objects (Figure 1).",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现稳定且无碰撞的双臂抓取？  \n2. 现有方法在双臂抓取中缺乏有效的稳定性和碰撞检测机制。  \n3. 如何在不依赖区域特定训练的情况下生成有效的抓取区域？  \n\n【用了什么创新方法】  \nDAGDiff是一个端到端的双臂抓取生成框架，利用扩散模型并通过分类器信号引导生成过程。该方法将S​E​(3)扩散扩展到双臂设置，生成同时稳定且物理有效的抓取对。通过力闭合模块和碰撞模块的引导，DAGDiff能够生成多样化、稳定且物理有效的双臂抓取。评估结果表明，该框架在生成稳定抓取方面表现优异，并在真实世界的点云上成功实现了对未见物体的抓取。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems",
            "authors": "Junfeng Yan,Biao Wu,Meng Fang,Ling Chen",
            "subjects": "Robotics (cs.RO); Computation and Language (cs.CL)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21143",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21143",
            "arxiv_html_link": "https://arxiv.org/html/2509.21143v1",
            "abstract": "Multimodal agents have demonstrated strong performance in general GUI interactions, but their application in automotive systems has been largely unexplored. In-vehicle GUIs present distinct challenges: drivers’ limited attention, strict safety requirements, and complex location-based interaction patterns. To address these challenges, we introduce Automotive-ENV, the first high-fidelity benchmark and interaction environment tailored for vehicle GUIs. This platform defines 185 parameterized tasks spanning explicit control, implicit intent understanding, and safety-aware tasks, and provides structured multimodal observations with precise programmatic checks for reproducible evaluation. Building on this benchmark, we propose ASURADA, a geo-aware multimodal agent that integrates GPS-informed context to dynamically adjust actions based on location, environmental conditions, and regional driving norms. Experiments show that geo-aware information significantly improves success on safety-aware tasks, highlighting the importance of location-based context in automotive environments. We will release Automotive-ENV, complete with all tasks and benchmarking tools, to further the development of safe and adaptive in-vehicle agents.",
            "introduction": "Autonomous agents that interpret natural language instructions and control graphical user interfaces (GUI) can provide enormous value to users by automating repetitive tasks, augmenting human cognitive capabilities, and accomplishing complex workflows (Gravitas, 2023; Wu et al., 2023; Xie et al., 2023; Yao et al., 2022b; Yang et al., 2023b; Ding, 2024; Park et al., 2023). To realize this potential, current research efforts have primarily focused on building and evaluating GUI agents capable of operating within desktop operating systems, mobile applications, and web environments (Deng et al., 2023; Rawles et al., 2023; Zheng et al., 2024a; Koh et al., 2024; Kim et al., 2024; He et al., 2024), establishing important foundations for GUI automation research. These existing evaluation methods typically rely on static interface screenshots and user instructions as input, measuring performance by comparing agent behaviors with pre-collected human demonstrations (Deng et al., 2023; Rawles et al., 2023; Toyama et al., 2021; Li et al., 2024; Chai et al., 2024; Xie et al., 2024; Baek & Bae, 2016). Such approaches work well in traditional computing environments because desktop and mobile devices operate in relatively stable and controlled scenarios where device state has limited impact on task execution. However, this focus represents only a subset of the diverse interface ecosystems that people interact with daily, notably excluding In-vehicle GUI systems that support navigation, communication, media, and safety functions in millions of automobiles worldwide.\n\nIn-vehicle GUI systems introduce evaluation challenges that existing methods cannot adequately address. First, automotive agents operate in highly dynamic and safety-critical contexts, where factors such as real-time location, driving state, weather, and traffic conditions directly determine correct task execution (Zhou et al., 2023; Koh et al., 2024). For example, as shown in Figure 1, the seemingly simple command “I can’t see through the windshield, it’s all fogged up” requires the agent to first perform contextual reasoning over current driving conditions, and then correctly operate the interface (e.g., enabling the front defroster). Second, because drivers must prioritize road attention, their commands are typically brief, ambiguous, or incomplete, forcing agents to infer intent from limited information. Third, mistakes in automotive tasks can have immediate safety implications: a single incorrect navigation instruction or inappropriate system response may distract the driver or induce hazardous behavior. Existing evaluation frameworks, centered on interface screenshots and static inputs, fail to capture these challenges as they lack awareness of vehicle state, environmental conditions, and safety constraints, and cannot assess an agent’s adaptability or reliability under real-time driving dynamics (Liu et al., 2023; Wu et al., 2024).\n\nTo address these challenges, we introduce Automotive-ENV, a comprehensive evaluation platform built on a real in-car operating system spanning 8 functional modules with 185 parameterized tasks. Unlike prior benchmarks based on synthetic interfaces or static specifications, Automotive-ENV dynamically instantiates tasks with randomly generated parameters, creating millions of unique scenarios that require agents to generalize across diverse interface states and driving contexts. Our platform leverages production-grade automotive software architectures and their embedded event-handling mechanisms to ensure robust reward signal generation under the safety-aware conditions characteristic of real automotive environments. Beyond the core automotive tasks, we extend Automotive-ENV by integrating external geographic, environmental, and sensor-driven scenarios, thereby enriching the diversity of evaluation conditions and enabling comprehensive assessment across varied driving contexts. Meanwhile, this platform is designed for practical deployment and broad accessibility, requiring less than 4 GB of memory and 10 GB of disk space while connecting agents to automotive systems through standard APIs without proprietary hardware requirements.\n\nTo demonstrate the utility of Automotive-ENV, we develop ASURADA (Automotive Multimodal Agent), a prototype multimodal agent designed to address the unique challenges of in-vehicle GUI environments. Unlike desktop or mobile GUIs, automotive tasks are inherently geo-dependent: user needs and system behaviors vary significantly with GPS location, traffic conditions, and regional driving rules. For example, the seemingly simple utterance “Adjust the air conditioning temperature” may require different actions depending on whether the vehicle is driving through a hot coastal city, a cold mountainous region, or a humid rainy environment. Motivated by this, ASURADA incorporates a novel GPS-informed context integration that conducts reasoning over GPS signals to infer environmental context and location-specific driving regulations. We evaluate ASURADA under both GPS-enhanced multimodal input—screenshot, text, and GPS—and GPS-absent input with only screenshots and text, across realistic scenarios ranging from congestion rerouting to climate control adjustments. Results show that while incorporating geographic context enhances robustness in safety-aware tasks, substantial challenges remain: ASURADA achieves a 65% success rate, outperforming adapted web-based GUI agent baselines but still falling far below human performance at 100%, underscoring both the necessity of geo-aware reasoning and the current limitations of reliable automotive GUI automation.\n\nIn summary, our main contributions are as follows:\n\nWe introduce Automotive-ENV, a high-fidelity evaluation platform for in-vehicle GUI systems that balances generality and safety. It supports multimodal interactions, structured observations, and programmatic feedback to comprehensively assess agent robustness and generalization.\n\nWe introduce Automotive-ENV, a high-fidelity evaluation platform for in-vehicle GUI systems that balances generality and safety. It supports multimodal interactions, structured observations, and programmatic feedback to comprehensively assess agent robustness and generalization.\n\nWe develop ASURADA, a structured VLM-based agent architecture that integrates perception, intent understanding, planning, and execution. A GPS reasoning module is incorporated to adapt agent behavior to geographic context and regional driving rules, improving robustness across diverse driving environments.\n\nWe demonstrate that agents can leverage GPS to perceive richer environmental context and support decision-making, leading to significant improvements in reliability and responsiveness on safety-critical tasks.\n\n1. We introduce Automotive-ENV, a high-fidelity evaluation platform for in-vehicle GUI systems that balances generality and safety. It supports multimodal interactions, structured observations, and programmatic feedback to comprehensively assess agent robustness and generalization.\n\n2. We develop ASURADA, a structured VLM-based agent architecture that integrates perception, intent understanding, planning, and execution. A GPS reasoning module is incorporated to adapt agent behavior to geographic context and regional driving rules, improving robustness across diverse driving environments.\n\n3. We demonstrate that agents can leverage GPS to perceive richer environmental context and support decision-making, leading to significant improvements in reliability and responsiveness on safety-critical tasks.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在汽车环境中评估和提升多模态代理的性能？  \n2. 现有的GUI代理评估方法为何无法满足汽车系统的动态和安全需求？  \n3. 如何有效整合地理位置信息以增强汽车代理的任务执行能力？  \n\n【用了什么创新方法】  \n本研究提出了Automotive-ENV，一个高保真评估平台，专为汽车GUI系统设计，涵盖185个参数化任务，支持多模态交互。通过动态生成任务场景，平台能够评估代理在真实驾驶环境中的适应性和可靠性。此外，开发的ASURADA代理集成了GPS感知模块，能够根据地理上下文调整行为，从而在安全关键任务中显著提高成功率。实验结果表明，ASURADA在安全任务上的成功率达到65%，尽管仍低于人类表现，但展示了地理信息在汽车环境中的重要性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Rich State Observations Empower Reinforcement Learning to Surpass PID: A Drone Ball Balancing Study",
            "authors": "Mingjiang Liu,Hailong Huang",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted for presentation at the Advancements in Aerial Physical Interaction Workshop of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.21122",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21122",
            "arxiv_html_link": "https://arxiv.org/html/2509.21122v1",
            "abstract": "This paper addresses a drone ball-balancing task, in which a drone stabilizes a ball atop a movable beam through cable-based interaction. We propose a hierarchical control framework that decouples high-level balancing policy from low-level drone control, and train a reinforcement learning (RL) policy to handle the high-level decision-making. Simulation results show that the RL policy achieves superior performance compared to carefully tuned PID controllers within the same hierarchical structure. Through systematic comparative analysis, we demonstrate that RL’s advantage stems not from improved parameter tuning or inherent nonlinear mapping capabilities, but from its ability to effectively utilize richer state observations. These findings underscore the critical role of comprehensive state representation in learning-based systems and suggest that enhanced sensing could be instrumental in improving controller performance.",
            "introduction": "Aerial manipulation, which aims to endow aerial robots with the capability to physically interact with their surroundings [1], has attracted significant attention in recent years. The most common aerial manipulation systems employ unmanned aerial vehicles (UAVs) as floating bases and equip them with robotic arms or custom grippers. These configurations have been successfully applied to tasks such as millimeter-level peg-in-hole insertion [2], light bulb removal [3], and in-flight grasping [4]. Alternatively, manipulation can also be achieved through direct attachment or cable suspension, with cable-suspended payload transportation being a prominent example [5, 6, 7].\n\nDespite these advances in aerial manipulation, significant challenges persist in real-world applications. Most existing systems are confined to direct physical contact, yet many practical scenarios require indirect manipulation, which involves interacting with objects via an intermediate tool or platform. For instance, during transportation, a robot might need to carry a box using a cable-suspended carrier. Here, the robot must manipulate the carrier rather than the box directly, while actively stabilizing it to prevent falling. Such forms of indirect manipulation introduce new challenges, especially in learning to control intermediary tools.\n\nTo explore the potential of indirect aerial manipulation, this paper studies a drone ball-balancing task (see Fig. 1), where a drone tethered to one beam end via a rope and is required to manipulate a beam to position a ball at a target location. To solve this problem, we proposed a hierarchical control framework comprising: A high-level policy that generates velocity commands according to system state observations and a low-level velocity controller for precise reference tracking.\n\nMotivated by recent advances in RL [8, 9], we formulated the drone ball-balancing task as a Markov Decision Process (MDP) and developed an RL-based high-level controller to address this problem. Specifically, we created a drone ball-balancing simulator using Isaac Lab [10]. To enable GPU-accelerated parallel training, we implemented a PyTorch-based low-level flight controller based on the SE(3) geometry controller [11]. The balancing policy was parameterized by a neural network and trained using the Robust Policy Optimization algorithm (RPO) [12].\n\nSimulation experiments demonstrate that the RL-based policy exhibits excellent performance in the drone ball-balancing task. To further validate the superiority of the RL-based approach, we replaced the high-level RL agent with incremental PID controllers [13] within the same hierarchical framework. Empirical results reveal that PID controllers fail to stabilize the system, even when augmented with additional velocity constraints.\n\nWhile numerous studies demonstrate RL’s superiority over classical control methods (e.g., PID and model predictive control (MPC)) in robotics [14, 15, 16], these findings remain largely empirical, lacking systematic analysis of the fundamental factors contributing to RL’s success or classical methods’ limitations. In contrast, a recent comparative study in autonomous drone racing [17] reveals that RL’s advantage stems primarily from its ability to optimize more effective objectives. This insight provides a principled foundation for future method selection and advancement in similar robotic applications. Inspired by this work, we conducted systematic controlled experiments to investigate why RL outperforms PID control. Our results demonstrate that RL’s superiority stems not from discovering superior control gains or employing nonlinear error mapping, but rather from its ability to leverage richer state observations for decision-making effectively.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何通过强化学习（RL）实现无人机在球平衡任务中的优越表现？  \n2. RL相对于PID控制器的优势源于什么因素？  \n3. 如何设计一个分层控制框架以解决间接操控的挑战？  \n\n【用了什么创新方法】  \n本研究提出了一种分层控制框架，将高层平衡策略与低层无人机控制解耦。通过将无人机球平衡任务建模为马尔可夫决策过程（MDP），并使用强化学习算法（RPO）训练高层控制器，研究展示了RL在处理复杂状态观察时的优势。模拟实验表明，RL策略在无人机球平衡任务中表现优异，超越了传统PID控制器，强调了全面状态表示在学习系统中的重要性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Cross-Modal Instructions for Robot Motion Generation",
            "authors": "William Barron,Xiaoxiang Dong,Matthew Johnson-Roberson,Weiming Zhi",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21107",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21107",
            "arxiv_html_link": "https://arxiv.org/html/2509.21107v1",
            "abstract": "Teaching robots novel behaviors typically requires motion demonstrations via teleoperation or kinaesthetic teaching, that is, physically guiding the robot. While recent work has explored using human sketches to specify desired behaviors, data collection remains cumbersome, and demonstration datasets are difficult to scale. In this paper, we introduce an alternative paradigm, Learning from Cross-Modal Instructions, where robots are shaped by demonstrations in the form of rough annotations, which can contain free-form text labels, and are used in lieu of physical motion. We introduce the CrossInstruct framework, which integrates cross-modal instructions as examples into the context input to a foundational vision–language model (VLM). The VLM then iteratively queries a smaller, fine-tuned model, and synthesizes the desired motion over multiple 2D views. These are then subsequently fused into a coherent distribution over 3D motion trajectories in the robot’s workspace. By incorporating the reasoning of the large VLM with a fine-grained pointing model, CrossInstruct produces executable robot behaviors that generalize beyond the environment of in the limited set of instruction examples. We then introduce a downstream reinforcement learning pipeline that leverages CrossInstruct outputs to efficiently learn policies to complete fine-grained tasks. We rigorously evaluate CrossInstruct on benchmark simulation tasks and real hardware, demonstrating effectiveness without additional fine-tuning and providing a strong initialization for policies subsequently refined via reinforcement learning.",
            "introduction": "Imitation learning, also known as Learning from Demonstration, is a dominant paradigm for teaching robots new skills [1]. Imitation learning requires the collection of a dataset of demonstrations, which are generally provided by a human user teleoperating the robot via a remote controller or by kinesthetic teaching, i.e. physically handling the robot. In both of these cases, collecting a sufficient number of demonstrations for even marginal levels of generalization is challenging. There have also been efforts to reduce the human burden of providing demonstrations, including hardware adaptations [2], and using human sketches as a demonstration interface [3, 4, 5, 6, 7]. Humans have a remarkable ability to adapt the same skill to different environments and setups, even after observing one or two demonstrations. However, imitation learning is far less demonstration-efficient, and it can be challenging to collect a sufficient number of demonstrations to generalize behavior when the scene changes. In this work, we aim to circumvent the cumbersome, large-scale human input required for imitation learning.\n\nWe introduce Learning from Cross-modal Instructions as an alternative paradigm to shape robot behavior. In the proposed method, human operators provide cross-modal instructions, which are free-form sketches and textual labels on an image of an operating environment. The robot infers the intended behavior from these inputs and generalizes execution across varying setups. To learn from cross-modal instructions, we propose the CrossInstruct framework. LABEL:fig:teaser illustrates several examples in both simulation and the real world with cross-modal instructions and subsequent robot behavior generated by CrossInstruct. CrossInstruct takes cross-modal instructions as in-context learning examples [8] for a large VLM. The large model performs high-level task reasoning and delegates pixel-level keypoint localization to a smaller VLM fine-tuned for 2D pointing, which returns precise coordinates for task-critical features identified by the large model. The hierarchical coupling between the large reasoning VLM with the smaller, fine-tuned model enables the efficient and accurate identification of task-relevant keypoints. These are then used to produce motion trajectory sketches over a small set of multi-view images of the setup. The resulting 2D trajectories are lifted into the robot’s workspace to yield a distribution over 3D trajectories. Combined with end-effector orientations and gripper actions generated by the reasoning model, this produces executable motion sequences.\n\nWe demonstrate the capabilities of CrossInstruct to shape behavior to complete and generalize across a wide range of tasks, in both simulation and in the real world, without additional fine-tuning. Beyond directly executing the produced motions, we can enable the robots to achieve an even higher level of precision by joining CrossInstruct with a downstream reinforcement learning pipeline. Concretely, the technical contributions in this paper include:\n\nThe Learning from Cross-Modal Instructions paradigm, circumventing the need for physical demonstrations;\n\nThe CrossInstruct framework to leverage cross-modal instructions in an in-context fashion to synthesize robot behavior. This is achieved by a hierarchical coupling of two VLMs and using raycasting to fuse multi-view 2D sketches into executable robot motion;\n\nEvaluations on benchmark tasks, demonstrating robustness of CrossInstruct out of the box in both simulation and the real world, as well as leveraging the distribution of trajectories to enable efficient reinforcement learning.\n\n1. The Learning from Cross-Modal Instructions paradigm, circumventing the need for physical demonstrations;\n\n2. The CrossInstruct framework to leverage cross-modal instructions in an in-context fashion to synthesize robot behavior. This is achieved by a hierarchical coupling of two VLMs and using raycasting to fuse multi-view 2D sketches into executable robot motion;\n\n3. Evaluations on benchmark tasks, demonstrating robustness of CrossInstruct out of the box in both simulation and the real world, as well as leveraging the distribution of trajectories to enable efficient reinforcement learning.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何减少机器人学习新行为所需的物理演示数据。  \n2. 如何利用跨模态指令来生成机器人运动而无需大量示范。  \n3. 如何实现机器人行为的高效泛化和执行。  \n\n【用了什么创新方法】  \n提出了一种新的学习范式——跨模态指令学习，利用人类提供的草图和文本标签作为输入。开发了CrossInstruct框架，将跨模态指令整合到大型视觉-语言模型中，通过层次化的模型耦合实现任务推理和关键点定位。最终，生成的2D轨迹被提升为3D运动轨迹，能够在多种环境中执行。CrossInstruct在基准任务上表现出色，且无需额外微调，为后续的强化学习提供了强有力的初始化。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Flight Dynamics to Sensing Modalities: Exploiting Drone Ground Effect for Accurate Edge Detection",
            "authors": "Chenyu Zhao,Jingao Xu,Ciyu Ruan,Haoyang Wang,Shengbo Wang,Jiaqi Li,Jirong Zha,Weijie Hong,Zheng Yang,Yunhao Liu,Xiao-Ping Zhang,Xinlei Chen",
            "subjects": "Robotics (cs.RO); Networking and Internet Architecture (cs.NI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21085",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21085",
            "arxiv_html_link": "https://arxiv.org/html/2509.21085v1",
            "abstract": "Drone-based rapid and accurate environmental edge detection is highly advantageous for tasks such as disaster relief and autonomous navigation. Current methods, using radars or cameras, raise deployment costs and burden lightweight drones with high computational demands. In this paper, we propose AirTouch, a system that transforms the ground effect from a stability ”foe” in traditional flight control views, into a ”friend” for accurate and efficient edge detection. Our key insight is that analyzing drone basic attitude sensor readings and flight commands allows us to detect ground effect changes. Such changes typically indicate the drone flying over a boundary of two materials, making this information valuable for edge detection. We approach this insight through theoretical analysis, algorithm design, and implementation, fully leveraging the ground effect as a new sensing modality without compromising drone flight stability, thereby achieving accurate and efficient scene edge detection. We also compare this new sensing modality with vision-based methods to clarify its exclusive advantages in resource efficiency and detection capability. Extensive evaluations demonstrate that our system achieves a high detection accuracy with mean detection distance errors of 0.051m, outperforming the baseline method performance by 86%86\\%. With such detection performance, our system requires only 43 mW power consumption, contributing to this new sensing modality for low-cost and highly efficient edge detection.",
            "introduction": "Rapid and accurate environmental edge detection is crucial in various applications, including disaster response [2, 3, 4], rescue-and-relief [5, 6], and autonomous navigation [7, 8, 9]. A key aspect involves detecting terrain edges, such as sudden changes in height (e.g., steps, cliffs) and variations in ground materials (e.g., water, soil, solid rock). With prior knowledge of these edges, intelligent systems can plan paths for humans and robots more logically, efficiently, and safely  [10, 11, 12, 13]. To boost efficiency and cut costs in large-scale edge detection [14], mainstream systems leverage swarms of lightweight drones [15] (a.k.a., UAV) to execute the task collaboratively as they fly and scan the entire scene [16, 17, 18, 19, 20].\n\nIn disaster relief scenarios, such as earthquake rescue or flood response, the ability to quickly and precisely detect edges, including sudden height changes and material boundaries, can significantly enhance the efficiency and safety of rescue missions [21, 22]. For example, drones equipped with edge detection capabilities can navigate through debris fields, identify safe landing zones, and locate survivors trapped in complex terrains. This enables rescue teams to reach affected areas more rapidly and with greater precision, potentially saving lives [23, 24]. In autonomous navigation, edge detection plays a vital role in enabling drones to operate safely and efficiently in unknown environments. By detecting edges such as cliffs, steps, and material transitions, drones can avoid obstacles, plan optimal flight paths, and make real-time decisions to ensure smooth and collision-free navigation [25, 26]. This is particularly important in scenarios where GPS signals are weak or unavailable, such as in indoor environments or dense forests. Accurate edge detection allows drones to maintain situational awareness and adapt to dynamic surroundings, improving their autonomy and reliability.\n\nExisting drone-based edge detection methods face challenges, categorized into two main types:\n(i) Wireless-signal based methods: These methods utilize signals like mmWave Radar [2], Wi-Fi [27], terahertz radar [28], LiDAR [29], and acoustic signals [30] for contactless edge detection.\nWhile effective, they often rely on infrastructure and struggle with operations in inaccessible areas [31], and are typically power-hungry, making them less suitable for lightweight drones with limited battery capacity.\n(ii) Visual-sensor-based methods: These algorithms aim to accurately detect object boundaries using computer vision techniques or neural networks [32, 33, 34].\nHowever, they require significant computational resources, limiting their deployment on resource-constrained drones [35], which can be a bottleneck for real-time applications. Visual sensors like cameras struggle in low-light conditions or when dealing with surfaces of similar colors and textures, which can compromise their detection accuracy and robustness.\nFurthermore, the existing pioneer study [36] proposes multi-modal fusion sensing, achieving promising accuracy by integrating the information from both wireless and visual domains.\n\nIn this work, we aim to introduce a novel approach for edge detection on lightweight drones, which will serve as a significant complement to the aforementioned two envelopes, especially in situations where computational resources are scarce and scene visibility is low.\nAs illustrated in Fig .1, when drones fly closely to the ground (i.e., a surface): the airflow generated by the rotating rotors bounces off the surface below the drone, creating additional upward lift and leading to disturbances in a drone’s flight state.\nThis phenomenon, widely known as ground effect (GE) [37, 38], varies with the drone’s altitude above the surface and the type of surface material.\n\nGround effect of drones.\nAchieving precise control over drone positions is paramount, yet remains a considerable challenge [39, 40].\nThis challenge is predominantly attributed to the intricate interplay between rotor and wing airflows with the ground surface [41, 42].\nThe aerospace industry has recognized the ground effect for some time, acknowledging its potential to amplify lift forces while decreasing aerodynamic drag [43, 44].\nDespite advantages, they also pose challenges to flight stability [45].\nConsequently, mitigating the impacts of ground effect has been a persistent issue [46].\nIn contrast, this paper diverges from conventional approaches by harnessing the ground effect to detect edges rather than attempting to neutralize it.\nAs far as we are aware, it is the first system to perform edge detection without the use of additional sensors.\n\nNew sensing modality.\nThe key insight behind this work is to translate the physical phenomenon ground effect in flight dynamics into a fresh sensing modality for edge detection on lightweight drones - as shown in Fig. 1(b), by identifying ground effect changes, we can deduce sudden alterations in the drone’s relative height above a surface or in the surface material itself, pinpointing the edge information within the scene. The process has similarities with sensing the surface by touching it using air, but only using the most basic sensing ability on lightweight drones.\nHowever, translating this insight into a practical system still faces two challenges:\n\n∙\\bullet The target discrepancy between sensing and flight control complicates ground effect profiling.\nFor the ground effect, the sensing modalities treat it as a friend, aiming to detect drone flight instability through abrupt changes in sensor readings (e.g., Inertial Measurement Unit (IMU) samples) to measure it.\nHowever, drone flight control systems view it as a foe, striving to minimize its impact on flight dynamic stability. \nThis leads to sensor (e.g., IMU) readings being extensively smoothed out after those complex proportional–integral–derivative (PID) operations [47], challenging the effective profiling of the ground effect.\n\n∙\\bullet The noisy sensing data overwhelms the vital feedback related to the ground effect.\nThe dynamic measurements from inexpensive, low-power sensors on lightweight drones make it difficult to extract actual ground-effect-related fluctuations from those complex noises in raw data.\nThe circumstance is further complicated by the flight control system’s smoothing and attenuating functions on the ground effect.\n\nRemark.\nUnder the premise that the flight control module treats the ground effect as a foe and tries to negate its impact on drone stability, accurately and efficiently measuring and profiling the attenuated ground effect from noisy sensor data is crucial for edge detection.\n\nTo tackle the above challenges, we design and implement AirTouch, the first system that treats the ground effect as a friend and offers methods to extract related data from onboard sensors of lightweight drones, despite the flight control module’s influence.\nBenefiting from AirTouch, the ground effect can be leveraged as a new sensing modality for tasks such as edge detection.\nIn general, AirTouch excels in the following three aspects.\n\n∙\\bullet On the sensory input front.\nWe demonstrate that leveraging the onboard IMU readings and motor commands from flight controllers could effectively profile the ground effect.\nBy examining the complex physical dynamics and drone stability control, we uncover how drone attitudes (i.e., measured by the IMU) and control signals (i.e., indicated by motor commands) interrelate and complement each other.\nTheir combination offers a full insight into the ground effect, even with the flight control module’s adjustments.\n\n∙\\bullet On the algorithm front.\nWe propose a ground effect-informed environmental edge detection pipeline, which comprises\n(i)(i) a fluctuation components feature extraction method and a cascaded cross-spectrum feature fusion technique to facilitate the extraction of ground-effect-related information from noisy IMU measurements and motor commands;\n(i​i)(ii) a compact neural network (NN) designed to detect environmental edges from the extracted features;\nand (i​i​i)(iii) an aerodynamics-instructed physical filter to further improve edge detection performance of neural network.\n\n∙\\bullet On the implementation front.\nTo further boost computational efficiency and enable lightweight drones to run the proposed NN in real-time, we apply techniques such as neural unit pruning and weight quantization on the NN before onboard deployment.\nAdditionally, during the NN training, we introduce a meticulously designed Disturbance Force-Informed loss function by analysis and modeling, incorporating binary cross-entropy loss, to expedite network convergence and make the network learn fine-grained bias.\n\nWe evaluate the performance of AirTouch by conducting extensive experiments and comparing it with the baseline using a real-world testbed. Based on a lightweight drone and its onboard IMU and motors, we conducted abrupt height discontinuity edge detection and material interface transition edge detection, respectively. The results demonstrate that our system achieves a high detection accuracy with mean detection distance errors of 0.051m. Furthermore, our system surpasses the baseline performance by 86%86\\% with the same available sensor information. Additionally, we compare AirTouch with vision-based edge detection methods to clarify its exclusive advantages and discuss some concerns about our system’s capabilities. Note that AirTouch is open-source on GitHub1.\n\n11footnotetext: https://github.com/ChenyuZhaoTHU/AirTouch\n\nThe main contributions of this paper are as follows:\n\nWe propose AirTouch, as far as we are aware, the first system that translates the traditionally negative ground effect into a new, positive sensing modality for accurate and efficient environmental edge detection.\n\nWe demonstrate that combining IMU sampling and motor commands provides an effective sensing paradigm to characterize the ground effect under the influence of the flight control system.\nOn this basis, we present a comprehensive and neural network-based pipeline aided by modeled physical knowledge for profiling, extracting, and utilizing the ground effect for sensing tasks from noisy sensory input.\n\nWe develop a prototype system and evaluate the AirTouch system through real-world data and in-field experiments on a lightweight drone by deploying our system on onboard computing chips. Extensive evaluation results show the effectiveness of our system in impressive edge detection accuracy on low-cost drones and low-energy consumption sensors and represent its exclusive advantages of capabilities by comparison with a vision-based method.\n\nThe remainder of this paper is structured as follows: §2 presents the core intuition and prerequisites underlying the AirTouch system. After introducing the system overview in §3, we elaborate on the two main components of the system in §4 and §5, respectively. In §6, we introduce the implementation. In §7, we evaluate our system. In §8, we have a discussion section. In the last, we conclude this paper in §9.\n\n1. We propose AirTouch, as far as we are aware, the first system that translates the traditionally negative ground effect into a new, positive sensing modality for accurate and efficient environmental edge detection.\n\n2. We demonstrate that combining IMU sampling and motor commands provides an effective sensing paradigm to characterize the ground effect under the influence of the flight control system.\nOn this basis, we present a comprehensive and neural network-based pipeline aided by modeled physical knowledge for profiling, extracting, and utilizing the ground effect for sensing tasks from noisy sensory input.\n\n3. We develop a prototype system and evaluate the AirTouch system through real-world data and in-field experiments on a lightweight drone by deploying our system on onboard computing chips. Extensive evaluation results show the effectiveness of our system in impressive edge detection accuracy on low-cost drones and low-energy consumption sensors and represent its exclusive advantages of capabilities by comparison with a vision-based method.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用无人机的地面效应进行高效的边缘检测。  \n2. 现有的边缘检测方法在资源受限的轻量级无人机上面临的计算负担和能耗问题。  \n3. 如何从噪声传感器数据中提取与地面效应相关的信息以进行边缘检测。  \n\n【用了什么创新方法】  \n本文提出了AirTouch系统，将无人机的地面效应转变为一种新的传感模式，用于环境边缘检测。通过分析无人机的基本姿态传感器读数和飞行命令，系统能够检测地面效应的变化，从而识别材料边界。方法包括利用IMU和电机命令的组合进行地面效应特征提取，设计紧凑的神经网络进行边缘检测，并应用物理滤波器以提高检测性能。实验结果表明，AirTouch在边缘检测准确性上超越基线方法86%，且功耗仅为43 mW，展现了其在低成本和高效能边缘检测中的独特优势。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Normalizing Flows are Capable Visuomotor Policy Learning Models",
            "authors": "Simon Kristoffersson Lind,Jialong Li,Maj Stenmark,Volker Krüger",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21073",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21073",
            "arxiv_html_link": "https://arxiv.org/html/2509.21073v1",
            "abstract": "The field of general purpose robotics has recently embraced powerful probabilistic models, such as diffusion models, to model and learn complex behaviors. However, these models often come with significant trade-offs, namely high computational costs for inference and a fundamental inability to quantify output uncertainty. We argue that a model’s trustworthiness, a critical factor for reliable, general-purpose robotics, is inherently linked to its ability to provide confidence measures.",
            "introduction": "Diffusion Policy (DP) [1] is one of the mainstream methods for visuomotor policy learning using a Denoising Diffusion Probabilistic Model (DDPM) [2]. This approach has secured a place in robot policy learning due to its powerful expressive capabilities.\n\nDespite impressive performance in modeling robotic behaviors, diffusion models exhibit a number of drawbacks. Most importantly, they do not produce any measure of confidence or uncertainty in their outputs.\nAn implication of this is that diffusion models cannot give any guarantees regarding the quality of their outputs, which is a weakness.\n\nFirst and foremost, uncertainty is inevitable in general-purpose robotics, and we argue that a measure of confidence is necessary for a model to be considered trustworthy. Crucially, the model should be able to use this confidence to improve the generated action sequences.\n\nAdditionally, the training and inference costs for diffusion policy are often substantial, primarily because of the sequential denoising steps required for action generation.\n\nWith this work, we propose to use Normalizing Flows (NFs) [3] for visuomotor policy learning.\nWe argue that NFs are a natural alternative to diffusion models,\npossessing neither of the aforementioned flaws.\nNFs provide a measure of confidence for their outputs through a statistically sound approximation to the underlying probability.\nAdditionally, inference in an NFs requires only a single pass through the model,\nwhich makes inference much more efficient.\nWhile providing an output probability, and fast inference,\nNFs also exhibit the same properties that make diffusion models desirable,\nnamely stochastic generative sampling, and the ability to model complex and multimodal distributions.\nThrough experiments within the RoboTwin 2.0 simulation framework [4],\nwe show that NFs are able to learn robotic tasks with a success rate comparable to, and often better than, DP,\nespecially when training data is sparse.\n\nTo summarize, our contributions are as follows:\n\nWe propose to use NFs for visuomotor policy learning and introduce our model, Normalizing Flow Policy \n(NF-P).\n\nThrough experiments with 4 different simulated robotic tasks of varying difficulty, we show that NF-P reaches comparable performance to DP, and provides superior performance compared to DP when little training data is available.\n\nWe demonstrate that NFs can provide an order of magnitude faster inference times compared to diffusion models.\n\nWe validate several key techniques on normalizing flows to achieve strong performance in Visuomotor Policy Learning: visual and action conditioning, action-sequence based generation with stride, and likelihood optimization of the output.\n\nIn an ablation study, we highlight various factors and design choices that affect the performance of NFs for visuomotor policy learning.\n\n1. First and foremost, uncertainty is inevitable in general-purpose robotics, and we argue that a measure of confidence is necessary for a model to be considered trustworthy. Crucially, the model should be able to use this confidence to improve the generated action sequences.\n\n2. Additionally, the training and inference costs for diffusion policy are often substantial, primarily because of the sequential denoising steps required for action generation.\n\n1. We propose to use NFs for visuomotor policy learning and introduce our model, Normalizing Flow Policy \n(NF-P).\n\n2. Through experiments with 4 different simulated robotic tasks of varying difficulty, we show that NF-P reaches comparable performance to DP, and provides superior performance compared to DP when little training data is available.\n\n3. We demonstrate that NFs can provide an order of magnitude faster inference times compared to diffusion models.\n\n4. We validate several key techniques on normalizing flows to achieve strong performance in Visuomotor Policy Learning: visual and action conditioning, action-sequence based generation with stride, and likelihood optimization of the output.\n\n5. In an ablation study, we highlight various factors and design choices that affect the performance of NFs for visuomotor policy learning.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有的扩散模型在输出不确定性量化方面存在缺陷。  \n2. 扩散政策的训练和推理成本高，影响了其在机器人政策学习中的应用。  \n3. 需要一种能够提供输出置信度的模型，以提高生成的动作序列的可靠性。  \n\n【用了什么创新方法】  \n本研究提出使用归一化流（Normalizing Flows, NFs）进行视觉运动政策学习，介绍了我们的模型——归一化流政策（Normalizing Flow Policy, NF-P）。通过在RoboTwin 2.0仿真框架中进行的实验，我们表明NF-P在四个不同难度的机器人任务上达到了与扩散政策相当的性能，且在训练数据稀缺时表现更优。NFs不仅提供了输出概率，还显著提高了推理速度，相较于扩散模型快了一个数量级。此外，我们验证了多项关键技术以实现强大的视觉运动政策学习性能，并通过消融研究揭示了影响NFs性能的设计选择。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models",
            "authors": "Sibo Li,Qianyue Hao,Yu Shang,Yong Li",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21027",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21027",
            "arxiv_html_link": "https://arxiv.org/html/2509.21027v1",
            "abstract": "Robotic world models are a promising paradigm for forecasting future environment states, yet their inference speed and the physical plausibility of generated trajectories remain critical bottlenecks, limiting their real-world applications. This stems from the redundancy of the prevailing frame-to-frame generation approach, where the model conducts costly computation on similar frames, as well as neglecting the semantic importance of key transitions.\nTo address this inefficiency, we propose KeyWorld, a framework that improves text-conditioned robotic world models by concentrating transformers computation on a few semantic key frames while employing a lightweight convolutional model to fill the intermediate frames.\nSpecifically, KeyWorld first identifies significant transitions by iteratively simplifying the robot’s motion trajectories, obtaining the ground truth key frames.\nThen, a DiT model is trained to reason and generate these physically meaningful key frames from textual task descriptions.\nFinally, a lightweight interpolator efficiently reconstructs the full video by inpainting all intermediate frames.\nEvaluations on the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68×\\times acceleration compared to the frame-to-frame generation baseline, and focusing on the motion-aware key frames further contributes to the physical validity of the generated videos, especially on complex tasks.\nOur approach highlights a practical path toward deploying world models in real-time robotic control and other domains requiring both efficient and effective world models. Code is released at https://anonymous.4open.science/r/Keyworld-E43D.",
            "introduction": "Robotic world models are generative frameworks that predict future environment states based on an initial observation and a conditioning input (Ding et al., 2024; Agarwal et al., 2025). Their ability to simulate plausible future trajectories is crucial for a variety of applications, ranging from model-based reinforcement learning (MBRL) (Luo et al., 2023; Hansen et al., 2023) to policy evaluation (Shang et al., 2025; Li et al., 2025; Kawaharazuka et al., 2024).\nHowever, the practical deployment of these models faces two significant challenges. First, their powerful predictive capability comes at a substantial computational cost, severely hindering applications like online planning. Second, the prevailing per-frame generation paradigm often fails to produce physically consistent trajectories, leading to implausible motions that undermine the utility of the simulation for downstream tasks. These bottlenecks severely limit the realism and efficiency of model-based reasoning, calling for more efficient and physically-grounded generation paradigms.\n\nWithin such robotic scenarios, observations are typically captured by a fixed camera, with the robot being the primary moving entity (Liu et al., 2023; Brohan et al., 2022). From the human perspective, it is easy to imagine the video progression by visualizing a few key motions, such as “move left”, “grasp the object”, and “lift”.\nHowever, in stark contrast to this efficient reasoning process, current world models follow a frame-by-frame approach. They incur substantial computational redundancy by generating every frame from scratch with costly image generation modules (Wu et al., 2024; Yang et al., 2024). In addition, the standard practice of applying a uniform reconstruction loss during training forces the model to allocate its capacity equally across all frames, regardless of their semantic importance (Agarwal et al., 2025; Cen et al., 2025). This dilutes the learning signal for critical state transitions and ultimately hinders the generation of physically coherent long-horizon sequences.\n\nAn intuitive solution to reduce this redundancy is to synthesize only a sparse subset of frames (key frames) using the expensive world model, and reconstruct the remaining frames with a lightweight model conditioned on those key frames. However, each step in this roadmap is challenging: (1) Selecting appropriate key frames. Key frames must retain the trajectory’s essential semantics while leaving intermediate motion simple enough for a lightweight interpolator. (2) Generating key frames. Unlike per-frame generation, this task requires the model to synthesize temporally distant anchors while preserving global coherence and physical plausibility, posing distribution-shift and long-range dependency challenges. (3) Reconstructing between key frames. The number of intermediate frames is unknown, and large pose differences between key frames can produce substantial motion gaps that are difficult to model.\n\nTo address the above challenges, we propose KeyWorld, a framework that enhances the efficiency and effectiveness of text-conditioned robotic world models through explicitly focusing the computational load on key-frame reasoning.\nFirst, we construct a motion-aware key frames dataset from robotic poses using the Ramer–Douglas–Peucker (RDP) (Ramer, 1972; Douglas & Peucker, 1973) algorithm, which retains significant motion transitions and discards the steady movements. This ensures that the preserved key frames capture essential semantics and the intervals between them remain simple enough for lightweight interpolation.\nWith the key frame dataset, we train a Diffusion Transformer (DiT) to reason about critical motions from the task description and initial state, and then synthesize the corresponding key frames.\nBy fine-tuning on motion-aware key frames, the model learns to generate semantically critical anchors, significantly reducing the computational burden while enhancing its focus on essential physical interactions.\nFinally, we employ a lightweight Convolutional Neural Network (CNN) model, which is powerful enough for reconstructing the full video sequence by predicting frame gaps and generating intermediate frames between consecutive key frames while also eliminating heavy computational overhead.\nWe evaluate KeyWorld on the representative robotic benchmark LIBERO (Liu et al., 2023), and results demonstrate a 5×\\times acceleration compared to the frame-to-frame model. Furthermore, the motion-aware key frames guide the model to produce trajectories with higher physical plausibility, notably resulting in a substantially increased probability of the robot manipulating the correct target object.\nBy addressing the dual challenges of efficiency and physical fidelity, our approach enables more practical deployment of world models in real-time robotic applications.\nThe contribution of our work is summarized as follows:\n\nWe propose KeyWorld, an efficient and modular framework that decouples text-conditioned robotic world model inference into diffusion-based key frame generation and lightweight intermediate frame interpolation. This design significantly reduces video rollout costs and enhances semantic understanding at critical frames.\n\nWe propose KeyWorld, an efficient and modular framework that decouples text-conditioned robotic world model inference into diffusion-based key frame generation and lightweight intermediate frame interpolation. This design significantly reduces video rollout costs and enhances semantic understanding at critical frames.\n\nWe introduce a motion-aware key-frame detection paradigm that selects semantically critical states directly from robot pose trajectories. By aligning frame selection with meaningful physical transitions, this design not only provides a grounded abstraction of robotic videos but also fosters a sharper representation of physical dynamics within the model.\n\nWe extensively evaluate KeyWorld on the LIBERO benchmark and demonstrate that it achieves up to 5.68×\\times acceleration while maintaining superior video quality across multiple metrics. These results suggest that motion-aware key-frame reasoning offers a viable path for making robotic world models more practical in time-sensitive and physics-sensitive applications\n\n1. We propose KeyWorld, an efficient and modular framework that decouples text-conditioned robotic world model inference into diffusion-based key frame generation and lightweight intermediate frame interpolation. This design significantly reduces video rollout costs and enhances semantic understanding at critical frames.\n\n2. We introduce a motion-aware key-frame detection paradigm that selects semantically critical states directly from robot pose trajectories. By aligning frame selection with meaningful physical transitions, this design not only provides a grounded abstraction of robotic videos but also fosters a sharper representation of physical dynamics within the model.\n\n3. We extensively evaluate KeyWorld on the LIBERO benchmark and demonstrate that it achieves up to 5.68×\\times acceleration while maintaining superior video quality across multiple metrics. These results suggest that motion-aware key-frame reasoning offers a viable path for making robotic world models more practical in time-sensitive and physics-sensitive applications",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高机器人世界模型的推理速度和生成轨迹的物理合理性？  \n2. 当前的逐帧生成方法导致计算冗余和缺乏语义重要性，如何解决这些问题？  \n\n【用了什么创新方法】  \n提出了KeyWorld框架，通过集中计算在少量语义关键帧上，结合轻量级卷积模型填补中间帧，从而提高了文本条件下的机器人世界模型的效率和有效性。首先，通过简化机器人的运动轨迹识别重要的过渡帧，获得真实的关键帧。然后，训练Diffusion Transformer模型生成这些物理上有意义的关键帧。最后，使用轻量级插值器高效重建完整视频。评估结果表明，KeyWorld相比于逐帧生成基线实现了5.68倍的加速，并且在复杂任务中生成的轨迹具有更高的物理合理性，展示了在实时机器人控制等领域的应用潜力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Multi-Robot Vision-Based Task and Motion Planning for EV Battery Disassembly and Sorting",
            "authors": "Abdelaziz Shaarawy,Cansu Erdogan,Rustam Stolkin,Alireza Rastegarpanah",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21020",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21020",
            "arxiv_html_link": "https://arxiv.org/html/2509.21020v1",
            "abstract": "Electric-vehicle (EV) battery disassembly requires precise multi-robot coordination, short and reliable motions, and robust collision safety in cluttered, dynamic scenes. We propose a four-layer task-and-motion planning (TAMP) framework that couples symbolic task planning and cost- and accessibility-aware allocation with a TP-GMM-guided motion planner learned from demonstrations. Stereo vision with YOLOv8 provides real-time component localization, while OctoMap-based 3D mapping and FCL(Flexible Collision Library) checks in MoveIt unify predictive digital-twin collision checking with reactive, vision-based avoidance. Validated on two UR10e robots across cable, busbar, service plug, and three leaf-cell removals, the approach yields substantially more compact and safer motions than a default RRTConnect baseline under identical perception and task assignments: average end-effector path length drops by −63.3%-63.3\\% and makespan by −8.1%-8.1\\%; per-arm swept volumes shrink (R1: 0.583→0.139​m30.583\\rightarrow 0.139\\,\\mathrm{m}^{3}; R2: 0.696→0.252​m30.696\\rightarrow 0.252\\,\\mathrm{m}^{3}), and mutual overlap decreases by 47%47\\% (0.064→0.034​m30.064\\rightarrow 0.034\\,\\mathrm{m}^{3}). These results highlight improved autonomy, precision, and safety for multi-robot EV battery disassembly in unstructured, dynamic environments.",
            "introduction": "Task planning enables autonomous robots to execute complex, multi-step operations under resource constraints by selecting and sequencing high-level actions. When coupled with motion planning, it determines how and when to act amid environmental dynamics and hardware limits. In single-robot settings, learning- and optimisation-based approaches (e.g., ANFIS, GA, RL) improve allocation and ordering under uncertainty [1]. In multi-robot systems, continuous coordination, collision constraints, and shared resources make scheduling more challenging. Recent methods jointly encode geometric feasibility and semantic context for collision-free execution [2, 3]. Electric-vehicle (EV) battery disassembly is a demanding, safety-critical domain: tasks are sequential and tightly coupled (e.g., cable/busbar removal, service plug operations, cell extraction), fixtures and tolerances are strict, and the workspace is cluttered and dynamic [4, 5]. Efficient execution, therefore, hinges on an integrated view of what to plan (symbolic reasoning and allocation) and how to execute (geometrically feasible, collision-safe motions) with the ability to adapt online.\n\nThis work presents a four-layer planning architecture for coordinated multi-robot operation in dynamic environments (Fig. 2). The framework comprises high-level task planning (detecting and localising target objects, and sequencing by priority), low-level task planning (, accessibility, and tool compatibility with cross-robot allocation), high-level motion planning (trajectories consistent with task and spatio-temporal constraints), and low-level motion planning (online updates, collision avoidance, and replanning). Stereo vision with depth cameras integrated with YOLOv8, maintains a 3D OctoMap that imposes spatial constraints on planning. A MoveIt [6]/FCL (Flexible Collision Library) [7] digital twin predicts collisions before execution, while a complementary vision-based module reacts during execution; together they prevent both environment and self-collisions. In EV battery disassembly, the architecture improves safety and efficiency by continuously adjusting sequences and trajectories to environmental feedback, enabling robust dual-robot cooperation with better time, resource, and reliability performance.\n\nThis work introduces a modular TAMP framework that integrates a perception-driven task scheduler with a GMM-informed RRT motion planner. The scheduler handles object sorting, task reasoning, and dual-arm task allocation, while the motion planner exploits demonstrations for guided sampling and enables responsive replanning for collision avoidance and dynamic adaptation. Due to its modular design, the framework is scalable to different robot setups and adaptable to varying execution costs and optimisation criteria.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现多机器人在动态环境中对电动车电池拆解的有效协调与规划。  \n2. 如何在复杂的任务和运动规划中保证安全性和效率。  \n3. 如何结合视觉感知与运动规划以应对环境动态变化。  \n\n【用了什么创新方法】  \n本研究提出了一种四层次的任务与运动规划（TAMP）框架，结合了符号任务规划和基于成本与可达性的分配。通过集成YOLOv8的立体视觉实现实时组件定位，使用OctoMap进行3D映射，并在MoveIt中应用FCL进行碰撞检测。该方法在两个UR10e机器人上验证，显著减少了末端执行器路径长度和任务完成时间，同时提高了多机器人协作的自主性、精确性和安全性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation",
            "authors": "Konstantin Gubernatorov,Artem Voronov,Roman Voronov,Sergei Pasynkov,Stepan Perminov,Ziang Guo,Dzmitry Tsetserukou",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21006",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21006",
            "arxiv_html_link": "https://arxiv.org/html/2509.21006v1",
            "abstract": "We address natural language pick-and-place in unseen, unpredictable indoor environments with AnywhereVLA, a modular framework for mobile manipulation. A user text prompt serves as an entry point and is parsed into a structured task graph that conditions classical SLAM with LiDAR and cameras, metric semantic mapping, and a task-aware frontier exploration policy. An approach planner then selects visibility and reachability aware pre grasp base poses. For interaction, a compact SmolVLA manipulation head is fine tuned on platform pick and place trajectories for the SO-101 by TheRobotStudio, grounding local visual context and sub-goals into grasp and place proposals. The full system runs fully onboard on consumer-level hardware, with Jetson Orin NX for perception and VLA and an Intel NUC for SLAM, exploration, and control, sustaining real-time operation. We evaluated AnywhereVLA in a multi-room lab under static scenes and normal human motion. In this setting, the system achieves a 46%46\\% overall task success rate while maintaining throughput on embedded compute. By combining a classical stack with a fine-tuned VLA manipulation, the system inherits the reliability of geometry-based navigation with the agility and task generalization of language-conditioned manipulation. All code, models, and datasets are open source and are available on the project GitHub repository.",
            "introduction": "Mobile manipulation is accelerating beyond limited indoor workcells towards large unstructured environments, in which robots need to explore unfamiliar cluttered spaces and physically interact with diverse objects and people. The execution of complex mobile manipulation tasks conditional on natural language instructions has gained attention in recent years in the field of service robotics[1]. Research on intelligent robotic systems in fields such as household service [2, 3], retail automation [4, 5], warehouse logistics [6], and manufacturing [7] has gained popularity, highlighting the importance of developing mobile manipulation solutions capable of operating in large-scale and open-plan indoor environments. Recent studies have increasingly focused on natural language processing to enable robots to interpret human instructions and facilitate intuitive task specification [8], positioning language-guided manipulation as a key approach for effective human-robot collaboration [9]. However, unifying language-based control, environment exploration, and manipulation in expansive environments presents a significant challenge [10].\n\nVision-language-action (VLA) models show strong generalization in various mobile manipulation tasks [11, 12], enabling robots to perform complex operations integrating perception, language, and control. Despite these advancements, several critical limitations persist for end-to-end control use for mobile robots. Most VLA models are confined to specific tasks and have limited spatial awareness [11, 12, 13], restricting their operational scope to localized settings and impeding their ability to navigate or manipulate objects in unseen or occluded regions of larger indoor spaces.\n\nVision-Language Navigation (VLN) approaches [14] present an end-to-end VLM-based framework that navigates building-wide environments while manipulating previously unseen household objects. However, [14], as all VLN models, require instructions about the location of the target object within the environment, which is often impractical in dynamic or unexplored settings. In contrast, classical navigation stacks [15] provide robust solutions for mapping and environment exploration, enabling robots to systematically traverse and model unknown spaces. However, these traditional systems lack the advanced language comprehension and semantic reasoning capabilities necessary to interpret complex instructions or contextual cues [8], limiting their ability to perform goal-directed tasks that require understanding natural language or high-level semantic objectives.\n\nIn this paper we introduce AnywhereVLA, a novel modular architecture for large-scale indoor mobile manipulation, addressing the limitations of existing Vision-Language-Action (VLA) models that adapt pretrained vision-language models (VLMs) to enable natural language-driven perception and control, but are often constrained to room-scale environments due to high computational demands. Drawing from advancements in VLA paradigms, AnywhereVLA integrates the rich visual and linguistic knowledge encoded in VLMs—leveraged through co-fine-tuning on robotic trajectory data and Internet-scale vision-language tasks—with the robust traversability afforded by classical navigation stacks, representing robot actions as tokenized sequences to facilitate end-to-end control and emergent semantic reasoning.\n\nAnywhereVLA is a pipeline for large-scale indoor mobile manipulation in unseen environments. As shown in Fig. 1, AnywhereVLA combines robust traversability of classical navigation algorithms and simultaneous localization and mapping (SLAM) with the generalizable scene understanding and task grounding of VLA models. Our pipeline translates high-level language instructions into low-level control commands by generating actions via VLA model for task-specific manipulation and computes navigation trajectories via language-conditioned exploration algorithm, directly actuating the wheels of the mobile base and the joints of the manipulator. Our main contribution is the following:\n\nWe propose a cohesive modular framework that accepts a single language-based task instruction as input, which conditions the environment exploration and navigation modules and simultaneously drives the VLA model for manipulation task execution. Our system achieves real-time performance exceeding 10 Hz across all modules that are deployed on consumer-available edge computing units, ensuring efficient and responsive operation in dynamic settings.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在未知和不可预测的室内环境中实现自然语言驱动的移动操控。  \n2. 如何将语言理解与环境探索和操控任务统一在大规模室内环境中。  \n3. 现有的视觉-语言-动作模型在空间意识和操作范围上的限制。  \n\n【用了什么创新方法】  \nAnywhereVLA是一个模块化框架，通过解析用户的文本提示生成结构化任务图，结合经典的SLAM、语义映射和任务感知的探索策略。该系统使用VLA模型生成低级控制命令，并通过语言条件探索算法计算导航轨迹，确保实时性能超过10Hz。通过将经典导航算法与VLA模型的场景理解相结合，AnywhereVLA在动态环境中实现了46%的整体任务成功率，展示了语言驱动操控的灵活性和几何导航的可靠性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Autoregressive End-to-End Planning with Time-Invariant Spatial Alignment and Multi-Objective Policy Refinement",
            "authors": "Jianbo Zhao,Taiyu Ban,Xiangjie Li,Xingtai Gui,Hangning Zhou,Lei Liu,Hongwei Zhao,Bin Li",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20938",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20938",
            "arxiv_html_link": "https://arxiv.org/html/2509.20938v1",
            "abstract": "The inherent sequential modeling capabilities of autoregressive models make them a formidable baseline for end-to-end planning in autonomous driving. Nevertheless, their performance is constrained by a spatio-temporal misalignment, as the planner must condition future actions on past sensory data. This creates an inconsistent worldview, limiting the upper bound of performance for an otherwise powerful approach.\nTo address this, we propose a Time-Invariant Spatial Alignment (TISA) module that learns to project initial environmental features into a consistent ego-centric frame for each future time step, effectively correcting the agent’s worldview without explicit future scene prediction. In addition, we employ a kinematic action prediction head (i.e., acceleration and yaw rate) to ensure physically feasible trajectories. Finally, we introduce a multi-objective post-training stage using Direct Preference Optimization (DPO) to move beyond pure imitation. Our approach provides targeted feedback on specific driving behaviors, offering a more fine-grained learning signal than the single, overall objective used in standard DPO. Our model achieves a state-of-the-art 89.8 PDMS on the NAVSIM dataset among autoregressive models. The video document is available at https://tisa-dpo-e2e.github.io/.",
            "introduction": "End-to-end autonomous driving aims to learn a driving policy directly from raw sensor inputs [1], bypassing the cascaded perception and planning modules of traditional frameworks and their associated cumulative errors [2]. Prominent end-to-end approaches include trajectory regression [3], autoregressive models [4], and diffusion models [5]. Regression methods are known to suffer from the mode averaging issue [6]. The other methods are generative approaches and can capture multi-modal driving behaviors. Among them, autoregressive models offer unique flexibility by explicitly modeling the action probability distribution, facilitating straightforward integration with techniques like reinforcement learning [7].\n\nHowever, autoregressive end-to-end planning presents a unique challenge: maintaining a consistent worldview for future steps. In modular, two-stage pipelines, this is often resolved by predicting future environmental states, which is a tractable task when operating on lightweight, structured data [8]. This allows the planner to act within a consistently updated, inferred world [6].\nThis strategy, however, is intractable in the end-to-end paradigm where predicting future high-dimensional sensor data is computationally infeasible. Consequently, end-to-end models typically base their entire plan on merely past snapshots of the world [4]. This creates a critical spatio-temporal misalignment: the agent plans for a future step t+kt+k while perceiving the world from a stale viewpoint at time tt. This issue is illustrated in Fig. 1.\n\nTo resolve this spatio-temporal misalignment, we propose a novel alignment mechanism in the latent space. Our approach is motivated by the observation that an ego-view update, which consists of rotation and translation, can be modeled as a time-invariant spatial transformation. We hypothesize that this transformation can be learned and applied within a latent space to align environmental features with future ego states. To realize this, we introduce the Time-Invariant Spatial Alignment (TISA) module. For each future time step, TISA processes a query that fuses static environmental features with the ego-vehicle’s prospective state. In response, the module outputs a spatially-aligned environmental context, ensuring that the model’s subsequent action prediction is always conditioned on a geometrically consistent and relevant worldview.\n\nMoreover, we introduce two additional components to enhance the realism and quality of the learned driving policy.\nFirst, instead of predicting spatial waypoints, our model outputs discretized kinematic actions (i.e., acceleration and yaw rate). This approach not only creates a more compact action space but also ensures that the generated trajectories are inherently physically feasible. To move beyond the limitations of simple imitation learning, we introduce a multi-objective post-training stage using DPO [9]. We construct a preference dataset by generating diverse trajectories and labeling them using multiple safety-focused metrics111Our fine-tuning prioritizes safety, as the model’s kinematic action space already produces trajectories that score highly on comfort metrics.. Fine-tuning on this data provides our model with targeted feedback on distinct driving aspects, offering a more fine-grained and effective learning signal than the single, overall objective used in standard DPO.\n\nWithout bells and whistles, our model achieves a state-of-the-art 89.8 PDMS on the NAVSIM dataset, using the same backbone as competing methods. Comprehensive ablation studies validate the effectiveness of both the TISA module and the multi-objective post-training process in significantly advancing end-to-end planning capabilities.\n\nOur contributions are three-fold:\n\nWe formalize and address the spatio-temporal misalignment problem in autoregressive end-to-end planning, a critical issue caused by conditioning future actions on a stale worldview.\n\nWe propose a novel and efficient TISA module that performs view transformations in the latent space, significantly improving planning performance without notable computational overhead.\n\nWe introduce a multi-objective DPO strategy that uses targeted, fine-grained preference pairs to refine the driving policy, demonstrably outperforming the standard single-objective DPO baseline.\n\n1. We formalize and address the spatio-temporal misalignment problem in autoregressive end-to-end planning, a critical issue caused by conditioning future actions on a stale worldview.\n\n2. We propose a novel and efficient TISA module that performs view transformations in the latent space, significantly improving planning performance without notable computational overhead.\n\n3. We introduce a multi-objective DPO strategy that uses targeted, fine-grained preference pairs to refine the driving policy, demonstrably outperforming the standard single-objective DPO baseline.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何解决自回归端到端规划中由于过时的感知数据导致的时空不一致问题。  \n2. 如何在不增加计算开销的情况下，提高自回归模型的规划性能。  \n3. 如何通过多目标优化策略提升驾驶策略的学习效果。  \n\n【用了什么创新方法】  \n提出了时间不变空间对齐（TISA）模块，通过在潜在空间中进行视图变换，解决了自回归端到端规划中的时空不一致问题。该模块为每个未来时间步生成空间对齐的环境上下文，从而确保模型的动作预测基于一致的世界观。此外，模型输出离散化的运动学动作，确保生成的轨迹物理可行。通过引入多目标后训练阶段，利用直接偏好优化（DPO）策略，提供针对特定驾驶行为的反馈，显著提升了学习信号的细致度。最终，该模型在NAVSIM数据集上达到了89.8的PDMS，展示了其在自回归模型中的领先性能。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "MTRDrive: Memory-Tool Synergistic Reasoning for Robust Autonomous Driving in Corner Cases",
            "authors": "Ziang Luo,Kangan Qian,Jiahua Wang,Yuechen Luo,Jinyu Miao,Zheng Fu,Yunlong Wang,Sicong Jiang,Zilin Huang,Yifei Hu,Yuhao Yang,Hao Ye,Mengmeng Yang,Xiaojian Dong,Kun Jiang,Diange Yang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20843",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20843",
            "arxiv_html_link": "https://arxiv.org/html/2509.20843v1",
            "abstract": "Vision-Language Models(VLMs) have demonstrated significant potential for end-to-end autonomous driving, yet a substantial gap remains between their current capabilities and the reliability necessary for real-world deployment. A critical challenge is their fragility, characterized by hallucinations and poor generalization in out-of-distribution (OOD) scenarios.\nTo bridge this gap, we introduce MTRDrive, a novel framework that integrates procedural driving experiences with a dynamic toolkit to enhance generalization and proactive decision-making.",
            "introduction": "The emergence of Vision-Language Models (VLMs) [1] has propelled the paradigm of end-to-end autonomous driving forward, building a single, cohesive system that mirrors human cognitive processes. Unlike traditional modular pipelines that separate perception[2, 3], prediction[4], and decision-making[5, 6], VLMs break down these divisions by jointly modeling multi-modal inputs and generating holistic driving responses in a unified framework.\nRecent impressive demonstrations show that these models, often enhanced by techniques like Chain-of-Thought (CoT), can achieve strong performance across various tasks, from perception-based Visual Question Answering (VQA) [7, 8, 9, 10] to intricate motion planning [11, 12, 13].\n\nDespite these advances, a significant gap remains between current VLM performance and the reliability required for real-world deployment: The models are inherently fragile, often exhibiting visual hallucinations and failing in out-of-distribution (OOD) scenarios [7, 14]. In autonomous driving, a field where even a minor error can lead to catastrophic outcomes, such limitations present a critical barrier to adoption[15, 16].\n\nFundamentally, a robust driving decision is influenced by two key factors: the accuracy of perception and the soundness of reasoning. Human cognition itself can be viewed as a lifelong feedback loop:\n\nInspired by this human-like process, we propose MTRDrive, a novel framework built on the principle of Interactive Reasoning to address the limitations of current VLM-based driving agents. Unlike traditional methods that treat each input as a one-shot decision, MTRDrive endows the agent with the ability to proactively retrieve driveing experience and employ tools to query its environment. As vividly demonstrated in Figure 1, by shifting from a static decision-making model to a dynamic, interactive one, our approach mitigates the risk of hallucinations and dramatically improves performance in novel, unseen scenarios.\n\nIn summary, our key contributions are:\n\nWe introduce MTRDrive, a novel framework that models autonomous driving as a dynamic, interactive process, moving beyond static, one-shot decision-making paradigms.\n\nWe introduce MTRDrive, a novel framework that models autonomous driving as a dynamic, interactive process, moving beyond static, one-shot decision-making paradigms.\n\nWe propose a memory-tool synergy mechanism that enables the agent to utilize both a knowledge base of past driving experiences and active toolkits for real-time information retrieval.\n\nWe demonstrate that our approach effectively mitigates visual hallucinations and improves generalization in challenging out-of-distribution scenarios, which is critical for real-world deployment.\n\n1. We introduce MTRDrive, a novel framework that models autonomous driving as a dynamic, interactive process, moving beyond static, one-shot decision-making paradigms.\n\n2. We propose a memory-tool synergy mechanism that enables the agent to utilize both a knowledge base of past driving experiences and active toolkits for real-time information retrieval.\n\n3. We demonstrate that our approach effectively mitigates visual hallucinations and improves generalization in challenging out-of-distribution scenarios, which is critical for real-world deployment.",
            "llm_summary": "【关注的是什么问题】  \n1. VLMs在自动驾驶中的脆弱性，尤其是在OOD场景中的表现不佳。  \n2. 如何增强自动驾驶模型的泛化能力和主动决策能力。  \n3. 传统方法的静态决策模型无法应对复杂的驾驶环境。  \n\n【用了什么创新方法】  \nMTRDrive框架通过整合程序化驾驶经验与动态工具，构建了一个互动推理的模型，旨在提升自动驾驶的可靠性。该方法允许代理主动检索驾驶经验，并使用工具实时查询环境，从而减少视觉幻觉的风险，并显著改善在新颖、未见场景中的表现。最终，该框架有效提升了自动驾驶在复杂情况下的决策能力，推动了VLMs在现实世界应用的可行性。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation",
            "authors": "Dekun Lu,Wei Gao,Kui Jia",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "First two authors contribute equally. Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.20841",
            "code": "https://sites.google.com/view/imaginationpolicy",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20841",
            "arxiv_html_link": "https://arxiv.org/html/2509.20841v1",
            "abstract": "End-to-end robot manipulation policies offer significant potential for enabling embodied agents to understand and interact with the world. Unlike traditional modular pipelines, end-to-end learning mitigates key limitations such as information loss between modules and feature misalignment caused by isolated optimization targets. Despite these advantages, existing end-to-end neural networks for robotic manipulation—including those based on large vision-language-action (VLA) models—remain insufficiently performant for large-scale practical deployment. In this paper, we take a step towards an end-to-end manipulation policy that is generalizable, accurate and reliable.\nTo achieve this goal, we propose a novel Chain of Moving Oriented Keypoints (CoMOK) formulation for robotic manipulation. Our formulation is used as the action representation of a neural policy, which can be trained in an end-to-end fashion (details in Sec. III). Such an action representation is general, as it extends the standard end-effector pose action representation and supports a diverse set of manipulation tasks in a unified manner.\nThe oriented keypoint in our method enables natural generalization to objects with different shapes and sizes, while achieving sub-centimeter accuracy.\nMoreover, our formulation can easily handle multi-stage tasks, multi-modal robot behaviors, and deformable objects.\nExtensive simulated and hardware experiments demonstrate the effectiveness of our method.\nVideo demonstration, source code and supplemental materials are available on https://sites.google.com/view/imaginationpolicy",
            "introduction": "During the past decade, deep learning has profoundly reshaped the landscape of robotic manipulation. One popular approach leverages deep neural networks (DNNs) as functional submodules in traditional modularized pipelines. This significantly enhances the capabilities of robot manipulators, particularly in robot perception [46, 37, 31]. Concurrently, a distinct paradigm has emerged, pursuing end-to-end, pixel-to-torque learning strategies [4, 16, 48]. This alternative methodology aims to circumvent the information bottleneck between discrete modules in traditional pipelines by directly mapping raw sensory input to low-level control actions (e.g., robot joint torque commands).\n\nThe success of Large Language Models (LLMs) [1, 2, 36] and Vision-Language Models (VLMs) [40, 21, 20] over the past years has injected new inspiration into robotic manipulation research. A key insight driving this interest is the promising generalization capability of LLMs/VLMs, benefits from large-scale, multiple-task pretraining. Motivated by this potential, researchers are increasingly exploring Vision-Language-Action (VLAs) models [14, 3, 22] – aiming to imbue robots with similar generalization capability. For robotic manipulation, this desired generalization manifests primarily in two critical dimensions: 1) generalization across scenes and objects (robustly handling novel objects with variations on shape, size, and appearance), and 2) generalization across diverse manipulation tasks (adapting to new instructions and tasks without expensive retraining).\n\nDespite significant research advances, current VLA models have not yet seen large-scale deployment in real-world industrial or service applications. Their performance in key operational perspectives, such as reliability and accuracy, can be insufficient for practical deployment and sometimes inferior to well-engineered traditional modularized pipelines. This shortcoming becomes particularly pronounced when these VLAs are deployed in unseen objects or different robot platforms. Consequently, bridging the gap between the potential for broad generalization and the practical demands of real-world performance remains a central challenge.\n\nIn this paper, we propose an alternative approach for end-to-end robot manipulation that emphasizes reliability, accuracy, and interpretability.\nTo achieve this, we propose a novel affordance-based formulation for robotic manipulation, where we define affordance as task-specific, semantically meaningful local object parts.\nTo translate this cognitive concept into executable robotic actions, we propose to represent affordance as semantic, task-specific oriented keypoint(s). Subsequently, the manipulation behavior can be represented as a Chain of Moving Oriented Keypoints (CoMOK), as detailed in Sec. III.\nThis affordance-base formulation is used as the action representation of a neural network based manipulation policy, which can be trained end-to-end.\nThis action representation is general, as it reduces to the standard end-effector pose action in a special case. The proposed formulation provides a unified framework for diverse manipulation tasks, including ones previously studied in isolation (such as robot grasping algorithms).\nThe oriented keypoint affordance enables natural generalization to objects with different shape and size, while achieving sub-centimeter level accuracy. Moreover, the proposed formulation can easily handle multi-stage tasks, multi-modality robot behaviors and deformable objects.\n\nWe also design a new neural network architecture based on the proposed formulation. The network takes sensory observation and task encoding (e.g., language prompt) as inputs. It uses score-matching [38, 4] (a variant of diffusion model) to produce the multi-modal robot actions. The network can be trained end-to-end and the output can be directly used for joint-level motion generation. This joint-level motion generator can be either 1) a traditional motion planning algorithms; or 2) a neural-network based trajectory generation pipeline (which can be trained end-to-end). We attempt both types of motion generators on simulated and real-world experiments. Our network demonstrates promising generalization capability without sacrificing accuracy/reliability.\n\nThis paper is organized as follows: in Sec. II we review related works. Sec. III describes our manipulation formulation and several important extensions. Sec. IV introduces the diffusion-based neural network to address multi-modal, multi-stage manipulation tasks. Sec. V demonstrates our methods on several simulated and real world tasks and shows generalization of our method. Sec. VI concludes.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现通用、精确且可靠的端到端机器人操控策略。  \n2. 当前基于视觉-语言-动作（VLA）模型的机器人操控在实际应用中的表现不足。  \n3. 如何在多阶段任务和多模态机器人行为中保持高效的操控能力。  \n\n【用了什么创新方法】  \n提出了一种新的“移动定向关键点链”（CoMOK）形式作为机器人操控的动作表示，能够在端到端的方式下进行训练。该方法通过定义任务特定的语义关键点，促进了对不同形状和大小物体的自然泛化，同时实现了亚厘米级的精度。此外，设计了一种新的神经网络架构，结合感知观察和任务编码，使用分数匹配生成多模态机器人动作。通过广泛的模拟和硬件实验，验证了该方法在多样化操控任务中的有效性和可靠性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "SemSight: Probabilistic Bird's-Eye-View Prediction of Multi-Level Scene Semantics for Navigation",
            "authors": "Jiaxuan He,Jiamei Ren,Chongshang Yan,Wenjie Song",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20839",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20839",
            "arxiv_html_link": "https://arxiv.org/html/2509.20839v1",
            "abstract": "In target-driven navigation and autonomous exploration, reasonable prediction of unknown regions is crucial for efficient navigation and environment understanding. Existing methods mostly focus on single objects or geometric occupancy maps, lacking the ability to model room-level semantic structures. We propose SemSight, a probabilistic bird’s-eye-view prediction model for multi-level scene semantics. The model jointly infers structural layouts, global scene context, and target area distributions, completing semantic maps of unexplored areas while estimating probability maps for target categories. To train SemSight, we simulate frontier-driven exploration on 2,000 indoor layout graphs, constructing a diverse dataset of 40,000 sequential egocentric observations paired with complete semantic maps. We adopt an encoder–decoder network as the core architecture and introduce a mask-constrained supervision strategy. This strategy applies a binary mask of unexplored areas so that supervision focuses only on unknown regions, forcing the model to infer semantic structures from the observed context. Experimental results show that SemSight improves prediction performance for key functional categories in unexplored regions and outperforms non-mask-supervised approaches on metrics such as Structural Consistency (SC) and Region Recognition Accuracy (PA). It also enhances navigation efficiency in closed-loop simulations, reducing the number of search steps when guiding robots toward target areas.",
            "introduction": "Autonomous robot navigation requires not only local perception but also the ability to predict unobserved areas. However, occlusions and limited sensing range often force robots to rely on egocentric partial observations. As a result, they lack a holistic understanding of the environment’s structural and semantic layout. Critical semantic structures in unobserved regions (e.g., corridors or doorways) have a notable impact on the efficiency of navigation. Therefore, predicting unknown areas from partial observations is a key challenge. The goal is to infer potential traversable structures and semantic distributions that support efficient exploration and navigation.\n\nRecent studies have explored scene reasoning and map prediction, employing learning-based approaches to infer unknown regions from partial observations. Most existing work focuses on geometric occupancy completion[2, 3, 4, 5, 1] and fails to address higher-level semantic reasoning. For example, such methods cannot answer questions like “where is the bedroom or kitchen likely to be?”. Other approaches target specific semantic categories[6, 7, 8, 9, 10] but cannot capture semantic regularities across large spatial extents. As a result, their predictions are often fragmented and localized, without providing room-level semantic priors. High-level semantic structures play an important role in autonomous navigation. For instance, in target-driven tasks, a robot that anticipates “an unknown region is likely a bedroom” or “a corridor connects to other rooms” can plan paths more efficiently and accomplish navigation more effectively. Such semantic prediction capability provides a valuable prior for task-level reasoning. Compared with robots that rely solely on immediate perception, robots capable of scene-structural reasoning and semantic prediction gain the ability to “imagine the unseen world.”\n\nMotivated by this, we propose SemSight, a probabilistic semantic foresight model. It is designed to predict the global semantic structure of an environment from partial observations, rather than being limited to local occupancy estimation. SemSight takes sparse exploration observations and target categories as input, predicts probabilities associated with target-related structures, and infers room-level semantic layouts and structural relations. By formulating semantic map prediction as a probabilistic inference problem, we directly support structured reasoning for navigation decisions, going beyond pure geometric reconstruction.\n\nTo train the SemSight model, we construct a semantic prediction dataset under the BEV representation. Unlike previous methods relying on static annotations, we simulate boundary-driven exploration processes on a large collection of floorplans, This process dynamically collects egocentric observation sequences paired with complete semantic maps as training samples. This dataset covers diverse indoor layouts with rich room types and structural information, effectively supporting the learning of local-to-global semantic reasoning. In summary, the main contributions are as follows:\n\nSemSight model – a probabilistic prediction model that integrates structural layouts, global scene context, and target area distributions for navigation. It maps partial observations and structural target conditions to global semantic maps and target probability maps, supporting target-driven navigation and exploration.\n\nSemSight model – a probabilistic prediction model that integrates structural layouts, global scene context, and target area distributions for navigation. It maps partial observations and structural target conditions to global semantic maps and target probability maps, supporting target-driven navigation and exploration.\n\nDiverse boundary-driven exploration dataset – establishing 40,000 pairings of continuous partial observations and complete semantic maps, covering 2,000 varied indoor layouts. This provides diverse training data for semantic completion and prediction tasks.\n\nSemantic foresight with navigation impact – experiments demonstrate that our mask-constrained supervision enables SemSight to infer room-level layouts and probabilistic target area distributions in unexplored regions. Furthermore, navigation simulations show that incorporating SemSight predictions improves exploration efficiency and target finding success, confirming its practical value for autonomous navigation.\n\n1. SemSight model – a probabilistic prediction model that integrates structural layouts, global scene context, and target area distributions for navigation. It maps partial observations and structural target conditions to global semantic maps and target probability maps, supporting target-driven navigation and exploration.\n\n2. Diverse boundary-driven exploration dataset – establishing 40,000 pairings of continuous partial observations and complete semantic maps, covering 2,000 varied indoor layouts. This provides diverse training data for semantic completion and prediction tasks.\n\n3. Semantic foresight with navigation impact – experiments demonstrate that our mask-constrained supervision enables SemSight to infer room-level layouts and probabilistic target area distributions in unexplored regions. Furthermore, navigation simulations show that incorporating SemSight predictions improves exploration efficiency and target finding success, confirming its practical value for autonomous navigation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效预测未知区域以支持自主导航和环境理解。  \n2. 现有方法缺乏对房间级语义结构的建模能力。  \n3. 如何从部分观察中推断潜在可通行结构和语义分布。  \n\n【用了什么创新方法】  \n提出了SemSight模型，一种概率鸟瞰图预测模型，能够联合推断结构布局、全局场景上下文和目标区域分布。该模型通过掩码约束监督策略，专注于未知区域的推断，利用编码器-解码器网络架构进行训练。实验结果表明，SemSight在未探索区域的关键功能类别预测性能上有所提升，并在导航闭环模拟中提高了效率，减少了引导机器人到目标区域的搜索步骤。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning",
            "authors": "Gawon Lee(1),Daesol Cho(1),H. Jin Kim(1) ((1) Seoul National University)",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "Accepted for publication in the proceedings of the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "pdf_link": "https://arxiv.org/pdf/2509.20766",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20766",
            "arxiv_html_link": "https://arxiv.org/html/2509.20766v1",
            "abstract": "Multi-task reinforcement learning (MTRL) offers a promising approach to improve sample efficiency and generalization by training agents across multiple tasks, enabling knowledge sharing between them.\nHowever, applying MTRL to robotics remains challenging due to the high cost of collecting diverse task data.\nTo address this, we propose MT-Lévy, a novel exploration strategy that enhances sample efficiency in MTRL environments by combining behavior sharing across tasks with temporally extended exploration inspired by Lévy flight [1].\nMT-Lévy leverages policies trained on related tasks to guide exploration towards key states, while dynamically adjusting exploration levels based on task success ratios.\nThis approach enables more efficient state-space coverage, even in complex robotics environments.\nEmpirical results demonstrate that MT-Lévy significantly improves exploration and sample efficiency, supported by quantitative and qualitative analyses.\nAblation studies further highlight the contribution of each component, showing that combining behavior sharing with adaptive exploration strategies can significantly improve the practicality of MTRL in robotics applications.",
            "introduction": "Reinforcement learning (RL) aims to autonomously train an agent to solve complex control tasks in simulated and real-world environments [2].\nNumerous successful applications of RL have been demonstrated in domains such as board games [3], Atari games [4], and continuous control [5].\nHowever, applying RL to real-world tasks remains challenging because most RL algorithms require vast training data and are typically designed to solve only a single task.\nTo mitigate these issues of sample inefficiency and task specificity, multi-task RL (MTRL) has emerged as a promising research area [6, 7, 8].\nIn MTRL, an agent is trained on multiple tasks simultaneously, enabling it to leverage knowledge acquired from one task to benefit learning in others.\nThis approach enhances sample efficiency and leads to more generalizable knowledge across tasks [9, 10, 11, 12].\nDespite its benefits, implementing MTRL in robotics environments continues to encounter substantial hurdles.\nTypical MTRL setups require a diverse set of tasks, often demanding intricate experimental configurations—such as deploying fleets of robots [13] or employing automated reset mechanisms to collect data without human intervention [14].\nWhile these approaches simplify data collection by scaling up experiments and reducing manual effort, they do not directly address the core issue: the inherent sample inefficiency of current MTRL algorithms.\nFrom a robotics standpoint, reducing sample complexity is essential for making these algorithms practically viable.\nOne promising approach to improving sample efficiency is to adopt a more effective exploration mechanism [15, 16, 17].\nHowever, previous works have predominantly focused on exploration strategies for single-task environments, thus directly applying those strategies to MTRL settings can be inefficient and complex because they do not account for the presence of multiple tasks.\nThus, there is a growing need to develop an exploration method that is tailored to MTRL environments.\n\nTo address the data collection challenge by improving sample efficiency, we propose three components that enhance exploration.\nFirst, since tasks in MTRL commonly share features such as state/action spaces, behavior patterns, and reward structures, we propose leveraging policies trained on other tasks to facilitate exploration in a new task—a strategy known as behavior sharing [12, 18].\nAlthough these policies might not yield optimal behavior for the new task, they tend to exhibit purposeful actions that effectively guide exploration [19].\nSecond, executing these exploration policies in a temporally extended manner helps direct the agent toward states of interest [17, 20].\nIn our approach, we employ Lévy flight [1]—a pattern observed in natural phenomena such as food foraging, transportation, and microbial movement—to implement this extended temporal exploration.\nFinally, we utilize an exponential moving average of success ratios and track it to automatically adjust both the onset and duration of exploration.\nThis reduces the distribution mismatch between the data collection policy and the learned policy.\nBy integrating these components, we present MT-Lévy, which combines behavior sharing, temporally extended exploration using Lévy flight, and an automatic exploration adjustment mechanism based on success ratios.\nEvaluations on MTRL benchmarks demonstrate that (1) MT-Lévy discovers more key states that are closely related to solving tasks, and (2) it achieves higher sample efficiency and performance than single-task oriented exploration methods and previously proposed MTRL algorithms.\nIn summary, our key contributions are as follows:\n\nWe propose MT-Lévy, an exploration method that enhances sample efficiency in MTRL environments by integrating behavior sharing, Lévy flight-based temporally extended exploration, and an automatic exploration adjustment mechanism.\n\nWe empirically demonstrate that MT-Lévy facilitates more effective exploration by discovering key states, improves sample efficiency, and outperforms existing methods.\nAdditionally, we provide comprehensive ablation studies to analyze the contribution of each component.\n\n1. We propose MT-Lévy, an exploration method that enhances sample efficiency in MTRL environments by integrating behavior sharing, Lévy flight-based temporally extended exploration, and an automatic exploration adjustment mechanism.\n\n2. We empirically demonstrate that MT-Lévy facilitates more effective exploration by discovering key states, improves sample efficiency, and outperforms existing methods.\nAdditionally, we provide comprehensive ablation studies to analyze the contribution of each component.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在多任务强化学习（MTRL）中提高样本效率和泛化能力。  \n2. 如何有效收集多样化任务数据以应对机器人应用中的高成本问题。  \n3. 现有的探索策略在多任务环境中的适用性不足。  \n\n【用了什么创新方法】  \n提出MT-Lévy，一种新颖的探索策略，通过结合跨任务的行为共享与基于Lévy飞行的时间扩展探索，来增强MTRL环境中的样本效率。该方法利用已训练的相关任务策略引导探索，动态调整探索水平，确保更高效的状态空间覆盖。实验证明，MT-Lévy显著提高了探索和样本效率，发现了更多关键状态，并在MTRL基准测试中超越了现有方法。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "MASt3R-Fusion: Integrating Feed-Forward Visual Model with IMU, GNSS for High-Functionality SLAM",
            "authors": "Yuxuan Zhou,Xingxing Li,Shengyu Li,Zhuohao Yan,Chunxi Xia,Shaoquan Feng",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20757",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20757",
            "arxiv_html_link": "https://arxiv.org/html/2509.20757v1",
            "abstract": "Visual SLAM is a cornerstone technique in robotics, autonomous driving and extended reality (XR), yet classical systems often struggle with low-texture environments, scale ambiguity, and degraded performance under challenging visual conditions. Recent advancements in feed-forward neural network-based pointmap regression have demonstrated the potential to recover high-fidelity 3D scene geometry directly from images, leveraging learned spatial priors to overcome limitations of traditional multi-view geometry methods.\nHowever, the widely validated advantages of probabilistic\nmulti-sensor information fusion are often discarded in these\npipelines. In this work, we propose MASt3R-Fusion,\na multi-sensor-assisted visual SLAM framework that tightly integrates feed-forward\npointmap regression with complementary sensor information, including inertial\nmeasurements and GNSS data. The system introduces Sim(3)-based visual\nalignment constraints (in the Hessian form) into a universal metric-scale SE(3) factor graph for effective information fusion.\nA hierarchical factor graph design is developed, which allows both\nreal-time sliding-window optimization and global optimization with aggressive loop closures,\nenabling real-time pose tracking, metric-scale structure perception and globally consistent mapping.\nWe evaluate our approach on both public benchmarks and self-collected datasets,\ndemonstrating substantial improvements in accuracy and robustness over existing visual-centered multi-sensor SLAM systems.\nThe code will be released open-source to support reproducibility and further research111https://github.com/GREAT-WHU/MASt3R-Fusion.",
            "introduction": "Visual SLAM is a widely utilized technique in applications such as robotics, autonomous driving,\nand extended reality (XR)[1]. SLAM systems generally require comprehensive functionality\nto support diverse spatial tasks. Compared with 3D reconstruction, visual SLAM places a\nstronger emphasis on real-time pose estimation and scene structure perception. In contrast to visual odometry,\nSLAM systems aim to provide a more complete understanding of the environment while\nsimultaneously maintaining long-term global consistency.\n\nAlthough classical visual SLAM frameworks have achieved notable successes,\ntheir practical usability remains limited due to several critical factors.\nFirst, traditional visual SLAM methods based on sparse features or photometric consistency\noften fail to achieve complete spatial perception, particularly in low-texture\nenvironments. Second, the system performance tends to degrade significantly under challenging visual conditions.\nIt also remains an open problem how to achieve life-long consistency across time periods and under varying illumination conditions.\n\nTo overcome these limitations, the integration of deep learning/foundation models has\nemerged as an effective solution[2]. In particular, recent feed-forward neural\nnetwork-based approaches for visual geometry estimation (e.g., DUSt3R[3], MASt3R[4], VGGT[5]),\nhave attracted growing attention. These methods encode the images and decode the latents to directly\nrecover 2D-to-3D pointmaps and other dense information. By leveraging large-scale data\nto learn spatial priors, they alleviate many of the degeneracies encountered in traditional\nmulti-view stereo (MVS) and SLAM systems, which greatly inspire recent SLAM implementations[6, 7].\n\nDespite significant progress, vision-based methods still suffer from inherent\nlimitations, especially regarding scale consistency and performance degradation in visually\ndeprived environments. To address this, a promising direction is to integrate these novel visual paradigms\nwith complementary sensors,\nsuch as inertial measurement units (IMUs) and global navigation satellite system (GNSS),\nthus to exploit the respective strengths of traditional multi-sensor\nfusion and learning-based spatial priors.\n\nIn this work, we propose MASt3R-Fusion,\na framework that integrates multi-sensor information with the\nfeed-forward pointmap regression paradigm, facilitating high-functionality SLAM that supports real-time state estimation,\ncomplete-view structure perception and\nglobally consistent mapping.\n\nThe contributions of this work are listed as follows:\n\nWe propose a framework to fuse feed-forward pointmap regression with multi-sensor information in a tight way,\nbridging Sim(3)-based visual alignment constraints\nand metric-scale SE(3) states and factors.\n\nWe propose a framework to fuse feed-forward pointmap regression with multi-sensor information in a tight way,\nbridging Sim(3)-based visual alignment constraints\nand metric-scale SE(3) states and factors.\n\nBased on the feed-forward model, we develop a real-time visual-inertial SLAM system that supports metric-scale pose estimation and dense perception.\n\nA globally consistent SLAM system that leverages loop closure and GNSS information is developed, in which\ngeometry-based loop closure candidate filtering and full-information iterative optimization are utilized.\n\nBoth public and self-made datasets are employed to comprehensively evaluate the system performance.\n\nThe code is made open-source to benefit the community.",
            "llm_summary": "【关注的是什么问题】  \n1. 传统视觉SLAM在低纹理环境中的性能不足。  \n2. SLAM系统在挑战性视觉条件下的表现下降。  \n3. 如何实现长期一致性和跨时间段的环境理解。  \n\n【用了什么创新方法】  \n本研究提出了MASt3R-Fusion框架，通过将前馈点图回归与IMU和GNSS等多传感器信息紧密集成，克服了传统视觉SLAM的局限性。该系统引入了基于Sim(3)的视觉对齐约束，构建了一个通用的度量尺度SE(3)因子图，支持实时滑动窗口优化和全局优化，增强了姿态跟踪、结构感知和全局一致性映射的能力。通过在公共基准和自收集数据集上的评估，显示出相较于现有视觉中心多传感器SLAM系统的显著准确性和鲁棒性提升。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning",
            "authors": "Guoyang Zhao,Yudong Li,Weiqing Qi,Kai Zhang,Bonan Liu,Kai Chen,Haoang Li,Jun Ma",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20739",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20739",
            "arxiv_html_link": "https://arxiv.org/html/2509.20739v1",
            "abstract": "Conventional SLAM pipelines for legged robot navigation are fragile under rapid motion, calibration demands, and sensor drift, while offering limited semantic reasoning for task-driven exploration. To deal with these issues, we propose a vision-only, SLAM-free navigation framework that replaces dense geometry with semantic reasoning and lightweight topological representations. A hierarchical vision-language perception module fuses scene-level context with object-level cues for robust semantic inference. And a semantic-probabilistic topological map supports coarse-to-fine planning: LLM-based global reasoning for subgoal selection and vision-based local planning for obstacle avoidance. Integrated with reinforcement-learning locomotion controllers, the framework is deployable across diverse legged robot platforms. Experiments in simulation and real-world settings demonstrate consistent improvements in semantic accuracy, planning quality, and navigation success, while ablation studies further showcase the necessity of both hierarchical perception and fine local planning. This work introduces a new paradigm for SLAM-free, vision-language-driven navigation, shifting robotic exploration from geometry-centric mapping to semantics-driven decision making.",
            "introduction": "Autonomous exploration and navigation remain fundamental challenges for mobile robots in open and unstructured environments. These capabilities are critical for applications such as search-and-rescue, warehouse logistics, and environmental monitoring, where robots must explore unknown areas, interpret scene semantics, and reach task-relevant targets [1]. Conventional navigation pipelines typically rely on Simultaneous Localization and Mapping (SLAM) with LiDAR or multi-sensor fusion for pose estimation and dense metric map construction [2]. However, such geometry-driven methods are brittle under the rapid motion and ground impacts of legged robots, where visual odometry often suffers from drift and tracking loss [3]. In addition, multi-sensor calibration and the high computational cost of dense mapping hinder deployment in lightweight, camera-only systems. For many task-driven applications, building dense maps is unnecessary; what is needed is a compact, scalable representation that supports efficient exploration.\n\nTraditional SLAM-based navigation primarily focuses on geometric reconstruction and pose optimization, with limited support for semantic reasoning about the environment [4]. This limitation is especially critical for open-set object search and semantic exploration tasks, where the robot must not only localize itself but also reason about which regions are worth exploring next, which objects are relevant to its mission, and how to plan under uncertainty. Compared with dense maps, topological representations are more compact and lightweight, while still capturing the essential connectivity of the environment and naturally supporting the integration of semantic labels for long-horizon exploration.\n\nRecent advances in Vision-Language Models (VLMs) and Vision-Language Navigation (VLN) have opened new opportunities to endow robots with rich semantic understanding and language grounding [5]. These models can infer task-relevant object categories and scene descriptions from natural language instructions and visual inputs [6]. However, existing VLN approaches largely focus on perception and recognition, while overlooking the integration of global planning, decision-making, and fine-grained obstacle avoidance [7]. Furthermore, most prior works have been evaluated primarily in static or simulated environments and have not addressed the challenges of deploying such systems on real-world, heterogeneous legged platforms.\n\nTo address these challenges, we revisit the design of the robotic navigation stack from a system-level perspective and propose a purely visual, SLAM-free navigation framework (Fig. 1). Our framework relies exclusively on onboard cameras to achieve semantic-driven exploration and is designed with cross-platform deployability in mind. Specifically, we address four core challenges: (i) achieving robust perception and incremental exploration without explicit geometric SLAM; (ii) enabling robust semantic understanding in unstructured scenes; (iii) integrating global reasoning, local obstacle avoidance, and interpretable planning within a unified pipeline; and (iv) supporting rapid deployment and consistent performance across different types of legged robots.\n\nTo tackle these challenges, we propose a SLAM-Free visual navigation framework with hierarchical Vision-language perception and coarse-to-fine semantic topological planning, which integrates semantic perception, topological reasoning, and motion control into a unified architecture. Concretely, we design an adaptive hierarchical perception module that fuses scene-level and object-level outputs from VLMs to achieve robust semantic inference; construct a semantic-probabilistic topological map and perform coarse-to-fine planning via LLM-augmented global reasoning and vision-based local obstacle avoidance, replacing traditional geometry-based global path planning; and train reinforcement learning (RL)-based locomotion policies for different legged robot morphologies, enabling seamless integration and cross-platform deployment. Our main contributions are summarized as follows:\n\nWe present a purely visual, SLAM-free semantic exploration and navigation framework that addresses the fragility of SLAM on legged robots under rapid motion and sensor drift. The framework supports incremental target search in open-world environments while overcoming the limitations of geometry-centric approaches.\n\nWe present a purely visual, SLAM-free semantic exploration and navigation framework that addresses the fragility of SLAM on legged robots under rapid motion and sensor drift. The framework supports incremental target search in open-world environments while overcoming the limitations of geometry-centric approaches.\n\nWe design a hierarchical visual perception module with a lightweight adaptive fusion strategy that combines scene- and object-level VLM outputs for robust, context-aware understanding.\n\nWe construct a semantic-probabilistic topological map and propose a coarse-to-fine planning paradigm combining LLM-driven global reasoning with vision-based obstacle avoidance, serving as an alternative to SLAM.\n\nWe conduct comprehensive experiments in both simulation and real-world settings, showing gains in success rate, semantic accuracy, and path efficiency.\n\n1. We present a purely visual, SLAM-free semantic exploration and navigation framework that addresses the fragility of SLAM on legged robots under rapid motion and sensor drift. The framework supports incremental target search in open-world environments while overcoming the limitations of geometry-centric approaches.\n\n2. We design a hierarchical visual perception module with a lightweight adaptive fusion strategy that combines scene- and object-level VLM outputs for robust, context-aware understanding.\n\n3. We construct a semantic-probabilistic topological map and propose a coarse-to-fine planning paradigm combining LLM-driven global reasoning with vision-based obstacle avoidance, serving as an alternative to SLAM.\n\n4. We conduct comprehensive experiments in both simulation and real-world settings, showing gains in success rate, semantic accuracy, and path efficiency.",
            "llm_summary": "【关注的是什么问题】  \n1. 传统SLAM方法在快速运动和传感器漂移下的脆弱性。  \n2. 任务驱动探索中对语义推理的支持不足。  \n3. 如何实现无SLAM的视觉导航框架以提高灵活性。  \n4. 在不同类型的腿式机器人上实现快速部署和一致性能。  \n\n【用了什么创新方法】  \n提出了一种无SLAM的视觉导航框架，利用分层的视觉-语言感知模块融合场景和物体级信息，进行稳健的语义推理。构建了语义-概率拓扑图，并通过大型语言模型（LLM）驱动的全局推理和基于视觉的局部障碍物规避实现粗到细的规划。结合强化学习的运动控制策略，框架在多种腿式机器人平台上实现了跨平台部署。实验结果显示，该方法在语义准确性、规划质量和导航成功率上均有显著提升，展示了从几何中心映射转向语义驱动决策的新范式。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking",
            "authors": "Zhenguo Sun,Yibo Peng,Yuan Meng,Xukun Li,Bo-Sheng Huang,Zhenshan Bing,Xinlong Wang,Alois Knoll",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20717",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20717",
            "arxiv_html_link": "https://arxiv.org/html/2509.20717v1",
            "abstract": "Long-horizon, high-dynamic motion tracking on humanoids remains brittle because absolute joint commands cannot compensate model–plant mismatch, leading to error accumulation.\nWe propose RobotDancing, a simple, scalable framework that predicts residual joint targets to explicitly correct dynamics discrepancies.\nThe pipeline is end-to-end—training, sim-to-sim validation, and zero-shot sim-to-real—and uses a single-stage reinforcement learning (RL) setup with a unified observation, reward, and hyperparameter configuration.\nWe evaluate primarily on Unitree G1 with retargeted LAFAN1 dance sequences and validate transfer on H1/H1-2.\nRobotDancing can track multi-minute, high-energy behaviors (jumps, spins, cartwheels) and deploys zero-shot to hardware with high motion tracking quality.",
            "introduction": "Humanoid robots are increasingly expected to execute long-horizon, highly dynamic behaviors such as dance, where small tracking errors compound rapidly and destabilize control. A principal source of such drift is the mismatch between idealized reference trajectories and the robot’s true physics (actuation limits, friction, inertia, latency).\nSince the field of animation has shown incredible progress in whole body dynamic control of physics-based characters [1, 2, 3], precise and robust motion tracking has been one of the core challenges in humanoid robotics, particularly when transferring motions from simulation or captured human data to real-world robotic platforms.\n\nRecent advances in physics-based humanoid control [4, 5, 6, 7, 8, 9, 10, 11] have demonstrated the potential of learning-based controllers integrated with physical constraints. Nevertheless, most existing methods generally still predict absolute joint commands, which works well for short, quasi-cyclic skills but remains fragile on long, high-energy sequences, leaving untapped the potential of explicitly modeling and compensating for dynamic discrepancies.\n\nInspired by residual learning paradigms [12, 13], we propose a novel residual-action reinforcement learning (RL) framework RobotDancing for high-quality tracking of long high-dynamic dance motions. Unlike previous methods, our framework explicitly learns the dynamics discrepancy between the reference motion and the robot. Instead of issuing absolute commands, the policy outputs residual actions that correct the reference in joint space, thereby allocating model capacity to physics compensation rather than re-synthesizing the motion.\nThis strategy markedly reduces the accumulation of motion tracking errors and enables robust and accurate motion tracking over long horizons.\n\nOur preliminary experiments conducted on the Unitree G1 humanoid robot platform demonstrate notably superior tracking accuracy compared to absolute actions methods, while yielding high motion quality. Furthermore, we also validated our framework’s generalization capabilities across multiple humanoid robot platforms (Unitree H1 and H1-2).\n\nThe primary contributions of this paper include:\n\nA Simple, Open, and Scalable Motion-Tracking Framework.\nWe present an end-to-end pipeline—training, sim-to-sim validation, and sim-to-real deployment—that achieves long-horizon, high-dynamic tracking with a single-stage RL algorithm (no multi-stage distillation or teacher-student schemes [14]).\n\nResidual-Action Tracking Targets Dynamics Discrepancy.\nA residual-action policy corrects reference joints online, reducing error accumulation and improving stability on demanding sequences (e.g., dance).\n\nEffective Sampling for Long-Tail Motions.\nA two-part strategy (distribution-aware balancing + failure-aware prioritization) improves coverage of rare but informative poses and accelerates progress on difficult segments.\n\nGeneralization across Motions and Platforms.\nA unified set of observations, rewards, and hyperparameters trains diverse motions without per-task tuning, and transfers to multiple humanoid platforms including 2 full-size robots; to the best of our knowledge, this is one of the first demonstrations of high-dynamic, long-horizon motion tracking on full-size humanoids.\n\n1. A Simple, Open, and Scalable Motion-Tracking Framework.\nWe present an end-to-end pipeline—training, sim-to-sim validation, and sim-to-real deployment—that achieves long-horizon, high-dynamic tracking with a single-stage RL algorithm (no multi-stage distillation or teacher-student schemes [14]).\n\n2. Residual-Action Tracking Targets Dynamics Discrepancy.\nA residual-action policy corrects reference joints online, reducing error accumulation and improving stability on demanding sequences (e.g., dance).\n\n3. Effective Sampling for Long-Tail Motions.\nA two-part strategy (distribution-aware balancing + failure-aware prioritization) improves coverage of rare but informative poses and accelerates progress on difficult segments.\n\n4. Generalization across Motions and Platforms.\nA unified set of observations, rewards, and hyperparameters trains diverse motions without per-task tuning, and transfers to multiple humanoid platforms including 2 full-size robots; to the best of our knowledge, this is one of the first demonstrations of high-dynamic, long-horizon motion tracking on full-size humanoids.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现长时间、高动态的类人机器人运动跟踪？  \n2. 如何解决模型与实际植物之间的动态差异导致的跟踪误差积累？  \n3. 如何在不同类人机器人平台上实现运动的通用性和迁移能力？  \n\n【用了什么创新方法】  \n本研究提出了RobotDancing，一个简单且可扩展的框架，通过预测残差关节目标来显式校正动态差异。该流程为端到端，包括训练、sim-to-sim验证和零-shot sim-to-real，采用单阶段强化学习设置，使用统一的观察、奖励和超参数配置。实验表明，RobotDancing能够有效跟踪多分钟的高能行为（如跳跃、旋转、翻滚），并在硬件上实现高质量的运动跟踪，显著优于传统的绝对动作方法。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Digital Twin-Guided Robot Path Planning: A Beta-Bernoulli Fusion with Large Language Model as a Sensor",
            "authors": "Mani Amani,Reza Akhavian",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20709",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20709",
            "arxiv_html_link": "https://arxiv.org/html/2509.20709v1",
            "abstract": "Integrating natural language (NL) prompts into robotic mission planning has attracted significant interest in recent years. In the construction domain, Building Information Models (BIM) encapsulate rich NL descriptions of the environment. We present a novel framework that fuses NL directives with BIM–derived semantic maps via a Beta–Bernoulli Bayesian fusion by interpreting the LLM as a sensor: each obstacle’s design‐time repulsive coefficient is treated as a Beta​(α,β)\\mathrm{Beta}(\\alpha,\\beta) random variable and LLM‐returned “danger scores” are incorporated as pseudo‐counts to update α\\alpha and β\\beta. The resulting posterior mean yields a continuous, context‐aware repulsive gain that augments a Euclidean‐distance‐based potential field for cost heuristics. By adjusting gains based on sentiment and context inferred from user prompts, our method guides robots along safer, more context‐aware paths. This provides a numerically stable method that can chain multiple natural commands and prompts from construction workers and foreman to enable planning while giving flexibility to be integrated in any learned or classical AI framework. Simulation results demonstrate that this Beta–Bernoulli fusion yields both qualitative and quantitative improvements in path robustness and validity. Our code is publicly available on GitHub.",
            "introduction": "The emergence of large language models (LLMs) has fundamentally transformed natural language processing (NLP) capabilities across diverse domains, with robotic planning experiencing similar paradigm shifts through enhanced semantic understanding and contextual reasoning [Dai et al. (2024)][Hu and Zhou (2024)]. While there has been a concentrated focus on the advancement of prompted robotic applications, most of the research on language-commanded robotics is limited to predefined objects and primitives [Arenas et al. (2024)]. This restriction confines robots to a limited set of predefined actions. Sometimes, mission parameters must be adjusted and tuned given unseen and undefined prompts. Furthermore, it is well established that humans expect their collaborators, including AI, to interpret implicit cues beyond explicit wording [Liang et al. (2019)]. This creates a need for a more flexible framework to analyze implicit sentiment information from natural language prompts. Our proposed BIM–LLM Bayesian fusion framework, illustrated in Figure 1, addresses this gap.\nSince implicit and semantic information inherently depend on subjective human interpretation, a probabilistic framework provides a principled approach to quantify and propagate these uncertainties through the planning system. Specifically, the inherent ambiguity and context-dependency of natural language sentiment, combined with the possibility of LLM hallucinations, require a stable probabilistic approach. Therefore, leveraging deep learning models is considered a suitable approach since they have been shown to have high-accuracy sentiment analysis [Aslan (2023)]. LLMs have demonstrated extensive success in sentiment analysis, positioning them as attractive candidates for assessing Theory of Mind-like associations [Zhang et al. (2023)]. Theory of Mind refers to the ability to attribute mental states (like beliefs, desires, and intentions) to oneself and others. In this context, the ’Theory of Mind-like associations’ means that the system is designed to infer and relate implicit emotional or cognitive states from verbal communication and, more generally, observed behavior [Street (2024)]. While these advancements show promise in human-machine interaction, in robotics, the analysis must operate under strict mission constraints and heuristics, requiring that the sentiment data be transformed into actionable insights that can be adapted across various scenarios. This introduces additional complexity in transforming raw sentiment data into an actionable and generalizable format across diverse mission scenarios. Furthermore, for practical applications, it is essential to establish a general mapping between sentiment cues and environmental context to ensure accurate robot operations.\n\nSemantic maps have been extensively studied as a means to integrate qualitative and spatial information [Kostavelis and Gasteratos (2015)]. Typically, these maps assign a class label and descriptive textual information to spatial objects, enabling real-time map generation and planning. In many applied settings, there are existing data formats analogous to semantic maps. For example, BIM is a widely used format that represents both the three-dimensional geometry and associated textual information of a built environment [Cerovsek (2011)]. Given its dual representation of spatial and semantic data, BIM serves as an excellent analog to a semantic map. Indeed, previous research has successfully leveraged BIM to provide the semantic information for robot planning [Park et al. (2025)]. Furthermore, the integration of internet of things (IoT) sensors, real-time data streams, and machine learning has driven an evolution from static BIM representations toward dynamic Digital Twins that continuously update to reflect current site conditions, enabling more responsive robotic navigation in changing construction environments.\n\nPrevious works typically treat the LLM as the primary planning module, leveraging environmental information to generate semantically meaningful plans [Wang et al. (2024)].\nIn contrast, we propose an alternative interpretation: using the LLM as a sensor rather than a planner. In this framework, the LLM provides semantic information that can be embedded into the map or any intermediate representation used during planning. The planning module itself remains agnostic to the LLM, allowing for the use of any classical or learning-based planner in conjunction with the LLM-as-sensor approach. Although visual–language maps have shown great promise [Huang et al. (2023)], most prior work focuses on robot planning from direct commands. In contrast, our contribution demonstrates that LLMs, combined with probabilistic map updates, can account for contextual and implicit sentiments that are not explicitly expressed in the natural-language command.\n\nUsing NL as a cost signal for obstacle avoidance has been explored previously [Oh et al. (2025)]. Prior work shows that VLMs can identify dynamic obstacles and assign danger scores. We extend this line of work by incorporating implicit and contextual information about the environment and by enabling the chaining and consolidation of multiple commands, yielding more flexible and general language-guided costmaps. Semantic planning has also been studied in the realm of visual-language-action models (VLA). However, these models are often expensive to train and deploy. Furthermore, these models often do not have formal verification and guarantees regarding their performance [Sapkota et al. (2025)]. By incorporating NL data as sensor information that can augment costs, we can integrate NL with classical and certifiable AI algorithms such as A* for path-finding, which is commonly used in robotics [Halder et al. (2024)] [Dai et al. (2024)]. In the presented framework, the ideas proposed in studies such as [Dai et al. (2024)] are extended to risk shaping rather than task satisfaction, with BIM‑grounded semantics.\nOur contributions in this paper can be summarized as:\n\nWe propose a novel interpretation of LLMs as semantic sensors for robotic navigation, where natural language prompts about construction site conditions update obstacle danger coefficients through Beta-Bernoulli fusion. This approach leverages BIM family semantics to ground language understanding while maintaining map structure invariance.\n\nWe derive a closed-form Beta–Bernoulli fusion of prompt-derived danger scores into continuous repulsive gains, thereby formulating object avoidance as a repulsive potential-field cost metric.\n\nWe demonstrate that off-the-shelf LLMs correctly interpret domain-specific construction prompts (no domain fine-tuning needed), and that our fusion preserves the bounded-suboptimal guarantees of Multi-Heuristic A*.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将自然语言指令与建筑信息模型（BIM）结合以增强机器人路径规划的灵活性和安全性？  \n2. 如何处理自然语言中的隐含情感信息，以便在机器人导航中实现更好的环境感知？  \n3. 如何利用大语言模型（LLM）作为传感器来更新障碍物的危险系数，从而改进路径规划的鲁棒性？  \n\n【用了什么创新方法】  \n本研究提出了一种新颖的框架，将自然语言指令与BIM派生的语义地图通过Beta-Bernoulli贝叶斯融合进行结合。具体而言，LLM被视为传感器，通过将每个障碍物的设计时间排斥系数视为Beta随机变量，并将LLM返回的“危险分数”作为伪计数来更新这些系数。该方法生成的后验均值提供了一个连续的、上下文感知的排斥增益，增强了基于欧几里得距离的潜在场成本启发式。通过根据用户提示推断的情感和上下文调整增益，机器人能够沿着更安全、上下文感知的路径导航。模拟结果表明，Beta-Bernoulli融合在路径鲁棒性和有效性方面都取得了定性和定量的改善。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework",
            "authors": "Reza Akhavian,Mani Amani,Johannes Mootz,Robert Ashe,Behrad Beheshti",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20705",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20705",
            "arxiv_html_link": "https://arxiv.org/html/2509.20705v1",
            "abstract": "未获取到摘要",
            "introduction": "The adoption of cyber-physical systems and jobsite intelligence that connects design models, real-time site sensing, and autonomous field operations can dramatically enhance digital management in the Architecture, Engineering, and Construction (AEC) industry. This paper introduces BIM2RDT (Building Information Models to Robot-Ready Site Digital Twins), an agentic artificial intelligence (AI) framework designed to transform static Building Information Modeling (BIM) into dynamic, robot-ready digital twins (DTs) that prioritize safety during construction execution. The framework bridges the gap between pre-existing BIM data and real-time site conditions by integrating three key data streams: geometric and semantic information from BIM models, real-time activity data from IoT sensor networks, and visual-spatial data collected by quadruped robots during site traversal. The methodology introduces Semantic-Gravity ICP (SG-ICP), a novel point cloud registration algorithm that leverages large language model (LLM) reasoning. Unlike traditional methods, SG-ICP utilizes an LLM to infer object-specific, physically-plausible orientation priors based on BIM semantics, significantly improving alignment accuracy by avoiding convergence on local minima. This creates an intelligent feedback loop where robot-collected data updates the DT, which in turn optimizes paths for subsequent missions. The framework employs YOLOE open-vocabulary object detection and Shi-Tomasi corner detection to identify and track construction elements while using BIM geometry as robust a priori maps. The framework also integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping sensor-detected safety events to the digital twin using IFC standards for proactive intervention. Major findings from experiments demonstrate SG-ICP’s superiority over standard ICP, achieving RMSE reductions of 64.3%–88.3% in alignment across varied scenarios with occluded or sparse features, ensuring physically plausible orientations. HAV integration triggers real-time warnings and tasks upon exceeding exposure limits, enhancing compliance with such standards as ISO 5349-1.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将静态建筑信息模型（BIM）转化为动态、机器人就绪的数字双胞胎（DT）？  \n2. 如何在建筑施工中优先考虑安全性并实现实时数据集成？  \n3. 如何提高点云注册的准确性，避免传统方法中的局部最小值收敛问题？  \n\n【用了什么创新方法】  \n本研究提出了BIM2RDT框架，通过将几何和语义信息、实时活动数据及机器人收集的视觉空间数据整合，构建机器人就绪的数字双胞胎。采用了新颖的语义引力ICP（SG-ICP）算法，利用大语言模型推断物体特定的物理可行方向，从而显著提高了对齐精度。框架还集成了YOLOE开放词汇物体检测和Shi-Tomasi角点检测，以识别和跟踪施工元素，并使用BIM几何作为稳健的先验地图。此外，实时手臂振动监测与数字双胞胎相结合，能够在超出暴露限制时触发实时警告和任务，增强了安全合规性。实验结果表明，SG-ICP在多种场景下的对齐精度显著优于标准ICP，RMSE减少了64.3%-88.3%。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Joint Flow Trajectory Optimization For Feasible Robot Motion Generation from Video Demonstrations",
            "authors": "Xiaoxiang Dong,Matthew Johnson-Roberson,Weiming Zhi",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20703",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20703",
            "arxiv_html_link": "https://arxiv.org/html/2509.20703v1",
            "abstract": "Learning from human video demonstrations offers a scalable alternative to teleoperation or kinesthetic teaching, but poses challenges for robot manipulators due to embodiment differences and joint feasibility constraints. We address this problem by proposing the Joint Flow Trajectory Optimization (JFTO) framework for grasp pose generation and object trajectory imitation under the video-based Learning-from-Demonstration (LfD) paradigm. Rather than directly imitating human hand motions, our method treats demonstrations as object-centric guides, balancing three objectives: (i) selecting a feasible grasp pose, (ii) generating object trajectories consistent with demonstrated motions, and (iii) ensuring collision-free execution within robot kinematics. To capture the multimodal nature of demonstrations, we extend flow matching to 𝐒𝐄​(3)\\mathbf{SE}(3) for probabilistic modeling of object trajectories, enabling density-aware imitation that avoids mode collapse. The resulting optimization integrates grasp similarity, trajectory likelihood, and collision penalties into a unified differentiable objective. We validate our approach in both simulation and real-world experiments across diverse real-world manipulation tasks.",
            "introduction": "Generating feasible motions is a key challenge in robotics. Traditional approaches address this by formulating and solving optimization problems with carefully designed costs and constraints [1, 2, 3]. An alternative line of work focuses on enabling robots to imitate humans through Learning from Demonstration (LfD) [4, 5]. Historically, this has required collecting demonstrations through teleoperation or precise kinesthetic teaching. More recently, researchers have attempted to leverage videos of human interaction with objects as demonstrations [6, 7]. Although promising, this paradigm introduces challenges: Human arms and robot manipulators differ significantly in morphology, and direct imitation can cause robots to violate joint constraints when tracking human motions. All demonstrations in this paper refer to such human video demonstrations.\n\nIn this paper, we tackle the problem of generating grasp poses and motion trajectories for robot manipulators under the video-based LfD paradigm, where the tracked human motion may often not be kinesthetically feasible for a robot manipulator to execute. Our goal is to identify feasible grasp configurations and ensure that grasped objects move consistently with human video demonstrations. Concretely, given videos of humans interacting with objects, we propose the Joint Flow Trajectory Optimization (JFTO) framework, as shown in Fig.˜1, with three desiderata: (1) identifying a feasible grasp pose; (2) generating trajectories of the grasped object that align with object poses in the videos; (3) ensuring downstream robot motions remain collision-free and within joint limits. To this end, our formulation treats human video demonstrations as object-centric guides rather than strict motion references. By focusing on how objects are manipulated, rather than how the human hand is configured, we allow robots to synthesize grasps and trajectories that are consistent with observed outcomes, while respecting the robot’s own embodiment.\n\nUncertainty is critical to robotics tasks [8, 9, 10]. We take a probabilistic approach and model the distribution of object pose trajectories extracted from videos as a generative model Flow Matching [11] over the Special Euclidean Group 𝐒𝐄​(3)\\mathbf{SE}(3).\nHere, as the grasps and motions of the human from the video-based demonstrations may not be valid for robot execution, our goal is not only to ensure grasp and execution feasibility, but also to mimic the motion patterns of human demonstrations in the videos.\n\nConcretely, the technical contributions of this work are threefold:\n\nWe introduce the Joint Flow Trajectory Optimization (JFTO) framework that integrates grasp similarity, object trajectory likelihood, and collision avoidance into a unified differentiable objective for video-based LfD.\n\nWe extend flow matching to 𝐒𝐄​(3)\\mathbf{SE}(3) for probabilistic modeling of demonstrated object trajectories, enabling density-aware imitation that preserves multi-modality.\n\nWe demonstrate the effectiveness of JFTO in both simulation and real-world experiments on diverse manipulation tasks, showing that joint optimization achieves higher fidelity to demonstrations compared to sequential baselines.\n\n1. We introduce the Joint Flow Trajectory Optimization (JFTO) framework that integrates grasp similarity, object trajectory likelihood, and collision avoidance into a unified differentiable objective for video-based LfD.\n\n2. We extend flow matching to 𝐒𝐄​(3)\\mathbf{SE}(3) for probabilistic modeling of demonstrated object trajectories, enabling density-aware imitation that preserves multi-modality.\n\n3. We demonstrate the effectiveness of JFTO in both simulation and real-world experiments on diverse manipulation tasks, showing that joint optimization achieves higher fidelity to demonstrations compared to sequential baselines.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何从人类视频演示中生成机器人可行的抓取姿势和运动轨迹。  \n2. 如何在视频基础的学习中平衡抓取相似性、物体轨迹一致性和碰撞避免。  \n3. 如何处理人类与机器人在形态上的差异以避免关节约束的违反。  \n\n【用了什么创新方法】  \n提出了联合流轨迹优化（JFTO）框架，将抓取相似性、物体轨迹可能性和碰撞避免整合为一个统一的可微分目标。通过将流匹配扩展到SE(3)，实现了对演示物体轨迹的概率建模，从而支持密度感知的模仿，避免模式崩溃。该方法在模拟和真实世界实验中验证了其有效性，显示出与传统顺序基线相比，联合优化在演示的保真度上具有显著提升。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "RuN: Residual Policy for Natural Humanoid Locomotion",
            "authors": "Qingpeng Li,Chengrui Zhu,Yanming Wu,Xin Yuan,Zhen Zhang,Jian Yang,Yong Liu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20696",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20696",
            "arxiv_html_link": "https://arxiv.org/html/2509.20696v1",
            "abstract": "Enabling humanoid robots to achieve natural and dynamic locomotion across a wide range of speeds, including smooth transitions from walking to running, presents a significant challenge. Existing deep reinforcement learning methods typically require the policy to directly track a reference motion, forcing a single policy to simultaneously learn motion imitation, velocity tracking, and stability maintenance. To address this, we introduce RuN, a novel decoupled residual learning framework. RuN decomposes the control task by pairing a pre-trained Conditional Motion Generator, which provides a kinematically natural motion prior, with a reinforcement learning policy that learns a lightweight residual correction to handle dynamical interactions. Experiments in simulation and reality on the Unitree G1 humanoid robot demonstrate that RuN achieves stable, natural gaits and smooth walk–run transitions across a broad velocity range (0–2.5 m/s), outperforming state-of-the-art methods in both training efficiency and final performance.",
            "introduction": "Humanoid robots are expected to operate in human-centric environments across a broad spectrum of tasks. Among these capabilities, reliable locomotion is the most fundamental and central prerequisite. However, developing a unified locomotion controller that spans a wide range of speeds, from walking to running, and enables smooth transitions between these gaits remains a pivotal challenge.\n\nClassical control methods, such as those based on Zero-Moment Point (ZMP), can ensure stability but often produce rigid, robotic gaits ill-suited for high-speed dynamic tasks [1]. In parallel, Model Predictive Control (MPC) optimizes short-horizon plans under simplified dynamics and preset contact schedules due to real-time and modeling limits, which can restrict agility and robustness on humanoids [2]. In response, Deep Reinforcement Learning (DRL) has emerged as a powerful paradigm for learning complex motor skills end-to-end [3, 4]. However, the effectiveness of DRL depends on reward design, and even small misspecification can steer learning toward easy-to-exploit objectives, yielding efficient yet non-anthropomorphic motions rather than structured, human-like behavior.\n\nTo promote naturalness in DRL, researchers incorporate data-driven motion priors. Paradigms such as the Generative Motion Prior (GMP) [5] employ generative models to synthesize natural reference motions as priors, feed these priors directly into a reinforcement learning policy, and train the policy with imitation and task rewards to produce natural movements. However, this direct-tracking strategy creates a significant learning-complexity challenge in which a single policy must simultaneously satisfy three often competing objectives—imitating the kinematic style of the motion prior, maintaining dynamic stability, and executing task commands. This tight coupling exposes the policy to a large and complex action space, lengthening training time. Additionally, inherent conflicts among the objectives constrain performance and necessitate trade-offs among motion quality, stability, and task accuracy.\n\nTo address the inherent complexities in DRL, we propose RuN, a novel decoupled residual learning framework. The core idea of this framework is to decompose the complex end-to-end control task into two more tractable sub-modules. The first module, a Conditional Motion Generator (CMG), autoregressively generates high-quality kinematic reference trajectories from given velocity commands, which serve as a motion prior. The second module is a residual policy that learns a significantly simplified task, which is providing small and dynamic corrections to the motion prior to compensate for external perturbations. The final control command executed by the robot is the summation of the motion priors generated by the CMG and the residual action output by the policy. This residual formulation drastically narrows the policy’s exploration space, allowing it to focus its learning capacity on mastering complex dynamic interactions, thereby enabling our controller to achieve smooth transitions from walking to running.\n\nWe conduct extensive experiments on the Unitree G1 humanoid robot. The results demonstrate that our framework trains significantly faster and achieves superior final performance compared to state-of-the-art methods across all key metrics, including motion naturalness and velocity tracking error. Our robot successfully performs natural walking and running across a continuous velocity range of 0–2.5 m/s, with smooth gait transitions in the real world.\n\nOur contributions can be summarized as follows:\n\nA novel decoupled residual learning framework, RuN, that simplifies humanoid locomotion by decomposing the action into a motion prior and a learned residual, leading to significant gains in final performance.\n\nA novel decoupled residual learning framework, RuN, that simplifies humanoid locomotion by decomposing the action into a motion prior and a learned residual, leading to significant gains in final performance.\n\nA novel Conditional Motion Generator that autoregressively produces a range of natural motions from walking to running, serving as a high-quality motion prior for the residual policy.\n\nIn extensive real-world and simulation trials on the Unitree G1 humanoid robot, our controller, RuN, demonstrates superior performance over state-of-the-art methods regarding motion naturalness and velocity tracking.\n\n1. A novel decoupled residual learning framework, RuN, that simplifies humanoid locomotion by decomposing the action into a motion prior and a learned residual, leading to significant gains in final performance.\n\n2. A novel Conditional Motion Generator that autoregressively produces a range of natural motions from walking to running, serving as a high-quality motion prior for the residual policy.\n\n3. In extensive real-world and simulation trials on the Unitree G1 humanoid robot, our controller, RuN, demonstrates superior performance over state-of-the-art methods regarding motion naturalness and velocity tracking.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现自然且动态的人形机器人在多种速度下的运动，包括平滑的行走与奔跑过渡。  \n2. 现有深度强化学习方法在运动模仿、速度跟踪和稳定性维护之间的复杂性挑战。  \n3. 如何简化复杂的控制任务以提高训练效率和最终性能。  \n\n【用了什么创新方法】  \n提出了一种新颖的解耦残差学习框架RuN，通过将控制任务分解为两个模块：条件运动生成器（CMG）和残差策略。CMG生成高质量的运动先验，残差策略则学习动态修正以应对外部扰动。该方法显著缩小了策略的探索空间，使其能够专注于复杂动态交互的学习，从而实现平滑的行走与奔跑过渡。实验结果表明，RuN在训练效率和最终性能上均优于现有方法，成功实现了0-2.5 m/s速度范围内的自然行走和奔跑。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Incorporating Human-Inspired Ankle Characteristics in a Forced-Oscillation-Based Reduced-Order Model for Walking",
            "authors": "Chathura Semasinghe,Siavash Rezazadeh",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20689",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20689",
            "arxiv_html_link": "https://arxiv.org/html/2509.20689v1",
            "abstract": "This paper extends the forced-oscillation-based reduced-order model of walking to a model with ankles and feet. A human-inspired paradigm was designed for the ankle dynamics, which results in improved gait characteristics compared to the point-foot model. In addition, it was shown that while the proposed model can stabilize against large errors in initial conditions through combination of foot placement and ankle strategies, the model is able to stabilize against small perturbations without relying on the foot placement control and solely through the designed proprioceptive ankle scheme. This novel property, which is also observed in humans, can help in better understanding of anthropomorphic walking and its stabilization mechanisms.",
            "introduction": "Reduced-order models of legged systems aim to capture the fundamental traits of locomotion within a lower-dimensional framework. Locomotion control, gait generation, and stability remain key challenges in both biomechanical and robotic locomotion, and the complexity and nonlinear dynamics of these systems have motivated the use of simplified models. One of the oldest and most essential models of bipedal walking is the Inverted Pendulum Model (IPM), which has long been used for control and stability analyses of bipedal locomotion [1]. Building on this and by incorporating the Zero Moment Point (ZMP) concept proposed by Vukobratović and Juričić [2], Kajita et al. [3, 4] extended the IPM to the Linear Inverted Pendulum Model (LIPM), a simplified model widely used for humanoid gait generation and control. The LIPM is among the most prevalent templates for bipedal walking, as it enables real-time control through fast, convex trajectory generation of the point mass, which is also the center of mass (CoM) [4, 5]. However, the assumption of constant leg length constrains vertical CoM motion, allowing only horizontal dynamics. With this restriction, the vertical ground reaction force (GRF) remains effectively constant and equal to the weight, which prevents the reproduction of the human-like M-shaped vertical GRF profiles [6, 7]. These limitations raise concerns about the ability of the LIPM template to capture essential characteristics of anthropomorphic walking. Although the original IPM produces attributes more closely resembling human walking, notable differences remain, particularly in GRFs and CoM displacement [8]. More recently, by adding a force actuator to the IPM and applying trajectory optimization methods, these discrepancies have been addressed [9]; however, this extended model is primarily valuable for understanding biomechanical costs, while its application to robotic systems remains an open question.\n\nAnother widely adopted reduced-order model is the Spring-Loaded Inverted Pendulum (SLIP) model, which represents the system as a single point mass supported by one or two massless elastic legs. Originally developed for running, it was later extended to capture key features of human walking [10, 8, 11]. Unlike IPM and LIPM, the energy storage and release mechanism provided by its compliance enables this model to reproduce human-like gait characteristics, including the double-hump (M-shaped) GRF profile and realistic CoM trajectories [8, 12].\n\nOver the years, several variations of the SLIP model have been proposed to improve walking performance and provide closer approximations of human locomotion characteristics. The Variable SLIP (V-SLIP) model [13, 14] incorporates active modulation of leg stiffness to achieve a cost of transport comparable to that of human walking, thereby enabling more energy-efficient and robust bipedal locomotion. To account for upper-body dynamics, Sharbafi and Seyfarth introduced the Bipedal Trunk-SLIP (BTSLIP) model [15, 16], which redirects GRFs to stabilize trunk posture while preserving compliant leg mechanics, resulting in more realistic GRF profiles and CoM dynamics. Pelit et al.[17] extended the framework to the SLIP with Swing Legs (SLIP-SL) model, which incorporates passive swing-leg dynamics without requiring separate swing-leg trajectory planning. This extension enhances stability and cyclic locomotion by producing human-like gait timing and step lengths. Geyer et al. [8] proposed a simplified Bipedal SLIP (BSLIP) model capable of generating stable walking using different combinations of overall leg stiffness and attack angles. Hao et al. [18] further advanced the BSLIP framework by introducing a damper in parallel with the spring on each massless leg, resulting in the D-SLIP model, which allows evaluation of the effects of hip actuator torque on the energetics and stability of bipedal walking.\n\nHowever, the SLIP model and its variations, while useful in capturing human locomotion traits, are highly simplified and underactuated. Hence their deployment to robots remains challenging, often because of underactuation and merely marginal stability in the template level [19, 20]. Notwithstanding, the forced-oscillation-based variation of SLIP (Fig. 1) is a template that has resulted in highly stable gaits on bipedal robots [21, 22]. In this model, a damper and a leg length actuator for setting the spring’s neutral length are added to the bipedal SLIP. The stability of this model and its wide basin of attraction have been proved and its human-like characteristics have been presented in [22]. As such, adopted this paradigm as the foundation for an extension aimed at incorporating more anthropomorphic traits.\n\nA significant shortcoming in the aforementioned point-foot models is that they neglect the kinematics and dynamics of the foot and ankle, which play essential roles in human walking. In particular, ankle flexion contributes a substantial amount of positive mechanical work for forward progression and provides vertical support to sustain body weight [23, 24]. Therefore, incorporating an ankle joint and a finite-sized foot into an extended reduced-order model can potentially enhance stability and robustness in bipedal walking. In addition, the ankle push-off in such models can produce more realistic GRF profiles, CoM dynamics closer to that of humans, human-like forward center-of-pressure (CoP) travel, and ankle behavior, which are features that point-foot templates fail to capture [25, 26]. An example of such an extension is proposed in [25], where the V-SLIP model was equipped with a 1-DoF ankle joint and a finite-sized foot (VSLIP-FF) for each leg, enabling compliant bipedal walking in complex environments. Their approach employed a finite state machine–based gait planner combined with a differential evolution (DE) optimizer to generate periodic gaits. The gait planning method incorporated an adaptive leg stretching and contracting strategy inspired by human walking, with pre-determined footholds used to update step length for placing the foot on known targets. This strategy was controlled by a PD controller and its gains were selected through offline tuning/optimization to realize a periodic gait. Although this method can generate stable gaits, its reliance on offline tuning and optimization limits its flexibility. This challenges the online adaptability and robustness of bipedal walking, particularly for achieving anthropomorphic stance mechanics, real-time foot placement, and step regulation in response to speed variations or disturbances. More importantly, the stability in this framework does not inlcude the distinct separation between ankle and foot placement strategies [27]. In general, many SLIP model variations ultimately produce gaits dictated by the natural dynamics of the system, without explicitly comparing to anthropomorphic walking gaits [20].\n\nIn this paper, we propose an extended version of the forced-oscillation-based reduced-order model for human-like walking, where an ankle and a foot are added. A human-inspired scheme is designed for the ankle characteristics and all the tunable parameters are selected using human biomechanical information. To evaluate the performance of the proposed model, we compare its results against both human data and the point-foot forced-oscillation-based model and show how the ankle stabilization can work without foot placement regulation, similar to humans.\n\nThe rest of this paper is organized as follows. Section II describes the proposed model and its designed characteristics. Section III details the model parameters and the simulation process. Section IV presents the simulation results including comparisons with the point-foot model and the human data. These results and comparisons are discussed in Section V. Finally, Section VI concludes the paper and outlines directions for future works.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在简化模型中有效地整合人类步态的踝关节特性？  \n2. 如何提高步态模型在面对初始条件误差和小扰动时的稳定性？  \n3. 如何通过改进的模型更好地模拟人类步态的地面反作用力和重心动态？  \n\n【用了什么创新方法】  \n本研究提出了一种扩展的强迫振荡基础的简化模型，通过引入人类灵感的踝关节特性，增强了步态模型的稳定性和人类步态的相似性。模型通过结合脚的放置和踝关节策略，实现了在大误差和小扰动下的稳定性。与传统的点足模型相比，新模型在地面反作用力和重心动态上表现出更接近人类的特征。通过与人类数据的比较，验证了该模型在无脚放置调节情况下的踝关节稳定性，展示了其在理解人类步态和稳定机制方面的潜力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "RAM-NAS: Resource-aware Multiobjective Neural Architecture Search Method for Robot Vision Tasks",
            "authors": "Shouren Mao,Minghao Qin,Wei Dong,Huajian Liu,Yongzhuo Gao",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "Joint first authors: Shouren Mao and Minghao Qin. Published in IEEE/RSJ IROS 2024. This arXiv version adds a joint first-authorship note to correct an omission in the IEEE Xplore version. No technical changes. Please cite the IEEE version",
            "pdf_link": "https://arxiv.org/pdf/2509.20688",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20688",
            "arxiv_html_link": "https://arxiv.org/html/2509.20688v1",
            "abstract": "Neural architecture search (NAS) has shown great promise in automatically designing lightweight models. However, conventional approaches are insufficient in training the supernet and pay little attention to actual robot hardware resources. To meet such challenges, we propose RAM-NAS, a resource-aware multi-objective NAS method that focuses on improving the supernet pretrain and resource-awareness on robot hardware devices. We introduce the concept of subnets mutual distillation, which refers to mutually distilling all subnets sampled by the sandwich rule. Additionally, we utilize the Decoupled Knowledge Distillation (DKD) loss to enhance logits distillation performance. To expedite the search process with consideration for hardware resources, we used data from three types of robotic edge hardware to train Latency Surrogate predictors. These predictors facilitated the estimation of hardware inference latency during the search phase, enabling a unified multi-objective evolutionary search to balance model accuracy and latency trade-offs. Our discovered model family, RAM-NAS models, can achieve top-1 accuracy ranging from 76.7%76.7\\% to 81.4%81.4\\% on ImageNet. In addition, the resource-aware multi-objective NAS we employ significantly reduces the model’s inference latency on edge hardware for robots. We conducted experiments on downstream tasks to verify the scalability of our methods. The inference time for detection and segmentation is reduced on all three hardware types compared to MobileNetv3-based methods. Our work fills the gap in NAS for robot hardware resource-aware.",
            "introduction": "The application of deep learning techniques in robot environmental perception is a current focal point of research. However, their demanding resource requirement in terms of processing time challenges their sustained operation on edge devices embedded within robots[1]. Nevertheless, specific visual perception tasks are driven by deep learning, along with related tasks like object detection[2] and semantic SLAM[3]. Consequently, developing lightweight deep learning models becomes crucial for deploying them on edge computing devices embedded within robots. Model lightweighting strives to reduce model parameters while maintaining performance without significant degradation, thus enhancing computational efficiency. Research[4] suggests that numerous parameters within existing deep neural network models are unnecessary, indicating the potential for model lightweighting. This is particularly crucial for deploying deep learning models on robot platforms. NAS by limiting model complexity, uses reinforcement learning[5] or evolutionary algorithms[6] to automatically pick and adjust key parameters like network layers, width, and depth based on task performance, aiming for the lightweighting model design. NAS stands out for its automation and innovation, reducing the need for manual design efforts by automatically finding the optimal neural network structure.\n\nRecently, one-shot NAS [7] became popular due to low computation overhead and competitive performance. It does not require training thousands of individual models from scratch but only trains a single supernet that contains any architecture in the search space, weights are shared among subnets respectively. However, when a supernet shares weights across subnets, it might not guarantee adequate training for each specific subnet. This inconsistency in ranking the architecture could diminish the efficacy of the search process[8]. Retrain-free NAS methods[8, 9] propose simultaneously training multiple networks, slicing different-sized subnets directly from the supernet and synchronously training them during the supernet’s training process.\nIn this paper, we present subnets mutual distillation and use DKD loss to enable the knowledge better transfer between subnets, without requiring an external teacher model. Through these strategies, the consistency in ranking individual subnets.\n\nOn the other hand, previous NAS[10, 11] mainly abstracts the search problem into a single objective optimization problem under given resource constraints, it demands multiple executions, consuming considerable computing resources and time in the pursuit of a suitable model. Compared to single objective optimization, multiobjective optimization not only allows for simultaneous optimization of multiple given objectives but also yields a Pareto-optimal frontier containing multiple optimal solutions[12]. These solutions exhibit different performances across multiple optimization objectives, thereby facilitating more convenient selection and deployment of different robot computational hardware configurations.\nAdditionally, these methods[13, 14], predominantly consider the computing resources of mobile phones, and few take into account the more frequently used edge hardware of robots. However, as the NAS operating on the server side lacks direct latency aware of hardware, inspired by[12, 15], we propose surrogate predictors to assess the architecture’s latency. In this paper, we conduct a neural architecture search to be aware of the resources of actual robot edge hardware devices with surrogate-assisted.\n\nIn summary, to address the limitation of existing NAS methods, which often overlook robot edge device resources and necessitate multiple executions to identify models with varied computing power requirements, we introduce RAM-NAS.\nThe key contributions are summarized below:\n\nWe propose RAM-NAS, a Resource-aware Multiobjective Neural Architecture Search method. This method allows for a single search to compute the Pareto frontier under various constraints, decision makers can deploy models under different resource constraints.\n\nWe adopt a method where multiple subnets mutually distill each other, enabling joint parameter updates across these subnets. It uses DKD Loss to compute the loss function, ensuring more effective convergence of supernet and boosting the consistency in ranking individual subnets.\n\nWe adopt reusable surrogate predictors to predict hardware latency,\nthe predictor demonstrates high sample efficiency and exhibits strong consistency in predicting latency with actual performance.\n\n1. We propose RAM-NAS, a Resource-aware Multiobjective Neural Architecture Search method. This method allows for a single search to compute the Pareto frontier under various constraints, decision makers can deploy models under different resource constraints.\n\n2. We adopt a method where multiple subnets mutually distill each other, enabling joint parameter updates across these subnets. It uses DKD Loss to compute the loss function, ensuring more effective convergence of supernet and boosting the consistency in ranking individual subnets.\n\n3. We adopt reusable surrogate predictors to predict hardware latency,\nthe predictor demonstrates high sample efficiency and exhibits strong consistency in predicting latency with actual performance.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有神经架构搜索（NAS）方法忽视了机器人边缘设备的资源限制。  \n2. 传统NAS方法需要多次执行，耗费大量计算资源和时间。  \n3. 如何在保证模型性能的同时，实现轻量化和资源感知的深度学习模型。  \n\n【用了什么创新方法】  \n提出了RAM-NAS，一种资源感知的多目标神经架构搜索方法，通过引入子网互蒸馏和使用解耦知识蒸馏损失（DKD Loss）来提升子网间的知识转移。同时，采用可重用的延迟预测器来评估硬件延迟，从而在搜索过程中实现模型准确性与延迟的平衡。实验结果表明，RAM-NAS模型在ImageNet上实现了76.7%到81.4%的顶级准确率，并显著降低了在边缘硬件上的推理延迟，验证了方法的可扩展性。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Efficient Construction of Implicit Surface Models From a Single Image for Motion Generation",
            "authors": "Wei-Teng Chu,Tianyi Zhang,Matthew Johnson-Roberson,Weiming Zhi",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20681",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20681",
            "arxiv_html_link": "https://arxiv.org/html/2509.20681v1",
            "abstract": "Implicit representations have been widely applied in robotics for obstacle avoidance and path planning. In this paper, we explore the problem of constructing an implicit distance representation from a single image. Past methods for implicit surface reconstruction, such as NeuS and its variants generally require a large set of multi-view images as input, and require long training times. In this work, we propose Fast Image-to-Neural Surface (FINS), a lightweight framework that can reconstruct high-fidelity surfaces and SDF fields based on a single or a small set of images.\nFINS integrates a multi-resolution hash grid encoder with lightweight geometry and color heads, making the training via an approximate second-order optimizer highly efficient and capable of converging within a few seconds. Additionally, we achieve the construction of a neural surface requiring only a single RGB image, by leveraging pre-trained foundation models to estimate the geometry inherent in the image. Our experiments demonstrate that under the same conditions, our method outperforms state-of-the-art baselines in both convergence speed and accuracy on surface reconstruction and SDF field estimation. Moreover, we demonstrate the applicability of FINS for robot surface following tasks and show its scalability to a variety of benchmark datasets.",
            "introduction": "For autonomous robots to navigate and interact safely with the real world, they must form reliable geometric representations of their surroundings. Distance-based representations are a powerful representation widely used in motion planning and obstacle avoidance [1, 2, 3, 4, 5, 6, 7, 8]. Accurate and efficient SDF estimation is therefore a key enabler of downstream decision-making and control.\n\nRecent neural implicit surface methods, such as NeuS [9] and its successors [10, 11, 12, 13, 14], have demonstrated impressive capability in reconstructing fine-grained object surfaces. However, these approaches suffer from two key drawbacks: (i) they rely on dense multi-view supervision, which is impractical in robotics settings where only sparse observations are available; and (ii) they require long training times, from minutes to hours, making them unsuitable for real-time use in navigation or manipulation. A complementary line of work [15, 16, 17, 18] has sought to improve generalization to sparse views, reducing the dependency on extensive image collections. However, these approaches can often still require a sizeable number of images, and can be relatively inefficient to train from scratch.\n\nIn this paper, we introduce Fast Image-to-Neural Surface (FINS), a lightweight framework that overcomes these limitations. FINS reconstructs high-fidelity surfaces and SDF fields from as few as a single image, or a small set of images, within seconds. Our framework integrates three components: (1) off-the-shelf 3D foundation models, such as DUSt3R [19] and VGGT [20], to lift single-view inputs into point clouds for SDF supervision; (2) a multi-resolution hash grid encoder [21] to enable efficient feature encoding; and (3) lightweight geometry and color heads trained with an approximate second-order optimizer, yielding rapid convergence.\n\nBy leveraging strong priors from pre-trained 3D models, FINS scales naturally from single objects to multi-view, scene-level settings. This flexibility supports deployment on mobile platforms, where continuous observations can be assimilated into an evolving SDF representation. As a result, FINS enables real-time reconstruction and refinement of neural surfaces for downstream robotics tasks such as obstacle avoidance, path planning, and surface following. We empirically evaluate the quality and efficiency of building implicit distance reconstructions of a diverse range of objects, and demonstrate the applicability of these representations for robot surface following [22]. Concretely, this paper presents the following technical contributions:\n\nWe propose FINS, an end-to-end method that achieves high-precision SDF training from a single image in only a few seconds.\n\nWe leverage pre-trained 3D foundation models to generate point clouds for SDF supervision, enabling efficient and complete reconstruction with limited visual input.\n\nWe adopt multi-resolution hash encoding and lightweight geometry/color heads with a mixed optimization strategy to eliminate heavy optimization and enable real-time convergence.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何从单张图像构建隐式距离表示以支持机器人运动生成。  \n2. 现有方法依赖于多视角图像，训练时间长，难以实时应用。  \n3. 如何提高稀疏观察条件下的表面重建精度与效率。  \n\n【用了什么创新方法】  \n提出了Fast Image-to-Neural Surface (FINS)框架，集成了预训练的3D基础模型、一个多分辨率哈希网格编码器和轻量几何/颜色头，能够在几秒内从单张图像重建高保真表面和SDF字段。通过利用强先验，FINS实现了实时的隐式表面重建与优化，适用于机器人避障、路径规划和表面跟随等任务。实验表明，FINS在收敛速度和重建精度上优于现有方法，展示了其在多种基准数据集上的可扩展性。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Equi-RO: A 4D mmWave Radar Odometry via Equivariant Networks",
            "authors": "Zeyu Han,Shuocheng Yang,Minghan Zhu,Fang Zhang,Shaobing Xu,Maani Ghaffari,Jianqiang Wang",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20674",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20674",
            "arxiv_html_link": "https://arxiv.org/html/2509.20674v1",
            "abstract": "Autonomous vehicles and robots rely on accurate odometry estimation in GPS-denied environments. While LiDARs and cameras struggle under extreme weather, 4D mmWave radar emerges as a robust alternative with all-weather operability and velocity measurement. In this paper, we introduce Equi-RO, an equivariant network-based framework for 4D radar odometry. Our algorithm pre-processes Doppler velocity into invariant node and edge features in the graph, and employs separate networks for equivariant and invariant feature processing. A graph-based architecture enhances feature aggregation in sparse radar data, improving inter-frame correspondence. Experiments on the open-source dataset and self-collected dataset show Equi-RO outperforms state-of-the-art algorithms in accuracy and robustness. Overall, our method achieves 10.7% and 20.0% relative improvements in translation and rotation accuracy, respectively, compared to the best baseline on the open-source dataset.",
            "introduction": "Odometry is indispensable for autonomous vehicles and robots, providing accurate support for downstream perception, planning, and control. When Global Positioning System (GPS) signals are degraded, such as under bridges, in tunnels, or during severe rain or snow, it becomes crucial to obtain relative localization using sensors such as LiDARs and cameras.\n\nHowever, the performance of LiDARs and cameras degrades significantly under extreme weather conditions, limiting their applicability for odometry estimation in such scenarios. Recently, 4D millimeter-wave (mmWave) radar—characterized by compact size, cost efficiency, all-weather adaptability, velocity-measuring capability, and long detection range—has been increasingly regarded as a promising solution for robust perception and localization in autonomous driving, particularly under adverse weather conditions [1].\n\nCurrent 4D mmWave radar odometry algorithms are mostly adapted from traditional LiDAR odometry methods. Some exploit unique properties of 4D mmWave radar point clouds like Doppler velocity and Radar Cross Section (RCS) to perform ego-velocity estimation and point cloud registration [2, 3]. Regarding learning-based approaches, a few pioneering works have adapted Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) for stand-alone 4D mmWave radar [4] and radar-camera fusion [5] odometry.\n\nIn learning-based odometry, accurate feature extraction and matching are critical steps. However, features in adjacent frames may become highly inconsistent when the ego-vehicle moves rapidly or turns sharply. Equivariant neural networks have emerged as a promising approach to address this issue [7, 8]. They learn features that transform predictably under specific geometric transformations, such as 3D translations and rotations represented by rigid-body motion group SE(3), thereby enhancing feature matching performance. The geometric symmetry can improve the generalizability and interpretability of learning-based odometry systems.\n\nThe 4D mmWave radar provides additional physical attributes compared to LiDAR point clouds. For example, it measures the RCS of each point, which can be regarded as SE(3)-invariant since it reflects the strength of the returned signal and is determined mainly by the target’s size, shape, and material rather than the sensor’s pose. In addition, the Doppler effect enables estimation of the radial relative velocity of detected targets. However, Doppler velocity is neither equivariant nor invariant under SE(3) transformations, as it depends on both the sensor’s motion and the target’s motion, which limits the applicability of existing equivariant networks to 4D mmWave radar odometry. Moreover, 4D mmWave radar point clouds are inherently noisier, sparser, and more irregular than those captured by scanning LiDARs. Consequently, effectively aggregating point-wise features with those of their spatial neighbors remains a significant challenge for learning-based 4D mmWave radar odometry.\n\nIn this paper, we propose Equi-RO, a novel framework for 4D mmWave radar odometry built upon equivariant neural networks. Compared with existing 4D mmWave radar odometry algorithms, we design a pre-processing pipeline to derive invariant Doppler features and construct a graph representation to mitigate noise and sparsity. The equivariant and invariant features are then processed through separate networks to estimate the relative transformation. The invariant velocity features and equivariant network design enable our method to remain robust in challenging real-world driving scenarios, where existing methods often degrade. Leveraging both a public dataset [6] and a self-collected dataset, we demonstrate consistent improvements over state-of-the-art 4D mmWave radar odometry approaches.\n\nIn summary, our main contributions are as follows:\n\nWe develop a novel 4D mmWave radar odometry algorithm based on equivariant neural networks, capable of producing accurate and robust odometry estimates under large rotations and fast motions, as commonly encountered in sharp turns and challenging driving maneuvers.\n\nWe develop a novel 4D mmWave radar odometry algorithm based on equivariant neural networks, capable of producing accurate and robust odometry estimates under large rotations and fast motions, as commonly encountered in sharp turns and challenging driving maneuvers.\n\nWe design a dedicated pre-processing pipeline for Doppler velocity that derives invariant features, thereby stabilizing motion-related features. And we design a framework that separately extracts equivariant and invariant features to preserve geometric consistency, making it applicable to 4D mmWave radar point clouds with physical attributes.\n\nWe conduct extensive experiments on both an open-source dataset and a self-collected dataset, along with ablation studies, to validate the effectiveness of the proposed method across diverse scenarios.\n\nThe source code will be released after receiving the final decision.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在GPS信号缺失的环境中实现准确的4D mmWave雷达里程计估计。  \n2. 现有的4D mmWave雷达里程计算法在极端天气条件下的性能不足。  \n3. 如何有效处理4D mmWave雷达点云的噪声和稀疏性问题。  \n\n【用了什么创新方法】  \n本研究提出了Equi-RO，一个基于等变网络的4D mmWave雷达里程计框架。该方法通过预处理多普勒速度来提取不变节点和边特征，并采用图形架构增强稀疏雷达数据的特征聚合。通过分别处理等变和不变特征，Equi-RO在快速运动和大旋转情况下保持了鲁棒性。实验结果显示，与现有方法相比，Equi-RO在翻译和旋转精度上分别提高了10.7%和20.0%。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation",
            "authors": "Junzhe Wang,Jiarui Xie,Pengfei Hao,Zheng Li,Yi Cai",
            "subjects": "Robotics (cs.RO)",
            "comment": "submitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.20656",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20656",
            "arxiv_html_link": "https://arxiv.org/html/2509.20656v1",
            "abstract": "Reliable brain–computer interface (BCI) control of robots provides an intuitive and accessible means of human–robot interaction, particularly valuable for individuals with motor impairments. However, existing BCI–Robot systems face major limitations: electroencephalography (EEG) signals are noisy and unstable, target selection is often predefined and inflexible, and most studies remain restricted to simulation without closed-loop validation. These issues hinder real-world deployment in assistive scenarios. To address them, we propose a closed-loop BCI–AR–Robot system that integrates motor imagery (MI)-based EEG decoding, augmented reality (AR) neurofeedback, and robotic grasping for zero-touch operation. A 14-channel EEG headset enabled individualized MI calibration, a smartphone-based AR interface supported multi-target navigation with direction-congruent feedback to enhance stability, and the robotic arm combined decision outputs with vision-based pose estimation for autonomous grasping. Experiments are conducted to validate the framework: MI training achieved 93.1% accuracy with an average information transfer rate (ITR) of 14.8 bit/min; AR neurofeedback significantly improved sustained control (SCI = 0.210) and achieved the highest ITR (21.3 bit/min) compared with static, sham, and no-AR baselines; and closed-loop grasping achieved a 97.2% success rate with good efficiency and strong user-reported control. These results show that AR feedback substantially stabilizes EEG-based control and that the proposed framework enables robust zero-touch grasping, advancing assistive robotic applications and future modes of human–robot interaction.",
            "introduction": "Human–robot interaction (HRI) plays a crucial role in assistive robotics, teleoperation, and human-inspired systems [1, 2, 3], enabling users to control robots naturally and intuitively. For individuals with severe motor impairments, brain–computer interfaces (BCIs) provide a direct communication pathway between the human brain and external devices without requiring any physical movement [4]. Compared to invasive BCIs, non-invasive BCIs are safer, simpler, and more practical for widespread use [5], although they typically suffer from lower information transfer rates (ITRs) [6]. Among non-invasive BCI modalities, electroencephalography (EEG) is widely adopted for its high temporal resolution, portability, and safety [7]. EEG-based paradigms such as steady-state visual evoked potentials (SSVEP) and motor imagery (MI) have demonstrated strong potential for real-time intention decoding in control tasks [8, 9].\n\nIn conventional systems, visual stimuli for EEG-based interaction are often presented on an independent monitor [10]. For complex, multi-target tasks, this setup is inconvenient: the display is usually not within the same field of view as the workspace, forcing users to repeatedly shift their gaze to verify task progress. This attention switching degrades the interaction experience and limits the practical usability of BCIs [11]. To overcome this limitation, augmented reality (AR) has emerged as a powerful tool for enhancing perceptual feedback in robotic control, overlaying virtual elements directly onto the real world [12]. Integrating AR into BCIs allows users to select and manipulate virtual representations of physical objects in a more immersive, context-aware manner, reducing cognitive load compared to traditional monitor-based interfaces [13].\n\nHowever, most existing BCI-AR frameworks rely on fixed stimulus–command mappings, lack adaptability to dynamically changing environments, and are often demonstrated only in simplified simulation settings without a direct bridge to physical robotic execution. AR technologies can be classified as video see-through (VST) or optical see-through (OST) [14]. VST-AR acquires real-world imagery via a camera, fuses it with virtual content, and displays the composite view on a screen, such as a smartphone or tablet [15]. OST-AR employs a head-mounted optical combiner to overlay translucent virtual elements directly onto the real-world [16], allowing simultaneous perception of both without intermediate video processing [17]. In this context, VST-AR realized on a handheld device provides practical advantages over OST-based HMDs, primarily for two reasons. First, using an EEG-based BCI headset together with an OST-AR HMD can place excessive pressure on the scalp, causing discomfort during prolonged operation [18, 19]. Second, placing the AR headset directly over EEG electrodes may cause mechanical displacement or uneven contact pressure, degrading signal quality and stability [20]. In contrast, a handheld VST-AR device avoids physical interference with EEG sensors, ensuring user comfort and reliable neural signal acquisition.\n\nRobotic grasping in complex, multi-target environments poses substantial challenges. EEG signals are inherently noisy, and their effective integration with AR-based real-time visual feedback must be robust to prevent unintended actions (e.g., AR-guided assistance in hybrid gaze–BMI systems has been shown to improve grasping accuracy and reduce user burden significantly [21]). Nevertheless, prior studies on BCI-AR interfaces often rely on predefined and static target layouts, which limit their adaptability to dynamically changing environments [22]. Only a few systems demonstrate a complete closed-loop pipeline—from EEG intention decoding, to AR-based target confirmation, to autonomous robotic grasp execution—while leveraging AR feedback to enhance signal persistence and stability. Moreover, recent surveys emphasize the scarcity of fully integrated, physically realistic BCI–AR–robot systems capable of seamless deployment across both physical and simulated settings [23, 24, 25].\n\nBuilding upon these gaps, we validate a closed-loop framework on the MyCobot 280Pi robot, enabling fully hands-free operation. Users can select and grasp multiple objects solely through EEG control and AR interaction, without the need for manual intervention. The AR feedback mechanism further stabilizes command execution, directly addressing assistive robotics needs by offering an intuitive and accessible interaction modality for mobility-impaired individuals. Experimental results confirm that the system supports flexible multi-object selection, enhances EEG command accuracy, adapts robustly to changing object layouts, and achieves precise grasp execution without requiring manual remapping.\n\nThe main contributions of this study are as follows:\n\nIntegrated framework: A fully developed multimodal closed-loop pipeline combining EEG intention decoding, AR-based target selection, and autonomous robotic grasping across both physical and simulated environments.\n\nIntegrated framework: A fully developed multimodal closed-loop pipeline combining EEG intention decoding, AR-based target selection, and autonomous robotic grasping across both physical and simulated environments.\n\nDynamic interaction: Real-world AR feedback enabling multi-object decisions via EEG commands, ensuring continuity and accuracy in dynamic settings.\n\nRobust grasping: Adaptive strategies based on inverse kinematics control, enhancing stability and transferability across robotic platforms.\n\nHands-free accessibility: Enabling barrier-free human–robot collaboration, allowing users to accomplish object selection and grasping tasks solely through EEG control, fulfilling requirements for assistive robotics applications.\n\n1. Integrated framework: A fully developed multimodal closed-loop pipeline combining EEG intention decoding, AR-based target selection, and autonomous robotic grasping across both physical and simulated environments.\n\n2. Dynamic interaction: Real-world AR feedback enabling multi-object decisions via EEG commands, ensuring continuity and accuracy in dynamic settings.\n\n3. Robust grasping: Adaptive strategies based on inverse kinematics control, enhancing stability and transferability across robotic platforms.\n\n4. Hands-free accessibility: Enabling barrier-free human–robot collaboration, allowing users to accomplish object selection and grasping tasks solely through EEG control, fulfilling requirements for assistive robotics applications.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高EEG信号的稳定性和准确性以实现有效的人机交互？  \n2. 如何在动态环境中实现灵活的多目标选择和无接触抓取？  \n3. 如何将增强现实(AR)与脑机接口(BCI)结合以改善用户体验？  \n4. 如何实现完整的闭环系统以支持物理和模拟环境中的机器人操作？  \n\n【用了什么创新方法】  \n本研究提出了一种闭环BCI-AR-机器人系统，结合了基于运动想象的EEG解码、增强现实反馈和自主抓取。通过14通道EEG头盔进行个性化MI校准，智能手机AR界面支持多目标导航，并提供方向一致的反馈以增强稳定性。实验结果显示，MI训练准确率达到93.1%，闭环抓取成功率为97.2%。AR反馈显著提高了EEG控制的稳定性，推动了助残机器人应用的发展。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Cyber Racing Coach: A Haptic Shared Control Framework for Teaching Advanced Driving Skills",
            "authors": "Congkai Shen,Siyuan Yu,Yifan Weng,Haoran Ma,Chen Li,Hiroshi Yasuda,James Dallas,Michael Thompson,John Subosits,Tulga Ersal",
            "subjects": "Robotics (cs.RO); Human-Computer Interaction (cs.HC)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20653",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20653",
            "arxiv_html_link": "https://arxiv.org/html/2509.20653v1",
            "abstract": "This study introduces a haptic shared control framework designed to teach human drivers advanced driving skills. In this context, shared control refers to a driving mode where the human driver collaborates with an autonomous driving system to control the steering of a vehicle simultaneously.\nAdvanced driving skills are those necessary to safely push the vehicle to its handling limits in high-performance driving such as racing and emergency obstacle avoidance. Previous research has demonstrated the performance and safety benefits of shared control schemes using both subjective and objective evaluations.\nHowever, these schemes have not been assessed for their impact on skill acquisition on complex and demanding tasks.\nPrior research on long-term skill acquisition either applies haptic shared control to simple tasks or employs other feedback methods like visual and auditory aids.\nTo bridge this gap, this study creates a cyber racing coach framework based on the haptic shared control paradigm and evaluates its performance in helping human drivers acquire high-performance driving skills.\nThe framework introduces (1) an autonomous driving system that is capable of cooperating with humans in a highly performant driving scenario;\nand (2) a haptic shared control mechanism along with a fading scheme to gradually reduce the steering assistance from autonomy based on the human driver’s performance during training.\nTwo benchmarks are considered: self-learning (no assistance) and full assistance during training.\nResults from a human subject study indicate that the proposed framework helps human drivers develop superior racing skills compared to the benchmarks, resulting in better performance and consistency.",
            "introduction": "Advanced driving skills refer to a set of competencies that go beyond basic driving abilities in terms of situational awareness, hazard perception, risk management, and vehicle handling [1].\nThey are crucial in high-performance driving tasks such as racing, and can also improve safety in everyday driving [2, 1].\nHowever, they can be challenging to learn due to several reasons: (1) Complexity: Advanced driving skills require proficiency in vehicle control, hazard perception, and risk management, making them inherently complex and difficult to master quickly.\nFor instance, accurately gauging handling limits and responding appropriately demands extensive practice under conditions not experienced regularly in everyday driving. (2) Risk: Advanced driving involves higher risk levels than basic driving.\nThis can intimidate learners and requires time to build necessary confidence and decision-making skills. (3) Cognative & physical Demanding: Mastering these skills requires high mental workload, dexterity, and coordination, which can be challenging for learners.\n\nOn the other hand, autonomy algorithms have been witnessed to safely explore high-performance driving limits [3, 4, 5, 6, 7].\nTherefore, it is possible to conceive a framework that leverages highly performant autonomy algorithms to teach human drivers advanced driving skills that enable precise handling at the dynamic limits of vehicles.\nHowever, as the literature review below shows, such a framework has not yet been created.\nThis study addresses this gap by using racing as a context for assessing human drivers’ advanced driving skills.\n\nIn recent advancements in racing training programs and games, it has been established that providing visual and auditory feedback is beneficial for novice drivers [8, 9].\nIn [10], visual feedback, such as illustrating the racing line on the track, assists drivers in determining the optimal path to steer around corners.\nIn [11], the integration of auditory and visual feedback components culminates in the creation of an immersive learning environment tailored for novice players.\nNevertheless, these feedback modalities are often advisory, meaning that they only provide suggestions for humans instead of directly interacting with their behavior.\nWith a motivation to provide physical interactions with humans, the present study examines the viability of haptic feedback in instructing advanced driving techniques.\n\nHaptic feedback allows for physical interactions between the human and autonomous driving system, enabling the human to feel the intentions of autonomy in real time.\nThis continuous mode of collaboration enhances seamless transitions of control authority, which is particularly crucial for applications such as shared control in semi-autonomous driving [12].\nHere, haptic shared control means that both the human driver and autonomy can exert a torque on the steering wheel at the same time, reflecting their respective intentions to steer.\nThis shared control mechanism allows both agents to communicate their control intentions through the steering wheel.\nThis mutual feedback forms a closed-loop system, enhancing coordination and responsiveness, and turning the steering wheel into a negotiation interface.\nIn [13], haptic shared control is shown to reduce the visual demand and shorten the reaction time while performing better in the driving test.\nThese benefits make haptic shared control a valuable tool in enhancing driver safety and efficiency.\nIt has also been shown to reduce the workload of the human driver in performing ordinary driving tasks such as lane keeping [14, 15, 16, 17, 18].\n\nPrior work has primarily focused on characterizing the immediate benefits of haptic shared control.\nMost studies have concentrated on immediate improvements in task performance and safety, without exploring the longer term implications and potential for driver training.\nIn other words, haptic shared control has been initially used as a driver assistance tool rather than as a teaching aid.\n\nWhile haptic shared control effectively supports humans in real-time, there is considerable potential for its further development as a training mechanism for advanced driving skills.\nThe rationale for this hypothesis is grounded in the applications of haptic shared control in normal driving tasks [19, 20, 21, 22, 23, 24].\nIn [20, 23], continuous haptic feedback was provided to guide drivers in a steering task, helping them to follow lanes.\nHaptic feedback has also been utilized as a continuous disturbance to help drivers improve their skills [19, 22].\nNotably, in [19], a fading scheme is created to gradually fade away the torque assist to help the human’s learning process.\nComparatively, as noted in [25], consistent assistance without a fading mechanism may lead to deskilling, as automation can hinder long-term skill retention rather than enhance it.\nHaptic feedback can also be provided in a bandwidth-limited manner, activating only when certain system states exceed a threshold.\nIn [21, 24], researchers demonstrate that both continuous guidance and bandwidth feedback can effectively enhance driver performance.\nAlthough haptic feedback has demonstrated success in teaching longer-term skills to human drivers in driving scenarios, it has only been applied in relatively simple contexts.\nThe potential for utilizing haptic shared control in more complex and performance-oriented driving scenarios has not yet been explored.\n\nTo effectively teach human drivers advanced driving skills via haptic shared control, it is essential to have a highly performant autonomous system that not only possesses such skills itself but also can seamlessly collaborate with humans. In [26], an optimal control-based algorithm is used to track the path with its own steering commands, and the final steering command is a weighted sum of both human and autonomy inputs.\nIn [27, 28], researchers hired a human expert that is utilized as a source of autonomy that generates haptic feedback.\nTo avoid the inconsistency of human experts, other researchers introduced a\nmodel predictive control (MPC) formulation within the haptic shared control framework [16].\nAlthough such studies have shown that autonomy and human drivers can collaborate, the autonomous systems used in current haptic shared control frameworks are not performant enough for advanced driving scenarios such as racing.\n\nRecent efforts resulted in advanced autonomous systems capable of achieving performance comparable to the best human drivers.\nSpecifically in the context of racing, machine learning algorithms, such as reinforcement learning, have been widely used due to their ability to find optimal policies and their general applicability [29, 30, 31].\nOther works utilize feedforward and feedback controllers to calculate the steering, throttle and braking commands over the racetrack [32, 33, 34].\nMPC has also been popular because it captures the vehicle model and maps control actions based on this knowledge [35, 36, 37, 38, 39].\nIn [40], MPC is used in a shared control mode, where it manages the throttle and braking positions while the human driver controls the steering.\nNevertheless, these solutions either rely on a pre-defined global trajectory to inform the local controller or use learned objectives for control. With human intervention, the vehicle can easily deviate from the original lane in trajectory tracking methods or from the safe states set in learning-based MPC, which can cause infeasible solutions.\n\nIn [41], a safe envelope-based MPC method is introduced, which neither requires training nor relies on pre-defined trajectories when planning dynamically feasible paths on the track. However, it is designed to operate autonomously, without consideration for shared control with human intervention.\n\nIn light of the background review above, the potential benefits of haptic shared control in helping human drivers acquire advanced driving skills is identified as an unexplored but important domain.\nTo explore this potential, this paper presents and evaluates a novel haptic shared control framework aimed at teaching humans advanced driving skills.\nThe original contributions of this work are:\n\nA real-time Model Predictive Control (MPC) framework capable of optimizing vehicle trajectories online without requiring a predefined path while being robust to human intervention.\n\nA haptic shared control framework using a fading scheme to smoothly transition control authority by fading away the autonomy assistance based on human driver’s performance during training.\n\nValidation of the efficacy of the proposed framework for teaching advanced driving skills through a human subject study, with self-learning and full assistance teaching methods as the benchmarks.\n\nThe remainder of the paper is organized as follows.\nSec. II covers the methodology, including the design of autonomy, the haptic shared control framework, performance evaluation metrics, and the fading scheme in training.\nSec. III describes the hardware and software setup for the experiment, the test procedure, and the demographics of the human subjects.\nSec. IV discusses the results of the human subject study and further highlights the benefit of utilizing the proposed methods.\nFinally, Sec. V concludes the study, summarizing the key findings and future directions of research.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效地教导人类驾驶员掌握高级驾驶技能。  \n2. 现有的共享控制方案在复杂任务中的技能获取效果尚未评估。  \n3. 如何利用触觉反馈提升驾驶员的学习效果和技能保留。  \n\n【用了什么创新方法】  \n本研究提出了一种基于触觉共享控制的框架，旨在帮助人类驾驶员学习高级驾驶技能。该框架包括一个能够与人类协作的自主驾驶系统，以及一种逐渐减少自主辅助的触觉共享控制机制。通过在人类受试者研究中进行验证，结果显示该框架显著提升了驾驶员的赛车技能，相较于自学和全辅助的基准方法，表现和一致性均有所改善。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Suction Leap-Hand: Suction Cups on a Multi-fingered Hand Enable Embodied Dexterity and In-Hand Teleoperation",
            "authors": "Sun Zhaole,Xiaofeng Mao,Jihong Zhu,Yuanlong Zhang,Robert B. Fisher",
            "subjects": "Robotics (cs.RO)",
            "comment": "An IEEE conference paper currently under review",
            "pdf_link": "https://arxiv.org/pdf/2509.20646",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20646",
            "arxiv_html_link": "https://arxiv.org/html/2509.20646v1",
            "abstract": "Dexterous in-hand manipulation remains a foundational challenge in robotics, with progress often constrained by the prevailing paradigm of imitating the human hand.\nThis anthropomorphic approach creates two critical barriers: 1) it limits robotic capabilities to tasks humans can already perform, and 2) it makes data collection for learning-based methods exceedingly difficult.\nBoth challenges are caused by traditional force-closure which requires coordinating complex, multi-point contacts based on friction, normal force, and gravity to grasp an object.\nThis makes teleoperated demonstrations unstable and amplifies the sim-to-real gap for reinforcement learning.\nIn this work, we propose a paradigm shift: moving away from replicating human mechanics toward the design of novel robotic embodiments.\nWe introduce the Suction Leap-Hand (SLeap Hand), a multi-fingered hand featuring integrated fingertip suction cups that realize a new form of suction-enabled dexterity.\nBy replacing complex force-closure grasps with stable, single-point adhesion, our design fundamentally simplifies in-hand teleoperation and facilitates the collection of high-quality demonstration data.\nMore importantly, this suction-based embodiment unlocks a new class of dexterous skills that are difficult or even impossible for the human hand, such as one-handed paper cutting and in-hand writing.\nOur work demonstrates that by moving beyond anthropomorphic constraints, novel embodiments can not only lower the barrier for collecting robust manipulation data but also enable the stable, single-handed completion of tasks that would typically require two human hands.\nOur webpage is https://sites.google.com/view/sleaphand.",
            "introduction": "Dexterous manipulation, the ability to reconfigure objects within a single hand, remains a grand challenge in robotics [1, 2].\nThe dominant paradigm for achieving this goal has been data-driven learning on anthropomorphic hands, an approach that has led to successes in grasping and reorientation [3, 4, 5].\nHowever, this focus on human-like forms has also fundamentally tethered robotic potential to the limits of human dexterity.\nCurrent research is largely centered on replicating those simple tasks that human hands can do, rather than solving challenging dexterous tasks or exploring what novel robotic morphologies might achieve.\nThis reality raises our core question: to achieve advanced dexterity, must robots merely imitate the human hand, or can they leap beyond these biological constraints by exploiting a unique physical embodiment?\n\nRecent work has begun to challenge this human-centric paradigm by demonstrating that dexterity emerges not only from sophisticated control but also from morphology itself [6]. A systematic exploration of both anthropomorphic and non-anthropomorphic designs revealed that novel configurations can yield unique, emergent manipulation behaviors without complex controllers. This provides compelling evidence that robotic dexterity is not intrinsically bound to the imitation of the human form.\n\nDespite this insight, the current anthropomorphic approach has created two interconnected barriers that impede progress.\nBoth barriers are caused by a deep reliance on force-closure to maintain a stable grasp through a balance of gravity, normal forces, and friction across multiple contact points.\n\nThe first barrier is the extreme difficulty of teleoperating dexterous in-hand manipulation.\nWhether using wearable exoskeletons [7, 8] or motion capture systems [9, 10], the human operator is cognitively burdened with constantly managing multiple contact points to prevent grasp failure during long-horizon tasks.\nThis makes the collection of high-quality, successful demonstrations inefficient and unreliable.\nThe second barrier is the inherent limitation of the anthropomorphic designs themselves.\nDesigns like the Shadow Hand and Allegro Hand are mechanically anthropomorphic, but are also bounded by the functions and dexterity as the human hand.\nThose complex dexterous manipulations are still within human hands’ capabilities, e.g., in-hand arbitrary object reorientation and solving Rubik’s Cube [3, 4].\nThe root cause of both challenges is a reliance on force-closure: the need to form a stable grasp through a precise balance of gravity, normal forces, and friction across multiple contact points.\nAs a result, even the simplest primitive, grasping, typically requires at least two fingers or a finger and the palm.\nFor imitation learning, this makes it incredibly difficult for a human to demonstrate complex in-hand motions without dropping the object, resulting in easily failed demonstrations in long-horizon tasks.\nFor reinforcement learning, this reliance on rich, sensitive contact dynamics massively complicates sim-to-real transfer, requiring much effort for real-world deployment.\nBy continuing to follow this principle, we are not only making data collection harder but also capping robotic dexterity at the human level.\n\nTo overcome these limitations, we propose a paradigm shift from friction-based force-closure to adhesion-based manipulation.\nWe introduce the Suction Leap-Hand (SLeap Hand), a 5-DoF-per-finger robotic hand where each fingertip is augmented with a suction cup, shown in Figure 1.\nThis suction-based embodiment directly addresses the two core challenges.\nFirst, it dramatically simplifies teleoperation.\nA single suctioned fingertip can create a firm grasp, freeing the operator to focus on the manipulation task itself without the constant risk of dropping the object.\nThis lowers the barrier to collecting high-quality data for imitation learning.\nThe teleoperation can be paused at any time to reduce operators’ workload without keeping lifting arms in the air or wearing exoskeletons all the time.\nSecond, and more importantly, it unlocks new dexterous skills.\nBy leveraging adhesion, the SLeap Hand can perform tasks that are exceedingly difficult or impossible for a single human hand, such as cutting paper or writing on a notebook held by the same hand.\n\nThis work shows that by exploring suction-based embodiments that extend beyond human hand capabilities, the SLeap Hand enables teleoperation of complex dexterous tasks.\nThe main contributions of this paper are:\n\nA novel hardware embodiment:\nWe present the design and realization of the SLeap Hand, a 15-DoF, three-fingered manipulator featuring suction cups on each fingertip and the palm.\nThis design facilitates a hybrid manipulation strategy, combining traditional frictional contacts with controlled adhesion.\n\nA novel hardware embodiment:\nWe present the design and realization of the SLeap Hand, a 15-DoF, three-fingered manipulator featuring suction cups on each fingertip and the palm.\nThis design facilitates a hybrid manipulation strategy, combining traditional frictional contacts with controlled adhesion.\n\nAn teleoperation system to collect challenging dexterous demonstrations precisely and comfortably:\nWe develop a teleoperation system that uniquely leverages the hand’s suction capabilities to provide stable and intuitive control.\nThis enables human to generate high-fidelity demonstrations of complex tasks in a quasi-static way without the assistance of learned models or complex controllers on non-anthropomorphic hands.\nBesides, it reduces operators’ workload.\n\nDemonstration of advanced dexterity:\nWe conduct a series of demonstrations on challenging manipulation tasks, such as paper cutting and in-hand writing, to benchmark the system’s performance.\nThe results demonstrate that our SLeap Hand with suction-based embodiment enables the next level of dexterity previously difficult to achieve with conventional robotic hands.\n\n1. A novel hardware embodiment:\nWe present the design and realization of the SLeap Hand, a 15-DoF, three-fingered manipulator featuring suction cups on each fingertip and the palm.\nThis design facilitates a hybrid manipulation strategy, combining traditional frictional contacts with controlled adhesion.\n\n2. An teleoperation system to collect challenging dexterous demonstrations precisely and comfortably:\nWe develop a teleoperation system that uniquely leverages the hand’s suction capabilities to provide stable and intuitive control.\nThis enables human to generate high-fidelity demonstrations of complex tasks in a quasi-static way without the assistance of learned models or complex controllers on non-anthropomorphic hands.\nBesides, it reduces operators’ workload.\n\n3. Demonstration of advanced dexterity:\nWe conduct a series of demonstrations on challenging manipulation tasks, such as paper cutting and in-hand writing, to benchmark the system’s performance.\nThe results demonstrate that our SLeap Hand with suction-based embodiment enables the next level of dexterity previously difficult to achieve with conventional robotic hands.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何克服传统人形手在灵巧操作中的局限性？  \n2. 如何简化远程操作以提高数据收集的效率和质量？  \n3. 如何利用新型物理形态实现超越人类手的灵巧技能？  \n\n【用了什么创新方法】  \n本研究提出了一种新的机器人手设计——Suction Leap-Hand（SLeap Hand），该手具有集成的吸盘，能够实现基于吸附的灵巧操作。通过替代传统的多点接触力闭合抓握，SLeap Hand简化了远程操作过程，使操作员能够专注于任务而无需担心物体掉落。此外，吸附的应用使得执行如单手剪纸和在手中书写等任务成为可能，这些技能是传统人形手难以实现的。通过这些创新，SLeap Hand不仅降低了数据收集的难度，还展示了超越人类手的灵巧能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Learning Terrain-Specialized Policies for Adaptive Locomotion in Challenging Environments",
            "authors": "Matheus P. Angarola,Francisco Affonso,Marcelo Becker",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Accepted to the 22nd International Conference on Advanced Robotics (ICAR 2025).",
            "pdf_link": "https://arxiv.org/pdf/2509.20635",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20635",
            "arxiv_html_link": "https://arxiv.org/html/2509.20635v1",
            "abstract": "Legged robots must exhibit robust and agile locomotion across diverse, unstructured terrains, a challenge exacerbated under blind locomotion settings where terrain information is unavailable. This work introduces a hierarchical reinforcement learning framework that leverages terrain-specialized policies and curriculum learning to enhance agility and tracking performance in complex environments. We validated our method on simulation, where our approach outperforms a generalist policy by up to 16% in success rate and achieves lower tracking errors as the velocity target increases, particularly on low-friction and discontinuous terrains, demonstrating superior adaptability and robustness across mixed-terrain scenarios.",
            "introduction": "Legged robots operating in real-world environments must function reliably across complex and unstructured terrains to achieve task-oriented goals, requiring locomotion controllers that are both agile and adaptable. Traditional approaches based on model predictive control (MPC) and trajectory optimization (TO) have demonstrated effectiveness under nominal conditions but often fail in edge cases, where simplified dynamics and constraint models are insufficient to capture the complexities of robot–environment interactions [1, 2].\n\nTo address these limitations, learning-based methods have seen growing adoption, leveraging the capacity of neural networks to model high-dimensional, nonlinear control problems from data [3]. In particular, deep reinforcement learning (RL) has emerged as a promising paradigm, enabling agents to learn control policies through direct interaction with the environment, without requiring curated datasets—a fundamental bottleneck in supervised learning [4, 5].\n\nIn RL-based locomotion, control is formulated as a sequential decision-making problem, where policies are optimized through trial-and-error to maximize expected cumulative rewards, typically using simulation environments [6]. To represent the diversity of real-world conditions, domain randomization is applied to factors such as terrain profiles, external perturbations, and sensor noise. As a result, legged robots can learn robust mappings from sensory observations to control actions, enabling them to track desired velocity commands while maintaining stability across diverse and challenging terrains [7, 8, 9].\n\nFurthermore, a particular class of locomotion frameworks, known as blind policies, relies solely on proprioceptive information, without access to exteroceptive sensors such as cameras or LiDAR. These policies are typically employed in systems with limited sensing capabilities or constrained computational resources. However, the lack of terrain awareness encourages the controller to operate under worst-case assumptions, as the robot can only sense the terrain after physical contact rather than perceive it in advance. This reduces agility and limits grades overall locomotion performance, especially when tracking velocity commands in challenging environments [10, 11].\n\nIn parallel, prior work has demonstrated the effectiveness of hierarchical learning in solving complex locomotion and navigation tasks, where a high-level policy selects among low-level behaviors, each specialized for a specific context [12, 13]. Nevertheless, this framework remains underexplored in the context of adaptive locomotion with terrain-specialized policies, particularly in blind locomotion settings.\n\nIn this paper, we present a hierarchical RL framework for blind legged locomotion that leverages terrain-specialized control policies (Fig. 1). Our method decomposes the locomotion task into terrain-specific subtasks, each addressed by a dedicated low-level policy trained with proprioceptive inputs and tailored reward shaping. A high-level policy selector uses privileged observations during deployment to identify the terrain type and activate the corresponding expert policy. To enable agile behavior, each policy is trained under a progressive curriculum that gradually expands the range of velocity commands based on performance.\n\nThrough systematic experimentation with terrain-specialized policies, we analyze their performance at high speeds, achieved by curriculum learning strategy, in complex environments. Furthermore, we compare the effectiveness of specialized versus generalized policies under equivalent training conditions, highlighting trade-offs in robustness and adaptability.\n\nThe key contributions of this work are:\n\nHierarchical control with terrain-specialized policies for blind locomotion\n\nCurriculum learning tailored to terrain-specific agility\n\n1. Hierarchical control with terrain-specialized policies for blind locomotion\n\n2. Curriculum learning tailored to terrain-specific agility",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在盲行走环境中实现腿部机器人适应性运动控制。  \n2. 如何利用分层强化学习框架提升在复杂地形中的灵活性和跟踪性能。  \n3. 传统控制方法在复杂和非结构化地形中的局限性。  \n\n【用了什么创新方法】  \n本文提出了一种分层强化学习框架，利用地形专用控制策略和课程学习来解决盲行走中的适应性运动问题。通过将运动任务分解为特定地形的子任务，每个子任务由专门的低级策略处理，并在部署时使用高层策略选择器识别地形类型，从而激活相应的专家策略。实验结果表明，该方法在复杂环境中相较于通用策略成功率提高了16%，并在低摩擦和不连续地形中表现出更低的跟踪误差，展示了其在混合地形场景中的卓越适应性和鲁棒性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation",
            "authors": "Satyajeet Das,Darren Chiu,Zhehui Huang,Lars Lindemann,Gaurav S. Sukhatme",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20623",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20623",
            "arxiv_html_link": "https://arxiv.org/html/2509.20623v1",
            "abstract": "Reinforcement learning has enabled significant progress in complex domains such as coordinating and navigating multiple quadrotors. However, even well-trained policies remain vulnerable to collisions in obstacle-rich environments.\nAddressing these infrequent but critical safety failures through retraining or fine-tuning is costly and risks degrading previously learned skills. Inspired by activation steering in large language models and latent editing in computer vision, we introduce a framework for inference-time Latent Activation Editing (LAE) that refines the behavior of pre-trained policies without modifying their weights or architecture. The framework operates in two stages: (i) an online classifier monitors intermediate activations to detect states associated with undesired behaviors, and (ii) an activation editing module that selectively modifies flagged activations to shift the policy towards safer regimes. In this work, we focus on improving safety in multi-quadrotor navigation.\nWe hypothesize that amplifying a policy’s internal perception of risk can induce safer behaviors. We instantiate this idea through a latent collision world model trained to predict future pre-collision activations, thereby prompting earlier and more cautious avoidance responses. Extensive simulations and real-world Crazyflie experiments demonstrate that LAE achieves statistically significant reduction in collisions (nearly 90% fewer cumulative collisions compared to the unedited baseline) and substantially increases the fraction of collision-free trajectories, while preserving task completion. More broadly, our results establish LAE as a lightweight paradigm, feasible on resource-constrained hardware, for post-deployment refinement of learned robot policies.",
            "introduction": "Advances in robot learning have significantly\npushed the boundaries of autonomy, including multi robot systems,\ndriven primarily by both reinforcement learning (RL) and imitation learning [1, 2, 3, 4]. Despite these successes, most learned models function as black boxes with limited interpretability and explainability [5]. Enhancing specific behaviors or addressing edge cases typically demands expensive retraining or fine-tuning, involving substantial real-world data or large amounts of simulated interactions [6].\nFurthermore, retraining carries the risk of catastrophic forgetting, where policies lose previously learned skills when adapting to new or updated task specifications [7, 8].\nBeyond forgetting, a broader limitation arises from the asymptotic performance plateaus often observed in RL policies. Once a policy achieves strong average performance, further optimization usually yields only marginal gains [9, 10].\nClosing this final remaining performance gap (e.g., from 95% to 99.9%) is essential for robust real-world deployment [10].\nThese challenges\nmotivate the need for methods that target refinement of specific policy behaviors, without incurring the costs and risks of full re-optimization.\n\nRecent advances in natural language processing and computer vision have shown that the behavior of learned models can be modified at inference time without retraining [11, 12, 13, 14, 15]. Activation steering and representation engineering allow precise, inference-time interventions in large language models (LLMs) to guide outputs towards desired characteristics. Latent space editing in generative and diffusion models enables fine-grained control over generated content. The application of these ideas to robotics remains unexplored, primarily due to challenges associated with real-world interactions compared to text or static vision outputs.\n\nInspired by these advances, we propose a framework for altering the behavior of learned robot policies at inference time.\nSpecifically, we focus on the problem of multi-quadrotor navigation in obstacle-rich environments, leveraging a pre-trained RL policy [2]. While this policy achieves strong overall performance, it continues to struggle in certain edge cases and more challenging scenarios. Further retraining or architectural changes do not alleviate these failures, underscoring the need for alternative approaches [2].\n\nWe investigate whether targeted latent activation editing (LAE) (Figure 1) during inference can enhance safety, quantified through collision avoidance, without costly retraining or fine-tuning.\nWe define LAE as the process of modifying hidden activations of a network during inference, without altering its trained weights. By intervening directly in the latent space, LAE temporarily adjusts the policy’s internal representations to steer behavior along desired axes, such as safety (fewer collisions) in cluttered environments.\nOur key insight is that LAE is a promising mechanism to reduce undesirable behaviors or enhance specific desired behaviors in pre-trained models. The specific latent dimensions chosen for editing and the underlying editing logic vary depending on the behavior to be influenced.\n\nLAE (Figure 1) operates in two stages. First, we identify undesired states by passing the selected intermediate latent activations through an online behavior classifier. Second, we perform targeted editing of these flagged latent activations using a principled strategy. To promote safer behavior, we hypothesize that artificially amplifying the robot’s internal perception of environmental risk can trigger earlier and more cautious maneuvers, thereby improving collision avoidance.\nTo realize this idea, we propose a latent collision world model (LCWM), an action-free latent world model [16, 17, 18] that predicts how latent activations evolve along trajectories leading to collisions, using the current activation together with a short history of past activations (Sec. IV-C).\nOur experiments show that among multiple baseline editing strategies, LCWM is the most effective, consistently yielding statistically superior safety performance across extensive evaluations.\nFinally, we demonstrate the real-world feasibility and effectiveness of our approach through deployment on Crazyflie quadrotors, establishing LAE as a practical and effective tool for enhancing the safety of pre-trained RL-based multirobot navigation policies.\n\nOur key contributions are as follows:\n\nWe present LAE, a novel plug-in framework that steers pre-trained policies by modifying intermediate activations at inference time, enabling targeted refinement of specific behaviors. This is the first activation-space intervention demonstrated on learned robot policies.\n\nWe instantiate LAE on the task of navigating multiple quadrotors in cluttered environments, focusing on improving the collision avoidance behavior of a pretrained RL policy.\n\nWe demonstrate the efficacy of LAE through large-scale simulation studies and real-world quadrotor experiments, achieving statistically significant safety improvements while remaining feasible on highly resource-constrained robots.\n\nAblation studies show that effective latent editing must preserve activations relating to the robot’s own dynamics to avoid dynamically infeasible behaviors.\n\n1. We present LAE, a novel plug-in framework that steers pre-trained policies by modifying intermediate activations at inference time, enabling targeted refinement of specific behaviors. This is the first activation-space intervention demonstrated on learned robot policies.\n\n2. We instantiate LAE on the task of navigating multiple quadrotors in cluttered environments, focusing on improving the collision avoidance behavior of a pretrained RL policy.\n\n3. We demonstrate the efficacy of LAE through large-scale simulation studies and real-world quadrotor experiments, achieving statistically significant safety improvements while remaining feasible on highly resource-constrained robots.\n\n4. Ablation studies show that effective latent editing must preserve activations relating to the robot’s own dynamics to avoid dynamically infeasible behaviors.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在不重训练的情况下提高多机器人导航的安全性。  \n2. 如何通过修改中间激活来改善预训练策略的特定行为。  \n3. 如何避免在复杂环境中发生碰撞。  \n\n【用了什么创新方法】  \n提出了一种名为Latent Activation Editing (LAE)的框架，通过在线分类器监测中间激活，识别不良状态，并通过激活编辑模块选择性修改激活，以引导策略向更安全的行为转变。该方法在多旋翼导航任务中实现了近90%的碰撞减少，并显著提高了无碰撞轨迹的比例，同时保持了任务完成率。LAE展示了在资源受限的硬件上进行后期优化的可行性和有效性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Uncertainty-Aware Active Source Tracking of Marine Pollution using Unmanned Surface Vehicles",
            "authors": "Song Ma,Richard Bucknall,Yuanchang Liu",
            "subjects": "Robotics (cs.RO)",
            "comment": "Accepted for presentation at Oceantech: Marine Robotics & Science Workshop, IROS 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.20593",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20593",
            "arxiv_html_link": "https://arxiv.org/html/2509.20593v1",
            "abstract": "This paper proposes an uncertainty-aware marine pollution source tracking framework for unmanned surface vehicles (USVs). By integrating high-fidelity marine pollution dispersion simulation with informative path planning techniques, we demonstrate effective identification of pollution sources in marine environments. The proposed approach is implemented based on Robot Operating System (ROS), processing real-time sensor data to update probabilistic source location estimates. The system progressively refines the estimation of source location while quantifying uncertainty levels in its predictions. Experiments conducted in simulated environments with varying source locations, flow conditions, and starting positions demonstrate the framework’s ability to localise pollution sources with high accuracy. Results show that the proposed approach achieves reliable source localisation efficiently. This work contributes to the development of full autonomous environmental monitoring capabilities essential for rapid response to marine pollution incidents.",
            "introduction": "Pollution discharged into the marine environment causes severe consequences to ecosystems [1, 2] and human health [3]. The marine pollution problem is a result of various reasons. According to Biswas et al. [4], coastal waters are endangered by sewerage discharge, agricultural, and industrial waste. Even in the far seas, water bodies also face hazards like oil spills and chemical leaks from various maritime activities [5]. To minimise the influence of marine pollution, reliable monitoring and rapid responses play a crucial role.\n\nIn order to better monitor marine pollution and restrict its influence, the use of robotics systems has attracted much attention, which reduces the risks that humans are exposed to [6]. In particular, the use of robotic systems in environmental monitoring has been widely investigated [7], and it demonstrates significantly higher efficiency compared with conventional solutions [8]. A crucial sub-problem within robotic marine monitoring is the task of localising the pollution source, often referred to as source tracking [9].\n\nEarly approaches to pollution source tracking focused on establishing estimation models of the source locations. For example, Pang and Farrel [10] used Bayesian inference to generate a probability map of the source location in a marine environment, while Hutchinson et al. [11] applied a similar principle to airborne source localisation using a particle filter. A key limitation of these methods is their reliance on external control inputs, such as a predefined search pattern (e.g., a lawnmower pattern), to collect data. Consequently, these methods are not fully autonomous and can be considered as a variant of deploying a fixed sensor network [12].\n\nTo overcome this reliance on predefined paths and achieve full autonomy, research has shifted towards active control strategies, often referred to as active information gathering or Informative Path Planning (IPP) [13]. Several researchers have proposed to incorporate the IPP to substitute the external control in the fields of localisation [14], Simultaneous Localisation and Mapping (SLAM) [15], and 3D reconstruction [16]. However, little attention has been paid to the active marine pollution source tracking. A limited number of existing cases [17, 18] applied IPP for gas source localisation, but the scenarios were restricted to indoor applications where their scales were considerably smaller than in the marine environment. Although Bayat et al. [19] put forward active source tracking for marine pollution, the simulated validation was over-simplistic, and their investigated method for estimating the source location in Bayat et al. [19] was restricted to particle filters. Further study in active source tracking in the marine environment using Bayesian-based estimation and the associated validations based on high-fidelity simulations are necessary.\n\nIn the present paper, an uncertainty-aware active pollution source tracking framework using Bayesian estimation is proposed. The proposed framework can be fully integrated into a carefully constructed high-fidelity marine pollution simulator, as shown in Fig. 1.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在海洋环境中有效定位污染源？  \n2. 如何实现无人表面车辆（USVs）的全自主污染源追踪？  \n3. 如何量化预测中的不确定性以提高源定位的准确性？  \n\n【用了什么创新方法】  \n本研究提出了一种不确定性感知的主动污染源追踪框架，结合高保真海洋污染扩散模拟与信息路径规划技术。通过实时处理传感器数据，系统能够更新源位置的概率估计，并逐步优化定位精度，同时量化预测的不确定性。实验结果表明，该框架在不同源位置和流动条件下，能够高效、准确地定位污染源，显著提升了海洋环境监测的自主能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "GraspFactory: A Large Object-Centric Grasping Dataset",
            "authors": "Srinidhi Kalgundi Srinivas,Yash Shukla,Adam Arnold,Sachin Chitta",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20550",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20550",
            "arxiv_html_link": "https://arxiv.org/html/2509.20550v1",
            "abstract": "Robotic grasping is a crucial task in industrial automation, where robots are increasingly expected to handle a wide range of objects. However, a significant challenge arises when robot grasping models trained on limited datasets encounter novel objects. In real-world environments such as warehouses or manufacturing plants, the diversity of objects can be vast, and grasping models need to generalize to this diversity. Training large, generalizable robot-grasping models requires geometrically diverse datasets. In this paper, we introduce GraspFactory, a dataset containing over 109 million 6-DoF grasps collectively for the Franka Panda (with 14,690 objects) and Robotiq 2F-85 grippers (with 33,710 objects). GraspFactory is designed for training data-intensive models, and we demonstrate the generalization capabilities of one such model trained on a subset of GraspFactory in both simulated and real-world settings. The dataset and tools are made available for download at graspfactory.github.io.",
            "introduction": "Large datasets have been a major contributor to the success of AI models. The fields of Computer Vision and Natural Language Processing have seen tremendous progress due to the presence of internet-scale datasets like ImageNet [1] and Laion-5b [2]. Models such as Chat-GPT [3] and Dall-E[4] demonstrate strong generalization capabilities for tasks that were not explicitly represented in their training data, thanks to the use of diverse training datasets and large-scale transformer-based architectures. Similar efforts have been undertaken in robotics to collect large datasets, such as Open X-Embodiment [5] and DROID [6]. These datasets focus on end-to-end training of robots but there is still a need for task-specific datasets. Robot grasping is one such task, and a generalized grasping model remains elusive, in part due to the lack of geometrically diverse objects in existing datasets. In this work, we present an object-centric grasping dataset that offers greater geometric diversity compared to existing datasets.\n\nCurrently, object-centric grasping datasets [7, 8, 9] and scene-based grasping datasets [10, 11, 12] are mostly geared toward domestic robotics applications. These datasets have been used to train robot grasping models such as [13, 14, 15, 16]. The grasping datasets are generated using 3D CAD models111We use the term “CAD models” in this paper to specifically refer to triangular meshes. from 3D datasets such as Shapenet [17], YCB [18], Objaverse [19] and the Princeton Shape Benchmark [20]. These datasets, however, contain objects of low geometric diversity, as they contain only a small number of semantic classes [8]. Some of the recent advancements in 3D generative models, however, are fueled by larger 3D datasets like those presented in [21, 19, 17]. We leverage one such 3D dataset, ABC-Dataset [21], that contains 1M+ high quality geometric models.\n\nWe introduce GraspFactory, a large-scale dataset of 6-DoF parallel-jaw grasps generated in simulation. The dataset provides two-fingered grasps for the Franka Panda and Robotiq 2F-85 grippers. We utilize a scalable robotics simulation and synthetic data generation tool to annotate the objects with 6-DoF grasps. Further, we train an existing diffusion-based grasping model, SE(3)-DiffusionFields [14] on the Franka Panda subset, and evaluate the model’s generalization capabilities on unseen objects. To the best of our knowledge, this is the largest object-centric grasping dataset containing 6-DoF, parallel-jaw grasps for geometrically diverse 3D data.\n\nOur contributions are as follows:\n\nWe present GraspFactory, a large-scale, object-centric dataset of 6-DoF parallel-jaw grasps with corresponding gripper widths, comprising over 109 million grasps in total. The dataset includes grasps for 33,710 objects randomly selected from the ABC dataset [21] for the Robotiq 2F-85 gripper, and 14,690 objects for the Franka Panda gripper, selected as a subset of the Robotiq object set. As part of ongoing work, we plan to extend the dataset with grasps for additional objects from [21].\n\nWe train a diffusion-based grasp generative model [14] on the Franka Panda subset of GraspFactory, and demonstrate that training on geometrically diverse data improves generalization in both simulation and real-world experiments.\n\nThe rest of the paper is organized as follows: In Section 2, we review prior work. In Section 3, we present the method used for generating the dataset. Section 4 describes the experimental setup, both in simulation and real, and the results from training a model with GraspFactory.\n\n1. We present GraspFactory, a large-scale, object-centric dataset of 6-DoF parallel-jaw grasps with corresponding gripper widths, comprising over 109 million grasps in total. The dataset includes grasps for 33,710 objects randomly selected from the ABC dataset [21] for the Robotiq 2F-85 gripper, and 14,690 objects for the Franka Panda gripper, selected as a subset of the Robotiq object set. As part of ongoing work, we plan to extend the dataset with grasps for additional objects from [21].\n\n2. We train a diffusion-based grasp generative model [14] on the Franka Panda subset of GraspFactory, and demonstrate that training on geometrically diverse data improves generalization in both simulation and real-world experiments.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高机器人抓取模型在新物体上的泛化能力。  \n2. 现有抓取数据集缺乏几何多样性，限制了模型的性能。  \n3. 需要一个大规模、面向对象的抓取数据集来支持训练。  \n\n【用了什么创新方法】  \n本研究提出了GraspFactory，一个包含超过1.09亿个6-DoF抓取的对象中心数据集，专为Franka Panda和Robotiq 2F-85抓手设计。我们利用可扩展的机器人仿真和合成数据生成工具生成抓取数据，并在Franka Panda子集上训练了基于扩散的抓取生成模型SE(3)-DiffusionFields。实验结果表明，使用几何多样性数据进行训练显著提高了模型在未见物体上的泛化能力，验证了该数据集的有效性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning",
            "authors": "Anujith Muraleedharan,Anamika J H",
            "subjects": "Robotics (cs.RO)",
            "comment": "Preprint., 1 algorithm. CoRL 2025 style (preprint). Code/data to be released",
            "pdf_link": "https://arxiv.org/pdf/2509.20541",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20541",
            "arxiv_html_link": "https://arxiv.org/html/2509.20541v1",
            "abstract": "Human feedback can greatly accelerate robot learning, but in real-world settings, such feedback is costly and limited. Existing human-in-the-loop reinforcement learning (HiL-RL) methods often assume abundant feedback, limiting their practicality for physical robot deployment. In this work, we introduce SPARQ, a progress-aware query policy that requests feedback only when learning stagnates or worsens, thereby reducing unnecessary oracle calls. We evaluate SPARQ on a simulated UR5 cube-picking task in PyBullet, comparing against three baselines: no feedback, random querying, and always querying. Our experiments show that SPARQ achieves near-perfect task success, matching the performance of always querying while consuming about half the feedback budget. It also provides more stable and efficient learning than random querying, and significantly improves over training without feedback. These findings suggest that selective, progress-based query strategies can make HiL-RL more efficient and scalable for robots operating under realistic human effort constraints.",
            "introduction": "Robots deployed in the real world must adapt to diverse and dynamic environments while operating under safety and efficiency constraints. HiL-RL has emerged as a powerful paradigm to align robot behavior with human intent by incorporating interactive signals such as evaluative feedback [1], corrective actions[2, 3], and preference comparisons [4]. These approaches accelerate learning in tasks ranging from manipulation to navigation [5, 6]. However, their practicality is limited by the cost of human supervision: attention is a scarce resource, constrained by fatigue, multitasking demands, and operational limitations [6, 7]. Continuous querying for feedback overwhelms supervisors, while too few queries slow adaptation and degrade performance.\n\nTo address this tension, interactive strategies have been proposed where robots query humans only when needed. “Human-gated” approaches allow supervisors to intervene when they see fit [3, 8], but require continuous monitoring and cannot scale to multi-robot settings. “Robot-gated” methods shift the responsibility to the agent, enabling it to request feedback when encountering novel or risky states [9, 10]. While these strategies reduce unnecessary interactions, they typically lack explicit mechanisms for managing strict feedback budgets and often struggle in continuous-control tasks where queries must be carefully timed to avoid disrupting smooth execution.\n\nInspired by how humans naturally allocate their effort rationally, we propose SPARQ (Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning), a budget-aware HiL-RL method. Rather than relying on continuous or uncertainty-driven feedback, SPARQ monitors task progress and selectively requests help only when learning stagnates or worsens. By explicitly modeling human attention as a limited budget and enforcing cooldowns between queries, SPARQ balances learning efficiency with supervision cost. An overview of our approach is illustrated in Fig. 1. Panel (a) shows the SPARQ-augmented training pipeline, where the agent selectively queries a human oracle only when needed. Panel (b) zooms into the SPARQ decision rule, which determines when queries are triggered based on progress, patience, and budget constraints.\n\nOur contributions are as follows:\n\nWe propose SPARQ, a resource-rational HiL-RL method that models human attention as a budgeted resource and allocates queries selectively.\n\nWe introduce a progress-aware query rule that triggers feedback requests based on learning stagnation or deterioration, with patience and cooldown to prevent redundancy.\n\nWe empirically evaluate SPARQ on a simulated UR5 cube-picking task, showing that it matches the success rate of full-feedback baselines while using about half the feedback budget, and performs comparably to Random on this task while providing explicit budget control and smoother temporal allocation of queries.\n\n1. We propose SPARQ, a resource-rational HiL-RL method that models human attention as a budgeted resource and allocates queries selectively.\n\n2. We introduce a progress-aware query rule that triggers feedback requests based on learning stagnation or deterioration, with patience and cooldown to prevent redundancy.\n\n3. We empirically evaluate SPARQ on a simulated UR5 cube-picking task, showing that it matches the success rate of full-feedback baselines while using about half the feedback budget, and performs comparably to Random on this task while providing explicit budget control and smoother temporal allocation of queries.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在有限的人类反馈下提高机器人学习效率？  \n2. 现有的HiL-RL方法在实际应用中受限于人类监督的成本。  \n3. 如何有效管理反馈预算以避免过度查询？  \n\n【用了什么创新方法】  \n本文提出了SPARQ（Selective Progress-Aware Querying），一种资源理性的HiL-RL方法，通过监测任务进展，选择性地请求反馈，仅在学习停滞或恶化时进行查询。SPARQ引入了进展感知查询规则，结合耐心和冷却机制，防止冗余查询。通过在模拟UR5立方体抓取任务中的实验，SPARQ显示出与全反馈基线相当的成功率，同时使用约一半的反馈预算，提供了更稳定和高效的学习效果。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Action-Informed Estimation and Planning: Clearing Clutter on Staircases via Quadrupedal Pedipulation",
            "authors": "Prasanna Sriganesh,Barath Satheeshkumar,Anushree Sabnis,Matthew Travers",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20516",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20516",
            "arxiv_html_link": "https://arxiv.org/html/2509.20516v1",
            "abstract": "For robots to operate autonomously in densely cluttered environments, they must reason about and potentially physically interact with obstacles to clear a path. Safely clearing a path on challenging terrain, such as a cluttered staircase, requires controlled interaction. For example, a quadrupedal robot that pushes objects out of the way with one leg while maintaining a stable stance with its three other legs. However, tightly coupled physical actions, such as one-legged pushing, create new constraints on the system that can be difficult to predict at design time. In this work, we present a new method that addresses one such constraint, wherein the object being pushed by a quadrupedal robot with one of its legs becomes occluded from the robot’s sensors during manipulation. To address this challenge, we present a tightly coupled perception-action framework that enables the robot to perceive clutter, reason about feasible push paths, and execute the clearing maneuver. Our core contribution is an interaction-aware state estimation loop that uses proprioceptive feedback regarding foot contact and leg position to predict an object’s displacement during the occlusion. This prediction guides the perception system to robustly re-detect the object after the interaction, closing the loop between action and sensing to enable accurate tracking even after partial pushes. Using this feedback allows the robot to learn from physical outcomes, reclassifying an object as immovable if a push fails due to it being too heavy. We present results of implementing our approach on a Boston Dynamics Spot robot that show our interaction-aware approach achieves higher task success rates and tracking accuracy in pushing objects on stairs compared to open-loop baselines.",
            "introduction": "Successful real-world deployment of quadrupedal robots requires the ability to autonomously perceive, reason about, and physically navigate unpredictable environments. This requires them to go beyond simple avoidance and actively interact with obstacles, such as pushing objects that block their path. This capability becomes critical in complex terrains like a narrow, cluttered staircase, where a robot can neither treat items as static obstacles to be avoided nor step on them without compromising safety. Instead, the robot must reason about the clutter’s physical properties to determine if a path can be safely cleared through interaction. For a quadruped, this action can be achieved by balancing on three legs while using the fourth to manipulate an object, an act termed “pedipulation”. This demands a tight integration of perception and action, as the robot must manage the physical push while dealing with inevitable perceptual challenges like occlusion. In this paper, we present a complete perception-to-action framework that addresses these challenges, enabling a quadruped robot to safely perceive, reason about, and clear movable clutter from a staircase.\n\nMany robotic pushing methods operate by decoupling perception from execution, an approach that is fundamentally limited in real-world scenarios. For example, during a push, the robot’s motion to maintain balance while pushing causes the robot’s body to tilt, shifting the camera’s field-of-view away from the object on the stair. Additionally, the manipulating leg itself physically obstructs any remaining line of sight. Given these geometric constraints, achieving a consistently unobstructed view through additional sensors is infeasible. This loss of exteroception is consequential because push outcomes are not simple binary events. Factors like noisy state estimation and complex object shapes make partial pushes and foot slips common. Such unpredicted displacements can cause standard visual trackers to fail, leading the system to lose track of an object’s state entirely.\n\nThis failure to track presents a new challenge–reasoning about the cause of a failed interaction. The robot must be able to distinguish whether an attempt failed because the foot missed the object or because it contacted an object too heavy to move. A robust system must be able to resolve this ambiguity in the failed push, and learn from it to intelligently update the robot’s world model allowing it to reclassify an object as static or re-attempt the push.\n\nTo address these challenges, we introduce a perception framework that robustly segments clutter on staircases and maintains a low-dimensional world model of the environment. To overcome the challenge of visual occlusion within this model, it employs an interaction-aware state estimation loop. During a push, this loop uses proprioceptive feedback to predict an object’s displacement, guiding the perception system to re-detect the object and update its state. This predict-correct cycle ensures robust tracking through complex interactions and updates key object properties, such as movability, within the world model. This state estimation is integrated into a hierarchical task execution framework. At the high level, a planner manages the robot’s behavior by switching between navigation and pedipulation modes, while using contact feedback to learn from interactions to reclassify static objects, or retry pushes. It directs a low-level planner to compute safe foot trajectories for object pushing. Finally, a policy trained via reinforcement learning executes these trajectories, generating stable joint commands for the maneuver.\n\nThe main contributions of this work are:\n\nA 3D perception framework that robustly segments and localizes clutter on staircases by leveraging geometric priors of the environment.\n\nAn interaction-aware state estimation pipeline that robustly tracks an object’s state through partial pushes and updates its movability by fusing visual measurements with proprioceptive contact feedback.\n\nA hierarchical planning and control architecture where a planning pipeline computes collision-free foot trajectories for pushing, and a learned pedipulation policy to executes these trajectories.\n\n1. A 3D perception framework that robustly segments and localizes clutter on staircases by leveraging geometric priors of the environment.\n\n2. An interaction-aware state estimation pipeline that robustly tracks an object’s state through partial pushes and updates its movability by fusing visual measurements with proprioceptive contact feedback.\n\n3. A hierarchical planning and control architecture where a planning pipeline computes collision-free foot trajectories for pushing, and a learned pedipulation policy to executes these trajectories.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂环境中实现机器人对障碍物的主动交互以清理路径。  \n2. 如何解决在推物体过程中传感器视野被遮挡的问题。  \n3. 如何通过反馈学习更新物体的可移动性状态。  \n\n【用了什么创新方法】  \n提出了一种紧密耦合的感知-动作框架，利用本体反馈预测物体位移，从而在遮挡情况下实现物体的重新检测。该方法通过交互感知状态估计循环，确保在部分推送过程中保持对物体状态的准确跟踪，并更新物体的可移动性。实现了在复杂环境中更高的任务成功率和跟踪精度，特别是在推物体的过程中，展示了相较于开放循环基线的显著优势。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "MELEGROS: Monolithic Elephant-inspired Gripper with Optical Sensors",
            "authors": "Petr Trunin,Diana Cafiso,Anderson Brazil Nardin,Trevor Exley,Lucia Beccai",
            "subjects": "Robotics (cs.RO)",
            "comment": ". SI . Submitted to Wiley Advanced Science",
            "pdf_link": "https://arxiv.org/pdf/2509.20510",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20510",
            "arxiv_html_link": "https://arxiv.org/html/2509.20510v1",
            "abstract": "The elephant trunk exemplifies a natural gripper where structure, actuation, and sensing are seamlessly integrated. Inspired by the distal morphology of the African elephant trunk, we present MELEGROS, a Monolithic ELEphant-inspired GRipper with Optical Sensors, emphasizing sensing as an intrinsic, co-fabricated capability. Unlike multi-material or tendon-based approaches, MELEGROS directly integrates six optical waveguide sensors and five pneumatic chambers into a pneumatically actuated lattice structure (12.5 mm cell size) using a single soft resin and one continuous 3D print. This eliminates mechanical mismatches between sensors, actuators, and body, reducing model uncertainty and enabling simulation-guided sensor design and placement. Only four iterations were required to achieve the final prototype, which features a continuous structure capable of elongation, compression, and bending while decoupling tactile and proprioceptive signals. MELEGROS (132 g) lifts more than twice its weight, performs bioinspired actions such as pinching, scooping, and reaching, and delicately grasps fragile items like grapes. The integrated optical sensors provide distinct responses to touch, bending, and chamber deformation, enabling multifunctional perception. MELEGROS demonstrates a new paradigm for soft robotics where fully embedded sensing and continuous structures inherently support versatile, bioinspired manipulation.",
            "introduction": "The elephant trunk is a remarkably versatile biological manipulator that integrates sensing and actuation within a jointless structure to grasp objects of many shapes, weights, and sizes. In the African elephant, the trunk tip has two asymmetric finger-like projections enable pinching, scooping, and supporting actions [1, 2, 3]. The absence of any division between the continuum arm and the tip, combined with distributed sensory feedback, supports smooth reaching, grasping, and highly dexterous manipulation tasks (e.g., ripping leaves from a wrapped branch). Unlike engineered systems that separate sensing, actuation, and structure, the elephant trunk links them through the integration of muscles, connective tissue, skin, and embedded mechanoreceptors. This arrangement allows the elephant to control movement and respond efficiently to contact with its environment, even with occluded vision during prehensile activity [4]. In this sense, the trunk is monolithic: structure, actuation, and sensing are inseparable.\n\nThis type of integration is still uncommon in robotic systems. Although today robots can deform and adapt to their surroundings through compliant materials and structures [5, 6, 7], many are built by combining separate sensing, actuation, and structural elements [8, 9, 10]. This gap is related to a broader challenge in soft robotics: the lack of truly monolithic systems that combine all functional elements into a single material body. In particular, this problem stems from the fact that common transduction mechanisms, e.g., resistive [11, 12, 13, 14] and capacitive [15, 16, 17, 18] sensors, depend on conductive materials, with inherent mechanical characteristics (e.g., stiffness) different from the ones typically used for the robot’s body. Moreover, sensing elements are often added post-fabrication. This leads to mechanical mismatches that can compromise compliance and induce failure under cyclic loading. Recent efforts have achieved various degrees of actuator–sensor integration [19]. For example, Truby et al. combined EMB3D printing with molding to produce soft somatosensitive actuators by injecting conductive ionogel into elastomeric matrices [20], while Xiao et al. fabricated a fully 3D-printed robotic hand incorporating soft capacitive sensors via dual-extrusion of dielectric and conductive silicones [21]. In addition to transduction strategies, the pressure feedback from fluidic channels has been leveraged to maintain material uniformity [22], though at the expense of increased design complexity (thin hollow channels) and potential performance trade-offs between sensing and actuation. Although these approaches represent meaningful advances toward the monolithic approach, they still rely on multiple materials, involve elaborate fabrication workflows, and restrict design versatility. To fully eliminate material mismatches and post-assembly procedures, a solution is to build sensors with the same material as the actuators and the robot body, a goal attainable by implementing transducers that exploit the optical, rather than the electrical, conductivity of sensing materials. In fact, the monolithic integration of optical sensing set just one requirement, i.e., the material of both sensors and robot must be transparent to light. This feature, even if challenging, is still less constraining than those required from other transduction mechanisms, which may imply the addition of functional fillers (e.g., magnetic, electrically-conductive) stiffening the robot and creating bi-material interfaces. As a first step, our previous work introduced the Monolithic Perceptive Unit (MPU): a fully 3D-printed lattice cell in which the constituent elastomer itself functions as an optical sensor [23].\n\nLattice architectures have emerged as a promising alternative to bulk soft bodies. Although bulk material functionalization is possible, it often comes at the expense of mechanical performance [24]. In contrast, lattice architectures retain the softness of the bulk material while offering internal pathways and anchor points, which simplify the integration of actuators and sensors and enable support-free fabrication by creating in situ supports during 3D printing. For example, tendon-driven lattices have been used to reproduce musculoskeletal behaviors by routing cables through the structure [25, 26]. Alternatively, we have previously introduced lattice-embedded actuators which can achieve bending and jointless behavior [27]. In this work, an IWP-TPMS (triply periodic minimal surface) lattice is adopted. While the underlying topology defines the deformation modes, the stiffness -in addition to the intrinsic material properties- depends on the cell dimensions. Since the lattice serves as a compliant medium for the embedded actuators, and it must enable the gripper to extend, bend, and conform around objects during grasping, a low bending stiffness of the chosen lattice configuration is pursued.\n\nThe draw of the proposed monolithic method in soft robotics lies in simplifying the design process: using a single-material body that combines actuation and sensing without post-processing. However, few materials are capable of delivering all required functionalities (i.e., flexibility, printability, sensing) while remaining compatible with streamlined fabrication. Recent advances in commercial 3D printing [28] have enabled one-step processes to create hybrid systems, yet fully printed monolithic soft systems are still rare.\n\nInspired by the morphology and behavior of the distal region of the African elephant trunk [2, 4], and enabled by an architected design, we introduce the MELEGROS concept: a Monolithic Elephant-Inspired Gripper with Optical Sensors (Figure 1). The system is built from a soft lattice with smoothly connected bladder-shaped actuators, which not only allow the system to elongate, compress, and bend, but act as the body of both gripper and soft optical sensors. The design does not aim to mimic natural muscular arrangements, but rather focuses on functional integration of actuation and sensing. To achieve this objective, and building on our recent results in simulating soft lattice structures [29], our method is based on a workflow linking design and fabrication through simulation in SOFA (Simulation Open Framework Architecture) [30, 31] to design and position the soft optical sensors. The output is a fully-integrated design that is fabricated via a single 3D-printing process. In this work, we focus on investigating the sensing functionality by targeting the discrimination of exteroceptive from proprioceptive information during grasping tasks. We show how the specific monolithic architecture enables MELEGROS to perform enveloping grasps and delicate pinching maneuvers (extending its functionality beyond simple parallel-jaw closure) and to reach and bend independently in an intrinsic workspace, where a specific object can be reached from multiple directions before being grasped, similar to the natural model.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现感知、驱动和结构的单一材料整合以提高软机器人性能。  \n2. 现有软机器人系统在传感器与驱动器集成方面的局限性。  \n3. 如何通过光学传感器实现更高效的多功能感知。  \n\n【用了什么创新方法】  \n本研究提出MELEGROS，一个灵感来源于非洲象鼻的单体抓手，采用光学传感器与气动驱动器的无缝集成。通过使用单一软树脂材料和连续3D打印技术，MELEGROS实现了传感与驱动的内在结合，消除了机械不匹配，简化了设计流程。该抓手能够进行延伸、压缩和弯曲，执行如夹持、舀取和精细抓取等生物启发的动作，且能够区分外部和内部感知信息。最终，MELEGROS展示了一种新型的软机器人设计理念，支持多功能的生物启发操控。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting",
            "authors": "Boqi Li,Siyuan Li,Weiyi Wang,Anran Li,Zhong Cao,Henry X. Liu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20499",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20499",
            "arxiv_html_link": "https://arxiv.org/html/2509.20499v1",
            "abstract": "With the rapid progress of foundation models and robotics, vision-language navigation (VLN) has emerged as a key task for embodied agents with broad practical applications. We address VLN in continuous environments, a particularly challenging setting where an agent must jointly interpret natural language instructions, perceive its surroundings, and plan low-level actions. We propose a zero-shot framework that integrates a simplified yet effective waypoint predictor with a multimodal large language model (MLLM). The predictor operates on an abstract obstacle map, producing linearly reachable waypoints, which are incorporated into a dynamically updated topological graph with explicit visitation records. The graph and visitation information are encoded into the prompt, enabling reasoning over both spatial structure and exploration history to encourage exploration and equip MLLM with local path planning for error correction. Extensive experiments on R2R-CE and RxR-CE show that our method achieves state-of-the-art zero-shot performance, with success rates of 41% and 36%, respectively, outperforming prior state-of-the-art methods.",
            "introduction": "Vision-language navigation (VLN) is the task in which an embodied agent follows a natural language instruction to navigate through an environment and reach a specified destination. It represents an important capability for embodied AI, with applications ranging from search and rescue to autonomous navigation and daily human–robot interaction. However, VLN remains highly challenging: the agent must not only comprehend natural language instructions but also ground them in its surrounding environment [1, 2, 3]. Existing approaches [4, 5, 6] typically rely on RGB-D sensory inputs, which demand both a detailed understanding of complex visual scenes and robust reasoning over the task context.\n\nEarly research on VLN has largely focused on discrete environment settings, where a navigation graph is predefined and the agent’s movement is restricted to a finite set of checkpoints [1, 2, 7]. More recently, it has been extended to continuous environments, where the agent can move freely [3, 8]. This setting is considerably more challenging and has motivated a line of work that employs waypoint prediction models [9]: candidate waypoints are first generated in the continuous space, and a navigation policy then selects one to pursue. This decomposition is intended to ease the learning process by separating spatial grounding from long-horizon reasoning.\n\nDespite their promise, existing waypoint prediction approaches exhibit drawbacks. First, the choice of input representations remains underexplored. The original waypoint model relied on RGB and depth images; however, subsequent work found that RGB inputs can sometimes degrade performance, while others introduced increasingly complex architectures for RGB-D processing, further increasing model complexity [5, 10]. Second, predicted waypoints are not guaranteed to be reachable from the agent’s current position and may be obstructed by intervening obstacles.\n\nRather than designing waypoint prediction models with increasingly complex sensory inputs and neural architectures, we propose a simplified waypoint prediction model that proves rather effective. Our key insight is that providing raw RGB and depth images introduces large amounts of irrelevant information, which can obscure the underlying spatial relationships and hinder generalization. To address this, we abstract the input representation: depth images are first processed into an obstacle map centered on the agent, which is then used as the sole input to the waypoint predictor. This abstraction encourages the model to focus on the dynamic patterns of obstacle distribution, enabling it to better identify critical regions where waypoints should be generated. Empirically, we find that our approach produces waypoints that are both more feasible and reliable than those generated by prior methods.\n\nWe integrate our proposed waypoint model with a zero-shot navigator based on a multimodal large language model (MLLM) [11]. Zero-shot VLN with large language models (LLMs) has attracted increasing attention in recent years [12, 4, 13, 14, 15], as purely learning-based navigation models still suffer from data scarcity and struggle to generalize to unseen environments. By leveraging the broad knowledge encoded in LLMs, zero-shot approaches enable agents to better interpret both natural language instructions and perceptual inputs.\n\nRecent works in VLN-CE have explored textual and visual prompt design for LLM-based navigators [13, 15, 10], incorporating different forms of contextual information. However, most of these approaches overlook the structure of the topological graph, causing the LLM agent to quickly lose track of which waypoints have already been explored. To address this limitation, we propose a dynamically updated topological graph that explicitly encodes node visitation status. This graph is incorporated into the prompt design, providing the LLM with structured knowledge of explored and unexplored regions, alongside perceptual inputs, thereby enabling more consistent and informed reasoning about the agent’s navigation choices.\n\nWe evaluate our proposed waypoint prediction + MLLM navigation framework in the R2R-CE [8] and RxR-CE [7] datasets, achieving 41% and 36% success rates and outperforming recent zero-shot methods. Our contributions are threefold:\n\nAbtract obstacle-map based waypoint prediction: A lightweight predictor using obstacle maps as input, improving waypoint feasibility and reachability.\n\nTopoGraph-and-VisitInfo aware prompting: A dynamically updated topological graph with visited-node tracking, incorporated into prompts to help the MLLM reason over spatial structure and exploration history.\n\nIntegrated zero-shot VLN framework: A unified system combining our waypoint predictor with an MLLM navigator, achieving state-of-the-art results on both R2R-CE and RxR-CE datasets.\n\n1. Abtract obstacle-map based waypoint prediction: A lightweight predictor using obstacle maps as input, improving waypoint feasibility and reachability.\n\n2. TopoGraph-and-VisitInfo aware prompting: A dynamically updated topological graph with visited-node tracking, incorporated into prompts to help the MLLM reason over spatial structure and exploration history.\n\n3. Integrated zero-shot VLN framework: A unified system combining our waypoint predictor with an MLLM navigator, achieving state-of-the-art results on both R2R-CE and RxR-CE datasets.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在连续环境中实现有效的视觉-语言导航（VLN）。  \n2. 现有的路径预测方法在可达性和复杂性方面的不足。  \n3. 如何利用大语言模型（LLM）进行零-shot导航。  \n\n【用了什么创新方法】  \n本研究提出了一种基于抽象障碍地图的轻量级路径预测模型，简化了输入表示，专注于障碍分布的动态模式，从而提高了路径的可行性和可靠性。我们还设计了一个动态更新的拓扑图，将访问状态编码到提示中，帮助多模态大语言模型（MLLM）进行更有效的空间结构和探索历史推理。通过将这两者结合，我们的零-shot VLN框架在R2R-CE和RxR-CE数据集上实现了41%和36%的成功率，超越了现有的零-shot方法，显示出显著的性能提升。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Revisiting Formal Methods for Autonomous Robots: A Structured Survey",
            "authors": "Atef Azaiez,David A. Anisi,Marie Farrell,Matt Luckcuck",
            "subjects": "Robotics (cs.RO)",
            "comment": "Appeal accepted: MOD-66548 This is an appeal request regarding our submission MOD-65174 - 6681725",
            "pdf_link": "https://arxiv.org/pdf/2509.20488",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20488",
            "arxiv_html_link": "https://arxiv.org/html/2509.20488v1",
            "abstract": "This paper presents the initial results from our structured literature review on applications of Formal Methods (FM) to Robotic Autonomous Systems (RAS). We describe our structured survey methodology; including database selection and associated search strings, search filters and collaborative review of identified papers. We categorise and enumerate the FM approaches and formalisms that have been used for specification and verification of RAS. We investigate FM in the context of sub-symbolic AI-enabled RAS and examine the evolution of how FM is used over time in this field. This work complements a pre-existing survey in this area and we examine how this research area has matured over time. Specifically, our survey demonstrates that some trends have persisted as observed in a previous survey. Additionally, it recognized new trends that were not considered previously including a noticeable increase in adopting Formal Synthesis approaches as well as Probabilistic Verification Techniques.",
            "introduction": "Formal Methods have been incorporated in the software production life cycle since the early adoption of computers.\nRelying solely on testing has been shown to be not enough to guarantee the absence of bugs in software. This quote from Dijkstra in 1969 emphasises that the computer science community needed to develop alternative methods to testing \"Testing shows the presence, not the absence of bugs\"  [6].\nAs technology and aspirations have evolved, the use of Robotic Autonomous Systems (RAS) in safety- and/or mission-critical applications has increased, including in the nuclear [4], aerospace [27], agriculture [3, 2], transport [14] and space domains [8]. These sorts of critical systems among others involving safety and security requirements clearly need to be robustly verified using Formal Methods for specification and verification. The use of Formal Methods (FM) is admitted, recommended and can be required by some standards  [16]. The strong verification provided by potentially combinations of distinct FM and testing approaches is advantageous as it guarantees mathematical proof of correctness. This aides in the assurance process in critical settings and helps to provide various stakeholders with sufficient confidence that the systems function as expected.\nVariousFM approaches have been developed to fulfil specific needs of verification.\n\nA 2019 survey provides an overview of these methods and acts as a guidebook for those seeking to apply FM in RAS [17]. Apart from the obvious benefits to developing reliable software, there is a reciprocal benefit to the FM community: the modularity of RAS, as exemplified in the various middleware by which they are supported, fosters creative and interesting opportunities for examining and demonstrating the efficacy of these FM [7]. These observations have given rise to a novel sub-domain called Formal Methods for Autonomous Systems111https://fmasworkshop.github.io/ and many conferences have held special tracks in related topics since. On the other hand, there are some challenges of applying FM to RAS, namely the complexity of the this kind of systems as they usually combine discrete software logic with continuous physical dynamics and that can lead to scalability issues. Moreover, the dynamic nature of the environment where RAS operate makes it difficult to capture all interactions and uncertainties. last but not least, there can be a gap between the trustworthiness of formal verification results and the expectations of regulatory acquirements  [1].\n\nIn this paper, we present the methodology we adopted to conduct our structured literature survey, initial results which examines how the application of FM to RAS has evolved over time. We analyse which trends have persisted since the original survey [17] and identify emerging trends. We examine the relative use of different formal methods and verification approaches, and discuss the role played by Sub-Symbolic AI (SSAI) (e.g. machine learning). We reflect on potential reasons for these various evolutions, providing insight and set the stage for future development in this field.\n\nNext, we present some related work, while the rest of the paper is structured as follows. In Sect. 2 we describe our survey’s methodology, including the scope and search terms. We present the results in Sect. 3 and discuss the implications of the results in Sect. 4. Finally, Sect. 5 gives our concluding remarks.\n\nOur survey builds on a previous survey of FM applied to autonomous robotic systems [17], though we extend the time frame from 2007—2018, to 2007—2024;\nWe also use a different methodology and work-flow, and used Rayyan [20] 222Rayyan: https://www.rayyan.ai/ a dedicated tool for conducting structured surveys.\nThis gives broader coverage of the literature, including both wider search terms and additional search sources, initially returning 20,764 papers. We examine similar research questions to [17] but with an explicit focus on the trends that have emerged over time, and examining the impact of SSAI.\n\nAs the sub-domain of FM for autonomous systems has evolved and become more popular over time, it is no surprise that other related research efforts exist. These include a manifesto for applicable FM that provides ten principles concerning their use [9]. This project however does not specifically focus on RAS, rather it discusses the use and promotion of FM in practice more generally.\n\nLeahy et al. [15] define three categories of grand challenge for verification of autonomous systems: (1) Requirements and Specifications, (2) Models and Abstractions, and (3) Tools, Techniques and Algorithms.\nRelated work in [22] provides a research roadmap for verification of autonomous systems which points to several of these open challenges and emerging standards in the area.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效地应用Formal Methods (FM)于Robotic Autonomous Systems (RAS)的规范和验证？  \n2. FM在RAS中的应用趋势及其演变如何？  \n3. Sub-Symbolic AI (SSAI)在FM与RAS结合中的角色是什么？  \n\n【用了什么创新方法】  \n本研究采用结构化文献综述的方法，使用Rayyan工具进行文献筛选和分析。通过广泛的文献搜索，涵盖2007年至2024年的相关研究，识别出FM在RAS中的应用趋势及新兴方法。结果表明，Formal Synthesis和Probabilistic Verification Techniques的采用显著增加，反映出FM在自适应系统中的重要性和发展潜力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Boosting LiDAR-Based Localization with Semantic Insight: Camera Projection versus Direct LiDAR Segmentation",
            "authors": "Sven Ochs,Philip Schörner,Marc René Zofka,J. Marius Zöllner",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20486",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20486",
            "arxiv_html_link": "https://arxiv.org/html/2509.20486v1",
            "abstract": "Semantic segmentation of LiDAR data presents considerable challenges, particularly when dealing with diverse sensor types and configurations. However, incorporating semantic information can significantly enhance the accuracy and robustness of LiDAR-based localization techniques for autonomous mobile systems. We propose an approach that integrates semantic camera data with LiDAR segmentation to address this challenge. By projecting LiDAR points into the semantic segmentation space of the camera, our method enhances the precision and reliability of the LiDAR-based localization pipeline.",
            "introduction": "Accurate and robust localization is a fundamental requirement for autonomous vehicles in complex environments, such as urban canyons and parking spaces. LiDAR sensors, with their ability to provide high-resolution depth information, play a crucial role in modern localization systems. However, LiDAR-based localization methods often struggle in challenging conditions, such as feature-sparse environments, dynamic scenes, or adverse weather conditions. To address these limitations, semantic information can be integrated into localization frameworks, enhancing the robustness and accuracy of pose estimation.\n\nRecent advancements in machine learning, particularly deep neural networks (DNN), have led to significant improvements in the semantic segmentation of LiDAR point clouds. These methods can classify point cloud data into meaningful categories, providing valuable contextual information for localization. However, LiDAR-only semantic segmentation approaches face inherent challenges, including sensor sparsity, domain adaptation issues, and computational complexity [1].\n\nIn contrast, leveraging semantic information from camera images, it is possible to improve LiDAR-based localization without solely relying on point cloud segmentation. This paper proposes an alternative approach integrating semantic features from camera images into a LiDAR-based localization pipeline through projection techniques. By mapping semantic labels from camera images onto the LiDAR point cloud, we aim to enhance localization performance without requiring direct LiDAR point cloud segmentation.\n\nThis paper evaluates the proposed method compared to state-of-the-art DNN LiDAR point cloud segmentation techniques. Specifically, we analyze the impact of semantic camera integration on localization accuracy and robustness to environmental variations. Our contributions are as follows:\n\nAn approach for LiDAR-based localization that integrates semantic camera information through projection.\n\nAn approach for LiDAR-based localization that integrates semantic camera information through projection.\n\nA comparative analysis of our method with state-of-the-art machine learning-based semantic segmentation of LiDAR point clouds.\n\nExperimental validation demonstrates the advantages and disadvantages of localization accuracy.\n\nThe remainder of this paper is structured as follows: Section II provides an overview of related work, including machine learning-based LiDAR segmentation and camera-LiDAR fusion techniques. Section III details the proposed methodology, including the semantic projection pipeline. Section IV presents experimental results. Finally, Section V concludes the paper with potential future research directions.\n\n1. An approach for LiDAR-based localization that integrates semantic camera information through projection.\n\n2. A comparative analysis of our method with state-of-the-art machine learning-based semantic segmentation of LiDAR point clouds.\n\n3. Experimental validation demonstrates the advantages and disadvantages of localization accuracy.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提升LiDAR数据的语义分割精度以增强定位系统的准确性和鲁棒性。  \n2. 在复杂环境中，LiDAR定位方法如何克服特征稀疏和动态场景等挑战。  \n3. 如何有效整合来自相机的语义信息以改善LiDAR定位性能。  \n\n【用了什么创新方法】  \n本研究提出了一种将相机的语义信息通过投影技术整合到LiDAR定位管道中的方法。该方法通过将LiDAR点映射到相机的语义分割空间，提升了LiDAR定位的精度和可靠性。与传统的LiDAR点云分割方法相比，实验结果表明，整合语义相机信息能够显著改善定位的准确性和对环境变化的鲁棒性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences",
            "authors": "Julius Pesonen,Arno Solin,Eija Honkavaara",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20906",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20906",
            "arxiv_html_link": "https://arxiv.org/html/2509.20906v1",
            "abstract": "3D object localisation based on a sequence of camera measurements is essential for safety-critical surveillance tasks, such as drone-based wildfire monitoring.\nLocalisation of objects detected with a camera can typically be solved with dense depth estimation or 3D scene reconstruction.\nHowever, in the context of distant objects or tasks limited by the amount of available computational resources, neither solution is feasible.\nIn this paper, we show that the task can be solved using particle filters for both single and multiple target scenarios.\nThe method was studied using a 3D simulation and a drone-based image segmentation sequence with global navigation satellite system (GNSS)-based camera pose estimates.\nThe results showed that a particle filter can be used to solve practical localisation tasks based on camera poses and image segments in these situations where other solutions fail. The particle filter is independent of the detection method, making it flexible for new tasks.\nThe study also demonstrates that drone-based wildfire monitoring can be conducted using the proposed method paired with a pre-existing image segmentation model.",
            "introduction": "This work addresses the problem of locating distant objects from a series of camera-based detections from known locations and orientations. At a glance, the problem of locating target objects based on multiperspective imagery seems fairly well-addressed. The earlier proposed methods typically operate as optimisation problems where the object locations of individual keypoints are determined using a set of images and known camera poses. Alternatively, full scene 3D reconstruction or dense depth estimation methods have been used for similar problems.\n\nHowever, the task at hand, presented in this work, is specific to far-away objects which have been detected with separate image segmentation models, such as neural networks which inherently produce noisy segments. Other sources of noise occur from possible target object dynamics and the camera-pose estimation methods.\nIn addition, due to the nature of the target detection models, direct correspondence between features from consecutive frames does not hold.\n\nThe motivation for this work originates from drone-based wildfire detection, in which the position of the drone-carried camera is estimated using GNSS measurements and known dynamics of the drone camera setup. Our previous work showed that wildfire smoke can be detected from almost ten kilometres away using only drone-carried resources [Pesonen_2025_WACV]. Pairing the segmentation model with a lightweight target localisation method enables fully on-board wildfire detection and localisation.\nThis enables the wildfire detection system to be used in areas of poor telecommunication where cloud-based computing can not be relied on.\nThe sketch in Figure 1 illustrates a use case of a UAV scanning for wildfires (sketch by DALL-E 3) with masked RGB images (real data).\n\nTo combat the noise induced by the camera and image observation systems, we focus on Bayesian filters. They enable reliable modelling of various noise sources, and with particle filters in particular, more complex observation and target dynamics can be modelled. The use of particle filters in camera-based 3D localisation literature has been limited. Specifically, for locating distant objects from a moving camera, there appears to be a lack of extensive research.\n\nThis work extends the literature on filter-based target object localisation from a moving camera by addressing the problem of distant objects detected by separate models.\nWe propose using particle filters to iteratively improve the localisation and uncertainty estimation of the target object’s 3D position.\nUsing both simulations and a drone-captured image sequence with GNSS-estimated camera positions, we show that the method can locate multiple target objects from moving camera segmentation sequences despite the various sources of noise.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何从噪声摄像机运动和语义分割序列中定位远处物体。  \n2. 现有的3D重建和深度估计方法在远处物体定位中不适用。  \n3. 如何处理由于目标动态和相机姿态估计引起的噪声。  \n\n【用了什么创新方法】  \n本研究提出了一种基于粒子滤波器的方法，用于从已知位置和方向的摄像机检测序列中定位远处物体。该方法通过迭代改进目标物体的3D位置和不确定性估计，能够有效应对图像分割模型产生的噪声。通过在3D模拟和无人机捕获的图像序列中验证，结果表明该方法在多目标场景中表现出色，能够在其他解决方案失效的情况下进行可靠定位。该方法的灵活性使其能够适应新的任务，特别是在无人机监测野火等安全关键任务中。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning",
            "authors": "Yufan Mao,Hanjing Ye,Wenlong Dong,Chengjie Zhang,Hong Zhang",
            "subjects": "Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20754",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20754",
            "arxiv_html_link": "https://arxiv.org/html/2509.20754v1",
            "abstract": "Navigating complex environments requires robots to effectively store observations as memories and leverage them to answer human queries about spatial locations—a critical yet underexplored research challenge. While prior work has made progress in constructing robotic memory, few have addressed the principled mechanisms needed for efficient memory retrieval and integration. To bridge this gap, we propose Meta-Memory, a large language model (LLM)-driven agent that constructs a high-density memory representation of the environment. The key innovation of Meta-Memory lies in its capacity to retrieve and integrate relevant memories through joint reasoning over semantic and spatial modalities in response to natural language location queries, thereby empowering robots with robust and accurate spatial reasoning capabilities. To evaluate its performance, we introduce SpaceLocQA, a large-scale dataset encompassing diverse real-world spatial question-answering scenarios. Experimental results show that Meta-Memory significantly outperforms state-of-the-art methods on both the SpaceLocQA and the public NaVQA benchmarks. Furthermore, we successfully deployed Meta-Memory on real-world robotic platforms, demonstrating its practical utility in complex environments. Project page: https://itsbaymax.github.io/meta-memory.github.io/.",
            "introduction": "A critical capability in robot navigation is the ability to understand and reason about human queries in order to identify and provide spatial locations as navigation destinations. This requires robots to construct a comprehensive memory from their sensory observations and to perform effective retrieval and reasoning over this memory. However, due to the complexity of real-world environments, building a complete and coherent visual representation from sequential observations remains a significant challenge. Even more demanding is the task of effectively utilizing the constructed memory for accurate spatial reasoning. In this work, we formalize these challenges as the Spatial Localization Question-Answering (SLQA) task—aiming to enable robots to build a holistic memory of large, complex environments and leverage it to answer diverse natural language queries about spatial relationships and locations. To the best of our knowledge, this is the first work to formally define the SLQA task.\n\nCurrent approaches to robot memory construction typically rely on captions generated by Vision-Language Models (VLMs) [1, 2, 3] or semantic embeddings extracted from foundational models [4, 5]. These low-level representations are then used to build higher-level structures such as 3D scene graphs [6, 7, 8, 9], semantic maps [10, 11, 12], or structured databases [13]. However, this paradigm faces several critical limitations.\n\nFirst, significant information loss occurs during the encoding process. Even highly detailed captions and semantic embeddings inevitably fail to capture the full richness of raw sensory observations, leading to incomplete and potentially ambiguous memory representations.\n\nMore importantly, there is a notable lack of principled mechanisms for effective memory retrieval and integration—a core requirement for the SLQA task. Existing methods often treat memories as isolated fragments, lacking coherent organization or contextual linking. While some approaches, such as 3D scene graphs [14, 15] or topological maps [16], impose graph-based structures to encode spatial and semantic relationships, they still struggle with complex, multi-hop spatial reasoning. For instance, when a stranger on campus asks, “Where’s the nearest coffee shop southwest of here?”, humans effortlessly retrieve the relevant memories, plan a route, and translate it into directions—an intuitive demonstration of the cognitive map [17] in action. In contrast, current robotic systems remain far from achieving such flexible, context-aware spatial understanding.\n\nRecent work [18] demonstrates that explicitly generating cognitive maps can significantly enhance the spatial reasoning capabilities of Multimodal Large Language Models (MLLMs). Inspired by this insight, we propose Meta-Memory, an LLM-based agent designed for the SLQA task. Our core idea is to enable the LLM agent to dynamically generate task-specific cognitive maps through comprehensive semantic and spatial retrieval over a rich, structured memory repository. To construct this memory system, we store the robot’s raw sensory observations—comprising images and corresponding positions of the robot—as semantic-spatial memories. Building upon this foundation, the agent is equipped with two memory retrieval tools (semantic and spatial) and one memory integration tool. These tools facilitate thorough and fine-grained memory access, enabling the integration tool to synthesize a tailored cognitive map for each query. This structured representation strengthens the agent’s spatial reasoning, leading to more accurate and contextually grounded responses. As illustrated in Fig. 1, we present a complete reasoning pipeline of Meta-Memory, from perception and memory retrieval to cognitive map generation and response.\n\nTo evaluate our method, we conduct experiments on the spatial position questions from the NaVQA dataset[13]. To more rigorously assess the performance of various approaches on SLQA, we introduce SpaceLocQA, a new benchmark dataset. SpaceLocQA contains a broader and more diverse collection of real-world human spatial queries, enabling a comprehensive evaluation of a robot’s ability to effectively construct, retrieve, and integrate its memories for accurate spatial inference.\n\nThe key contributions of this paper are:\n\nMeta-Memory, an LLM agent capable of comprehensively retrieving the constructed memories and effectively integrating the retrieved memories for spatial reasoning.\n\nSpaceLocQA, a comprehensive spatial-localization dataset that encompasses diverse, realistic human location queries.\n\nWe deploy Meta-Memory on a physical robot and demonstrate its practical success at retrieving and integrating semantic-spatial memories in real-world environments.\n\n1. Meta-Memory, an LLM agent capable of comprehensively retrieving the constructed memories and effectively integrating the retrieved memories for spatial reasoning.\n\n2. SpaceLocQA, a comprehensive spatial-localization dataset that encompasses diverse, realistic human location queries.\n\n3. We deploy Meta-Memory on a physical robot and demonstrate its practical success at retrieving and integrating semantic-spatial memories in real-world environments.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效存储和检索机器人在复杂环境中的观察记忆。  \n2. 如何利用构建的记忆进行准确的空间推理以回答人类查询。  \n3. 如何解决现有方法在记忆检索和整合中的缺陷。  \n\n【用了什么创新方法】  \n提出了Meta-Memory，一个基于大语言模型（LLM）的智能体，能够构建高密度的环境记忆表示。该方法通过对语义和空间模态的联合推理，动态生成任务特定的认知地图，从而增强机器人的空间推理能力。实验结果表明，Meta-Memory在SpaceLocQA和NaVQA基准测试中显著优于现有方法，并成功在真实机器人平台上部署，展示了其在复杂环境中的实用性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration",
            "authors": "Yiyuan Pan,Zhe Liu,Hesheng Wang",
            "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20648",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20648",
            "arxiv_html_link": "https://arxiv.org/html/2509.20648v1",
            "abstract": "Autonomous exploration in complex multi-agent reinforcement learning (MARL) with sparse rewards critically depends on providing agents with effective intrinsic motivation. While artificial curiosity offers a powerful self-supervised signal, it often confuses environmental stochasticity with meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform novelty bias, treating all unexpected observations equally. However, peer behavior novelty, which encode latent task dynamics, are often overlooked, resulting in suboptimal exploration in decentralized, communication-free MARL settings. To this end, inspired by how human children adaptively calibrate their own exploratory behaviors via observing peers, we propose a novel approach to enhance multi-agent exploration. We introduce Cermic, a principled framework that empowers agents to robustly filter noisy surprise signals and guide exploration by dynamically calibrating their intrinsic curiosity with inferred multi-agent context. Additionally, Cermic generates theoretically-grounded intrinsic rewards, encouraging agents to explore state transitions with high information gain. We evaluate Cermic on benchmark suites including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that exploration with Cermic significantly outperforms SoTA algorithms in sparse-reward environments.",
            "introduction": "Achieving effective exploration in complex Multi-Agent Reinforcement Learning (MARL) settings, particularly those characterized by sparse rewards and partial observability, remains a formidable scientific challenge [7, 43]. Intrinsic motivation, instantiated as artificial curiosity, has emerged as a key ingredient for unlocking autonomous learning by providing self-supervised signals in the absence of immediate extrinsic feedback [29, 38]. This internal drive enables agents to acquire skills and knowledge that support robust, adaptive intelligence.\n\nHowever, such novelty-seeking algorithms is susceptible to the stochastic environment dynamics or other unlearnable noises (the “Noisy-TV” problem) [23]. Existing algorithms mitigate this challenge through uncertainty quantification or by exploiting global information in multi-agent systems. However, these strategies prove insufficient for intelligent agents, particularly heterogeneous ones or those in large-scale systems: Firstly, such agents frequently encounter severe partial observability, rendering inaccurate uncertainty estimates due to insufficient replay experiences [20]; Secondly, in decentralized execution without effective communication, agents struggle to form accurate beliefs about others’ latent states, undermining methods that presuppose shared inter-agent information. [13]. Altogether, these limitations highlight a critical need for exploration mechanisms that are robust to partial observable and communication-less environments.\n\nInsights from human cognitive development suggest a pathway forward: children rapidly adapt to new social games not only through solo trial-and-error, but also by observing peers, inferring intentions, and selectively imitating successful strategies [42, 16]. Such form of social learning, often driven by an innate curiosity about ‘why’ others act as they do, allows for swift coordination and an understanding of task dynamics, even without complete information or explicit instruction [21, 30]. Naturally, the success of this human-centric learning process motivates translating its core principles to Multi-Agent Systems (MAS). Therefore, this paper seeks to answer:\n\nTo this end, we propose Curiosity Enhancement via Robust Multi-Agent Intention Calibration (Cermic), a modular, plug-and-play component designed to augment existing MARL exploration algorithms. Based on the Information Bottleneck (IB) principle [41, 40], Cermic learns a multi-agent contextualized exploratory representation that steers exploration toward semantically meaningful novelty, and filters unpredictable and spurious novelty. Specifically, it incorporates a graph-based module to model the inferred intentions of surrounding agents and use the context to calibrate raw individual curiosity signal at a given coverage level. At each episode, Cermic yields a loss for self-training and a theoretically-grounded intrinsic reward for exploration. We empirically validate Cermic by integrating it with various MARL algorithms and evaluating its performance across challenging benchmark suites. In summary, our contributions are threefold:\n\nWe introduce Cermic, a novel framework that empowers MARL agents with socially contextualized curiosity. Inspired by developmental psychology, this offers a novel perspective on the crucial challenges of effective exploration in sparse-reward settings.\n\nWe propose a robust and controllable multi-agent calibration mechanism in challenging partially observable and communication-limited environments. Cermic allows for adaptive tuning based on the learned reliability of the intention graph, effectively dampening exploration instability often plaguing vanilla novelty-seeking agents.\n\nWe deliver Cermic as a lightweight, readily integrable module and demonstrate consistent gains over strong baselines across standard benchmarks under sparse rewards.\n\n1. We introduce Cermic, a novel framework that empowers MARL agents with socially contextualized curiosity. Inspired by developmental psychology, this offers a novel perspective on the crucial challenges of effective exploration in sparse-reward settings.\n\n2. We propose a robust and controllable multi-agent calibration mechanism in challenging partially observable and communication-limited environments. Cermic allows for adaptive tuning based on the learned reliability of the intention graph, effectively dampening exploration instability often plaguing vanilla novelty-seeking agents.\n\n3. We deliver Cermic as a lightweight, readily integrable module and demonstrate consistent gains over strong baselines across standard benchmarks under sparse rewards.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂的多智能体强化学习（MARL）环境中实现有效的探索，尤其是在稀疏奖励的情况下。  \n2. 现有的好奇心机制如何受到环境随机性和统一新奇偏见的影响，导致探索效率低下。  \n3. 如何通过观察同伴行为来增强智能体的内在动机，从而改善探索策略。  \n\n【用了什么创新方法】  \n提出了一种名为Cermic的框架，通过动态校准多智能体的内在好奇心，来增强探索能力。该方法基于信息瓶颈原则，利用图形模块建模周围智能体的意图，并通过上下文信息来过滤噪声信号。Cermic生成理论基础的内在奖励，鼓励智能体探索具有高信息增益的状态转移。实验证明，Cermic在稀疏奖励环境中显著优于现有的最先进算法。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "Large Pre-Trained Models for Bimanual Manipulation in 3D",
            "authors": "Hanna Yurchyk,Wei-Di Chang,Gregory Dudek,David Meger",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid Robots",
            "pdf_link": "https://arxiv.org/pdf/2509.20579",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20579",
            "arxiv_html_link": "https://arxiv.org/html/2509.20579v1",
            "abstract": "We investigate the integration of attention maps from a pre-trained Vision Transformer into voxel representations to enhance bimanual robotic manipulation. Specifically, we extract attention maps from DINOv2, a self-supervised ViT model, and interpret them as pixel-level saliency scores over RGB images. These maps are lifted into a 3D voxel grid, resulting in voxel-level semantic cues that are incorporated into a behavior cloning policy. When integrated into a state-of-the-art voxel-based policy, our attention-guided featurization yields an average absolute improvement of 8.2% and a relative gain of 21.9% across all tasks in the RLBench bimanual benchmark.",
            "introduction": "Robotic manipulation remains a core challenge in intelligent robotics [1, 2]. Even in well-structured settings, reliable manipulation requires seamless integration of visual perception, geometric reasoning, task planning, and closed-loop control [3]. These challenges are accentuated in bimanual manipulation, where two robot arms must operate in close coordination while sharing the same workspace. Bimanual systems unlock interactions beyond those of single-arm platforms, including simultaneous grasping, handovers, and in-hand re-orientation of a manipulated object [4]. Depending on the system, each arm or a global agent must reason about both the global scene and the fine-grained affordances of the object it handles, while coordinating with the other arm. Importantly, bimanual dexterity is crucial for humanoid and mobile robots with a camera and a dual-arm set-up. Such robots will be expected to perform assistive tasks such as folding laundry [5, 6, 7], stocking shelves [8], and providing other assistance in unstructured human environments [9, 10, 3]. Humanoids and mobile platforms with manipulators must reason about the global scene while manipulating small objects precisely with each hand, without self-collision. Thus, they impose even stricter requirements on perception and control than conventional industrial manipulators.\n\nIn this paper, we study how semantic information from pre-trained Vision Transformers (ViTs) can enhance voxel-based representations for bimanual manipulation. Building on recent advances in visual reasoning, prior work has shown that features extracted from models like DINO [11] and CLIP [12] significantly improve downstream performance in tasks such as navigation [13] and localization [14], when lifted from 2D images into 3D representations. Since ViTs capture meaningful semantic cues from raw visual inputs, we hypothesize that embedding these features into structured 3D scene representations will improve performance in robotic manipulation.\n\nThis trend has also been explored in imitation learning for manipulation. For instance, Chang et al. [15] showed that DINO-derived keypoints can establish homeomorphic correspondences across object instances, improving generalization in grasping. Di Palo et al. [16] leveraged these keypoints to train a policy from a single demonstration that generalizes effectively to real-world deployment. More broadly, recent works demonstrate that augmenting geometric representations with semantic priors improves the effectiveness of both single and dual-arm policies [17, 18, 19, 20]. Our work continues on this trend by injecting ViT-derived semantic attention into voxel-based policies, improving task performance without modifying the downstream architecture.\n\nWe build on the voxel-based manipulation framework introduced by James et al. [21], who proposed the C2F-ARM policy for single-arm manipulation using coarse-to-fine spatial attention, along with the RLBench benchmark [22]. Shridhar et al. extended this line with PerAct [23], a transformer-based behavioral cloning (BC) agent operating on voxel inputs. More recent methods have further developed voxel policies: Act3D [17] introduced a continuous 3D feature field with coarse-to-fine attention over sampled 3D points, while RVT [24] proposed a multi-view transformer that fuses re-rendered viewpoints to increase robustness to camera viewpoint variations.\n\nIn the bimanual setting, VoxAct-B [25] and PerAct2 [26] extend RLBench to support dual-arm manipulation. We adopt VoxAct-B as our baseline since it incorporates several practical improvements such as role identifiers and the use of the Segment Anything Model (SAM) [27] to crop task-relevant regions.\n\nThese works support a broader insight: combining structured 3D scene representations with high-level semantic priors leads to more robust and generalizable manipulation policies. We propose a lightweight voxel featurization strategy that injects DINOv2 [28] derived semantic priors from a single attention head into voxel-based policy learning, generating per-voxel semantic cues for bimanual manipulation. Our main contributions are:\n\nA pre-processing method that injects ViT-derived attention features into 3D voxel grids with minimal modifications to existing policy frameworks.\n\nExperiments showing that, when combined with VoxAct-B [25], our method achieves an absolute improvement of 8.2% and a relative improvement of 21.9% across all tasks in the bimanual RLBench.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将预训练的视觉变换器（ViT）中的注意力图与体素表示结合，以增强双手机器人操作的性能？  \n2. 在双手操作中，如何有效地利用语义信息来提高行为克隆策略的表现？  \n\n【用了什么创新方法】  \n本研究提出了一种轻量级的体素特征化策略，通过将DINOv2模型提取的语义注意力特征注入到3D体素网格中，增强了双手操作的表现。具体流程包括从RGB图像中提取注意力图，并将其提升至3D体素网格，生成每个体素的语义线索。实验结果表明，该方法在与VoxAct-B结合时，平均绝对提升8.2%，相对提升21.9%，显著改善了在RLBench双手基准测试中的任务表现。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent",
            "authors": "Yandan Yang,Baoxiong Jia,Shujie Zhang,Siyuan Huang",
            "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "Accepted by NeurIPS 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.20414",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20414",
            "arxiv_html_link": "https://arxiv.org/html/2509.20414v1",
            "abstract": "Indoor scene synthesis has become increasingly important with the rise of Embodied AI, which requires 3D environments that are not only visually realistic but also physically plausible and functionally diverse. While recent approaches have advanced visual fidelity, they often remain constrained to fixed scene categories, lack sufficient object-level detail and physical consistency, and struggle to align with complex user instructions. In this work, we present SceneWeaver, a reflective agentic framework that unifies diverse scene synthesis paradigms through tool-based iterative refinement. At its core, SceneWeaver employs a language model-based planner to select from a suite of extensible scene generation tools, ranging from data-driven generative models to visual- and LLM-based methods, guided by self-evaluation of physical plausibility, visual realism, and semantic alignment with user input. This closed-loop reason-act-reflect design enables the agent to identify semantic inconsistencies, invoke targeted tools, and update the environment over successive iterations. Extensive experiments on both common and open-vocabulary room types demonstrate that SceneWeaver not only outperforms prior methods on physical, visual, and semantic metrics, but also generalizes effectively to complex scenes with diverse instructions, marking a step toward general-purpose 3D environment generation.",
            "introduction": "3D scene synthesis Qi_2018_CVPR ; tang2023diffuscene ; yang2024physcene ; wei2023legonet ; echoscene ; yang2025mmgdreamer ; ccelen2024design ; sun2024layoutvlm ; feng2024layoutgpt ; Yang_2024_CVPR ; fu2024anyhome  has been a long-standing research topic in computer vision and graphics, primarily focused on generating visually realistic 3D environments for applications such as interior design, virtual content creation, and gaming asset creation. With the recent rise of  embodied artificial intelligence (EAI), the scope of scene synthesis has naturally expanded to accommodate new functional demands procthor ; khanna2023hssd ; yang2024physcene . Beyond achieving visual realism, scenes are now expected to be physically interactable within simulators and precisely controllable in response to task-specific user instructions, particularly in constructing tailored environments for training and evaluating embodied agents. These extended requirements pose significant new challenges for 3D scene synthesis.\n\nDespite rapid progress, existing methods fall short of holistically addressing the requirements for realistic, controllable, and physically plausible scene synthesis, as summarized in Tab.˜1. Rule-based systems procthor ; infinigen2024indoors  ensure physical validity through hand-crafted constraints, but lack extensibility across diverse scene types and offer limited controllability due to their rigid, manually defined logic. Data-driven generative learning methods paschalidou2021atiss ; tang2023diffuscene ; yang2024physcene , while more flexible, are constrained by the scarcity of high-quality, scene-level 3D datasets (e.g., 3D-Front fu20213d ). As a result, they typically produce visually realistic scenes within pre-defined categories but generalize poorly to novel scene types or layout instructions. Methods based on Large Language Models approaches ccelen2024design ; sun2024layoutvlm ; feng2024layoutgpt ; Yang_2024_CVPR ; fu2024anyhome  offer stronger open-vocabulary understanding and semantic flexibility, yet often struggle with spatial reasoning and 3D awareness, resulting in physically implausible rearrangements. Collectively, these limitations highlight that no single approach is sufficient to meet the combined demands of realism, physical plausibility, and controllability. This motivates the need for a comprehensive and adaptable scene synthesis framework capable of synthesizing high-quality 3D scenes.\n\nInspired by recent advances in LLM-based agents, which demonstrate strong reasoning and planning capabilities in complex tasks, recent works in 3D scene synthesis have begun to move beyond monolithic approaches by decomposing the generation process into sequential compositions of modular synthesis components, forming multi-step pipelines coordinated by LLMs. A common strategy starts with generating coarse, scene-level layouts through interaction with LLMs Yang_2024_CVPR ; sun2024layoutvlm ; feng2024layoutgpt ; ccelen2024design , followed by progressive refinement using pre-trained 2D generative models or Multi-modal LLMs for asset generation wangarchitect ; zhou2024gala3d , object placement yu2025metascenes ; dai2024acdc ; ling2025scenethesis , and texture inpainting fu2024anyhome ; chen2024scenetex . While these pipelines leverage both the specialization of individual models and the semantic flexibility of MLLMs, they remain largely “static”, i.e., their planning and execution are governed by fixed prompts and hard-coded module invocation logic over a limited set of synthesis tools. This design overlooks the potential to couple reasoning with adaptive decision-making based on generation feedback, and the ability to seamlessly integrate diverse synthesis tools through a unified interface. As a result, these systems fall short of enabling self-refining and extensible agents, leaving the full potential of multi-modal foundation models underutilized.\n\nTo address the aforementioned challenges, we propose SceneWeaver, a reflective agentic framework that enables MLLMs to synthesize 3D scenes in a self-refining manner through a set of easily extensible tool interfaces. Specifically, SceneWeaver consists of two core components: 1) a standardized and extensible tool interface that abstracts diverse scene synthesis methods into modular tools operating at different levels of generation granularity; 2) a self-reflective planner that dynamically selects tools and iteratively refines the scene by reasoning over feedback from previous generations, while applying the planned modifications and enforcing physical plausibility with a physics-aware executor. This framework enables closed-loop, feedback-driven scene evolution, where the agent identifies areas for improvement, invokes appropriate tools, and updates the scene under physical constraints. Extensive experiments show that SceneWeaver achieves new state-of-the-art across a broad range of scene types and open-vocabulary instructions, demonstrating strong visual realism, physical plausibility, and precision in instruction following. We also provide ablation studies showing that the self-refining design is critical to achieving high-quality scene synthesis and that integrating diverse tools leads to significant performance improvement compared to monolithic approaches. In summary, our contributions are as follows:\n\nWe propose SceneWeaver, the first reflective agentic framework for 3D scene synthesis, enabling MLLMs to iteratively refine scenes through feedback-driven planning with modular tools.\n\nWe propose SceneWeaver, the first reflective agentic framework for 3D scene synthesis, enabling MLLMs to iteratively refine scenes through feedback-driven planning with modular tools.\n\nSceneWeaver introduces a comprehensive reason-act-reflect paradigm that formalizes the planner’s decision making, reflection, and action protocols, along with a standardized and extensible tool interface for synergizing diverse scene synthesis methods based on their respective strengths.\n\nExtensive experiments on open-vocabulary scene synthesis demonstrate that SceneWeaver outperforms existing methods in both visual realism, physical plausibility, and instruction following. We also provide meticulously designed ablation studies to highlight the effectiveness of the proposed reflective agentic framework.\n\n1. We propose SceneWeaver, the first reflective agentic framework for 3D scene synthesis, enabling MLLMs to iteratively refine scenes through feedback-driven planning with modular tools.\n\n2. SceneWeaver introduces a comprehensive reason-act-reflect paradigm that formalizes the planner’s decision making, reflection, and action protocols, along with a standardized and extensible tool interface for synergizing diverse scene synthesis methods based on their respective strengths.\n\n3. Extensive experiments on open-vocabulary scene synthesis demonstrate that SceneWeaver outperforms existing methods in both visual realism, physical plausibility, and instruction following. We also provide meticulously designed ablation studies to highlight the effectiveness of the proposed reflective agentic framework.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现高质量的3D场景合成，满足视觉真实、物理可行性和功能多样性的要求？  \n2. 现有方法在场景合成中缺乏灵活性和可扩展性，难以处理复杂用户指令。  \n3. 如何将多种合成工具有效整合，以实现自我反思和自我改进的场景生成？  \n\n【用了什么创新方法】  \n本文提出了SceneWeaver，一个反思性代理框架，通过工具驱动的迭代优化实现3D场景合成。核心方法包括一个基于语言模型的规划器，选择多种可扩展的场景生成工具，并通过自我评估物理可行性、视觉真实感和与用户输入的语义一致性来指导生成过程。该框架采用闭环的推理-行动-反思设计，使代理能够识别语义不一致，调用目标工具，并在多次迭代中更新环境。实验表明，SceneWeaver在物理、视觉和语义指标上超越了现有方法，并能有效泛化到复杂场景和多样指令，标志着朝向通用3D环境生成的一步。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-26 05:37:59",
            "title": "SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment",
            "authors": "Binod Singh,Sayan Deb Sarkar,Iro Armeni",
            "subjects": "Graphics (cs.GR); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.20401",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.20401",
            "arxiv_html_link": "https://arxiv.org/html/2509.20401v1",
            "abstract": "Aligning 3D scene graphs is a crucial initial step for several applications in robot navigation and embodied perception. Current methods in 3D scene graph alignment often rely on single-modality point cloud data and struggle with incomplete or noisy input. We introduce SGAligner++, a cross-modal, language-aided framework for 3D scene graph alignment. Our method addresses the challenge of aligning partially overlapping scene observations across heterogeneous modalities by learning a unified joint embedding space, enabling accurate alignment even under low-overlap conditions and sensor noise. By employing lightweight unimodal encoders and attention-based fusion, SGAligner++ enhances scene understanding for tasks such as visual localization, 3D reconstruction, and navigation, while ensuring scalability and minimal computational overhead.\nExtensive evaluations on real-world datasets demonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40% on noisy real-world reconstructions, while enabling cross-modal generalization.",
            "introduction": "3D scene understanding is a foundational challenge in robotics and computer vision [1, 2] and serves as the basis for applications in mixed reality, robot navigation, and embodied perception. Recently, cross-modal approaches [3, 4] have gained significant attention due to their ability to bridge different types of data. In particular, tasks such as robot navigation [5], object-centric planning [6], and semantic SLAM [2] require accurate and consistent scene understanding across modalities. As robots operate in dynamic, real-world environments, multimodal fusion becomes crucial. Different sensory data–visual, depth, and textual information–capture distinct aspects of the scene. Fusing these modalities into a coherent spatial-semantic map enables better perception, decision-making, and interaction.\n\nThe 3D semantic scene graph is a structured representation that captures the context of the scene as an attributed and directed graph, enabling unified spatial understanding [7, 8, 9]. Recent work has explored the alignment of 3D scene graphs to support downstream tasks such as 3D registration [10, 11, 12] and visual localization [4]. These capabilities are especially important in robot navigation, for example, when a robot scanning an environment attempts to match current observations with a prior map generated from a different sensing modality. In such cases, alignment remains highly challenging, for instance, when fusing LiDAR scans with CAD-based graphs or grounding objects in a 3D point cloud using natural language descriptions. However, existing methods [11, 10] rely on single-modal (unimodal) sensor data, mainly point clouds, and a fixed label vocabulary for graph annotations. Thus, they struggle with incomplete reconstructions and lack the flexibility to handle text-grounded or multimodal scenes, motivating a lightweight, generalizable framework for cross-modal alignment under noise.\n\nTo address these challenges, we introduce SGAligner++, a method that fuses structural, geometric, and linguistic information into a unified representation space. This enables the model to reason about spatial relationships from language, capture rich semantic context, and resolve ambiguities using multimodal data. Our approach enables efficient, scalable alignment even in noisy or low-overlap settings. Its modular design supports new modalities, and language grounding ensures adaptability to diverse 3D environments. SGAligner++ learns a unified embedding space by representing objects through features from multiple modalities: point clouds, CAD meshes, text captions, and spatial referrals. Furthermore, SGAligner++ handles missing data, such as when sensor inputs vary across environments (e.g., when a robot uses visual data but lacks text referrals). Our approach ensures robust object representation despite such gaps.\n\nUnlike previous methods [10, 11, 4] that depend on fixed semantic labels and simplistic interpretation of object relationships in 3D scene graphs, SGAligner++ enables seamless multimodal scene graph integration that could be used in downstream tasks such as visual localization, 3D reconstruction, and navigation. The approach is fast, lightweight, and annotation-free, making it ideal for embodied perception. We summarize the contributions as follows:\n\nWe enable cross-modal alignment and generate a unified 3D scene graph to achieve semantic consistency.\n\nWe enable cross-modal alignment and generate a unified 3D scene graph to achieve semantic consistency.\n\nWe use lightweight unimodal encoders with attention-based fusion for robust matching on noisy input.\n\nWe achieve state-of-the-art performance while maintaining low runtime and memory overhead, ensuring scalable deployment.\n\nThe data and code will be made public upon acceptance.\n\n1. We enable cross-modal alignment and generate a unified 3D scene graph to achieve semantic consistency.\n\n2. We use lightweight unimodal encoders with attention-based fusion for robust matching on noisy input.\n\n3. We achieve state-of-the-art performance while maintaining low runtime and memory overhead, ensuring scalable deployment.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效对齐3D场景图以支持机器人导航和具身感知的应用。  \n2. 现有方法在处理不完整或噪声输入时的局限性。  \n3. 如何实现跨模态的3D场景图对齐以提高语义一致性。  \n\n【用了什么创新方法】  \nSGAligner++通过学习统一的联合嵌入空间，融合结构、几何和语言信息，解决了部分重叠场景观察的对齐问题。该方法采用轻量级的单模态编码器和基于注意力的融合技术，确保在低重叠和传感器噪声条件下的准确对齐。通过这种方式，SGAligner++在真实世界数据集上的评估显示，其在噪声较大的重建任务中性能提升高达40%。该框架的模块化设计支持新模态的集成，确保了在多样化3D环境中的适应性和鲁棒性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        }
    ],
    "2025-09-27": [],
    "2025-09-28": [],
    "2025-09-29": [
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation",
            "authors": "Chih Yao Hu,Yang-Sen Lin,Yuna Lee,Chih-Hai Su,Jie-Ying Lee,Shr-Ruei Tsai,Chin-Yang Lin,Kuan-Wen Chen,Tsung-Wei Ke,Yu-Lun Liu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "CoRL 2025. Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.22653",
            "code": "https://spf-web.pages.dev",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22653",
            "arxiv_html_link": "https://arxiv.org/html/2509.22653v1",
            "abstract": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs). SPF is capable of navigating to any goal based on any type of free-form instructions in any kind of environment. In contrast to existing VLM-based approaches that treat action prediction as a text generation task, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. SPF harnesses VLMs to decompose vague language instructions into iterative annotation of 2D waypoints on the input image. Along with the predicted traveling distance, SPF transforms predicted 2D waypoints into 3D displacement vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the traveling distance to facilitate more efficient navigation. Notably, SPF performs navigation in a closed-loop control manner, enabling UAVs to follow dynamic targets in dynamic environments. SPF sets a new state of the art in DRL simulation benchmark, outperforming the previous best method by an absolute margin of 63%. In extensive real-world evaluations, SPF outperforms strong baselines by a large margin. We also conduct comprehensive ablation studies to highlight the effectiveness of our design choice. Lastly, SPF shows remarkable generalization to different VLMs.\nProject page: https://spf-web.pages.dev",
            "introduction": "The rapid development of unmanned aerial vehicles (UAVs) has revolutionized applications from environmental monitoring to security patrol. However, autonomous UAV navigation remains challenging due to requirements for strong visual reasoning in unstructured environments, language understanding for user instructions, and high-level task planning with low-level action control [1]. These autonomous UAV navigation tasks are often framed as aerial vision-and-language (AVLN) tasks [2, 3].\n\nThe autonomous UAV navigation tasks are commonly framed as aerial vision-and-language (AVLN) tasks [2, 3]. Conventional methods primalily adopt end-to-end policy learning frameworks which consist of a text and vision encoder that maps language instructions and visual observations into latent representations, followed by a policy head that converts these representations into UAV actions [4, 5, 6, 7, 8, 9, 10]. The entire models are trained on a curated set of expert demonstrations [11, 12, 13, 14]. However, due to the limited scale and diversity of the training data, these methods fail to generalize to unseen environments or task instructions. In contrast, recent works explore a training-free direction that directly converts Vison Large Language Models (VLM) into AVLN policies [15, 16, 17, 18]. As VLMs are trained on large-scale internet data, these models have demonstrated not only rich common-sense knowledge of the world, strong capabilities in visual/language understanding, reasoning and planning, but also, strong generalization to novel environments and tasks [19, 20, 21].\n\nHow to repurpose VLMs that generate texts into embodied agents that generate physical actions has attracted increasing interest in robotics [22, 23, 24, 25], while the research direction is still underexplored in AVLN. Existing VLM-based approaches to AVLN build atop a direct solution, that considers action prediction simply as a text-generation task. VLMs are prompoted to output either continuous actions [16] or pre-defined skills [15, 17, 18], in terms of texts. Despite the simplicity of these methods, they have two obvious problems: (1) embodied agents need to execute fine-grained actions, while texts are not suitable to represent high-precision floating numbers; (2) these VLMs have not been trained on aerial navigation data to predict 3D actions for navigation. In contrast, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. Instead of predicting 3D actions directly, we harnesses VLMs to annotate 2D waypoints [26, 27, 28, 29] on the image, which do not require any domain knowledge of AVLN but general spatial understanding [30, 31]. As these 2D waypoints are grounded in the visual scene, they inherently contain precise action information. These 2D waypoints can then be transformed into 3D actions using the camera information.\n\nNotably, we do not introduce the concept of predicting 2D waypoints for action selection—similar ideas have been explored in both robot manipulation and navigation [28, 26, 27, 20]. For example, RT-Trajectory [26] leverages VLMs to directly label 2D waypoints on the image, which are then used by a separately trained policy network to predict corresponding actions. PIVOT [28], in contrast, samples multiple candidate actions as 2D waypoints and employs a VLM to select the most appropriate one for execution. In this work, we build on this general idea and adapt it to the AVLN setting. Our method requires no additional neural network training, yet it significantly outperforms PIVOT, which is also a training-free approach.\n\nWe introduce See, Point, Fly (SPF), a novel VLM-based AVLN framework that navigates to any goal based on any free-form instructions in any environment. At the core of our method is a VLM [20] that conditions on the current scene and language instructions, and outputs the 2D waypoints in terms of pixel locations. These 2D waypoints are unprojected into unit-length 3D positions based on the camera parameters. These 3D positions denote the relative 3D actions to the current UAV location. To enhance the navigation speed, we propose an adaptive controller module that adjust the scale of the actions based on the distance between the UAV and the target. Since our method naturally enables closed-loop control of the UAV, as shown in Fig. 1, UAVs are capable of following dynamic targets. Moreover, building atop VLMs, our method can easily tackle long-horizon and even ambiguous task instructions in a zero-shot manner.\n\nWe test SPF on a simulation and a real-world benchmark. Our method outperforms prior state-of-the-art, TypeFly [15] by a large margin. We show that our method works well across a wide range of tasks, including long-horizon, abstract, and dynamic navigation tasks. We also conduct an extensive ablation study to validate the effectiveness of each design choice.\n\nIn summary, our contributions are: (1) We propose a state-of-the-art AVLN framework that generalizes to novel scenes and free-form instructions; (2) We set a new state-of-the-art in the DRL simulator [32] simulation benchmark, outperforming prior SOTAs with a margin of 63%63\\% in success rate; (3) We set a new state of the art in the real-world benchmark, outperforming prior SOTAs with a margin of 82%82\\% in success rate.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的无人机导航方法在未见环境或任务指令上缺乏泛化能力。  \n2. 传统的基于文本生成的动作预测方法不适合高精度的动作执行。  \n3. 需要一种无需训练的框架来处理多样化的导航任务。  \n\n【提出了什么创新的方法】  \n本文提出了See, Point, Fly (SPF)框架，利用视觉语言模型（VLM）将模糊的语言指令分解为2D路径点的迭代注释。SPF通过将预测的2D路径点转换为3D位移向量，生成无人机的动作指令，并自适应调整旅行距离以提高导航效率。SPF在DRL模拟基准上设定了新的状态，成功率提高了63%。在实际评估中，SPF也显著超越了强基线，展示了对不同VLM的卓越泛化能力。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Pixel Motion Diffusion is What We Need for Robot Control",
            "authors": "E-Ro Nguyen,Yichi Zhang,Kanchana Ranasinghe,Xiang Li,Michael S. Ryoo",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22652",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22652",
            "arxiv_html_link": "https://arxiv.org/html/2509.22652v1",
            "abstract": "We present DAWN (Diffusion is All We Need for robot control), a unified diffusion-based framework for language-conditioned robotic manipulation that bridges high-level motion intent and low-level robot action via structured pixel motion representation. In DAWN, both the high-level and low-level controllers are modeled as diffusion processes, yielding a fully trainable, end-to-end system with interpretable intermediate motion abstractions.\nDAWN achieves state-of-the-art results on the challenging CALVIN benchmark, demonstrating strong multi-task performance, and further validates its effectiveness on MetaWorld. Despite the substantial domain gap between simulation and reality and limited real-world data, we demonstrate reliable real-world transfer with only minimal finetuning, illustrating the practical viability of diffusion-based motion abstractions for robotic control. Our results show the effectiveness of combining diffusion modeling with motion-centric representations as a strong baseline for scalable and robust robot learning. Project page: https://nero1342.github.io/DAWN/.",
            "introduction": "Multi-stage pixel or point tracking based methods have recently emerged as a promising direction for robot manipulation, offering interpretable intermediate pixel motion and modular control (Yuan et al., 2024a; Gao et al., 2024; Xu et al., 2024; Bharadhwaj et al., 2024b; a; Ranasinghe et al., 2025).\nHowever, despite their promise, approaches such as Im2Flow2Act (Xu et al., 2024), ATM (Wen et al., 2023), and LangToMo (Ranasinghe et al., 2025) still fall short of state-of-the-art vision-language action (VLA) models (Black et al., 2024a; Intelligence et al., 2025) and latent feature–based hierarchical methods (Hu et al., 2024; Nvidia et al., 2025) on established benchmarks.\nWe argue that this performance gap does not arise from limitations in the two-stage intermediate pixel-motion based framework itself. The high-level motion generator in these frameworks does not fully reflect recent advances in visual generative modeling (Ge et al., 2022; Kumari et al., 2023; Zhang et al., 2022; Ren et al., 2022; Chen et al., 2023), while the low-level controllers have not leveraged recent progress in diffusion-based action policies (Janner et al., 2022; Du et al., 2023a; Chi et al., 2023; Shridhar et al., 2024; Li et al., 2024a) in an optimal way.\n\nTo address these limitations, we introduce a two-stage diffusion-based visuomotor framework in which both the high-level and low-level controllers are instantiated as diffusion models and glued by explicit pixel motions as illustrated in Figure 1.\nThe high-level motion director, which is a latent diffusion module, takes current (multiview) visual observations and language instruction, and predicts desired dense pixel motion from a third-person view.\nThis pixel motion could be regarded as a structured intermediate representation of desired scene dynamics to accomplish the language instruction.\nThese pixel motion are then translated into executable actions through a diffusion-based policy head.\nWe highlight how intermediate pixel motion is grounded on visual inputs, endowing the intermediate representations with interpretability.\nTherein, we introduce Diffusion is All We Need for robot control (DAWN), which bridges the strengths of hierarchical motion decomposition and end-to-end visuomotor agents, while maintaining interpretability and modularity.\n\nOur framework illustrated in Figure 1 builds upon insights from prior hierarchical visuomotor approaches. VPP (Hu et al., 2024) employs a video diffusion model to extract predictive feature embeddings, which subsequently condition a downstream action policy. However, it operates in RGB space (with no motion specific representation) and uses the video diffusion model as a feature extractor as opposed to iterative denoising of motion features. LangToMo (Ranasinghe et al., 2025) predicts pixel-space motion trajectories from language instructions, but its high-level motion director uses pixel-level diffusion, limiting the resolution of the generated motion representation and training scalability. Its low-level controller is based on weaker ViT architectures or hand-crafted heuristics. In contrast, DAWN utilizes an efficient pretrained latent diffusion model for motion generation with iterative denoising during inference, and a strong diffusion-based action expert, thus benefiting from powerful vision and language models.\n\nWe evaluate our method on two challenging simulation benchmarks—CALVIN (Mees et al., 2022) and MetaWorld (Yu et al., 2019), as well as across real-world environments with only very limited in-domain training data.\n\nOur results demonstrate that, despite using limited data and substantially smaller model capacity, our method can match or even surpass state-of-the-art VLA models by leveraging explicit structured pixel motion and the strengths of diverse pretrained models, highlighting its high data efficiency.\n\nOur key contributions are as follows:\n\nWe propose DAWN, a two-stage\ndiffusion-based framework that generates structured intermediate pixel motion as an efficient language-conditioned visuomotor policy.\n\nDespite relying on limited data and a substantially smaller model capacity, we achieve competitive or even state-of-the-art performance on CALVIN, MetaWorld, and real-world benchmarks.\n\nOur approach is explicitly designed to leverage pretrained vision and language models, enabling highly data-efficient transfer across domains, while providing interpretability and modularity.",
            "llm_summary": "【论文的motivation是什么】  \n1. 当前的机器人控制方法在高层和低层控制器之间缺乏有效的连接，导致性能不足。  \n2. 现有的视觉生成模型和扩散模型未能充分利用，造成了性能差距。  \n3. 需要一种能够有效桥接高层意图和低层动作的框架，以提高机器人操控的可解释性和模块化。  \n\n【提出了什么创新的方法】  \n提出了一种名为DAWN的两阶段扩散基础框架，通过结构化像素运动表示连接语言条件的机器人操控的高层和低层控制器。高层运动生成器使用潜在扩散模块，从多视角视觉观察和语言指令中预测所需的密集像素运动。这些像素运动被转化为可执行动作，通过扩散基础策略头实现。该方法在CALVIN和MetaWorld基准上取得了最先进的结果，并在现实环境中展示了良好的迁移能力，尽管只使用了有限的训练数据，表明扩散基础运动抽象在机器人控制中的实际可行性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search",
            "authors": "Wenkai Guo,Guanxing Lu,Haoyuan Deng,Zhenyu Wu,Yansong Tang,Ziwei Wang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22643",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22643",
            "arxiv_html_link": "https://arxiv.org/html/2509.22643v1",
            "abstract": "Vision-Language-Action models (VLAs) achieve strong performance in general robotic manipulation tasks by scaling imitation learning. However, existing VLAs are limited to predicting short-sighted next-action, which struggle with long-horizon trajectory tasks due to incremental deviations.\nTo address this problem, we propose a plug-in framework named VLA-Reasoner that effectively empowers off-the-shelf VLAs with the capability of foreseeing future states via test-time scaling.\nSpecifically, VLA-Reasoner samples and rolls out possible action trajectories where involved actions are rationales to generate future states via a world model, which enables VLA-Reasoner to foresee and reason potential outcomes and search for the optimal actions.\nWe further leverage Monte Carlo Tree Search (MCTS) to improve search efficiency in large action spaces, where step-wise VLA predictions seed the root.\nMeanwhile, we introduce a confidence sampling mechanism based on Kernel Density Estimation (KDE), to enable efficient exploration in MCTS without redundant VLA queries.\nWe evaluate intermediate states in MCTS via an offline reward shaping strategy, to score predicted futures and correct deviations with long-term feedback.\nWe conducted extensive experiments in both simulators and the real world, demonstrating that our proposed VLA-Reasoner achieves significant improvements over the state-of-the-art VLAs. Our method highlights a potential pathway toward scalable test-time computation of robotic manipulation.",
            "introduction": "Vision-Language-Action models (VLAs) [1, 2, 3] leverage the grounded perception and commonsense reasoning of large, pre-trained vision–language models (VLMs) to advance general-purpose robot manipulation. Within a supervised imitation learning paradigm, they map visual observations and natural-language instructions directly to sequences of low-level actions using extensive robot demonstration datasets [4, 5, 6]. By decoupling task specification from policy learning, VLAs adapt to diverse manipulation scenarios via language prompts and image conditions. Recent results show stronger generalization across object categories and environments, and reduced dependence on explicit task engineering, positioning VLAs as a promising route toward scalable embodied intelligence [2, 7].\n\nHowever, current VLAs also face critical limitations. As the action prediction of VLAs fundamentally relies on direct mappings from short-horizon environment states to actions, they remain fragile during deployment. This short-sighted prediction discards long-horizon sequential dependencies, becoming a primary cause of incremental deviations across tasks and environments. Consequently, the accuracy and exploration capability of VLAs are significantly constrained.\nThis raises a core question: \"Can VLAs explore the long-horizon future influence of actions at test time, and decide the optimal action?\"\n\nTo this end, we propose a plug-in framework named VLA-Reasoner that empowers off-the-shelf VLAs with the ability to foresee future states via test-time scaling.\nOur proposed VLA-Reasoner can effectively mitigate the incremental deviations of VLAs caused by the lack of considering future impact (Figure˜1).\nSpecifically, VLA-Reasoner samples and rolls out possible action trajectories to generate future states via a world model, where the future states and corresponding actions can reflect the potential outcomes.\nTo enhance search efficiency, we employ MCTS to handle the expansive action space, in which step-wise VLA predictions seed the root node.\nWe introduce a KDE-based confidence distribution that samples candidates in MCTS from an expert-like prior, reducing redundant VLA queries while preserving exploration.\nSince sparse task feedback arrives only at episode ending, we design an offline reward shaping strategy to evaluate intermediate states in MCTS, providing dense feedback signals that correct deviations with stable long-horizon guidance.\nVLA-Reasoner effectively improves the reasoning capability of VLAs in long-horizon trajectory tasks, through enabling structured exploration in expansive action spaces and foreseeing the potential outcomes of the current action.\n\nOur method significantly delivers consistent gains in both simulation and on real robots. On the LIBERO benchmark, wrapping a modest baseline VLA with VLA-Reasoner lifts it beyond competing VLAs. In real-world deployments, our approach achieves higher success rates compared to popular VLAs fine-tuned with a few demonstrations, indicating stronger generalization and adaptivity at test time. Our contributions are summarized as follows:\n\nWe propose a plug-in framework named VLA-Reasoner that empowers VLAs with structured reasoning to address their incremental deviations during deployment.\n\nWe propose a plug-in framework named VLA-Reasoner that empowers VLAs with structured reasoning to address their incremental deviations during deployment.\n\nWe adapt a modified test-time MCTS to search efficiently rather than just use it. We apply KDE for efficient plausible expansion, and provide an offline-based reward shaping method to evaluate intermediate states.\n\nWe conduct extensive experiments that validate the effectiveness of integrating MCTS into VLAs for test-time optimization. We also show the potential to achieve great real-world performance with a few data acquisitions.\n\n1. We propose a plug-in framework named VLA-Reasoner that empowers VLAs with structured reasoning to address their incremental deviations during deployment.\n\n2. We adapt a modified test-time MCTS to search efficiently rather than just use it. We apply KDE for efficient plausible expansion, and provide an offline-based reward shaping method to evaluate intermediate states.\n\n3. We conduct extensive experiments that validate the effectiveness of integrating MCTS into VLAs for test-time optimization. We also show the potential to achieve great real-world performance with a few data acquisitions.",
            "llm_summary": "【论文的motivation是什么】  \n1. 当前的Vision-Language-Action模型在长时间轨迹任务中面临短视预测的限制。  \n2. 现有模型在执行过程中容易产生增量偏差，影响准确性和探索能力。  \n3. 需要一种方法来探索动作的长远影响并决定最优行动。  \n\n【提出了什么创新的方法】  \n本文提出了一种名为VLA-Reasoner的插件框架，旨在通过测试时扩展赋予现有VLA模型预见未来状态的能力。该框架通过采样和展开可能的动作轨迹，利用世界模型生成未来状态，从而使VLA-Reasoner能够推测潜在结果并搜索最优动作。我们采用了蒙特卡洛树搜索（MCTS）来提高在大动作空间中的搜索效率，并引入基于核密度估计（KDE）的置信采样机制，以减少冗余的VLA查询。通过离线奖励塑形策略评估MCTS中的中间状态，提供密集反馈信号以纠正偏差。实验结果表明，VLA-Reasoner在模拟和真实机器人上均显著提升了性能，展示了其在可扩展机器人操作中的潜力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "WoW: Towards a World omniscient World model Through Embodied Interaction",
            "authors": "Xiaowei Chi,Peidong Jia,Chun-Kai Fan,Xiaozhu Ju,Weishi Mi,Kevin Zhang,Zhiyuan Qin,Wanxin Tian,Kuangzhi Ge,Hao Li,Zezhong Qian,Anthony Chen,Qiang Zhou,Yueru Jia,Jiaming Liu,Yong Dai,Qingpo Wuwu,Chengyu Bai,Yu-Kai Wang,Ying Li,Lizhang Chen,Yong Bao,Zhiyuan Jiang,Jiacheng Zhu,Kai Tang,Ruichuan An,Yulin Luo,Qiuxuan Feng,Siyuan Zhou,Chi-min Chan,Chengkai Hou,Wei Xue,Sirui Han,Yike Guo,Shanghang Zhang,Jian Tang",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22642",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22642",
            "arxiv_html_link": "https://arxiv.org/html/2509.22642v1",
            "abstract": "Humans develop an understanding of intuitive physics through active interaction with the world. This approach is in stark contrast to current video models, such as Sora, which rely on passive observation and therefore struggle with grasping physical causality. This observation leads to our central hypothesis: authentic physical intuition of world model must be grounded in extensive, causally rich interactions with the real world.\nTo test this hypothesis, we present WoW, a 14B-parameter generative world model trained on 2 million robot interaction trajectories. Our findings reveal that the model’s understanding of physics is a probabilistic distribution of plausible outcomes, leading to stochastic instabilities and physical hallucinations.\nFurthermore, we demonstrate that this emergent capability can be actively constrained toward physical realism by SOPHIA, where vision language model agents evaluate the DiT’s generated output and guide its refinement by iteratively evolving the language instruction.\nBesides, a co-trained Inverse Dynamics Model translates these refined plans into executable robotic actions, thus closing the imagination-to-action loop.\nWe establish WoWBench, a new benchmark focused on physical consistency and causal reasoning of video, where WoW achieves state-of-the-art performance of both human and autonomous evaluation, demonstrating strong ability on physical causality, collision dynamics, and object permanence.\nOur work provides the systematic evidence that large-scale, real-world interaction is a cornerstone for developing physical intuition in AI. Models, data, and benchmarks will be open-sourced in wow-world-model.github.io",
            "introduction": "“The ladder of causation has three rungs: seeing, doing, and imagining.”\n\nIn contrast, many recent advances in predictive models, particularly in video generation, is predicated on passive observation, a principle fundamentally distinct from active experimentation that fosters accurate causal understanding. While models like Sora (Brooks et al., 2024) and others (Wan et al., 2025) achieve stunning photorealism and demonstrate emergent physical intuition, this intuition remains brittle. Their training objective prioritizes modeling statistical correlations from internet-scale data over inferring the underlying causal mechanisms of physics. Consequently, their grasp of physical laws is often superficial. When tasked with scenarios requiring genuine physical reasoning, they can produce logically and physically inconsistent outcomes. These models master the appearance of our world, but the generative dynamics they learn are an approximation rather than an accurate representation.\n\nThis distinction motivates our core hypothesis, for an embodied model to develop genuine physical intuition, it must learn from large-scale, causally-rich, real-world interaction data, thereby lifting the generative model toward a world model (Ha & Schmidhuber, 2018; Agarwal et al., 2025). To further validate our approach, we first introduce SOPHIA, a novel architectural paradigm that couples the reasoning capabilities of a Vision Language Model (VLM) with the generative power of a Diffusion Transformer (DiT) (Peebles & Xie, 2023). We then present WoW, a concrete instantiation of this paradigm. WoW is a generative world model trained on a large-scale dataset of 2 million real-world robotic interaction trajectories, spanning 5275 tasks and 12 different robots. The objective of WoW is to directly synthesize pixel-level future predictions, learning to imagine and reason through generation itself.\n\nTo close the perception-to-action loop, we designed the Flow-Mask Inverse Dynamics Model (FM-IDM), which functions as the agent’s equivalent of the cerebellum and motor cortex. By analyzing the optical flow and scene context between the current state and the imagined next state, the FM-IDM infers the 7-DoF end-effector action necessary to enact the transition. This module grounds the agent’s imagination in physical reality, translating pixel-level futures into executable actions.\n\nTo empirically validate this complete perception-imagination-reflection-action cognitive architecture, we established WoWBench, a new benchmark focused on physical consistency and causal reasoning. WoWBench is composed of 4 core abilities and 20 sub-tasks, containing 606 samples, each with an initial image and a textual instruction. For a comprehensive evaluation, we comprehend 4 indispensable metrics: Video quality, Planning reasoning, Physical rules, and Instruction following. Our experiments demonstrate that our 14B-parameter WoW achieves SOTA performance on this benchmark, especially 96.53% on Instruction understanding, and 80.16% on Physical law, providing compelling evidence in support of our central hypothesis. To further verify the precision of our WoWBench, we conduct a human evaluation proving that WoWBench is highly correlated with human preference, and WoW achieves SOTA performance on both sides. Beyond its benchmark performance, WoW demonstrates versatility in broader applications. We show it is more than a simple generator, capable of enhancing VLM reasoning, serving as a physical simulator, and enabling 3D-aware representation learning.\n\nIn summary, we propose SOPHIA, a paradigm for developing embodied intelligence through a data-driven feedback loop. This approach involves deploying capable models to collect large-scale corrective feedback from physical interactions, a process that drives a continuous cycle of improvement. Our model, WoW, serves as a powerful instantiation of this paradigm, representing a significant advance from passive video models to an embodied world model that closes the perception-imagination-reflection-action loop. Our main contributions are as follows.\n\nA Unified Architecture for Imagination and Action. We introduce an embodied world model WoW, which instantiates a novel self-optimization framework SOPHIA for imagining physically plausible futures, and incorporates a Flow-Mask Inverse Dynamics Model that infers the corresponding executable actions.\n\nSelf-Supervised Feature Alignment. We are the first to integrate powerful, pre-trained self-supervised visual features into the backbone of a diffusion-based world model. This novel approach significantly boosts the model’s perceptual capabilities, accelerates training convergence, and improves the fidelity and physical consistency of generated futures.\n\nAn Interaction Benchmark. We propose WoWBench, a new public benchmark designed to evaluate the physical consistency and action-generation capabilities of world models. Comprising 606 diverse, high-quality robot trajectories, the benchmark facilitates a rigorous performance comparison between existing methods and proposed WoW across 4 core metrics and 20 associated tasks.\n\nPostraining Application Discussion. We demonstrate that WoW is more than a generator, showcasing its versatility across a range of downstream applications. These applications include synthesizing novel views, generating trajectory-guided videos, producing action-conditioned videos for robot manipulation, enhancing visual style transfer and improving VLM task planning at test-time.\n\nA Scaling Analysis and Open-Source Models. We perform a systematic scaling analysis of our architecture up to 14B parameters, uncovering nascent capabilities and potential precursors to emergence in physical reasoning. This provides strong empirical evidence for the scaling hypothesis in this domain. We will release all trained model checkpoints to provide a foundation for future research in the embodied world model.\n\n1. A Unified Architecture for Imagination and Action. We introduce an embodied world model WoW, which instantiates a novel self-optimization framework SOPHIA for imagining physically plausible futures, and incorporates a Flow-Mask Inverse Dynamics Model that infers the corresponding executable actions.\n\n2. Self-Supervised Feature Alignment. We are the first to integrate powerful, pre-trained self-supervised visual features into the backbone of a diffusion-based world model. This novel approach significantly boosts the model’s perceptual capabilities, accelerates training convergence, and improves the fidelity and physical consistency of generated futures.\n\n3. An Interaction Benchmark. We propose WoWBench, a new public benchmark designed to evaluate the physical consistency and action-generation capabilities of world models. Comprising 606 diverse, high-quality robot trajectories, the benchmark facilitates a rigorous performance comparison between existing methods and proposed WoW across 4 core metrics and 20 associated tasks.\n\n4. Postraining Application Discussion. We demonstrate that WoW is more than a generator, showcasing its versatility across a range of downstream applications. These applications include synthesizing novel views, generating trajectory-guided videos, producing action-conditioned videos for robot manipulation, enhancing visual style transfer and improving VLM task planning at test-time.\n\n5. A Scaling Analysis and Open-Source Models. We perform a systematic scaling analysis of our architecture up to 14B parameters, uncovering nascent capabilities and potential precursors to emergence in physical reasoning. This provides strong empirical evidence for the scaling hypothesis in this domain. We will release all trained model checkpoints to provide a foundation for future research in the embodied world model.",
            "llm_summary": "【论文的motivation是什么】  \n1. 当前视频模型依赖被动观察，难以理解物理因果关系。  \n2. 真实的物理直觉需要通过大量的因果丰富的真实世界交互数据来学习。  \n3. 现有模型在处理需要真实物理推理的场景时，常常产生逻辑和物理不一致的结果。  \n\n【提出了什么创新的方法】  \n本文提出了WoW，一个14B参数的生成世界模型，基于200万机器人交互轨迹进行训练。通过引入SOPHIA架构，将视觉语言模型与扩散变换器结合，WoW能够生成物理上合理的未来预测，并通过Flow-Mask逆动力学模型将这些预测转化为可执行的机器人动作。实验结果表明，WoW在物理因果性、碰撞动态和物体持久性等方面表现出色，尤其在WoWBench基准测试中达到了人类和自主评估的最先进性能，验证了大规模真实交互对AI物理直觉发展的重要性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation",
            "authors": "Yuan Xu,Jiabing Yang,Xiaofeng Wang,Yixiang Chen,Zheng Zhu,Bowen Fang,Guan Huang,Xinze Chen,Yun Ye,Qiang Zhang,Peiyan Li,Xiangnan Wu,Kai Wang,Bing Zhan,Shuo Lu,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22578",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22578",
            "arxiv_html_link": "https://arxiv.org/html/2509.22578v1",
            "abstract": "Imitation learning based policies perform well in robotic manipulation, but they often degrade under egocentric viewpoint shifts when trained from a single egocentric viewpoint. To address this issue, we present EgoDemoGen, a framework that generates paired novel egocentric demonstrations by retargeting actions in the novel egocentric frame and synthesizing the corresponding egocentric observation videos with proposed generative video repair model EgoViewTransfer, which is conditioned by a novel-viewpoint reprojected scene video and a robot-only video rendered from the retargeted joint actions. EgoViewTransfer is finetuned from a pretrained video generation model using self-supervised double reprojection strategy. We evaluate EgoDemoGen on both simulation (RoboTwin2.0) and real-world robot. After training with a mixture of EgoDemoGen-generated novel egocentric demonstrations and original standard egocentric demonstrations, policy success rate improves absolutely by +17.0% for standard egocentric viewpoint and by +17.7% for novel egocentric viewpoints in simulation. On real-world robot, the absolute improvements are +18.3% and +25.8%. Moreover, performance continues to improve as the proportion of EgoDemoGen-generated demonstrations increases, with diminishing returns. These results demonstrate that EgoDemoGen provides a practical route to egocentric viewpoint-robust robotic manipulation.",
            "introduction": "Imitation learning has emerged as a powerful paradigm in robotic manipulation, enabling end-to-end visuomotor policies that map raw observations to control actions. Recent imitation learning policies including Vision-Language-Action models (Chi et al., 2023; Zhao et al., 2023; Zitkovich et al., 2023; Ghosh et al., 2024; Liu et al., 2024; O’Neill et al., 2024; Black et al., 2024) have demonstrated remarkable performance when trained on large and diverse demonstration datasets (Wu et al., 2024; Khazatsky et al., 2024; Walke et al., 2023; O’Neill et al., 2024). However, these policies remain sensitive to distribution shift: policies trained or finetuned from a single egocentric viewpoint often fail to generalize to unseen egocentric viewpoints (Tian et al., 2025; Xing et al., 2025), shown in Figure 1(a). This limitation underscores the need to increase viewpoint diversity.\n\nGenerating novel egocentric viewpoint demonstrations serves as one effective solution to this problem. Existing efforts to mitigate this issue can be broadly categorized into two lines of work. One line of works focus on novel viewpoint synthesis using techniques such as point cloud rendering, 3D reconstruction, or image generation models (Sargent et al., 2024; Xue et al., 2025; Yang et al., 2025). These approaches synthesize novel visual observations but maintain original actions, leading to visual-action mismatch in egocentric setting, shown in Figure 1(b). Another line of works employ world models or action-conditioned video generation to target prediction or planning, rather than observation-action paired demonstration generation (Wang et al., 2025a; Rigter et al., 2024; Bruce et al., 2024; Luo & Du, 2024; Hafner et al., 2025). Moreover, these works do not explicitly model changes in the egocentric viewpoint caused by robot motion. Generating demonstrations from a novel egocentric viewpoint requires coherent synthesis of both the visual observations and the corresponding actions.\n\nOur key insight addresses this fundamental gap: generating novel egocentric demonstrations requires not only synthesizing realistic observations from novel egocentric viewpoints, but also retargeting the original actions to align with the shifted viewpoint. This entails tackling two core challenges: (1) producing kinematically feasible robot actions that achieve the task under the novel egocentric viewpoint, and (2) generating realistic, temporally consistent observation videos that match these retargeted actions. Crucially, the generated demonstrations must preserve the style and intent of the original demonstrations while ensuring visual–action alignment.\n\nTo tackle these challenges, we propose EgoDemoGen, a novel framework for generating demonstrations from novel egocentric viewpoints. First, on the action side, we perform kinematics-based action retargeting to produce joint actions corresponding to novel egocentric viewpoint. Second, on the visual side, we propose EgoViewTransfer, a generative vide repair model that fuses the reprojected scene videos with the robot motion videos rendered under the retargeted actions and generate novel egocentric observation videos. We conducted experiments in the RoboTwin2.0 (Chen et al., 2025b) simulation environment and on a real-world dual-arm robot to evaluate the effectiveness of the generated demonstrations.\n\nOur main contributions can be summarized as follows:\n\nWe present EgoDemoGen, a framework that generates novel egocentric demonstrations with paired retargeted actions and egocentric observation videos, improving policy generalization to egocentric viewpoint shifts.\n\nWe generate novel demonstrations by retargeting actions in the novel egocentric frame and synthesizing corresponding observation videos from a novel-viewpoint reprojected scene video and a robot-only video rendered from the retargeted joint actions. The generated paired demonstrations are used to train downstream policies.\n\nWe introduce EgoViewTransfer, a generative video repair model finetuned from a pretrained video generation model with a double reprojection strategy, which fuses reprojected scene video and rendered robot video to synthesize consistent, realistic egocentric observation video.\n\nExperiments on simulation (RoboTwin2.0) and real-world robot show policy success rate absolute improvements of +17.0% (standard egocentric viewpoint) and +17.7% (novel egocentric viewpoints) in simulation when incorporating demonstrations generated by EgoDemoGen into the training mixture, and +18.3% and +25.8% on real-world robot. Moreover, performance continues to improve as the proportion of EgoDemoGen-generated demonstrations increases, with diminishing returns.\n\n1. We present EgoDemoGen, a framework that generates novel egocentric demonstrations with paired retargeted actions and egocentric observation videos, improving policy generalization to egocentric viewpoint shifts.\n\n2. We generate novel demonstrations by retargeting actions in the novel egocentric frame and synthesizing corresponding observation videos from a novel-viewpoint reprojected scene video and a robot-only video rendered from the retargeted joint actions. The generated paired demonstrations are used to train downstream policies.\n\n3. We introduce EgoViewTransfer, a generative video repair model finetuned from a pretrained video generation model with a double reprojection strategy, which fuses reprojected scene video and rendered robot video to synthesize consistent, realistic egocentric observation video.\n\n4. Experiments on simulation (RoboTwin2.0) and real-world robot show policy success rate absolute improvements of +17.0% (standard egocentric viewpoint) and +17.7% (novel egocentric viewpoints) in simulation when incorporating demonstrations generated by EgoDemoGen into the training mixture, and +18.3% and +25.8% on real-world robot. Moreover, performance continues to improve as the proportion of EgoDemoGen-generated demonstrations increases, with diminishing returns.",
            "llm_summary": "【论文的motivation是什么】  \n1. Imitation learning policies struggle to generalize across different egocentric viewpoints.  \n2. Existing methods either synthesize novel viewpoints without retargeting actions or focus on prediction rather than demonstration generation.  \n3. There is a need for coherent synthesis of visual observations and corresponding actions in novel egocentric contexts.  \n\n【提出了什么创新的方法】  \nEgoDemoGen is introduced as a framework that generates novel egocentric demonstrations by retargeting actions and synthesizing corresponding observation videos. The process involves kinematics-based action retargeting to align actions with novel viewpoints and utilizes EgoViewTransfer, a generative video repair model, to create realistic observation videos. This approach significantly improves policy generalization to egocentric viewpoint shifts, with notable success rate increases of +17.0% and +17.7% in simulation, and +18.3% and +25.8% on real robots. Performance continues to improve with more EgoDemoGen-generated demonstrations, demonstrating its effectiveness in enhancing robotic manipulation.  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data",
            "authors": "Farida Mohsen,Ali Safa",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22573",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22573",
            "arxiv_html_link": "https://arxiv.org/html/2509.22573v1",
            "abstract": "Efficiently detecting human intent to interact with ubiquitous robots is crucial for effective human-robot interaction (HRI) and collaboration. Over the past decade, deep learning has gained traction in this field, with most existing approaches relying on multimodal inputs, such as RGB combined with depth (RGB-D), to classify time-sequence windows of sensory data as interactive or non-interactive. In contrast, we propose a novel RGB-only pipeline for predicting human interaction intent with frame-level precision, enabling faster robot responses and improved service quality. A key challenge in intent prediction is the class imbalance inherent in real-world HRI datasets, which can hinder the model’s training and generalization. To address this, we introduce MINT-RVAE, a synthetic sequence generation method, along with new loss functions and training strategies that enhance generalization on out-of-sample data. Our approach achieves state-of-the-art performance (AUROC: 0.95) outperforming prior works (AUROC: 0.90–0.91.2), while requiring only RGB input and supporting precise frame onset prediction. Finally, to support future research, we openly release our new dataset with frame-level labeling of human interaction intent.",
            "introduction": "Service robots operating in public spaces should be able to recognize when humans intend to interact with them to respond in a timely and socially appropriate manner [1]. In domains such as hotels, shopping centers, and healthcare facilities, this capability is essential for delivering seamless user experiences; for example, a robot receptionist must recognize that a person approaches with the intent to engage before explicit verbal or gestural signals occur [2]. Accurate intent detection improves fluency, safety, and user trust by minimizing delays and avoiding inappropriate or missed responses [3, 4].\n\nRecent research on human–robot interaction (HRI) has focused on human intent prediction for collaborative tasks between humans and robots [7]. These models, typically trained on relations between human motion trajectories and inferred intentions, enable robots to adapt or plan their actions proactively. However, many existing methods rely on more specialized hardware, such as depth cameras or motion capture systems (MOCAP), to capture 3D poses [7, 8], limiting system scalability and leading to more significant equipment costs that are less suited for real-world robot system deployments [9, 10]. Moreover, the high variability of human nonverbal cues constitutes one of the main challenges for the robust detection of human interaction intent, where trained models must generalize well to out-of-distribution test data unseen during training. Humans rely on posture, facial expressions, and proxemics to signal social intent [11], but endowing robots with the ability to interpret such signals requires multimodal perception and temporal reasoning [12].\n\nAnother limitation is the frame labeling resolution. Three common practices are observed: i) Sequence-level labels: most studies provide intent labels for entire data segments, such that all frames within a temporal window are assigned to the same intent label [8, 13]; ii) Action segmentation post-processing: some approaches assign intent labels after segmenting actions or interactions in post-processing, with intent annotation occurring retrospectively based on observed events [14, 15]; iii) Frame-wise outputs with sequence labels (coarse evaluation): other studies produce a probability per frame but train and evaluate their model using the same label replicated across frames, that is, without frame-accurate onset annotation [7, 8, 13]. While suitable for sequence classification, such methods omit the critical moment when intention first emerges (i.e., the onset) and hence, jeopardizes the system’s response time. More broadly, surveys on human motion and intent prediction stress that temporal fidelity is critical for responsive behavior: earlier, more precise predictions enable earlier actions [16]. Recognizing this onset at the frame level (vs. sequence level) is crucial for practical service robots, as it determines the earliest point at which a robot can and should act, thereby leading to an elevated quality of service and robot responsiveness.\n\nAnother central challenge in HRI intent prediction is class imbalance. In public deployments, genuine interaction events are rare relative to prolonged non-interaction; for example, the PAR-D dataset in [17] reports 112 / 4245 interacted vs. non-interacted trajectories. Furthermore, field studies in shopping malls observe engagement rates of ∼3.6%\\sim 3.6\\% [18]. Imbalance manifests at both the sequence level (few sequences contain interactions) and the frame level (positive labels are vastly outnumbered), motivating imbalance-aware training and imbalance-robust metrics (AUROC, macro-F1F_{1}, and balanced accuracy). Common remedies such as undersampling [19] or synthetic oversampling with SMOTE-like methods [20] have limitations in sequential, multimodal settings: undersampling reduces effective data and discards informative negatives, while SMOTE-like oversampling disrupts temporal coherence and multimodal consistency due to their non-learning-based approach (vs. generative models such as variational autoencoders [21] and generative adversarial networks [22]), ultimately degrading the performance of sequential models.\n\nTo address the above limitations and challenges in HRI intent detection contexts, this paper proposes a novel RGB-only pipeline for accurate frame-level detection of human intentions to interact with a robot arm (see Fig. 1). Unlike prior works that mostly focus on sequence-level detection and rely on more expensive sensor suites (e.g., RGB–D) [8, 13], our approach reduces hardware cost (approximately $​350\\mathdollar 350 for an Intel RealSense RGB–D camera vs. $​10\\mathdollar 10 for a commodity USB webcam), while enabling onset-accurate decisions. To tackle class imbalance, we introduce MINT–RVAE, a multimodal recurrent variational autoencoder (VAE) [21] for imbalance-aware sequence augmentation, which significantly improves the generalization of intent detector models. The contributions of this paper are as follows:\n\nDataset: We release a novel dataset (anonymized) with frame-level onset annotations of human intention to interact with a robot arm across diverse indoor public-space scenarios, collected with informed consent and privacy safeguards. To our knowledge, this is the first publicly available dataset that enables the study of frame-level intent prediction, in contrast to previous datasets that provide only sequence-level labels.\n\nMINT-VAE for HRI data re-balancing: We introduce a multimodal recurrent VAE together with custom loss function combinations and training strategies for learning a joint sequential latent representation over pose, emotions, and intent labels, with the goal of generating temporally coherent, cross-modal synthetic sequences for alleviating training data imbalance\n\nRGB-only intent detection: We develop a novel modular pipeline for the detection of human interaction intent (i.e., performing detection before any explicit interaction takes place) using only a single RGB camera, substantially simplifying hardware and cost overheads.\n\nExperiments: We train three different backbone networks (GRU, LSTM, Transformer), while using our proposed MINT-RVAE for data augmentation, and show that the resulting pipeline achieves strong detection performance in both frame-level and sequence-level cases (AUROC: 0.950.95), outperforming previously introduced methods (with typical AUROC: 0.900.90-0.920.92).",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的人机交互意图预测方法依赖于多模态输入，限制了系统的可扩展性和成本效益。  \n2. 真实世界中的人机交互数据集存在类别不平衡问题，影响模型的训练和泛化能力。  \n3. 现有方法在时间精度上不足，无法准确识别意图出现的时刻，影响机器人响应的及时性。  \n\n【提出了什么创新的方法】  \n本文提出了一种新颖的RGB-only管道，通过引入MINT-RVAE（多模态递归变分自编码器）来实现人类意图的帧级检测。该方法通过生成合成序列来解决类别不平衡问题，并结合自定义损失函数和训练策略，显著提高了意图检测模型的泛化能力。实验结果显示，该方法在帧级和序列级的检测性能上均达到了0.95的AUROC，超越了之前的研究（AUROC: 0.90-0.92）。此外，研究还公开发布了带有帧级标注的新数据集，以支持未来的研究。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment",
            "authors": "Xiaoyun Qiu,Haichao Liu,Yue Pan,Jun Ma,Xinhu Zheng",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22550",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22550",
            "arxiv_html_link": "https://arxiv.org/html/2509.22550v1",
            "abstract": "In mixed-traffic environments, where autonomous vehicles (AVs) must interact with diverse human-driven vehicles (HVs), the unpredictability of human intentions and heterogeneous driving behaviors poses significant challenges to safe and efficient lane change maneuvers. Existing methods often oversimplify these interactions by assuming uniform or fixed behavioral patterns.\nTo address this limitation, we propose an intention-driven lane change framework that integrates driving-style recognition with cooperation-aware decision-making and motion-planning. First, a deep learning-based classifier is developed to identify distinct human driving styles from the NGSIM dataset in real time. Second, we introduce a cooperation score composed of intrinsic and interactive components, which estimates surrounding drivers’ intentions and quantifies their willingness to cooperate with the ego vehicle’s lane change. Third, a decision-making module is designed by combining behavior cloning (BC) with inverse reinforcement learning (IRL) to determine whether a lane change should be initiated under current conditions. Finally, a coordinated motion-planning architecture is established, integrating IRL-based intention inference with model predictive control (MPC) to generate collision-free and socially compliant trajectories.\nExtensive experiments demonstrate that the proposed intention-driven BC–IRL model achieves superior performance, reaching 94.2% accuracy and 94.3% F1-score, and outperforming multiple rule-based and learning-based baselines. In particular, it improves lane change recognition by 4–15% in F1-score, highlighting the benefit of modeling inter-driver heterogeneity via intrinsic and interactive cooperation scores.\nBy bridging the gap between real-world human behaviors and automated lane change strategies, this work advances the development of context-aware and human-like AV systems for safe and efficient operations in complex traffic environments.",
            "introduction": "The rapid advancement of autonomous driving technology has accelerated the transition toward mixed-traffic environments where autonomous vehicles (AVs) will inevitably share roadways with human-driven vehicles (HVs) for the foreseeable future (Yurtsever et al. (2020)). Within this operational paradigm, the ability of AVs to accurately interpret and adapt to human drivers’ heterogeneous behavioral patterns during lane change maneuvers emerges as a critical safety imperative (Xing et al. (2021); Chen et al. (2022)). This challenge is particularly acute given the fundamental uncertainty inherent in human decision-making processes and the dynamic complexity of traffic interactions (Schwarting et al. (2018)).\n\nModern lane change algorithms have evolved from classical models (Gipps (1986)) to contemporary data-driven approaches (Zheng (2014); Wang et al. (2019b)), and can be broadly categorized into three paradigms: rule-based, data-based, and incentive-based systems. Recent advances in inverse reinforcement learning (Sun et al. (2022)) and probabilistic modeling (Sun et al. (2018)) have partially incorporated driver intentions into decision-making (Sheng et al. (2022)). Nevertheless, several critical limitations persist. Firstly, existing frameworks often rely on a behavioral homogeneity assumption, which oversimplifies surrounding human drivers by adopting uniform behavioral models and thereby overlooks inter-driver heterogeneity in decision-making. Secondly, although some methods incorporate personalized intentions, they typically fail to capture the temporal evolution of interactions among surrounding vehicles. Lastly, there remains an intention modeling gap: current prediction approaches lack mechanisms to effectively link observable driving styles with latent reward structures, resulting in a disconnect between behavior recognition and motion-planning.\n\nThese limitations are particularly problematic given that sudden lane changes account for about 17.0% of total severe crashes (Shawky (2020)), underscoring the urgent need for AV frameworks capable of simultaneously predicting trajectories and adapting to individual driving styles (Kuderer et al. (2015)).\n\nTo address the identified shortcomings in existing algorithms, we introduce an intention-driven lane change framework that synergistically integrates driving style recognition with adaptive decision-making as shown in Fig.1. In the context of this research, “driving style” is conceptualized as a habitual manner of vehicle operation, distinct to an individual or subset of drivers, that serves as a proxy for underlying personalized driving intentions. Our proposed lane change framework incorporates driving style into two distinct yet interrelated components: intention-prediction and motion-planning. The main contributions of this study are summarized as follows:\n\nWe introduce a cooperation score to quantify human drivers’ cooperative intentions during lane change interactions, where adaptive parameters are optimized to align with diverse driving styles.\n\nWe introduce a cooperation score to quantify human drivers’ cooperative intentions during lane change interactions, where adaptive parameters are optimized to align with diverse driving styles.\n\nWe design a dual-perspective intention-prediction framework that integrates intrinsic vehicle states with interactive contextual cues, thereby improving the interpretability, adaptability, and prediction accuracy of cooperation-aware lane change behavior.\n\nWe develop a BC–IRL integrated decision-making model that produces human-like lane change strategies conditioned on the inferred cooperation score and surrounding traffic context.\n\nWe propose an IRL–MPC pipeline that jointly learns driver-specific reward functions and generates collision-free, kinematically feasible trajectories, enabling safe and personalized maneuver planning.\n\n1. We introduce a cooperation score to quantify human drivers’ cooperative intentions during lane change interactions, where adaptive parameters are optimized to align with diverse driving styles.\n\n2. We design a dual-perspective intention-prediction framework that integrates intrinsic vehicle states with interactive contextual cues, thereby improving the interpretability, adaptability, and prediction accuracy of cooperation-aware lane change behavior.\n\n3. We develop a BC–IRL integrated decision-making model that produces human-like lane change strategies conditioned on the inferred cooperation score and surrounding traffic context.\n\n4. We propose an IRL–MPC pipeline that jointly learns driver-specific reward functions and generates collision-free, kinematically feasible trajectories, enabling safe and personalized maneuver planning.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有方法简化了人类驾驶行为的复杂性，未能有效处理驾驶者之间的异质性。  \n2. 传统算法未能捕捉周围车辆之间的动态交互与时间演变。  \n3. 当前的意图建模方法缺乏将可观察的驾驶风格与潜在奖励结构有效链接的机制。  \n\n【提出了什么创新的方法】  \n本文提出了一种意图驱动的变道框架，结合了驾驶风格识别、合作意识决策和运动规划。首先，开发了一个深度学习分类器，实时识别不同的人类驾驶风格。其次，引入了一个合作评分，评估周围驾驶者的意图和合作意愿。然后，设计了一个结合行为克隆（BC）和逆强化学习（IRL）的决策模块，以确定是否在当前条件下发起变道。最后，建立了一个协调的运动规划架构，将基于IRL的意图推断与模型预测控制（MPC）结合，生成无碰撞且符合社会规范的轨迹。实验结果表明，所提出的BC-IRL模型在变道识别方面的准确率和F1分数均达到94.2%和94.3%，显著优于多个基线方法，强调了通过内在和交互合作评分建模驾驶者异质性的优势。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes",
            "authors": "Katrina Ashton,Chahyon Ku,Shrey Shah,Wen Jiang,Kostas Daniilidis,Bernadette Bucher",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22498",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22498",
            "arxiv_html_link": "https://arxiv.org/html/2509.22498v1",
            "abstract": "Language-specified mobile manipulation tasks in novel environments simultaneously face challenges interacting with a scene which is only partially observed, grounding semantic information from language instructions to the partially observed scene, and actively updating knowledge of the scene with new observations. To address these challenges, we propose HELIOS, a hierarchical scene representation and associated search objective to perform language specified pick and place mobile manipulation tasks. We construct 2D maps containing the relevant semantic and occupancy information for navigation while simultaneously actively constructing 3D Gaussian representations of task-relevant objects. We fuse observations across this multi-layered representation while explicitly modeling the multi-view consistency of the detections of each object.\nIn order to efficiently search for the target object, we formulate an objective function balancing exploration of unobserved or uncertain regions with exploitation of scene semantic information.\nWe evaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and place benchmark in which perception is challenging due to large and complex scenes with comparatively small target objects. HELIOS achieves state-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also transfer to the real world without requiring additional data, as we illustrate by demonstrating it in a real world office environment on a Spot robot.\nVideos and code are available at our project website: https://helios-robot-perception.github.io/",
            "introduction": "Consider an autonomous robot tasked with bringing a mug from a coffee table to the kitchen counter in a home. If that robot sees a coffee table but cannot currently detect a mug on it, should it go closer to investigate if the mug is actually present? Or should it look in new parts of the home? An autonomous robot should be able to efficiently reason through this question using environment cues. In addition, the robot should be able to successfully perform this task of language-specified pick and place for mobile manipulation using the observations it accumulates during this search process.\n\nMethods for embodied physical intelligence can accumulate information about a novel scene and act on it though observation history with no explicit scene representation (Stone et al., 2023; Physical Intelligence et al., 2025; Team et al., 2025), only 2D maps (Yenamandra et al., 2023b; Melnik et al., 2023) or 3D scene graphs (Rana et al., 2023; Liu et al., 2024; Honerkamp et al., 2024). However, these methods all assume dense associations between language, observation, and action. Very different representations for long horizon spatio-temporal reasoning have been developed in problems for semantic search where language grounding is sparse (Georgakis et al., 2021; Yokoyama et al., 2023b; Chang et al., 2023). In order to perform mobile manipulation which includes semantic search, reasoning over vision, language, and action must occur simultaneously in both long and short horizons. Low success rates on new benchmarks targeting open vocabulary pick and place tasks in novel environments have demonstrated that combining this long and short horizon reasoning is still an open challenge (Liu et al., 2024; Yenamandra et al., 2023b).\n\nReasoning jointly over short and long spatio-temporal contexts requires very different policy objectives in addition to the differences in scene representations. Prior work in object search explicitly manages local and global search problems distinctly (Zheng et al., 2023; Schmalstieg et al., 2023; Li et al., 2022). Search policies must figure out when to switch between local and global reasoning by deciding the likelihood of being close to the target object. In addition to exploring unobserved regions, efficient search policies also exploit semantic information about the scene in order to search more likely locations of the target object first (Chaplot et al., 2020b; Ramakrishnan et al., 2022; Ye et al., 2021; Zhang et al., 2023; Georgakis et al., 2021; Yu et al., 2023; Yokoyama et al., 2023b). This exploration-exploitation tradeoff adds additional complexity to the task of performing object search as a component of mobile manipulation.\n\nContributions. We present HELIOS, a hierarchical scene representation and search objective for language specified mobile pick and place tasks in novel environments. We create a hierarchical scene representation using layered 2D value and occupancy maps to efficiently navigate and explore, and sparse collections of 3D Gaussians to represent objects of interest (see fig. 1). We then formulate an objective function on our hierarchical scene representation that balances exploring the scene to find regions which might contain the target object with exploiting observed semantic information. We introduce an uncertainty-weighted object score to take into account the multi-view consistency of the detections of an object before interacting with it. We conduct an ablation study to verify that each of these components increases our method’s performance.\nThrough our experiments, we show the contribution of uncertainty-based reasoning over our novel visual representation in improving robust perception in mobile manipulation. We evaluate HELIOS on the HomeRobot Open-vocabulary Mobile Manipulation benchmark (Yenamandra et al., 2023b; a) in the Habitat simulator (Savva et al., 2019b), achieving state-of-the-art results. We use HELIOS in semantic navigation as a stop decision, improving overall search success on the Habitat-Matterport 3D (Ramakrishnan et al., 2021) object search benchmark. The zero-shot nature of our approach means it can transfer to the real world without requiring additional data, as we show by demonstrating HELIOS in a real world office environment on a Spot robot.",
            "llm_summary": "【论文的motivation是什么】  \n1. 语言指定的移动操作任务在新环境中面临部分观察场景的交互挑战。  \n2. 需要将语言指令中的语义信息与部分观察到的场景进行有效关联。  \n3. 需要主动更新对场景的知识，以应对动态变化的环境。  \n\n【提出了什么创新的方法】  \n提出了HELIOS，一种层次化场景表示和搜索目标的方法，旨在执行语言指定的抓取和放置移动操作任务。该方法构建了包含相关语义和占用信息的2D地图，同时主动构建任务相关对象的3D高斯表示。通过在多层次表示中融合观察结果，并显式建模每个对象检测的多视图一致性，HELIOS能够有效地在未观察或不确定区域进行探索，同时利用场景的语义信息进行目标搜索。实验结果表明，HELIOS在OVMM基准测试中取得了最先进的结果，并且由于其零-shot特性，能够在不需要额外数据的情况下转移到现实世界。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Ontological foundations for contrastive explanatory narration of robot plans",
            "authors": "Alberto Olivares-Alarcos,Sergi Foix,Júlia Borràs,Gerard Canal,Guillem Alenyà",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Logic in Computer Science (cs.LO)",
            "comment": "This version was submitted to the journal Information Sciences and is under review since October 2024",
            "pdf_link": "https://arxiv.org/pdf/2509.22493",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22493",
            "arxiv_html_link": "https://arxiv.org/html/2509.22493v1",
            "abstract": "Mutual understanding of artificial agents’ decisions is key to ensuring a trustworthy and successful human-robot interaction. Hence, robots are expected to make reasonable decisions and communicate them to humans when needed.\nIn this article, the focus is on an approach to modeling and reasoning about the comparison of two competing plans, so that robots can later explain the divergent result. First, a novel ontological model is proposed to formalize and reason about the differences between competing plans, enabling the classification of the most appropriate one (e.g., the shortest, the safest, the closest to human preferences, etc.). This work also investigates the limitations of a baseline algorithm for ontology-based explanatory narration. To address these limitations, a novel algorithm is presented, leveraging divergent knowledge between plans and facilitating the construction of contrastive narratives. Through empirical evaluation, it is observed that the explanations excel beyond the baseline method.",
            "introduction": "Autonomous artificial decision-making in environments with different agents (e.g., robots collaborating with or assisting humans) is complex to model. This is often due to the high degree of uncertainty and potential lack of communication among agents. For instance, robots might need to choose between competing plans (i.e. sequences of actions that would allow the robot to achieve goals), comparing their properties and deciding which one is better. Note that this decision-making problem is different from finding a single plan through automated planning, as here the idea is that there are already two valid plans to execute and the robot shall compare them and identify the best one.\nThis might happen when a human gives an ambiguous command (e.g. ‘can you bring me a drink?’), thus the robot may decompose the abstract command into different concrete goals [1], and find a plan to achieve each of the goals (such as bringing any of the available drinks). Then it would be needed to compare and disambiguate the plans. In these cases, mutual understanding of the ongoing decisions and communication between agents become crucial [2]. Hence, trustworthy robots shall be able to model their plans’ properties to make sound decisions when contrasting them. Furthermore, they shall also be capable of narrating (explaining) the knowledge acquired from the comparison. Note that robots add the possibility of physically executing the plan, which may affect the human, strongly motivating the need for explanations. This may serve two purposes: justifying the robot’s selection of a plan, or asking the human to help in the disambiguation (i.e. the human may prefer the plan that the robot inferred as worse). Reflecting on these thoughts, this work addresses the following research questions:\n\nRQ1 - How could robots model and reason about what differentiates plans, making one better?\n\nRQ2 - How could robots leverage the proposed ontological model to explain (narrate) what differentiates plans?\n\nFirst, an ontological analysis is conducted and a new ontological model is obtained, augmenting the scope of an ontology from the literature (OCRA [3]), which answers RQ1. Specifically, a new theory for plan comparison is formalized, focusing on the properties and relationships that allow comparing plans. The robot’s knowledge about the plans to compare is stored, and together with some logical rules, it is used to infer which plan is better. Second, RQ2 is addressed by introducing a novel Algorithm for Contrastive eXplanatory Ontology-based Narratives (ACXON), extending an existing literature methodology (XONCRA [4]) to contrastive cases. From the robot knowledge, ACXON retrieves the divergent information about the plans, and then it constructs the final textual contrastive narrative. The proposed algorithm produces different types of narratives based on the chosen amount of detail (specificity), addressing different users’ preferences. Based on objective evaluation metrics and using several planning domains, the algorithm is evaluated with respect to the original algorithm proposed in XONCRA, which is used as a baseline. The proposed algorithm outperforms the baseline, using less knowledge to build the narratives (skipping repetitive knowledge), which shortens the time to communicate the narratives. Figure 1 provides an overview of the complete approach. At the end of the article, it is briefly discussed how the proposed algorithm can be slightly modified to enhance and restrict the knowledge selection, which helps to shorten the constructed narratives.111An extended abstract of this paper appears in the Proc. of AAMAS’24 [5].\n\n1. RQ1 - How could robots model and reason about what differentiates plans, making one better?\n\n2. RQ2 - How could robots leverage the proposed ontological model to explain (narrate) what differentiates plans?",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人需要在不确定环境中做出合理决策，以确保人机交互的信任和成功。  \n2. 机器人需要能够比较多个计划并解释其选择，以便与人类有效沟通。  \n\n【提出了什么创新的方法】  \n本文提出了一种新的本体模型，用于形式化和推理竞争计划之间的差异，从而帮助机器人识别最佳计划。通过引入一种新算法（ACXON），该算法利用计划间的差异知识，构建对比叙述，增强了机器人的解释能力。经过实证评估，结果显示该算法在叙述生成上优于基线方法，能够更快地传达信息并减少冗余知识的使用。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards",
            "authors": "Ben Rossano,Jaein Lim,Jonathan P. How",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22469",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22469",
            "arxiv_html_link": "https://arxiv.org/html/2509.22469v1",
            "abstract": "This paper proposes a task allocation algorithm for teams of heterogeneous robots in environments with uncertain task requirements. We model these requirements as probability distributions over capabilities and use this model to allocate tasks such that robots with complementary skills naturally position near uncertain tasks, proactively mitigating task failures without wasting resources. We introduce a market-based approach that optimizes the joint team objective while explicitly capturing coupled rewards between robots, offering a polynomial-time solution in decentralized settings with strict communication assumptions. Comparative experiments against benchmark algorithms demonstrate the effectiveness of our approach and highlight the challenges of incorporating coupled rewards in a decentralized formulation.",
            "introduction": "Heterogeneous multi-robot systems can achieve higher levels of efficiency and effectiveness than individual robots can by leveraging task parallelization and diverse sets of capabilities [1]. At the core of these systems lies multi-robot task allocation (MRTA), the process of determining how to distribute tasks across the team. Many critical missions, such as search and rescue [2] and disaster relief [3], require allocating tasks to robots in unknown environments. In these missions, robots may encounter disturbances that either prevent task completion or delay progress.\nSuch disturbances often arise from underlying uncertainty in the environment or the robots themselves. Making effective decisions in the face of uncertainty remains an open challenge in MRTA problems.\n\nOne approach to addressing uncertainty is to model mission parameters (e.g., task durations/requirements or agent speeds/sensing capabilities) as random variables and reformulate the problem as a probabilistic optimization [4]. A natural question then arises: how are stochastic mission parameters determined? Many methods assume that these parameters are given as a priori knowledge about the mission [5]. For example, in a time-sensitive disaster relief mission with high-value search tasks and lower-value infrastructure repair tasks, there may exist a known probability that search robots encounter untraversable terrain. This renders search tasks infeasible without assistance from larger support robots, which are otherwise assigned to infrastructure repair tasks. Allocation thus becomes a strategic challenge: planners must position support robots so that they can assist the search robots if necessary before task deadlines expire, while still maximizing the completion of infrastructure repair tasks.\n\nA priori knowledge about disturbances may not always be available, highlighting a key distinction in MRTA problem formulations: robustness versus resilience. Robustness is achieved by proactively computing pre-execution plans to anticipate modeled disturbances [6]. Many existing methods achieve robustness through redundancy [7] or idle pre-positioning [8], which can lead to an inefficient use of resources.\nOn the other hand, resilience is the ability of a multi-robot system to adapt to unmodeled disturbances [9]. Unlike robustness, resilience is achieved by reactively modifying plans in response to unanticipated conditions. Existing work under the resilient MRTA framework has primarily focused on detecting unmodeled disturbances and replanning with updated information only after task completion has been declared infeasible [10], [11].\n\nThis paper offers a novel perspective on resilience by emphasizing that reactive planning can still incorporate anticipatory actions. The key insight is that when an unmodeled disturbance is discovered, there is often a delay before the affected task is formally declared infeasible. Rather than treating this delay as wasted time, it can be leveraged to proactively reconfigure resources. For example, in the disaster relief scenario, when a search robot encounters untraversable terrain, it may take time to confirm infeasibility (e.g., the robot may first explore alternative routes). During this period, repositioning a support robot toward a nearby task enables faster intervention if the task ultimately fails, whereas existing reactive planners risk missing the deadline. Capturing this kind of proactive response requires a new way of quantifying uncertainty, since a priori mission parameters cannot account for unmodeled disturbances.\n\nTo encourage strategic allocations, we adjust task utilities to reflect not only intrinsic task value but also the potential benefit of supporting high-value, uncertain tasks (HVUTs). This adjustment incentivizes robots with support capabilities to remain productive on nearby tasks while simultaneously positioning themselves to assist HVUTs if needed. The resulting MRTA formulation is particularly challenging because of strong inter-robot dependencies: while all support robots benefit from positioning near HVUTs, only one will ultimately be reassigned if assistance is needed. Efficient allocation therefore requires capturing this coupling to avoid over-incentivizing robots to cluster around the same HVUTs. To address this, our algorithm introduces a market-based mechanism that resolves these dependencies, providing a polynomial-time solution suitable for decentralized settings with strict communication constraints. Furthermore, we evaluate how the solution quality changes as communication assumptions are relaxed under varying degrees of inter-robot coupling, illustrating the tradeoffs of formulating this problem in a decentralized setting.\nOur work can be summarized by three primary contributions:\n\nWe propose a model for task capability uncertainty that provides a unified framework for solving the multi-robot task allocation problem with both in-distribution and out-of-distribution disturbances.\n\nWe present a market-based algorithm to allocate tasks to robots in the face of these disturbances by strategically prioritizing tasks to maximize the expected mission rewards.\n\nWe demonstrate the utility of our algorithm through a comparative analysis against several benchmark algorithms, demonstrating how the mission performance changes with different assumptions.\n\n1. We propose a model for task capability uncertainty that provides a unified framework for solving the multi-robot task allocation problem with both in-distribution and out-of-distribution disturbances.\n\n2. We present a market-based algorithm to allocate tasks to robots in the face of these disturbances by strategically prioritizing tasks to maximize the expected mission rewards.\n\n3. We demonstrate the utility of our algorithm through a comparative analysis against several benchmark algorithms, demonstrating how the mission performance changes with different assumptions.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有多机器人任务分配方法在面对环境不确定性时效率低下。  \n2. 现有方法往往依赖于先验知识，无法适应未建模的干扰。  \n3. 需要一种新的方法来量化不确定性并优化任务分配。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于市场机制的多机器人任务分配算法，能够在面对不确定任务要求时有效分配任务。该方法通过建模任务能力的不确定性，优化机器人之间的相互依赖关系，确保支持机器人能够在关键时刻迅速响应高价值不确定任务。通过比较实验，验证了该算法在不同通信假设下的有效性，展示了在不确定环境中提升任务完成率的潜力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation",
            "authors": "Zhangyuan Wang,Yunpeng Zhu,Yuqi Yan,Xiaoyuan Tian,Xinhao Shao,Meixuan Li,Weikun Li,Guangsheng Su,Weicheng Cui,Dixia Fan",
            "subjects": "Robotics (cs.RO)",
            "comment": "This paper introduces the first VLA framework for AUVs, featuring a dual-brain architecture and zero-data MPC for real-world underwater navigation",
            "pdf_link": "https://arxiv.org/pdf/2509.22441",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22441",
            "arxiv_html_link": "https://arxiv.org/html/2509.22441v1",
            "abstract": "This paper presents UnderwaterVLA, a novel framework for autonomous underwater navigation that integrates multimodal foundation models with embodied intelligence systems. Underwater operations remain difficult due to hydrodynamic disturbances, limited communication bandwidth, and degraded sensing in turbid waters. To address these challenges, we introduce three innovations. First, a dual-brain architecture decouples high-level mission reasoning from low-level reactive control, enabling robust operation under communication and computational constraints. Second, we apply Vision–Language–Action (VLA) models to underwater robotics for the first time, incorporating structured chain-of-thought reasoning for interpretable decision-making. Third, a hydrodynamics-informed Model Predictive Control (MPC) scheme compensates for fluid effects in real time without costly task-specific training. Experimental results in field tests show that UnderwaterVLA reduces navigation errors in degraded visual conditions while maintaining higher task completion by 19%\\% to 27% over baseline. By minimizing reliance on underwater-specific training data and improving adaptability across environments, UnderwaterVLA provides a scalable and cost-effective path toward the next generation of intelligent AUVs.",
            "introduction": "Autonomous Underwater Vehicles (AUVs) are indispensable tools for marine science and industry, supporting missions ranging from seabed mapping and ecological monitoring to subsea infrastructure inspection and resource exploration [1]. Their ability to operate untethered in extreme environments provides unique advantages over remotely operated vehicles (ROVs), including extended range, reduced human supervision, and lower operational costs [2]. AUVs have enabled breakthroughs in deep-sea research and offshore engineering by delivering high-resolution data from environments that are otherwise inaccessible.\n\nYet, AUV autonomy remains fundamentally constrained by the hostile and unpredictable nature of the ocean. Unlike aerial or terrestrial domains, underwater environments present a combination of challenges:\n\nStrong, nonlinear hydrodynamics, where ocean currents, turbulence, and vortical structures introduce unmodeled disturbances that degrade stability and control [3, 4].\n\nStrong, nonlinear hydrodynamics, where ocean currents, turbulence, and vortical structures introduce unmodeled disturbances that degrade stability and control [3, 4].\n\nExtreme physical conditions, including high hydrostatic pressures, spatially varying salinity, and thermocline effects, which alter vehicle buoyancy and structural reliability [5].\n\nSevere perception difficulties, such as turbidity, backscatter, and non-uniform illumination, which compromise optical sensors; meanwhile, sonar imaging suffers from low resolution and multipath interference [6, 7].\n\nCommunication constraints, where radio is ineffective underwater and acoustic channels are bandwidth-limited, delay-prone, and vulnerable to noise [8].\n\nConventional approaches—including proportional–integral–derivative (PID) controllers, adaptive model-based control, and deep reinforcement learning—have shown partial success, but often lack generalization and robustness in unstructured marine environments [9, 10]. Recent efforts such as factor-graph-based cooperative localization [11] and data-informed domain randomization for reinforcement learning [12, 13] have improved resilience but remain constrained by reliance on accurate modeling and costly task-specific training. This highlights the need for autonomy frameworks capable of generalizing across tasks and adapting to uncertainty without prohibitive data requirements.\n\nThe emerging Vision–Language–Action (VLA) paradigm offers such potential. By unifying perception, natural language understanding, and action generation, VLA systems enable multimodal reasoning and robust task generalization. These models have achieved remarkable success in diverse domains, including industrial manipulation, autonomous driving, aerial robotics, and quadruped locomotion [14, 15, 16, 17]. For instance, OpenVLA demonstrates unified multimodal control and instruction following [18], while DiffusionVLA shows how large-scale diffusion and autoregressive models can enhance general-purpose robot performance [19]. QUAR-VLA has successfully applied VLA models to quadrupeds for locomotion and manipulation [16]. Such advances indicate strong potential for VLA to move beyond task-specific controllers toward general-purpose autonomy with natural language interaction and contextual understanding.\n\nHowever, VLA remains largely unexplored in underwater robotics [20]. Three barriers explain this gap: (i) No underwater-suited hierarchy: Existing end-to-end VLA models cannot effectively decouple high-level mission planning from low-level real-time control under the stringent communication and computational constraints of underwater operations, leading to insufficient system robustness.\n(ii) Data-hungry policies: Conventional VLA require vast amounts of expensive underwater demonstration data to learn policies, severely limiting scalability and generalization due to prohibitive data collection costs [12].\n(iii) No real-time hydro-compensation: Nonlinear fluid dynamics (e.g., turbulence, vortices) significantly disrupt AUV motion; existing VLA or learning-based controllers lack embedded physical priors and cannot perform real-time dynamic compensation without additional task-specific training, resulting in degraded navigation accuracy [21, 5].\nThese challenges have previously hindered the application of VLA frameworks to AUVs, despite their transformative promise. Importantly, recent advances in VLA research increasingly converge toward model-based, zero-data training paradigms — leveraging physical priors [22].\n\nTo address these issues, we present Underwater VLA, the first VLA framework specifically designed for AUVs. Our system systematically addresses the unique challenges of underwater environments by introducing a zero-data training methodology, cross-domain adaptation with physical priors, and a dual-brain architecture that integrates high-level reasoning with low-level control for robust autonomy.\n\nThe main contributions of this paper are summarized as follows:\n\nUnderwater VLA framework: We introduce the first VLA framework tailored for AUVs, systematically addressing underwater-specific challenges such as limited communication, harsh sensing conditions, and costly data collection.\n\nZero-data training methodology: We propose a training strategy that minimizes reliance on underwater demonstrations by leveraging pre-trained multimodal foundation models with underwater-specific transfer learning.\n\nDual-brain architecture: We design a hybrid control structure that decouples high-level deliberative reasoning from low-level reactive control, ensuring reliable autonomy under computational constraints and enhancing safety.\n\nExperimental validation: We demonstrate that Underwater VLA achieves superior language understanding, task generalization, and mission success rates compared to conventional control and previous VLA baseline.\n\nThe remainder of this paper is organized as follows: Section I reviews related work on AUV autonomy and VLA frameworks. Section II details the methodology of Underwater VLA. Section III presents experimental results and discussion. Section IV concludes the paper with future directions.\n\n1. Underwater VLA framework: We introduce the first VLA framework tailored for AUVs, systematically addressing underwater-specific challenges such as limited communication, harsh sensing conditions, and costly data collection.\n\n2. Zero-data training methodology: We propose a training strategy that minimizes reliance on underwater demonstrations by leveraging pre-trained multimodal foundation models with underwater-specific transfer learning.\n\n3. Dual-brain architecture: We design a hybrid control structure that decouples high-level deliberative reasoning from low-level reactive control, ensuring reliable autonomy under computational constraints and enhancing safety.\n\n4. Experimental validation: We demonstrate that Underwater VLA achieves superior language understanding, task generalization, and mission success rates compared to conventional control and previous VLA baseline.",
            "llm_summary": "【论文的motivation是什么】  \n1. AUV autonomy受限于复杂的水下环境，面临多种挑战。  \n2. 现有的控制方法缺乏在不确定性下的泛化能力和鲁棒性。  \n3. VLA框架在水下机器人领域尚未得到充分探索。  \n\n【提出了什么创新的方法】  \n本论文提出了UnderwaterVLA，一个专为AUV设计的VLA框架，解决了水下环境的独特挑战。首先，采用零数据训练方法，减少对昂贵水下示范数据的依赖。其次，设计了双脑架构，将高层推理与低层控制解耦，以确保在计算约束下的可靠自主性。最后，通过实验验证，UnderwaterVLA在语言理解、任务泛化和任务成功率上显著优于传统控制方法和先前的VLA基线，展示了其在复杂水下环境中的有效性和适应性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics",
            "authors": "Margherita Martorana,Francesca Urgese,Ilaria Tiddi,Stefan Schlobach",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22434",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22434",
            "arxiv_html_link": "https://arxiv.org/html/2509.22434v1",
            "abstract": "Personal service robots are increasingly deployed to support daily living in domestic environments, particularly for elderly and individuals requiring assistance. Operating effectively in these settings involves not only physical interaction but also the ability to interpret dynamic environments, understand tasks, and choose appropriate actions based on context. This process requires integrating both hardware components (e.g. sensors, actuators) and software systems capable of reasoning about tasks, environments, and the robot’s own capabilities. Framework such as the Robot Operating System (ROS) provide open-source libraries and tools that help connect low-level hardware data with higher-level functionalities. However, real-world deployments remain tightly coupled to specific hardware and software platforms. As a result, solutions are often isolated and hard-coded, limiting interoperability, reusability, and knowledge sharing. Ontologies and knowledge graphs offer a structured and interpretable way to represent tasks, environments, and robot capabilities. Existing ontologies, such as the Socio-physical Model of Activities (SOMA) and the Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE), provide models for representing activities, spatial relationships, and reasoning structures. However, they often focus on specific domains and do not fully capture the connection between environment, action, robot’s capabilities, and system-level integration. In this work, we propose the Ontology for roBOts and acTions (OntoBOT), which builds upon and extends existing ontologies to provide a unified representation of tasks, actions, environments, and capabilities. Our contributions are twofold: (1) we unify these core aspects into a cohesive ontology to support formal reasoning about task execution, and (2) we demonstrate its generalizability by evaluating competency questions across four distinct embodied agents — TIAGo, HSR, UR3, and Stretch — each with different capabilities, showing how OntoBOT enables context-aware reasoning and facilitates task-oriented execution, thereby promoting knowledge sharing in service robotics.",
            "introduction": "Personal service robots are increasingly deployed to assist in domestic environments, particularly in support of older adults and individuals requiring physical assistance (Macis et al., 2023; Nanavati et al., 2024; Holland et al., 2021; Roy et al., 2000; Sørensen et al., 2024). The tasks that service robots typically perform range from cleaning and tidying to meal preparation and care-giving, often within open-ended, dynamic, and unstructured environments. Effective operation in these settings requires a high-level understanding of the context in which tasks are performed, the specific actions involved, and the robot’s own physical and functional capabilities (Paulius and Sun, 2019; Bajd et al., 2010). This process depends on the integration of hardware components (e.g. sensors, actuators) with software systems capable of representing and reasoning about tasks, environments, and the robot’s own capabilities. Middleware platforms like the Robot Operating System (ROS)111https://www.ros.org/ (Quigley et al., 2009) offer open-source tools and libraries to help connect low-level hardware with higher-level functionalities. However, real-world deployments are often tightly coupled to isolated and platform-dependent solutions (Axelsson, 2015; García et al., 2023; Wang et al., 2024), with hard-coded instructions designed for specific hardware and software stacks (Axelsson, 2015; García et al., 2023), limiting interoperability, knowledge sharing, cross-domain reusability and system integration.\n\nOntologies and Knowledge Graphs (KG) offer a promising approach for representign robotic knowledge in a structured, shareable, and machine-interpretable knowledge (Paulius and Sun, 2019). Several ontologies relevant to robotics have been proposed, including efforts toward standardization such as the IEEE Robotics and Automation Society (RAS) Ontologies (Prestes et al., 2013; Gonçalves et al., 2021; Schlenoff et al., 2017), representations like the Ontology for Collaborative Robotics and Adaptation (OCRA) (Olivares-Alarcos et al., 2022) and the Autonomous Robot Task Processing Framework (ART-ProF) (Ge et al., 2024a), and foundational models such as the Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) (Masolo et al., 2003; Borgo et al., 2022) and the Socio-physical Model of Activities (SOMA) (Beßler et al., 2021). These ontologies provide a formal representation for reasoning about objects, actions, spatial relationships, and activities in physical and social contexts. However, they focus on specific aspects of robotic systems, such as task planning or perception, and do not capture the full interplay between a robot’s capabilities, the actions required to complete a task, and the environment in which those tasks occur. This creates a gap when reasoning about whether a robot is suited to perform a given task in a particular context, especially in heterogeneous, dynamic and collaborative settings. For example, domestic environments such as kitchens often involve recurring tasks and spatial configurations. While the robots deployed in these settings may vary in hardware and capabilities, the structure of the environment and the nature of the tasks often remain comparable. Existing ontologies can partially represent these individual elements – such as actions, objects, or spatial relations – but they lack an integrated way to connect between each other, and to robot-specific capabilities.\n\nIn this work, we introduce the Ontology for roBOts and acTions (OntoBOT)222https://w3id.org/onto-bot, a unified model designed to formally represents the interconnected elements of robotic task execution, namely: the robot and its capabilities, the task it has to perform, and the environment in which it operates. Through OntoBOT, we achieved two key contributions: unifying these core aspects into a cohesive semantic framework, and enabling reasoning about task execution across embodied agents, as demonstrated through the evaluation of competency questions involving four different robots — TIAGo, HSR, UR3, and Stretch — with varying capabilities. All materials are publicly available on GitHub333https://github.com/kai-vu/OntoBOT, including a Jupyter notebook specifically developed for reproducing the competency question evaluation444https://github.com/kai-vu/OntoBOT/blob/main/case-study/cqs.ipynb.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有服务机器人在动态环境中执行任务时缺乏对任务、环境和自身能力的全面理解。  \n2. 现有的本体和知识图谱无法有效整合机器人能力、任务和环境之间的关系。  \n3. 机器人系统的解决方案往往依赖于特定的硬件和软件平台，限制了互操作性和知识共享。  \n\n【提出了什么创新的方法】  \n本文提出了“Ontology for roBOts and acTions (OntoBOT)”，旨在统一表示任务、动作、环境和能力之间的关系。该方法通过构建一个连贯的本体框架，支持对任务执行的形式推理，并通过对四种不同能力的机器人（TIAGo、HSR、UR3和Stretch）进行能力问题的评估，展示了其通用性。OntoBOT促进了上下文感知推理和任务导向执行，从而增强了服务机器人领域的知识共享。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive Grasping",
            "authors": "Leonel Giacobbe,Jingdao Chen,Chuangchuang Sun",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22421",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22421",
            "arxiv_html_link": "https://arxiv.org/html/2509.22421v1",
            "abstract": "Grasping is a core task in robotics with various applications. However, most current implementations are primarily designed for rigid items, and their performance drops considerably when handling fragile or deformable materials that require real-time feedback. Meanwhile, tactile-reactive grasping focuses on a single agent, which limits their ability to grasp and manipulate large, heavy objects.\nTo overcome this, we propose a learning-based, tactile-reactive multi-agent Model Predictive Controller (MPC) for grasping a wide range of objects with different softness and shapes, beyond the capabilities of preexisting single-agent implementations. Our system uses two Gelsight Mini tactile sensors [1] to extract real-time information on object texture and stiffness. This rich tactile feedback is used to estimate contact dynamics and object compliance in real time, enabling the system to adapt its control policy to diverse object geometries and stiffness profiles. The learned controller operates in a closed loop, leveraging tactile encoding to predict grasp stability and adjust force and position accordingly.\nOur key technical contributions include a multi-agent MPC formulation trained on real contact interactions, a tactile-data driven method for inferring grasping states, and a coordination strategy that enables collaborative control.\nBy combining tactile sensing and a learning-based multi-agent MPC, our method offers a robust, intelligent solution for collaborative grasping in complex environments, significantly advancing the capabilities of multi-agent systems.\nOur approach is validated through extensive experiments against independent PD and MPC baselines. Our pipeline outperforms the baselines regarding success rates in achieving and maintaining stable grasps across objects of varying sizes and stiffness.",
            "introduction": "Robotic grasping and manipulation have been the subject of extensive research, with many approaches promising to be valid methods to enhance accuracy in both initial grasp success and the ability to maintain stable grips[2]. However, the majority of these methods have focused on rigid objects, often overlooking the challenges posed by soft or irregularly shaped items. Grasping and manipulating such objects remains difficult due to their compliance, variable surface properties, and the need to apply finely tuned contact forces.\n\nHumans naturally rely on tactile feedback to infer properties such as stiffness, texture, and rigidity, which are crucial information when handling objects. In robotics, tactile sensors offer the potential to replicate this capability, providing dense, high-dimensional data well-suited for learning-based controllers. The rich sensory information embedded in tactile images can be effectively leveraged by neural networks to improve performance in grasping and manipulation in commonplace robotic scenarios.\n\nDespite progress in single-agent tactile control, effective multi-agent grasping coordination remains a fundamental challenge in manipulation tasks, especially in unstructured, cluttered environments.\nRecent work, such as LeTac-MPC [3], proposed a learning-based Model Predictive Control (MPC) layer for tactile-reactive grasping using a single robotic manipulator. While promising, their implementation assumes ideal object positioning and orientation, which may not be a realistic expectation in real-world environments. In real-world deployments, where grasp prediction algorithms such as [4] are employed, performance can decrease significantly, as some objects can be quite challenging to grasp and manipulate with a single contact point if that point is not ideal [5].\n\nEnvironments such as construction sites, where large, heavy and irregularly shaped materials are frequently manipulated, exemplify this need. In those contexts, the inherent limitations of single-agent grasping become apparent. For instance, consider the task of lifting a heavy structural beam. A single robotic manipulator, even when executing an optimal grasp, would struggle due to the beam’s extended length and weight. These factors increase the likelihood of unstable or failed grasps due to contacts being limited to a single point. Other situations where multi-agency could be leveraged include search and rescue missions (with delicate handling of patients), food preparation, and robotic surgery. A multi-agent controller addresses the issue of requiring close to optimal grasping positions by increasing the number of contact points (which improves grasp stability), and by enabling collaborative behaviors that facilitate smoother and more natural manipulation of complex or irregularly shaped objects.\n\nWe propose a multi-agent, collaborative framework of a tactile-reactive Model Predictive Control (MPC) layer. This layer enables coordinated bi-manual manipulation, broadening the range of objects that can effectively be grasped and manipulated, particularly those that are difficult to handle with a single manipulator.\n\nIn spite of the many uses of tactile-based controllers, the deployment of these approaches comes with some inherent limitations:\n\nChallenges with soft objects: Tactile sensors rely on gel deformations to infer qualities about the grasped objects. When interacting with objects that are softer than the sensor’s gel, the resulting deformations are often minimal or imperceptible in the captured tactile images. This can lead the controller to interpret such interactions as no-contact scenarios, thus failing to execute appropriate grasping behaviors.\n\nComputational Complexity: Learning-based control methods are naturally computationally expensive, especially when run at high frequencies. They also require a high amount of data points to operate. Homogeneous multi-agent approaches look to make better use of collected experience by sharing parameters across agents. High-frequency control is crucial in manipulation tasks, especially when rapid changes in conditions occur due to collisions or unexpected dynamics, so elevated computing power is required to make these controllers feasible. Two main factors affect controller runtime: the dimensionality of the tactile image encoding and the length of the MPC prediction horizon. Although increasing either of these would theoretically improve prediction accuracy, it also incurs higher computational cost and may lead to optimization issues such as excessively large initial losses during training that yield a non-converging controller. Thus, a balance must be struck between expressiveness and real-time feasibility.\n\nOur proposed multi-agent tactile-reactive layer addresses the aforementioned challenges as follows:\n\nBi-manual tactile-reactive grasping of soft objects for improved stability: By distributing contact forces across two manipulators, our system reduces the likelihood of applying excessive localized pressure. This not only decreases the risk of permanent damage to grasped objects but also provides more stable manipulation through more contact points.\n\nBi-manual tactile-reactive grasping of soft objects for improved stability: By distributing contact forces across two manipulators, our system reduces the likelihood of applying excessive localized pressure. This not only decreases the risk of permanent damage to grasped objects but also provides more stable manipulation through more contact points.\n\nA collaborative learning based control approach with improved efficiency: According to our experimental evaluations, the introduction of an additional manipulator does not result in a proportional increase in runtime due to the leveraging of parameter sharing across the learnable MPC and CNN layers. This indicates that, although the overall system is slower compared to the baseline single-agent case [3], the per-agent efficiency improves. The relative runtime improvement arises mainly from a more efficient construction of the state transition matrices and lifted system matrices.\n\nReal-world comparative evaluation between baseline models and our proposed approach: In our experiments, two GelSight Mini sensors are mounted on Robotiq 2F-series grippers, with all trials conducted under an identical configuration. An extensive evaluation on various objects across multiple baselines is conducted to showcase the advantage of the proposed pipeline.\n\nTactile Grasping.\nThe main focus of tactile sensors is to mimic the information stream human touch can provide, enabling robotic systems to extract fine-grained information about grasped objects and dynamically adapt their opening and grasping forces. Compared to conventional modalities such as vision or point cloud data, tactile images are significantly richer in encoded contact information, making them especially effective for training data-driven controllers.\nWhile image-based tactile sensors remain the norm, recent research has explored augmenting them with complementary information streams, such as ultrasound [6] and Vision-Language-Action [7] [8], which have demonstrated promising results in expanding the horizons of tactile sensors.\n\nA key challenge, however, lies in the lack of tactile sensing datasets and the difficulty of modeling tactile sensors in simulation. Work such as [9] has addressed this limitation by leveraging measured contact forces at an end-effector to generate realistic synthetic tactile images, helping to bridge the gap between simulation and deployment. These high-fidelity tactile images could prove very useful in the future, as most current learning-based tactile approaches, including ours, rely exclusively on real sensor data, which can be time-consuming to acquire.\n\nBi-manual Manipulation.\nIn recent years, significant progress has been made in bi-manual robotic manipulation, with many approaches aiming to replicate the dexterity and rich sensing capabilities of human arms. Collaboration between the two manipulators is often achieved through Reinforcement Learning [10] or Imitation Learning [11]. A central challenge in this field is accurately discretizing and modeling the environment along with the full range of possible interactions between agents, which makes long-horizon, multi-step tasks particularly complex [12]. This is one of the reasons our approach utilizes parallel-jaw grippers with one degree of freedom: they substantially reduce the action space compared to multi-finger grippers, simplifying the learning process and making it more tractable.\n\nLearning-based MPC.\nModel Predictive Control (MPC) is one of the most robust and widely accepted control frameworks in robotics. However, traditional MPC approaches often face limitations when applied to systems that are inherently difficult to model with high fidelity. Learning-based MPC seeks to overcome these challenges by incorporating real-world (sensory) data to refine system dynamics, enabling the design of more accurate and adaptive controllers. In many cases, learning-based MPC has been shown to achieve superior performance, streamline deployment, and reduce the need for extensive manual controller tuning compared to baseline MPC methods [13]. Applications of MPC span a wide range of domains, including dexterous manipulation [14], autonomous vehicle steering [15], and Unmanned Aerial Vehicles (UAVs) [16].\n\nThe remainder of the paper is organized as follows: In the Data Collection section, we provide an overview of the methods we employed to obtain data for training our learned controller.\nIn the Pipeline Architecture section, we describe the individual components of the proposed system and their interactions, outlining how they collectively enable multi-agent tactile reactive grasping. In the Results section, we report on training and experiment results, comparing the performance of our method against established baselines. Following that, in the Advantages over Single-agent Model section, we present a comparative analysis highlighting the benefits of the proposed layer relative to baseline single-agent approaches. Lastly, in the Conclusion, Limitations and Future Work section, we summarize the contributions, discuss the current limitations, and outline potential directions for future research.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的抓取方法主要针对刚性物体，处理软性或变形物体时性能显著下降。  \n2. 单一智能体的触觉反应抓取限制了对大型重物的抓取和操作能力。  \n3. 多智能体协作抓取在复杂环境中的有效性仍然是一个基本挑战。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于学习的触觉反应多智能体模型预测控制（MPC）框架，旨在实现双手协作抓取。该系统利用Gelsight Mini触觉传感器实时提取物体的纹理和刚度信息，通过丰富的触觉反馈估计接触动态和物体的顺应性，从而自适应控制策略以应对不同物体的几何形状和刚度特征。通过多智能体MPC的协作控制，该方法显著提高了在复杂环境中抓取和操作各种物体的能力。实验结果表明，该方法在稳定抓取的成功率上超越了独立的PD和MPC基线，展示了其在处理不同尺寸和刚度物体时的优势。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation",
            "authors": "Enguang Liu,Siyuan Liang,Liming Lu,Xiyu Zeng,Xiaochun Cao,Aishan Liu,Shuchao Pang",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22356",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22356",
            "arxiv_html_link": "https://arxiv.org/html/2509.22356v1",
            "abstract": "The safety and reliability of embodied agents rely on accurate and unbiased visual perception. However, existing benchmarks mainly emphasize generalization and robustness under perturbations, while systematic quantification of visual bias remains scarce. This gap limits a deeper understanding of how perception influences decision-making stability. To address this issue, we propose RoboView-Bias, the first benchmark specifically designed to systematically quantify visual bias in robotic manipulation, following a principle of factor isolation. Leveraging a structured variant-generation framework and a perceptual-fairness validation protocol, we create 2,127 task instances that enable robust measurement of biases induced by individual visual factors and their interactions. Using this benchmark, we systematically evaluate three representative embodied agents across two prevailing paradigms and report three key findings: (i) all agents exhibit significant visual biases, with camera viewpoint being the most critical factor; (ii) agents achieve their highest success rates on highly saturated colors, indicating inherited visual preferences from underlying VLMs; and (iii) visual biases show strong, asymmetric coupling, with viewpoint strongly amplifying color-related bias. Finally, we demonstrate that a mitigation strategy based on a semantic grounding layer substantially reduces visual bias by approximately 54.5% on MOKA. Our results highlight that systematic analysis of visual bias is a prerequisite for developing safe and reliable general-purpose embodied agents.",
            "introduction": "The safety and reliability of general-purpose robots depend on accurate and unbiased visual perception, which is the primary channelLiu et al. (2025c) through which embodied agentsMa et al. (2024); Li et al. (2024b) perceive and act in the physical world. In hierarchical control, top-level vision-language planners can be biased with respect to color, viewpoint, or scale. Such biases can be amplified as high-level plans are broken into steps and constraints, destabilizing both planning and execution. These observations underscore the need to explore the safety  of multimodal large language models (MLLMs) Liang et al. (2023; 2025b; 2025a); Liu et al. (2025b), motivating research on alignment Ho et al. (2024), robustness Wang et al. (2025); Zhang et al. (2024); Ying et al. (2024), and bias mitigation Xiao et al. (2025) in vision-language systems.\n\nExisting robot manipulation benchmarks primarily evaluate an algorithm’s generalization James et al. (2020); Zhu et al. (2020); Heo et al. (2023); Pumacay et al. (2024); Luo et al. (2025) and robustness Puig et al. ; Xie et al. (2024); Li et al. (2024a); Liu et al. (2025a) under new tasks and environment changes. However, common metrics emphasize average success rates while overlooking variation and instability across visual attributes, thereby hiding failure risks under specific visual conditions. Specifically, they rarely independently isolate and quantify systematic biases from visual attributes, such as color and camera viewpoint, under controlled conditions. They also lack sensitivity and interaction metrics along the perception-to-decision pipeline, as well as fair and clear comparison sets.\n\nWe introduce RoboView-Bias, a benchmark to systematically quantify visual bias in robots using the principle of factorial isolation.\nTo generate evaluation instances, our structured variant-generation framework (SVGF) partitions all variables into two disjoint sets.\n❶ Dimensions of Visual Perturbation (V), comprise the attributes under evaluation: 141 object colors, 9 camera orientations, 21 full camera poses, and 9 distance scales.\n❷ Dimensions of Task Context Generalization (D), includes 4 initial positions, 4 shapes, and 3 language instructions to ensure robust findings across diverse task contexts. This methodology yields 2,127 instances and each instance is further validated for perceptual fairness, ensuring it is visually clear and solvable.\n\nIn the RoboView-Bias benchmark, we comprehensively evaluated two prevailing paradigms of embodied agents. The results show that these agents exhibit pronounced visual bias. In controlled trials where only the camera viewpoint (pose) varied while all other factors were fixed, success rates fluctuated sharply even across nearby viewpoints, identifying viewpoint as the most influential factor. Similarly, color-focused trials revealed a strong performance bias towards high-saturation hues over achromatic and low-saturation ones, with the extent of the bias varying by agents. In factorial (“color × viewpoint”) experiments, analyses of the interaction effect showed that viewpoint changes substantially amplify color-induced performance variation, whereas the reverse effect is weaker. This reveals a strong, asymmetric coupling between the two factors and motivates joint evaluation and mitigation. Based on these observations, we propose the “Semantic Grounding and Perceptual Calibration” (SGL) strategy. We execute pre-training alignment instructions and visible evidence, employing color-invariant calibration to reduce visual bias on MOKA Liu et al. (2024a). This research advances the systematic measurement of visual bias, providing a foundation for bias diagnosis and mitigation to enhance embodied agent stability. Our contributions can be summarized in three key aspects:\n\nWe present RoboView-Bias, a factor-isolated benchmark (color, camera viewpoint) that enables quantitative measurement of visual bias in embodied manipulation.\n\nWe provide cross-paradigm evaluations (VLM-driven, VLA) with fine-grained bias profiles, revealing significant bias and strong, asymmetric color–viewpoint coupling along the perception–decision pipeline.\n\nWe introduce SGL (Semantic Grounding Layer), which aligns commands with visible evidence before execution, reducing visual bias and improving agent stability.\n\n1. We present RoboView-Bias, a factor-isolated benchmark (color, camera viewpoint) that enables quantitative measurement of visual bias in embodied manipulation.\n\n2. We provide cross-paradigm evaluations (VLM-driven, VLA) with fine-grained bias profiles, revealing significant bias and strong, asymmetric color–viewpoint coupling along the perception–decision pipeline.\n\n3. We introduce SGL (Semantic Grounding Layer), which aligns commands with visible evidence before execution, reducing visual bias and improving agent stability.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有基准未能系统量化机器人操作中的视觉偏差。  \n2. 视觉偏差影响决策稳定性，需深入理解其影响。  \n3. 需要开发安全可靠的通用具身智能体，依赖于准确的视觉感知。  \n\n【提出了什么创新的方法】  \n本研究提出了RoboView-Bias基准，采用因素隔离原则系统量化机器人操作中的视觉偏差。通过结构化变体生成框架，创建了2,127个任务实例，评估了不同视觉因素的偏差及其相互作用。研究发现所有代理均表现出显著的视觉偏差，尤其是摄像机视角对成功率影响最大。引入的“语义对齐与感知校准”（SGL）策略有效减少了约54.5%的视觉偏差，提升了代理的稳定性。这一工作为视觉偏差的系统分析和缓解提供了基础，推动了具身智能体的安全性和可靠性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM",
            "authors": "Johan Hatleskog,Morten Nissov,Kostas Alexis",
            "subjects": "Robotics (cs.RO)",
            "comment": "accepted by The 22nd International Conference on Advanced Robotics (ICAR 2025). Supplementary video:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.22288",
            "code": "https://youtu.be/95jeWXBMN7c",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22288",
            "arxiv_html_link": "https://arxiv.org/html/2509.22288v1",
            "abstract": "Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor graph node per measurement to compensate for the lack of time synchronization between radar and LiDAR. For a radar-LiDAR sensor pair with equal rates, this strategy results in a state creation rate of twice the individual sensor frequencies. This doubling of the number of states per second yields high optimization costs, inhibiting real-time performance on resource-constrained hardware. We introduce IMU-preintegrated radar factors that use high-rate inertial data to propagate the most recent LiDAR state to the radar measurement timestamp. This strategy maintains the node creation rate at the LiDAR measurement frequency. Assuming equal sensor rates, this lowers the number of nodes by \\qty50 and consequently the computational costs. Experiments on a single board computer (which has 4 cores each of \\qty2.2\\giga A73 and \\qty2\\giga A53 with \\qty8\\giga RAM) show that our method preserves the absolute pose error of a conventional baseline while simultaneously lowering the aggregated factor graph optimization time by up to \\qty56.",
            "introduction": "Accurate and reliable state estimation is a prerequisite for autonomous navigation in GPS-denied and perceptually challenging environments. Recently, Frequency-Modulated Continuous Wave (FMCW) radar has gained increasing attention as a complementary sensor to LiDAR. FMCW radar systems offer direct Doppler velocity measurements and operate robustly in degraded visual conditions such as dust, fog, rain, or darkness as well as geometrically uninformative environments such as tunnels or open spaces [1]. When fused with LiDAR and inertial measurements, radar measurements can improve resilience in scenarios where LiDAR-only approaches degrade.\n\nIn factor graph-based smoothing for multi-modal sensor fusion, the conventional method to address measurement asynchronicity is to create a distinct state node in the graph for each measurement from each asynchronous sensor. For a typical Radar-LiDAR-Inertial setup with a \\qty10 radar and a \\qty10 LiDAR, this results in 20 state nodes being added to the graph every second. The theoretic computational complexity of factor graph optimization scales quadratically with the number of states in the optimization window [2]. Consequently, this doubling of the state creation rate imposes a significant computational burden, which can inhibit real-time performance, especially on the resource-constrained, embedded systems commonly found on mobile robots.\n\nTo address this limitation, we propose a principled approach that avoids creating state nodes for radar measurements. Instead, we introduce IMU-preintegrated radar factors. Our method creates state nodes only upon the arrival of a LiDAR measurement. When a radar measurement is received, we use high-rate IMU data to propagate the state from the preceding LiDAR node to the timestamp of the radar measurement. This propagation yields a predicted state that is used to evaluate the radar factor residual, creating a constraint on the previous LiDAR node without adding a new state to the graph.\n\nWith \\qty10 radar-LiDAR sensor rates, this strategy reduces the number of nodes in the fixed-lag smoother by \\qty50 compared to the standard approach. Our main contributions are:\n\nA new IMU-preintegrated radar factor that removes the need for dedicated radar state nodes.\n\nA computationally efficient factor graph smoother formulation, utilizing the IMU-preintegrated radar factor, suitable for real-time Radar-LiDAR-Inertial fusion on resource-constrained embedded hardware.\n\nExperimental evaluations on real-world datasets on constrained hardware, demonstrating that our approach substantially reduces computational load with a negligible effect on localization accuracy compared to a conventional baseline.\n\nThe remainder of this work is organized as follows: Section II presents related work and the proposed approach is described in Section III. The evaluation study is presented in Section IV and we conclude in Section V.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的多传感器融合方法在处理异步传感器数据时，导致状态节点数量过多，增加了计算负担。  \n2. 资源受限的嵌入式系统上，实时性能受到影响，难以满足自主导航的需求。  \n\n【提出了什么创新的方法】  \n本文提出了一种新颖的IMU预积分雷达因子方法，避免为雷达测量创建独立的状态节点。该方法仅在接收到LiDAR测量时创建状态节点，并利用高频IMU数据将先前的LiDAR状态传播到雷达测量时间戳。这种策略使得固定延迟平滑器中的节点数量减少了50%，显著降低了计算成本。实验结果表明，该方法在保持定位精度的同时，将因子图优化时间降低了多达56%，有效提升了在资源受限硬件上的实时性能。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment",
            "authors": "Ke Ye,Jiaming Zhou,Yuanfeng Qiu,Jiayi Liu,Shihui Zhou,Kun-Yu Lin,Junwei Liang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22205",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22205",
            "arxiv_html_link": "https://arxiv.org/html/2509.22205v1",
            "abstract": "Generalizing to long-horizon manipulation tasks in a zero-shot setting remains a central challenge in robotics. Current multimodal foundation based approaches, despite their capabilities, typically fail to decompose high-level commands into executable action sequences from static visual input alone.\nTo address this challenge, we introduce Super-Mimic, a hierarchical framework that enables zero-shot robotic imitation by directly inferring procedural intent from unscripted human demonstration videos.\nOur framework is composed of two sequential modules. First, a Human Intent Translator (HIT) parses the input video using multimodal reasoning to produce a sequence of language-grounded subtasks. These subtasks then condition a Future Dynamics Predictor (FDP), which employs a generative model that synthesizes a physically plausible video rollout for each step. The resulting visual trajectories are dynamics-aware, explicitly modeling crucial object interactions and contact points to guide the low-level controller.\nWe validate this approach through extensive experiments on a suite of long-horizon manipulation tasks, where Super-Mimic significantly outperforms state-of-the-art zero-shot methods by over 20%. These results establish that coupling video-driven intent parsing with prospective dynamics modeling is a highly effective strategy for developing general-purpose robotic systems.",
            "introduction": "The pursuit of generalized robotic manipulation aims to develop systems capable of performing a wide array of tasks in diverse, unstructured environments without task-specific training. These systems must comprehend high-level instructions, perceive visual scenes, and generate feasible action plans in a zero-shot manner.\nThe recent success of multimodal large language models (MLLMs)[1, 2, 3], pre-trained on vast internet-scale datasets, has demonstrated their inherent capabilities in perception and reasoning[4, 5, 6].\nLeveraging these models as core components presents a promising pathway toward building such general-purpose robotic systems.\n\nHowever, existing zero-shot approaches [7, 8, 9] based on foundation models are often limited to simple, short-horizon tasks, such as picking up a cup.\nWhile some works [10, 11] have attempted more complex, long-horizon tasks like tidying up the floor, their planning and execution rely heavily on textual descriptions.\nThis is problematic because it forces MLLMs to infer complex action sequences from simple descriptions and cluttered visual scenes, a process that is often unreliable and difficult to control.\nFurthermore, providing detailed instructions for complex, long-horizon tasks is laborious, and the constituent steps of a long sequence often cannot be explicitly segmented or described with language.\n\nWe believe that for a robot to achieve human-level proficiency in complex tasks, it should learn from a more direct and richer source of instruction: human demonstrations.\nTo this end, we propose that unscripted human videos serve as a more effective and natural high-level instruction modality.\nThe rich visual information contained within these videos implicitly encodes the necessary subtasks and corresponding manipulation skills required to complete long-horizon tasks.\nTherefore, we introduce the Human Intent Translator (HIT). This module first efficiently extracts key action frames from the human video.\nIt then leverages MLLMs to build a unified intent translation process that generates a transferable subtask plan based on the parsed keyframe sequence and the robot’s current visual observation.\nThis process can also be refined with additional text prompts for targeted plan modification, enabling effective imitation of the long-horizon task demonstrated in the human video.\n\nFurthermore, in a zero-shot setting, the successful execution of each subtask is critical for completing a long-horizon task.\nPrior works often struggle to extract robust dynamic semantics for subtask execution using only MLLMs, which primarily excel at static scene understanding.\nFor instance, MOKA[9] proposes 2D keypoints on a static image to form a trajectory.\nSimilarly, other approaches [7, 8, 12, 13, 14, 15] attempt to regress end-effector poses directly from static observations, often failing to capture the nuances of object interactions.\nIn this work, to ensure robust subtask planning in open-world scenarios, we introduce the Future Dynamics Predictor (FDP), which employs video generation models[16, 17, 18, 19, 20] to prospectively imagine the execution of each subtask.\nThis practice not only preserves the spatial structure and fine details from the robot’s current visual observation but also accurately infers the complex contact dynamics and constraints between multiple objects.\nThese imagined futures serve as a dense, physically-grounded reference for the subsequent control phase.\nFor low-level control, we extract depth estimates and key trajectories from these generated videos, which are then translated into executable 3D target poses and control signals.\nThis provides fine-grained manipulation and stable performance, particularly in scenes involving multi-object interactions.\n\nIn this paper, we present Super-Mimic, a framework that combines the HIT module for task planning and the FDP module for subtask execution to tackle zero-shot long-horizon robotic manipulation (Fig. LABEL:fig:head).\nOur key contributions are:\n\n1) We propose a novel hierarchical framework to solve zero-shot long-horizon manipulation, by using human demonstration videos to guide high-level task planning and video generation models to inform robust low-level control.\n\n2) We introduce the HIT module, which parses complex human actions into transferable, language-guided robotic plans, and the FDP module, which imagines future states to provide dense, physically-grounded guidance for execution.\n\n3) We conduct extensive experiments demonstrating that Super-Mimic significantly outperforms state-of-the-art methods by over 20% in complex, long-horizon tasks and validate the distinct advantages of our hierarchical video-based approach.",
            "llm_summary": "【论文的motivation是什么】  \n1. 长期操控任务的零-shot泛化仍然是机器人技术中的一个核心挑战。  \n2. 现有的多模态基础方法在将高层指令分解为可执行的动作序列时存在局限性。  \n3. 复杂的长时间任务通常依赖于文本描述，导致推理过程不可靠且难以控制。  \n\n【提出了什么创新的方法】  \n本文提出了Super-Mimic，一个分层框架，通过解析人类演示视频中的程序意图，直接实现零-shot机器人模仿。该框架包含两个模块：人类意图翻译器（HIT）和未来动态预测器（FDP）。HIT模块通过多模态推理解析视频，生成语言引导的子任务序列；FDP模块则利用生成模型为每个步骤合成物理上合理的视频展望。这种结合有效地捕捉了对象交互和接触点，从而指导低级控制。经过广泛实验，Super-Mimic在复杂的长时间操控任务中显著超越了现有的零-shot方法，提升超过20%。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training",
            "authors": "Haoyun Li,Ivan Zhang,Runqi Ouyang,Xiaofeng Wang,Zheng Zhu,Zhiqin Yang,Zhentao Zhang,Boyuan Wang,Chaojun Ni,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang,Zhenbo Song,Xingang Wang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22199",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22199",
            "arxiv_html_link": "https://arxiv.org/html/2509.22199v1",
            "abstract": "Vision Language Action (VLA) models derive their generalization capability from diverse training data, yet collecting embodied robot interaction data remains prohibitively expensive. In contrast, human demonstration videos are far more scalable and cost-efficient to collect, and recent studies confirm their effectiveness in training VLA models. However, a significant domain gap persists between human videos and robot-executed videos, including unstable camera viewpoints, visual discrepancies between human hands and robotic arms, and differences in motion dynamics. To bridge this gap, we propose MimicDreamer, a framework that turns fast, low-cost human demonstrations into robot-usable supervision by jointly aligning vision, viewpoint, and actions to directly support policy training.\nFor visual alignment, we propose H2R Aligner, a video diffusion model that generates high-fidelity robot demonstration videos by transferring motion from human manipulation footage.\nFor viewpoint stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos via homography and inpaints occlusions and distortions caused by warping.\nFor action alignment, we map human hand trajectories to the robot frame and apply a constrained inverse kinematics solver to produce feasible, low-jitter joint commands with accurate pose tracking.\nEmpirically, VLA models trained purely on our synthesized human-to-robot videos achieve few-shot execution on real robots. Moreover, scaling training with human data significantly boosts performance compared to models trained solely on real robot data; our approach improves the average success rate by 14.7%14.7\\% across six representative manipulation tasks.",
            "introduction": "Vision Language Action (VLA) models (Black et al., 2024; 2025; X Square Robot Team, 2025; Cheang et al., 2025a; Bjorck et al., 2025) have shown strong generalization in robotic manipulation, but their progress is constrained by the cost and efficiency of data collection. Meanwhile, large-scale datasets (Khazatsky et al., 2025; Collaboration et al., 2025; AgiBot-World-Contributors et al., 2025) often rely on long teleoperation across heterogeneous hardware, which is time-consuming and limits task diversity. Unlike computer vision and natural language processing that can leverage Internet-scale corpora (Schuhmann et al., 2022; Dodge et al., 2021), robotics lacks cheap and abundant data sources. Human demonstrations (Bahl et al., 2022; Lepert et al., 2025; Grauman et al., 2022b) provide a more efficient and lower-cost path. Hand videos and action trajectories can be gathered quickly without continuous robot execution (Chao et al., 2021; Kwon et al., 2021), reducing hardware dependence and maintenance overhead. More importantly, human motion naturally encapsulates strategies and efficiencies observed in real operations, not brittle, preprogrammed paths, but adaptable procedures. Using human demonstrations as a primary data source, therefore, both reduces collection cost and supplies broadly applicable supervision for VLA training.\n\nExisting mimic methods (Wang et al., 2023; Kareer et al., 2024; Xie et al., 2025; Yang et al., 2025a; Qiu et al., 2025) show that human demonstrations can effectively improve robot policy learning. Most of these methods incorporate human data as auxiliary signals or in limited pipelines, rather than turning them into fully robot-usable supervision for large-scale training. Human demonstrations cannot be used directly (Bahl et al., 2022; Kareer et al., 2024) because of domain and embodiment mismatches. We therefore convert human demonstrations into robot supervision and train VLA models end-to-end on the converted data. Direct transfer, however, still faces three gaps: viewpoint, actions, and vision. (1) For the viewpoint, first-person human operation videos are typically captured by moving cameras with parallax and jitter, which complicates spatiotemporal alignment across sequences and tasks. (2) For actions, humans express intent through end-effector trajectories and dexterous manipulation, whereas robots operate in joint space under kinematic and dynamic constraints, making the semantics-to-control mapping often indirect and difficult to implement. (3) For vision, human hands and robot arms differ significantly in appearance, materials, and motion statistics, limiting the direct transfer of visual representations. Existing methods typically address only one of these issues (Kareer et al., 2024; Yang et al., 2025a), lacking a systematic approach that simultaneously tackles viewpoint stabilization, executable action mapping, and visual consistency.\n\nTherefore, we propose MimicDreamer, a framework that turns fast, low-cost human demonstrations into robot-usable supervision by jointly aligning vision, viewpoint, and actions. To bridge the vision gap, we propose H2R Aligner, a video diffusion model that renders high-fidelity robot-arm videos by transferring motion from human manipulation footage while respecting arm geometry and camera priors (Yang et al., 2025c). Quantitative and qualitative results show realistic arm appearance and contact geometry consistent with the source task. For viewpoint stabilization, EgoStabilizer canonicalizes egocentric frames via homography-based warping to a task-level reference view (estimated by averaging per-category rotations) and inpaints distortions or occlusions introduced by warping (Zhou et al., 2023). Experiment results confirm reduced ego-motion drift and improved cross-sequence comparability. To align the action space, we encode intention as relative end-effector pose increments in the shared frame and execute it via a constrained inverse kinematics (IK) solver with distributional normalization and temporal smoothness, yielding feasible, low-jitter joint trajectories. Visualized rollouts exhibit accurate end-effector tracking while respecting joint and velocity limits.\n\nIn experiments, training the VLA model (Black et al., 2024) solely on MimicDreamer-synthesized human to robot videos enables few-shot execution on real robots. Across six representative manipulation tasks, increasing the scale of human demonstration data yields consistent gains, improving an average success rate by 14.7% over a baseline trained only on real robot data.\nThe primary contributions of this work are as follows:\n\n1. We propose MimicDreamer, a unified human–robot egocentric demonstrations transferring framework that simultaneously reduces the human-to-robot discrepancy along vision, viewpoint, and action dimensions and enables scalable VLA training from low-cost human demonstrations.\n\n2. For vision, we introduce H2R Aligner based on video diffusion and geometric camera priors to synthesize high-fidelity robot arm videos. For viewpoint, we introduce EgoStabilizer, which canonicalizes frames to a task reference view by homography and repairs warping occlusions. For actions, we map human hand trajectories to the robot frame and apply constrained IK to produce feasible, low-jitter joint commands with accurate pose tracking.\n\n3. The VLA policy trained on synthesized robot demonstrations achieves few-shot execution on real robots, and across six manipulation tasks, we realize scalable VLA training, improving an average success rate over the robot data baseline by 14.7%14.7\\%, demonstrating both stronger generalization and higher sample efficiency.",
            "llm_summary": "【论文的motivation是什么】  \n1. 机器人交互数据收集成本高，限制了VLA模型的训练效率。  \n2. 人类演示视频更易收集，但存在领域差距，影响机器人执行效果。  \n3. 现有方法未能系统性解决人类与机器人演示之间的视觉、视角和动作对齐问题。  \n\n【提出了什么创新的方法】  \n提出了MimicDreamer框架，通过联合对齐视觉、视角和动作，将低成本的人类演示转化为机器人可用的监督信号。具体方法包括：  \n- H2R Aligner：利用视频扩散模型生成高保真机器人演示视频，解决视觉差异问题。  \n- EgoStabilizer：通过单应性变换稳定视角，修复因变换引入的遮挡和失真。  \n- 动作对齐：将人类手部轨迹映射到机器人框架，并应用约束逆运动学求解器生成低抖动的关节指令。  \n实验结果表明，基于合成的人类到机器人视频训练的VLA模型在真实机器人上实现了少量示例执行，并在六个代表性操作任务中，成功率平均提高了14.7%。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting",
            "authors": "Asher J. Hancock,Xindi Wu,Lihan Zha,Olga Russakovsky,Anirudha Majumdar",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22195",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22195",
            "arxiv_html_link": "https://arxiv.org/html/2509.22195v1",
            "abstract": "Fine-tuning vision-language models (VLMs) on robot teleoperation data to create vision-language-action (VLA) models is a promising paradigm for training generalist policies, but it suffers from a fundamental tradeoff: learning to produce actions often diminishes the VLM’s foundational reasoning and multimodal understanding, hindering generalization to novel scenarios, instruction following, and semantic understanding. We argue that this catastrophic forgetting is due to a distribution mismatch between the VLM’s internet-scale pretraining corpus and the robotics fine-tuning data. Inspired by this observation, we introduce VLM2VLA: a VLA training paradigm that first resolves this mismatch at the data level by representing low-level actions with natural language. This alignment makes it possible to train VLAs solely with Low-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and averting catastrophic forgetting. As a result, the VLM can be fine-tuned on robot teleoperation data without fundamentally altering the underlying architecture and without expensive co-training on internet-scale VLM datasets. Through extensive Visual Question Answering (VQA) studies and over 800 real-world robotics experiments, we demonstrate that VLM2VLA preserves the VLM’s core capabilities, enabling zero-shot generalization to novel tasks that require open-world semantic reasoning and multilingual instruction following. Website with additional information, videos, and code: https://vlm2vla.github.io/.",
            "introduction": "The pursuit of generalist robot policies capable of understanding and executing human commands has been significantly advanced by the integration of vision-language models (VLMs) [1, 2, 3] throughout the autonomy stack. Trained on internet-scale datasets of image-text pairs, these models have acquired sophisticated capabilities in perception, semantic understanding, and commonsense reasoning. To endow robots with similar capabilities, the prevailing paradigm involves fine-tuning pretrained VLMs on robot demonstration data, transforming them into vision-language-action models (VLAs) that map from natural language commands and visual observations to robot actions. This approach has yielded impressive results across a wide range of robotic manipulation tasks [4, 5, 6, 7, 8, 9, 10, 11].\n\nHowever, the standard methodology of fundamentally modifying the VLM’s architecture, tokenization vocabulary, or a combination thereof, coupled with full parameter fine-tuning on robot imitation learning data, introduces a crucial yet often overlooked trade-off. In adapting the VLM for robotic control, we risk overfitting to the robot fine-tuning data, thereby overwriting the general-purpose world knowledge acquired during pretraining (see Fig. 2). The consequences of this trade-off are far-reaching: current VLAs often exhibit a diminished ability to generalize to novel objects, handle linguistic variations, be robust to distractions, or reason about concepts outside the narrow scope of their robotic training data [4, 9, 12, 13].\n\nPreserving the VLM’s foundational world knowledge during VLA fine-tuning is essential for creating truly generalist robot policies; consequently, numerous techniques have been developed to address this challenge. The most common approach is to co-train with non-robotic data. This training regimen regularizes the VLM against overfitting to robot datasets, thereby mitigating the loss of its foundational capabilities [9, 6, 14]. While these methods can mitigate knowledge loss, co-training with VLM-scale datasets is inherently expensive and requires a carefully constructed dataset mixture for optimal performance.\n\nThis paper aims to preserve the world knowledge of the VLM while adapting it for robotic control without co-training. We address this issue by resolving the distribution mismatch between the low-level action spaces needed for robotic control and the image-text distributions of the VLM’s pretraining corpus. This mismatch often compels researchers to use full parameter fine-tuning when training VLAs, which contributes to catastrophic forgetting by overfitting to the robot teleoperation data.\n\nOur key insight is that while parameter-efficient methods like Low-Rank Adaptation (LoRA) [15] can avert catastrophic forgetting, their effectiveness relies on the fine-tuning data being sufficiently close to the model’s pretrained representations. We therefore propose resolving this representational mismatch at the data level. Our data-centric approach re-represents robot actions as natural language descriptions, thereby aligning the VLA fine-tuning data directly with the VLM’s pretrained representation space. This alignment enables LoRA to effectively adapt the VLM for robotic control without significantly perturbing its pretrained weights. Fig. 3 illustrates this idea, showing our language-based actions are assigned significantly higher probabilities by the VLM backbone than actions mapped to arbitrary tokens, a common strategy in state-of-the-art VLAs [8, 4, 9, 10, 11]. Our method is model agnostic and simple to implement, obviating the need for sophisticated architectures, complex co-training schemes, or multi-stage training procedures to achieve robust knowledge retention and superior generalization capabilities.\n\nStatement of Contributions. We present VLM2VLA, a data pipeline and training methodology for fine-tuning VLMs into VLAs while preserving their foundational perceptual and reasoning capabilities. Our core contributions are as follows: 1) Representing actions as language: We propose translating low-level robotic imitation data into text, thereby aligning the VLA fine-tuning data with the VLM’s pretraining distribution to mitigate catastrophic forgetting. 2) A data re-labeling and training pipeline for knowledge retention: Building on our action representation, we present a scalable methodology for re-labeling robot teleoperation datasets for fine-tuning a VLM into a VLA through LoRA. 3) Empirical validation of action and reasoning capabilities: We provide extensive empirical validation showing our VLA preserves a suite of crucial capabilities that are often lost in other state-of-the-art models. Specifically, our method’s efficacy is demonstrated through extensive real-world evaluation (over 800 robotic experiments), showing generalization to novel tasks with objects and language instructions unseen during training. Moreover, our policy averts catastrophic forgetting, retaining over 85% of the base model’s performance across challenging VQA benchmarks.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的VLM在机器人控制中容易导致灾难性遗忘，影响其推理和多模态理解能力。  \n2. 机器人控制数据与VLM预训练数据之间存在分布不匹配，导致模型泛化能力下降。  \n3. 传统的共训练方法成本高且复杂，难以有效保留VLM的基础知识。  \n\n【提出了什么创新的方法】  \n本研究提出了VLM2VLA，一个数据驱动的训练方法，通过将低级动作表示为自然语言，解决了VLM与机器人控制数据之间的分布不匹配问题。该方法利用低秩适应（LoRA）技术，最小化对VLM主干的修改，从而避免灾难性遗忘。通过在超过800个真实世界机器人实验中的验证，VLM2VLA展示了在新任务上的零-shot泛化能力，保持了VLM的核心能力，能够处理开放世界的语义推理和多语言指令。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions",
            "authors": "Quanzhou Li,Zhonghua Wu,Jingbo Wang,Chen Change Loy,Bo Dai",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22175",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22175",
            "arxiv_html_link": "https://arxiv.org/html/2509.22175v1",
            "abstract": "Learning to generate dual-hand grasps that respect object semantics is essential for robust hand–object interaction but remains largely underexplored due to dataset scarcity. Existing grasp datasets predominantly focus on single-hand interactions and contain only limited semantic part annotations. To address these challenges, we introduce a pipeline, SymOpt, that constructs a large-scale dual-hand grasp dataset by leveraging existing single-hand datasets and exploiting object and hand symmetries. Building on this, we propose a text-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand Affordance-aware Grasps for unseen objects. Our approach incorporates a novel dual-hand affordance representation and follows a two-stage design, which enables effective learning from a small set of segmented training objects while scaling to a much larger pool of unsegmented data. Extensive experiments demonstrate that our method produces diverse and semantically consistent grasps, outperforming strong baselines in both grasp quality and generalization to unseen objects. The project page is at https://quanzhou-li.github.io/DHAGrasp/.",
            "introduction": "Defining effective grasp targets is fundamental for enabling robust interaction between hands and objects. While extensive research\n[1]\nhas been conducted on single-hand grasping, more sophisticated tasks often require coordinated dual-hand grasps. Moreover, in practical situations, interaction with specific parts of an object is frequently necessary, calling for a generator that generates coordinated two-hand grasps guided by the functional semantics of object parts rather than geometry along.\n\nDespite the importance of dual-hand approaches, research in this area remains underexplored, primarily due to the scarcity of suitable two-hand grasp datasets. Existing datasets, such as [2] and [3], include sequences of two-hand manipulations but focus mainly on manipulation motions rather than the grasping phase itself and feature only a limited number of objects with restricted grasp diversity. In the absence of two-hand grasp datasets, approaches such as [4] employ energy-based optimization to generate dual-hand grasps. These processes, however, tend to be slow and yield relatively low success rates.\n\nTo overcome these limitations, we propose a novel approach that leverages the abundance of existing single-hand grasp datasets. Specifically, we exploit object and hand symmetries to construct a large-scale dual-hand grasp dataset. Most objects exhibit approximate bilateral symmetry; thus, we mirror ground-truth right-hand grasps across an identified pseudo-symmetric plane to obtain corresponding left-hand grasps. These mirrored grasps are then combined into candidate dual-hand proposals. To ensure physical plausibility, we introduce an optimization scheme, termed SymOpt, which eliminates interpenetrations between the hands and the object. Using this pipeline, we derive a large-scale dataset, DualHands-Full, from DexGraspNet [5], and further construct DualHands-Sem, a semantics-augmented subset that associates functional object parts with dual-hand grasps. As summarized in Table I, our dataset features a substantially larger number of grasp configurations per object compared with prior work and uniquely incorporates semantics into dual-hand grasping.\n\nAlthough our dataset contains abundant dual-hand grasps, it inherits the limitations of single-hand datasets, namely the limited availability of consistent semantic part labels, which are costly to obtain at scale. Directly training on dual-hand data with such limited segmentation would lead to suboptimal performance. To address this problem, we propose a novel dual-hand contact representation, which bridges the unsegmented objects to the objects with semantics. Our contact representation consists of a contact map, a part map, and most importantly, a set of affordance directions. By leveraging the representation, our approach learns effectively from a small subset of segmented training objects while scaling to a much larger pool of unsegmented data.\n\nWith the dual-hand contact representation, we introduce a text-guided Dual-Hand Affordance-aware Grasp (DHAGrasp) generator. The key intuition is inspired by how humans recognize the functional parts of novel objects by drawing on prior knowledge of similar items. Specifically, our generator operates in two stages: (1) Text2Dir, a semantics-based affordance module that predicts category-level affordance directions from object geometry and text embeddings; and (2) Dir2Grasp, an affordance-conditioned grasp synthesizer that generates dual-hand grasps aligned with the predicted affordance directions. This design enables effective utilization of limited annotated data while scaling to large unsegmented datasets.\n\nWe conduct extensive experiments to evaluate both our datasets and the grasp generation method. Our data generation approach, SymOpt, produces datasets an order of magnitude larger than those in previous works and yields substantially higher success rates on the generated grasps. We further demonstrate the effectiveness of our generation pipeline, DHAGrasp, which excels in two-hand coordination and outperforms prior methods across multiple metrics.\n\nIn summary, our contributions are fourfold. 1) we introduce a pipeline, SymOpt, that constructs a dual-hand object grasp dataset leveraging the advances of single-hand datasets. 2) We assemble a large-scale dual-hand grasp dataset DualHands-Full, together with a semantics-based sub-dataset, DualHands-Sem. 3) We propose a novel dual-hand contact representation that enables within-category generalization. 4) We design a Dual-Hand Affordance-aware Grasp Model, DHAGrasp, that synthesizes dual-hand grasps based on text instructions.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的双手抓取研究较少，缺乏适合的双手抓取数据集。  \n2. 现有数据集主要集中在单手抓取，且抓取多样性有限。  \n3. 生成双手抓取需要考虑物体的功能语义，而不仅仅是几何形状。  \n\n【提出了什么创新的方法】  \n本文提出了一种名为SymOpt的流程，用于构建大规模的双手抓取数据集，利用现有的单手数据集和物体、手的对称性。基于此，我们设计了一个文本引导的双手抓取生成器DHAGrasp，能够为未见过的物体合成双手抓取。该方法采用了新颖的双手接触表示，并遵循两阶段设计，有效地从少量分割训练物体中学习，同时扩展到更大规模的未分割数据集。实验结果表明，我们的方法在抓取质量和对未见物体的泛化能力上均优于强基线，生成的抓取多样且语义一致。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "DemoGrasp: Universal Dexterous Grasping from a Single Demonstration",
            "authors": "Haoqi Yuan,Ziye Huang,Ye Wang,Chuan Mao,Chaoyi Xu,Zongqing Lu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22149",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22149",
            "arxiv_html_link": "https://arxiv.org/html/2509.22149v1",
            "abstract": "Universal grasping with multi-fingered dexterous hands is a fundamental challenge in robotic manipulation. While recent approaches successfully learn closed-loop grasping policies using reinforcement learning (RL), the inherent difficulty of high-dimensional, long-horizon exploration necessitates complex reward and curriculum design, often resulting in suboptimal solutions across diverse objects. We propose DemoGrasp, a simple yet effective method for learning universal dexterous grasping. We start from a single successful demonstration trajectory of grasping a specific object and adapt to novel objects and poses by editing the robot actions in this trajectory: changing the wrist pose determines where to grasp, and changing the hand joint angles determines how to grasp. We formulate this trajectory editing as a single-step Markov Decision Process (MDP) and use RL to optimize a universal policy across hundreds of objects in parallel in simulation, with a simple reward consisting of a binary success term and a robot–table collision penalty. In simulation, DemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow Hand, outperforming previous state-of-the-art methods. It also shows strong transferability, achieving an average success rate of 84.6% across diverse dexterous hand embodiments on six unseen object datasets, while being trained on only 175 objects. Through vision-based imitation learning, our policy successfully grasps 110 unseen real-world objects, including small, thin items. It generalizes to spatial, background, and lighting changes, supports both RGB and depth inputs, and extends to language-guided grasping in cluttered scenes.",
            "introduction": "Universal dexterous grasping [2, 13] is a fundamental capability for real-world robots. The anthropomorphic design of dexterous robotic hands makes them the most suitable manipulators for real-world manipulation tasks, such as tool use, in-hand reorientation, and bimanual coordination. Universal grasping is therefore an essential prerequisite for enabling these sophisticated interactions. Though basic in concept, learning universal dexterous grasping policies remains far from simple.\nThe high-dimensional action space introduced by dexterous hands with many degrees of freedom (DoFs), together with the long-horizon nature of closed-loop grasping, imposes substantial exploration challenges for reinforcement learning (RL). At the same time, the diverse geometries of objects make universal dexterous grasping a multi-task optimization problem, introducing additional difficulties such as catastrophic forgetting [20, 32] and gradient interference [34, 43].\n\nRecent studies have extensively investigated the use of RL for training universal dexterous grasping policies. Xu et al. [41], Wan et al. [35], Zhang et al. [46], Chen et al. [8] introduce techniques in observation feature design, dense reward shaping, and curriculum learning strategies to facilitate policy learning. UniDexGrasp++ [35] employs an iterative distillation process to improve teacher–student learning. ResDex [16] introduces a two-stage residual RL framework to accelerate multi-task exploration. UniGraspTransformer [37] proposes exhaustive RL on individual objects and distillation with expressive Transformer policies to bypass multi-task RL.\nHowever, many of these approaches train on hands without robot arms [41, 35], use privileged contact information as observations [35, 16], and face a trade-off between collision penalties and other complex reward terms [41, 16], limiting their potential for deployment on real robots. Singh et al. [33], Zhang et al. [46] achieve sim-to-real on a wide variety of objects but still fall short on grasping small, thin objects in tabletop settings. In addition, their reliance on complicated observation design, reward shaping, and multi-stage pipelines increases the barrier to extending these methods to new embodiments and task settings.\n\nIn this research, we propose DemoGrasp, a simple yet powerful framework for universal dexterous grasping that addresses these challenges. Our key insight is that a single demonstration trajectory of grasping a specific object encodes many transferable patterns for universal grasping, such as approaching the object’s grasp center, squeezing the hand pose, and lifting the wrist. To grasp various objects in different poses, we can slightly modify the robot actions within this trajectory and replay the edited actions. For example, to grasp the same object at a different location, we can apply a transformation to the wrist poses in the trajectory, changing where to grasp; to grasp a larger object at the same position, we adjust the grasp poses to be more open, changing how to grasp. In our method, the RL policy explores how to edit the demonstration along these two axes, rather than exploring in the low-level robot action space as in prior methods, resulting in more efficient trial-and-error.\n\nSpecifically, we formulate the demonstration-editing task as a single-step Markov Decision Process (MDP). At each trial, given an arbitrary object placed at a random position, the policy outputs an SE​(3)\\mathrm{SE}(3) transformation and delta hand joint angles, which are used to modify the end-effector poses and hand actions in the demonstration. The edited demonstration is then replayed in simulation, yielding a reward for the whole episode.\nBy restricting the policy to a compact action space and a single-step decision-making horizon, the multi-task exploration burden is significantly reduced, removing the need for complex reward shaping. This enables us to effectively train a universal grasping policy on hundreds or thousands of objects by optimizing a simple combination of binary success reward and a collision penalty. We observe that this design yields both superior performance in simulation and easy sim-to-real transfer with minimal collisions. We train a flow-matching [23] policy on successful rollouts of the learned policy with rendered camera images in simulation, enabling zero-shot deployment on a real robot.\n\nWe conduct large-scale experiments in both simulation and the real world to evaluate DemoGrasp. On 3.4K objects from DexGraspNet [36], DemoGrasp achieves success rates of 95% in state-based settings and 92% in vision-based settings, surpassing previous state-of-the-art methods by a large margin. DemoGrasp also exhibits strong transferability to a wide variety of robotic embodiments and generalization to unseen object categories. Trained on 175 objects, the policies achieve an average success rate of 84.6% on six unseen object datasets across various embodiments, including dexterous hands with different numbers of fingers, grippers, and arm–hand systems.\nIn real-world experiments, DemoGrasp achieves a success rate of 86.5% on 110 unseen objects, covering a wide variety of geometries and visual appearances. For normal-sized objects, it achieves a superior success rate of 95.3%. Benefiting from the simple reward design, the policy is, to our knowledge, the first to grasp previously unseen small, thin objects in tabletop settings without severe collisions, achieving a success rate of 71.1%. DemoGrasp also exhibits generalization to spatial, background, and lighting changes, and is extensible to various camera configurations (RGB and depth) and cluttered scenes, underscoring its practical applicability.\n\nOur contributions are summarized as follows:\n\nWe propose DemoGrasp, a simple yet powerful learning framework that addresses key challenges in learning universal dexterous grasping policies. With a novel formulation of demonstration editing and single-step RL, DemoGrasp enables robust policy learning, minimal reliance on reward shaping, and sim-to-real transferability.\n\nDemoGrasp achieves state-of-the-art performance in large-scale evaluations in both simulation and the real world, demonstrating strong capability in grasping diverse, unseen objects on real robots.\n\nWe demonstrate the strong extensibility of DemoGrasp to novel embodiments, camera configurations, and cluttered scenes, establishing a foundation for future research and applications in dexterous manipulation.\n\n1. We propose DemoGrasp, a simple yet powerful learning framework that addresses key challenges in learning universal dexterous grasping policies. With a novel formulation of demonstration editing and single-step RL, DemoGrasp enables robust policy learning, minimal reliance on reward shaping, and sim-to-real transferability.\n\n2. DemoGrasp achieves state-of-the-art performance in large-scale evaluations in both simulation and the real world, demonstrating strong capability in grasping diverse, unseen objects on real robots.\n\n3. We demonstrate the strong extensibility of DemoGrasp to novel embodiments, camera configurations, and cluttered scenes, establishing a foundation for future research and applications in dexterous manipulation.",
            "llm_summary": "【论文的motivation是什么】  \n1. Universal dexterous grasping is essential for real-world robotic manipulation tasks.  \n2. High-dimensional action spaces and long-horizon exploration pose significant challenges for reinforcement learning in grasping.  \n3. Existing methods often rely on complex reward structures and are limited in their applicability to real robots.  \n\n【提出了什么创新的方法】  \nDemoGrasp introduces a framework for universal dexterous grasping that leverages a single successful demonstration trajectory. By editing the robot's actions in this trajectory—adjusting wrist poses for where to grasp and hand joint angles for how to grasp—it formulates this as a single-step Markov Decision Process (MDP). This approach simplifies the exploration process, allowing for efficient learning and optimization of a universal policy across hundreds of objects. In simulations, DemoGrasp achieves a 95% success rate on DexGraspNet objects and demonstrates strong transferability to unseen objects and diverse robotic embodiments, including successful grasping of small, thin items in real-world settings.  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation",
            "authors": "Xiaohuan Pei,Yuxing Chen,Siyu Xu,Yunke Wang,Yuheng Shi,Chang Xu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22093",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22093",
            "arxiv_html_link": "https://arxiv.org/html/2509.22093v1",
            "abstract": "Robotic manipulation with Vision-Language-Action models requires efficient inference over long-horizon multi-modal context, where attention to dense visual tokens dominates computational cost. Existing methods optimize inference speed by reducing visual redundancy within VLA models, but they overlook the varying redundancy across robotic manipulation stages. We observe that the visual token redundancy is higher in coarse manipulation phase than in fine-grained operations, and is strongly correlated with the action dynamic.\nMotivated by this observation, we propose Action-aware Dynamic Pruning (ADP), a multi-modal pruning framework that integrates text-driven token selection with action-aware trajectory gating. Our method introduces a gating mechanism that conditions the pruning signal on recent action trajectories, using past motion windows to adaptively adjust token retention ratios in accordance with dynamics, thereby balancing computational efficiency and perceptual precision across different manipulation stages.\nExtensive experiments on the LIBERO suites and diverse real-world scenarios demonstrate that our method significantly reduces FLOPs and action inference latency (e.g. 1.35× speed up on OpenVLA-OFT) while maintaining competitive success rates (e.g. 25.8% improvements with OpenVLA) compared to baselines, thereby providing a simple plug-in path to efficient robot policies that advances the efficiency and performance frontier of robotic manipulation. Our project website is: ADP.com.",
            "introduction": "Large vision language models Liu et al. (2023c; b; 2024a); Team et al. (2023); Awadalla et al. (2023) have recently been extended into Vision–Language-Action (VLA) models Kim et al. (2024; 2025); Black et al. (2024); Li et al. (2024); Brohan et al. (2024); Wen et al. (2025b; a); Bjorck et al. (2025) that map both the visual observation and language instruction to executable robot actions. In the mainstream pipeline, a vision encoder produces dense visual tokens from one or more camera views, a projector aligns them to the language space, and an LLM fuses all modalities to predict actions. However, this multi-modal design introduces long input sequences with numerous visual tokens that are only weakly relevant to the current manipulation operation,\nwhich inflates compute, memory footprint, and latency, and it can dilute attention over truly task-relevant cues.\n\nExisting work pursues efficiency via architectural lightening and modality-aware compression, such as RoboMamba Liu et al. (2024b) that focuses on lightweight designs, DeeR-VLA Yue et al. (2024) that aims at structured pruning/reparameterization, Mole-VLA Zhang et al. (2025) that targets conditional layer activation, VLA-Cache Xu et al. (2025) that focuses on cache reuse, and EfficientVLA Yang et al. (2025) that aims to prune visual tokens via attention.\nHowever, a key but underexplored property of robotic manipulation is that visual redundancy in VLAs is action-aware across different manipulation stages.\nAs Fig. 1 shows, during coarse-grained operations (e.g., relocating), global movement dominates and redundant tokens can be pruned; during fine-grained phases (e.g., grasping), local geometry and detailed cues dominate and preserving full vision is preferred.\nMoreover, the relevance of visual patches is not only text conditioned (semantics of the instruction) but also action conditioned (instantaneous end-effector motion and gripper state). Treating all steps uniformly, or ranking tokens solely by mixed attention scores, therefore yields suboptimal pruning schedules that either prune too little (limited savings) or prune too much (accuracy loss), especially in multi-view settings (scene and wrist/gripper cameras) where importance is unevenly distributed across\ntime.\n\nTo address this challenge, we introduce Action-aware Dynamic Pruning (ADP), a plug-and-play strategy that reduces computation while preserving manipulation fidelity. ADP is built on two complementary ideas: (1) Text-driven Pruning evaluates the relevance of visual patch using cross-modal similarities, selecting only the most relevant tokens before entering deep fusion in the subsequent layers. (2) Action-aware Dynamics modulate whether pruning is activated at a given step using a lightweight decision signal derived from the end-effector trajectory within each action window.\nSpecifically, when the recent motion magnitude is relatively low compared to past motion statistics (delicate phases), pruning is disabled to preserve the full visual field for precise control. Conversely, when the motion magnitude is relatively high compared to past motion statistics (coarse phases), pruning is engaged to suppress redundancy and save FLOPs.\nWe implement a gated mechanism that treats recent action statistics as a pruning signal over sliding trajectory windows, adaptively adjusting retention ratios according to motion dynamics and balancing efficiency with precision across manipulation stages. Our contributions can be summarized as:\n(1) We show that the importance of the visual token in VLA models varies within different stages of robotic manipulation. This insight motivates our dynamic pruning method tailored to manipulation phases compared to static pruning approaches.\n(2) We propose text-driven action-aware pruning that combines task instruction relevance with a gating rule based on end-effector motion, enabling adaptive switching between pruned and full-vision states.\n(3) We present a principled complexity analysis and extensive experiments in simulation and real-world settings, demonstrating that our method reduces FLOPs and latency while maintaining fine visual details required for successful manipulation.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有VLA模型在长时间多模态上下文推理中计算成本高。  \n2. 视觉标记的冗余性在不同的操作阶段变化，现有方法未能考虑这一点。  \n3. 视觉冗余与动作动态密切相关，影响了操作的效率和精度。  \n\n【提出了什么创新的方法】  \n提出了Action-aware Dynamic Pruning (ADP)方法，通过文本驱动的标记选择和动作感知的轨迹门控机制，动态调整视觉标记的保留比例。该方法在粗操作阶段启用剪枝以减少冗余，而在精细操作阶段则保留完整视觉信息。通过这种方式，ADP在保持操作精度的同时显著提高了计算效率，实验结果显示在OpenVLA-OFT上实现了1.35倍的速度提升，并在成功率上有25.8%的改善，展示了其在机器人操作中的有效性和实用性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot",
            "authors": "Ethan Fulcher,J. Diego Caporale,Yifeng Zhang,John Ruck,Feifei Qian",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "7+ICRA Submission This work has been submitted to the IEEE for possible publication",
            "pdf_link": "https://arxiv.org/pdf/2509.22065",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22065",
            "arxiv_html_link": "https://arxiv.org/html/2509.22065v1",
            "abstract": "In-situ robotic exploration is an important tool for advancing knowledge of geological processes that describe the Earth and other Planetary bodies.\nTo inform and enhance operations for these roving laboratories, it is imperative to understand the terramechanical properties of their environments, especially for traversing on loose, deformable substrates.\nRecent research suggested that legged robots with direct-drive and low-gear ratio actuators can sensitively detect external forces, and therefore possess the potential to measure terrain properties with their legs during locomotion, providing unprecedented sampling speed and density while accessing terrains previously too risky to sample.\nThis paper explores these ideas by investigating the impact of gait on proprioceptive terrain sensing accuracy, particularly comparing a sensing-oriented gait, Crawl N’ Sense, with a locomotion-oriented gait, Trot-Walk.\nEach gait’s ability to measure the strength and texture of deformable substrate is quantified as the robot locomotes over a laboratory transect consisting of a rigid surface, loose sand, and loose sand with synthetic surface crusts. Our results suggest that with both the sensing-oriented crawling gait and locomotion-oriented trot gait, the robot can measure a consistent difference in the strength (in terms of penetration resistance) between the low- and high-resistance substrates; however, the locomotion-oriented trot gait contains larger magnitude and variance in measurements.\nFurthermore, the slower crawl gait can detect brittle ruptures of the surface crusts with significantly higher accuracy than the faster trot gait.\nOur results offer new insights that inform legged robot “sensing during locomotion” gait design and planning for scouting the terrain and producing scientific measurements on other worlds to advance our understanding of their geology and formation.",
            "introduction": "Robotic exploration plays a crucial role in advancing our understanding of Earth and other planetary bodies by enabling in situ experiments remotely[1].\nMany terrestrial and planetary environments (Figs. 1, 1 and 2) present significant challenges for traditional wheeled rovers[2], as hazardous terrains limit access to areas of scientific interest.\nGiven the high mission costs and risks, scientists must be risk-averse, often foregoing scientific opportunities that pose potential threats to robotic platforms.\nThis is partly due to the challenge in acquiring terramechanical information about surface properties such as regolith strength and texture[3], which are difficult to infer without direct tactile feedback.\nTraditional methods[3] require dedicated sensors that demand stopping the rover for measurements, leading to sparse data collection and operational inefficiencies.\n\nLegged robots[6, 7, 8, 9, 10] offer an alternative mobility paradigm that can expand the operational envelope of planetary exploration.\nRecent work has demonstrated their potential not only for traversing extreme terrain[11, 12, 13, 14] but also for proprioceptively “feeling” surface interactions as they walk, effectively acting as penetrometers and making every step an experiment.\nRecent advances in actuator transparency and torque density have led to accurate force estimation in direct-drive ( i.e., gearless) [15] and quasi-direct-drive (QDD,  i.e., low gear ratio) robotic limbs (Figs. 1 and 1), which has shown promise in enabling high-density ground reaction force measurements [12, 16].\nHowever, most prior work has focused on static or slow-moving test setups[17, 12, 4, 16, 18] (Figs. 1 and 1.1).\nHere, we explore the possibility for proprioceptive terrain sensing to be effectively conducted while a legged robot is in motion (Figs. 1 and 1.2-3).\nThis capability would allow every step to serve as a scientific experiment, significantly increasing the density and speed of geotechnical surveys and planetary explorations.\nFrom a scientific perspective, this could reveal spatial gradients in surface properties[19, 20, 21], providing deeper insight into planetary surface processes[12].\nOperationally, it could enhance mission planning by informing rover and astronaut activities such as excavation, sampling, navigation[22], and construction [23].\n\nTo achieve this, a key question that needs to be answered is: how does gait design affect the accuracy and coverage of proprioceptive regolith sensing?\nHigh-speed locomotion enables rapid data collection but introduces complex regolith reaction force profiles [24] that can complicate signal interpretation, potentially reducing measurement accuracy.\nConversely, low-speed gaits, which keep footsteps in the quasi-static regime, provide higher confidence in force estimation but limit spatial coverage.\n\nTo answer these questions, this study examines how variations in gait design influence a QDD quadrupedal robot’s ability to measure terrain properties such as strength and texture change.\nField experiments with a mounted leg penetrometer (Figs. 1 and 1.1), in an analogue environment (Fig. 2) motivate the need for sensitive, dense measurement of the terrain strength (Sec. II).\nFurthermore, we develop a custom sensing-oriented gait, the Crawl N’ Sense (Fig. 1.2), and compare it to a baseline locomotion-oriented gait, the Trot-Walk (Fig. 1.3), in\nlab experiments (Sec. III).\nAs the robot traverses varied terrains, we characterize the proprioceptive joint signals and propose methods to extract terrain properties from these signals, including a method for proprioceptively estimating the ground surface on soft terrain via a contact-based correction (Sec. III-C).\nUsing the proposed methods, we systematically compare the two gaits in terms of sensing accuracy for (i) characterizing regolith strength (Sec. IV) and (ii) detecting texture and layering (Sec. V).\nOur results provide key insights into the gait design principles needed to optimize both mobility and sensing in legged robots, with implications for deployment in complex earth and planetary surface explorations.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统轮式探测器在极端地形中面临挑战，限制了科学探索。  \n2. 需要高效获取地表机械特性信息，以提高探测效率和数据密度。  \n3. 探索在运动中进行本体感知的可能性，以增强地质调查的速度和准确性。  \n\n【提出了什么创新的方法】  \n本研究开发了一种新的感知导向步态“Crawl N’ Sense”，并与传统的“Tro-Walk”步态进行比较。通过在不同的实验环境中进行测试，量化了两种步态在测量变形基质强度和纹理方面的准确性。结果表明，尽管两种步态都能有效测量基质强度，但“Crawl N’ Sense”在检测表面破裂方面的准确性显著高于“Tro-Walk”。这些发现为优化腿式机器人在复杂地表探测中的步态设计提供了重要见解。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose",
            "authors": "Qifeng Wang,Weigang Li,Lei Nie,Xin Xu,Wenping Liu,Zhe Xu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22058",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22058",
            "arxiv_html_link": "https://arxiv.org/html/2509.22058v1",
            "abstract": "As a key technology for autonomous navigation and positioning in mobile robots, light detection and ranging (LiDAR) odometry is widely used in autonomous driving applications. The Iterative Closest Point (ICP)-based methods have become the core technique in LiDAR odometry due to their efficient and accurate point cloud registration capability. However, some existing ICP-based methods do not consider the reliability of the initial pose, which may cause the method to converge to a local optimum. Furthermore, the absence of an adaptive mechanism hinders the effective handling of complex dynamic environments, resulting in a significant degradation of registration accuracy. To address these issues, this paper proposes an adaptive ICP-based LiDAR odometry method that relies on a reliable initial pose. First, distributed coarse registration based on density filtering is employed to obtain the initial pose estimation. The reliable initial pose is then selected by comparing it with the motion prediction pose, reducing the initial error between the source and target point clouds. Subsequently, by combining the current and historical errors, the adaptive threshold is dynamically adjusted to accommodate the real-time changes in the dynamic environment. Finally, based on the reliable initial pose and the adaptive threshold, point-to-plane adaptive ICP registration is performed from the current frame to the local map, achieving high-precision alignment of the source and target point clouds. Extensive experiments on the public KITTI dataset demonstrate that the proposed method outperforms existing approaches and significantly enhances the accuracy of LiDAR odometry.",
            "introduction": "Light detection and ranging (LiDAR), with its high-precision distance measurement and 3D modeling capabilities, provides significant advantages in low-light environments. Consequently, it has become an essential sensor in autonomous driving and robotic systems [1, 2]. Leveraging its depth perception, LiDAR odometry technology has emerged to enhance autonomous navigation, enabling real-time motion state calculation and high-precision positioning in dynamic, complex environments [3, 4, 5].\n\nAmong the various LiDAR odometry methods, Iterative Closest Point (ICP)-based methods have become the mainstream due to their efficient and accurate point cloud registration capabilities [6, 7, 8]. The ICP-based methods estimate pose by iteratively aligning the source point cloud with the target point cloud through optimization. However, existing ICP-based methods present significant limitations in practical applications. Firstly, these methods often do not adequately consider the reliability of the initial pose estimation, making the algorithm susceptible to converging to local optima when the initial pose is inaccurate, thereby leading to registration failures or decreased accuracy [9, 10, 11]. Secondly, traditional ICP-based methods lack adaptive mechanisms, making it challenging to effectively handle dynamic and complex environmental changes, such as numerous moving objects or rapidly changing conditions. This results in a substantial reduction in registration accuracy and negatively impacts the overall performance of LiDAR odometry [12, 13, 14].\n\nTo address these challenges, this paper proposes an adaptive ICP-based LiDAR odometry method based on a reliable initial pose. The method first preprocesses the source point cloud through distributed coarse registration using density filtering to obtain an initial pose estimation. Subsequently, historical pose information is incorporated for motion prediction and compared with the initial pose obtained in the previous step to select the most reliable initial pose, thereby reducing the initial error between the source and target point clouds. Furthermore, an adaptive threshold is dynamically adjusted based on the current and historical errors, which enables the method to flexibly adjust registration parameters according to different motion states and real-time environments. Finally, an adaptive weight mechanism is employed to weight each pair of points during the point cloud registration process, reducing the influence of outliers and achieving high-precision alignment of the source and target point clouds. Fig. 1 shows the model of our approach. To verify the effectiveness of the proposed method, we conducted extensive experimental evaluations using the public KITTI dataset and compare our approach with the latest mainstream LiDAR odometry methods. The experimental results demonstrate that the proposed method outperforms existing approaches in registration accuracy, effectively enhancing the overall performance of LiDAR odometry in complex dynamic environments.\n\nThe main contributions of our work are as follows:\n\nTo address the issue of unreliable initial poses leading to local optimal solutions, a method for obtaining a more reliable initial pose is proposed. Distributed coarse registration based on density filtering is first employed to estimate the initial pose. The reliable initial pose is then selected by comparing it with the motion prediction pose, thereby minimizing the initial error between the source and target point clouds.\n\nTo solve the problem of insufficient adaptability in dynamic environment, an adaptive frame-to-local map ICP registration method is proposed. The registration parameters were dynamically adjusted by combining current and historical errors to enhance the adaptability of the method in dynamic environments.\n\nTo evaluate the effectiveness of these methods in real-world scenarios, extensive experiments were conducted using the public KITTI dataset [15, 11, 16, 14]. These experiments compared our method with existing mainstream LiDAR odometry methods. The results demonstr ate that the proposed method is superior to other mainstream odometry methods in terms of accuracy.\n\nThe rest of this paper is as follows: Section II introduces related work and reviews the current research progress in LiDAR odometry and the ICP-based methods. Section III elaborates on the theoretical foundation and implementation steps of the proposed method. Section IV presents the experimental results and analysis. Finally, Section V concludes the paper and discusses future research directions.\n\n1. To address the issue of unreliable initial poses leading to local optimal solutions, a method for obtaining a more reliable initial pose is proposed. Distributed coarse registration based on density filtering is first employed to estimate the initial pose. The reliable initial pose is then selected by comparing it with the motion prediction pose, thereby minimizing the initial error between the source and target point clouds.\n\n2. To solve the problem of insufficient adaptability in dynamic environment, an adaptive frame-to-local map ICP registration method is proposed. The registration parameters were dynamically adjusted by combining current and historical errors to enhance the adaptability of the method in dynamic environments.\n\n3. To evaluate the effectiveness of these methods in real-world scenarios, extensive experiments were conducted using the public KITTI dataset [15, 11, 16, 14]. These experiments compared our method with existing mainstream LiDAR odometry methods. The results demonstr ate that the proposed method is superior to other mainstream odometry methods in terms of accuracy.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的ICP方法未考虑初始姿态的可靠性，导致局部最优解问题。  \n2. 传统ICP方法缺乏自适应机制，难以处理动态复杂环境的变化。  \n\n【提出了什么创新的方法】  \n本文提出了一种基于可靠初始姿态的自适应ICP LiDAR里程计方法。首先，通过基于密度过滤的分布式粗注册获得初始姿态估计。然后，通过与运动预测姿态的比较选择最可靠的初始姿态，减少源点云与目标点云之间的初始误差。接着，结合当前和历史误差动态调整自适应阈值，以适应实时变化的动态环境。最后，基于可靠初始姿态和自适应阈值，执行点到平面的自适应ICP注册，实现源点云与目标点云的高精度对齐。实验结果表明，该方法在KITTI数据集上显著提高了LiDAR里程计的准确性，优于现有方法。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "One-DoF Robotic Design of Overconstrained Limbs with Energy-Efficient, Self-Collision-Free Motion",
            "authors": "Yuping Gu,Bangchao Huang,Haoran Sun,Ronghan Xu,Jiayi Yin,Wei Zhang,Fang Wan,Jia Pan,Chaoyang Song",
            "subjects": "Robotics (cs.RO)",
            "comment": ". Accepted by Fundamental Research. For Supplementary Videos, seethis https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.22002",
            "code": "https://bionicdl.ancorasir.com/?p=1668",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22002",
            "arxiv_html_link": "https://arxiv.org/html/2509.22002v1",
            "abstract": "While it is expected to build robotic limbs with multiple degrees of freedom (DoF) inspired by nature, a single DoF design remains fundamental, providing benefits that include, but are not limited to, simplicity, robustness, cost-effectiveness, and efficiency. Mechanisms, especially those with multiple links and revolute joints connected in closed loops, play an enabling factor in introducing motion diversity for 1-DoF systems, which are usually constrained by self-collision during a full-cycle range of motion. This study presents a novel computational approach to designing one-degree-of-freedom (1-DoF) overconstrained robotic limbs for a desired spatial trajectory, while achieving energy-efficient, self-collision-free motion in full-cycle rotations. Firstly, we present the geometric optimization problem of linkage-based robotic limbs in a generalized formulation for self-collision-free design. Next, we formulate the spatial trajectory generation problem with the overconstrained linkages by optimizing the similarity and dynamic-related metrics. We further optimize the geometric shape of the overconstrained linkage to ensure smooth and collision-free motion driven by a single actuator. We validated our proposed method through various experiments, including personalized automata and bio-inspired hexapod robots. The resulting hexapod robot, featuring overconstrained robotic limbs, demonstrated outstanding energy efficiency during forward walking.",
            "introduction": "Designing robotic limbs for a specific application is a challenging optimization problem that involves mechanical configuration, workspace, energy efficiency, payload capacity, and numerous other factors [1]. The open-chain limbs are a common choice by serially connecting rotation motors with specific configurations for agile motion and avoiding self-collision, such as the robotic manipulators [2, 3, 4], quadruped robots [5, 6], and humanoid robots [7, 8, 9]. However, the resulting drawbacks include the redundant inertia of moving components and relatively lower payload capacity. In contrast, the closed-chain limbs usually arrange the motors near the body and leverage the extra links and joints in closure form to achieve lower inertia for moving parts and improved end-effector stiffness, which becomes a preferred choice of legged robots [10, 11, 12, 13, 14, 15]. The increased number of links and joints makes the design of closed-chain robotic limbs more complex when considering the parametric choices for better performance and collision avoidance for larger workspaces concurrently. The scissor-like joint is widely used in robotic limb design [16] due to its simplicity and reliability, which ensures an extensive motion range for the joints. At the same time, the collision between links would still significantly reduce the workspace, leaving a research question regarding the simultaneous optimization of the parameters and geometries of closed-chain robotic limbs for specified tasks. The computational design method provides an alternative solution to task-specified robotic limb optimization [17, 18]. Ha et al. [19] present a computational approach to designing the robotic device by combining modular components for high-level motion specifications. On the other hand, it remains challenging to formulate an optimization problem for generating desired trajectories with collision-free closed-chain configurations and reduced actuation.\n\nAlthough robotic arms can achieve these spatial trajectories, they require integrating multiple actuators using complicated control software [20]. Instead, the one-DoF robotic design can perform receptive motion approximately driven by a single actuator. This single-actuator-driven characteristic has several advantages, including ease of control, robustness, low cost, and lightweight design, making it widely used in various engineering applications, such as modern machines [21], automata [22, 23], and robotics [24]. The RHex robot series provides an instructive insight into robotic limb design by rotating each limb in the sagittal plane and mimicking the behavior of cockroaches [25, 26, 27]. The simplicity of the limb design has resulted in a robust robot platform for learning legged locomotion [28]. With the development of this series of robots, the limb design converged on a planar four-bar structure with full-cycle motion for dynamic locomotion [29].\n\nOn the other hand, the planar four-bar mechanism is also widely adopted in other robotic designs [16]. In contrast, the generalized four-bar linkage (Bennett linkage) still has limited engineering applications as a robotic limb. The early study by Carvalho and Silvestre [30] utilized the Bennett linkage for designing a hexapod robot’s limb, with each limb comprising an active revolute joint and two additional actuators for overcoming obstacles. However, the proposed design is redundant and only demonstrates the limited advantages of the Bennett limb in a simulation environment. Gu et al. [31] investigate the design of the Bennett linkage as a robotic limb with coaxial actuation, enabling omni-directional locomotion. This demonstrates the potential advantage of an overconstrained linkage, which results in a systematic reduction in actuation.\n\nFurthermore, the Bennett robotic limb has also been validated to be energy-efficient for specific tasks by comparing it with other configurations [32, 33]. A research gap remains in leveraging overconstrained linkages for energy-efficient robotic limb design with reduced actuation due to their relatively complicated geometric condition and kinematic constraints [34]. This study aims to develop a computational design model that optimizes the parameters and geometries of overconstrained robotic limbs for efficiently realizing a target trajectory with a single actuator.\n\nLinkage synthesis is a classical kinematic design task that involves constructing a linkage mechanism to transfer an input motion (typically a rotation input from a motor) to an output motion that satisfies a set of specified characteristics [35]. Two typical linkage synthesis problems include rigid body guidance and path generation [36]. The goal of rigid body guidance is to lead a rigid body past a series of given positions and orientations [37]. Several recent works have addressed this problem in designing mechanical automata, including planar mechanical characters [23], multi-pose mechanical objects [38], and drawing machines [39]. Instead of focusing on rigid body motion, path generation aims to generate a mechanical linkage that enables the end-effector point to move along a desired trajectory. Due to its simplicity and ease of fabrication, the 1-DoF planar linkage is the most widely used mechanism for generating 2D paths. Some researchers have developed computational tools to address the 2D path generation problem, such as mechanical character design [23, 40] and interactive editing methods [41]. Additionally, they have explored walking machines [42]. Cheng et al. [43] leverage the 3D cam-follower mechanism to generate a 2D path on a planar surface and a cylindrical or spherical surface.\n\nFurthermore, much effort has been made to generate a 3D path [44, 45, 46]. One of the typical solutions is to use 1-DoF spatial linkages, whose links can move in 3D space and be driven by a single actuator, such as the RCCC mechanism [47] and the RRSS mechanism [48]. Similar to their 2D counterparts, these designs can approximate a desired 3D path but still have limited engineering application cases. While little literature addresses the path generation problem with the overconstrained linkage family, it has been demonstrated for its engineering potential in robotic applications [31] and energy efficiency in robotic limb design [32]. Compared with 1-DoF spatial linkages, Cheng et al. [49] combine the 3D cam-follower with a spatial linkage mechanism for exact path generation. However, the proposed mechanism has limited application in the robotics field due to its higher number of pairs and lack of stiffness. In contrast, our overconstrained mechanism consists of only revolute joints, which are convenient to maintain and assemble and capable of carrying a relatively high payload. As its planar counterpart (planar four-bar linkage) has been widely used in modern machinery [36], the overconstrained linkage should have sufficient potential for advanced robotic design [31].\n\nOne of the most widely studied geometric generation problems is the topology optimization problem, aiming to optimize the geometric shape of static objects under dynamic constraints [50]. There is relatively little literature on optimizing the geometric shapes of dynamic objects under kinematic constraints, such as designing self-collision-free structures. In the graphics community, recent work has focused on the design of transformable objects [51], which are typically realized via 3D printing. For example, Yuan et al. [52] propose a computational approach to generate transformable robots that shape-shift into different forms. However, the design of these objects extends beyond linkages and typically only concerns the initial and final states [53]. The geometric design of the 2D linkage can be resolved by straightforwardly offsetting each component along the directional normal to its motion plane [23].\n\nRegarding spatial linkages, it becomes a challenge to design collision-free geometries due to the increased dimensions [54]. Li et al. [55] leverage the boolean operation to trim the link design for generation collision-free linkage, as well as imply the conjecture of the existence of collision-free spatial linkages over the complete motion circle. However, the resulting link design is defined by the swept volume and can not be directly assembled for a practical linkage mechanism. Zhang et al. [56] transfer the link bar into the deformable chain to obtain the collision-free deployable 3D structure. At the same time, their linkages do not aim at invertible motion and are not directly applicable to robotic structure design. In addition, their method employs a gradient-free approach that requires a relatively longer computing time than our formulation. Therefore, unlike the existing literature, our work aims to generate self-collision-free robotic limb design by a gradient-based formulation for engineering purposes.\n\nThis study presents a computational design method for energy-efficiently realizing user-specified spatial motion via the self-collision-free overconstrained linkage driven by a single actuator. We proposed a generalized method for self-collision-avoidance design by formulating the geometric optimization problem with continuous constraints, which is solvable by non-linear programming problem solvers. Then, we formulated the motion design problem with 1-DoF overconstrained linkages and optimized the design parameters to obtain energy-efficient target trajectories. Finally, we conducted several experiments to validate the proposed method and designs, including kinematic and dynamic evaluations, mechanical character demonstrations, and a hexapod prototype robot. We found that the overconstrained robotic limb design shows the potential to generate energy-efficient motion in specific tasks. The contributions of this study are listed as follows:\n\nProposed a generalized gradient-based design approach for linkage-based robotic limbs with self-collision-free motions.\n\nDeveloped a computational design framework by optimizing the similarity and energy-related metrics for 1-DoF overconstrained robotic limbs with desired trajectories.\n\nValidated the kinematic and dynamic performance of the resulting designs and demonstrated the superior energy efficiency of overconstrained limbs with a hexapod walker.\n\nThe remainder of this study, Section 2, presents the collision-free design method and optimization problem formulation for overconstrained motion generation. Validation with hardware experiments and further discussion are enclosed in Sections 3 and 4. Section 5 presents the conclusion, limitations, and future work, which conclude this work.\n\n1. Proposed a generalized gradient-based design approach for linkage-based robotic limbs with self-collision-free motions.\n\n2. Developed a computational design framework by optimizing the similarity and energy-related metrics for 1-DoF overconstrained robotic limbs with desired trajectories.\n\n3. Validated the kinematic and dynamic performance of the resulting designs and demonstrated the superior energy efficiency of overconstrained limbs with a hexapod walker.",
            "llm_summary": "【论文的motivation是什么】  \n1. 设计具有多个自由度的机器人肢体面临复杂性和自碰撞问题。  \n2. 单自由度设计在控制、成本和效率方面具有优势，但缺乏运动多样性。  \n3. 现有的闭链机器人肢体设计在优化参数和几何形状时存在挑战。  \n\n【提出了什么创新的方法】  \n本研究提出了一种新的计算设计方法，旨在通过优化几何形状和参数，实现单自由度的过约束机器人肢体的自碰撞自由运动。首先，研究者提出了一个几何优化问题的广义公式，以确保设计的自碰撞自由。接着，针对过约束连杆的空间轨迹生成问题进行了优化，确保运动的平滑性和能效。通过一系列实验验证了该方法，结果显示，采用过约束肢体的六足机器人在前进行走时展现出卓越的能效。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Developing Vision-Language-Action Model from Egocentric Videos",
            "authors": "Tomoya Yoshida,Shuhei Kurita,Taichi Nishimura,Shinsuke Mori",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21986",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21986",
            "arxiv_html_link": "https://arxiv.org/html/2509.21986v1",
            "abstract": "Egocentric videos capture how humans manipulate objects and tools, providing diverse motion cues for learning object manipulation. Unlike the costly, expert-driven manual teleoperation commonly used in training Vision-Language-Action models (VLAs), egocentric videos offer a scalable alternative. However, prior studies that leverage such videos for training robot policies typically rely on auxiliary annotations, such as detailed hand-pose recordings. Consequently, it remains unclear whether VLAs can be trained directly from raw egocentric videos.\nIn this work, we address this challenge by leveraging EgoScaler, a framework that extracts 6DoF object manipulation trajectories from egocentric videos without requiring auxiliary recordings. We apply EgoScaler to four large-scale egocentric video datasets and automatically refine noisy or incomplete trajectories, thereby constructing a new large-scale dataset for VLA pre-training.\nOur experiments with a state-of-the-art π0\\pi_{0} architecture in both simulated and real-robot environments yield three key findings: (i) pre-training on our dataset improves task success rates by over 20% compared to training from scratch, (ii) the performance is competitive with that achieved using real-robot datasets, and (iii) combining our dataset with real-robot data yields further improvements.\nThese results demonstrate that egocentric videos constitute a promising and scalable resource for advancing VLA research.",
            "introduction": "Vision-Language-Action models (VLAs) aim to learn general-purpose robot behaviors that follow natural language instructions across environments [3, 4, 5, 6, 7, 8, 9, 10, 11].\nSuch models are pre-trained with large-scale, multi-embodiment datasets [5, 8, 11] and then fine-tuned on embodiment-specific datasets.\nHowever, most pre-training datasets for VLAs heavily rely on human teleoperation, where a number of experts directly manipulate robots to collect instances for imitation learning.\nThis is inherently costly and labor-intensive, leaving a data scarcity problem.\n\nOne promising direction to address this problem is to leverage first-person perspective recordings of humans performing everyday tasks, enabled by the advancement of AR/VR devices and smart glasses [12, 13, 14].\nParticularly, such egocentric videos provide diverse human-object interactions at a close range and inherently provide motion cues for learning object manipulation.\nSeveral studies have begun to explore how to utilize egocentric videos in robot learning [15, 16, 17, 18].\nFor example, EgoMimic [16] and EgoVLA [17] leverage enriched egocentric recordings including hand poses to learn robot policies.\nThese studies demonstrate that utilizing egocentric videos is more time- and scale-efficient than those from teleoperation-based data collection.\nUnfortunately, these approaches depend on dense auxiliary recordings, such as hand poses and action start/end timestamps.\nObtaining these dense auxiliary recordings requires specialized hardware, such as multi-camera systems or depth sensors, as well as extensive manual annotation.\nIn a recent study, LAPA [19] attempted to learn latent action representations from egocentric videos.\nWhile this approach is scalable because it does not require auxiliary labels, such latent representations often struggle to capture fine-grained motions. For example, they showed strong performance on simple actions like pushing but only mediocre performance on more complex skills like pick-and-place.\n\nConsidering the limited scalability of rich egocentric recordings and the lack of fine-grained motion cues in egocentric videos, existing methods provide valuable insights but may fall short of offering sufficiently detailed and diverse action examples for robotic foundation models (see Fig. 1).\nIt is also notable that robot policies trained on diverse real-world egocentric recordings can fall short when evaluated within controlled environments, particularly simulators, due to simplified visual systems [20, 21].\nTherefore, although egocentric recordings are undeniably valuable resources of human motion cues, they remain underexplored in the existing literature.\n\nTo address this issue, we focus on extracting explicit action trajectories, which provide supervision that represents how to move and rotate objects.\nWe leverage EgoScaler [1], a framework designed to extract object manipulation trajectories from egocentric videos.\nEach pose in a trajectory represents the centroid and rotation of the manipulated object, approximated as the end-effector states of a robot, excluding the gripper.\nWe apply this framework to four large egocentric video datasets, including Ego4D [22], Ego-Exo4D [23], HD-EPIC [24], and Nymeria [25].\nThe extracted trajectories are then curated by automatically removing noisy or incomplete instances. After this careful filtering process, we construct a new large-scale dataset for VLA pre-training.\n\nWe conduct our experiments based on a state-of-the-art π0\\pi_{0} [8] architecture.\nFor comparison, we include three real-robot datasets—BC-Z [26], BridgeData V2 [27], and Fractal [3], which match our dataset in scale and diversity.\nWe evaluate performance in both simulated (SIMPLER [20]) and real-robot (ALOHA [28]) environments.\nOur key findings are threefold:\n\nWe successfully train π0\\pi_{0} from egocentric videos without auxiliary labels, achieving significant improvements over both training from scratch and LAPA.\n\nOur dataset achieves performance on par with leading real-robot datasets, while slightly outperforming BC-Z and BridgeData V2.\n\nCombining our dataset with BridgeData V2 yields further gains, surpassing the performance of pre-training on either dataset alone.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的VLA模型训练依赖于昂贵的人工遥控数据，导致数据稀缺问题。  \n2. 现有方法利用第一人称视角视频进行机器人学习，但依赖于密集的辅助标注，限制了其可扩展性。  \n3. 现有的从第一人称视频学习的模型在捕捉细粒度动作方面表现不足，难以支持复杂技能的学习。  \n\n【提出了什么创新的方法】  \n本研究提出了EgoScaler框架，能够从第一人称视频中提取6DoF对象操作轨迹，而无需辅助标注。该方法应用于四个大型第一人称视频数据集，自动过滤噪声或不完整的轨迹，构建了一个新的大规模数据集用于VLA的预训练。实验表明，使用该数据集进行预训练的模型在任务成功率上提高了20%以上，且在模拟和真实机器人环境中表现出色，证明了第一人称视频在VLA研究中的潜力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning",
            "authors": "Sigmund Hennum Høeg,Aksel Vaaler,Chaoqi Liu,Olav Egeland,Yilun Du",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": ". This work has been submitted to the IEEE for possible publication. Seethis https URLfor the project website",
            "pdf_link": "https://arxiv.org/pdf/2509.21983",
            "code": "https://sigmundhh.com/hybrid_diffusion/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21983",
            "arxiv_html_link": "https://arxiv.org/html/2509.21983v1",
            "abstract": "Constructing robots to accomplish long-horizon tasks is a long-standing challenge within artificial intelligence. Approaches using generative methods, particularly Diffusion Models, have gained attention due to their ability to model continuous robotic trajectories for planning and control. However, we show that these models struggle with long-horizon tasks that involve complex decision-making and, in general, are prone to confusing different modes of behavior, leading to failure. To remedy this, we propose to augment continuous trajectory generation by simultaneously generating a high-level symbolic plan. We show that this requires a novel mix of discrete variable diffusion and continuous diffusion, which dramatically outperforms the baselines. In addition, we illustrate how this hybrid diffusion process enables flexible trajectory synthesis, allowing us to condition synthesized actions on partial and complete symbolic conditions. Project website: sigmundhh.com/hybrid_diffusion.",
            "introduction": "In the quest for general-purpose robotics, learning from demonstrations has proven a widely applicable paradigm. The primary task of imitation learning is to absorb a large number of demonstrations involving diverse behaviors. A performant and widely used technique for this task is to apply diffusion models [2, 3] for modeling robotic behavior. In addition to handling multimodal behavior [4], they are stable to train, and allow for flexible guidance through conditioning and composition [1, 5, 6, 7]. They are, as a result, ubiquitous in a number of robotic systems, such as open-loop trajectory modelling [1, 8, 6], closed-loop action inference using image-conditioning [4, 9, 10, 11], or as modules in composite systems [12, 13, 14].\n\nHowever, diffusion models often struggle to form long-horizon, non-smooth plans, which restricts them to only modeling relatively short and dense trajectories in Cartesian space [5]. This limits their ability to do long-horizon decision-making tasks. A motivating example is shown on the left of Figure 1, where a trajectory-level diffusion model is tasked with sorting three blocks. Despite the demonstrations always terminating in a sorted state, sampled trajectories from the planner fail to sort the blocks. This is exacerbated when task complexity is increased, as the diffusion model struggles to account for interdependencies over long time horizons. Indeed, our experiments (Sec. IV) demonstrate that when tasked with sorting an increasing number of blocks, pure diffusion models quickly fail.\n\nA popular paradigm for allowing robots to perform long-horizon decision-making tasks is Task-and-Motion Planning (TAMP). TAMP methods typically exploit the connection between symbolic and continuous motion plans to simplify and reduce the overall size of the search space [15]. For example, symbolic planners can construct symbolic abstracted plans that transfer the system to the goal state, while continuous motion planning can find motion plans that correspond to this symbolic plan [16]. The inclusion of planning in symbolic space not only increases the planner’s efficiency and performance, but it also allows for transparency, unlike that of pure continuous planners. Upon generation, the symbolic plan provides a clear indication of the high-level steps involved, offering clarity. Additionally, their connection between continuous and symbolic plans allows for direct control of the robot by modifying parts of the symbolic plan and having the continuous plan respect these restrictions. For example, when a robot is tasked with moving three boxes from one location to another, we may want to specify at test time that a particular box should be moved first. It would be of interest to combine these techniques with planning using diffusion models to make them more transparent by providing a symbolic description of the robot plan and allowing for guidance and conditioning at a symbolic level.\n\nAs a response, we present Hybrid Diffusion Planner (HDP), a performant method for simultaneously generating both continuous and symbolic plans, as shown on the right of Fig. 1. Its connection to symbolic plans enables unprecedented transparency and guidance functionality. Surprisingly, incorporating modeling of symbolic information with HDP improves the long-horizon planning performance drastically compared to motion-only diffusion. Through our experiments, we show that our formulation of a joint objective consisting of masked diffusion [17, 18] and continuous diffusion [3] is crucial to the success of the method. In addition to the performance gain, HDP enables flexible conditional sampling at inference. By fixing a partial or complete symbolic plan through inpainting, HDP can generate a continuous plan that satisfies the specified constraints. Such flexible conditioning allows HDP to be easily controlled and used for diverse tasks outside of explicit plan generation.\n\nTo further highlight the challenges of long-horizon planning and show the benefits of HDP, we present a novel task suite of simulated and real robotic tasks focused on long-horizon complex planning. Previous IL benchmarks either focus on single-task performance [4, 19] or, when considering long-horizon operations, the subtasks are specified to the policy by an external oracle [20, 21]. In contrast, we focus on long-horizon robotic manipulation tasks where the agent is tasked with determining the sequence of subtasks itself. We find that traditional diffusion-based planning from demonstrations performs poorly in this scenario.\n\nOverall, our contributions are threefold: (1) We introduce a novel Imitation Learning task suite, exhibiting complex and long-horizon planning for robotic manipulation, and show that widely used diffusion-model planning struggles in the face of long-horizon, multimodal demonstrations. (2) We introduce Hybrid Diffusion Planner, a novel diffusion-based planner that uses a coupled discrete and continuous diffusion process for generating both symbolic and continuous motion plans. (3) Lastly, we empirically demonstrate HDP’s flexible conditioning capabilities.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的扩散模型在处理长时间跨度的复杂决策任务时表现不佳。  \n2. 现有方法缺乏将符号规划与连续轨迹生成相结合的能力。  \n3. 需要提高机器人在长时间跨度任务中的透明性和指导功能。  \n\n【提出了什么创新的方法】  \n提出了一种混合扩散规划器（HDP），该方法通过同时生成符号计划和连续轨迹来解决长时间跨度的决策问题。HDP结合了离散变量扩散和连续扩散，显著提高了规划性能。通过在推理过程中进行灵活的条件采样，HDP能够生成满足特定约束的连续计划。这种方法不仅提升了长时间跨度的规划能力，还增强了对复杂任务的控制和适应性。实验结果表明，HDP在复杂的长时间跨度操作中表现优异，超越了传统的扩散模型。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "FlowDrive: moderated flow matching with data balancing for trajectory planning",
            "authors": "Lingguang Wang,Ömer Şahin Taş,Marlon Steiner,Christoph Stiller",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21961",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21961",
            "arxiv_html_link": "https://arxiv.org/html/2509.21961v1",
            "abstract": "Learning-based planners are sensitive to the long-tailed distribution of driving data. Common maneuvers\ndominate datasets, while dangerous or rare scenarios are sparse. This imbalance can bias\nmodels toward the frequent cases and degrade performance on critical scenarios. To tackle this problem,\nwe compare balancing strategies for sampling training data and find reweighting by\ntrajectory pattern an effective approach. We then present FlowDrive, a flow-matching trajectory planner that\nlearns a conditional rectified flow to map noise directly to trajectory distributions with few flow-matching steps.\nWe further introduce moderated, in-the-loop guidance that injects small perturbation between flow steps to\nsystematically increase trajectory diversity while remaining scene-consistent. On nuPlan and the interaction-focused\ninterPlan benchmarks, FlowDrive achieves state-of-the-art results among learning-based planners and approaches methods\nwith rule-based refinements. After adding moderated guidance and light\npost-processing (FlowDrive*), it achieves overall state-of-the-art performance across nearly all benchmark splits.",
            "introduction": "Trajectory planning in autonomous driving requires both safety and efficiency.\nTraditional planners rely on rule-based methods like model predictive control, graph or sampling-based methods, which are interpretable\nand safety-driven but often fail in complex, real-world conditions (Schwarting et al., 2018).\nRecent learning-based planners learn policies from data, capturing nuanced human driving behaviors\n(Wang et al., 2023; Cheng et al., 2024; 2023), and can rival classical systems on large-scale benchmarks.\n\nDespite progress, challenges remain. Real driving data exhibits long-tailed distributions, where common behaviors\nlike lane-following dominate while rare but safety-critical cases are underrepresented (Karnchanachari et al., 2024).\nThis imbalance biases planners toward frequent patterns, reducing reliability in corner cases. Moreover, dataset bias and\nlimited diversity lead to poor generalization—especially in dynamic traffic scenarios unseen\nduring training (Codevilla et al., 2019).\n\nGenerative models provide a promising solution. Diffusion models (Song & Ermon, 2019; Ho et al., 2020) generate trajectories\nvia iterative denoising and can model multi-modal behaviors, but often require many steps or careful guidance for feasibility.\nFlow matching (Lipman et al., 2023; Liu et al., 2023) offers an alternative generative paradigm.\nInstead of iterative denoising, it trains a continuous transformation mapping a simple prior directly to the data\ndistribution (Lipman et al., 2023). This enables fast, few-step sampling, while preserving the ability to model\nmulti-modal behaviors.\n\nMotivated by the need for diverse yet fast trajectory generation, we propose FlowDrive, a flow-matching\nplanner for autonomous driving. Unlike diffusion planners that produce trajectories via many denoising steps, FlowDrive\nlearns a continuous motion flow that directly transforms random initial noise into diverse driving trajectories,\nyielding faster sampling. We also observe that naively training such a planner on a standard driving dataset can\nlead to biased behavior, and the model may overfit to the most common scenarios and neglect underrepresented but critical\ncases. To address this, we analyze how data imbalance in the training set affects planning performance and introduce\na data balancing method that increases the coverage of rare behaviors. Furthermore, we introduce a mechanism to steer\nFlowDrive’s output trajectories in order to systematically diversify the generated candidates. Finally,\nwe evaluate FlowDrive on the nuPlan benchmark (Karnchanachari et al., 2024) and interPlan benchmark\n(Hallgarten et al., 2024). An anonymous code repository was provided to reviewers during peer review; the\nfinal code will be made public upon publication.\n\nIn summary, the contributions of this paper are:\n\nWe identify the impact of unbalanced training data on planning performance and propose a data-balancing\nstrategy to improve robustness to rare scenarios. Furthermore, we present FlowDrive, a flow-matching\ntrajectory planner that efficiently generates feasible driving trajectories for autonomous vehicles.\n\nWe introduce a guidance mechanism that steers FlowDrive’s outputs to produce more diverse trajectory samples.\nThis enables state-of-the-art performance on the nuPlan and interPlan benchmarks, outperforming previous rule-based,\nlearning-based and hybrid baselines.\n\n1. We identify the impact of unbalanced training data on planning performance and propose a data-balancing\nstrategy to improve robustness to rare scenarios. Furthermore, we present FlowDrive, a flow-matching\ntrajectory planner that efficiently generates feasible driving trajectories for autonomous vehicles.\n\n2. We introduce a guidance mechanism that steers FlowDrive’s outputs to produce more diverse trajectory samples.\nThis enables state-of-the-art performance on the nuPlan and interPlan benchmarks, outperforming previous rule-based,\nlearning-based and hybrid baselines.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统规划方法在复杂的真实世界条件下表现不佳。  \n2. 驾驶数据的长尾分布导致常见行为主导，稀有但安全关键的场景被低估。  \n3. 数据集偏差和有限的多样性导致在动态交通场景中的泛化能力差。  \n\n【提出了什么创新的方法】  \n本论文提出FlowDrive，一种流匹配轨迹规划器，通过学习条件修正流直接将噪声映射到轨迹分布，减少流匹配步骤。为了解决数据不平衡问题，提出了一种数据平衡策略，增加稀有行为的覆盖率。此外，引入了引导机制，在流步骤之间注入小扰动，系统性地增加轨迹多样性，同时保持场景一致性。FlowDrive在nuPlan和interPlan基准测试中实现了学习型规划器的最先进结果，并在几乎所有基准分割中达到了整体的最先进性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Learnable Conformal Prediction with Context-Aware Nonconformity Functions for Robotic Planning and Perception",
            "authors": "Divake Kumar,Sina Tayebati,Francesco Migliarba,Ranganath Krishnan,Amit Ranjan Trivedi",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG); Statistics Theory (math.ST)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21955",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21955",
            "arxiv_html_link": "https://arxiv.org/html/2509.21955v1",
            "abstract": "Deep learning models in robotics often output point estimates with poorly calibrated confidences, offering no native mechanism to quantify predictive reliability under novel, noisy, or out-of-distribution inputs. Conformal prediction (CP) addresses this gap by providing distribution-free coverage guarantees, yet its reliance on fixed nonconformity scores ignores context and can yield intervals that are overly conservative or unsafe. We address this with Learnable Conformal Prediction (LCP), which replaces fixed scores with a lightweight neural function sθ​(x)=fθ​(ϕ​(x))s_{\\theta}(x)=f_{\\theta}(\\phi(x)) that leverages geometric, semantic, and model cues. Trained to balance coverage, efficiency, and calibration, LCP preserves CP’s finite-sample guarantees while producing intervals that adapt to instance difficulty, achieving context-aware uncertainty without ensembles or repeated inference. On the MRPB benchmark, LCP raises navigation success to 91.5% versus 87.8% for Standard CP, while limiting path inflation to 4.5% compared to 12.2%. For object detection on COCO, BDD100K, and Cityscapes, it reduces mean interval width by 46–54% at 90% coverage, and on classification tasks (CIFAR-100, HAM10000, ImageNet) it shrinks prediction sets by 4.7–9.9%. The method is also computationally efficient, achieving real-time performance on resource-constrained edge hardware (Intel NUC with footprint 4.6×4.44.6\\times 4.4 inch2 and power <30<30 W) while simultaneously providing uncertainty estimates along with the mean prediction.",
            "introduction": "Learning from data is an inherently ill-conditioned problem that often admits multiple optimal solutions. Selecting a single solution while discarding others is theoretically unjustified and thus limits predictive robustness. Consequently, most learning models that output point predictions or poorly calibrated confidences [1] incur prediction errors that depend heavily on context such as occlusion, clutter, or distribution shift [2]. Moreover, these models are typically optimized for average-case accuracy rather than worst-case reliability in deployment [3]. As a result, while they may deliver strong mean performance, yet can fail catastrophically in rare yet safety-critical corner cases, resulting in a crucial limitation for their deployment for mission/safety-critical robotics.\n\nTwo primary sources of uncertainty exist in learning models: epistemic uncertainty, arising from limited data or model capacity and reducible with additional information, and aleatoric uncertainty, caused by inherent sensor noise or environmental ambiguity which is irreducible even with infinite data [4, 5]. Recent advances have shown promise in autonomous navigation [6, 7] and human-robot collaboration [8]. Recent work has explored separating these uncertainties through conformal inference and evidential learning [9], enabling more nuanced risk assessment. A range of uncertainty quantification (UQ) methods aim to capture these effects. Bayesian neural networks and variational inference estimate epistemic uncertainty but require multiple stochastic passes, making them impractical for time-constrained control [10]. Deep ensembles provide stronger calibration but are computationally expensive [11]. Approximate approaches such as Monte Carlo dropout reduce overhead but often yield poorly calibrated estimates [12], while post-hoc calibration adjusts confidence scores without statistical guarantees [1].\n\nConformal prediction (CP) has recently attracted significant interest as a principled framework for uncertainty quantification [13, 14, 15, 16, 17], with recent extensions to multi-sensor fusion [18] and adaptive abstention policies [19]. Originating in statistical learning theory, CP constructs prediction sets calibrated on held-out data and provides distribution-free, finite-sample coverage guarantees under the assumption of exchangeability. Unlike many heuristic post-hoc calibration methods, CP offers explicit statistical guarantees, ensuring that true outcomes fall within the predicted sets at a user-specified confidence level. Moreover, CP is suited even for legacy prediction models that do not necessarily rely on learning from data. These properties make CP particularly appealing for robotics, where models must operate under distribution shift and safety requires formal reliability bounds on decision-making.\n\nDespite its generality, CP is most often implemented with fixed nonconformity functions that fail to capture the intense interaction of input data, application domain, and context in shaping uncertainty. For instance, in regression, standard CP with residual-based nonconformity produces intervals of constant width across all inputs, ignoring heteroscedasticity [20, 21]. As a result, CP provides valid coverage but does not account for how uncertainty emerges from the joint structure of observations and operating conditions. This limitation is especially critical in robotics, where risk depends not only on the raw input but also on situational context: for example, a partially occluded object may be harmless clutter in a warehouse aisle yet represent a pedestrian entering a crosswalk in an urban scene. Treating both as equally uncertain either wastes efficiency in benign settings or under-protects in safety-critical ones.\n\nWe address this limitation with Learnable Conformal Prediction (LCP) (Fig. 1). Instead of fixed scores, we introduce a feature-driven function sθ​(x)=fθ​(ϕ​(x))s_{\\theta}(x)=f_{\\theta}(\\phi(x)) that adapts to the structure of prediction errors. Features ϕ​(x)\\phi(x) encode geometric, semantic, and model-derived cues, while fθf_{\\theta} is a lightweight neural network trained to balance coverage, efficiency, and calibration. Calibration over these learned scores preserves the finite-sample coverage guarantees of CP [13, 21], while producing intervals that shrink in simple cases and expand in difficult ones.\n\nWe evaluate LCP for (i) robotic path planning under noisy and incomplete sensing on the MRPB benchmark, (ii) object detection with uncertainty calibration on COCO, BDD100K, and Cityscapes, and (iii) image classification on CIFAR-100, HAM10000, and ImageNet. Across these benchmarks, LCP consistently improves the safety–efficiency trade-off across planning, perception, and classification tasks. On the MRPB path-planning benchmark, LCP raises success rates to 91.5% while limiting path inflation to 4.5%, compared to 87.8% success and 12.2% inflation with standard CP. For object detection on COCO, BDD100K, and Cityscapes, LCP reduces mean interval width by 46–54% while sustaining ≈\\approx90% coverage. In classification (CIFAR-100, HAM10000, ImageNet), it cuts prediction set sizes by 4.7–9.9% relative to fixed baselines without losing validity. The proposed framework is also computationally efficient, achieving real-time performance on resource-constrained edge hardware (Intel NUC, area 4.6×4.44.6\\times 4.4 inch2, power <30<30 W) while simultaneously extracting uncertainty estimates and prediction. This aligns with recent advances in edge robotics [22] and intelligent sensing-to-action systems [23].",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有深度学习模型在机器人领域输出的点估计缺乏可靠的置信度量化。  \n2. 固定的非顺应性评分无法捕捉上下文信息，导致置信区间过于保守或不安全。  \n3. 需要在不确定性量化中平衡覆盖、效率和校准，以提高机器人决策的可靠性。  \n\n【提出了什么创新的方法】  \n提出了可学习的顺应性预测（LCP），通过引入特征驱动的神经网络函数来替代固定的非顺应性评分。该方法利用几何、语义和模型线索，经过训练以平衡覆盖、效率和校准，保持了顺应性预测的有限样本保证。LCP在多个基准测试中表现出色，如在MRPB基准上，将导航成功率提高到91.5%，并将路径膨胀限制在4.5%。在COCO、BDD100K和Cityscapes的物体检测中，LCP将均值区间宽度减少了46-54%，在CIFAR-100、HAM10000和ImageNet的分类任务中，预测集大小减少了4.7-9.9%。该方法在资源受限的边缘硬件上实现了实时性能，同时提供不确定性估计和均值预测。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks",
            "authors": "Jialiang Li,Wenzheng Wu,Gaojing Zhang,Yifan Han,Wenzhao Lian",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21928",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21928",
            "arxiv_html_link": "https://arxiv.org/html/2509.21928v1",
            "abstract": "Successfully solving long-horizon manipulation tasks remains a fundamental challenge. These tasks involve extended action sequences and complex object interactions, presenting a critical gap between high-level symbolic planning and low-level continuous control. To bridge this gap, two essential capabilities are required: robust long-horizon task planning and effective goal-conditioned manipulation. Existing task planning methods, including traditional and LLM-based approaches, often exhibit limited generalization or sparse semantic reasoning. Meanwhile, image-conditioned control methods struggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a novel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon Manipulation Tasks. SAGE utilizes semantic scene graphs as a structural representation for scene states. A structural scene graph enables bridging task-level semantic reasoning and pixel-level visuo-motor control. This also facilitates the controllable synthesis of accurate, novel sub-goal images. SAGE consists of two key components: (1) a scene graph-based task planner that uses VLMs and LLMs to parse the environment and reason about physically-grounded scene state transition sequences, and (2) a decoupled structural image editing pipeline that controllably converts each target sub-goal graph into a corresponding image through image inpainting and composition. Extensive experiments have demonstrated that SAGE achieves state-of-the-art performance on distinct long-horizon tasks.",
            "introduction": "Robots are expected to work in complex, real-world environments and perform long-horizon manipulation tasks that involve multiple cascading sub-task phases [1, 2]. Unlike short-horizon operations, these tasks require extended action sequences, with complex dependencies between objects and states. Despite the importance, robustly solving long-horizon manipulation tasks remains a significant challenge. A key difficulty lies in bridging the semantic gap between high-level symbolic planning and low-level continuous control. For example, a high-level plan for making a cup of tea might be put the teabag in the mug, then pour water. While this plan is easy for humans to understand, it is too abstract to be translated into a series of precise, continuous actions, such as correctly grasping the teabag and transfering it into the mug without spilling. Successfully bridging this gap requires two capabilities: (1) generating robust long-horizon task plans that are grounded in the physical world, and (2) providing controllers with actionable goal representations that translate abstract sub-tasks into pixel-level visuo-motor commands, enabling reliable continuous control.\n\nTo address long-horizon task planning, traditional approaches like Sense-Plan-Act (SPA) [3, 4] and Task and Motion Planning (TAMP) [5, 6] rely on manually predefined symbolic rules and known dynamic models, which limits their application in novel real-world scenarios. A more recent line of work leveraging large language models (LLMs) [7, 8, 9, 10] and vision-language models (VLMs) [11, 12, 13, 14] face new challenges including hallucination, uncontrollable generation, and extraction of only low-level sparse semantic information from raw images.\n\nTo provide strong goal guidance, efforts have been made to explore different forms of goals, encompassing language instructions [15, 16, 17] and images [18, 19]. Language is much flexible for human to specify the task goals, but it lacks the precise pixel-level information needed for effective policy learning. Meanwhile, researchers have studied image-conditioned manipulation via text-to-image diffusion-based image synthesis models [20, 21]. However, due to text embeddings, being not aligned with images, these works suffer in generating high-quality, novel images for unseen tasks.\n\nTo address the above challenges, we propose SAGE, a Scene Graph-Aware Guidance and Execution framework for long-horizon manipulation tasks. The key insight underlying SAGE is that many long-horizon manipulation tasks are uniquely defined by their specific temporal execution orderings and spatial object relationships. Based on this, as shown in Figure. 1, SAGE leverages LLMs for planning and generated images for goal guidance, while mitigating their limitations. At its core, SAGE uses semantic scene graphs [22] as a structural representation of the scene state, effectively aligning task-level semantic reasoning with pixel-level visuo-motor control. A scene graph models the physical world by representing objects as nodes and their spatial relationships (e.g., On, In) as edges. We employ this representation for two reasons. First, at the planning level, the scene graphs enable LLMs to extract structural, high-level semantic information, enabling them to generate physically-grounded task plans. Second, for image-conditioned manipulation, by decomposing a scene into a disentangled scene graph with independent objects and their pairwise relations, modifications can be applied locally and consistently, allowing controllable sub-goal image generation for even unseen tasks.\n\nSAGE consists of two key components: (1) a scene graph task planner that uses a VLM to parse the scene into a scene graph and a LLM to reason about a scene graph transition chain, which serves as a physically-grounded task plan; and (2) a decoupled structural image editing pipeline that controllably converts each scene graph of this plan into a corresponding image through image inpainting and composition. These generated images guide a visuo-motor policy to accomplish the entire long-horizon manipulation task by executing each sub-task iteratively. Experimental results have demonstrated that SAGE achieves state-of-the-art performance on various long-horizon tasks.\n\nWe summarize our contributions as follows:\n\nWe introduce SAGE, a framework that uses scene graphs to extract task keyframes, enabling the alignment between task-level semantic reasoning and pixel-level visuo-motor control for long-horizon manipulation.\n\nWe develop a scene graph task planner that robustly decomposes long-horizon tasks into interpretable, physically-grounded scene graph transition chains, facilitating structural symbolic reasoning.\n\nWe design a decoupled structural image editing pipeline that controllably synthesizes sub-goal images from predicted scene graphs, providing reliable visual guidance for visuo-motor control.\n\nWe demonstrate that SAGE achieves SOTA performance across diverse long-horizon tasks with distinct and novel temporal execution orderings and spatial object relationships.\n\n1. We introduce SAGE, a framework that uses scene graphs to extract task keyframes, enabling the alignment between task-level semantic reasoning and pixel-level visuo-motor control for long-horizon manipulation.\n\n2. We develop a scene graph task planner that robustly decomposes long-horizon tasks into interpretable, physically-grounded scene graph transition chains, facilitating structural symbolic reasoning.\n\n3. We design a decoupled structural image editing pipeline that controllably synthesizes sub-goal images from predicted scene graphs, providing reliable visual guidance for visuo-motor control.\n\n4. We demonstrate that SAGE achieves SOTA performance across diverse long-horizon tasks with distinct and novel temporal execution orderings and spatial object relationships.",
            "llm_summary": "【论文的motivation是什么】  \n1. 长期操作任务的复杂性使得高层符号规划与低层连续控制之间存在显著差距。  \n2. 现有任务规划方法在新任务的泛化能力和语义推理方面存在局限性。  \n3. 图像条件控制方法在适应未见任务时表现不佳。  \n\n【提出了什么创新的方法】  \nSAGE框架通过语义场景图作为场景状态的结构化表示，解决了长期操作任务中的规划和控制问题。该框架包含两个关键组件：场景图任务规划器和解耦的结构图像编辑管道。场景图任务规划器利用视觉-语言模型和大型语言模型解析环境并推理物理基础的场景状态转移序列，而图像编辑管道则通过图像修复和合成将目标子目标图转换为相应图像。SAGE在不同的长期任务上实现了最先进的性能，成功地将任务级语义推理与像素级的视觉运动控制对齐。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "WAVE: Worm Gear-based Adaptive Variable Elasticity for Decoupling Actuators from External Forces",
            "authors": "Moses Gladson Selvamuthu,Tomoya Takahashi,Riichiro Tadakuma,Kazutoshi Tanaka",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21878",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21878",
            "arxiv_html_link": "https://arxiv.org/html/2509.21878v1",
            "abstract": "Robotic manipulators capable of regulating both compliance and stiffness offer enhanced operational safety and versatility. Here, we introduce Worm Gear-based Adaptive Variable Elasticity (WAVE), a variable stiffness actuator (VSA) that integrates a non-backdrivable worm gear.\nBy decoupling the driving motor from external forces using this gear, WAVE enables precise force transmission to the joint, while absorbing positional discrepancies through compliance.\nWAVE is protected from excessive loads by converting impact forces into elastic energy stored in a spring.\nIn addition, the actuator achieves continuous joint stiffness modulation by changing the spring’s precompression length.\nWe demonstrate these capabilities, experimentally validate the proposed stiffness model, show that motor loads approach zero at rest–even under external loading–and present applications using a manipulator with WAVE.\nThis outcome showcases the successful decoupling of external forces.\nThe protective attributes of this actuator allow for extended operation in contact-intensive tasks, and for robust robotic applications in challenging environments.",
            "introduction": "Introducing robots into environments not specifically designed for them, such as human living spaces, requires flexibility to mitigate collision forces and prevent damage, while maintaining sufficient rigidity for precise and powerful actuation as shown in Fig. 1 (right).\nOne effective approach is incorporating inherent compliance into the robot design, which helps absorb unexpected collision forces, enhancing safety and tolerance to positional errors.\nHowever, in many scenarios, high payload capacity or accurate position control is also necessary for performing tasks at a level comparable to human capabilities.\nImplementing variable joint compliance is beneficial in integrating these features into a single hardware system.\n\nA Variable Stiffness Actuator (VSA) is a hardware module designed to control compliance, enabling robots to change their joint stiffness in a mechanical manner to perform diverse tasks [1], [2].\nCompared to software-based stiffness control, VSA has a fast response speed and can store impact force energy in the spring, protecting both the robot and the actuator from the force [3].\nBased on actuator configuration as shown in Fig. 2, VSAs can be classified into the antagonistic type [4], [5] and independent type [6].\nAntagonistic VSAs utilize two motors to control both elastic force and joint actuation through differential motion as shown in Fig. 2(a).\nHowever, because the motors are constantly subjected to spring compression forces, additional torque is required beyond what is needed for joint movement. In contrast, as shown in Fig. 2(b), independent VSAs separate the roles of the angle control motor that primarily controls joint actuation and the stiffness control motor that adjusts spring precompression.\nSince the VSA motor does not necessarily need to follow the angle control motor’s speed during joint actuation, it can be smaller, reducing the overall mass of the two motors. Nevertheless, in terms of impact absorption, external impact forces transmitted through the spring can still reach the angle control motor, potentially leading to actuator damage. This study aims to develop a VSA that ensures the independence of force transmission paths for external impact absorption and joint actuation. Worm gears or power screws are known for their non-backdrivable properties and have been utilized in applications where large forces are applied [7].\nA worm gear is a transmission component that converts rotational motion between perpendicular shafts, typically used to achieve high reduction ratios. A worm gear with small lead angle and single-start thread is non-backdrivable, where the worm wheel cannot drive the worm screw because the friction at the contact surface exceeds the torque generated by the reverse force.\n\nIn this paper, we propose a novel VSA mechanism named WAVE (Worm gear-based Adaptive Variable Elasticity) as shown in Fig. 2(c).\nThis mechanism incorporates a worm gear, which is typically fixed in the linear direction but, in our design, is constrained via a spring along that axis.\nThis allows for two distinct motion pathways: (A) rotation of the joint by the angle control motor (M1\\text{M}_{\\text{1}}), shown in red in Fig. 1(a), and (B) precompression of the linear spring by external force feedback or the stiffness control motor (M2\\text{M}_{\\text{2}}), shown in blue in Fig. 1(b).\nAs shown in Fig. 1 (left), this mechanism transmits the angle control motor’s torque to the joint via the worm gear while the spring passively absorbs displacement. This allows for adaptive trajectory generation in response to environmental interactions. Additionally, since the worm gear completely decouples the angle control motor, the impact force is immediately converted into elastic energy of the spring, effectively protecting the actuator. Furthermore, active compliance control–referring to the ability to adjust passive joint deflection in response to external forces by modifying the spring preload–enables the system to support high payloads.\n\nIn this paper, we introduce WAVE as a novel variable stiffness actuator that decouples the angle control motor from external forces through a worm gear mechanism.\nThe main contributions of our work are threefold:\n\nWe propose a compact and integrated hardware design that leverages non-backdrivability to isolate the angle control motor from external forces.\n\nWe demonstrate how active control of the worm gear’s linear axis enables continuous switching between high-stiffness for precise and powerful actuation, and low-stiffness for safe interaction and impact absorption.\n\nThrough experimental evaluation, we show that our mechanism can robustly handle a wide range of tasks, from high-load manipulation to contact-rich operations, without compromising safety or performance.",
            "llm_summary": "【论文的motivation是什么】  \n1. 需要在机器人操作中实现灵活性，以减轻碰撞力并防止损坏。  \n2. 机器人在执行任务时必须具备高负载能力和精确的位置控制。  \n3. 现有的可变刚度执行器在抗冲击和安全性方面存在不足。  \n\n【提出了什么创新的方法】  \n本文提出了一种名为WAVE的可变刚度执行器，利用蜗轮机制将角度控制电机与外部力解耦。该机制通过蜗轮传递扭矩，同时允许弹簧被动吸收位移，从而实现环境交互中的自适应轨迹生成。WAVE的设计确保了在高刚度和低刚度之间的连续切换，以适应不同的操作需求。实验结果表明，WAVE能够在高负载操作和接触丰富的任务中稳健地工作，而不影响安全性或性能。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Improved Vehicle Maneuver Prediction using Game Theoretic Priors",
            "authors": "Nishant Doshi",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21873",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21873",
            "arxiv_html_link": "https://arxiv.org/html/2509.21873v1",
            "abstract": "未获取到摘要",
            "introduction": "Conventional maneuver prediction methods use some sort of classification model on temporal trajectory data to predict behavior of agents over a set time horizon. Despite of having the best precision and recall, these models cannot predict a lane change accurately unless they incorporate information about the entire scene. Level-k game theory can leverage the human-like hierarchical reasoning to come up with the most rational decisions each agent can make in a group. This can be leveraged to model interactions between different vehicles in presence of each other and hence compute the most rational decisions each agent would make. The result of game theoretic evaluation can be used as a “prior “ or combined with a traditional motion-based classification model to achieve more accurate predictions. The proposed approach assumes that the states of the vehicles around the target lead vehicle are known. The module will output the most rational maneuver prediction of the target vehicle based on an online optimization solution. These predictions are instrumental in decision making systems like Adaptive Cruise Control (ACC) or Traxen’s iQ-Cruise further improving the resulting fuel savings.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的行为预测模型在准确预测车道变换方面存在局限性。  \n2. 需要更好地建模车辆之间的相互作用以提高预测精度。  \n\n【提出了什么创新的方法】  \n提出了一种结合了层级博弈理论的车辆机动预测方法。该方法通过利用人类般的推理能力来模拟不同车辆之间的相互作用，从而计算出每个代理的最合理决策。通过将博弈理论评估的结果作为“先验”，与传统的基于运动的分类模型相结合，显著提高了预测的准确性。该方法在已知周围车辆状态的基础上，输出目标车辆的最合理机动预测，进而优化决策系统，如自适应巡航控制（ACC），实现更好的燃油节省效果。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors",
            "authors": "Ning Huang,Zhentao Xie,Qinchuan Li",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21810",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21810",
            "arxiv_html_link": "https://arxiv.org/html/2509.21810v1",
            "abstract": "Despite growing interest in developing legged robots that emulate biological locomotion for agile navigation of complex environments, acquiring a diverse repertoire of skills remains a fundamental challenge in robotics. Existing methods can learn motion behaviors from expert data, but they often fail to acquire multiple locomotion skills through a single policy and lack smooth skill transitions. We propose a multi-skill learning framework based on Conditional Adversarial Motion Priors (CAMP), with the aim of enabling quadruped robots to efficiently acquire a diverse set of locomotion skills from expert demonstrations. Precise skill reconstruction is achieved through a novel skill discriminator and skill-conditioned reward design. The overall framework supports the active control and reuse of multiple skills, providing a practical solution for learning generalizable policies in complex environments.",
            "introduction": "In recent years, reinforcement learning (RL) has demonstrated significant potential in enabling robots to master diverse skills. In the domain of quadrupedal locomotion, RL has been successfully applied to allow quadruped robots to perform diverse and challenging tasks in real-world environments [1]–[4]. Typically, RL techniques rely on carefully designed reward functions tailored to specific tasks in order to guide the learning of desired behaviors. Yet engineering rewards for\ncomplex skills necessitates balancing competing objectives an error-prone process where desired behaviors seldom emerge.\n\nAmong various reinforcement learning approaches, imitation-based reinforcement learning offers a convenient and effective means for acquiring complex skills. By leveraging limited number of expert demonstrations, imitation learning enables agents to replicate expert-like behaviors. Escontrela et\nal.[5] have demonstrated that Adversarial Motion Priors (AMP) constitute an effective framework for this purpose. AMP employs a Generative Adversarial Network (GAN) structure, in which a discriminator distinguishes demonstration samples and agent-generated samples, thus providing\nreward signals that motivate the agent to produce motion behaviors stylistically similar to expert demonstrations.\n\nBeyond simply mimicking expert motions, AMP also supports more flexible and generalizable behavior synthesis. It enables quadruped robots to learn user-specific gait patterns by enforcing motion styles specified by reference trajectories, even in the absence of direct low-level control data. This makes AMP particularly suitable for solving well-defined tasks where stylistic fidelity is important, while alleviating the need for dense or fine-grained expert annotations. However,\nthe training process of GANs is known to be highly unstable[12],[15]. When learning multiple motion skills simultaneously, the generator may collapse to producing only a limited subset of behaviors, failing to cover the full data distribution. This lack of diversity in the generated samples undermines the representational richness required to capture the complexity of real expert demonstrations.\n\nIn this study, we propose a novel approach based on the fundamental idea of Conditional Generative Adversarial Networks (CGAN) to address the challenge of multi-skill learning in the Adversarial Motion Priors (AMP) framework. Traditional AMP frameworks usually focus on learning a single target task and cannot effectively handle complex multi-skill learning problems. To overcome this limitation, we introduce additional conditional information to make the learning process more controllable and flexible. Specifically, we use skill categories as conditional inputs to guide the generator in producing specific motion sequences under different skill contexts, thereby enabling simultaneous learning of multiple skills. This method not only allows handling multiple tasks within the same network but also ensures that the generation and learning of each task or skill can be adjusted according to the different requirements of the conditional variables, thus avoiding the task interference problems that may occur in traditional approaches.\n\nTo further improve the quality and controllability of skill-conditioned motion generation, we extend our framework by introducing an additional skill discriminator. While the conditional generator enables the policy to produce diverse motion sequences under different skill contexts, it does not explicitly ensure that the generated behaviors match the characteristics of the intended skill. To address this, the skill discriminator is designed to classify the type of skill expressed\nby the current behavior, using reference samples from the expert dataset as supervision. By incorporating this discriminator, the agent receives informative feedback during training,\nguiding the policy to better align the generated motions with the desired skill characteristics and enhancing both motor capability and control precision.\n\nThis mechanism enhances the policy’s dynamic adaptability to diverse skill requirements while improving robust training stability and cross-skill generalization performance. We validate the proposed approach through extensive experiments in both simulation and on a physical quadruped robot. The results demonstrate that our system is capable of generating diverse motion skills conditioned on user-specified commands, and achieve smooth transitions between skills during motion.",
            "llm_summary": "【论文的motivation是什么】  \n1. 多技能学习在四足机器人中面临挑战，现有方法难以通过单一策略获取多种运动技能。  \n2. 现有的模仿学习方法在技能转换的平滑性和多样性方面存在不足。  \n\n【提出了什么创新的方法】  \n提出了一种基于条件对抗运动先验（CAMP）的多技能学习框架，利用条件生成对抗网络（CGAN）来引导生成器在不同技能上下文中产生特定运动序列。通过引入技能鉴别器，增强了对生成运动的控制和准确性，从而实现了多技能的同时学习。该方法在复杂环境中有效地生成多样化的运动技能，并实现了技能之间的平滑过渡，提升了四足机器人的运动能力和控制精度。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions",
            "authors": "Hyeonseong Kim,Roy El-Helou,Seungbeen Lee,Sungjoon Choi,Matthew Pan",
            "subjects": "Robotics (cs.RO); Human-Computer Interaction (cs.HC)",
            "comment": "for more videos, seethis https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.21776",
            "code": "https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21776",
            "arxiv_html_link": "https://arxiv.org/html/2509.21776v1",
            "abstract": "Playful deception, a common feature in human social interactions, remains underexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice Cream (TIC) vendor routine, we investigate how bounded, culturally familiar forms of deception influence user trust, enjoyment, and engagement during robotic handovers. We design a robotic manipulator equipped with a custom end-effector and implement five TIC-inspired trick policies that deceptively delay the handover of an ice cream-shaped object. Through a mixed-design user study with 91 participants, we evaluate the effects of playful deception and interaction duration on user experience. Results reveal that TIC-inspired deception significantly enhances enjoyment and engagement, though reduces perceived safety and trust, suggesting a structured trade-off across the multi-dimensional aspects. Our findings demonstrate that playful deception can be a valuable design strategy for interactive robots in entertainment and engagement-focused contexts, while underscoring the importance of deliberate consideration of its complex trade-offs. You can find more information, including demonstration videos, on https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/.",
            "introduction": "In Human–Robot Interaction (HRI), deceptive behaviours of robots are often treated as harmful since they often reduce user trust [1, 2, 3, 4]. Prior studies have mainly focused on avoiding deception through predictable behaviours and intent displays [5, 6, 7], or on repairing trust [8, 9, 10]. On the other hand, empirical investigations into the potential benefits of robot deception have been conducted in game contexts where deception is permitted or even encouraged [11, 12, 13].\n\nHowever, beyond game-like contexts where the deception is explicitly allowed, some playful deceptions also enrich interpersonal experiences in everyday human-human interaction. Playful deception can create enjoyable, light-hearted moments, so long as it is interpreted as part of a benign performance rather than as manipulation. For instance, a human handing over an object might playfully delay the delivery or briefly mislead the receiver through unexpected but harmless actions, forming humour that is only possible through physical embodiment.\n\nThe Turkish ice cream (TIC) vendor routine exemplifies this point: it relies on intentional misdirection to entertain while ultimately delivering the treat. The TIC interaction is a short street performance in which a vendor playfully prolongs the handover of an ice cream cone. Using a long spatula-like rod, the cone is presented and then briefly withdrawn, with light feints and showy gestures that tease the customer while keeping the exchange clearly playful. The routine is recognizably performative and always resolves with a successful handover, marking the deception as benign rather than malicious.\n\nIn this paper, we explore the potential role of playful deception as a design component in HRI through a study inspired by the TIC routine. To this end, as shown in Fig. 1, we design and implement a robotic handover system that reproduces TIC-inspired playful deceptive behaviours and conduct a user study with 91 participants to investigate the multi-dimensional effects of playful deception, such as enjoyment, trust, and perceived safety.\n\nIn summary, this paper makes three primary contributions:\n\nWe design and implement a robotic system capable of reproducing TIC-inspired deceptive handovers, introducing a novel playful interaction scenario.\n\nWe conduct a large-scale user study evaluating the multi-dimensional effects of playful TIC deception on user experience, revealing both its positive and negative impacts.\n\nWe emphasize the nuanced trade-offs that playful deception introduces and the importance of situating its use within appropriate contexts, offering guidance for both design and future research.\n\n1. We design and implement a robotic system capable of reproducing TIC-inspired deceptive handovers, introducing a novel playful interaction scenario.\n\n2. We conduct a large-scale user study evaluating the multi-dimensional effects of playful TIC deception on user experience, revealing both its positive and negative impacts.\n\n3. We emphasize the nuanced trade-offs that playful deception introduces and the importance of situating its use within appropriate contexts, offering guidance for both design and future research.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有研究主要关注避免机器人欺骗，缺乏对其潜在益处的探讨。  \n2. 玩耍式欺骗在日常人际互动中能够增添乐趣和轻松感。  \n3. 需要探索如何将玩耍式欺骗作为人机交互设计的组成部分。  \n\n【提出了什么创新的方法】  \n本文设计并实现了一种能够重现土耳其冰淇淋（TIC）风格的欺骗性交接的机器人系统，创造了一种新颖的互动场景。通过对91名参与者进行的大规模用户研究，评估了玩耍式欺骗对用户体验的多维影响，结果显示这种欺骗显著增强了用户的享受和参与感，但同时降低了安全感和信任感，表明在设计中需要权衡这些复杂的影响。研究结果为娱乐和互动为主的机器人设计提供了有价值的策略，同时强调了在适当的上下文中使用玩耍式欺骗的重要性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "VLBiMan: Vision-Language Anchored One-Shot Demonstration Enables Generalizable Robotic Bimanual Manipulation",
            "authors": "Huayi Zhou,Kui Jia",
            "subjects": "Robotics (cs.RO)",
            "comment": "under review",
            "pdf_link": "https://arxiv.org/pdf/2509.21723",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21723",
            "arxiv_html_link": "https://arxiv.org/html/2509.21723v1",
            "abstract": "Achieving generalizable bimanual manipulation requires systems that can learn efficiently from minimal human input while adapting to real-world uncertainties and diverse embodiments. Existing approaches face a dilemma: imitation policy learning demands extensive demonstrations to cover task variations, while modular methods often lack flexibility in dynamic scenes. We introduce VLBiMan, a framework that derives reusable skills from a single human example through task-aware decomposition, preserving invariant primitives as anchors while dynamically adapting adjustable components via vision-language grounding. This adaptation mechanism resolves scene ambiguities caused by background changes, object repositioning, or visual clutter without policy retraining, leveraging semantic parsing and geometric feasibility constraints. Moreover, the system inherits human-like hybrid control capabilities, enabling mixed synchronous and asynchronous use of both arms. Extensive experiments validate VLBiMan across tool-use and multi-object tasks, demonstrating: (1) a drastic reduction in demonstration requirements compared to imitation baselines, (2) compositional generalization through atomic skill splicing for long-horizon tasks, (3) robustness to novel but semantically similar objects and external disturbances, and (4) strong cross-embodiment transfer, showing that skills learned from human demonstrations can be instantiated on different robotic platforms without retraining. By bridging human priors with vision-language anchored adaptation, our work takes a step toward practical and versatile dual-arm manipulation in unstructured settings.",
            "introduction": "Recent years have witnessed rapid progress in embodied robotic manipulation, particularly under the paradigm of visuomotor imitation learning through large-scale teleoperated demonstrations Fang et al. (2024a); Khazatsky et al. (2024); O’Neill et al. (2024); Bu et al. (2025). By collecting thousands of real-world samples for each task and object setting, Vision-Language-Action (VLA) models Team et al. (2024); Kim et al. (2024); Lin et al. (2025) are trained to directly map raw sensory inputs to motor commands. This end-to-end approach avoids explicitly modeling task- or object-specific priors (even for challenging cases involving deformable or articulated objects), by embedding such complexities into high-dimensional latent representations. Such strategies are especially compatible with high-DoF collaborative scenarios like bimanual manipulation, enabling impressive performance on long-horizon tasks, as demonstrated by works such as ALOHA series Zhao et al. (2023a); Fu et al. (2024); Aldaco et al. (2024); Zhao et al. (2024), RDT-1B Liu et al. (2025a), π0\\pi_{0} Black et al. (2024), and FAST Pertsch et al. (2025). However, this line of research is bottlenecked by its reliance on large-scale data collection and retraining cycles: adapting to new objects or tasks typically demands a full demonstration pipeline and model retraining, hindering scalability in open-world settings with unbounded task-object combinations and robot types.\n\nTo alleviate this, recent efforts have embraced modularized VLA pipelines that leverage the generalization capabilities of pre-trained LLMs Achiam et al. (2023) and VLMs Radford et al. (2021); Xiao et al. (2024). These models are repurposed to handle perception and semantic grounding, while downstream motion execution is delegated to either optimization-based controllers or pretrained visuomotor modules such as atomic skills or diffusion policies Chi et al. (2023); Ze et al. (2024); Yang et al. (2024). Reinforcement learning in simulation also serves as a strategy for learning skill-specific controllers Xie et al. (2020); Chen et al. (2022); Yuan et al. (2024b). This modular design allows robotic agents to inherit part of the generalization capability from foundation models, while maintaining flexibility and interpretability. A common practice in these pipelines is to define generalizable representations (e.g., keypoints, affordances and correspondences), as structured anchors between perception and control. For instance, ReKep Huang et al. (2024b) plans robot motion by anchoring on multiple predicted relation points, MOKA Fang et al. (2024b) extracts fine-grained functional regions via multi-modal visual question answering, and RobotPoint Yuan et al. (2024a) identifies object-centric task-relevant point clusters. Such approaches demonstrate that keypoint-affordance abstractions are effective for transferring behavior across objects, viewpoints, or instances, and have become a cornerstone of generalizable manipulation.\n\nBuilding on this insight, we propose VLBiMan for one-shot bimanual manipulation that leverages vision-language anchoring without retraining. Our approach also relies on object-centric representation points, but rather than predicting them via learned networks, we utilize VLMs to perform stable and robust object segmentation, followed by two heuristic strategies for anchor selection: geometric center of masks and plane-contact points. These anchors, though reminiscent of affordances, are far more controllable and lightweight. Unlike prior zero-shot methods Huang et al. (2024b) that require fragile prompt engineering and suffer from unreliable trajectory execution, our framework is demonstration-conditioned: we structure the action plan based on a one-shot, fine-labeled demonstration, then adapt it using language-grounded object anchors and motion optimization techniques. This enables robust execution on complex bimanual tasks while reusing invariant sub-skills.\n\nOur methodology unfolds in three stages: (1) Task-Aware Bimanual Decomposition, which splits the one-shot demonstration into semantically meaningful left/right arm primitives with inter-arm dependencies; (2) Vision-Language Anchored Adaptation, which grounds the invariant motion primitives onto new scenes by aligning demonstration anchors with newly segmented objects via VLMs; (3) Autonomous Trajectory Composition, which composes new robot trajectories through kinematics-aware blending of adapted sub-skills, ensuring smooth coordination under scene variations. The related illustrations can be glimpsed in Fig. 1 and Fig. 2. VLBiMan actually is inspired by a key principle: what to achieve matters more than how to execute it. For instance, rather than mimicking the exact poses or insignificant diversities involved in pouring water, our approach focuses on capturing and re-instantiating the relative spatial relationship between the cup and bottle, emphasizing coordination rather than absolute motion. We validate VLBiMan across ten diverse bimanual tasks (including six basic bimanual skills, two long-horizon tasks consisting of skill combinations, and two multi-stage tool-use tasks), demonstrating superior generalization and minimal engineering overhead compared to prior strong baseline methods.\n\nTo summarize, our contributions are as follows:\n(i) We propose VLBiMan, a novel framework that enables generalizable bimanual manipulation through one-shot demonstration and vision-language anchoring, without retraining.\n(ii) We introduce a task-aware motion decomposition and adaptation mechanism, which reuses invariant sub-skills via object-centric anchors from VLMs and supports cross-embodiment transfer from human demonstrations to different robotic embodiments.\n(iii) We validate VLBiMan on ten diverse bimanual tasks, showing superior generalization, sample efficiency, and robustness compared to strong baselines.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有方法在模仿学习中需要大量示范以覆盖任务变体。  \n2. 模块化方法在动态场景中缺乏灵活性。  \n3. 需要从最小的人类输入中高效学习并适应现实世界的不确定性。  \n\n【提出了什么创新的方法】  \nVLBiMan框架通过任务感知分解从单个示范中提取可重用技能，利用视觉-语言锚定动态适应可调组件。该方法分为三个阶段：任务感知双手分解、视觉-语言锚定适应和自主轨迹组合，确保在场景变化下的平滑协调。通过在十个多样化的双手任务上进行验证，VLBiMan显示出显著的示范需求减少、组合泛化能力和对新物体的鲁棒性，且无需重新训练，展现了在非结构化环境中双手操作的实用性和灵活性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation",
            "authors": "Muqun Hu,Wenxi Chen,Wenjing Li,Falak Mandali,Zijian He,Renhong Zhang,Praveen Krisna,Katherine Christian,Leo Benaharon,Dizhi Ma,Karthik Ramani,Yan Gu",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21690",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21690",
            "arxiv_html_link": "https://arxiv.org/html/2509.21690v1",
            "abstract": "Humanoid table tennis (TT) demands rapid perception, proactive whole-body motion, and agile footwork under strict timing—capabilities that remain difficult for unified controllers. We propose a reinforcement learning framework that maps ball-position observations directly to whole-body joint commands for both arm striking and leg locomotion, strengthened by predictive signals and dense, physics-guided rewards. A lightweight learned predictor, fed with recent ball positions, estimates future ball states and augments the policy’s observations for proactive decision-making. During training, a physics-based predictor supplies precise future states to construct dense, informative rewards that lead to effective exploration. The resulting policy attains strong performance across varied serve ranges (hit rate ≥\\geq 96% and success rate ≥\\geq 92%) in simulations. Ablation studies confirm that both the learned predictor and the predictive reward design are critical for end-to-end learning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute joints, the policy produces coordinated lateral and forward–backward footwork with accurate, fast returns, suggesting a practical path toward versatile, competitive humanoid TT.",
            "introduction": "Humanoid robots hold great promise as general-purpose embodied intelligent agents capable of performing diverse real-world tasks. Recent advances in learning and control have substantially expanded their physical capability, with impressive demonstrations of walking [1], jumping [2], running, and dancing [3]. Still, most research has focused on scenarios where humanoids operate in free space or interact only with static objects. Reacting to fast-moving objects with highly coordinated versatile behaviors remains a fundamental challenge in humanoid whole-body control (WBC).\n\nTable tennis (TT), both a cerebral and physically demanding sport, exemplifies this challenge. Competitive TT matches are characterized by breathtakingly dynamic exchanges that demand rapid eye-hand-leg and whole-body coordination as well as high-level strategies that adapt to an opponent’s strengths and weaknesses. Versatile stroke strategies, which requires agile footwork, core stabilization, and precisely coordinated arm swing, play a key role in winning a table tennis match [4, 5]. In contrast to some of the previous dynamic locomotion or quasi-static manipulation tasks, robotic TT is distinguished by the need for high-speed perception, control, and high-level physical versatility – the ball needs to be played very precisely in time and space.\n\nTowards solving these challenges, this paper presents an end-to-end reinforcement learning (RL) framework\nthat maps ball-position observations and robot proprioception directly to whole-body reference motions for coordinated arm swing and footwork with high success rate (Fig. 1).\n\nPrior work on robotic TT control has been predominantly based on analytical models, combining ball-trajectory prediction with inverse kinematics for robot motion planning.\n\nMost approaches adopt the virtual hitting-plane assumption, where a virtual hitting point [6] is computed from a partial ball trajectory in real-time. With the predicted ball velocity at a given time instant, the target racket pose (i.e., position and orientation) and velocity are then specified, and a robot’s motion trajectory is generated to bring the racket to the desired pose at the required time [7, 8, 9, 10].\nYet, the virtual hitting-plane assumption is restrictive.\nIt reduces TT to a predictable machine task, eliminating the need for highly adaptive footwork and hand-leg coordination, and preventing both humans and humanoids from learning the true variability of play.\n\nRecently, the virtual hitting plane assumption has been relaxed to enable model-based controllers to realize combat strategies with competitive success rates [11] and returning speeds across multiple swing types [12].\nStill, most of the existing model-based approaches have been primarily focused on robotic arms with relatively few degrees of freedom (DoFs), making it difficult to directly scale to humanoid whole-body movement, which involves far more DoFs. For example, a humanoid robot has been shown to play TT using impedance control [13], but its motions are constrained to standing still without agile locomotion, thereby limiting the effective hitting range.\n\nLearning-based methods have made substantial progress toward achieving human-level athletic intelligence in robotic TT [14, 15, 16, 17, 18].\n\nConcurrently with this work, HITTER hierarchically integrates an analytical model based interception planner with an RL controller for humanoid whole-body TT, yielding the first demonstration with agile footwork and accurate strikes [19]. However, due to its underlying virtual-hitting plane constraint, the robot relies primarily on lateral footwork.\n\nResearchers at Google DeepMind have introduced an end-to-end, model-free RL framework that relaxes the virtual hitting-plane assumption and learns a perception-to-action policy on a robotic arm [17]. The robot arm has 6 revolute joints, with its base sliding along two orthogonal axes (lateral and forward–backward).\nThis configuration enlarges the reachable workspace and helps enable diverse emergent strokes.\nSubsequent work demonstrates competitive, strategy-aware play [15]. While the end-to-end framework works for robotic arms, its effectiveness on legged humanoids remains an open challenge.\nDue to the high-dimensional action space and inherent locomotion instability of humanoid robots, the training will struggle with efficient exploration. Also, the training can suffer from low sampling efficiency due to the sparse nature of ball-hitting rewards [19].\n\nTowards enabling versatile humanoid TT, this paper introduces an:\n\nEnd-to-end, unified RL formulation that jointly coordinates locomotion and striking directly based on ball-position observations and robot proprioception without modular or hierarchical planning.\n\nAugmentation of the RL pipeline with a ball-hitting position predictor that enables the RL policy to act proactively for returning the ball, as well as with dense, immediate reward design based on physics-based trajectory prediction. Such an augmentation accelerates policy learning toward effective TT strikes and footwork.\n\nTo the best of our knowledge, this is among the first unified RL frameworks that generate whole-body motions and demonstrate successful humanoid TT play, both in simulations and experimentally. The learned policy achieves high success rates at competitive ball speeds across a wide serving range, covering both width (side-to-side) and length (short-long) variations.\n\nThe remainder of this paper is organized as follows. Section II formulates the problem, followed by the RL design details in Section III. Section IV presents the setup for policy training and experimental validation, while the results are presented and discussed in Section V. Section VI concludes the paper and outlines potential future work.\n\n1. End-to-end, unified RL formulation that jointly coordinates locomotion and striking directly based on ball-position observations and robot proprioception without modular or hierarchical planning.\n\n2. Augmentation of the RL pipeline with a ball-hitting position predictor that enables the RL policy to act proactively for returning the ball, as well as with dense, immediate reward design based on physics-based trajectory prediction. Such an augmentation accelerates policy learning toward effective TT strikes and footwork.\n\n3. To the best of our knowledge, this is among the first unified RL frameworks that generate whole-body motions and demonstrate successful humanoid TT play, both in simulations and experimentally. The learned policy achieves high success rates at competitive ball speeds across a wide serving range, covering both width (side-to-side) and length (short-long) variations.",
            "llm_summary": "【论文的motivation是什么】  \n1. Humanoid robots struggle with rapid perception and coordinated motion in dynamic environments like table tennis.  \n2. Existing methods often rely on restrictive assumptions that limit adaptability and performance in real-world scenarios.  \n3. There is a need for an effective learning-based approach that can handle high-dimensional action spaces in humanoid robots.  \n\n【提出了什么创新的方法】  \n本论文提出了一种端到端的统一强化学习框架，直接根据球的位置观察和机器人本体感知来协调运动和击球，而无需模块化或分层规划。通过引入一个球击打位置预测器，增强了RL管道，使得RL策略能够主动返回球，并基于物理轨迹预测设计了密集的即时奖励。这种增强加速了策略学习，提升了乒乓球击打和步伐的有效性。实验结果表明，该策略在多种发球范围内实现了高成功率，展示了在仿真和实际应用中的有效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Generating Stable Placements via Physics-guided Diffusion Models",
            "authors": "Philippe Nadeau,Miguel Rogel,Ivan Bilić,Ivan Petrović,Jonathan Kelly",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "Submitted to the IEEE International Conference on Robotics and Automation 2026, Vienna, Austria, June 1-5, 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.21664",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21664",
            "arxiv_html_link": "https://arxiv.org/html/2509.21664v1",
            "abstract": "Stably placing an object in a multi-object scene is a fundamental challenge in robotic manipulation, as placements must be penetration-free, establish precise surface contact, and result in a force equilibrium.\nTo assess stability, existing methods rely on running a simulation engine or resort to heuristic, appearance-based assessments.\nIn contrast, our approach integrates stability directly into the sampling process of a diffusion model.\nTo this end, we query an offline sampling-based planner to gather multi-modal placement labels and train a diffusion model to generate stable placements.\nThe diffusion model is conditioned on scene and object point clouds, and serves as a geometry-aware prior.\nWe leverage the compositional nature of score-based generative models to combine this learned prior with a stability-aware loss, thereby increasing the likelihood of sampling from regions of high stability.\nImportantly, this strategy requires no additional re-training or fine-tuning, and can be directly applied to off-the-shelf models.\nWe evaluate our method on four benchmark scenes where stability can be accurately computed. Our physics-guided models achieve placements that are 56% more robust to forceful perturbations while reducing runtime by 47% compared to a state-of-the-art geometric method.",
            "introduction": "The task of planning to stably place an object among other objects, henceforth referred to as stable placement planning, is a core element of several higher level tasks, like construction [1, 2], scene rearrangement [3], and dense packing [4].\nThe challenge lies in satisfying strict geometric and physical constraints that characterize the problem.\nA placement pose must establish precise surface contact with objects in the scene while avoiding penetration with any other object. At the same time, it must result in a force equilibrium to prevent toppling.\nIdeally, a placement planner would sample directly from the space of valid poses; however, defining this subset is complex.\nOnly a very small proportion of all poses in the workspace result in valid placements, making random sampling or searching approaches highly inefficient [5].\n\nWe argue that a principled approach to determine stable placements requires: (i) a geometry-aware placement algorithm, (ii) reasoning about scene equilibrium, and (iii) a stability verifier to assess the outcome. To reason about scene equilibrium, existing methods either rely on executing a simulation engine [6, 7], or assess placement quality solely based on appearance [8, 9]. The former is time-consuming, while the latter requires making assumptions about object inertial parameters.\n\nWe propose a simulation-free approach that addresses stability in a principled manner, by merging requirements (i) and (ii): enhancing the placement algorithm with a physics-based feature, obtained from [10], termed robustness. Robustness describes the maximum force that can be applied at a point in a scene before any object moves.\n\nDue to the lack of publicly available datasets that capture the multi-object stability information necessary for our approach, we employ a state-of-the-art sampling-based planner [5] to act as an expert. We construct four scenes, and use the planner offline to generate multi-modal stable placement examples and physics-based robustness features. Our scenes comprise flat placement surfaces, similar to recent works which consider stability [6, 7, 8, 9]. While such scenes do not capture the full complexity of the real world, they allow us to precisely assess the quality of stability reasoning and a certain degree of generalization to unseen data.\n\nDiffusion models are capable of capturing complex distributions and excel at conditional and controllable generation [11, 12].\nTherefore, we introduce a physics-guided score-based generative model for stable placement planning. The compositional nature of score-based generative models allows us to combine multiple likelihood terms—such as the prior and differentiable loss—into a single sampling distribution.\nOur model is trained to generate stable placements conditioned only on scene and object geometry, serving as a prior with no explicit physics information. During sampling, we introduce a stability-aware diffusion guidance to inform the learned prior about scene statics, steering the sampling process toward regions of high stability, as shown in Fig. 1.\nOur guidance is easy to compute and does not affect model training, making it applicable to off-the-shelf models. Compared to our geometry-based learned prior and [5], we observe our guidance can increase the placement validity rate up to 40%40\\% on unseen scenes, and overall scene robustness after placing by 56%56\\%.\nWe also discuss architectural details that we identify as crucial for enabling the model to generalize, independent of guidance.\nTo the best of our knowledge, our work is the first to incorporate a physics-informed prior into a generative stable placement model.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的稳定放置方法依赖于模拟引擎或启发式评估，效率低下。  \n2. 需要一个无模拟的、有效的稳定放置规划方法，以满足几何和物理约束。  \n3. 缺乏公开数据集来捕捉多物体稳定性信息，限制了方法的发展。  \n\n【提出了什么创新的方法】  \n本研究提出了一种物理引导的扩散模型，用于稳定放置规划。通过查询离线采样规划器生成多模态稳定放置示例，并结合几何和稳定性信息，训练扩散模型以生成稳定放置。该模型在采样过程中引入稳定性引导，提升了采样的有效性和稳定性。实验结果表明，所提出的方法在四个基准场景中实现了56%的抗扰动能力提升和47%的运行时间减少，展示了其在实际应用中的有效性和高效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Real-Time Indoor Object SLAM with LLM-Enhanced Priors",
            "authors": "Yang Jiao,Yiding Qiu,Henrik I. Christensen",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21602",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21602",
            "arxiv_html_link": "https://arxiv.org/html/2509.21602v1",
            "abstract": "Object-level Simultaneous Localization and Mapping (SLAM), which incorporates semantic information for high-level scene understanding, faces challenges of under-constrained optimization due to sparse observations. Prior work has introduced additional constraints using commonsense knowledge, but obtaining such priors has traditionally been labor-intensive and lacks generalizability across diverse object categories. We address this limitation by leveraging large language models (LLMs) to provide commonsense knowledge of object geometric attributes, specifically size and orientation, as prior factors in a graph-based SLAM framework.\nThese priors are particularly beneficial during the initial phase when object observations are limited.\nWe implement a complete pipeline integrating these priors, achieving robust data association on sparse object-level features and enabling real-time object SLAM. Our system, evaluated on the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8% over the latest baseline. Additionally, we present real-world experiments in the supplementary video, demonstrating its real-time performance.",
            "introduction": "Object Simultaneous Localization and Mapping (SLAM) builds environment maps by identifying and localizing objects, and using this information to infer the robot’s position. Unlike traditional feature-based SLAM, object-level representations are sparse, focusing on semantic object data. Comparing to semantic segmentation on dense representations, such sparsity improves computational efficiency and reduces storage requirements. Meanwhile, high-level semantic information makes object SLAM useful for downstream tasks, such as 3D semantic scene reconstruction, object-goal navigation, and object retrieval [1].\n\nWhen navigating an unfamiliar environment to find specific objects, the agent must efficiently build a map while moving quickly to cover large areas, often resulting in limited observations of objects from different viewpoints [2]. SLAM with sparse observations commonly occurred in real-world applications, such as when the camera frame rate is low relative to the agent’s motion speed or when loop closure is unavailable. However, due to the inherent sparsity of object-level features, fewer-constraint object SLAM can be extremely challenging. Such sparsity affects the quality of initial mapping and might hinder subsequent optimization and data association.\n\nTo address these issues, we introduce additional constraints from commonsense knowledge. Objects typically exhibit common attributes, such as size, shape, and orientation, which provide valuable priors for mapping systems. These attributes are essential during the initial observation phase, where limited 2D features and uncertain pose estimations bring challenges. By incorporating these priors, the solution space for object estimation can be effectively constrained, enhancing mapping accuracy in sparse observation scenarios. However, acquiring comprehensive object priors has traditionally been labor-intensive, requiring substantial human annotation across diverse object categories [3]. Furthermore, commonsense knowledge about objects can vary by context, leading to potential mismatches between expected and actual observations.\n\nWe leveraged large language models (LLMs) to provide prior information, as commonsense inference is LLM’s strong suit [4].\nWe propose a LLM-enhanced prior-assisted object SLAM framework to address the challenge of under-constrained optimization in object SLAM.\nOur approach models the prior knowledge from LLM as factors using a factor graph [5]. Specifically, we focus on the geometric intrinsics of size and orientation for various object categories.\n\nOur method integrates prior information from LLM and a combined data association method to achieve a full online object SLAM (Fig. 1). We validated it on real-world datasets.\nThe key contributions of this paper are outlined below:\n\nExploration of using commonsense priors, particularly object size and orientation, and encoding them as prior factors for a graph-based SLAM system;\n\nA complete pipeline from embedding LLM-enhanced priors, extracting object-level features, performing data association, to solving incremental optimization;\n\nEvaluation using two real-world datasets: TUM RGB-D and 3RScan, and test of our SLAM method in real-time.\n\n1. Exploration of using commonsense priors, particularly object size and orientation, and encoding them as prior factors for a graph-based SLAM system;\n\n2. A complete pipeline from embedding LLM-enhanced priors, extracting object-level features, performing data association, to solving incremental optimization;\n\n3. Evaluation using two real-world datasets: TUM RGB-D and 3RScan, and test of our SLAM method in real-time.",
            "llm_summary": "【论文的motivation是什么】  \n1. Object-level SLAM faces challenges of under-constrained optimization due to sparse observations.  \n2. Traditional methods for acquiring commonsense knowledge are labor-intensive and lack generalizability across object categories.  \n3. Limited observations during initial mapping hinder accurate data association and optimization.  \n\n【提出了什么创新的方法】  \n本文提出了一种基于大语言模型（LLM）的先验辅助对象SLAM框架，通过将对象的几何属性（如大小和方向）作为图优化中的先验因素，来解决对象SLAM中的欠约束优化问题。该方法集成了LLM增强的先验信息和数据关联技术，形成了一个完整的在线对象SLAM流程。经过在TUM RGB-D和3RScan数据集上的评估，该系统在稀疏对象特征的映射精度上提高了36.8%。此外，实地实验展示了其实时性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control",
            "authors": "HaoZhe Xu,Cheng Cheng,HongRui Sang,Zhipeng Wang,Qiyong He,Xiuxian Li,Bin He",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21571",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21571",
            "arxiv_html_link": "https://arxiv.org/html/2509.21571v1",
            "abstract": "Autonomous docking between Unmanned Aerial Vehicles (UAVs) and ground robots is essential for heterogeneous systems, yet most existing approaches target wheeled platforms whose limited mobility constrains exploration in complex terrains. Quadruped robots offer superior adaptability but undergo frequent posture variations, making it difficult to provide a stable landing surface for UAVs. To address these challenges, we propose an autonomous UAV–quadruped docking framework for GPS-denied environments. On the quadruped side, a Hybrid Internal Model with Horizontal Alignment (HIM-HA), learned via deep reinforcement learning, actively stabilizes the torso to provide a level platform. On the UAV side, a three-phase strategy is adopted, consisting of long-range acquisition with a median-filtered YOLOv8 detector, close-range tracking with a constraint-aware controller that integrates a Nonsingular Fast Terminal Sliding Mode Controller (NFTSMC) and a logarithmic Barrier Function (BF) to guarantee finite-time error convergence under field-of-view (FOV) constraints, and terminal descent guided by a Safety Period (SP) mechanism that jointly verifies tracking accuracy and platform stability. The proposed framework is validated in both simulation and real-world scenarios, successfully achieving docking on outdoor staircases higher than 17 cm and rough slopes steeper than 30∘30^{\\circ}. Supplementary materials and videos are available at: https://uav-quadruped-docking.github.io.",
            "introduction": "Heterogeneous cooperative systems that integrate Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) can expand operational scope and improve efficiency compared to single-domain platforms [1]. Autonomous docking is a key capability for many UAV–UGV collaborative tasks, yet most existing schemes focus on wheeled UGVs, whose mobility is restricted to flat terrain, limiting exploration in complex environments. Moreover, dynamic docking requires UAVs to achieve precise localization and safe landing on moving platforms, imposing high demands on sensor fusion and robust control[2].\n\nQuadruped robots, with their legged morphology, surpass wheeled and tracked UGVs in unstructured terrains and enable UAV collaboration in challenging environments such as mountains or tunnels. Recent advances in deep reinforcement learning (DRL) provide a new paradigm for quadruped locomotion [3, 4, 5]. Unlike model-based controllers (e.g., Model Predictive Control, MPC [6]), DRL maps states and observations directly to joint actions, allowing adaptation to unseen terrains and sensor noise with strong generalization. However, prior studies mainly optimize locomotion of single quadrupeds, neglecting collaborative tasks that require posture and velocity constraints to support UAV docking.\n\nThis work addresses UAV–quadruped docking in complex three-dimensional terrains, where quadrupeds undergo large posture variations. On stairs higher than 15 cm or slopes steeper than 20∘20^{\\circ}, pitch angles often exceed 20∘20^{\\circ}, producing an unstable landing surface for UAVs. Such instability also degrades vision-based tracking (e.g., AprilTag), making it necessary for UAVs to fuse onboard sensing with quadruped state feedback for robust tracking and precise localization.\n\nAutonomous docking Autonomous docking techniques can be broadly categorized into two types: infrastructure-based methods and on-board sensor–based methods. Infrastructure-assisted schemes typically rely on external positioning systems such as GPS, BeiDou, or motion capture systems (MCS) [7, 8]. In [9], LiDAR, IMU, and GPS were tightly fused to improve environment perception and landing point selection for powered parachute UAVs. In [10], the effectiveness of a landing algorithm combining GPS and ArUco marker detection was verified in the Webots simulation environment. However, these methods depend heavily on infrastructure, incur high deployment costs, and lack adaptability. In cluttered environments such as vegetation, water bodies, or indoor spaces, GPS signals are often degraded or lost [11], making such approaches unreliable.\n\nOn-board sensor–based docking methods [12, 13] employ vision, LiDAR, and IMU to operate in GPS-denied settings. In [14], a lightweight vision-based controller enabled quadrotors to land autonomously on unknown moving platforms. In [15], semantic and depth information from cameras and LiDAR were fused to autonomously search and select safe landing sites. These methods, however, are sensitive to sensor quality, perform poorly in low-light or adverse conditions, and impose significant computational overhead.\n\nMost existing docking methods are designed for wheeled UGVs, which are limited to flat or structured terrain. In rugged environments such as rubble fields, steep slopes, or stairs, wheeled platforms struggle to operate effectively, and frequent posture variations of the landing surface further increase the risk of field-of-view (FOV) loss during docking.\n\nLearning-based quadruped locomotion Quadruped robots surpass wheeled UGVs in unstructured terrains through flexible foothold planning and are increasingly employed in exploration tasks. Reinforcement learning (RL) has emerged as a powerful paradigm for quadruped locomotion [3, 5, 16], with policies trained entirely in simulation and transferred to hardware via zero-shot sim-to-real. In [17], the authors introduced an adaptation module within a teacher–student framework to achieve highly robust locomotion. In [18], behavior parameters such as body height command, body pitch command, and footswing height command were incorporated into RL observation and reward functions, resulting in a versatile low-level controller capable of structured behaviors. However, these approaches mainly address mildly uneven terrains and lack active posture control for complex environments.\n\nRecent RL-based strategies have demonstrated superior adaptability to traditional controllers in unstructured 3D terrains [19, 20, 21, 22]. In [19], a context-aided estimator was proposed to jointly infer body state and environmental context, showing strong robustness across complex terrains. In [20], the authors combined internal model control (IMC) [23] with reinforcement learning and proposed a hybrid internal model (HIM) to estimate robot velocity and simulate system response, enabling stable and agile locomotion over stairs and slopes while mitigating noise and sampling inefficiency\n\nNevertheless, most RL frameworks treat body posture alignment as a passive byproduct of terrain adaptation. In UAV–quadruped cooperative tasks such as autonomous docking, quadrupeds must not only maintain robust locomotion but also actively adjust posture to provide a level dorsal surface for UAV landing. Current methods lack such task-oriented posture control, limiting the potential of quadrupeds in collaborative aerial–ground missions.\n\nTo address the landing difficulties caused by the dynamic torso posture of the quadruped robot, we model the UAV-quadruped robot docking task in complex terrain environments as a heterogeneous robot cooperative planning problem. Optimization methods are designed separately for the UAV and the quadruped robot to achieve safe and precise docking.\nOur contributions are as follows:\n\nWe propose an autonomous UAV–quadruped docking framework that integrates multi-sensor fusion with an active posture alignment mechanism to enhance reliability in GPS-denied environments.\n\nWe propose an autonomous UAV–quadruped docking framework that integrates multi-sensor fusion with an active posture alignment mechanism to enhance reliability in GPS-denied environments.\n\nOn the quadruped side, we extend the Hybrid Internal Model (HIM) and introduce HIM with Horizontal Alignment (HIM-HA), a reinforcement learning–based locomotion strategy that enables quadrupeds to actively stabilize their torso and provide a level landing platform in complex 3D terrains.\n\nOn the UAV side, we develop a constraint-aware controller that integrates a Nonsingular Fast Terminal Sliding Mode Controller (NFTSMC) with a logarithmic Barrier Function (BF) to guarantee finite-time error convergence under field-of-view (FOV) constraints, and introduce a Safety Period (SP) strategy to ensure robust and reliable terminal descent.\n\nWe validate the proposed framework in both simulation and real-world experiments across challenging environments, including stairs and steep slopes. To the best of our knowledge, this is the first study to demonstrate successful UAV–quadruped docking in unstructured terrains.\n\n1. We propose an autonomous UAV–quadruped docking framework that integrates multi-sensor fusion with an active posture alignment mechanism to enhance reliability in GPS-denied environments.\n\n2. On the quadruped side, we extend the Hybrid Internal Model (HIM) and introduce HIM with Horizontal Alignment (HIM-HA), a reinforcement learning–based locomotion strategy that enables quadrupeds to actively stabilize their torso and provide a level landing platform in complex 3D terrains.\n\n3. On the UAV side, we develop a constraint-aware controller that integrates a Nonsingular Fast Terminal Sliding Mode Controller (NFTSMC) with a logarithmic Barrier Function (BF) to guarantee finite-time error convergence under field-of-view (FOV) constraints, and introduce a Safety Period (SP) strategy to ensure robust and reliable terminal descent.\n\n4. We validate the proposed framework in both simulation and real-world experiments across challenging environments, including stairs and steep slopes. To the best of our knowledge, this is the first study to demonstrate successful UAV–quadruped docking in unstructured terrains.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的无人机-地面机器人对接方法主要针对轮式平台，限制了在复杂地形中的探索能力。  \n2. 四足机器人在不规则地形中具有更好的适应性，但其姿态变化频繁，导致无人机难以提供稳定的着陆表面。  \n3. 在GPS信号缺失的环境中，现有的对接技术缺乏可靠性和适应性。  \n\n【提出了什么创新的方法】  \n本研究提出了一种自主无人机-四足机器人对接框架，结合多传感器融合与主动姿态对齐机制，以增强在GPS-denied环境中的可靠性。四足机器人方面，扩展了混合内部模型（HIM），引入了HIM-HA策略，通过深度强化学习主动稳定其躯干，提供平坦的着陆平台。无人机方面，开发了一种约束感知控制器，结合了非奇异快速终端滑模控制器（NFTSMC）和对数障碍函数（BF），确保在视野约束下的有限时间误差收敛，并引入安全期（SP）策略以确保可靠的终端下降。该框架在模拟和真实场景中得到了验证，成功实现了在超过17厘米高的户外楼梯和超过30°的陡坡上的对接，展示了在不规则地形中无人机-四足机器人对接的可行性。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "PL-VIWO2: A Lightweight, Fast and Robust Visual-Inertial-Wheel Odometry Using Points and Lines",
            "authors": "Zhixin Zhang,Liang Zhao,Pawel Ladosz",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21563",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21563",
            "arxiv_html_link": "https://arxiv.org/html/2509.21563v1",
            "abstract": "Vision-based odometry has been widely adopted in autonomous driving owing to its low cost and lightweight setup; however, its performance often degrades in complex outdoor urban environments. To address these challenges, we propose PL-VIWO2, a filter-based visual–inertial–wheel odometry system that integrates an IMU, wheel encoder, and camera (supporting both monocular and stereo) for long-term robust state estimation. The main contributions are: (i) a novel line feature processing framework that exploits the geometric relationship between 2D feature points and lines, enabling fast and robust line tracking and triangulation while ensuring real-time performance; (ii) an SE(2)-constrained SE(3) wheel pre-integration method that leverages the planar motion characteristics of ground vehicles for accurate wheel updates; and (iii) an efficient motion consistency check (MCC) that filters out dynamic features by jointly using IMU and wheel measurements. Extensive experiments on Monte Carlo simulations and public autonomous driving datasets demonstrate that PL-VIWO2 outperforms state-of-the-art methods in terms of accuracy, efficiency, and robustness.",
            "introduction": "The complex nature of urban environments makes it difficult to ensure long-term stable state estimation in autonomous driving, which continues to attract considerable research attentions. Visual-inertial Navigation Systems (VINS) or Visual-inertial-Odometry (VIO), which integrate cameras and inertial measurement units (IMUs), have been widely regarded as a promising solution for state estimation in unmanned systems due to their low cost and complementary sensing characteristics. Over the past decade, numerous representative VIO have been proposed, including ORB-SLAM3 [1], VINS-Mono [2], and OpenVINS [3].\n\nHowever, the IMU often fails to provide accurate measurements under high-speed motion, such as highway driving, leading to inaccurate estimation [4]. Furthermore, VINS may exhibit additional unobservable degrees of freedom during specific motion patterns, further degrading its performance in autonomous driving scenarios [5]. These challenges can be effectively addressed by incorporating wheel encoder, a standard sensor on ground vehicles, which provides additional observation information [6, 7]. Since the wheel measurements only provide 2D motion information, while the visual-inertial-wheel odometry (VIWO) system estimates a full 3D state, the planar motion characteristics of ground vehicles can be exploited to bridge this gap. Specifically, the 2D wheel measurements are extended into an SE(2) constricted SE(3) state space under planar motion assumption, thereby improving estimation accuracy.\n\nFor vision-based odometry, point features are commonly tracked across different camera poses and constrain the system state by using descriptors [8] or optical flow [9] methods. However, acquiring sufficiently reliable features in urban environments remains challenging due to dynamic objects, poor lighting conditions, and texture-less surfaces. To address this, incorporating additional geometric features, such as lines, has been proven to be effective in previous works [10, 11, 12]. Despite its potential, the application of line features in autonomous driving scenarios faces several limitations that hinder their practical deployment. First, processing line features introduces additional computational overhead, which may compromise the real-time performance. Secondly, conventional line tracking methods typically rely on descriptors, which often perform poorly in complex outdoor environments. For instance, high-speed linear motion or false line detections can lead to unreliable feature correspondences between views. Thirdly, due to the dominant planar motion of ground vehicles, classical triangulation methods may suffer from degenerate motion, resulting in inaccurate triangulation results and ultimately degrading the state estimation accuracy [13].\n\nAnother challenge for state estimation in autonomous driving environments is dynamic objects, such as moving vehicles and pedestrians. The inclusion of dynamic feature points can significantly degrade estimation accuracy. To mitigate this issue, some approaches utilize deep learning techniques to perform semantic segmentation and filter out dynamic points [14, 15]. But such methods are often computationally demanding and may not guarantee real-time performance on resource-limited platforms. Therefore, this paper leverages a lightweight and efficient Motion Consistency Check (MCC) by using the measurement information of IMU and wheel to reject dynamic features. Specifically, the motion of each feature point within a sliding window is compared against the predicted motion derived from inertial and wheel data. Features exhibiting inconsistent motion patterns are classified as dynamic or outliers and further excluded from state estimation.\n\nTo address the aforementioned challenges, we propose a novel visual-inertial-wheel odometry (VIWO) system that integrates both point and line features, named PL-VIWO2, designed for long-term robust and accurate state estimation of autonomous driving in complex outdoor environments. The main contributions of this work are:\n\nA lightweight, fast, and robust VIWO system leveraging both point and line features is proposed for handling complex autonomous driving scenarios, with support for both monocular and stereo configurations.\n\nTwo novel optical-flow-based line feature tracking methods are developed and combined to enable efficient and reliable line tracking in complex outdoor environments.\n\nA robust and accurate 3D line triangulation pipeline exploiting the geometric relationship between point and line features is implemented, including both initialization and optimization-based refinement.\n\nAn SE​(2)\\mathrm{SE}(2)-constrained SE​(3)\\mathrm{SE}(3) wheel pre-integration method is designed based on the planar motion characteristics of vehicles for wheel updates.\n\nA motion consistency check (MCC) module that eliminates the influence of dynamic features by leveraging IMU and wheel measurements.\n\nValidation through both Monte Carlo simulations and real-world experiments, demonstrating robustness, efficiency, and accuracy.\n\nThe source code of the system will be released as open source upon acceptance of the paper.\n\nCompared with our previous publication PL-VIWO [16], the new contributions of PL-VIWO2 are:\n\nPL-VIWO2 is extended to support both monocular and stereo configurations, ensuring compatibility with various hardware setups, whereas PL-VIWO supports only the monocular configuration.\n\nA second-stage optical-flow-based line matching method for tracking line features that cannot be directly tracked using point tracking results.\n\nAn optimization refinement strategy is introduced after 3D line initialization to further improve the accuracy of line triangulation results.\n\nExtensive evaluations are conducted on both simulations and real-world datasets, demonstrating that PL-VIWO2 achieves superior robustness and accuracy compared with PL-VIWO and other SOTA methods.\n\nThe rest of this paper is organized as follows. Section III reviews related work. Section III introduces the system overview and the state estimation method that integrates measurements from three sensors. Section IV details the proposed novel visual processing pipeline, including the point-based line feature tracking, triangulation, stereo configuration and motion consistency check. Section V presents Monte Carlo simulation results and evaluates the proposed line triangulation methods. Section VI reports real-world experimental results and performance evaluation. Finally, Section VII concludes the paper.\n\n1. A lightweight, fast, and robust VIWO system leveraging both point and line features is proposed for handling complex autonomous driving scenarios, with support for both monocular and stereo configurations.\n\n2. Two novel optical-flow-based line feature tracking methods are developed and combined to enable efficient and reliable line tracking in complex outdoor environments.\n\n3. A robust and accurate 3D line triangulation pipeline exploiting the geometric relationship between point and line features is implemented, including both initialization and optimization-based refinement.\n\n4. An SE​(2)\\mathrm{SE}(2)-constrained SE​(3)\\mathrm{SE}(3) wheel pre-integration method is designed based on the planar motion characteristics of vehicles for wheel updates.\n\n5. A motion consistency check (MCC) module that eliminates the influence of dynamic features by leveraging IMU and wheel measurements.\n\n6. Validation through both Monte Carlo simulations and real-world experiments, demonstrating robustness, efficiency, and accuracy.\n\n7. The source code of the system will be released as open source upon acceptance of the paper.\n\n1. PL-VIWO2 is extended to support both monocular and stereo configurations, ensuring compatibility with various hardware setups, whereas PL-VIWO supports only the monocular configuration.\n\n2. A second-stage optical-flow-based line matching method for tracking line features that cannot be directly tracked using point tracking results.\n\n3. An optimization refinement strategy is introduced after 3D line initialization to further improve the accuracy of line triangulation results.\n\n4. Extensive evaluations are conducted on both simulations and real-world datasets, demonstrating that PL-VIWO2 achieves superior robustness and accuracy compared with PL-VIWO and other SOTA methods.",
            "llm_summary": "【论文的motivation是什么】  \n1. 在复杂的城市环境中，视觉惯性导航系统的长期稳定状态估计面临挑战。  \n2. 现有的视觉惯性里程计在高速度运动下IMU的测量不准确，导致估计误差。  \n3. 动态物体的存在会显著降低状态估计的准确性。  \n\n【提出了什么创新的方法】  \n提出了一种轻量级、快速且稳健的视觉-惯性-轮子里程计系统PL-VIWO2，集成了IMU、轮子编码器和相机，支持单目和立体配置。该系统采用了一种新颖的线特征处理框架，结合了光流法进行高效的线跟踪和三角测量，同时实现了实时性能。此外，设计了SE(2)约束的SE(3)轮子预积分方法，以提高轮子更新的准确性。通过运动一致性检查（MCC）模块，有效过滤动态特征，确保了状态估计的鲁棒性和准确性。实验结果表明，PL-VIWO2在准确性、效率和鲁棒性方面超过了现有的最先进方法。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation",
            "authors": "Jinbang Huang,Zhiyuan Li,Zhanguang Zhang,Xingyue Quan,Jianye Hao,Yingxue Zhang",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21543",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21543",
            "arxiv_html_link": "https://arxiv.org/html/2509.21543v1",
            "abstract": "Large Language Models (LLMs) have recently shown strong potential in robotic task planning, particularly through automatic planning domain generation that integrates symbolic search. Prior approaches, however, have largely treated these domains as search utilities, with limited attention to their potential as scalable sources of reasoning data. At the same time, progress in reasoning LLMs has been driven by chain-of-thought (CoT) supervision, whose application in robotics remains dependent on costly, human-curated datasets.\nWe propose Plan2Evolve, an LLM self-evolving framework in which the base model generates planning domains that serve as engines for producing symbolic problem–plan pairs as reasoning traces. These pairs are then transformed into extended CoT trajectories by the same model through natural-language explanations, thereby explicitly aligning symbolic planning structures with natural language reasoning. The resulting data extend beyond the model’s intrinsic planning capacity, enabling model fine-tuning that yields a planning-enhanced LLM with improved planning success, stronger cross-task generalization, and reduced inference costs.",
            "introduction": "Large Language Models (LLMs) have shown strong potential in robotic task planning due to their reasoning capabilities and cross-task generalization (Huang et al., 2023, 2022; Wang et al., 2024; Li et al., 2023; Zhao et al., 2024). However, like other learning-based methods, LLMs often suffer from stochastic outputs and error accumulation over long-horizon tasks, leading to failure in verifying action feasibility, state tracking, and dependency verification. To resolve these issues, researchers have combined LLMs with symbolic search-based algorithms for better long-horizon planning performance (Meng et al., 2024; Hu et al., 2023; Liu et al., 2023a). More recently, LLMs have been used to automatically infer planning domains, which, once combined with tree search, yield task-specific planners (Oswald et al., 2025; Byrnes et al., 2024; Guan et al., 2023a; Han et al., 2024b; Huang et al., 2025b). While effective, the domain-inference approaches mainly treat planning domains as mere search utilities, overlooking their potential to serve as generative engines of scalable, verifiable training data (Dalal et al., 2023; Khodeir et al., 2023).\n\nIn parallel, reasoning-focused LLMs have advanced significantly through chain-of-thought (CoT) training, where models learn to decompose complex problems into multi-step reasoning traces (Wei et al., 2022; Cobbe et al., 2021; Zelikman et al., 2022), yet CoT training usually relies on large, manually curated datasets, which are costly to construct, especially for robotics. A promising alternative is to leverage symbolic structures for scalable data production (Dalal et al., 2023). However, while prior research has extensively studied language-to-symbol transformation for robotic planning (Pan et al., 2023a; Han et al., 2024a; Tafjord et al., 2021), the reverse process, symbol-to-language transformation, remains underexplored for model trainingWang et al. (2025a). This gap raises an important question: can LLMs bootstrap their own planning ability by aligning symbolic plans with natural-language reasoning to internalize planning ability?\n\nWe propose Plan2Evolve, a self-evolving framework that reinterprets LLM-generated planning domains as knowledge sources for data generation. Given diverse tasks, the base model first self-generates planning domains that produce problem–plan pairs. These symbolic plans are then explained by the same model in natural language, resulting in long CoT trajectories aligned with formal planning semantics. Together, these outputs form a unified dataset that extends beyond the intrinsic planning capability of the base model, enabling supervised fine-tuning (SFT) to internalize and enhance its planning performance. Our contributions are:\n\nPlan2Evolve Framework: We introduce Plan2Evolve, a novel framework that treats LLM-generated PDDL domains as evolving knowledge sources, whose compositionality enables systematic task generation and the automatic creation of scalable planning supervision.\n\nPlan2Evolve Framework: We introduce Plan2Evolve, a novel framework that treats LLM-generated PDDL domains as evolving knowledge sources, whose compositionality enables systematic task generation and the automatic creation of scalable planning supervision.\n\nSelf-evolving Data Generation: Plan2Evolve enables the base model to generate validated long-horizon robotic planning problem–plan pairs that go beyond its intrinsic planning capacity, thereby eliminating the need for human curation of training data.\n\nSymbolic–Language Alignment: We introduce an automatic self-alignment procedure that translates symbolic PDDL plans and states into natural-language CoT using the base model, and empirically demonstrate that symbolic–language alignment is essential for training LLM to plan, with higher-quality alignment yielding substantial performance gains.\n\nEmpirical Gains: By SFT, Plan2Evolve produces a planning-enhanced LLM that achieves robust planning performance, stronger cross-task generalization, and reduced inference token costs. These results indicate that Plan2Evolve can improve both the efficiency and planning capacity of LLM.\n\n1. Plan2Evolve Framework: We introduce Plan2Evolve, a novel framework that treats LLM-generated PDDL domains as evolving knowledge sources, whose compositionality enables systematic task generation and the automatic creation of scalable planning supervision.\n\n2. Self-evolving Data Generation: Plan2Evolve enables the base model to generate validated long-horizon robotic planning problem–plan pairs that go beyond its intrinsic planning capacity, thereby eliminating the need for human curation of training data.\n\n3. Symbolic–Language Alignment: We introduce an automatic self-alignment procedure that translates symbolic PDDL plans and states into natural-language CoT using the base model, and empirically demonstrate that symbolic–language alignment is essential for training LLM to plan, with higher-quality alignment yielding substantial performance gains.\n\n4. Empirical Gains: By SFT, Plan2Evolve produces a planning-enhanced LLM that achieves robust planning performance, stronger cross-task generalization, and reduced inference token costs. These results indicate that Plan2Evolve can improve both the efficiency and planning capacity of LLM.",
            "llm_summary": "【论文的motivation是什么】  \n1. LLM在机器人任务规划中的潜力尚未充分发挥，尤其是在生成可扩展的推理数据方面。  \n2. 现有的LLM训练依赖于昂贵的人为策划数据集，限制了其在机器人领域的应用。  \n3. 需要一种方法将符号规划与自然语言推理对齐，以增强LLM的规划能力。  \n\n【提出了什么创新的方法】  \n提出了Plan2Evolve，一个自我演化框架，利用LLM生成的规划领域作为知识源，自动生成符号问题-计划对。该模型首先自生成规划领域，然后将这些符号计划转化为自然语言的长链推理轨迹，从而将符号规划结构与自然语言推理明确对齐。通过这种方式，生成的数据超出了模型的内在规划能力，支持了模型的监督微调，显著提升了规划成功率、跨任务泛化能力，并降低了推理成本。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "DroneFL: Federated Learning for Multi-UAV Visual Target Tracking",
            "authors": "Xiaofan Yu,Yuwei Wu,Katherine Mao,Ye Tian,Vijay Kumar,Tajana Rosing",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.21523",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21523",
            "arxiv_html_link": "https://arxiv.org/html/2509.21523v1",
            "abstract": "Multi-robot target tracking is a fundamental problem that requires coordinated monitoring of dynamic entities in applications such as precision agriculture, environmental monitoring, disaster response, and security surveillance.\nWhile Federated Learning (FL) has the potential to enhance learning across multiple robots without centralized data aggregation, its use in multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely underexplored.\nKey challenges include limited onboard computational resources, significant data heterogeneity in FL due to varying targets and the fields of view, and the need for tight coupling between trajectory prediction and multi-robot planning.\nIn this paper, we introduce DroneFL, the first federated learning framework specifically designed for efficient multi-UAV target tracking.\nWe design a lightweight local model to predict target trajectories from sensor inputs, using a frozen YOLO backbone and a shallow transformer for efficient onboard training.\nThe updated models are periodically aggregated in the cloud for global knowledge sharing.\nTo alleviate the data heterogeneity that hinders FL convergence, DroneFL introduces a position-invariant model architecture with altitude-based adaptive instance normalization.\nFinally, we fuse predictions from multiple UAVs in the cloud and generate optimal trajectories that balance target prediction accuracy and overall tracking performance.\nOur results show that DroneFL reduces prediction error by 6%-83% and tracking distance by 0.4%-4.6% compared to a distributed non-FL framework. In terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has on average just 1.56 KBps data rate to the cloud.",
            "introduction": "Multi-robot systems are increasingly deployed in real-world applications, with aerial platforms playing a distinct role in large-scale environments such as industry, agriculture, and environmental monitoring [1, 2].\nTeams of UAVs provide wide-area coverage, maintaining visibility even in cluttered, GPS-denied settings like orchards, while adapting quickly to changing operational demands.\nIn agriculture, for example, UAVs enable large-scale monitoring by tracking ground vehicles during operations such as harvesting.\nUnlike static sensors, these targets have dynamic motions influenced by heterogeneous machinery, variable field conditions, and operator-dependent behaviors.\nTherefore, accurate monitoring requires coordinated efforts across multiple UAVs.\n\nIn this paper, we consider multi-robot target tracking as a representative scenario (Fig. 1).\nMost existing research [3, 4, 5, 6, 7] relies on low-dimensional sensors and fixed models or heuristics, which fail to handle high-dimensional inputs such as images and video streams, or to adapt to the variability of real-world environments.\nIn addition, UAVs face strict Size, Weight, and Power (SWaP) constraints that limit both onboard computation and communication. These challenges motivate the need for a multi-robot framework that processes rich visual inputs, delivers high performance (e.g., prediction accuracy and tracking quality) across varying environments, while minimizing resource consumption.\n\nFederated learning (FL) [8] provides a promising solution by allowing distributed UAVs to collaboratively learn a vision-based neural network (NN) model.\nEach FL round begins with a central server (cloud) distributing a global model to the robots. The robots then update their NNs using locally collected sensor data to better predict target trajectories. Finally, the updated models are sent back to the cloud for aggregation, enabling generalization across multiple robots.\nDuring inference, each robot sends its target trajectory predictions to the central server. The server fuses these predictions to form a global understanding of target movements and generate future trajectories for the UAV team.\nCompared to fixed models or heuristics, FL allows on-device adaptation to environmental variations, leading to better long-term performance. Unlike centralized approaches that send all raw data to the cloud, FL supports real-time inference and reduces communication overhead by keeping high-dimensional data local.\nAlthough deploying and training NN models on edge devices was a major challenge in the past, recent advances in efficient algorithms and more capable hardware have made it increasingly feasible.\nFor instance, the YOLOv11n model can process 640×640640\\times 640 images on a Raspberry Pi 5 in around 100 ms per image [9].\nOverall, an FL-enabled multi-robot target tracking framework offers the potential to improve scalability, robustness, and communication efficiency compared to traditional approaches.\n\nWhile FL is promising, several key challenges remain in designing a practical FL framework for multi-UAV target tracking. These challenges include:\n\nLimited device resources: Despite recent advances, deploying NNs on UAVs remains challenging due to their high resource demands for training and inference [10]. Wireless communication with the cloud also consumes significant energy [11].\nThe FL framework should minimize resource consumption and extend the endurance of SWaP-constrained UAVs.\n\nData heterogeneity in FL: Data heterogeneity, namely the varying distributions of data collected by distributed devices, is a long-standing challenge in FL [12, 13, 14].\nIn visual-based target tracking, data heterogeneity may result from variations in monitored targets or differences in the robots’ fields of view.\nIf not properly addressed, data heterogeneity can hinder learning convergence in FL and degrade trajectory prediction performance.\n\nRobust prediction-action coupling:\nMulti-UAV tracking requires integrating trajectory prediction with trajectory planning: accurate predictions are critical for generating effective maneuvers, while the chosen actions in turn influence future observations, making this closed-loop coupling essential for long-term performance.\n\nIn this paper, we propose DroneFL, an end-to-end federated learning framework for multi-robot target tracking using vision-based perception. DroneFL addresses the above challenges through three key designs spanning target trajectory prediction and drone trajectory planning. First, we introduce a lightweight neural network architecture to operate within SWaP constraints. Each robot uses a frozen YOLO model [9] to extract bounding box detections from images. The bounding boxes and recent odometry data are then fed into a shallow transformer model [20] that predicts future target trajectories. During FL, only the transformer layers are updated on the drone, minimizing onboard latency and energy consumption. Onboard storage is also reduced by storing only bounding boxes instead of raw image data. Second, we observe that FL convergence is mainly hindered by data heterogeneity caused by flight altitudes, which produce different field-of-view patterns. To address this, we propose an altitude-based adaptive instance normalization design in the transformer model, making it position-invariant and easier to train across drones with varying views.\nFinally, we develop a centralized trajectory planning module that fuses predictions from multiple robots using an Extended Kalman Filter with adaptive innovation, and generates optimal trajectories by minimizing a custom cost function that balances tracking robustness with perception quality.\nDroneFL is extensively evaluated in a simulated testbed, using an agricultural setting as a case study. DroneFL is further validated for efficiency on a Raspberry Pi 5, a representative computing unit for resource-constrained UAVs.\n\nIn summary, the contributions of this paper are:\n\nDroneFL, the first FL framework for multi-UAV target tracking, which tightly couples closed-loop trajectory prediction and trajectory planning through efficient and effective FL under data heterogeneity\n\nComprehensive evaluations showing that DroneFL reduces prediction error by 6%-83% and tracking distance by 0.4%-4.6 % compared to distributed non-FL framework. DroneFL achieves real-time prediction on a representative computing unit for UAVs, with only 1.561.56 KB/s communication with the cloud.\n\n1. DroneFL, the first FL framework for multi-UAV target tracking, which tightly couples closed-loop trajectory prediction and trajectory planning through efficient and effective FL under data heterogeneity\n\n2. Comprehensive evaluations showing that DroneFL reduces prediction error by 6%-83% and tracking distance by 0.4%-4.6 % compared to distributed non-FL framework. DroneFL achieves real-time prediction on a representative computing unit for UAVs, with only 1.561.56 KB/s communication with the cloud.",
            "llm_summary": "【论文的motivation是什么】  \n1. 多机器人目标追踪需要协调监控动态实体，现有方法无法处理高维输入和环境变化。  \n2. 联邦学习在多无人机目标追踪中的应用尚未得到充分探索，面临计算资源和数据异质性挑战。  \n3. UAV在执行任务时受限于尺寸、重量和功耗（SWaP），需要高效的框架来处理视觉输入。  \n\n【提出了什么创新的方法】  \n本文提出了DroneFL，一个专为多无人机目标追踪设计的联邦学习框架。该框架采用轻量级本地模型，通过冻结YOLO骨干网络和浅层变换器进行高效的目标轨迹预测。为解决数据异质性问题，DroneFL引入了基于高度的自适应实例归一化，使模型架构具有位置不变性。最终，多个无人机的预测结果在云端融合，生成最优轨迹，平衡预测准确性和整体追踪性能。实验结果表明，DroneFL在预测误差上减少了6%-83%，在追踪距离上减少了0.4%-4.6%，并在Raspberry Pi 5上实现实时运行，平均仅需1.56 KB/s的数据传输率。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Developing a Mono-Actuated Compliant GeoGami Robot",
            "authors": "Archie Webster,Lee Skull,Seyed Amir Tafrishi",
            "subjects": "Robotics (cs.RO)",
            "comment": "under-review",
            "pdf_link": "https://arxiv.org/pdf/2509.21445",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21445",
            "arxiv_html_link": "https://arxiv.org/html/2509.21445v1",
            "abstract": "This paper presents the design of a new soft-rigid robotic platform, ”GeoGami”. We leverage origami surface capabilities to achieve shape contraction and to support locomotion with underactuated forms. A key challenge is that origami surfaces have high degrees of freedom and typically require many actuators; we address repeatability by integrating surface compliance. We propose a mono-actuated GeoGami mobile platform that combines origami surface compliance with a geometric compliant skeleton, enabling the robot to transform and locomote using a single actuator. We demonstrate the robot, develop a stiffness model, and describe the central gearbox mechanism. We also analyze alternative cable-driven actuation methods for the skeleton to enable surface transformation. Finally, we evaluate the GeoGami platform for capabilities, including shape transformation and rolling. This platform opens new capabilities for robots that change shape to access different environments and that use shape transformation for locomotion.",
            "introduction": "Compliant origami offers a rich geometric design space where complex 3D behaviors emerge from simple, repeatable folds of a planar sheet [1, 2]. Translating these capabilities into robots that achieve shape change or locomotion with minimal actuation is challenging: underactuated designs must exploit structural physics—distributed stiffness, mass and inertia, and geometry-induced constraints—to route energy and motion effectively. This motivates co-design of crease patterns and compliance so the desired transformations can be achieved and controlled with as little actuation as possible.\n\nCompliance and softness are crucial for safer, more adaptable mechanisms [2] and robotic systems [3], with clear advantages in constrained or hazardous settings. In power-plant inspection and maintenance, where intrusive methods are costly and awkward [4, 5], a robot that can morph to the environment like a soft system yet endure harsh conditions like a rigid platform enables in-situ traversal and stable, on-site operations [6]. Recent reviews emphasize that compliance must be co-designed with geometry, materials, and control to achieve predictable interaction—spanning modeling frameworks for soft robots [7], compliant mechanisms for contact-rich tasks [8], and system-level overviews of soft robotics [9]. In parallel, origami-based robots illustrate how compliant folding structures integrate sensing/decision/actuation for robust interaction [2, 10], and surveys document diverse approaches for crease-pattern design and controllable deployment [11] or even using geometric one-print compliant models [12]. In practice, patterns such as Miura-ori and waterbomb combined with compliant hinges can deliver large, coordinated shape change with minimal actuation, while tendon-, pneumatic-, or SMA-based actuation governs deployment and force transmission. Nevertheless, striking the right balance between rigidity (load-bearing, durability), softness (safe contact, adaptability), and the repeatable “returnability” of origami folds remains non-trivial in practice.\n\nUnderactuated robotics, systems with fewer independent actuators than degrees of freedom, offers advantages in energy efficiency, material use, and compact form factors [13, 14, 15]. When paired with careful morphology and control, such designs can be exceptionally effective in tasks like locomotion [16] and shape transformation in constrained environments [17]. Because fewer actuators reduce mass, volume, and power draw, underactuated platforms are attractive for field deployment [13]. This mirrors biological strategies, where passive dynamics and compliant couplings distribute motion and forces efficiently. The design goal, therefore, is to use the minimum number of actuators while exploiting mechanical couplings, tendon routing, and structural compliance to modulate stiffness and geometry for navigation and manipulation—an idea exemplified by adaptive-synergy hands coordinating many joints with a single input [18]. Nevertheless, integrating geometric compliance (e.g., origami-inspired structures) with underactuation to achieve reversible shape change and size modulation using very few actuators remains an open challenge.\n\nWe address the limited study of actuation strategies for compliant origami surfaces, particularly when the platform must realize multiple shape transformations in addition to rolling. We introduce novel GeoGami as shown in Fig. 1, a mono-actuated robotic platform that integrates a modular geometric compliant skeleton with an origami surface; both structures provide programmable compliance and are manipulated by a centralized cable-driven gearbox. The paper develops and validates a stiffness model for the origami patches and the geometric skeleton, quantifies returnability, and presents an analytical center-of-mass formulation that maps cable retraction through the transmission to mass imbalance and locomotion. We further characterize the transmission and cable cyclic routing for single-actuator operation, and evaluate the resulting motion capabilities of the platform, including size compaction, controllable contact, and rolling.\n\nThe remainder of the paper is organized as follows. Section II introduces the GeoGami hardware, including the compliant skeleton, origami surface, joint measurement protocol, and the mono-actuated cyclic gearbox. Section III develops the planar kinematics, the stiffness aggregation from joint to side, and the center of mass model that links cable retraction to motion. Section IV presents experiments and motion behavior analysis; Section V concludes findings and outlines future work.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的origami表面设计通常需要多个驱动器，难以实现形状变化和运动。  \n2. 需要通过结构物理学来有效路由能量和运动，以实现最小化驱动器的设计目标。  \n3. 在受限或危险环境中，柔性和适应性机制的需求日益增加。  \n\n【提出了什么创新的方法】  \n本文提出了一种新的单驱动GeoGami机器人平台，结合了可编程的origami表面和几何合规骨架。该平台通过中央电缆驱动齿轮箱实现形状变化和运动，利用单个驱动器来控制多种形状变换和滚动能力。通过开发和验证刚度模型，量化可返回性，并提出了一个分析的质心公式，研究了电缆收缩与运动之间的关系。实验结果表明，该平台能够有效实现尺寸压缩、可控接触和滚动等运动能力，展示了其在复杂环境中的应用潜力。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Language-in-the-Loop Culvert Inspection on the Erie Canal",
            "authors": "Yashom Dighe,Yash Turkar,Karthik Dantu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "First two authors contributed equally",
            "pdf_link": "https://arxiv.org/pdf/2509.21370",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21370",
            "arxiv_html_link": "https://arxiv.org/html/2509.21370v1",
            "abstract": "Culverts on canals such as Erie Canal built originally in 1825 require frequent inspections to ensure safe operation.\nHuman inspection of culverts is challenging due to age, geometry, poor illumination, weather and lack of easy access.\nWe introduce VISION, an end-to-end, language-in-the-loop autonomy system that couples a web-scale vision–language model (VLM) with constrained viewpoint planning for autonomous inspection of culverts. Brief prompts to the VLM solicit open-vocabulary ROI proposals with rationales and confidences, stereo depth is fused to recover scale, and a planner—aware of culvert constraints commands repositioning moves to capture targeted close-ups. Deployed on a quadruped in Culvert under the Erie canal, VISION closes the see→decide→move→re-image loop on-board and produces high-resolution images for detailed reporting without domain-specific fine-tuning. In an external evaluation by New York Canal Corporation personnel, initial ROI proposals achieved 61.4% agreement with subject-matter experts, and final post-re-imaging assessments reached 80%, indicating that VISION converts tentative hypotheses into grounded, expert-aligned findings.",
            "introduction": "Canals are artificial waterways built for drainage management and water transport. Largely built in the 19th and 20th century, there are over 4000 miles of canals in the US [1]. Erie canal is one such canal that is 363 miles in length and runs east-west between the Hudson river and Lake Erie. It was initially built in 1825 with several expansions including a major one between 1905 and 1918. Culverts are small cylindrical passages under the canal typically built for draining water after wet spells. The erie canal has over 350 culverts. Most of them were built using concrete, metal or stone. However, over time, these culverts are subject to structural deterioration, as reflected in New York Canal Corporation (NYCC) inspection records. Typical defects include surface corrosion, spalling, and seepage, any of which can escalate to culvert failure with serious consequences for adjacent communities and ecosystems. Currently, these culverts are inspected manually, requiring personnel to enter confined underground spaces in remote locations. Such inspections pose significant risks: the culverts lie beneath the canal in restricted, poorly ventilated environments, where visibility and air quality are uncertain, and structural stability cannot be guaranteed. Reaching these sites is often difficult due to steep slopes, uneven terrain, and waterlogged conditions, all of which increase the likelihood of accidents. These hazards make manual inspection both dangerous and impractical, motivating the development of automated alternatives. Legged robots offer a promising alternative [2] as they can descend embankments, wade through shallow water, and carry multi-modal sensors for close-range imaging.\n\nHowever, despite the ability of legged robots to traverse such constrained environments, culvert inspection remains a challenging task due to unreliable defect detection. The problem is exacerbated by long-tailed, site-specific degradations and highly inconsistent illumination conditions, which limit the effectiveness of closed-set detectors such as  [3, 4]. On the other hand, open vocabulary based methods perform poorly due to out of domain inputs.\nFigure 2 shows the output of three state-of-the-art open-vocabulary baselines (Lang-SAM [5], Grounding DINO [6], and Grounding SAM [7]) for three commonly used terms in inspection reports: rust, ice, and scaling. Grounding DINO yields coarse boxes; Grounding SAM converts those boxes to masks but spreads labels broadly. Further, fine-tuning existing models for this task is infeasible due to the lack of domain-specific data. The limited number of available human inspection reports are often qualitative and expressed in unstructured free text (see Fig. 1), lacking the consistency needed to support fine-tuning.\nBeyond visual detection, inspection methods that rely on 3D or photogrammetric reconstruction are also hindered by the same environmental factors. Poor and inconsistent illumination combined with a feature-sparse, confined geometry renders such reconstructions particularly unreliable. In these conditions, reconstruction algorithms often fail to capture micro-scale structural details, leading to smoothing over real faults like fine cracks. Prevailing challenges in photogrammetric and multi-view reconstruction include low-texture regions and uneven lighting, which systematically degrade geometric fidelity [8]. Moreover, quantitative studies confirm that insufficient surface texture introduces reconstruction noise and reduces accuracy, especially in fine-detail retention [9]. In short, while robots can access these environments, perception and defect detection under challenging visual conditions are one of the major challenges for autonomous culvert inspection.\n\nLarge Vision–Language Models (VLMs) offer a promising alternative in this context. They have shown remarkable adaptability to vague or underspecified prompts, often outperforming hand-crafted baselines in zero- or few-shot settings [10]. Prompt engineering has been demonstrated to effectively steer VLMs across a wide range of tasks without fine-tuning, enabling robust generalization from loosely phrased instructions [11]. Moreover, combining textual and visual prompting can compensate for the ambiguity or sparsity in any single modality, further enhancing performance in under-defined scenarios such as out-of-distribution segmentation [12]. When inspecting a culvert, a human does not carefully analyze every surface in detail from the start; instead, they take a quick glance to judge whether something looks unusual, and only then move closer to examine those regions more carefully. VLMs naturally mirror this behavior: they can be prompted with broad, loosely phrased queries to quickly highlight potentially relevant regions, and then refined with more specific prompts or visual cues to focus attention where it is most needed.\n\nBuilding on this analogy, we present VISION (Visual Inspection System with Intelligent Observation and Navigation), a language-in-the-loop inspection stack that runs onboard the robot. VISION couples vision–language reasoning with constrained viewpoint planning to enable autonomous inspection in culverts.\nGiven a general prompt from an inspector and an image from a forward-facing camera, the VLM proposes candidate defect regions with bounding boxes, confidences, and natural-language rationales.\nThese proposals drive a planner that accounts for culvert geometry and the robot’s limited mobility, commanding short forward motions and pan–tilt adjustments of a second camera to capture close-range, high-resolution imagery suitable for downstream measurement and audit.\nOur contributions in the work are as follows:\n\nWe propose an end-to-end framework for fully autonomous culvert inspection\n\nWe demonstrate a methodology to leverage web-scale vision-language models zero-shot for an abstract inspection task\n\nWe deploy our system onboard a legged robot with a pan-tilt gimbal, and validate its effectiveness through field experiments in an actual culvert.\n\n1. We propose an end-to-end framework for fully autonomous culvert inspection\n\n2. We demonstrate a methodology to leverage web-scale vision-language models zero-shot for an abstract inspection task\n\n3. We deploy our system onboard a legged robot with a pan-tilt gimbal, and validate its effectiveness through field experiments in an actual culvert.",
            "llm_summary": "【论文的motivation是什么】  \n1. 人工检查culverts面临安全风险和环境挑战，难以进行有效的检测。  \n2. 现有的视觉检测方法在复杂环境中表现不佳，缺乏足够的领域特定数据进行微调。  \n3. 需要一种自动化的解决方案来提高culvert检查的效率和安全性。  \n\n【提出了什么创新的方法】  \n提出了VISION，一个结合了视觉-语言推理和受限视角规划的端到端自动化检查系统。该系统通过简要提示向视觉-语言模型（VLM）请求开放词汇的缺陷区域建议，并融合立体深度信息以恢复尺度。规划器根据culvert的几何特征指挥机器人进行短距离移动和摄像头的俯仰调整，从而捕获适合后续测量和审计的高分辨率图像。在实际的culvert检查中，VISION实现了61.4%的初步ROI提案与专家一致性，最终的后重成像评估达到了80%，有效将初步假设转化为与专家一致的发现。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation",
            "authors": "Shuang Zeng,Dekang Qi,Xinyuan Chang,Feng Xiong,Shichao Xie,Xiaolong Wu,Shiyi Liang,Mu Xu,Xing Wei",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.22548",
            "code": "https://miv-xjtu.github.io/JanusVLN.github.io/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22548",
            "arxiv_html_link": "https://arxiv.org/html/2509.22548v1",
            "abstract": "Vision-and-Language Navigation (VLN) requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream.\nRecent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models (MLLMs). However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation.\nInspired by the implicit scene representation in human navigation, analogous to the left brain’s semantic understanding and the right brain’s spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations.\nThis framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input.\nThen, the historical key-value (KV) caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates.\nExtensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data.\nThis indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.",
            "introduction": "Vision-and-Language Navigation (VLN) is a foundational task in embodied AI, requiring an agent to navigate through unseen environments guided by visual inputs and natural language instructions. Recently, capitalizing on the advanced visual perception and semantic understanding capabilities of Multimodal Large Language Models (MLLMs), a new line of research (Zhang et al., 2025b; Wei et al., 2025a) has emerged. These approaches leverage vast-scale training data to adapt MLLMs into VLN models, thereby reshaping the future landscape of VLN research.\n\nTo support navigation models in conducting prolonged and effective exploration, these approaches typically only construct an explicit semantic memory. One class of methods (Zhang et al., 2025c; Zeng et al., 2024) builds a semantic cognitive map using textual descriptions for object nodes and relational edges. However, purely textual descriptions struggle to precisely convey the spatial relationships and orientation of objects, leading to the loss of crucial visual, spatial-geometric, and contextual information. Moreover, repetitive descriptions introduce substantial redundancy and noise. Another class of methods (Zhang et al., 2025b; Wang et al., 2024b) stores historical observation frames, which necessitates reprocessing the entire history of observations along with the current frame at each action prediction step, resulting in significant redundant computation. Finally, in both types of approaches, the explicit semantic memory grows exponentially as navigation time increases. This makes it exceedingly difficult for the model to extract critical information from a vast, cluttered, and fragmented memory, thereby leading to severe inefficiency.\n\nMore importantly, these methods collectively face a fundamental contradiction. Navigation is an inherently 3D physical interaction, yet the visual encoders of existing VLA models almost exclusively inherit the CLIP paradigm pre-trained on 2D image-text pairs. This approach enables these encoders to excel at capturing high-level semantics while leaving them deficient in understanding 3D geometric structures and spatial information. However, a frequently overlooked yet critical insight is that 2D images are not merely isolated planes of pixels but are projections of the 3D physical world, inherently containing a wealth of 3D spatial cues such as perspective, occlusion, and geometric structures. Whereas human observers can effortlessly perceive depth and comprehend spatial layouts from a single static image, existing models neglect this readily available implicit 3D information in their inputs. This oversight fundamentally constrains their spatial reasoning capabilities in complex navigation tasks.\n\nInspired by the human brain’s hemispheric specialization for navigation, where the left hemisphere handles semantic understanding and the right manages 3D spatial cognition to form implicit representations (Gazzaniga, 1967), we propose a fundamental shift from a single, explicit memory to a dual, implicit neural memory. To this end, we introduce JanusVLN, a dual implicit memory framework for VLN that features both spatial-geometric and visual-semantic memory in Figure 1. We model these two types of memory respectively as fixed-size, compact neural memory, whose size does not grow with the trajectory length. This design is analogous to the human brain’s ability to perform efficient memorization within a finite capacity.\n\nTo construct this dual implicit memory, we extend the MLLM into a novel VLN model by incorporating a feed-forward 3D visual geometry foundation model, which provides 3D spatial geometric structural information solely from RGB video input, obviating the need for any explicit 3D data. Unlike the visual encoders of general MLLMs, which are predominantly trained on 2D image-text data, this spatial geometry model is typically trained on pixel-3D point cloud pairs, thereby embedding strong 3D perception priors. We establish implicit spatial-geometric and visual-semantic memory by caching historical key-value (KV) from a 3D spatial geometry encoder and MLLM’s semantic visual encoder, respectively. These dual implicit memory are dynamically and incrementally updated through the initial and sliding window, enabling the progressive integration of historical information for each new frame without recomputing past frames. Extensive experiments demonstrate that JanusVLN significantly enhances spatial comprehension while lowering inference overhead, achieving SOTA performance on VLN-CE benchmarks. It establishes a new paradigm for VLN research, propelling a shift from being 2D semantics-dominated to 3D spatial-semantic synergy. This marks a pivotal direction toward building the next generation of spatially-aware embodied agents.\n\nIn summary, our contributions are as follows:\n\nWe introduce a novel dual implicit memory paradigm for VLN. Inspired by human cognitive science, this framework simultaneously captures visual semantics and spatial geometry to overcome the inherent limitations of existing navigation LLM.\n\nWe introduce a novel dual implicit memory paradigm for VLN. Inspired by human cognitive science, this framework simultaneously captures visual semantics and spatial geometry to overcome the inherent limitations of existing navigation LLM.\n\nOur proposed JanusVLN incrementally updates its dual implicit memory with cached KV from initial and sliding window, which eliminates the need to recompute historical frames for each new frame, substantially lowering computational and inference overhead.\n\nComprehensive experiments on the VLN-CE benchmark demonstrate that JanusVLN achieves SOTA results without requiring auxiliary 3D data. This validates the efficacy of JanusVLN and establishes a new memory paradigm for the field of VLN.\n\n1. We introduce a novel dual implicit memory paradigm for VLN. Inspired by human cognitive science, this framework simultaneously captures visual semantics and spatial geometry to overcome the inherent limitations of existing navigation LLM.\n\n2. Our proposed JanusVLN incrementally updates its dual implicit memory with cached KV from initial and sliding window, which eliminates the need to recompute historical frames for each new frame, substantially lowering computational and inference overhead.\n\n3. Comprehensive experiments on the VLN-CE benchmark demonstrate that JanusVLN achieves SOTA results without requiring auxiliary 3D data. This validates the efficacy of JanusVLN and establishes a new memory paradigm for the field of VLN.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的VLN方法依赖显式语义记忆，导致空间信息丢失和计算冗余。  \n2. 现有模型缺乏对3D几何结构和空间信息的理解，限制了空间推理能力。  \n3. 需要一种新的记忆机制来有效整合视觉语义和空间几何信息。  \n\n【提出了什么创新的方法】  \n本研究提出了JanusVLN，一个具有双隐式神经记忆的VLN框架，分别建模空间几何和视觉语义记忆。该框架通过引入3D空间几何编码器，增强了模型的空间推理能力，并通过初始和滑动窗口动态更新历史键值缓存，避免了冗余计算。实验结果表明，JanusVLN在VLN-CE基准上显著提升了成功率，超越了20种近期方法，展示了其在空间理解和计算效率上的优势。这一创新方法为未来的VLN研究开辟了新的方向。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Learning to Ball: Composing Policies for Long-Horizon Basketball Moves",
            "authors": "Pei Xu,Zhen Wu,Ruocheng Wang,Vishnu Sarukkai,Kayvon Fatahalian,Ioannis Karamouzas,Victor Zordan,C. Karen Liu",
            "subjects": "Graphics (cs.GR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2025). Website:this http URL. Video:this https URL. Code:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.22442",
            "code": "https://youtu.be/2RBFIjjmR2I, http://pei-xu.github.io/basketball, https://github.com/xupei0610/basketball",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22442",
            "arxiv_html_link": "https://arxiv.org/html/2509.22442v1",
            "abstract": "Learning a control policy for a multi-phase, long-horizon task, such as basketball maneuvers, remains challenging for reinforcement learning approaches due to the need for seamless policy composition and transitions between skills. A long-horizon task typically consists of distinct subtasks with well-defined goals, separated by transitional subtasks with unclear goals but critical to the success of the entire task. Existing methods like the mixture of experts and skill chaining struggle with tasks where individual policies do not share significant commonly explored states or lack well-defined initial and terminal states between different phases.\nIn this paper, we introduce a novel policy integration framework to enable the composition of drastically different\nmotor skills in multi-phase long-horizon tasks with ill-defined intermediate states. Based on that, we further introduce a high-level soft router to enable seamless and robust transitions between the subtasks.\nWe evaluate our framework on a set of fundamental basketball skills and challenging transitions.\nPolicies trained by our approach can effectively control the simulated character to interact with the ball and accomplish the long-horizon task specified by real-time user commands, without relying on ball trajectory references.",
            "introduction": "Many real-world tasks consist of complex objectives that can be broken down into sequences of differing subtasks. Successfully executing these multi-phase, long-horizon tasks demands the mastery of heterogeneous skills and the ability to transition seamlessly between them. Basketball provides a compelling example of these challenges. For example, a fundamental maneuver, “shoot-off-the-dribble”, involves distinct subtasks such as dribbling, gathering the ball and shooting, as well as the ability to transition between these skills, ultimately culminating in the ball successfully going into the hoop. However, while dribble and shoot are characterized by well-defined stand-alone goals, gather acts largely as a transition subtask with poorly defined starting and ending states. Thus, executing such multi-phase tasks challenges control methods proposed to date.\n\nReinforcement learning (RL) has shown promise in training policies for individual skills for physics-based character control (peng2018deepmimic; shi2023hci; chentanez2018physics; yin2021discovering; kwiatkowski2022survey),\nbut composing these policies into a cohesive framework remains an open problem. Previous approaches have attempted to address this through methods like a mixture of experts (scadiver2; peng2018sfv). However, this technique relies on sufficiently exploring shared states across individual policies—a condition that does not hold for tasks like dribbling and shooting. Another line of work known as skill chaining (konidaris2009skill; lee2021adversarial; chen2023sequential; liu2017learning; clegg2018learning) allows concatenation of policies but requires each skill to have a well-defined set of terminal states. This limitation renders it ineffective for intermediate tasks where the subtask’s goal depends on the context of the subsequent policy. For example, the gathering motion between dribbling and shooting can be intuitively described as “bringing the agent to a state where shooting is possible.” However, crafting a reward function based solely on state or action variables to reflect this goal is challenging.\n\nTo tackle the problem of building policies for ill-defined, intermediate subtasks, we introduce a policy integration method to compose drastically different and/or ill-defined skills to achieve a multi-phase, long-horizon task. The core idea is to first train policies for well-defined subtasks independently and then use these policies to guide the training of the ambiguous, intermediate subtasks. Specifically, for a task sequence consisting of subtasks A, B, and C, where A and C have well-defined task goals but B does not, we train B using policy A to define a valid initial state distribution and policy C’s state value function to shape the terminal reward. To further improve the transition between B and C, we simultaneously adapt the pretrained policy C to the states generated by B under training. In this process, a state value estimator optimized in tandem with the adapted C will be provided to reflect the up-to-date state value evaluation for policy B optimization.\nWith the primitive policies for all subtasks in place, we finally train a high-level soft-routing policy that directs the execution of those primitive policies based on real-time external commands, such as dribbling destination\nand velocity, or a jump-shot action.\n\nAnother challenging aspect in learning policies for multi-phase, long-horizon tasks is the heterogeneity of movement that demands diverse and extensive human motion data. Previous work on basketball motion synthesis has demonstrated compelling results when using structured data with corresponding full body, fingers, and basketball movements for physics-based character control (liu2018learning; wang2024skillmimic; wang2023physhoi; starke2020local). However, such a special dataset is hard to scale for training a general policy capable of performing under a wide array of conditions.\nIn our work, instead, we demonstrate the generation of policies from unstructured data. We leverage a diverse collection of basketball motion data, including full-body motions without hands, and hand-only motions, as well as motion examples from unstructured videos.\nTo enrich locomotion behaviors, some normal running motions are also included.\nOur method makes no assumptions about the correspondence across datasets or availability of ball trajectories.\n\nOur results show that the proposed method enables the agent to perform smooth and coordinated basketball maneuvers, from gross body movements to fine finger actions, while responding adaptively to user commands. The agent can freely play basketball in real-time—for instance, dribbling to any location at variable speeds and finishing with a jump shot from any direction, achieving a shooting accuracy of 91.8% on a professional court. We further demonstrate team play with multiple agents interacting through catching, passing, rebounding, and defending. Extensive ablation studies validate key design choices, such as soft routing and policy fine-tuning, and expose the limitations of existing methods in handling ambiguous subtasks. By addressing skill integration and phase transitions in long-horizon tasks, our approach advances the capabilities of reinforcement learning in dynamic, interactive environments.",
            "llm_summary": "【论文的motivation是什么】  \n1. 多阶段、长时间任务的控制策略学习面临挑战，尤其是技能之间的无缝过渡。  \n2. 现有方法在处理缺乏明确初始和终止状态的任务时效果不佳。  \n3. 需要有效整合不同的运动技能以完成复杂的任务。  \n\n【提出了什么创新的方法】  \n本文提出了一种新颖的政策集成框架，旨在实现多阶段长时间任务中不同运动技能的组合。首先，独立训练具有明确目标的子任务政策，然后利用这些政策指导模糊中间子任务的训练。通过结合高层软路由策略，确保子任务之间的顺畅过渡。最终，训练出的政策能够在没有球轨迹参考的情况下，实时响应用户命令，成功执行篮球技能，展示出91.8%的投篮准确率，并实现多代理的团队配合。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer",
            "authors": "Zhehao Dong,Xiaofeng Wang,Zheng Zhu,Yirui Wang,Yang Wang,Yukun Zhou,Boyuan Wang,Chaojun Ni,Runqi Ouyang,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang",
            "subjects": "Artificial Intelligence (cs.AI); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22407",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22407",
            "arxiv_html_link": "https://arxiv.org/html/2509.22407v1",
            "abstract": "Vision–language–action (VLA) models increasingly rely on diverse training data to achieve robust generalization. However, collecting large-scale real-world robot manipulation data across varied object appearances and environmental conditions remains prohibitively time-consuming and expensive.\nTo overcome this bottleneck, we propose Embodied Manipulation Media Adaptation (EMMA), a VLA policy enhancement framework that integrates a generative data engine with an effective training pipeline. We introduce DreamTransfer, a diffusion Transformer-based framework for generating multi-view consistent, geometrically grounded embodied manipulation videos. DreamTransfer enables text-controlled visual editing of robot videos, transforming foreground, background, and lighting conditions without compromising 3D structure or geometrical plausibility. Furthermore, we explore hybrid training with real and generated data, and introduce AdaMix, a hard-sample-aware training strategy that dynamically reweights training batches to focus optimization on perceptually or kinematically challenging samples.\nExtensive experiments show that videos generated by DreamTransfer significantly outperform prior video generation methods in multi-view consistency, geometric fidelity, and text-conditioning accuracy. Crucially, VLAs trained with generated data enable robots to generalize to unseen object categories and novel visual domains using only demonstrations from a single appearance.\nIn real-world robotic manipulation tasks with zero-shot visual domains, our approach achieves over a 200% relative performance gain compared to training on real data alone, and further improves by 13% with AdaMix, demonstrating its effectiveness in boosting policy generalization.",
            "introduction": "Vision–language–action (VLA) models have demonstrated remarkable capabilities in enabling robots to perform complex manipulation tasks from natural language instructions and visual inputs (Black et al., 2024; Intelligence et al., 2025; Brohan et al., 2023; Kim et al., 2024; NVIDIA et al., 2025c; Deng et al., 2025). However, their success critically depends on large-scale, diverse training data.\nCollecting real-world robot manipulation data through human teleoperation is labor-intensive and expensive, severely limiting the scale and visual diversity of available datasets.\nWhile simulation offers a scalable alternative for generating annotated trajectories (Geng et al., 2025; Lin et al., 2024; Mu et al., 2025; Katara et al., 2023; Lin et al., 2024), simulated environments often suffer from visual realism gaps and are constrained by limited asset diversity. As a result, policies trained on simulated data frequently underperform when deployed in the real world.\n\nRecently, diffusion models (Wan et al., 2025; Kong et al., 2025; NVIDIA et al., 2025a; Yang et al., 2025; Zheng et al., 2024) have emerged as a promising method for generating realistic and diverse visual video. Several works have explored using diffusion models to generate vision-action data for policy training. Cosmos-Transfer1 (NVIDIA et al., 2025b) generates videos conditioned on semantic segmentation and depth, improving realism for sim-to-real transfer. RoboEngine (Yuan et al., 2025a) provides a flexible toolkit for generating diverse robot interaction scenes by combining background generation with accurate robot segmentation, without requiring camera calibration. RoboTransfer (Liu et al., 2025) further improves multi-view consistency by explicitly modeling 3D geometry using depth maps and surface normals, allowing controllable edits.\n\nDespite these advances, two key challenges remain. First, most methods (NVIDIA et al., 2025b; Yuan et al., 2025a) generate videos from a single view, without ensuring consistency across viewpoints. This limits their usefulness for downstream robot tasks that rely on multi-camera inputs. RoboTransfer takes a step toward multi-view consistency, but its diversity is limited because it often transfers poorly to new domains. Second, existing works treat generated data as a static augmentation, without considering how to use it effectively during training.\n\nIn this work, we propose Embodied Manipulation Media Adaptation (EMMA), a VLA policy enhancement framework that integrates two core components: DreamTransfer and AdaMix.\nDreamTransfer is a diffusion Transformer (DiT)-based framework for generating multi-view consistent, geometrically grounded embodied manipulation videos.\nIt jointly models appearance and geometry across multiple camera views, ensuring spatial and temporal coherence. DreamTransfer supports text-controlled visual transfer: users can edit the foreground objects, background, and lighting conditions of real or simulated demonstrations through natural language, while preserving the underlying 3D structure and geometrical plausibility of the scene.\nAs illustrated in Figure 1, DreamTransfer enables realistic and controllable video generation for both real-to-real and sim-to-real transfer scenarios, making it a powerful tool for scalable robotic policy training.\nTo improve policy learning, we further propose AdaMix, a hard-sample-aware training strategy. We define a set of functions to evaluate the quality of predicted trajectories from the VLA policy and use the performance score to drive an adaptive sampling mechanism. By iteratively refining the training distribution toward challenging cases, our method improves robustness and generalization.\n\nWe evaluate on a variety of robotic manipulation tasks in both video generation quality and real-world robot deployment, including Fold Cloth, Clean Desk, and Throw Bottle. These tasks span a wide range of challenges involving both rigid and deformable objects, short-horizon and long-horizon action sequences, and diverse skills such as grasping, pushing, placing, and draping.\nCompared to the state-of-the-art transfer model, DreamTransfer improves multi-view consistency by 42% and depth consistency by 24%, demonstrating superior geometric fidelity and cross-view coherence.\nIn real-world robotic manipulation tasks involving zero-shot visual appearances, our method achieves over a 200% relative improvement in task success rate compared to training on real data alone, with an additional 13% gain when integrated with AdaMix.\n\nIn summary, our contributions are:\n\nWe propose EMMA, a VLA policy enhancement framework that integrates a generative data engine with an effective training strategy.\nThe data engine generates diverse, multi-view consistent robot manipulation videos for both rigid and deformable objects, while adaptive sample weighting improves VLA policy generalization.\n\nWe propose DreamTransfer, a DiT-based model that generates multi-view consistent, geometrically grounded manipulation videos and supports text-controlled editing of foreground, background, and lighting conditions.\nWe further introduce AdaMix, a hard-sample-aware training strategy that identifies challenging trajectories and adaptively reweights them during training.\n\nEMMA demonstrates strong performance in video generation and real-world robotic deployment. Compared to the state-of-the-art model, DreamTransfer achieves a 42% gain in multi-view consistency and a 24% gain in depth consistency, measured relatively. In zero-shot visual settings, our method achieves over a 200% performance gain compared to real-data training, with AdaMix providing an additional 13% improvement and enhancing cross-domain visual generalization.\n\n1. We propose EMMA, a VLA policy enhancement framework that integrates a generative data engine with an effective training strategy.\nThe data engine generates diverse, multi-view consistent robot manipulation videos for both rigid and deformable objects, while adaptive sample weighting improves VLA policy generalization.\n\n2. We propose DreamTransfer, a DiT-based model that generates multi-view consistent, geometrically grounded manipulation videos and supports text-controlled editing of foreground, background, and lighting conditions.\nWe further introduce AdaMix, a hard-sample-aware training strategy that identifies challenging trajectories and adaptively reweights them during training.\n\n3. EMMA demonstrates strong performance in video generation and real-world robotic deployment. Compared to the state-of-the-art model, DreamTransfer achieves a 42% gain in multi-view consistency and a 24% gain in depth consistency, measured relatively. In zero-shot visual settings, our method achieves over a 200% performance gain compared to real-data training, with AdaMix providing an additional 13% improvement and enhancing cross-domain visual generalization.",
            "llm_summary": "【论文的motivation是什么】  \n1. 收集大规模真实世界机器人操作数据既耗时又昂贵，限制了数据集的规模和视觉多样性。  \n2. 现有的生成方法在多视角一致性和视觉真实感方面存在不足，影响了机器人任务的执行。  \n3. 现有生成数据被视为静态增强，未有效利用于训练过程。  \n\n【提出了什么创新的方法】  \n提出了EMMA框架，结合了DreamTransfer和AdaMix两个核心组件。DreamTransfer是基于扩散Transformer的模型，能够生成多视角一致且几何上合理的机器人操作视频，并支持文本控制的视觉编辑。AdaMix是一种动态重加权的训练策略，专注于优化感知或运动上具有挑战性的样本。实验表明，使用DreamTransfer生成的视频在多视角一致性和几何保真度上显著优于现有方法，且在零-shot视觉领域的真实世界机器人操作任务中，相比仅使用真实数据训练，性能提升超过200%。结合AdaMix后，进一步提升了13%的性能，展示了其在政策泛化方面的有效性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation",
            "authors": "Nan Tang,Jing-Cheng Pang,Guanlin Li,Chao Qian,Yang Yu",
            "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22402",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22402",
            "arxiv_html_link": "https://arxiv.org/html/2509.22402v1",
            "abstract": "Reward design remains a critical bottleneck in visual reinforcement learning (RL) for robotic manipulation. In simulated environments, rewards are conventionally designed based on the distance to a target position. However, such precise positional information is often unavailable in real-world visual settings due to sensory and perceptual limitations. In this study, we propose a method that implicitly infers spatial distances through keypoints extracted from images. Building on this, we introduce Reward Learning with Anticipation Model (ReLAM), a novel framework that automatically generates dense, structured rewards from action-free video demonstrations. ReLAM first learns an anticipation model that serves as a planner and proposes intermediate keypoint-based subgoals on the optimal path to the final goal, creating a structured learning curriculum directly aligned with the task’s geometric objectives. Based on the anticipated subgoals, a continuous reward signal is provided to train a low-level, goal-conditioned policy under the hierarchical reinforcement learning (HRL) framework with provable sub-optimality bound. Extensive experiments on complex, long-horizon manipulation tasks show that ReLAM significantly accelerates learning and achieves superior performance compared to state-of-the-art methods.",
            "introduction": "Reward design stands as one of the most fundamental challenges in reinforcement learning (RL), particularly in the domain of vision-based robotic manipulation (Tian et al.,, 2023; Lu et al.,, 2025; Escontrela et al.,, 2023; Huang et al.,, 2024; Pang et al.,, 2025). In simulated environments, a common and often effective approach is to engineer dense reward signals based on precise geometric information, such as the Euclidean distance between a robot’s end-effector and a target position. However, this paradigm faces a critical limitation in real-world applications: exact state information is typically unavailable due to sensory noise, occlusions, and perceptual ambiguities. Consequently, agents must rely on high-dimensional visual observations, making hand-engineered reward design not only labor-intensive but also notoriously challenging. This reward specification bottleneck severely impedes the scalability and adoption of RL in practical robotic settings.\n\nSome prior works overcome this limitation by adopting Learning from Observation (LfO) approaches. A common practice is to employ adversarial frameworks (Ho and Ermon,, 2016; Torabi et al.,, 2018; Kostrikov et al.,, 2019), where a discriminator that does not take action as input is trained and subsequently used as a reward function. However, when dealing with high-dimensional visual inputs, such methods suffer from significant challenges in terms of training difficulty and stability. In recent years, several works (Tian et al.,, 2023; Sontakke et al.,, 2023; Ma et al.,, 2023; Escontrela et al.,, 2023; Huang et al.,, 2024) have instead attempted to design visual rewards based on heuristic strategies. These approaches either yield sparse rewards or lack an explicit structured learning process, making them inefficient for long-horizon tasks with extended periods of partial observability or complex dynamics. Thus, there still remains a need for a framework that can automatically synthesize informative, dense reward signals from readily available video demonstrations, while guiding the agent through a structured and geometrically grounded learning curriculum.\n\nIn this work, we introduce Reward Learning with Anticipation Model (ReLAM), a framework that generates dense and structured rewards from action-free video demonstrations.\nReLAM is built on the recent insight that object keypoints can serve as a powerful intermediate representation for capturing task geometry and progression (Wen et al.,, 2024). ReLAM begins by extracting task-relevant keypoints from video demonstrations: we first use the Segment Anything Model (SAM) (Zhang et al.,, 2024) to isolate objects of interest, then apply a tracking model (Karaev et al., 2024b, ) to follow pixel-level features across frames. A sparse set of representative points is selected and propagated consistently, forming a trajectory of keypoints that encode object motion. From these, we identify keyframes that signify critical stages of the task, and define the keypoint configurations in those frames as subgoals.\nUsing this curated dataset, ReLAM learns an anticipation model capable of predicting a sequence of intermediate keypoint-based subgoals that lead to the final goal. This model acts as a high-level planner, constructing a structured curriculum aligned with the geometric requirements of the task. The anticipated subgoals then enable the computation of a continuous reward signal based on keypoint distance, which is used to train a low-level, goal-conditioned policy under the hierarchical RL (HRL) framework with provable sub-optimality bound.\n\nThrough extensive empirical validation in simulated environments, we demonstrate that this approach not only significantly accelerates learning but also achieves new state-of-the-art performance on long-horizon tasks, thereby offering a robust and practical pathway toward scalable visual reinforcement learning for robotics.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的奖励设计在视觉强化学习中存在瓶颈，尤其是在机器人操作中。  \n2. 实际应用中，精确的状态信息通常不可用，导致奖励设计困难。  \n3. 现有方法在高维视觉输入下训练难度大且稳定性差。  \n4. 需要一个能够从视频演示中自动合成密集奖励信号的框架。  \n\n【提出了什么创新的方法】  \n本研究提出了奖励学习与预期模型（ReLAM），通过从无动作视频演示中生成密集和结构化的奖励。ReLAM首先提取关键点，构建任务几何的中间表示，然后学习一个预期模型，预测中间关键点子目标，形成与任务几何要求一致的结构化学习课程。基于这些预期子目标，计算连续奖励信号来训练低级目标条件策略。通过广泛的实验，ReLAM显著加速了学习过程，并在长时间操作任务上达到了新的最优性能，展示了其在可扩展视觉强化学习中的潜力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems",
            "authors": "Stefano Carlo Lambertenghi,Mirena Flores Valdez,Andrea Stocco",
            "subjects": "Software Engineering (cs.SE); Robotics (cs.RO)",
            "comment": "In proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering (ASE '25)",
            "pdf_link": "https://arxiv.org/pdf/2509.22379",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22379",
            "arxiv_html_link": "https://arxiv.org/html/2509.22379v1",
            "abstract": "Simulation-based testing is a cornerstone of Autonomous Driving System (ADS) development, offering safe and scalable evaluation across diverse driving scenarios. However, discrepancies between simulated and real-world behavior—known as the reality gap—challenge the transferability of test results to deployed systems. In this paper, we present a comprehensive empirical study comparing four representative testing modalities: Software-in-the-Loop (SiL), Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing. Using a small-scale physical vehicle equipped with real sensors (camera and LiDAR), and its digital twin, we implement each setup and evaluate two ADS architectures (modular and end-to-end) across diverse indoor driving scenarios involving real obstacles, road topologies, and indoor environments. We systematically assess the impact of each testing modality along three dimensions of the reality gap: actuation, perception, and behavioral fidelity. Our results show that while SiL and ViL setups simplify critical aspects of real-world dynamics and sensing, MR testing improves perceptual realism without compromising safety or control. Importantly, we identify the conditions under which failures do not transfer across testing modalities and isolate the underlying dimensions of the gap responsible for these discrepancies. Our findings offer actionable insights into the respective strengths and limitations of each modality and outline a path toward more robust and transferable validation of autonomous driving systems.",
            "introduction": "To ensure the safety and reliability of autonomous driving systems (ADS) before deployment in public environments, rigorous system-level testing is indispensable [1, 2, 3]. A common industrial practice for ADS validation follows a two-phase testing pipeline. First, ADS components-such as perception and planning modules-are trained using real-world driving data and evaluated within virtual environments via simulation-based testing, also known as simulation-in-the-loop (SiL). Subsequently, the ADS undergoes real-world testing on real vehicles on closed tracks up to public roads [4, 5, 6, 7].\nReal-world testing, while more faithful, is costly, time-consuming, and constrained in scope and safety.\nDespite their scalability, simulations cannot fully replicate real-world physical phenomena, such as sensor noise, actuator delays, and environmental complexity. The resulting mismatch is known as the reality gap [1] and hinders the transferability of findings to real-world ADS, undermining their trustworthiness [8].\n\nVarious strategies have been proposed to mitigate the reality gap. Some aim to increase simulation fidelity through high-precision modeling (digital twins [9, 10, 11]),\nother address specific gap dimensions, such as the perception gap, by translating simulated sensor outputs into more realistic versions using generative models [12, 13, 14, 8, 15]. However, these methods are limited to model-level testing [16, 17] and do not capture the system-level interactions between perception, planning, and control modules that govern vehicle motion. As a result, they are susceptible to actuation gaps and often miss critical system-level failures [16, 14, 18].\nOther strategies involve vehicle-in-the-loop (ViL) and mixed reality (MR) testing [19], by integrating physical components such as ECUs, small-scale robots, or full vehicles-into simulation loops. While ViL provides closed-loop evaluation, it remains partially virtualized and fails to capture real-world imperfections, such as sensor noise and lighting variability (perception gap) [20]. MR partially mitigates this by injecting virtual elements (e.g., obstacles) into real sensor data, enriching scenario realism.\nPrior system-level studies using small-scale robots investigating failure transferability [8, 21, 22, 23, 24, 25] have primarily documented the existence of the reality gap, without isolating their root causes or comparing how different test modalities influence gap reduction.\n\nTo this aim, in this paper, we conduct an empirical study of the reality gap in autonomous driving by comparing four representative testing modalities: SiL, ViL, MR, and full real-world execution (RW). Our goal is to characterize the dimensions of the reality gap-specifically along actuation, perception, and behavioral fidelity-and to assess the degree to which each testing setup retains ADS behavior relative to real-world ground truth behavior.\nWhile prior work has evaluated or mitigated specific aspects of the reality gap, a broader evaluation spanning multiple testing modalities and ADS architectures remains unaddressed.\n\nTo investigate the impact of different testing modalities, we implemented a modular ROS-based evaluation framework that supports direct comparisons across synthetic, hybrid, and physical testing conditions. Our setup integrates both modular and end-to-end ADS architectures on a small-scale platform equipped with real sensors (camera and LiDAR) and its digital twin.\nWe conduct hundreds of tests across matched driving scenarios with shared road layouts, obstacle placements, and environmental conditions, allowing direct attribution of performance differences to the testing modality.\n\nOur findings reveal that:\n(i) SiL underestimates real-world variability due to idealized dynamics;\n(ii) ViL improves actuation realism but retains perception limitations;\n(iii) MR offers the best perceptual fidelity by blending virtual elements into real sensor data.\nBy isolating the effects of actuation, perception, and behavior on the reality gap, we find that critical failures often manifest differently across configurations, with the perception gap playing a greater role in behavioral divergence than actuation discrepancies. This underscores the importance of testing methods that retain real-world sensor complexity. Our results reveal the limitations of conventional simulation and point to MR as a practical middle ground between fidelity and scalability.\n\nOur paper makes the following contributions:\n\nWe provide a ROS framework for comparing SiL, ViL, MR, and RW testing for E2E and modular ADS, which is available [26].\n\nWe provide a ROS framework for comparing SiL, ViL, MR, and RW testing for E2E and modular ADS, which is available [26].\n\nWe present a systematic analysis of the reality gap in ADS testing across behavior, actuation, and perception fidelity,\nisolating which failures transfer across test modalities. We show that MR testing uniquely replicates real-world system failures, outperforming SiL and ViL across all metrics.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现实差距（reality gap）挑战了模拟测试结果在真实世界中的可转移性。  \n2. 现有方法未能全面评估不同测试模式对现实差距的影响。  \n3. 需要系统性分析以识别不同测试模式下的系统级失败。  \n\n【提出了什么创新的方法】  \n本研究提出了一种基于ROS的评估框架，比较了四种测试模式：SiL、ViL、MR和真实世界（RW）测试。通过在小规模平台上进行数百次测试，研究了各模式在行为、驱动和感知保真度上的表现。结果显示，MR测试在感知保真度上表现最佳，能够有效复制真实世界系统失败，超越了SiL和ViL。研究为理解和减少现实差距提供了新的见解，并为未来的自主驾驶系统验证提供了实用的方法论。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Trust and Human Autonomy after Cobot Failures: Communication is Key for Industry 5.0",
            "authors": "Felix Glawe,Laura Kremer,Luisa Vervier,Philipp Brauner,Martina Ziefle",
            "subjects": "Human-Computer Interaction (cs.HC); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22298",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22298",
            "arxiv_html_link": "https://arxiv.org/html/2509.22298v1",
            "abstract": "Collaborative robots (cobots) are a core technology of Industry 4.0. Industry 4.0 uses cyber-physical systems, IoT and smart automation to improve efficiency and data-driven decision-making. Cobots, as cyber-physical systems, enable the introduction of lightweight automation to smaller companies through their flexibility, low cost and ability to work alongside humans, while keeping humans and their skills in the loop. Industry 5.0, the evolution of Industry 4.0, places the worker at the centre of its principles: The physical and mental well-being of the worker is the main goal of new technology design, not just productivity, efficiency and safety standards. Within this concept, human trust in cobots and human autonomy are important. While trust is essential for effective and smooth interaction, the workers’ perception of autonomy is key to intrinsic motivation and overall well-being. As failures are an inevitable part of technological systems, this study aims to answer the question of how system failures affect trust in cobots as well as human autonomy, and how they can be recovered afterwards. Therefore, a VR experiment (n = 39) was set up to investigate the influence of a cobot failure and its severity on human autonomy and trust in the cobot. Furthermore, the influence of transparent communication about the failure and next steps was investigated. The results show that both trust and autonomy suffer after cobot failures, with the severity of the failure having a stronger negative impact on trust, but not on autonomy. Both trust and autonomy can be partially restored by transparent communication.",
            "introduction": "The fourth industrial revolution, Industry 4.0, refers to the interconnection between machines and production systems enabling increased efficiency and flexibility [1]. While the technologies driving Industry 4.0 are steadily being deployed in today’s production landscape, the European Commission put forth the vision of Industry 5.0. The main idea of Industry 5.0 is to develop a resilient, sustainable, and human-centred industry, harnessing the advances made within Industry 4.0 but to an end that goes beyond the sole increase in efficiency [2, 3]. In production, the physical and mental well-being of workers shall be achieved by providing a motivating and safe environment that meets workers’ needs, including factors such as autonomy, privacy and dignity. [4, 3, 5]. Industry 5.0 can therefore be viewed as the challenge of balancing human well-being and automation and its metrics of success [3].\n\nKey technologies of Industry 4.0 include cloud technologies, smart sensors, simulation, AI and advanced robotics, including collaborative robots (cobots) [6, 7]. Compared to traditional robots, cobots can operate in close proximity to a human partner and are designed to be highly flexible [8]. They are also designed to enhance the capabilities of human workers and not to replace them [9]. Due to their high flexibility, they can easily be adapted to the operators’ needs making them the ideal connecting element between Industry 4.0 and 5.0 [3]. Two measures are crucial to ensure the well-being of the worker and the effectiveness of the human-robot collaboration (HRC): appropriate trust in the collaboration and the satisfaction of the workers’ need for autonomy [10, 11]. Trust between a human and a robotic partner is necessary to ensure the adoption and appropriate use of the technology that enables the full potential of this relationship [12, 13, 14]. This is particularly evident in industrial workplaces, where cobots are increasingly employed. In such contexts, trust becomes even more critical, as the misuse or non-use of these cobots can compromise the safety of workers and also undermine the profitability and efficiency of the technology [15, 16, 17]. The need for autonomy, on the other hand, is a basic psychological need that must be satisfied to ensure the workers’ well-being and to foster intrinsic motivation [18]. However, designing the interaction between humans and robots (HRI) to preserve and foster human autonomy seems to be a non-trivial task. For example, Nikolova et al. 2022 reported a decrease in workers’ perceived autonomy with increasing levels of industrial robotisation. Nevertheless, it is theorized that robots could increase human autonomy by replacing dull, repetitive or even dangerous tasks [19, 20].\nThe question arises of how HRC must be designed to induce trust and fulfil the need for autonomy. One aspect that is expected to play a crucial role is the cobot’s behaviour after a mistake. In human-human relationships, mistakes and unexpected behaviours can occur due to misunderstanding or miscommunication, and in the case of technology, because of faulty software, hardware, or misuse [21]. However, these errors or failures can affect human trust in a cobot, and may also reduce human autonomy by compromising perceived system capabilities [10, 22]. To date, there is no consensus on the best strategies to restore trust after a failure and no study on the impact of failures on human autonomy. Therefore, it is important to investigate how failures in HRC affect human perception and how the cobot’s behaviour after a failure can restore trust and autonomy to allow for future optimal collaboration and the workers’ well-being. This article addresses the research question through a virtual reality (VR) experiment that examines the influence of failures and their severity on trust perception and autonomy satisfaction. Furthermore, it investigates whether transparent communication can serve as a potential way to restore trust and autonomy after a failure.\n\nIn the first part of the article, the theoretical foundations of trust and human autonomy and previous findings on the relationship between trust, autonomy, failures and transparent communication will be discussed. In the second part, we’ll derive the research question and hypotheses, and describe the method used to answer and test them. The results will be presented and discussed in the following two sections. Finally, we’ll discuss the findings and limitations of the study.",
            "llm_summary": "【论文的motivation是什么】  \n1. 如何在协作机器人（cobots）发生故障后恢复人类对其的信任。  \n2. 研究系统故障对人类自主感的影响及其恢复方法。  \n3. 探索透明沟通在恢复信任和自主感中的作用。  \n\n【提出了什么创新的方法】  \n本研究通过虚拟现实实验（n = 39）调查了cobot故障及其严重性对人类信任和自主感的影响。研究发现，cobot故障会导致信任和自主感下降，故障的严重性对信任的负面影响更大，但对自主感的影响较小。透明的沟通可以部分恢复信任和自主感。该研究为人机协作中的信任与自主感的维护提供了新的视角和方法。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning",
            "authors": "Jinkun Hao,Naifu Liang,Zhen Luo,Xudong Xu,Weipeng Zhong,Ran Yi,Yichen Jin,Zhaoyang Lyu,Feng Zheng,Lizhuang Ma,Jiangmiao Pang",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "Accepted by NeurIPS 2025; Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.22281",
            "code": "https://mesatask.github.io/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22281",
            "arxiv_html_link": "https://arxiv.org/html/2509.22281v1",
            "abstract": "The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability of task-relevant tabletop scenes for training.\nHowever, traditional methods for creating these scenes rely on time-consuming manual layout design or purely randomized layouts, which are limited in terms of plausibility or alignment with the tasks.\nIn this paper, we formulate a novel task, namely task-oriented tabletop scene generation, which poses significant challenges due to the substantial gap between high-level task instructions and the tabletop scenes.\nTo support research on such a challenging task, we introduce MesaTask-10K, a large-scale dataset comprising approximately 10,700 synthetic tabletop scenes with manually crafted layouts that ensure realistic layouts and intricate inter-object relations.\nTo bridge the gap between tasks and scenes, we propose a Spatial Reasoning Chain that decomposes the generation process into object inference, spatial interrelation reasoning, and scene graph construction for the final 3D layout.\nWe present MesaTask, an LLM-based framework that utilizes this reasoning chain and is further enhanced with DPO algorithms to generate physically plausible tabletop scenes that align well with given task descriptions.\nExhaustive experiments demonstrate the superior performance of MesaTask compared to baselines in generating task-conforming tabletop scenes with realistic layouts.",
            "introduction": "A fundamental challenge in robotic manipulation is enabling robots to accurately interpret human instructions and successfully execute complex tasks accordingly.\nThe conventional pipeline for achieving this involves task definition, simulatable tabletop scene construction, and policy training.\nHowever, traditional scene construction methods, which rely on manual design or purely randomized layouts, are limited by their labor-intensive nature and the resulting constraints on diversity and plausibility, ultimately hindering the generalization of learned policies.\nTherefore, automatic task-oriented tabletop scene generation emerges as a promising approach for effectively bridging the gap between task descriptions and scenes.\nCrucially, tabletop scene generation must satisfy three key requirements: covering task variables, enabling scene interactivity, and ensuring realistic layouts, thereby facilitating the learning of robust policies.\n\nExisting scene generation methods Wang et al. (2024); Dai et al. (2024); Huang et al. (2024) often start from a single scene image and attempt to recover the corresponding tabletop scene through object retrieval and layout optimization.\nUnfortunately, their ability to understand under-specified task instructions still requires empirical corroboration.\nOther approaches Yang et al. (2024f, e); Çelen et al. (2024) leverage powerful language models (LLMs) to interpret task prompts and then synthesize tabletop scenes in a zero-shot manner.\nNevertheless, these methods are hindered by inherent limitations, no matter the inevitable occlusion in scene images or the lack of fine-tuning on a scene dataset, which significantly impede the modeling of realistic table layouts and complex inter-object relations, such as stacking and containment, within the scene.\nAs a result, task-oriented tabletop scene generation remains a challenging problem due to the scarcity of datasets and the substantial gap between task instructions and scene layouts.\n\nTo tackle these challenges, we collect a first-of-its-kind dataset of synthetic tabletop scenes with manually crafted layouts, dubbed MesaTask-10K.\nAs shown in Figure 1, our dataset comprises approximately 10,70010,700 diverse tabletop scenes, spanning six common indoor table categories, including office tables, dining tables, kitchen counters, and more.\nThe 3D objects in MesaTask-10K originate from a large asset library containing over 12,00012,000 rigid and articulated 3D assets, each with detailed semantic information, such as object category, description, and materials, and featuring a comprehensive taxonomy of over 200 object classes on the tables.\nAs claimed in Wang et al. (2024), pretrained 2D image generative models better capture scene and object configurations both at the scene level and in fine-grained inter-object relations.\nInspired by this, our dataset is built upon diverse tabletop scene images with diversity and realistic layouts, generated by a large text-to-image model Labs (2024) pretrained on massive internet data.\nTo obtain a coarse layout from the scene image, we estimate the depth Yang et al. (2024c) of each image, extract the instance point cloud, and acquire the 3D bounding box of objects.\nWe then leverage the object descriptions labeled by VLM Achiam et al. (2023) to retrieve suitable 3D assets from the library and construct an initial replica of the tabletop scenes.\nSubsequently, human annotators meticulously refine these 3D layouts, adjusting the object size and positions as per the image prompt, addressing inaccuracies from occlusion and ensuring complex inter-object relations.\nUltimately, all the scenes are put into a physical simulator, IsaacSim NVIDIA (2024), to prevent object collisions.\n\nConfronted with the significant gap between tasks and scenes, we propose a novel paradigm referred to as Spatial Reasoning Chain, decomposing task-to-tabletop scene generation into a structured chain of thought (CoT).\nGiven a high-level task description, this chain of thought begins with the inference of requisite objects, accompanied by their semantic attributes and spatial interrelations, based on which a complete scene graph is formed, and finally leads to a concrete 3D layout of objects on the table.\nTo establish trainable spatial reasoning chains with our dataset, we design a set of delicate rules to extract the object attributes and inter-object relations, thus forming a scene graph for each tabletop scene.\nSubsequently, we leverage a multimodal large language model, taking scene graphs and rendered scene images as input, to generate corresponding task information and detailed spatial reasoning descriptions for training.\n\nThanks to our structured reasoning chains, it’s convenient to empower LLM with 3D spatial reasoning and scene generation capability.\nIn this paper, we propose MesaTask, a novel LLM-based framework for task-oriented tabletop scene generation.\nSpecifically, we initially employ the supervised fine-tuning (SFT) strategy on our constructed reasoning data to inject the LLM with 3D spatial reasoning capabilities.\nHowever, MesaTask occasionally generates unsatisfactory tabletop scenes with minor object collisions and misalignment with the given task.\nTo circumvent this hurdle, we devise paired training data and leverage a conventional RL algorithm, namely Direct Preference Optimization (DPO), to boost our MesaTask model, thereby ensuring that the generated scenes are devoid of object collisions and exhibit improved conformity with the provided task descriptions.\n\nFor a more comprehensive performance assessment, we leverage powerful VLMs to evaluate the rendered scene images from multiple perspectives, including task-scene alignment, physical viability, scene layout plausibility, etc.\nThrough extensive experiments, our MesaTask framework is capable of generating physically plausible tabletop scenes with realistic layouts, outperforming baseline methods in terms of FID, VLM-based metrics, and the user study.\nIn particular, our generated tabletop scenes strictly conform to given task instructions and exhibit rich inter-object relations, such as stacking or containing.\nIn summary, our contributions are threefold:\n\nWe pioneer the formulation of Task-to-Scene generation task, which aims to generate physically plausible tabletop scenes directly from high-level task descriptions.\n\nWe introduce MesaTask-10k, a large-scale tabletop scene dataset with human-crafted realistic layouts, characterized by rich inter-object relations and a tremendous amount of synthetic 3D object assets.\n\nAlong with the delicate design of spatial reasoning chains, we propose MesaTask, an LLM-based framework endowed with the capability of 3D spatial reasoning and tabletop scene generation, achieving superior performance across various evaluation criteria.\n\n1. We pioneer the formulation of Task-to-Scene generation task, which aims to generate physically plausible tabletop scenes directly from high-level task descriptions.\n\n2. We introduce MesaTask-10k, a large-scale tabletop scene dataset with human-crafted realistic layouts, characterized by rich inter-object relations and a tremendous amount of synthetic 3D object assets.\n\n3. Along with the delicate design of spatial reasoning chains, we propose MesaTask, an LLM-based framework endowed with the capability of 3D spatial reasoning and tabletop scene generation, achieving superior performance across various evaluation criteria.",
            "llm_summary": "【论文的motivation是什么】  \n1. 传统的场景生成方法依赖手动设计或随机布局，效率低且缺乏多样性。  \n2. 任务描述与桌面场景之间存在显著差距，导致生成的场景不符合任务要求。  \n3. 现有方法无法有效理解不明确的任务指令，限制了场景生成的真实性和复杂性。  \n\n【提出了什么创新的方法】  \n本文提出了MesaTask，一个基于LLM的框架，旨在通过空间推理链将高层任务描述转化为物理上合理的桌面场景。该方法首先进行对象推理、空间关系推理，然后构建场景图，最终生成3D布局。通过引入MesaTask-10K数据集，框架在生成符合任务要求的桌面场景方面表现优越，超越了基线方法，确保生成的场景不仅符合任务描述，还具备丰富的对象间关系，如堆叠和包含。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Human Autonomy and Sense of Agency in Human-Robot Interaction: A Systematic Literature Review",
            "authors": "Felix Glawe,Tim Schmeckel,Philipp Brauner,Martina Ziefle",
            "subjects": "Human-Computer Interaction (cs.HC); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22271",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22271",
            "arxiv_html_link": "https://arxiv.org/html/2509.22271v1",
            "abstract": "Human autonomy and sense of agency are increasingly recognised as critical for user well-being, motivation, and the ethical deployment of robots in human-robot interaction (HRI). Given the rapid development of artificial intelligence, robot capabilities and their potential to function as colleagues and companions are growing. This systematic literature review synthesises 22 empirical studies selected from an initial pool of 728 articles published between 2011 and 2024. Articles were retrieved from major scientific databases and identified based on empirical focus and conceptual relevance, namely, how to preserve and promote human autonomy and sense of agency in HRI. Derived through thematic synthesis, five clusters of potentially influential factors are revealed: robot adaptiveness, communication style, anthropomorphism, presence of a robot and individual differences. Measured through psychometric scales or the intentional binding paradigm, perceptions of autonomy and agency varied across industrial, educational, healthcare, care, and hospitality settings. The review underscores the theoretical differences between both concepts, but their yet entangled use in HRI. Despite increasing interest, the current body of empirical evidence remains limited and fragmented, underscoring the necessity for standardised definitions, more robust operationalisations, and further exploratory and qualitative research. By identifying existing gaps and highlighting emerging trends, this review contributes to the development of human-centered, autonomy-supportive robot design strategies that uphold ethical and psychological principles, ultimately supporting well-being in human-robot interaction.",
            "introduction": "Human-Robot Interaction (HRI) is a well-studied subject. From robots designed to act as colleagues in assembly tasks, simple cleaning robots, and to pet robots used in care settings, robots are envisioned and studied for a broad range of applications [1, 2, 3]. Robots that are designed to work with or accompany humans must be adapted to the humans’ diverse needs for effective and human-centred interaction and collaboration [4, 5].\n\nNumerous studies have examined how HRI and robots must be modelled and designed to foster user experience, acceptance of robots, trust in robots, or other context-specific goal variables like increased learning or even health benefits [6, 7, 8, 9, 10].\nWhile acceptance of robots and the intended outcome of the HRI are essential for the effective deployment of this technology, this focus neglects ethical concerns.\n\nIn 2017, the Institute of Electrical and Electronics Engineers (IEEE), the largest technical professional association in the world, published the document ”Ethically Aligned Design” to foster discourse and the development of autonomous technology that aligns with specified ethical design principles [11]. One central principle is the optimisation of technology for human well-being. They argue that without well-being as a central aspect of technology design, even the best design intentions could have serious negative consequences. One of the critical aspects of designing technology that upholds human well-being is the issue of maintaining and encouraging human autonomy.\n\nThe concept of human autonomy has a long history as a basic ethical principle, advanced for example by the highly influential groundwork of Immanuel Kant, who described autonomy as freedom of action and choice, without being subject to the control of others or to one’s own inclinations [12, 13].\n\nBesides human autonomy as an ethical principle, autonomy is also identified as a basic psychological need (BPN) in the well-known self-determination theory (SDT) by Ryan and Deci [14]. In SDT, autonomy and the psychological needs for competence and relatedness serve as prerequisites of intrinsic motivation, which in turn is an important factor for well-being. This connection was empirically verified for numerous contexts and across cultures or demographic variables [15]. They define the need for autonomy as the experience of willingness and volition as well as the sense of integrity when actions, feelings or thoughts are authentic and self-endorsed. In contrast, the frustration of autonomy leads to a sense of conflict and pressure [16].\n\nAt this point, it is important to highlight the difference between machine and human autonomy. Machines feature autonomy while humans are in a state of autonomy, but do not possess it [17]. Furthermore, human autonomy does not hinge on complete independence; it builds upon relationships and acknowledges the fact that a human can never be fully independent of others [18]. A well-trusted employer, who recognises and shares the goals and values of his or her employees, can foster autonomy satisfaction, and at the same time, be clear in his or her demands [18, 19]. Human autonomy cannot be oversatisfied [18]. In contrast, there exists an automation conundrum where higher machine autonomy relates to decreased situational awareness and the ability to take over control of a human operator [20]. Therefore, balancing human and machine autonomy is a crucial task.\n\nWith the importance of human autonomy in mind, it is evident why human autonomy serves as a foundational concept in human-computer interaction (HCI) and why the design for human autonomy and psychological needs in general should be placed at the centre of robot design as well [21].\n\nHowever, SDT and BPNs are rarely applied to the context of HRI, although evidence by Nikolova et al. suggests that robotisation at work in its current form might even lead to lowered autonomy, relatedness and meaningfulness [21, 22]. In contrast, theoretical considerations by Formosa highlight that robots could improve autonomy by providing the user with more valuable ends, improved autonomy competencies and more authentic choices [23].\n\nBased on SDT, Peters et al. published the influential METUX framework in 2018 to guide HCI researchers and developers to design technology for well-being based on the psychological needs of autonomy, relatedness and competence [18]. In their framework, they establish the idea of six spheres of experience in all of which the needs can be experienced and affected: adoption, interface, task, behaviour, life and society. The adoption sphere refers to the experience of deciding on the use of a new technology, the interface sphere to the interaction with the technology via its interface, the task sphere to the experience of the task carried out with or by the technology, the behaviour sphere to the overarching behaviour supported or induced by the technology (e.g. cooking or sleep tracking), the life sphere to the experience of how the technology and the induced behaviour shapes the user’s life in general and the society sphere concerned with how the society in general experiences the effects of the technology.\n\nBased on the METUX framework, Janssen and Schadenberg developed the first guidelines for well-being-oriented social robot design [21]. They based these guidelines on empirical studies in HRI concerned with related concepts like empowerment, perceived freedom or self-efficacy. However, they acknowledge that only a few articles have applied theories of well-being and motivation in HRI research.\n\nFor each sphere of experience (except the sphere of adoption and society), they define and give examples of how the individual psychological needs of relatedness, autonomy, and competence can be satisfied in social robot design. Because this literature review is concerned with autonomy, we will only summarise the recommendations given to foster autonomy:\n\nInterface Sphere:\n\nInterface Sphere:\n\nProvide choice and options for interaction modes\n\nApply non-controlling communication techniques\n\nTask Sphere:\n\nProvide choices of tasks and task characteristics performed by the robot\n\nAcknowledge negative feelings towards a task\n\nProvide a meaningful rationale for tasks\n\nBehaviour Sphere:\n\nEmpower decision about type, difficulty and frequency of (users’) behaviour\n\nAcknowledge negative feelings towards a behaviour\n\nProvide meaningful rationale for behaviour\n\nLife Sphere:\n\nPrevent excessive engagement and overreliance on the robot\n\nAssess whether users still act in line with their own values.\n\nJansen and Schadenberg describe their work as a call to action to implement psychological-need-fostering measures when designing robots and to use the framework as a reference point from which to start structured and increased research efforts [21].\n\nThe sense of agency, defined as the ”feeling of control over actions and their consequences” [24, p.1], is often used and treated as a concept synonymous with or highly entangled with autonomy, especially in HCI [25]. Autonomy and agency are even described as ”twin concepts,” highlighted by a shared focus on aspects of self-causality, perception of control, identity, and material independence [26, 25]. Both are valued as intrinsically good and contribute to physical and mental well-being [25, 24]. While the concept of the need for autonomy is grounded in motivational psychology, the sense of agency is often approached from a neuroscientific perspective. Wegner and Wheatley established the Model of Conscious Will, which explains how unconscious causes of our thoughts and actions lead to the actual thoughts and actions we exhibit [27]. While our sense of agency arises from our consciously experienced thoughts and the subsequently performed actions, the real causes of these actions are not our thoughts but the unconscious triggers that occur before we actively think about the intended actions [24].\n\nIn HCI, the sense of agency is acknowledged as an important variable in interface design [24]. Technology should allow users to feel agentic (i.e., in control). However, this conflicts with the rising automation in areas such as self-driving cars and flight assistance systems.\n\nConceptually, one could describe the sense of agency as a prerequisite for satisfying our need for autonomy: without the sense that we are the agents of our actions, it is likely that we do not perceive those actions as being in line with our values and identity [28]. This potential connection, however, remains to be proven. Regardless, in this work, we will apply both concepts to account for their potentially synonymous use and will try to shed light on how they are applied in HRI.\n\nWith this study, we seek to repeat the call to action of Janssen and Schadenberg and to add an empirical base to the theoretical frameworks with a focus on autonomy and agency.\nTherefore, we will identify and aggregate the empirical research on how to maintain and foster human autonomy and agency in HRI and try to identify trends in this research area. The findings of this study will serve as a foundation for future research and as a point of departure for developers and policymakers who seek to integrate human autonomy as a central psychological and ethical principle into the design of HRI.\n\nThe article is structured as follows: First, we describe our methodological approach based on the PRISMA workflow, including the utilised databases, deployed keywords, the process of article identification, characteristics of the final dataset, and the article analysis [29]. Second, we present the results of the analysis. Finally, we discuss the results and the limitations of this study and derive recommendations for subsequent research.\n\n1. Interface Sphere:\n\n2. Provide choice and options for interaction modes\n\n3. Apply non-controlling communication techniques\n\n4. Task Sphere:\n\n5. Provide choices of tasks and task characteristics performed by the robot\n\n6. Acknowledge negative feelings towards a task\n\n7. Provide a meaningful rationale for tasks\n\n8. Behaviour Sphere:\n\n9. Empower decision about type, difficulty and frequency of (users’) behaviour\n\n10. Acknowledge negative feelings towards a behaviour\n\n11. Provide meaningful rationale for behaviour\n\n12. Life Sphere:\n\n13. Prevent excessive engagement and overreliance on the robot\n\n14. Assess whether users still act in line with their own values.\n\n1. Provide choice and options for interaction modes\n\n2. Apply non-controlling communication techniques\n\n1. Provide choices of tasks and task characteristics performed by the robot\n\n2. Acknowledge negative feelings towards a task\n\n3. Provide a meaningful rationale for tasks\n\n1. Empower decision about type, difficulty and frequency of (users’) behaviour\n\n2. Acknowledge negative feelings towards a behaviour\n\n3. Provide meaningful rationale for behaviour\n\n1. Prevent excessive engagement and overreliance on the robot\n\n2. Assess whether users still act in line with their own values.",
            "llm_summary": "【论文的motivation是什么】  \n1. 人类自主性和代理感在机器人与人类互动中对用户的幸福感和动机至关重要。  \n2. 现有的实证研究数量有限且分散，缺乏标准化定义和更强的操作化。  \n3. 需要开发以人类为中心、支持自主性的机器人设计策略，以促进伦理和心理原则。  \n\n【提出了什么创新的方法】  \n本文通过系统文献综述，分析了22项实证研究，揭示了影响人类自主性和代理感的五个关键因素：机器人适应性、沟通风格、人性化、机器人存在感和个体差异。通过心理测量量表和意向绑定范式，研究发现不同应用场景下人们对自主性和代理感的感知存在差异。该研究为未来的HRI设计提供了理论基础和实证支持，强调了将人类自主性作为设计核心的重要性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach",
            "authors": "Seoyoung Lee,Seonbin Yoon,Seongbeen Lee,Hyesoo Kim,Joo Yong Sim",
            "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Multiagent Systems (cs.MA); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22137",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22137",
            "arxiv_html_link": "https://arxiv.org/html/2509.22137v1",
            "abstract": "GUI task automation streamlines repetitive tasks, but existing LLM or VLM-based planner-executor agents suffer from brittle generalization, high latency, and limited long-horizon coherence. Their reliance on single-shot reasoning or static plans makes them fragile under UI changes or complex tasks. Log2Plan addresses these limitations by combining a structured two-level planning framework with a task mining approach over user behavior logs, enabling robust and adaptable GUI automation. Log2Plan constructs high-level plans by mapping user commands to a structured task dictionary, enabling consistent and generalizable automation. To support personalization and reuse, it employs a task mining approach from user behavior logs that identifies user-specific patterns. These high-level plans are then grounded into low-level action sequences by interpreting real-time GUI context, ensuring robust execution across varying interfaces. We evaluated Log2Plan on 200 real-world tasks, demonstrating significant improvements in task success rate and execution time. Notably, it maintains over 60.0% success rate even on long-horizon task sequences, highlighting its robustness in complex, multi-step workflows.",
            "introduction": "Large Language Models (LLMs) now demonstrate such strong capabilities in natural–language understanding and generation that they are being adopted as general purpose agents for personal assistance, software automation, and GUI testing(Raffel et al., 2020; Nguyen et al., 2024). In particular, desktop based agents that can correctly interpret user commands and act inside real software environments have become a key research frontier(Gao et al., 2024; Zhang et al., 2024a; Qin et al., 2025; AGI, 2025).\n\nExisting desktop agent approaches fall into three families: (i) Prompt based zero/ few shot agents exploit pretrained models directly, yielding quick and flexible responses but suffering brittle behavior on multi step tasks(Yao et al., 2023; Yang et al., 2023; Shinn et al., 2023); (ii) Task specific fine tuned models deliver high accuracy within narrow domains yet demand costly retraining and scale poorly to unseen tasks(Li et al., 2020; Hong et al., 2024; Li et al., 2024); (iii) Planner–executor agents couple an LLM planner with external tools, handling complex workflows at the price of engineering overhead and context window explosion(Zhang et al., 2024a; Gao et al., 2024; Qin et al., 2025).\n\nDespite their differences, all three approaches share fundamental limitations in real world GUI environments. They rely on static planners or rigid pipelines, making them fragile when layouts change or personalized reasoning is required. While task mining and RPA research highlight the value of recurring structures in user logs(Choi et al., 2022; Krosnick and Oney, 2022; Huang et al., 2024a; Riva and Kace, 2021), existing agents ignore this opportunity. Replaying-based methods further show that small visual changes cause high failure rates(Deka et al., 2017; Riva and Kace, 2021). Moreover, vision encoding at each step inflates latency, while long plans overflow the model’s context window(Hong et al., 2024).\n\nLog2Plan addresses these gaps with a planning centric LLM framework that bridges symbolic task mining and retrieval augmented generation. It leverages user logs to learn and reuse generalizable task structures(Redis et al., 2024; Yuan et al., 2024). Concretely, Log2Plan (i) segments raw GUI logs into semantically meaningful action–object units, and (ii) mines frequently repeated patterns to synthesize modular automation flows. Compared with record and replay baselines, this yields markedly better generalization and interpret-ability. At runtime, Log2Plan dynamically retrieves and recombines prior workflows to generate robust plans for new commands, enabling fast adaptation without model fine-tuning. By separating high-level intent from low-level execution, the LLM operates over structured, behavior-grounded representations, while a lightweight executor handles GUI interactions.\n\nOur contributions are three fold:\n\nWe formularize GUI interactions as action–object pairs, enabling consistent and interpretable planning from both natural language commands and interaction logs.\n\nWe identify recurring user behaviors in interaction logs and compose them into modular automation units that can be flexibly reused across diverse tasks.\n\nWe propose a two level architecture that cleanly separates user intent from execution, allowing LLMs to plan in a robust and scalable manner.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的LLM或VLM基础的规划执行代理在复杂任务中表现出脆弱的泛化能力。  \n2. 静态计划的依赖使得现有方法在用户界面变化时变得脆弱。  \n3. 现有方法在处理长时间任务序列时成功率低且延迟高。  \n\n【提出了什么创新的方法】  \nLog2Plan提出了一种结合结构化两级规划框架和任务挖掘方法的自适应GUI自动化框架。该方法首先通过将用户命令映射到结构化任务字典来构建高层计划，并通过分析用户行为日志识别用户特定模式以支持个性化和重用。接着，Log2Plan根据实时GUI上下文将高层计划转化为低层行动序列，从而确保在不同界面上的稳健执行。通过在200个真实任务上的评估，Log2Plan显著提高了任务成功率和执行时间，尤其在长时间任务序列中保持超过60.0%的成功率，展示了其在复杂多步骤工作流中的稳健性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics",
            "authors": "Saurav Jha,Stefan K. Ehrlich",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.22014",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.22014",
            "arxiv_html_link": "https://arxiv.org/html/2509.22014v1",
            "abstract": "Healthcare robotics requires robust multimodal perception and reasoning to ensure safety in dynamic clinical environments. Current Vision-Language Models (VLMs) demonstrate strong general-purpose capabilities but remain limited in temporal reasoning, uncertainty estimation, and structured outputs needed for robotic planning. We present a lightweight agentic multimodal framework for video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model with a SmolAgent-based orchestration layer, it supports chain-of-thought reasoning, speech–vision fusion, and dynamic tool invocation. The framework generates structured scene graphs and leverages a hybrid retrieval module for interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark and a custom clinical dataset show competitive accuracy and improved robustness compared to state-of-the-art VLMs, demonstrating its potential for applications in robot-assisted surgery, patient monitoring, and decision support.",
            "introduction": "Robotics in healthcare has emerged as a critical domain where perception, reasoning, and safe decision-making intersect with high-stakes clinical applications. From robot-assisted surgery [1], to autonomous patient monitoring [2], and collaborative care robots [3], the demand for systems that can robustly interpret complex multimodal environments continues to grow. A central requirement across these applications is scene understanding—the ability to identify objects, infer spatial and temporal relations, and generate structured representations that inform safe robotic actions [4].\n\nRecent advances in Vision-Language Models (VLMs) such as Llava [5] demonstrate strong multimodal reasoning capabilities. These systems have achieved remarkable performance in tasks such as visual question answering, image captioning, and document analysis. However, their application in robotics, especially in healthcare, faces critical challenges. First, most VLMs operate as monolithic end-to-end pipelines, limiting flexibility, explainability, and integration with robotic control loops [6]. Second, temporal and spatial reasoning remains underdeveloped, hindering the accurate interpretation of dynamic surgical or clinical environments [7]. Third, the lack of structured outputs (e.g. scene graphs) complicates downstream integration into robotic planning frameworks [5, 8]. Finally, deployment in clinical workflows is often obstructed by the high computational cost of large proprietary models and unresolved concerns about data privacy [9].\n\nWithin the healthcare domain, these shortcomings are especially problematic. For example, surgical robots require interpretable scene representations to coordinate tool trajectories with evolving anatomical contexts. Assistive robots in hospitals must combine perception with reasoning to safely navigate dynamic environments involving patients and caregivers. Clinical decision-support robots further demand uncertainty estimation and fallback strategies to avoid unsafe actions under ambiguity.\n\nTo address these challenges, we present a leightweight modular agentic multimodal framework designed for video-based scene understanding in clinical and robotic contexts. By integrating the Qwen2.5-VL-3B-Instruct model with a SmolAgent-based orchestration layer, our framwork combines chain-of-thought reasoning, speech-vision fusion, and structured scene graph generation. In addition, a hybrid retrieval mechanism (LightRAG [10]) enables both efficient and interpretable knowledge integration. Together, these components aim to bridge the gap between raw perception and symbolic planning, enabling healthcare robots to act more safely, transparently, and adaptively in high-stakes environments.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的Vision-Language Models在动态临床环境中的时间推理和不确定性估计能力有限。  \n2. 需要生成结构化输出（如场景图）以便于机器人规划的集成。  \n3. 高计算成本和数据隐私问题阻碍了临床工作流中的部署。  \n\n【提出了什么创新的方法】  \n提出了一种轻量级的模块化代理多模态框架，专为临床和机器人场景理解设计。该框架结合了Qwen2.5-VL-3B-Instruct模型与SmolAgent的编排层，支持链式思维推理、语音-视觉融合和结构化场景图生成。此外，采用混合检索机制（LightRAG）实现高效且可解释的知识集成。通过在Video-MME基准和自定义临床数据集上的评估，展示了相较于现有VLM的竞争性准确性和增强的鲁棒性，表明该框架在机器人辅助手术、患者监测和决策支持等应用中的潜力。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation",
            "authors": "Jiahui Wang,Changhao Chen",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "Accepted as a poster in NeurIPS 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.21930",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21930",
            "arxiv_html_link": "https://arxiv.org/html/2509.21930v1",
            "abstract": "Visual navigation is essential for robotics and embodied AI. However, existing foundation models, particularly those with transformer decoders, suffer from high computational overhead and lack interpretability, limiting their deployment in resource-tight scenarios.\nTo address this, we propose DynaNav, a Dynamic Visual Navigation framework that adapts feature and layer selection based on scene complexity. It employs a trainable hard feature selector for sparse operations, enhancing efficiency and interpretability. Additionally, we integrate feature selection into an early-exit mechanism, with Bayesian Optimization determining optimal exit thresholds to reduce computational cost.\nExtensive experiments in real-world-based datasets and simulated environments demonstrate the effectiveness of DynaNav. Compared to ViNT, DynaNav achieves a 2.26×2.26\\times reduction in FLOPs, 42.3% lower inference time, and 32.8% lower memory usage, while improving navigation performance across four public datasets.",
            "introduction": "Visual navigation is a fundamental capability for robotics and embodied AI, enabling autonomous agents to perceive, interpret, and navigate complex 3D environments based on visual inputs [1, 2, 3]. Its applications span real-world scenarios, such as delivery and logistics, as well as virtual domains, including gaming and simulation. By bridging perception and action, visual navigation plays a crucial role in intelligent systems.\nRecently, there has been growing interest in developing foundation models for visual navigation [4, 5, 6, 7, 8, 9, 10, 1]. ViNT [5] is a notable example that learns from large-scale egocentric observations using transformer layers on CNN-extracted features, demonstrating strong generalization across robotic platforms and environments. NoMad [6] further builds on this by incorporating a diffusion policy and a goal-masking mechanism. PixNav [9] utilizes textual heuristics and large language models(LLMs) to explore zero-shot possibility. However, these approaches, particularly those relying on deep neural architectures such as transformer decoders, introduce significant computational overhead, posing challenges for edge deployment where efficiency is paramount.\n\nRobotic applications demand greater efficiency than large cloud-based models. As the trend toward efficient foundation models continues [11, 12], reducing the computational burden of visual navigation models is a key challenge. Additionally, existing models function as \"black boxes,\" raising concerns about interpretability. As humans and intelligent agents increasingly coexist, explainability becomes essential. These challenges lead to two critical research questions:\n\nIs it necessary to activate all transformer layers for every navigation scenario?\n\nWhich features are most important in the decoding process, and can we identify the most salient regions or pixels for navigation?\n\nTo this end, we propose DynaNav, a highly efficient Dynamic Visual Navigation framework that adaptively selects relevant features and neural layers based on visual observations. Our approach employs a trainable hard feature selector to create sparse representations, enabling computationally efficient sparse operations at the feature level.\nThis dynamic feature masking not only lowers computational overhead but also improves the understanding of which regions more relevantly influence the inference of visual navigation models, thereby enhancing explainability. Additionally, we introduce an early-exit strategy for deep Transformer layers by integrating feature selection into the early-exit mechanism, improving stability and computational efficiency. After training the decoder, Bayesian Optimization determines optimal early-exit thresholds. During inference, if a layer’s feature meets its threshold, computation terminates early, significantly reducing overall computational cost.\nExtensive experiments on real-world datasets and in simulated environments demonstrate the effectiveness of our proposed DynaNav. Compared to ViNT [5], DynaNav achieves a 2.26×2.26\\times reduction in FLOPs, 42.3% lower inference time, and 32.8% lower memory usage while improving navigation performance across four public datasets.\nTo the best of our knowledge, this is the first work to introduce dynamic network mechanisms to visual navigation models. To sum up, the main contributions of our work can be summarized as follows:\n\nWe propose DynaNav, a highly efficient and effective dynamic neural model for visual navigation, introducing a novel feature and layer selection strategy to improve efficiency without compromising performance.\n\nWe integrate sparse feature selection into the early exit mechanism, improving the stability and success rate of dynamic layer inference, while the visualized mask enhances the interpretability of the navigation decision process.\n\nExtensive experiments and simulations demonstrate that DynaNav achieves more than twice the efficiency while maintaining comparable success rates.\n\n1. Is it necessary to activate all transformer layers for every navigation scenario?\n\n2. Which features are most important in the decoding process, and can we identify the most salient regions or pixels for navigation?\n\n1. We propose DynaNav, a highly efficient and effective dynamic neural model for visual navigation, introducing a novel feature and layer selection strategy to improve efficiency without compromising performance.\n\n2. We integrate sparse feature selection into the early exit mechanism, improving the stability and success rate of dynamic layer inference, while the visualized mask enhances the interpretability of the navigation decision process.\n\n3. Extensive experiments and simulations demonstrate that DynaNav achieves more than twice the efficiency while maintaining comparable success rates.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有视觉导航模型在资源有限的场景中计算开销过大，限制了其应用。  \n2. 现有模型缺乏可解释性，难以理解模型决策过程。  \n\n【提出了什么创新的方法】  \nDynaNav是一个动态视觉导航框架，根据场景复杂性自适应选择特征和层。它采用可训练的硬特征选择器进行稀疏操作，提升了效率和可解释性。此外，DynaNav将特征选择整合进早退出机制，通过贝叶斯优化确定最佳退出阈值以降低计算成本。实验结果表明，DynaNav在四个公共数据集上相比ViNT实现了2.26倍的FLOPs减少，推理时间降低42.3%，内存使用降低32.8%，同时提升了导航性能。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-29 02:27:52",
            "title": "ShipwreckFinder: A QGIS Tool for Shipwreck Detection in Multibeam Sonar Data",
            "authors": "Anja Sheppard,Tyler Smithline,Andrew Scheffer,David Smith,Advaith V. Sethuraman,Ryan Bird,Sabrina Lin,Katherine A. Skinner",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO); Image and Video Processing (eess.IV)",
            "comment": "Accepted to OCEANS 2025 Great Lakes",
            "pdf_link": "https://arxiv.org/pdf/2509.21386",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.21386",
            "arxiv_html_link": "https://arxiv.org/html/2509.21386v1",
            "abstract": "In this paper, we introduce ShipwreckFinder, an open-source QGIS plugin that detects shipwrecks from multibeam sonar data. Shipwrecks are an important historical marker of maritime history, and can be discovered through manual inspection of bathymetric data. However, this is a time-consuming process and often requires expert analysis. Our proposed tool allows users to automatically preprocess bathymetry data, perform deep learning inference, threshold model outputs, and produce either pixel-wise segmentation masks or bounding boxes of predicted shipwrecks. The backbone of this open-source tool is a deep learning model, which is trained on a variety of shipwreck data from the Great Lakes and the coasts of Ireland. Additionally, we employ synthetic data generation in order to increase the size and diversity of our dataset. We demonstrate superior segmentation performance with our open-source tool and training pipeline as compared to a deep learning-based ArcGIS toolkit and a more classical inverse sinkhole detection method. The open-source tool can be found at https://github.com/umfieldrobotics/ShipwreckFinderQGISPlugin.",
            "introduction": "Recent advances in acoustic sensor technology and marine survey platforms have enabled efficient large area data collection to deliver massive amounts of data to marine scientists. For example, high-resolution mapping projects such as Lakebed 2030 [1] aim to fully map the seafloor of the Great Lakes in the next half decade, greatly increasing the amount of publicly available data. However, this data has yet to be fully leveraged for training machine learning models.\n\nSunken objects such as shipwrecks and airplanes hold important archaeological, historical, and environmental data. Finding shipwrecks in large-area seafloor surveys is a time-consuming task. Typically, this is done by hand with experts who manually inspect statistical anomalies in the data and cross-reference historical shipwreck logs [2]. In the past decade, interest in algorithmic and deep learning approaches to shipwreck detection has increased. A variety of sensors have been used for this problem, ranging from Multibeam Echosounder (MBES) [3, 4, 5] and backscatter data [6], to orbital ocean imagery taken from satellites [7], to sidescan sonar [8], to bathymetry from LiDaR [9]. However, these existing approaches often perform poorly on out-of-distribution data, still require expert oversight, and are rarely open-source.\n\nIn this work, we develop a machine learning-based tool to automate the detection of shipwreck sites from multibeam sonar data (see Fig. 1). Our tool, ShipwreckFinder, is designed to have seamless integration into QGIS [10], a freely available Geographic Information System (GIS) platform, to enable visualization and geo-referencing for detected shipwrecks. We train and validate a shipwreck segmentation model using existing data from Thunder Bay National Marine Sanctuary (TBNMS) [11] and deeper water data collected by the National Oceanic and Atmospheric Administration (NOAA) [12] and Integrated Mapping for the Sustainable Development of Ireland’s Marine Resource (INFOMAR) [13]. ShipwreckFinder aims to improve access to state-of-the-art machine learning methods within the marine archaeology community through an open-source toolset for automatic shipwreck detection.\n\nUltimately, the ShipwreckFinder tool has great potential to reduce the time and cost required to detect archaeological sites from multibeam sonar data collected across our lakes and oceans, accelerating the timeline for discoveries to be made by the scientific community and shared with the public.",
            "llm_summary": "【论文的motivation是什么】  \n1. 现有的船舶沉没物检测方法耗时且依赖专家分析。  \n2. 现有算法在处理分布外数据时表现不佳，缺乏开源工具。  \n3. 需要提高海洋考古学界对机器学习方法的访问和应用。  \n\n【提出了什么创新的方法】  \n本文提出了ShipwreckFinder，一个开源的QGIS插件，自动检测多波束声纳数据中的船舶沉没物。该工具通过预处理海底地形数据、执行深度学习推理、阈值模型输出，生成像素级分割掩膜或预测的船舶沉没物边界框。使用来自大湖区和爱尔兰海岸的多样化沉没物数据进行训练，并通过合成数据生成增强数据集的规模和多样性。实验结果表明，ShipwreckFinder在分割性能上优于现有的深度学习ArcGIS工具和经典的逆沉洞检测方法，显著提高了检测效率，降低了成本。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        }
    ],
    "2025-09-30": []
}