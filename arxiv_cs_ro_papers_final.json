{
    "2025-09-24": [
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies",
            "authors": "Lars Ankile,Zhenyu Jiang,Rocky Duan,Guanya Shi,Pieter Abbeel,Anusha Nagabandi",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19301",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19301",
            "arxiv_html_link": "https://arxiv.org/html/2509.19301v1",
            "abstract": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor control policies. However, these approaches are limited by the quality of human demonstrations, the manual effort required for data collection, and the diminishing returns from increasing offline data.\nIn comparison, reinforcement learning (RL) trains an agent through autonomous interaction with the environment and has shown remarkable success in various domains. Still, training RL policies directly on real-world robots remains challenging due to sample inefficiency, safety concerns, and the difficulty of learning from sparse rewards for long-horizon tasks, especially for high-degree-of-freedom (DoF) systems.\nWe present a recipe that combines the benefits of BC and RL through a residual learning framework. Our approach leverages BC policies as black-box bases and learns lightweight per-step residual corrections via sample-efficient off-policy RL.\nWe demonstrate that our method requires only sparse binary reward signals and can effectively improve manipulation policies on high-degree-of-freedom (DoF) systems in both simulation and the real world. In particular, we demonstrate, to the best of our knowledge, the first successful real-world RL training on a humanoid robot with dexterous hands.\nOur results demonstrate state-of-the-art performance in various vision-based tasks, pointing towards a practical pathway for deploying RL in the real world.",
            "introduction": "Enabling robots to learn and improve directly in their deployment environments remains a fundamental challenge in robotics. Recently, significant progress has been made in training visuomotor control policies in the real world with behavior cloning (BC) from human demonstrations [1, 2, 3, 4, 5, 6, 7, 8, 9]. However, this success requires significant infrastructure, as well as numerous hours of manual and cumbersome data collection.\nEven if unlimited data could be collected for every task, not only is human teleoperator performance generally suboptimal, but there is also emerging evidence that policy performance saturates with increasing demonstrations [10, 6, 11, 12, 13].\n\nReinforcement learning (RL) offers a complementary paradigm: agents learn autonomously through trial and error in the environment. Deep RL has shown great success in various domains [14, 15, 16, 17, 18, 19, 20, 21], including in-hand manipulation [22, 23] and locomotion [24, 25, 26, 27]. However, strong RL performance generally requires large amounts of data from online interactions, so its application has been mainly in simulation [28, 29] since real-world data are expensive and potentially unsafe to gather in large amounts.\n\nA natural direction to improve BC policies is to leverage online RL [12, 30, 31, 32], combining the strengths of each: BC policies provide a strong prior that can regularize exploration in the RL process, while online RL enhances policy performance by learning from interactions with the environment. However, modern BC architectures are typically deep models with tens of millions to billions of parameters that utilize action chunking or diffusion-based approaches, which can make it challenging to apply RL methods directly to optimize the policy. A simple yet powerful recipe that avoids several of the above issues is residual RL [33, 34, 35, 36, 12, 32, 31], where RL is applied not to learn a full policy, but only to learn corrective terms on top of a fixed base controller. Previous work has demonstrated that residual RL can indeed enhance the reliability of a pre-trained policy. Still, it has so far been limited to learning in simulation [12, 32, 31] or demonstrating results in simple or constrained settings [33, 34, 35, 36]; Applications to high-DoF systems learning directly in the real world are still lacking.\n\nIn this work, we present an off-policy residual fine-tuning (ResFiT) approach that utilizes online RL to enhance BC policies. By treating the base policy as a black box and learning a per-step residual correction that is independent of chunk size and policy parameterization, we sidestep the challenges of directly optimizing huge base policies. By carefully designing our off-policy recipe, we make the RL process sample efficient enough to scale to high-DoF bimanual systems, require only sparse binary reward signals, and be safe enough to deploy in the real world. We demonstrate robust performance on sparse-reward, long-horizon, vision-based tasks, showing that our approach achieves state-of-the-art performance for a range of tasks in simulation. We also investigate each design decision in our recipe. To the best of our knowledge, we provide the first demonstration of RL on a humanoid robot with five-fingered hands, trained entirely in the real world.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在保留行为克隆（BC）基线的同时通过离线/在线强化学习实现对复杂高自由度系统的高效微调与改进\n2. 如何实现对高DoF系统的安全、样本高效的Residual RL，以最少数据和稀疏奖励实现现实世界鲁棒性\n3. 如何在不直接优化大型基准策略的前提下，对其进行逐步残差修正以提升 visuomotor 控制性能\n4. 如何将离线BC与在线RL结合，解决数据质量、数据收集成本、以及现实世界训练的安全性挑战\n5. 如何在真实 humanoid/五指手等高复杂度机械臂上实现端到端的实证RL训练与评估\n\n【用了什么创新的方案】\n核心解决方案：提出离线行为克隆基线上的残差强化学习（ResFiT），将基线策略视为黑盒，通过学习逐步的每步残差信号来实现修正；使用面向离线/在线混合的高效RL配方，确保在高DoF、稀疏二值奖励下也能实现样本友好、真实世界的训练，并在仿真与现实中获得前所未有的鲁棒性与性能提升；通过残差学习避免直接优化庞大基线策略，降低参数化、数据需求和安全风险；首次在真实 humanoid/五指手上实现全现实世界的RL微调。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration",
            "authors": "Yang Jin,Jun Lv,Han Xue,Wendi Chen,Chuan Wen,Cewu Lu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19292",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19292",
            "arxiv_html_link": "https://arxiv.org/html/2509.19292v1",
            "abstract": "Intelligent agents progress by continually refining their capabilities through actively exploring environments. Yet robot policies often lack sufficient exploration capability due to action mode collapse. Existing methods that encourage exploration typically rely on random perturbations, which are unsafe and induce unstable, erratic behaviors, thereby limiting their effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a framework that enhances policy exploration and improvement in robotic manipulation. SOE learns a compact latent representation of task-relevant factors and constrains exploration to the manifold of valid actions, ensuring safety, diversity, and effectiveness. It can be seamlessly integrated with arbitrary policy models as a plug-in module, augmenting exploration without degrading the base policy performance. Moreover, the structured latent space enables human-guided exploration, further improving efficiency and controllability. Extensive experiments in both simulation and real-world tasks demonstrate that SOE consistently outperforms prior methods, achieving higher task success rates, smoother and safer exploration, and superior sample efficiency. These results establish on-manifold exploration as a principled approach to sample-efficient policy self-improvement.",
            "introduction": "“We want AI agents that can discover like we can, not which contain what we have discovered.”\n            — Richard Sutton, The Bitter Lesson\n\nIn recent years, data-driven robot learning [10, 53, 5, 7, 25] has attracted considerable attention, particularly for its potential to enhance robotic manipulation capabilities through large-scale data collection and training. By modeling visuomotor behaviors with neural networks, these approaches allow robot policies to learn from expert demonstrations and achieve near-human performance across a variety of tasks.\n\nDespite these advances, most existing methods still rely heavily on human teleoperation for data acquisition [53, 13] and policy refinement [30, 31], which presents several challenges. A primary concern is the high cost of teleoperation, as it typically requires skilled operators and specialized equipment, thereby limiting the scalability of data collection. More critically, teleoperated demonstrations often fail to cover the diverse scenarios a robot could encounter in the real world, resulting in distributional bias [52] and compounding error [39]. The problem is further exacerbated by the fact that human operators may act based on contextual cues inaccessible to robot sensors. Robots, on the other hand, may internalize human habits rather than task-relevant behaviors. As a result, simply scaling up teleoperated data is not the optimal path toward improving policy performance.\n\nInstead of passively imitating human-provided behavior, a line of research addresses this challenge by enabling robot policy self-improvement [6, 23, 35, 32]—actively exploring the environment to collect diverse experience and leveraging that experience to refine policies. Under this paradigm, robots can autonomously discover novel behaviors that go beyond the coverage of human demonstrations. By iteratively practicing the learned behaviors, they also develop a deeper understanding of the natural variability in their actions, ultimately leading to a more robust and resilient policy.\n\nThe key to sample-efficient robot policy self-improvement lies in effective exploration. Prior work [3, 23] has shown that imitation-learned policies often overfit demonstrations, collapse into single-modal motions, and fail to produce diverse behaviors. Without proper exploration, these policies tend to repeat failed behaviors, limiting their ability to discover improved solutions. While random exploration strategies can occasionally yield novel behaviors [29], they are generally ineffective in high-dimensional action spaces [28] and can pose safety risks in real-world deployment [16], causing potential hardware damage. This necessitates a more structured approach to exploration—one that ensures safety and effectiveness without sacrificing the diversity of experiences.\n\nTo this end, we propose SOE, a novel framework for Sample-Efficient Robot Policy Self-improvement via On-Manifold Exploration. The core idea of our method is to ensure that exploration remains constrained to the manifold of valid actions—critical for both safety and effectiveness. Prior works often perturb the action space directly [29] or inject random noise [23], leading to temporally inconsistent and unsafe behaviors, particularly under “action chunking” representations [53]. In contrast, we perform exploration in a compact latent space learned through a variational information bottleneck (VIB). The latent representation in this space preserves only task-essential information in observation while discarding irrelevant details, ensuring exploration remains structured and efficient. As illustrated in Fig. 1, by operating on this latent representation, our framework enables effective on-manifold exploration and more robust policy improvement. Furthermore, we demonstrate that in the latent space, action chunks are naturally disentangled into distinct modes. Leveraging this property, we achieve controllable exploration, which allows users to guide exploration toward preferred directions, thereby enhancing interpretability and further boosting sample efficiency. Implemented as a plug-in module, our approach can be seamlessly integrated with existing imitation learning algorithms and jointly optimized, without any degradation in their performance.\n\nTo evaluate the effectiveness of our method, we conduct extensive experiments across a variety of robot manipulation tasks in both simulation and real world. The results show that SOE consistently outperforms prior exploration methods in effectiveness, motion smoothness, and sample efficiency. With just one round of policy self-improvement, our method achieves substantial gains over the base policy, including an average relative improvement of 50.8% on real-world tasks. Additional experiments in simulation and ablation studies further confirm multi-round performance improvements and the contribution of each component in our framework. Collectively, these findings demonstrate that on-manifold exploration provides a structured, safe, and effective approach to sample-efficient robot policy self-improvement.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在高维机器人动作空间中实现安全、有效且样本高效的策略自我改进中的探索？\n2. 如何通过潜在表征约束探索到行动流形以避免无效或危险的随机扰动？\n3. 如何实现对探索的可控性与可解释性，同时保持对既有模仿学习的无干扰性？\n4. 如何在仿真与真实世界任务中提升任务成功率、平滑性和样本效率？\n\n【用了什么创新的方案】\nSOE在任务相关因素的紧凑潜在表示上进行探索，利用变分信息瓶颈学习一个仅保留任务本质信息的潜在空间，并在该潜在流形上进行探索以保持动作的有效性和安全性。它将探索与现有策略模型无缝对接，可作为插件模块嵌入到任意模仿学习框架中；潜在空间还使动作块在不同模式上解耦，从而实现可控探索，并支持人工引导以提高效率。实验结果表明在仿真和真实任务中，SOE实现更高的成功率、更平滑且更安全的探索以及更强的样本效率。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces",
            "authors": "Kuanqi Cai,Chunfeng Wang,Zeqi Li,Haowen Yao,Weinan Chen,Luis Figueredo,Aude Billard,Arash Ajoudani",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19261",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19261",
            "arxiv_html_link": "https://arxiv.org/html/2509.19261v1",
            "abstract": "Robotic manipulation in dynamic environments often requires seamless transitions between different grasp types to maintain stability and efficiency. However, achieving smooth and adaptive grasp transitions remains a challenge, particularly when dealing with external forces and complex motion constraints. Existing grasp transition strategies often fail to account for varying external forces and do not optimize motion performance effectively. In this work, we propose an Imitation-Guided Bimanual Planning Framework that integrates efficient grasp transition strategies and motion performance optimization to enhance stability and dexterity in robotic manipulation. Our approach introduces Strategies for Sampling Stable Intersections in Grasp Manifolds for seamless transitions between uni-manual and bi-manual grasps, reducing computational costs and regrasping inefficiencies. Additionally, a Hierarchical Dual-Stage Motion Architecture combines an Imitation Learning-based Global Path Generator with a Quadratic Programming-driven Local Planner to ensure real-time motion feasibility, obstacle avoidance, and superior manipulability. The proposed method is evaluated through a series of force-intensive tasks, demonstrating significant improvements in grasp transition efficiency and motion performance.\nA video demonstrating our simulation results can be viewed at https://youtu.be/3DhbUsv4eDo.",
            "introduction": "Robotic manipulation in dynamic forceful operations—such as collaborative cutting or drilling—demands real-time adaptation to varying external forces that critically affect grasp stability. Consider a human-robot woodworking scenario (Fig. 1) where the robot must continuously adjust between uni-manual and bi-manual grasps to counteract changing cutting and drilling forces. This fundamental requirement exposes two unresolved challenges in existing methods: efficient grasp transitions by minimizing execution time and arm movement and motion performance awareness, as crucial metrics like manipulability and joint limits essential for control safety are often overlooked. To bridge this gap, we propose an imitation-guided planning framework that integrates efficient grasp transitions with motion performance constraints, ensuring both stability and dexterity in forceful tasks.\n\nMulti-step manipulation planners have long tackled regrasping and grasping transitions [1]. Traditional grasping involves transporting an object by repeatedly releasing and regrasping it as needed [2]. Conventional regrasp planners rely on a supporting surface for single-arm manipulation [3, 4], while recent research extends these strategies to dual-arm scenarios [5, 6, 7].\nHowever, existing methods do not explicitly account for dynamic external forces, which vary over time, nor do they optimize regrasp transitions during forceful interactions. Studies in forceful human-robot collaboration [8, 9, 10] focus on regulating contact forces but assume a fixed or pre-determined grasp. The key challenge remains: determining where and how a robot should grasp for stability and when to transition seamlessly under complex external forces. Recent works [11, 12, 13] have made progress, but achieving stable grasps that withstand varying forces while ensuring efficient planning, manipulability, and dexterity remains difficult.\nThis paper addresses two key challenges in forceful robotic manipulation: efficient grasp transitions and motion performance optimization, proposing a novel framework to overcome them.\n\nEfficient Grasp Transitions. Previous methods [11, 13] mostly rely on random sampling-based planners for grasp transitions, often resulting in high computational costs and unstable changes. To reduce task execution time and minimize the movement distance of the robot arm, we introduce Strategies for Sampling Stable Intersections in Grasp Manifolds for seamless uni-manual and bi-manual transitions. Our Directional Gradient-Based Resampling locally adjusts the unimanual manipulator along the negative gradient, ensuring stability while maintaining a secure unimanual grasp and minimizing movement. For tasks with multiple grasp changes, Multi-Grasp Transition Check (MTC) identifies a shared intermediate configuration, reducing redundant regrasping. To further boost efficiency, we propose a Hierarchical Dual-Stage Motion Architecture, combining an Imitation Learning-based Global Path Generator with a QP-driven local planner for real-time motion optimization and obstacle avoidance, enabling faster collision-free path generation than sampling-based methods.\n\nMotion Performance Optimization.\nIn forceful operations, robotic stability is determined by three motion performance factors: manipulability, dexterity, and joint limits, which ensure kinematic feasibility under varying forces. However, many recent methods [14, 15] tend to overlook these critical aspects.\nOur framework optimizes motion performance at both the grasp configuration level and during execution. To enhance manipulability and avoid kinematic limitations, we introduce a Motion Performance Map that encodes feasibility, manipulability, and joint limit proximity. This map guides grasp sampling toward optimal workspace regions, improving selection efficiency. During execution, we enforce manipulability constraints within the QP framework, ensuring control authority over the end-effector while avoiding singularities and joint limits. This real-time optimization enables stable, collision-free motion.\n\nBy integrating these advancements, our framework enhances grasp transition efficiency while ensuring superior motion performance in both grasp selection and execution, effectively addressing key limitations in existing forceful robotic planning approaches.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在动态外部力下实现稳定而高效的双手抓持与无缝转换（uni-manual到bi-manual）？\n2. 如何在抓取转移中兼顾运动性能（可控性、可控度、关节极限）并降低计算成本？\n3. 如何在强力操作场景中实时规划可行路径并避免碰撞，同时优化抓取点选择？\n\n【用了什么创新的方案】\n策略性在抓取流形中采样稳定交点以实现单臂到双臂的平滑转变，并用方向梯度重采样局部调整单臂抓持以保持稳定性。提出多抓取转变检查以寻找共用中间配置，降低冗余重新抓取。建立分层双阶段运动架构：基于模仿学习的全局路径生成器+QP驱动的局部规划器实现实时、避障且高可 manipulability 的路径规划。引入运动性能地图（可行性、可操作性、关节极限接近度）以引导抓取选择并在执行中通过QP约束实现对端执行器的控制权和避免奇异点。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Proactive-reactive detection and mitigation of intermittent faults in robot swarms",
            "authors": "Sinan Oğuz,Emanuele Garone,Marco Dorigo,Mary Katherine Heinrich",
            "subjects": "Robotics (cs.RO); Multiagent Systems (cs.MA); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19246",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19246",
            "arxiv_html_link": "https://arxiv.org/html/2509.19246v1",
            "abstract": "Intermittent faults are transient errors that sporadically appear and disappear. Although intermittent faults pose substantial challenges to reliability and coordination, existing studies of fault tolerance in robot swarms focus instead on permanent faults. One reason for this is that intermittent faults are prohibitively difficult to detect in the fully self-organized ad-hoc networks typical of robot swarms, as their network topologies are transient and often unpredictable. However, in the recently introduced self-organizing nervous systems (SoNS) approach, robot swarms are able to self-organize persistent network structures for the first time, easing the problem of detecting intermittent faults. To address intermittent faults in robot swarms that have persistent networks, we propose a novel proactive–reactive strategy to detection and mitigation, based on self-organized backup layers and distributed consensus in a multiplex network. Proactively, the robots self-organize dynamic backup paths before faults occur, adapting to changes in the primary network topology and the robots’ relative positions. Reactively, robots use one-shot likelihood ratio tests to compare information received along different paths in the multiplex network, enabling early fault detection. Upon detection, communication is temporarily rerouted in a self-organized way, until the detected fault resolves.\nWe validate the approach in representative scenarios of faulty positional data occurring during formation control, demonstrating that intermittent faults are prevented from disrupting convergence to desired formations, with high fault detection accuracy and low rates of false positives.",
            "introduction": "Reliability in networked systems requires consistently accurate information exchange among components, often under dynamic and uncertain conditions [1, 2]. If communication links fail or become unreliable during multi-hop communication, system convergence and performance guarantees can be compromised [3, 4, 5, 6]. In self-organized robot swarms, this challenge is exacerbated by asynchronous ad-hoc communication and decentralized coordination of actuation and decision making. Robots in a self-organized swarm rely solely on local information and communication with nearby robots, without any estimation of the global state of the swarm or its environment, often leading to prolonged convergence times and vulnerability to the spread of incorrect information [7]. Frequent communication between robots can cause faulty information to spread quickly and potentially degrade overall swarm performance or lead to permanent failures.\n\nSelf-organized robot swarms exhibit some inherent fault tolerance, through redundancy and a lack of single points of failure [8, 9].\nHowever, many fault types are not mitigated by this passive tolerance and instead require dedicated mechanisms for detection and mitigation [10, 11, 12, 13]. Somewhat counter-intuitively, self-organized robot swarms are inherently much more tolerant to complete robot failures than to partial ones [14]. For example, a single robot producing faulty or malicious information has been shown to be capable of severe disruption to overall swarm behavior [14, 15]. Faulty robots can also physically obstruct the rest of the swarm, and this interference can paradoxically be worsened by the redundancy that provides swarms with some types of inherent fault tolerance [16].\n\nOther faults to which self-organized robot swarms are vulnerable and which require dedicated mechanisms for detection and mitigation are intermittent faults (IFs). IFs are temporary faults that can appear, disappear, and reappear [17], potentially caused by communication interference, sensor malfunctions, or software bugs [18]. IFs are difficult to detect and diagnose due to their transience [19] and can cause significant disruptions without leaving an easily detectable trace [18].\nA representative example involves intermittent GPS signal degradation in cluttered environments, which can induce sporadic localization errors. These errors propagate through decentralized state estimation protocols, gradually undermining coordination mechanisms without generating explicit failure indicators.\nIn real applications, e.g., in robot swarms deployed in inaccessible or dangerous environments [20, 12], the consequences of IFs to mission performance and to safety can be severe and in some cases could be irreversible. Detecting and resolving IFs before they escalate is key to minimizing disruption: early detection can prevent cascading failures leading to erroneous execution of tasks and can prevent culmination in permanent failures, either of individual robots or the swarm as a whole [21].\n\nIFs are difficult to detect in robot swarms with fully self-organized ad-hoc networks, because the network topology is transient and often unpredictable. IFs are much more straightforward to detect in fully centralized systems and in networks with static structures, for example in sensor networks [22, 23, 24]. However, for multi-robot systems, full centralization and fully static networks also present downsides, such as single points of failure and limited scalability.\n\nOur recently introduced self-organizing nervous systems (SoNS) [25] approach combines aspects of centralization and decentralization through self-organized hierarchy. Using the SoNS approach, robot swarms are coordinated via temporary logical networks that are hierarchical and culminate in a dynamic “brain” robot (i.e., leader), but which are not imposed from the outside, being instead established and maintained in a self-organized manner. This provides robot swarms with persistent and predictable network structures that are more amenable to detecting IFs, without introducing any single points of failure.\nIn short, the SoNS approach allows, for the first time, to apply centralized fault detection and mitigation strategies to robot swarms without sacrificing their oft-cited benefits of scalability, flexibility, and a lack of single points of failure.\n\nSwarm robotics usually studies passive tolerance to permanent faults [26]—that is, faults such as electromechanical failures that will remain unless they are actively repaired.\nWhen relying on passive fault tolerance, studies have usually demonstrated that a swarm continues its mission after some or many robots have failed, either by continuing with fewer robots  [10, 27, 28] or by replacing/repairing the failed robots without pausing the mission [29, 30, 31, 25].\n\nSwarm robotics studies that focus specifically on fault tolerance do not typically rely on passive tolerance, instead developing dedicated mechanisms to handle permanent faults.\nThe majority of these methods detect and react to permanent electromechanical failures after they have occurred [21, 32], often relying on time-out mechanisms in which a robot is considered non-operational if it does not respond to a message within a certain time. Existing methods for detecting permanent faults include LED synchronization [29], simulation comparison [33], shared sensor data analysis [28], and behavioral feature vectors (BFVs) [11].\nThese methods often focus on detection, assuming that once a fault is detected, a repair or other intervention is possible during normal operation (e.g., [34, 29, 21]). Although such repairs might be unrealistic in inaccessible, hazardous, or congested environments [35, 21, 12], future methods for autonomous repair could be developed to complement detection. In short, the existing reactive methods can be considered effective for many types of permanent faults [10].\nHowever, the above-mentioned detection approaches are unlikely to be applicable to the transience of IFs and their long response times [36] would likely be too slow for the early detection and recovery that IFs require. Methods to detect and repair IFs in robot swarms still need to be developed.\n\nTo the best of our knowledge, there are no existing swarm robotics methods focused on IF detection and recovery. Strategies developed for IFs in other types of systems, such as model-based analysis (e.g., discrete-event-system models [37], causal models [38]) and quantitative analysis (e.g., parameter estimation [39], geometric approaches [40], Kalman-like filtering [41]), provide valuable insights but primarily target single-unit systems with static and known system models [24, 42], which is incompatible with self-organized systems such as robot swarms. Likewise, IF strategies developed for sensor networks [22, 23] typically use fully centralized architectures to correct information transmission and reception [24], and are therefore incompatible with self-organized systems.\n\nFurthermore, although fully centralized monitoring is highly effective for detecting and correcting IFs, it can present problems of inflexibility, limited scalability, and single points of failure (e.g., at the point where monitoring is centralized). Fully self-organized approaches, by contrast, would be highly flexible and offer greater scalability and a lack of single points of failure, but would present problems of limited accuracy and potentially slow reaction times.\nIn this paper, we aim to combine elements of each system type to get the benefits of both. Using our proposed proactive–reactive approach, robots can monitor each other using self-organizing hierarchy, detecting IFs accurately and remedying them proactively.\n\nTo demonstrate our proposed proactive–reactive approach, we use the SoNS concept of self-organizing hierarchy in a robot swarm, which has been shown to incorporate temporarily centralized structures into an otherwise self-organized robot swarm without introducing single points of failure or inherently limiting scalability [25, 30, 43, 44, 45, 46]. We build on our recent theoretical foundations for self-organizing hierarchical frameworks: hierarchical Henneberg construction (HHC) [47]. In our previous work [47], we demonstrated HHC for key self-reconfiguration problems (framework merging, robot departure, and framework splitting), derived the mathematical conditions of these problems, and developed algorithms that preserve rigidity and hierarchy using only local information.\n\nIn the remainder of this paper, we assume all graphs are constructed using these already demonstrated HHC algorithms, and refer to such graphs as HHC-constructed graphs. See Appendix A for details on how HHC and SoNS are related.\n\nIn fault tolerance for multi-robot systems, both proactive and reactive mechanisms are important [48].\nIn this paper, we propose a novel proactive–reactive method to detect and mitigate IFs in robot swarms.\nIn the proposed proactive–reactive method, the robots first use distributed consensus to preemptively self-organize dynamic backup communication paths before IFs are detected. Then, the robots compare information received via primary and backup paths to detect IFs, using a one-shot likelihood ratio test. When IFs are detected, the robots react by rerouting communication through the dynamic backup paths. In this paper, we apply the proposed proactive–reactive method to a scenario of intermittently faulty relative positional information within multi-robot formations that have a hierarchical structure towards a fault-free leader, and demonstrate that the method mitigates IFs and robots are able to continue with the desired formations.\n\nThe main technical contributions of this paper can be summarized as follows:\n\nWe address a current gap in robot swarm networking, specifically how to establish back-up communication paths for leader–follower formation control in a self-organized robot swarm. We address this gap by extending the biased minimum consensus (BMC) [49] protocol for shortest path planning in static graphs. We introduce the adaptive biased minimum consensus (ABMC) protocol for dynamic graphs—addressing time-varying topologies, node neighborhoods, and costs. We demonstrate that our ABMC protocol addresses the minimum-cost path problem, with two objectives integrated into a single cost function: to minimize the number of hops to the destination (the leader robot) and to minimize the degree of network congestion (by minimizing the occurrence of parallel edges).We provide the mathematical properties and stability analysis of the ABMC protocol as a distributed consensus mechanism in dynamic graphs with piecewise constancy, including providing the necessary and sufficient conditions to uniquely determine an equilibrium point representing a minimum-cost backup path.\n\nWe address a current gap in robot swarm fault tolerance, specifically tolerance against intermittent faults (IFs). We address this gap by proposing a novel proactive–reactive fault-tolerance strategy for detection and mitigation of IFs in robot swarms. Our proposed strategy uses the ABMC protocol to construct backup network layers and combines it with a distributed likelihood ratio (LR) protocol to dynamically reroute traffic in the constructed multiplex network. We propose the mathematical conditions and design the distributed algorithms for backup layer construction and for execution of the proactive–reactive strategy for IF detection and mitigation. We also provide the time and space complexity and efficiency properties of both distributed algorithms. Finally, we demonstrate the proactive–reactive fault-tolerance strategy in formations of 20 robots with moving leaders.\n\nThe rest of the paper is organized as follows. In Sec. II, the foundational concepts regarding hierarchical frameworks are presented, along with the existing BMC protocol. In Sec. III, we formulate three key problems addressed in this paper: construction of dynamic minimum-cost backup paths, detection of IFs using the constructed backup paths, and mitigation of the detected IFs using the constructed backup paths. The first problem is addressed in Secs. IV and V, and the second and third problem are addressed in Sec. VI. Finally, in Sec. VII we validate our contributions in experiments of representative scenarios. The conclusions are summarized in Sec. VIII.",
            "llm_summary": "【关注的是什么问题】\n1. Intermittent faults (IFs) in self-organized robot swarms and their impact on formation convergence and reliability\n2. 如何在自组织机器人群体中检测与缓解IFs，兼顾可扩展性与无单点故障\n3. 在动态拓扑中实现备份路径的自组织构建与快速故障检测\n4. 将集中式检测策略与去中心化群体控制结合的主动-被动（proactive–reactive）两层防护\n\n【用了什么创新的方案】\n核心解决方案：提出主动-被动（proactive–reactive）IF检测与缓解框架，利用自组织层次化的备份通信路径实现多路径冗余；通过自组织的ABMC协议在动态图构建最小代价备份路径，并使用分布式一次性似然比检验比较主路径与备份路径的信息以实现早期IF检测；IF检测后，故事通路通过自组织方式重新路由通信直至故障消失；在20机器人队形实验中验证方法提升鲁棒性与收敛性。并将ABMC与LR检验结合，给出时空复杂度分析与稳定性条件。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap",
            "authors": "Tianyu Wu,Xudong Han,Haoran Sun,Zishang Zhang,Bangchao Huang,Chaoyang Song,Fang Wan",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 4 figures, accepted to Data@CoRL2025 Workshop",
            "pdf_link": "https://arxiv.org/pdf/2509.19169",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19169",
            "arxiv_html_link": "https://arxiv.org/html/2509.19169v1",
            "abstract": "The transfer of manipulation skills from human demonstration to robotic execution is often hindered by a “domain gap” in sensing and morphology. This paper introduces MagiClaw, a versatile two-finger end-effector designed to bridge this gap. MagiClaw functions interchangeably as both a handheld tool for intuitive data collection and a robotic end-effector for policy deployment, ensuring hardware consistency and reliability. Each finger incorporates a Soft Polyhedral Network (SPN) with an embedded camera, enabling vision-based estimation of 6-DoF forces and contact deformation. This proprioceptive data is fused with exteroceptive environmental sensing from an integrated iPhone, which provides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS application, MagiClaw streams synchronized, multi-modal data for real-time teleoperation, offline policy learning, and immersive control via mixed-reality interfaces. We demonstrate how this unified system architecture lowers the barrier to collecting high-fidelity, contact-rich datasets and accelerates the development of generalizable manipulation policies. Please refer to the iOS app at https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.",
            "introduction": "The success of modern robot learning paradigms, from Learning from Demonstration (LfD) [1, 2] to offline reinforcement learning, is fundamentally dependent on the quality and richness of the underlying data [3]. For contact-rich manipulation tasks, robust policies require more than just kinematic trajectories; they demand a holistic understanding of interaction forces, tactile feedback, and environmental context [4, 5]. Consider a human deftly handling a delicate object: the action is a symphony of precise motion, modulated forces, and continuous tactile adjustments [6]. Replicating such skills requires capturing this multi-modal information stream in its entirety.\n\nHowever, existing data collection methodologies present significant challenges. First, they often rely on a patchwork of disparate, expensive sensors—such as external motion capture systems, wrist-mounted force/torque sensors, and complex tactile skins [7, 8]—resulting in cumbersome and costly setups. This high barrier to entry limits the scale and diversity of data collection efforts [9]. Second, and more critically, a persistent domain gap exists between the human demonstrator and the robotic learner [10]. Data is often collected using one set of hardware (e.g., an instrumented glove) and deployed on a robot with entirely different sensor suites and end-effector morphology. This mismatch necessitates complex domain adaptation techniques and is a primary reason why policies trained on demonstration data often fail to generalize to physical hardware [11].\n\nTo address these challenges, we present MagiClaw, a unified hardware platform designed to seamlessly bridge the gap from human demonstration to robotic deployment. MagiClaw is a dual-purpose, two-fingered gripper that merges three key innovations:\n\nUnified Hardware Form Factor: The exact same MagiClaw device can be used as a hand-held tool for human demonstration or mounted on a robot arm for autonomous execution. This hardware consistency minimizes the sensor and morphological domain gap, facilitating direct policy transfer.\n\nVision-Based Proprioceptive Fingertips: Each finger integrates a Soft Polyhedral Network (SPN) [12] with an embedded miniature camera. This novel design enables visuotactile perception, inferring 6-DoF forces, torque, and high-resolution contact deformation from the distortion of the internal lattice structure, thereby obviating the need for costly external force sensors.\n\nIntegrated Multi-Modal Exteroception: An attached iPhone leverages its powerful sensor suite (LiDAR, RGB cameras, IMU) and ARKit framework [13] to provide synchronized, rich environmental context, including gripper pose, depth maps, and high-resolution video.\n\nOur primary contribution is an integrated system that fundamentally streamlines the collection of holistic, contact-centric data for robot learning. By fusing proprioceptive force/tactile data from the fingertips with exteroceptive visual and spatial data from a commodity smartphone, MagiClaw offers a low-cost, powerful, and user-friendly solution for both teleoperation and autonomous policy development. We posit that by democratizing access to such high-fidelity, multi-modal data, MagiClaw can serve as a catalyst for developing more robust and generalizable manipulation skills, advancing the pursuit of universal action embodiment in robotics.",
            "llm_summary": "【关注的是什么问题】\n1. 领域差距导致的人类示范到机器人执行的迁移困难（≤40词）\n2. 需要高质量多模态数据但现有传感器系统昂贵、零散、耦合度高的问题（≤40词）\n3. 如何在同一硬件上实现手持示范与机器人执行之间的无缝切换（≤40词）\n4. 如何通过 visuotactile 与环境感知实现对力、接触变形的高保真推断（≤40词）\n5. 如何降低数据采集成本并提高数据多样性以提升策略泛化能力（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：MagiClaw 将两指软聚合网格内嵌相机实现视觉-本体感知的六自由度力与接触变形估计；每指作為 SPN 传感器；通过 iPhone 的 LiDAR、RGB、深度数据进行同步的外感知；手持与机器人端可互换的统一硬件形式；自带 iOS 应用实现实时远控、离线学习数据流；降低数据采集成本、提升多模态数据质量与一致性。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination",
            "authors": "Mark Gonzales,Ethan Oh,Joseph Moore",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 Pages, 7 Figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19168",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19168",
            "arxiv_html_link": "https://arxiv.org/html/2509.19168v1",
            "abstract": "In this paper, we present a receding-horizon, sampling-based planner capable of reasoning over multimodal policy distributions. By using the cross-entropy method to optimize a multimodal policy under a common cost function, our approach increases robustness against local minima and promotes effective exploration of the solution space. We show that our approach naturally extends to multi-robot collision-free planning, enables agents to share diverse candidate policies to avoid deadlocks, and allows teams to minimize a global objective without incurring the computational complexity of centralized optimization. Numerical simulations demonstrate that employing multiple modes significantly improves success rates in trap environments and in multi-robot collision avoidance. Hardware experiments further validate the approach’s real-time feasibility and practical performance.",
            "introduction": "Local minima pose a fundamental challenge for finite-horizon, gradient-based planning approaches. In multi-robot scenarios, local minima can arise not only from the environment but also from dynamic factors, such as the changing trajectories of teammates, which may inadvertently block or cut off routes that would otherwise be viable. These pitfalls often cause robots to become stuck, find suboptimal solutions, or fail to coordinate effectively in complex environments.\n\nSampling-based planners, such as Model Predictive Path Integral (MPPI) [1] and Cross-Entropy Method (CEM) [2, 3], attempt to improve the trajectory cost by stochastically sampling and evaluating trajectories in the cost landscape. In practice, these methods utilize hyperparameters, such as sampling variance, number of samples, and the horizon length, to adapt the exploration to the environment. However, both MPPI and CEM typically sample trajectories around the prior best policy, leading to a concentration of samples in a narrow region of the solution space. This localized search impedes the planner’s ability to effectively navigate around traps or escape from local minima once they occur, especially in environments with challenging topology. As a result, the planner can become stuck in suboptimal regions, regardless of the variance or adaptation strategy.\n\nIn multi-robot systems, the difficulty is exacerbated by the need for robots to coordinate planned trajectories. Centralized control approaches [4, 5, 6, 7] can, in principle, achieve globally optimal coordination; however, they suffer from scalability issues and high computational costs as the team size increases. Distributed methods, while scalable, often require robots to individually select their optimal trajectory, subsequently negotiating with teammates to reach a feasible consensus. When each robot contributes only a single candidate trajectory, the team risks deadlock or persistent local minima, as a lack of trajectory diversity reduces the likelihood of discovering collision-free, cooperative maneuvers, especially when teammates dynamically update their plans or block each other’s routes in real-time.\n\nTo overcome these limitations, we introduce a multimodal sampling and clustering framework that maintains multiple policy candidates for each robot, thereby increasing diversity and robustness against local minima in both environmental and collaborative planning contexts.\n\nOur contributions are:\n\nA cross-entropy planning approach capable of preserving multiple policy modes for increased planning robustness.\n\nA multi-robot coordination framework that enables reasoning about sets of candidate policies to avoid local minima and deadlocks more reliably.\n\n1. A cross-entropy planning approach capable of preserving multiple policy modes for increased planning robustness.\n\n2. A multi-robot coordination framework that enables reasoning about sets of candidate policies to avoid local minima and deadlocks more reliably.",
            "llm_summary": "【关注的是什么问题】\n1. 本地最优陷阱与多模态政策在单机器人导航中的鲁棒性（≤40词）\n2. 多机器人协同中的死锁、协调与全局目标优化的可扩展性（≤40词）\n3. 受限搜索空间中多模态策略对避障与探索的提升（≤40词）\n\n【用了什么创新的方案】\n- 引入跨熵法的多模态策略规划，保留多条候选策略以提升对局部极小值的鲁棒性。\n- 基于采样与聚类的框架，在多机器人场景中共享多策略集合，避免死锁并实现分布式协同。\n- 通过 receding-horizon 与多模态分布采样实现对全局成本的高效优化，降低集中化计算开销。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer",
            "authors": "Kangmin Kim,Seunghyeok Back,Geonhyup Lee,Sangbeom Lee,Sangjun Noh,Kyoobin Lee",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 5 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19142",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19142",
            "arxiv_html_link": "https://arxiv.org/html/2509.19142v1",
            "abstract": "Bimanual grasping is essential for robots to handle large and complex objects. However, existing methods either focus solely on single-arm grasping or employ separate grasp generation and bimanual evaluation stages, leading to coordination problems including collision risks and unbalanced force distribution. To address these limitations, we propose BiGraspFormer, a unified end-to-end transformer framework that directly generates coordinated bimanual grasps from object point clouds. Our key idea is the Single-Guided Bimanual (SGB) strategy, which first generates diverse single grasp candidates using a transformer decoder, then leverages their learned features through specialized attention mechanisms to jointly predict bimanual poses and quality scores. This conditioning strategy reduces the complexity of the 12-DoF search space while ensuring coordinated bimanual manipulation. Comprehensive simulation experiments and real-world validation demonstrate that BiGraspFormer consistently outperforms existing methods while maintaining efficient inference speed (<<0.05s), confirming the effectiveness of our framework. Code and supplementary materials are available at https://sites.google.com/bigraspformer",
            "introduction": "Bimanual grasping enables robots to manipulate large, heavy, or unwieldy objects beyond single-arm capabilities, making it essential for tasks such as lifting furniture, carrying long boards, or moving large boxes [1, 2]. However, most robotic grasping research has focused on single-arm systems, primarily on learning to detect 6-DoF grasp poses from point clouds [3, 4, 5, 6, 7, 8]. While effective for single-arm tasks, these approaches cannot be directly extended to bimanual scenarios. First, bimanual grasping expands the action space to 12-DoF, doubling the computational complexity. Second, it introduces new challenges, including collision avoidance, balanced force/torque distribution, and dual-arm coordination for post-grasp manipulation.\n\nFor bimanual grasping, only a few methods have been proposed so far. The DA2 dataset [9] introduced the first benchmark by extending single-arm datasets [10, 11, 3] with dual-arm-specific metrics such as force closure, dexterity, and torque balance [9, 12]. However, most existing approaches adopt modular architectures that separate grasp generation and evaluation. For example, Dual-PointNetGPD [9] evaluates the quality of grasp pairs from given candidates, requiring external single-arm grasp generators. Similarly, CGDF [13] directly generates bimanual grasps but lacks integrated quality prediction, instead relying on additional scoring modules or heuristic pairing strategies [14, 15]. As a result, current methods yield limited diversity, poor coordination, and high computation due to modular pipelines.\n\nIn this paper, we propose BiGraspFormer, the first unified end-to-end framework that directly generates coordinated bimanual grasps from object point clouds (Fig. 1). The key insight is that single-grasp features can effectively guide bimanual grasp generation, rather than treating dual-arm coordination as two independent problems. BiGraspFormer introduces a novel Single-Guided Bimanual (SGB) strategy: it first generates diverse single-arm grasp candidates, then leverages their learned features through specialized attention mechanisms to jointly predict bimanual poses and quality scores. This unified approach eliminates separate modules and explicitly models coordination between grasps, enabling stable and efficient dual-arm manipulation. Comprehensive experiments in both simulation and real-world environments demonstrate that BiGraspFormer achieves superior success, diversity, and speed compared to existing methods.\n\nOur contributions are summarized as follows:\n\nWe propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\nWe propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\nWe introduce the Single-Guided Bimanual (SGB) strategy, which leverages single-arm grasp features to guide bimanual generation, reducing computational complexity and enhancing dual-arm coordination.\n\nWe achieve state-of-the-art bimanual grasping performance while maintaining fast inference suitable for real-world deployment.\n\n1. We propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\n2. We introduce the Single-Guided Bimanual (SGB) strategy, which leverages single-arm grasp features to guide bimanual generation, reducing computational complexity and enhancing dual-arm coordination.\n\n3. We achieve state-of-the-art bimanual grasping performance while maintaining fast inference suitable for real-world deployment.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在点云上直接生成协调的双臂抓取姿态（12-DoF）以实现稳定的双臂物体操作\n2. 现有方法的模块化导致双臂协作差、碰撞风险与力分配不均等问题\n3. 如何在端到端框架中有效建模双臂协调并提高推理速度\n\n【用了什么创新的方案】\nSingle-Guided Bimanual (SGB) 策略：先通过变换器解码器生成多样的单臂抓取候选，再利用这些单臂抓取的特征，通过专门的注意力机制共同预测双臂抓取的姿态与质量分数，从而端到端地产生协调的双臂抓取。该统一框架 eliminates 分离的抓取生成与评估模块，直接从对象点云进行协调抓取的生成与评分。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation",
            "authors": "Sarvesh Prajapati,Ananya Trivedi,Nathaniel Hanson,Bruce Maxwell,Taskin Padir",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 10 figures, submitted to Robotic Computing & Communication",
            "pdf_link": "https://arxiv.org/pdf/2509.19105",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19105",
            "arxiv_html_link": "https://arxiv.org/html/2509.19105v1",
            "abstract": "Successful navigation in outdoor environments requires accurate prediction of the physical interactions between the robot and the terrain. To this end, several methods rely on geometric or semantic labels to classify traversable surfaces. However, such labels cannot distinguish visually similar surfaces that differ in material properties. Spectral sensors enable inference of material composition from surface reflectance measured across multiple wavelength bands. Although spectral sensing is gaining traction in robotics, widespread deployment remains constrained by the need for custom hardware integration, high sensor costs, and compute-intensive processing pipelines. In this paper, we present RGB Image to Spectral Signature Neural Network (RS-Net), a deep neural network designed to bridge the gap between the accessibility of RGB sensing and the rich material information provided by spectral data. RS-Net predicts spectral signatures from RGB patches, which we map to terrain labels and friction coefficients. The resulting terrain classifications are integrated into a sampling-based motion planner for a wheeled robot operating in outdoor environments. Likewise, the friction estimates are incorporated into a contact-force–based MPC for a quadruped robot navigating slippery surfaces. Thus, we introduce a framework that learns the task-relevant physical property once during training and thereafter relies solely on RGB sensing at test time. The code is available at https://github.com/prajapatisarvesh/RS-Net.",
            "introduction": "Autonomous robots are increasingly deployed in everyday settings, ranging from self-driving taxis [1] and search-and-rescue missions [2, 3] to wildfire prevention [4, 5]. In such unstructured environments, reliable autonomy demands more than obstacle avoidance. It requires precise reasoning about how terrain properties influence motion. For example, vehicles must modulate braking on icy roads, and off-road platforms should bypass dense swamps to avoid entrapment. These scenarios show that perception must move beyond geometry and semantics toward reliable estimates of robot–terrain interactions.\n\nSeveral off-road motion planning pipelines use RGB cameras to identify terrain from images [6, 7, 8]. In some cases, visually similar surfaces with different physical properties, such as ice on asphalt, may be mislabeled, leading to invalid traversability cost maps. Depth cameras and LiDAR are often used to estimate the ease of motion over a surface [9, 10]. However, the robot must first drive the terrain to create a dataset, which risks hardware damage and necessitates tuning specific to the operating site.\n\nIn contrast, spectral sensors offer a non-invasive way to estimate material properties. This is accomplished by leveraging distinct patterns of light absorption and reflection, known as spectral signatures, to characterize underlying material composition. These capabilities are finding use in robotics applications such as wildfire risk monitoring [3], manipulation [11, 12], and exploration [13]. By mapping spectral signatures to physical quantities such as moisture content, rigidity, or surface type, the same sensing stack can be repurposed across robots and environments with minimal changes to the processing pipeline. However, challenges such as custom mounts, calibration requirements, large datasets, and high sensor costs currently limit deployment at scale.\n\nRGB cameras are inexpensive, widely available, and already standard in robotic perception pipelines such as object detection [14] and tracking [15]. Compared to hyperspectral systems, they are cheaper, lighter, and more power efficient, which simplifies integration on mobile platforms. Advances in deep learning [16] now allow RGB imagery to approximate measurements traditionally obtained from more information-dense sensors.\n\nMotivated by this, we seek to retain the deployment advantages of RGB cameras while recovering spectral sensor features. We introduce RGB Image to Spectral Signature Neural Network or RS-Net, a deep neural network architecture trained on spectral data collected from diverse materials. It maps RGB image patches to their corresponding spectral signatures. These estimates are passed to a lightweight feedforward neural network whose weights are fine-tuned for the target physical property. We retrain this network once per task, enabling the same neural network architecture to perform terrain classification and friction estimation. Our entire inference pipeline runs at approximately 5 Hz, making it suitable for real-time robotic applications. Fig. LABEL:fig:paper_intro outlines the proposed architecture.\n\nWe validate our method in both simulation and hardware experiments. The terrain classification is used in a sampling-based motion planner for outdoor navigation of a skid-steer robot. Similarly, the friction estimates are integrated into a model predictive control (MPC) scheme for a quadrupedal robot operating on slippery surfaces. Finally, we also discuss how the proposed approach generalizes to other robots and additional physical properties relevant to off-road planning.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在不增加硬件成本的情况下，通过RGB图像推断材料光谱特征以更准确地评估地形与摩擦\n2. how to bridge RGB sensing with spectral properties to improve terrain traversability prediction and motion planning\n3. 将光谱特征转化为可用于采样式路径规划和MPC的物理属性\n4. 实时性与通用性：在多种环境与机器人上实现低成本、快速推断的框架\n\n【用了什么创新的方案】\nRS-Net 将 RGB patch 映射到光谱签名，在此基础上训练一个轻量前馈网络输出地形标签和摩擦系数；整个管线在测试时仅需 RGB，推理约 5 Hz；通过在训练阶段明确学习任务相关的物理属性，使同一架构可一次性对不同任务进行微调并发布到实际机器人上。代码开放，支持仿真与硬件实验中的地形导航和滑移表面的 MPC/规划集成。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation",
            "authors": "Hongli Xu,Lei Zhang,Xiaoyue Hu,Boyang Zhong,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "project website:this https URL, 11 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.19102",
            "code": "https://sites.google.com/view/funcanon",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19102",
            "arxiv_html_link": "https://arxiv.org/html/2509.19102v1",
            "abstract": "General-purpose robotic skills from end-to-end demonstrations often leads to task-specific policies that fail to generalize beyond the training distribution.\nTherefore, we introduce FunCanon, a framework that converts long-horizon manipulation tasks into sequences of action chunks, each defined by an actor, verb, and object.\nThese chunks focus policy learning on the actions themselves, rather than isolated tasks, enabling compositionality and reuse. To make policies pose-aware and category-general, we perform functional object canonicalization for functional alignment and automatic manipulation trajectory transfer, mapping objects into shared functional frames using affordance cues from large vision–language models.\nAn object-centric and action-centric diffusion policy FuncDiffuser trained on this aligned data naturally respects object affordances and poses, simplifying learning and improving generalization ability.\nExperiments on simulated and real-world benchmarks demonstrate category-level generalization, cross-task behavior reuse, and robust sim-to-real deployment, showing that functional canonicalization provides a strong inductive bias for scalable imitation learning in complex manipulation domains.\nDetails of the demo and supplemental material are available on our project website\n https://sites.google.com/view/funcanon.",
            "introduction": "As robots transition from controlled laboratory settings to unstructured real-world environments, developing robust and generalizable manipulation policies becomes increasingly critical. A fundamental challenge is enabling agents to generalize across unseen objects, diverse poses, and varying tasks — a capability that remains elusive for current imitation learning approaches.\n\nImitation learning methods based on RGB images [1] or point clouds [2] often suffer from limited precision and generalization due to viewpoint sensitivity, noisy observations, and redundant scene encodings. In contrast, 3D scene representations have shown promise in improving generalization [3]. To further address these challenges, object-centric representations, which focus on structured, object-level information such as 6D poses [4, 5] and scene flow [6], have gained significant attention. SPOT [4] demonstrates that SE(3) pose diffusion policy can improve cross-embodiment generalization, even when trained solely on passive human videos.\nExisting object-centric approaches [4] often depend on instance-specific, goal-conditioned trajectories, limiting generalization across categories and tasks. We attribute this to viewing manipulation as monolithic programs rather than modular, reusable behaviors.\nIn the field of computer vision, prior work, such as UAD [7] and Object Canonicalization [8], mainly targets improving visual representations and semantic understanding. Related efforts have also investigated category-level affordance pose estimation [9]. However, these methods have not explored how such representations can be leveraged for improving category-level alignment of robotic manipulation data, or augmenting manipulation trajectories.\n\nKey open questions remain: how can generalized representations be leveraged to synthesize diverse manipulation data, model long-horizon tasks, and train robust, generalizable manipulation policies?\n\nTo address these challenges, we propose FunCanon, a framework that models manipulation as compositions of reusable action primitives—such as pouring, grasping, or inserting—defined over functionally aligned bi-object interactions. By leveraging affordance cues from large vision-language models (VLMs), FunCanon canonicalizes semantically related objects (e.g., kettles and pitchers) into shared functional frames. This functional alignment enables the automatic manipulation trajectory transfer and the training of pose-aware, object-centric diffusion policies that focus solely on interaction dynamics, decoupled from specific object identities, camera viewpoints, or task semantics. To achieve robust and generalizable manipulation, we first decompose long-horizon tasks into meaningful action chunks, each specifying an actor, an action, and an object. This segmentation is performed by a large multimodal language model (MLLM), such as GPT-4o, in combination with large vision models (LVM) that extract affordance cues to guide chunking based on functional relevance. Next, we integrate these affordance cues with precise object pose estimates to perform functional alignment, canonicalizing objects into shared functional frames. This process identifies action-related affordance regions and aligns bi-object poses, producing a semantically grounded representation of manipulation interactions. Leveraging this functional alignment, automatic trajectory transfer method is proposed to augment training data on RLBench base tasks to increase data diversity and functional coverage. During policy training and inference, our object-centric diffusion policy receives inputs encoding both affordances, poses of bi-object pairs, point clouds and action verb and estimate actions.\nOur main contributions are:\n\nIntroducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\nIntroducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\nLeveraging large vision-language models for affordance-driven functional canonicalization for functional alignment and automatic trajectory transfer, enabling pose-aware and category-generalizable policy learning.\n\nDeveloping an object-centric diffusion policy trained on functionally aligned data, achieving both instance-level and category-level generalization and robust sim-to-real transfer.\n\n1. Introducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\n2. Leveraging large vision-language models for affordance-driven functional canonicalization for functional alignment and automatic trajectory transfer, enabling pose-aware and category-generalizable policy learning.\n\n3. Developing an object-centric diffusion policy trained on functionally aligned data, achieving both instance-level and category-level generalization and robust sim-to-real transfer.",
            "llm_summary": "【关注的是什么问题】\n1. 如何将长时程操纵任务分解为可重复使用的动作原语以提高泛化性\n2. 如何利用功能对齐和对象-功能框架实现从示例到通用策略的迁移\n3. 如何在无关对象身份、视角和任务语义的前提下实现姿态感知的策略学习\n4. 如何通过对齐数据训练对象中心的扩散策略以增强实例级和类别级泛化能力\n\n【用了什么创新的方案】\n- 将长时程操作分解为由行为者-动作-对象组成的可重用动作原语，并在功能对齐的双对象框架内进行分段\n- 通过大视觉语言模型的 affordance 提供和功能 canonicalization 将相关对象映射到共享功能框架，实现自动轨迹迁移\n- 基于对齐数据训练对象中心的扩散策略 FuncDiffuser，使策略在对象姿态、可供性和交互动力学上具备姿态感知能力并实现良好跨域泛化\n- 结合多模态分段（MLLM/ LVM）与功能对齐，进行自动化的轨迹迁移和增强数据多样性\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation",
            "authors": "Zhennan Jiang,Kai Liu,Yuxin Qin,Shuai Tian,Yupeng Zheng,Mingcai Zhou,Chao Yu,Haoran Li,Dongbin Zhao",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19080",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19080",
            "arxiv_html_link": "https://arxiv.org/html/2509.19080v1",
            "abstract": "Robotic manipulation policies are commonly initialized through imitation learning, but their performance is limited by the scarcity and narrow coverage of expert data. Reinforcement learning can refine polices to alleviate this limitation, yet real-robot training is costly and unsafe, while training in simulators suffers from the sim-to-real gap. Recent advances in generative models have demonstrated remarkable capabilities in real-world simulation, with diffusion models in particular excelling at generation. This raises the question of how diffusion model-based world models can be combined to enhance pre-trained policies in robotic manipulation. In this work, we propose World4RL, a framework that employs diffusion-based world models as high-fidelity simulators to refine pre-trained policies entirely in imagined environments for robotic manipulation. Unlike prior works that primarily employ world models for planning, our framework enables direct end-to-end policy optimization. World4RL is designed around two principles: pre-training a diffusion world model that captures diverse dynamics on multi-task datasets and refining policies entirely within a frozen world model to avoid online real-world interactions. We further design a two-hot action encoding scheme tailored for robotic manipulation and adopt diffusion backbones to improve modeling fidelity. Extensive simulation and real-world experiments demonstrate that World4RL provides high-fidelity environment modeling and enables consistent policy refinement, yielding significantly higher success rates compared to imitation learning and other baselines. More visualization results are available at https://world4rl.github.io/.",
            "introduction": "Despite recent progress in robotic manipulation, the field still faces critical challenges for practical deployment. Imitation learning is widely used to bootstrap policies from demonstrations, but its effectiveness is constrained by the inconsistency[1] and limited diversity[2, 3, 4] of available datasets. Although offline reinforcement learning (RL) can extract better policies from imperfect data, its susceptibility to overestimation[5] still makes it difficult to work effectively with limited datasets. Online RL offers a natural way to refine such pre-trained policies through interaction. However, real-robot RL, while capable of overcoming dataset limitations, suffers from high interaction costs and significant safety risks that hinder large-scale training. Training in simulation avoids these risks but inevitably introduces discrepancies from real-world physics, leading to a persistent sim-to-real gap[6].\n\nIn recent years, generative models have achieved remarkable progress in the visual domain[7], with diffusion models[8] demonstrating particularly strong performance in image[9] and video generation[10, 11]. Such generative capacity opens new opportunities for modeling complex and dynamic environments, offering a promising path toward learnable world simulators that provide realistic yet flexible environments for RL training in robotic manipulation.\n\nBuilding on this idea, we introduce World4RL, a framework that systematically integrates diffusion world models into RL for robotic manipulation. World4RL follows a two-stage paradigm: we first pre-train a diffusion world model on multi-task datasets to capture diverse dynamics, and then refine policies entirely within the frozen model to avoid costly and unsafe online interactions. Serving as a high-fidelity simulator, the world model is composed of a diffusion transition model that predicts future observations conditioned on current observations and actions, and a reward classifier that provides sparse success signals, enabling policy optimization without real-world rollouts.\n\nThis design of framework contrasts with prior approaches such as IRASim[12] and NWM[13], which primarily use generative video models for planning at test time rather than for direct policy training. A closer line of work, DiWA[14], also employs world models for policy learning. However, it relies on recurrent state-space models (RSSM[15]), which lead to blurry generations and compounding rollout errors. In contrast, World4RL leverages diffusion backbones that generate sharper and temporally coherent rollouts, thereby supporting effective end-to-end reinforcement learning.\n\nTo further adapt world models to robotic manipulation, which involves high-dimensional action spaces and complex environment interactions compared to navigation[13] and games[16], we investigate two critical design choices: a two-hot action encoding[17] scheme that provides an efficient representation of continuous actions while enabling lossless reconstruction, thereby serving as a robust bridge between the RL agent and the world model, and diffusion backbone architectures that determine the fidelity and consistency of predictions. These considerations are essential for enabling diffusion world models to serve not only as visual predictors but also as reliable simulators for policy training. To this end, our work makes the following key contributions.\n\nWe propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\nWe propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\nTo improve modeling fidelity and enable more effective policy refinement, we design a two-hot action encoding tailored for robotic manipulation and adopt a diffusion backbone as the world model.\n\nWe validate the effectiveness of World4RL through extensive experiments, showing that it consistently outperforms competitive baselines and significantly enhances policy refinement, improving success rates by 16% and 25% in simulation and real-robot experiments, respectively.\n\n1. We propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\n2. To improve modeling fidelity and enable more effective policy refinement, we design a two-hot action encoding tailored for robotic manipulation and adopt a diffusion backbone as the world model.\n\n3. We validate the effectiveness of World4RL through extensive experiments, showing that it consistently outperforms competitive baselines and significantly enhances policy refinement, improving success rates by 16% and 25% in simulation and real-robot experiments, respectively.",
            "llm_summary": "【关注的是什么问题】\n1. 如何利用扩散模型构建高保真世界模型以提升机器人操控策略的学习效率与安全性（≤40词）\n2. 如何在不在线真实交互的前提下，通过在冻结的扩散世界模型中对策略进行端到端优化来实现改进（≤40词）\n3. 如何设计适合高维持续动作空间的两热编码以及扩散骨干网络以提升生成的时序一致性与预测保真度（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出 World4RL，将扩散式世界模型作为高保真仿真器用于策略的离线强化学习训练；先在多任务数据集上预训练扩散世界模型，再在冻结的模型中进行端到端策略优化；使用两热动作编码和扩散骨干来提升建模保真度与鲁棒性；通过扩散转移模型预测未来观测并结合奖励分类器提供稀疏成功信号，实现无真实滚动的策略 refinement。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions",
            "authors": "Laura Connolly,Aravind S. Kumar,Kapi Ketan Mehta,Lidia Al-Zogbi,Peter Kazanzides,Parvin Mousavi,Gabor Fichtinger,Axel Krieger,Junichi Tokuda,Russell H. Taylor,Simon Leonard,Anton Deguet",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19076",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19076",
            "arxiv_html_link": "https://arxiv.org/html/2509.19076v1",
            "abstract": "Image-guided robotic interventions involve the use of medical imaging in tandem with robotics. SlicerROS2 is a software module that combines 3D Slicer and robot operating system (ROS) in pursuit of a standard integration approach for medical robotics research. The first release of SlicerROS2 demonstrated the feasibility of using the C++ API from 3D Slicer and ROS to load and visualize robots in real time. Since this initial release, we’ve rewritten and redesigned the module to offer greater modularity, access to low-level features, access to 3D Slicer’s Python API, and better data transfer protocols. In this paper, we introduce this new design as well as four applications that leverage the core functionalities of SlicerROS2 in realistic image-guided robotics scenarios.",
            "introduction": "Medical robotics is an evolving and rapidly growing research field with the potential to transform standard clinical practice. It is possible that robots will one day transcend human capabilities while offering higher efficiency, lower costs, improved training outcomes and better safety [1]. The advancement of image-guided robotics in particular, which are systems that rely on both medical imaging and robotics, is critical for achieving this potential. This is because image-guided robots can be used to fuse preoperative and intraoperative realities [2].\n\nThere are several combinations of imaging modalities and robotic systems have been explored in this capacity. For example, the SpineBot uses computed tomography (CT) imaging to help define the trajectory of pedicle screws, and robotics to guide the surgeon through those trajectories [3], [4]. Another example is the MrBot, which was designed to help perform percutaneous needle interventions within the confines of a magnetic resonance imaging (MRI) scanner [5]. Similarly, the Artemis robot was designed to facilitate transrectal prostate biopsy under ultrasound guidance with MRI fusion [6]. These are just a few examples of the numerous procedures and therapies where the use of image-guidance in tandem with robotics has been investigated. More recently, advancements in image-guided robotics have enabled: navigation of catheters into blood vessels with magnetic continuum devices [7], autonomous needle steering for lung biopsy [8] and teleoperated neurovascular interventions [9].\n\nDespite this extensive investigation, there are only a few areas of intervention where image-guided robotic systems that have achieved widespread adoption and financial commercial success such as robotic bronchoscopy, radiation oncology and neurosurgery [2] [10]. One contributor to this slow growth and adoption is the lack of a common integration approach. For any image-guided robotic system, integration of the imaging modality and the robot is the most important factor for usability. However, several companies and research systems take their own unique approach to integration. This results in device-specific software, expensive research licenses, incompatible communication protocols, and overall, a high barrier to entry to develop such systems. Considering these challenges and their potential threat to continued development, it is imperative to provide a common integration scheme for image-guided robotics. We hypothesize that this will prevent re-engineering and promote reproducibility across different clinical applications.\n\nFrom a development perspective, image-guided therapy (IGT) platforms and medical robotics platforms are often separated. In the realm of IGT, the open-source medical imaging platform, 3D Slicer is the most commonly used research platform [11] [12]. With over one million downloads and an active research and support community, 3D Slicer is used for segmentation, virtual reality, image analysis, artificial intelligence, and several other applications [13]. Several research platforms have been enabled by or added to 3D Slicer such as SlicerIGT [12], SlicerVR [14], MONAI label [15], and Total Segmentator [16]. There are also open-source platforms like OpenIGTLink and the PLUS toolkit that allow users to interface commercial hardware with 3D Slicer to build complete IGT systems [17] [18]. As a result of these efforts, 3D Slicer is considered the de-facto open source software for developing navigated, image-guided interventions.\n\nIn the field of medical robotics, robot operating system (ROS), an open-source middleware designed to support robotics development, is the predominant framework. ROS is a modular development framework that provides tools for autonomous navigation, simulation, visualization, and control [19]. Like 3D Slicer, ROS has a very active research community that is constantly contributing to the platform. For medical robotics specifically, several research tools like the Computer Integrated Surgical Systems Surgical Assistant Workstation (CISST-SAW) libraries [20] and the Asynchronous Multi-Body Framework (AMBF) [21] support ROS. The da Vinci Research Kit (dVRK), a popular open-source medical robot [22], also supports ROS for software development. Furthermore, many commercially available robots provide a ROS interface off the shelf.\n\nIn an effort to pursue a common integration scheme for image-guided robotics research, we decided to bridge these two ecosystems. As evidenced by the numerous published papers that employ both 3D Slicer and ROS (71 papers available on Google Scholar using keywords “3D Slicer” AND “robot operating system”), there is also demand from the community for this integration. Previous attempts to bridge 3D Slicer and ROS such as the ROS-IGTL bridge [23], custom applications for specific robots [24] [25], and our initial offering of SlicerROS2 [26] have fallen short of meeting all of the needs of a common integration tool. These needs include greater usability by providing access to low-level features, robust data transfer protocols that support commonly used message types, thorough documentation, and maintainability [27]. We have since redesigned SlicerROS2 to further support image-guided robotics research considering these requirements. The details of this new design are described in the following sections. The contributions of this paper are: 1) A newly designed research module for efficient data transfer between 3D Slicer and ROS 2 and 2) four relevant applications that demonstrate how it can be used for rapid research prototyping.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在图像引导的机器人干预中实现 3D Slicer 与 ROS2 的高效、通用集成\n2. 提高跨平台数据传输的鲁棒性与对低级功能的可访问性\n3. 快速原型化研究所需的模块化、可维护的工具体系\n4. 在医学机器人研究中促进可重复性与协同开发的标准化方案\n【用了什么创新的方案】\n核心解决方案：重新设计的 SlicerROS2 模块实现更高模块化和低级功能访问，整合 3D Slicer 的 Python 与 C++ API，改进数据传输协议，支撑多种消息类型，并提供四个实际应用场景以展示核心功能在图像引导机器人中的应用与原型化能力\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation",
            "authors": "Geonhyup Lee,Yeongjin Lee,Kangmin Kim,Seongju Lee,Sangjun Noh,Seunghyeok Back,Kyoobin Lee",
            "subjects": "Robotics (cs.RO)",
            "comment": "9 pages, 9 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19047",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19047",
            "arxiv_html_link": "https://arxiv.org/html/2509.19047v1",
            "abstract": "Contact-rich manipulation tasks such as precision assembly require precise control of interaction forces, yet existing imitation learning methods rely mainly on vision-only demonstrations. We propose ManipForce, a handheld system designed to capture high-frequency force–torque (F/T) and RGB data during natural human demonstrations for contact-rich manipulation. Building on these demonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT). FMT encodes asynchronous RGB and F/T signals using frequency- and modality-aware embeddings and fuses them via bi-directional cross-attention within a transformer diffusion policy. Through extensive experiments on six real-world contact-rich manipulation tasks—such as gear assembly, box flipping, and battery insertion—FMT trained on ManipForce demonstrations achieves robust performance with an average success rate of 83% across all tasks, substantially outperforming RGB-only baselines. Ablation and sampling-frequency analyses further confirm that incorporating high-frequency F/T data and cross-modal integration improves policy performance, especially in tasks demanding high precision and stable contact.\nHardware, software, and video demos are available at: https://sites.google.com/view/manipforce/홈.",
            "introduction": "Contact-rich manipulation tasks such as precise assembly [1, 2, 3, 4], battery disassembly [5], and non-prehensile handling [6] require high precision and force-aware manipulation. Humans naturally perceive contact forces and their subtle changes when assembling parts, adjusting their strategies accordingly. Yet most robotic approaches rely solely on visual demonstrations, missing the rich F/T information humans provide.\n\nRecent advances in imitation learning [7, 8, 9] have demonstrated strong potential for dexterous and contact-rich manipulation by learning directly from human demonstrations.\nHowever, these methods still rely on high-quality demonstration data, which is costly and difficult to collect for fine-grained interactions.\nHand-held data collection systems such as UMI [10] have been proposed to address this challenge by enabling natural human demonstrations without the expertise requirements and remote-control limitations of teleoperation.\nWhile effective for simplifying demonstration collection, UMI does not capture force–torque (F/T) information, which is essential for accurately modeling contact behaviors.\nMore recent work [11] combines visual and F/T data but relies on point clouds to represent the scene, which introduces complex setup requirements and fundamentally limits the ability to perceive small objects and fine clearances essential for contact-rich manipulation.\nFurthermore, from a learning perspective, this approach down-samples high-frequency F/T signals to match the image frame rate, losing rich temporal information necessary for modeling contact dynamics.\n\nTo address these limitations, we introduce ManipForce a handheld system for simultaneous RGB–F/T data collection during natural human demonstrations, and the Frequency-Aware Multimodal Transformer (FMT), which learns robust policies from the collected data for diverse, precise, and contact-rich manipulation tasks.\n\nManipForce consists of a dual handheld camera setup with a wrist-mounted F/T sensor to capture both visual and high-frequency force signals during human-guided demonstrations.\nThis configuration enables robust perception of small objects, tight clearances, and fine-grained contacts, allowing collected demonstrations to transfer directly to robotic execution.\nWe replace SLAM-based wrist tracking with 3D ArUco marker pose estimation to maintain accuracy during close-contact interactions without environmental dependencies, and apply tool gravity compensation to ensure precise and interaction-focused F/T measurements.\nWe propose the FMT, which learns from asynchronous RGB (30 Hz) and F/T (>200 Hz) signals using a Transformer-based Diffusion Policy [7] architecture.\nTo exploit the higher-frequency force signals relative to images, the model tokenizes both RGB and F/T inputs using learnable frequency and modality embeddings.\nThis design enables the model to effectively handle heterogeneous modalities with asynchronous sampling rates.\nIn addition, bi-directional cross-attention modules fuse complementary information across modalities.\nWe evaluate our approach on six contact-rich manipulation tasks spanning precision assembly, non-prehensile manipulation, and complex disassembly, and observe significant performance gains over RGB-only baselines.\nAblation studies further confirm that high-frequency F/T sensing, unified positional embeddings, and bi-directional cross-attention each make complementary contributions to robust multimodal policy learning.\n\nOur main contributions are:\n\nWe introduce ManipForce, a handheld RGB–F/T data collection system enabling diverse and fine-grained contact-rich manipulation demonstrations.\n\nWe propose FMT, which handles inputs with asynchronous sampling rates through frequency-aware multimodal representation learning and cross-attention within a Transformer architecture, enabling robust policy learning for contact-rich manipulation.\n\nWe demonstrate robust performance on diverse contact-rich manipulation tasks—including gear assembly, plug insertion, battery disassembly, and lid operations—consistently outperforming RGB-only baselines.\n\n1. We introduce ManipForce, a handheld RGB–F/T data collection system enabling diverse and fine-grained contact-rich manipulation demonstrations.\n\n2. We propose FMT, which handles inputs with asynchronous sampling rates through frequency-aware multimodal representation learning and cross-attention within a Transformer architecture, enabling robust policy learning for contact-rich manipulation.\n\n3. We demonstrate robust performance on diverse contact-rich manipulation tasks—including gear assembly, plug insertion, battery disassembly, and lid operations—consistently outperforming RGB-only baselines.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在接触丰富的操纵任务中同时利用高频力矩信息与RGB视觉实现鲁棒仿生策略\n2. 如何在异步采样率下融合多模态数据以提升策略学习的准确性与稳定性\n3. 如何减少对复杂场景感知的依赖并实现自然人类演示的高效数据收集\n\n【用了什么创新的方案】\nManipForce 提供可手持的 RGB–F/T 数据采集系统，结合高频力矩与 RGB 数据；FMT 使用频率感知的多模态表示对异步 RGB(30 Hz) 与 F/T(>200 Hz) 信号进行编码，采用双向交叉注意力在 Transformer diffusion 策略中进行跨模态融合，显著提升对接触力与微小几何的建模能力；并通过 3D ArUco 标定与工具重力补偿实现近距离接触的高精度演示对齐与采样。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors",
            "authors": "Qingzheng Cong,Steven Oh,Wen Fan,Shan Luo,Kaspar Althoefer,Dandan Zhang",
            "subjects": "Robotics (cs.RO)",
            "comment": "14 pages, 8 figures. Equal contribution: Qingzheng Cong, Steven Oh, Wen Fan. Corresponding author: Dandan Zhang (d.zhang17@imperial.this http URL). Additional resources atthis http URL",
            "pdf_link": "https://arxiv.org/pdf/2509.19037",
            "code": "http://stevenoh2003.github.io/TacEva/, http://ac.uk",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19037",
            "arxiv_html_link": "https://arxiv.org/html/2509.19037v1",
            "abstract": "Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because of the high spatial resolution they offer and their relatively low manufacturing costs. However, variations in their sensing mechanisms, structural dimension, and other parameters lead to significant performance disparities between existing VBTSs. This makes it challenging to optimize them for specific tasks, as both the initial choice and subsequent fine-tuning are hindered by the lack of standardized metrics. To address this issue, TacEva is introduced as a comprehensive evaluation framework for the quantitative analysis of VBTS performance. The framework defines a set of performance metrics that capture key characteristics in typical application scenarios. For each metric, a structured experimental pipeline is designed to ensure consistent and repeatable quantification. The framework is applied to multiple VBTSs with distinct sensing mechanisms, and the results demonstrate its ability to provide a thorough evaluation of each design and quantitative indicators for each performance dimension. This enables researchers to pre-select the most appropriate VBTS on a task by task basis, while also offering performance-guided insights into the optimization of VBTS design. A list of existing VBTS evaluation methods and additional evaluations can be found on our website:\nhttps://stevenoh2003.github.io/TacEva/.",
            "introduction": "Robots have yet to attain the level of manipulative dexterity exhibited by humans, a challenge rooted in the difficulty of accurately acquiring detailed contact information in physical environments [1]. Tactile sensing has therefore become indispensable for delicate and precise robotic manipulation in embodied intelligence systems [2]. A notable development in this domain has been the rise of vision-based tactile sensors (VBTSs) [3]. These sensors employ high-resolution cameras to capture detailed contact surface information, thereby integrating seamlessly with computer vision and image-based deep learning methods. Nevertheless, among VBTSs, we see a wide variety of architectures, structural dimensions, and fabrication techniques, depending on the exact nature of the application requirements [4].\n\nThe rapid development of VBTSs has created an urgent need for standardized performance evaluation. Selecting an appropriate VBTS for a specific task scenario remains challenging, as distinct sensor designs offer substantial variation in performance. A universal evaluation protocol would therefore facilitate fair comparison across sensor designs, support informed selection, and guide design optimization. However, a significant gap persists in the field: no standardized framework currently exists for VBTS evaluation, and the inconsistency of current metrics limits objective, reproducible, and comprehensive cross-sensor assessment. This challenge is further compounded by the inherently multi-modal nature of tactile sensing [5], which necessitates coordinated evaluation across multiple performance dimensions.\n\nThe application scenarios for VBTSs are inherently diverse. This makes it difficult to establish universal, broadly applicable evaluation metrics that remain meaningful across the field. Further complexity arises from the fact that VBTSs are commonly fabricated using silicone elastomers, introducing additional considerations related to soft-material mechanics, optics, and imaging. Owing to differences in their underlying sensing mechanisms, each VBTS design inevitably exhibits its own profile of strengths and weaknesses. As a result, certain positive performance characteristics will typically tend to be emphasized, while weaker ones may be under-reported. This is likely to complicate informed selection, making it more challenging for prospective users.\n\nTo address these challenges, we propose TacEva, a systematic evaluation framework that integrates performance quantification with a structured and reproducible assessment pipeline. TacEva is designed to provide consistency and comparability across VBTS designs, enabling practitioners to make evidence-based decisions while obtaining a holistic understanding of sensor performance (see Fig. 1). It also offers sensor developers clear guidance for targeted optimization during the design process. By defining and standardizing a comprehensive set of performance metrics and evaluation protocols, TacEva aims to provide a unified and transparent characterization of VBTS designs, thereby facilitating objective comparison, reliable selection, and more informed innovation for future sensor development.",
            "llm_summary": "【关注的是什么问题】\n1. 缺乏标准化评估框架导致VBTSs在不同设计间难以公平比较与任务级优化（≤40词）\n2. 多模态触觉传感的评估指标缺乏统一、可重复的量化流程（≤40词）\n3. 不同VBTS sensing机制、结构与材料差异造成性能特征分布不一致，难以选型与设计优化（≤40词）\n4. 缺少可跨传感器的全面综合性能指标与任务驱动的性能指示（≤40词）\n\n【用了什么创新的方案】\nTacEva 提供一个综合的性能评估框架，定义一套标准化、可重复的评测指标和实验流水线，覆盖典型应用场景的关键性能维度，并可对多种VBTS 进行横向比较与任务驱动的选型与优化指导。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion",
            "authors": "Shuai Liu,Meng Cheng Lau",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "11 pages, 5 figures, 1 table, Computational Science Graduate Project",
            "pdf_link": "https://arxiv.org/pdf/2509.19023",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19023",
            "arxiv_html_link": "https://arxiv.org/html/2509.19023v1",
            "abstract": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a two-stage reinforcement learning framework for humanoid walking that requires no motion capture data or elaborate reward shaping. In the first stage, a compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via Proximal Policy Optimization. This generates energy-efficient gait templates. In the second stage, those dynamically consistent trajectories guide a full-body policy trained with Soft Actor–Critic augmented by an adversarial discriminator, ensuring the student’s five-dimensional gait feature distribution matches the ROM’s demonstrations. Experiments at 1 m/s and 4 m/s show that ROM-GRL produces stable, symmetric gaits with substantially lower tracking error than a pure-reward baseline. By distilling lightweight ROM guidance into high-dimensional policies, ROM-GRL bridges the gap between reward-only and imitation-based locomotion methods, enabling versatile, naturalistic humanoid behaviors without any human demonstrations.",
            "introduction": "Achieving natural humanoid locomotion is a longstanding goal in both robotics and computer animation. From bipedal robots that can walk and run with human-like grace, to virtual characters that move realistically in games, the ability to synthesize lifelike walking gaits remains a persistent challenge. Traditional model-based control has produced impressive feats, but often requires painstaking design and does not always capture the natural motion nuances of human walking Kuindersma et al. (2016). In recent years, reinforcement learning (RL) has emerged as a promising data-driven paradigm for developing locomotion controllers Radosavovic et al. (2024). RL allows simulated humanoids to learn complex gait behaviors through trial-and-error, offering the potential to discover agile and robust walking strategies that would be difficult to manually design.\n\nReinforcement learning (RL) methods for natural locomotion generally fall into two main paradigms aimed at producing lifelike gait behaviors.\n\nA purely objective‑driven RL policy can discover stable walking gaits by optimizing energy and stability rewards. These reference-free methods train locomotion policies from scratch by optimizing carefully crafted reward functions, without any motion capture examples. The reward terms are designed to encourage physically plausible and human-like traits, such as forward speed with energy efficiency, maintaining center-of-mass stability and upright posture, periodic foot contact patterns, and symmetric gait cycles between left and right legs. By rewarding such objectives, controllers can spontaneously develop stable walking gaits that emerge naturally from learning. Notably, researchers have demonstrated that symmetric bipedal walking can arise solely from reward design and curriculum training, without any reference motions Yu et al. (2018). Indeed, recent work showed that an RL policy trained with biomechanically inspired rewards could produce natural walking behaviors purely through self-exploration Peng et al. (2025). The appeal of this approach is that it does not require any pre-recorded motions – the agent invents its own walking cycle. However, designing the reward function is notoriously difficult: the policy’s behavior is highly sensitive to the choice and weighting of reward terms, often requiring elaborate hand-tuning and expertise. Even with careful tuning, purely objective-driven policies may develop subtle artifacts or unnatural quirks since there is no direct template of “human” motion to imitate. In summary, while motion-free RL can yield impressively natural gaits under the right conditions, it faces challenges in reward engineering and consistency of motion style.\n\nAlternatively, imitation‑driven RL uses human mocap examples to ensure motion realism but at the cost of dataset dependency. To directly ensure natural movement quality, a dominant approach is to imitate example locomotion trajectories from motion capture data. In this paradigm, the RL agent is guided by reference motions of humans (or animals) and receives rewards for matching the reference pose and velocity at each time step, or uses adversarial critics to judge realism against a motion dataset Peng et al. (2018, 2021). DeepMimic pioneered this line of work by showing that standard RL algorithms can learn robust control policies capable of imitating a broad range of example motion clips from a motion capture library Peng et al. (2018). By combining motion imitation objectives with task goals, DeepMimic enabled physically simulated characters to reproduce dynamic skills (flips, spins, walks) with high fidelity to the mocap examples. Following this, numerous studies have pushed the state of the art in imitation-based locomotion. For instance, ASE (Adversarial Skill Embeddings) uses large unstructured motion datasets to train latent skill models via adversarial imitation, yielding a repertoire of reusable behaviors that look remarkably life-like Peng et al. (2022). Another example is CALM (Conditional Adversarial Latent Models), which learns a rich latent representation of human movement through imitation learning, capturing the complexity and diversity of human motion while allowing direct user control over the character’s style and direction Tessler et al. (2023). Most recently, the concept of Behavioral Foundation Models have emerged – here a policy is pre-trained on massive collections of motion data (in an unsupervised manner) to serve as a generalist locomotion model. These foundation models, once trained on unlabeled motion trajectories, can be prompted to perform new tasks in a zero-shot fashion, while retaining human-like gait qualities Tirinzoni et al. (2025a). Motion imitation approaches thus achieve state-of-the-art realism in simulated walking; policies closely mimic human kinematics and can produce motions nearly indistinguishable from motion capture. The downside, however, is their heavy reliance on curated motion data – one must have access to large datasets of reference gaits, and the learned skills are inevitably tied to the distribution of motions in those datasets.\n\nDespite these advances, achieving natural, human‑like locomotion without any motion capture data remains an open challenge. Purely objective‑driven methods often fail to reproduce the fluidity and subtle timing of human gait, while imitation‑based approaches are fundamentally constrained by the availability and diversity of mocap archives.\n\nTo bridge this gap, we introduce a reduced‑order model–guided reinforcement learning(ROM-GRL) framework. It leverages a simplified locomotion model as a stand‑in for motion capture, providing high‑level gait guidance to the RL agent without any human demonstrations. Our framework unfolds in two stages. In the first stage, we train a lightweight teacher model to generate efficient, dynamically consistent gait templates that capture the essence of natural walking. In the second stage, we distill these templates into a full-body controller by rewarding adherence to the teacher’s motion distribution, ensuring smooth, human-like locomotion. By separating high-level gait planning from detailed control, we achieve a single walking policy that is both robust and naturally fluid, all without relying on motion capture data or intricate reward design.\n\nIn summary, our approach marries the insights from model-based gait generation with the flexibility of reinforcement learning. By using a ROM to guide RL instead of direct motion capture, we maintain a purely physics-driven training regime while still inducing realistic movement patterns. The proposed framework demonstrates that natural humanoid locomotion can emerge without demonstrations, closing the gap between reward engineering and motion imitation. We validate that our ROM-guided RL method produces walking controllers that are stable across different speeds and exhibit natural gait symmetry and fluidity comparable to reward-based policies, suggesting a novel solution to produce life-like humanoid locomotion in the absence of motion data.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在没有运动捕捉数据的情况下实现自然、稳定的人形机器人行走\n2. 如何在强化学习中兼顾高层步态规划与全身控制的协同学习\n3. 如何避免繁琐的奖励设计而仍获得自然的步态与对称性\n4. 如何将简化的ROM释放为对高维策略的有效引导\n\n【用了什么创新的方案】\n两阶段ROM-GRL：先用4-DOF简化模型通过PPO生成高效、动态一致的步态模板；再用这些模板对全身策略进行蒸馏，使策略通过对ROM分布的遵循实现自然、对称的行走，并结合带对抗判别的Soft Actor–Critic以匹配ROM示范分布，且无任何人类示范数据。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
            "authors": "Dapeng Zhang,Jin Sun,Chenghui Hu,Xiaoyan Wu,Zhenlong Yuan,Rui Zhou,Fei Shen,Qingguo Zhou",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19012",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19012",
            "arxiv_html_link": "https://arxiv.org/html/2509.19012v1",
            "abstract": "The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.",
            "introduction": "Robotics has long been a prominent area of scientific research. Historically, robots primarily relied on pre-programmed instructions and engineered control policies to perform task decomposition and execution. These methods were commonly applied to simple, repetitive tasks, such as factory assembly lines and logistics sorting. In recent years, the rapid advancement of artificial intelligence has enabled researchers to exploit the feature extraction and trajectory prediction capabilities of deep learning across diverse modalities, including images, text, and point clouds. By integrating techniques such as perception, detection, tracking, and localization, researchers have decomposed robotic tasks into multiple stages to meet execution requirements, thereby advancing the development of embodied intelligence and autonomous driving. However, most of these robots still operate as isolated agents, designed for specific tasks and lacking effective interaction with humans and external environment.\n\nTo address these limitations, researchers have begun exploring the incorporation of large language models (LLMs) and vision language models (VLMs) to enable more accurate and flexible robotic manipulation. Modern robotic manipulation methods [1, 2] typically leverage vision language generative paradigms (e.g., autoregressive models [3, 4, 5, 6] or diffusion models [7]), combined with large-scale datasets [8] and advanced fine-tuning strategies. We refer to these as VLA foundation models, which have substantially improved the quality of robotic manipulations. Fine-grained action control over generated content provides users with greater flexibility, unlocking the practical potential of VLA for task execution.\n\nDespite their promise, reviews of pure VLA methods remain scarce. Existing surveys either focus on taxonomy over VLM foundational models or provide broad overviews of robotic manipulation as a whole. Firstly, VLA methods represent a nascent field in robotics, with no established methodological landscape or consensus taxonomy, making it challenging to systematically summarize these approaches. Secondly, current reviews either classify VLA approaches based on differences in foundational models or present a comprehensive analysis of robotic applications across the entire history of the field, often emphasizing traditional methods at the expense of emerging techniques. While these reviews offer valuable insights, they provide only cursory examinations of robotic models or concentrate primarily on foundational models, leaving a significant gap in the literature regarding pure VLA methods.\n\nIn this paper, we investigate VLA methods and associated resources, providing a focused and comprehensive review of existing approaches. Our goal is to present a clear taxonomy, systematically summarize VLA research, and elucidate the development trajectory of this rapidly evolving field. After a brief overview of LLMs and VLMs, we focus on the policy strategies of VLA models, highlighting the unique contributions and distinctive features of previous studies. We classify VLA approaches into 4 categories: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods, and provide a detailed analysis of their motivations, core strategies, and mechanisms. As shown in Fig. 2, we present a VLA skeleton of these methods.\nWe examine application domains, including robotic arms, quadruped robots, humanoids, and wheeled robots (autonomous vehicles), offering a comprehensive assessment of VLA deployment across diverse scenarios. Given the strong dependence of VLA models on datasets and simulation platforms, we provide a concise overview of these resources. Finally, based on the current VLA landscape, we identify key challenges and outline future research directions—including data limitations, inference speed, and safety—to accelerate the advancement of VLA models and generalizable robotics.\n\nThe overall structure of this survey is illustrated in Fig. 1. First, Section 2 provides an overview of the background for VLA research. Section 3 presents the existing VLA approaches in the robotics field. Section 4 introduces the datasets and benchmarks employed by VLA approaches. Sections 5 and 6 discuss simulation platforms and robotic hardware. Section 7 further discusses the challenges and future directions for VLA-based robotic methods. Finally, we summarize the paper and provide our perspective on future developments.\n\nIn summary, our contributions are as follows:\n\nWe present the well-structured taxonomy of pure VLA methods, classifying approaches based on their action-generation strategies. This facilitates understanding of existing methods and highlights core challenges in the field.\n\nThe survey emphasizes the defining characteristics and methodological innovations of each category and technique, providing a clear perspective on current approaches.\n\nWe provide a comprehensive overview of associated resources (datasets, benchmarks and simulation platforms) for training and evaluating VLA models.\n\nWe investigate the practical impact of VLA in robotics, identify key limitations of existing techniques, and propose potential avenues for further exploration.\n\n1. We present the well-structured taxonomy of pure VLA methods, classifying approaches based on their action-generation strategies. This facilitates understanding of existing methods and highlights core challenges in the field.\n\n2. The survey emphasizes the defining characteristics and methodological innovations of each category and technique, providing a clear perspective on current approaches.\n\n3. We provide a comprehensive overview of associated resources (datasets, benchmarks and simulation platforms) for training and evaluating VLA models.\n\n4. We investigate the practical impact of VLA in robotics, identify key limitations of existing techniques, and propose potential avenues for further exploration.",
            "llm_summary": "【关注的是什么问题】\n1. 如何对纯VLA方法进行清晰分类与系统综述，以揭示其核心动机与实现机制？（≤40词）\n2. VLA模型在机器人执行中的生成策略、数据/基准资源、仿真平台与应用域的整合与挑战是什么？（≤40词）\n3. 现有VLA方法的局限性、安全性与可扩展性如何影响通用机器人能力的提升？（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出基于动作生成策略的纯VLA方法分型（自回归、扩散、强化学习、混合、专用），并对各类方法的动机、核心策略与实现机制进行系统比较；梳理数据集、基准与仿真平台；并给出未来方向与挑战。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Category-Level Object Shape and Pose Estimation in Less Than a Millisecond",
            "authors": "Lorenzo Shaikewitz,Tim Nguyen,Luca Carlone",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18979",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18979",
            "arxiv_html_link": "https://arxiv.org/html/2509.18979v1",
            "abstract": "Object shape and pose estimation is a foundational robotics problem, supporting tasks from manipulation to scene understanding and navigation. We present a fast local solver for shape and pose estimation which requires only category-level object priors and admits an efficient certificate of global optimality. Given an RGB-D image of an object, we use a learned front-end to detect sparse, category-level semantic keypoints on the target object. We represent the target object’s unknown shape using a linear active shape model and pose a maximum a posteriori optimization problem to solve for position, orientation, and shape simultaneously. Expressed in unit quaternions, this problem admits first-order optimality conditions in the form of an eigenvalue problem with eigenvector nonlinearities. Our primary contribution is to solve this problem efficiently with self-consistent field iteration, which only requires computing a 4×44\\times 4 matrix and finding its minimum eigenvalue-vector pair at each iterate. Solving a linear system for the corresponding Lagrange multipliers gives a simple global optimality certificate. One iteration of our solver runs in about 100 microseconds, enabling fast outlier rejection. We test our method on synthetic data and a variety of real-world settings, including two public datasets and a drone tracking scenario. Code is released at https://github.com/MIT-SPARK/Fast-ShapeAndPose.",
            "introduction": "A diverse set of robotics applications benefits from object shape and pose estimation. Autonomous cars, for example, need to locate obstacles and other cars [1], while household manipulators need to locate objects to interact with [2].\nIn many of these applications the object shape is not known exactly but its category is available (e.g., from a semantic segmentation method). We consider this setting and derive a shape and pose estimator using category-level priors.\n\nThe work of Shi et al. [3] established a certifiably optimal approach for category-level shape and pose estimation using a semidefinite relaxation. We consider a similar problem setup but emphasize both speed and certifiability. A fast estimator allows fast reaction to new inputs, performance with limited compute, and comprehensive outlier rejection [4, 5]. Certifiability provides an a posteriori guarantee that the estimate returned is statistically optimal. When the certificate fails, the user can decide to trust the output, try a different initialization, or acquire a new batch of measurements.\n\nOur algorithm relies on the eigenvalue structure of the first-order optimality conditions written in the quaternion representation of rotations. It returns local solutions which are often globally optimal. To verify this, we introduce a fast global optimality certifier based on Lagrangian duality. Specifically, our contributions are:\n\nA fast local solver for category-level shape and pose estimation\nusing self-consistent field iteration [6].\n\nA fast a posteriori certificate of global optimality for our local solutions.\n\nExperimental evaluation of runtime and accuracy on synthetic data, a drone tracking scenario, and two large-scale datasets.\n\nThe remainder of the paper is organized as follows. We begin with a literature review (Section II) and quaternion preliminaries (Section III). Then, we give the problem formulation in Section IV and reformulate it with quaternions in Section V.\nTo solve the nonlinear eigenproblem, we use self-consistent field iteration for local solutions and SDP optimality conditions to certify global optimality in Section VI. In Section VII, we show our method is significantly faster than other local solvers and learned baselines.\n\n1. A fast local solver for category-level shape and pose estimation\nusing self-consistent field iteration [6].\n\n2. A fast a posteriori certificate of global optimality for our local solutions.\n\n3. Experimental evaluation of runtime and accuracy on synthetic data, a drone tracking scenario, and two large-scale datasets.",
            "llm_summary": "【关注的是什么问题】\n1. 在低计算资源下，基于类别级先验进行对象形状与姿态的快速估计\n2. 给定RGB-D，如何通过稀疏语义关键点实现高效、可证伪的全局最优性口径\n3. 如何在四元数表示下将非线性优化转化为可迭代求解并给出全局最优性证据\n\n【用了什么创新的方案】\n核心解决方案：提出基于自洽场迭代的快速局部求解器，用线性主动形状模型表示未知形状，构造最大后验优化以同时估计位置、姿态和形状；在四元数表示下，将一阶最优性条件转化为特征值问题，通过迭代求解一个4×44×4的矩阵并获取最小特征值-向量对；通过拉格朗日乘子求解线性系统，提供快速的全局最优性证书（可后验判断是否可信）。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation",
            "authors": "Minoo Dolatabadi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18954",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18954",
            "arxiv_html_link": "https://arxiv.org/html/2509.18954v1",
            "abstract": "LiDAR-based localization and SLAM often rely on iterative matching algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align sensor data with pre-existing maps or previous scans. However, ICP is prone to errors in featureless environments and dynamic scenes, leading to inaccurate pose estimation. Accurately predicting the uncertainty associated with ICP is crucial for robust state estimation but remains challenging, as existing approaches often rely on handcrafted models or simplified assumptions. Moreover, a few deep learning-based methods for localizability estimation either depend on a pre-built map—which may not always be available—or provide a binary classification of localizable versus non-localizable, which fails to properly model uncertainty.\nIn this work, we propose a data-driven framework that leverages deep learning to estimate the registration error covariance of ICP before matching, even in the absence of a reference map. By associating each LiDAR scan with a reliable 6-DoF error covariance estimate, our method enables seamless integration of ICP within Kalman filtering, enhancing localization accuracy and robustness. Extensive experiments on the KITTI dataset demonstrate the effectiveness of our approach, showing that it accurately predicts covariance and, when applied to localization using a pre-built map or SLAM, reduces localization errors and improves robustness.",
            "introduction": "In recent years, autonomous vehicles have become an integral component of intelligent transportation systems, driving continuous research to push the boundaries of their capabilities. A key requirement for autonomous driving is achieving precise self-localization at the centimeter level [1]. The Global Navigation Satellite System (GNSS) is a cost-effective and widely used method for vehicle localization. Although GNSS delivers reliable positioning in open-sky environments, its accuracy is significantly compromised in urban settings due to factors such as signal blockage, non-line-of-sight (NLOS) conditions, and multipath effects [2]. To address these challenges, vision-based approaches—particularly those leveraging LiDAR—have been proposed as alternative or complementary solutions [3]. In these systems, map matching techniques like the Iterative Closest Point (ICP) algorithm are frequently employed to align sensor data with pre-existing maps, thereby enhancing localization precision [4].\n\nHowever, vision-based localization methods—like other relative localization techniques—are susceptible to error accumulation, where minor errors can progressively lead to significant drift over time. This phenomenon is observed in both Simultaneous Localization and Mapping (SLAM) and pre-built map-based localization approaches, although it tends to be more pronounced in SLAM [5]. In the context of ICP, previous work has demonstrated that factors such as featureless environments (e.g., tunnels) and the presence of dynamic objects can adversely affect the matching accuracy [6].\n\nState estimation methods, such as Kalman filtering, are commonly employed to mitigate these problems. Yet, these techniques depend on an accurate error model (often represented in the simplest form by an error covariance matrix), which is challenging to determine for matching algorithms\n[7].\n\nTo address this limitation, several studies have proposed data-driven approaches to predict ICP error, either using classification-based methods [8] or by directly estimating the covariance [7].\n\nSpecifically, we introduce a data-driven framework that predicts the full six-degree-of-freedom (6-DoF) ICP registration covariance from a single LiDAR scan prior to correspondence search and ICP refinement. The output is a symmetric positive-definite (SPD) 6×66\\times 6 covariance on S​E​(3)SE(3) suitable for probabilistic fusion. An overview of the pipeline is shown in Fig. 1.\n\nContributions.\n\nPre-ICP, per-scan S​E​(3)SE(3) covariance. We predict the full 6×66\\times 6 registration covariance directly from a single LiDAR scan, capturing translation–rotation coupling before ICP is run. Per-scan training targets are obtained by estimating empirical covariances from Monte Carlo ICP under randomized initializations.\n\nKalman fusion and evaluation. Each scan is paired with a reliable 6-DoF covariance, used as the measurement noise in a standard Kalman filter. This improves pose accuracy over fixed or heuristic covariances on the evaluated sequences.\n\nPracticality. The model does not require a pre-built map at inference, supporting both SLAM and map-based operation.\n\n1. Pre-ICP, per-scan S​E​(3)SE(3) covariance. We predict the full 6×66\\times 6 registration covariance directly from a single LiDAR scan, capturing translation–rotation coupling before ICP is run. Per-scan training targets are obtained by estimating empirical covariances from Monte Carlo ICP under randomized initializations.\n\n2. Kalman fusion and evaluation. Each scan is paired with a reliable 6-DoF covariance, used as the measurement noise in a standard Kalman filter. This improves pose accuracy over fixed or heuristic covariances on the evaluated sequences.\n\n3. Practicality. The model does not require a pre-built map at inference, supporting both SLAM and map-based operation.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在 ICP 匹配前直接估计六自由度（6-DoF）注册协方差以提升 LiDAR 基于定位的鲁棒性\n2. 缺少地图时如何进行无地图依赖的协方差预测，并实现与卡尔曼滤波的无缝融合\n3. 面对 featureless 或动态环境，如何更准确地量化 ICP 的不确定性并减少定位误差\n4. 将深度学习预测的协方差用于先验状态估计，从而提升 SLAM/基于地图的定位的鲁棒性\n\n【用了什么创新的方案】\nPre-ICP 直接从单帧 LiDAR 预测对称正定的 6×6 协方差矩阵，捕捉平移-旋转耦合；以蒙特卡洛 ICP 在随机初始值下得到的经验协方差作为训练目标。将该协方差作为卡尔曼滤波的测量噪声，实现 ICP 前后的一体化鲁棒定位；方法在推断阶段不依赖预构建地图，支持 SLAM 与地图匹配场景。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations",
            "authors": "Hanqing Liu,Jiahuan Long,Junqi Wu,Jiacheng Hou,Huili Tang,Tingsong Jiang,Weien Zhou,Wen Yao",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18953",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18953",
            "arxiv_html_link": "https://arxiv.org/html/2509.18953v1",
            "abstract": "Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60%, with object transformations causing up to 97.8% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.",
            "introduction": "Vision-Language-Action (VLA) models represent a paradigm shift in robotic manipulation, integrating visual perception, language understanding, and action generation into unified end-to-end systems [1]. Recent deployments across manufacturing [2], healthcare [3], and service robotics [4, 5] demonstrate their transformative potential. However, in real-world deployments, VLA models inevitably face challenging physical variations, such as spatial transformations, illumination variations, and visual disruptions, which can dramatically alter robot behavior without being immediately detectable, posing significant safety risks. Therefore, it is crucial to investigate VLA robustness across various physical conditions systematically.\n\nExisting research has explored the robustness of VLA-based robotic systems through approaches like adversarial patches [6], which generate localized perturbations via gradient-based white-box attacks to achieve visual interference. However, these methods suffer from critical limitations: they violate physical plausibility constraints and fail to capture the rich spectrum of real-world physical variations. Moreover, their reliance on gradient access restricts applicability to black-box deployment scenarios. Based on these limitations, we aim to generate more diverse and realistic physical variations for comprehensively evaluating VLA robustness, while two key challenges must be addressed: (1) How to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility? (2) How to discover worst-case scenarios without prohibitive real-world data collection costs efficiently?\n\nTo address these challenges, we propose Eva-VLA, a unified framework for evaluating vision-language-action models’ robustness. Our key innovation lies in transforming discrete physical variations into continuous optimization problems. First, as shown in Fig. 1, we decompose real-world variations into three distinct domains: object 3D transformations parameterized with rotation angles(α\\alpha, β\\beta, γ\\gamma), illumination variations defined by point light parameters including position(xx, yy), radius(σ\\sigma), intensity(II), and adversarial patch placement specified by (Δ​x\\Delta x, Δ​y\\Delta y). This parameterization enables systematic exploration of the variation space while maintaining physical plausibility through explicit constraints. Second, to overcome the black-box nature of VLA models and non-differentiable simulation environments, we employ Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [7], a gradient-free optimization algorithm, to efficiently discover worst-case scenarios by iteratively optimizing physical variations parameters. This approach enables comprehensive vulnerability assessment without requiring model gradients or expensive real-world data collection.\n\nOur main contributions are as follows: ❶ To the best of our knowledge, we are the first to decompose real-world physical variations into three key domains—object 3D transformation, illumination variations, and adversarial patches—enabling a comprehensive evaluation of VLA robustness under these physical variations. ❷ We propose Eva-VLA, a novel framework that transforms discrete physical variations into continuous parameter optimization. By leveraging a simulator environment that allows us to reset to the same conditions, we ensure the repeatability and reliability of the evaluation process, which enables efficient exploration of worst-case scenarios without the need for expensive real-world data collection. ❸ Through extensive experiments on state-of-the-art model OpenVLA across multiple benchmarks, we expose significant fragility in current VLA systems, with failure rates exceeding 60% across all variation categories, with object transformations causing up to 97.8% failure in long-horizon tasks. These findings provide crucial insights for developing more robust VLA architectures and underscore the urgent need for improved robustness training methodologies.",
            "llm_summary": "【关注的是什么问题】\n1. 如何系统评估 Vision-Language-Action (VLA) 模型在真实世界物理变异下的鲁棒性（robustness）  \n2. 如何在不依赖大量真实数据的情况下发现最坏场景（worst-case scenarios）并保证评估可重复性  \n3. 如何将离散物理变异转化为连续优化问题以便可控探索  \n4. 如何覆盖三大变异域（object 3D transformation、illumination、adversarial patches）的全面评估  \n【用了什么创新的方案】\n核心方案：提出 Eva-VLA 框架，将离散物理变异转化为连续参数优化问题；将变异分解为 object 3D transformations、illumination variations、adversarial patches 三大域，采用 CMA-ES（协方差矩阵适应进化策略）进行黑盒优化以高效发现 worst-case 场景，并在可复现的仿真环境中重置到相同条件进行评估，从而在无需梯度信息或大量真实数据的情况下系统评估 VLA 的鲁棒性。通过对 OpenVLA 在多基准上的大规模实验，揭示了超过 60% 的失败率，且对象变换在长时任务中可达 97.8% 的失败率，强调了现实部署中的鲁棒性缺口与改进必要性。  \n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands",
            "authors": "Yanyuan Qiao,Kieran Gilday,Yutong Xie,Josie Hughes",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18937",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18937",
            "arxiv_html_link": "https://arxiv.org/html/2509.18937v1",
            "abstract": "Designing robotic hand morphologies for diverse manipulation tasks requires balancing dexterity, manufacturability, and task-specific functionality. While open-source frameworks and parametric tools support reproducible design, they still rely on expert heuristics and manual tuning. Automated methods using optimization are often compute-intensive, simulation-dependent, and rarely target dexterous hands. Large language models (LLMs), with their broad knowledge of human-object interactions and strong generative capabilities, offer a promising alternative for zero-shot design reasoning. In this paper, we present Lang2Morph, a language-driven pipeline for robotic hand design. It uses LLMs to translate natural-language task descriptions into symbolic structures and OPH-compatible parameters, enabling 3D-printable task-specific morphologies. The pipeline consists of: (i) Morphology Design, which maps tasks into semantic tags, structural grammars, and OPH-compatible parameters; and (ii) Selection and Refinement, which evaluates design candidates based on semantic alignment and size compatibility, and optionally applies LLM-guided refinement when needed. We evaluate Lang2Morph across varied tasks, and results show that our approach can generate diverse, task-relevant morphologies. To our knowledge, this is the first attempt to develop an LLM-based framework for task-conditioned robotic hand design.",
            "introduction": "Designing the morphology and structure of a robotic hand for a specific application is a fundamental yet long-standing challenge in robotics.\nUnlike the design of general-purpose manipulators, for example anthropomorphic robotic hands, task-specific robotic hands must balance dexterity and generality with optimization for a specific task, whilst remaining manufacturable [1, 2].\nThe design of dexterous, yet tasks-specific hands requires a full understanding of the task, and also fabrication constraints. For example, whilst increasing finger count or joint complexity improves capability but complicates fabrication and control.\nAs a result, tasks specific hand design has typically relied on expert-driven heuristics and iterative prototyping, which are time-consuming and hard to scale.\n\nThe development of parametric hand designs can provide a design space for task-specific hand morphology design.\nOpen-source hardware projects such as the Yale OpenHand Project [3, 4] have released reproducible underactuated hand designs, lowering the barrier for building functional prototypes. Parametric frameworks like the Open Parametric Hand (OPH) [5] further expose a structured design space with tunable parameters, enabling systematic exploration of hand morphologies.\nHowever, both approaches still depend on human designers to interpret tasks, identify likely grasp types, and manually adjust parameters. Without automated mapping from task requirements to design instantiations, scaling to diverse, task-specific morphologies remains difficult.\n\nIn parallel, the robotics community has explored automated robot morphology optimization and generation through evolutionary optimization, grammar-based synthesis, and differentiable pipelines [6, 7, 8, 9].\nThese works demonstrate that morphology can indeed be generated algorithmically, but they are often computationally expensive, require carefully crafted objective functions, and are typically coupled to physics-based simulators and rely on the curation of algorithmic representation of a fitness function or objective.\nMoreover, they have rarely targeted dexterous hand design, where functional requirements such as fingertip precision, lateral pinching, or stabilizing support are used to determine the resulting morphology.\nThus, these approaches lacks generality and relies on expert identification of key functional metrics or objective functions.\n\nRecently, large language models (LLMs) have been explored in robotics, with applications in navigation and manipulation where language is grounded into action policies [10, 11].\nWhilst these efforts focus mainly on control and planning,\nLLMs are not only capable of semantic reasoning but also encode broad background knowledge across biology, engineering, and everyday practice, which traditional optimization-based methods typically lack.\nThis combination makes them promising candidates for reasoning about form and functionality in design.\nSome recent works have adapted LLMs for computer-aided design (CAD) [12, 13], translating text into parametric part models.\nYet these methods remain restricted to geometry generation and do not address morphology design, where task semantics such as lateral pinching or stabilizing support directly dictate structural choices.\nTo the best of our knowledge, the use of LLMs for robotic hand morphology design remains unexplored, motivating our study.\n\nIn this paper, we propose Lang2Morph, a language-driven pipeline for robotic hand morphology generation. Our key insight is that LLMs are well suited to reasoning about task semantics and mapping them into symbolic and geometric design representations. Unlike prior methods that rely on expert-driven heuristics or costly physics-based simulation, Lang2Morph leverages LLM reasoning and semantic feedback to achieve scalable, task-conditioned morphology generation, as illustrated in Fig. 1, where a user-provided instruction is mapped into a design rationale and a CAD-ready morphology.\n\nLang2Morph builds upon the Open Parametric Hand (OPH) framework [5], which defines a structured and fabricable design space for robotic hands. OPH supports single-piece 3D printing, allowing generated designs to be directly manufactured without additional assembly or simulation. This enables our method to output physically realizable morphologies from natural language instructions.\n\nSpecifically, the pipeline comprises two major modules: (i) LLM-powered Morphology Design, which performs dual-level task analysis (semantic and structural), followed by geometry parameterization and constraint-aware validation to produce OPH-compatible parameters; and (ii) LLM-Guided Selection and Refinement, which ranks rendered variants based on semantic alignment and size compatibility, and optionally provides design refinements through feedback.\nTogether, these components form an end-to-end pipeline that generates fabricable, task-adaptive morphologies directly from natural language task description.\n\nOur main contributions are as follows:\n\nWe present Lang2Morph, a novel framework that generates robotic hand morphologies from natural-language instructions using large language models.\n\nWe design a two-stage pipeline that combines symbolic grammar generation, geometric parameterization, and semantic feedback for task-adaptive hand design.\n\nWe explore the use of LLMs for early-stage robot morphology generation, offering a flexible alternative to expert tuning.\n\nWe evaluate our method on a range of manipulation tasks, demonstrating improved design validity, diversity, and semantic alignment.\n\n1. We present Lang2Morph, a novel framework that generates robotic hand morphologies from natural-language instructions using large language models.\n\n2. We design a two-stage pipeline that combines symbolic grammar generation, geometric parameterization, and semantic feedback for task-adaptive hand design.\n\n3. We explore the use of LLMs for early-stage robot morphology generation, offering a flexible alternative to expert tuning.\n\n4. We evaluate our method on a range of manipulation tasks, demonstrating improved design validity, diversity, and semantic alignment.",
            "llm_summary": "【关注的是什么问题】\n1. 如何从自然语言任务描述自动生成可直接制造的任务适应性机器人手形态\n2. 在确保可制造性与任务相关性的同时，减少对人工经验启发式设计的依赖\n3. 面向 dexterous 手掌的任务特定形态设计的高效自动化路径\n4. 将语言模型用于高层语义分析到几何参数化、结构约束的端到端设计\n\n【用了什么创新的方案】\nLang2Morph 使用大语言模型进行双层任务分析（语义与结构），将自然语言指令映射到 OPH 兼容的参数化几何；结合符号语法生成与几何参数化，通过语义对齐和尺寸兼容性评估进行候选选择与 refinement；实现从文本到可直接 3D 打印的任务特定形态的端到端流程，且无需仿真驱动。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation",
            "authors": "Masato Kobayashi,Thanpimon Buamanee",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18865",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18865",
            "arxiv_html_link": "https://arxiv.org/html/2509.18865v1",
            "abstract": "We propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral control-based imitation learning to handle more than one task within a single model. Conventional bilateral control methods exploit joint angle, velocity, torque, and vision for precise manipulation but require task-specific models, limiting their generality.\nBi-VLA overcomes this limitation by utilizing robot joint angle, velocity, and torque data from leader-follower bilateral control with visual features and natural language instructions through SigLIP and FiLM-based fusion.\nWe validated Bi-VLA on two task types: one requiring supplementary language cues and another distinguishable solely by vision. Real-robot experiments showed that Bi-VLA successfully interprets vision-language combinations and improves task success rates compared to conventional bilateral control-based imitation learning.\nOur Bi-VLA addresses the single-task limitation of prior bilateral approaches and provides empirical evidence that combining vision and language significantly enhances versatility.\nExperimental results validate the effectiveness of Bi-VLA in real-world tasks.\nFor additional material, please visit the website: https://mertcookimg.github.io/bi-vla/",
            "introduction": "Robotic manipulation is increasingly important in human-centered applications such as cooking, eldercare, and interactive service robots [1, 2, 3, 4].\nUnlike traditional industrial robots that excel at repetitive and pre-programmed routines, service and collaborative robots must adapt to dynamic environments and interact with objects of diverse shapes, sizes, and material properties [5, 6]. Achieving such adaptability requires learning frameworks capable of acquiring human-like manipulation strategies [7].\n\nImitation learning (IL) has emerged as a promising approach for transferring human manipulation skills directly to robots [8].\nLeader-follower teleoperation has become a common pipeline for collecting demonstrations. For example, ALOHA and Mobile ALOHA use position-based unilateral control to gather diverse datasets that enable a wide range of manipulation [9, 10].\nAlthough effective for kinematics-driven tasks, such unilateral control omits force feedback, which limits robustness in contact-rich interactions.\n\nBilateral control-based imitation learning addresses these limitations by exchanging both position and force information between the demonstrator and robot [11, 12].\nBilateral control allows demonstrators to feel contact forces directly, yielding richer demonstrations and improving generalization across objects with different hardness and weights.\n\nBuilding on this foundation, recent methods have explored combining bilateral control with modern architectures.\nBilateral Control-Based Imitation Learning via Action Chunking with Transformers (Bi-ACT) [13] integrates bilateral control with visual observations via Transformers, yielding improved manipulation accuracy.\nHowever, it remains restricted to single-task settings, limiting its practicality in dynamic environments where multiple tasks must be handled seamlessly.\nIn parallel, Bilateral Control-Based Imitation Learning via Natural Language and Action Chunking with Transformers (Bi-LAT) [14] introduced natural language instructions into bilateral control-based imitation learning, demonstrating effective force modulation in manipulation.\nWhile this work highlights the promise of language integration, it focuses on regulating applied force and does not address the challenge of enabling a single model to adapt to multiple task contexts.\n\nIn this paper, we propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that unifies robot joint angle, velocity, and torque data from bilateral control with visual and language features, as shown in Fig. 1.\nBy leveraging SigLIP-based text embeddings [15] and FiLM-based EfficientNet feature fusion [16], Bi-VLA learns a shared representation of vision and language aligned with robot state information. Unlike prior bilateral frameworks, Bi-VLA is designed to handle multiple tasks within a single model, enabling flexible task switching without retraining or manual model selection.\n\nThe main contributions of this paper are summarized as follows:\n\nWe propose Bi-VLA, the bilateral control-based imitation learning framework that fuses vision and language features into a unified representation.\n\nWe demonstrate that Bi-VLA enables a single model to perform multiple tasks, overcoming the single-task limitation inherent in prior bilateral control-based imitation learning approaches.\n\nWe validate Bi-VLA through real-robot experiments on two distinct tasks, showing improved performance and adaptability compared to conventional methods.\n\n1. We propose Bi-VLA, the bilateral control-based imitation learning framework that fuses vision and language features into a unified representation.\n\n2. We demonstrate that Bi-VLA enables a single model to perform multiple tasks, overcoming the single-task limitation inherent in prior bilateral control-based imitation learning approaches.\n\n3. We validate Bi-VLA through real-robot experiments on two distinct tasks, showing improved performance and adaptability compared to conventional methods.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在单一模型中实现多任务的双边控制模仿学习与任务切换（≤40词）\n2. 如何将视觉与语言信息与机器人关节角度、速度、力矩数据融合以提升操控鲁棒性（≤40词）\n3. 如何在现实机器人环境中验证视觉-语言融合对多任务泛化的提升（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：在双边控制基础上，结合 SigLIP 文本嵌入和 FiLM 机制对 EfficientNet 提取的视觉特征进行融合，形成一个统一的视觉-语言-状态表示；通过该表示实现单模型处理多任务、并通过任务切换无需重新训练；在两类任务与真实机器人上验证性能提升。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation",
            "authors": "Suzannah Wistreich,Baiyu Shi,Stephen Tian,Samuel Clarke,Michael Nath,Chengyi Xu,Zhenan Bao,Jiajun Wu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "Accepted to CoRL 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18830",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18830",
            "arxiv_html_link": "https://arxiv.org/html/2509.18830v1",
            "abstract": "Human skin provides a rich tactile sensing stream, localizing intentional and unintentional contact events over a large and contoured region.\nReplicating these tactile sensing capabilities for dexterous robotic manipulation systems remains a longstanding challenge.\nIn this work, we take a step towards this goal by introducing DexSkin. DexSkin is a soft, conformable capacitive electronic skin that enables sensitive, localized, and calibratable tactile sensing, and can be tailored to varying geometries.\nWe demonstrate its efficacy for learning downstream robotic manipulation by sensorizing a pair of parallel jaw gripper fingers, providing tactile coverage across almost the entire finger surfaces.\nWe empirically evaluate DexSkin’s capabilities in learning challenging manipulation tasks that require sensing coverage across the entire surface of the fingers, such as reorienting objects in hand and wrapping elastic bands around boxes, in a learning-from-demonstration framework. We then show that, critically for data-driven approaches, DexSkin can be calibrated to enable model transfer across sensor instances, and demonstrate its applicability to online reinforcement learning on real robots.\nOur results highlight DexSkin’s suitability and practicality for learning real-world, contact-rich manipulation.\nPlease see our project webpage for videos and visualizations: https://dex-skin.github.io/.",
            "introduction": "Tactile feedback is essential for robust and dexterous manipulation in natural and artificial systems. In humans, mechanoreceptors within the skin provide a rich sensory stream that guides tasks ranging from handling delicate objects to using tools with force [1]. This tactile feedback enables more precise reactive control than can be achieved with human and proprioceptive feedback alone [2].\n\nEmulating this tactile sensitivity in robotic systems has long been a challenge, starting from sensing hardware. Research efforts typically focus on rigid or partially flexible sensors that offer low spatial coverage, limited adaptability, and poor conformability to complex surfaces. In contrast, everyday tasks such as rotating a key in-hand or picking up delicate berries require sensing coverage in multiple contact regions.\nSimilarly, wrapping an elastic band around an object requires a sensor to detect, distinguish, and localize dynamic contact events on all surfaces of a human hand or end effector accurately, applying the appropriate force without allowing the band to slip.\n\nLearning-based approaches offer a general way to harness tactile sensing information. However, data-driven systems pose requirements beyond sensing coverage. Seemingly small distribution shifts in sensor readings from wear or replacement can render previously trained models unusable, thus, output signals must be replicable across sensor hardware instances. Additionally, for real-world online learning, sensors must remain consistent, durable, and precise under repeated stresses during trial-and-error interaction, and output interpretable signals amenable to reward or cost specification.\n\nTo address these challenges, we introduce a novel soft tactile skin named DexSkin that is particularly suitable for robotic learning applications. DexSkin can be conformably integrated onto robotic end-effectors with unparalleled spatial coverage.\nIt is based on a capacitive mechanism, and features high sensitivity and robustness under repeated interactions.\nBecause each of the dozens of taxels on the skin are individually addressable, DexSkin can localize and characterize simultaneous contacts from distinct regions.\nIt can also be calibrated to provide consistent readings across distinct sensor units, enabling re-use of learned networks. At the same time, it can withstand deformations encountered during typical dexterous tasks such as pinching, twisting, and bending.\n\nIn this work, we introduce the DexSkin framework and its fabrication, as well as a representative integration with a soft cylindrical robotic fingertip that sensorizes the distal dome of the finger and 294∘ of the circumference. Then, we evaluate DexSkin’s applicability to robot learning. First, we test whether DexSkin’s coverage and tailorability expands the range of learnable manipulation tasks. Then, we evaluate its calibration performance, which are critical for working with and transferring learned tactile models. Finally, we demonstrate its suitability for online robotic learning settings by performing real-world reinforcement learning for a delicate object picking task. The results highlight DexSkin’s robust applicability to a wide range of robotic tasks and morphologies, and its particular practicality for robot learning researchers and practitioners.",
            "llm_summary": "【关注的是什么问题】\n1. 需要大面积、可 conform 的触觉传感覆盖以实现高鲁棒的接触丰富操控\n2. 如何在多传感单元上获得高灵敏、可标定、跨传感器的一致性读取\n3. 在 learns-from-demonstration 与 在线强化学习 场景中实现对不同几何和材料的可转移性\n4. 传感硬件对姿态、变形的耐久性及对数据分布偏移的鲁棒性\n\n【用了什么创新的方案】\nDexSkin 是一种软性、可 conform 的电容式电子皮肤，具有大量独立可寻址的触觉单元（taxels），可覆盖手指表面且对变形鲁棒；通过逐个触觉单元标定实现跨传感器的一致性，使已学习的模型可在不同传感器实例间转移；支持对多点同时接触的定位与表面广覆盖的感知，适用于从演示学习到在线强化学习的任务。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations",
            "authors": "Lukas Zanger,Bastian Lampe,Lennart Reiher,Lutz Eckstein",
            "subjects": "Robotics (cs.RO); Multiagent Systems (cs.MA); Software Engineering (cs.SE)",
            "comment": "7 pages, 2 figures, 2 tables; Accepted to be published as part of the 2025 IEEE International Conference on Intelligent Transportation Systems (ITSC 2025), Gold Coast, Australia, November 18-21, 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18793",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18793",
            "arxiv_html_link": "https://arxiv.org/html/2509.18793v1",
            "abstract": "Vehicles are becoming increasingly automated and interconnected, enabling the formation of cooperative intelligent transport systems (C-ITS) and the use of offboard services. As a result, cloud-native techniques, such as microservices and container orchestration, play an increasingly important role in their operation.\nHowever, orchestrating applications in a large-scale C-ITS poses unique challenges due to the dynamic nature of the environment and the need for efficient resource utilization.\nIn this paper, we present a demand-driven application management approach that leverages cloud-native techniques – specifically Kubernetes – to address these challenges. Taking into account the demands originating from different entities within the C-ITS, the approach enables the automation of processes, such as deployment, reconfiguration, update, upgrade, and scaling of microservices.\nExecuting these processes on demand can, for example, reduce computing resource consumption and network traffic.\nA demand may include a request for provisioning an external supporting service, such as a collective environment model.\nThe approach handles changing and new demands by dynamically reconciling them through our proposed application management framework built on Kubernetes and the Robot Operating System (ROS 2).\nWe demonstrate the operation of our framework in the C-ITS use case of collective environment perception and make the source code of the prototypical framework publicly available at https://github.com/ika-rwth-aachen/application_manager.",
            "introduction": "In future cooperative intelligent transport systems (C-ITS), various entities, such as vehicles equipped with driving automation systems, sensor-equipped roadside infrastructure units, edge/cloud servers, and control centers, will be connected, exchange data, and may offer computational resources.\nThese advancements enable new applications – such as collective environment perception, cooperative decision-making, computation offloading, and intelligent traffic management – that can contribute to improved comfort and safety for road users [1], [2], [3].\nNot only cloud and edge servers but also vehicles and roadside units can be part of a distributed computing system.\nHowever, these applications may also introduce complexity that is difficult to manage. The dynamic nature of C-ITS, the presence of resource-constrained entities, and the strict requirements for safety and security pose unique challenges.\n\nCloud-native techniques provide a promising foundation for the development and operation of scalable applications in dynamic environments. Such techniques involve paradigms like containerization, microservice architectures, and container orchestration. They enable loosely coupled systems which are manageable and resilient [4]. Said techniques and paradigms have the potential to contribute to the advancement of C-ITS.\n\nKubernetes has evolved as the de facto standard for orchestrating containerized applications in distributed systems. It is open-source and widely adopted by software companies worldwide [5].\nNevertheless, Kubernetes lacks methods that are domain-specific, e.g., to C-ITS, considering that specific tasks like the deployment of required applications are only needed at certain times or may depend on the specific content of data exchanged in the C-ITS.\nWe have developed the approach RobotKube [6] to extend the regular capabilities of Kubernetes. RobotKube comprises software components designed to automate the identification of requirements and the formulation of specific Kubernetes workloads.\nThese components include the event detector and the application manager.\n\nIn this paper, we propose a demand-driven application management approach and present the methodology behind the application manager as part of an application management framework.\nThis methodology integrates seamlessly into the RobotKube architecture and complements parts of RobotKube which were not detailed yet.\nThe application management framework – comprising the application manager and a set of custom operators – addresses the orchestration challenges in C-ITS through a demand-driven approach.\nIn this context, applications are deployed, reconfigured, scaled, and updated based on the current demands of C-ITS entities.\n\nWith our work, we make the following main contributions:\n\nPresentation of the methodology for demand-driven application management allowing to deploy, reconfigure, update, upgrade, and scale applications based on demands of entities in a C-ITS.\n\nPrototypical implementation of the application manager and the custom operators in an application management framework based on Kubernetes and ROS 2.\n\nDemonstration and evaluation of the capabilities of the application management framework in the complex C-ITS use case of collective environment perception involving various C-ITS entities.\n\nOpen source code publication of the application management framework and the demonstration use case allowing for reproducibility and extensibility.\n\nComplementation of RobotKube [6] by providing the concrete methodology of the application manager.\n\n1. Presentation of the methodology for demand-driven application management allowing to deploy, reconfigure, update, upgrade, and scale applications based on demands of entities in a C-ITS.\n\n2. Prototypical implementation of the application manager and the custom operators in an application management framework based on Kubernetes and ROS 2.\n\n3. Demonstration and evaluation of the capabilities of the application management framework in the complex C-ITS use case of collective environment perception involving various C-ITS entities.\n\n4. Open source code publication of the application management framework and the demonstration use case allowing for reproducibility and extensibility.\n\n5. Complementation of RobotKube [6] by providing the concrete methodology of the application manager.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在动态且资源受限的C-ITS环境中对应用进行按需驱动的管理与部署（≤40词）\n2. 如何将云原生技术（Kubernetes、ROS 2、微服务）整合用于C-ITS中的应用编排与再配置（≤40词）\n3. 如何通过需求感知实现应用的部署、重配置、更新、升级与扩缩以优化资源与网络开销（≤40词）\n【用了什么创新的方案】\n核心解决方案：提出基于需求驱动的应用管理框架，结合 Kubernetes 与 ROS 2，开发应用管理器与自定义 Operator，用事件检测器识别需求并动态编排、部署、重配置、升级和扩缩，支持对外部服务等需求的 provisioning；框架作为 RobotKube 的扩展，公开源码以便复现与扩展。\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Human-Interpretable Uncertainty Explanations for Point Cloud Registration",
            "authors": "Johannes A. Gaus,Loris Schneider,Yitian Shi,Jongseok Lee,Rania Rayyes,Rudolph Triebel",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18786",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18786",
            "arxiv_html_link": "https://arxiv.org/html/2509.18786v1",
            "abstract": "In this paper, we address the point cloud registration problem, where well-known methods like ICP fail under uncertainty arising from sensor noise, pose‐estimation errors, and partial overlap due to occlusion. We develop a novel approach, Gaussian Process Concept Attribution (GP-CA), which not only quantifies registration uncertainty but also explains it by attributing uncertainty to well-known sources of errors in registration problems. Our approach leverages active learning to discover new uncertainty sources in the wild by querying informative instances. We validate GP-CA on three publicly available datasets and in our real-world robot experiment. Extensive ablations substantiate our design choices. Our approach outperforms other state-of-the-art methods in terms of runtime, high sample-efficiency with active learning, and high accuracy. Our real-world experiment clearly demonstrates its applicability. Our video also demonstrates that GP-CA enables effective failure-recovery behaviors, yielding more robust robotic perception.",
            "introduction": "Point cloud registration refers to the problem of estimating a relative transformation between two sets of 3D points [1]. This problem is essential in many robotic perception tasks, such as simultaneous localization and mapping (SLAM) [2], 3D reconstruction [3], and 6-DoF object pose estimation [4], to name a few. A widely used approach for point cloud registration is Iterative Closest Point (ICP) [5].\nHowever, in ICP, well-known causes\nof uncertainty are\nsensor noise, poor initialization of the optimization process of the relative transformation between the two point clouds, and insufficient overlap between the two point clouds (e.g., due to occlusions), which can make ICP inaccurate or unreliable in practice  [6]. Hence, several researchers attempt to improve the robustness of the registration process [7].\n\nAmong others, several probabilistic approaches were proposed to quantify uncertainty in point cloud registration [6, 8, 9, 10, 11]. The underlying idea is that uncertainty estimates provide insight into the reliability of the obtained results, enabling the identification and rejection of unreliable registrations. Moreover, information about uncertainty is often used in many downstream tasks, including sensor fusion, state estimation, and 3D reconstruction [6]. To this end, a variety of tools have been developed to quantify uncertainty in point cloud registration, ranging from closed-form Gaussian solvers [11] to particle-based methods [6].\n\nWhile quantifying uncertainty helps to improve the registration, in practice, the magnitude of uncertainty is rarely sufficient in practical robotic perception tasks, as it does not reveal why registration failed or how to recover from it. Distinct failure causes — such as sensor noise, poor initialization, or occlusion [6] — require different actions to recover from failure.\nIn this work, we propose a novel approach, Gaussian Process Concept Attribution (GP-CA). A major difference to existing work is that, our approach not only quantifies uncertainty, but also explains uncertainty in a human-interpretable manner. Our GP-CA integrates active learning, learned point-cloud representations, and a Gaussian Process classifier to provide concept-level uncertainty attribution. The active learning allows fast adaptation to new uncertainty sources.\nIn preliminary work [12], the explainability method SHAP was used to analyze the influence of different uncertainty sources on ICP-based registration. However, it cannot identify which uncertainty source is present in a registration and depends on manual control over uncertainty sources and heavy computation, limiting applicability and hindering its feasibility for robotic decision-making.\n\nGP-CA attributes uncertainty to semantically meaningful concepts that can be used later to enable robots to select targeted recovery actions. An example is depicted in Fig. 1, where the robot is tasked to perform a 6D pose estimation of objects using the ICP algorithm. Initially, an object of interest (the bottle) is occluded by another object, which causes the ICP algorithm to fail. Using our method, the occlusion is identified as the cause of failure, thereby enabling targeted recovery actions (e.g., changing the viewpoint).\nHence, our main idea of explaining uncertainty in point cloud registration can improve the robustness of the perception tasks.\n\nWhen registration fails or reports high uncertainty, GP-CA embeds the ICP-aligned source point cloud using a learned representation. Then, a multi-class Gaussian Process classifier (GPC) maps these representations to confidence scores over different concepts. In this way, we attribute the uncertainty to specific concepts, thereby identifying its source. The concepts are predefined by sets of examples. Still, a robot may encounter unknown sources of uncertainty during its operations. Therefore, GP-CA is equipped with an active learning mechanism [13, 14], which can learn a new concept by querying the user for the label and choosing the most informative data to learn from. Across experiments on the LINEMOD [15], the YCB [16], and the Coffee Cup [17] dataset and a custom real-world RGB-D dataset from our laboratory, we validate GP-CA and provide extensive ablation studies. We further propose how GP-CA can enable a more robust robotic perception by executing a set of failure recovery actions associated with uncertainty explanations.\n\nIn summary, our main contributions are as follows.\n\nWe propose a novel approach, GP-CA, which enables human-interpretable uncertainty explanations in point cloud registration. To the best of our knowledge, this is the first work to show how explainability can advance point cloud registration for real-world applications\n\nWe extend our approach with active learning to adapt and integrate new concepts.\n\nWe validate the GP-CA design through an extensive ablation study\n\nWe demonstrate clearly the runtime-efficiency, the high accuracy, and the sample-efficiency of our method, with SOTA baseline comparison across four datasets.\n\nWe propose how GP-CA can be employed for recovery actions in a real-world robot setting.\n\n1. We propose a novel approach, GP-CA, which enables human-interpretable uncertainty explanations in point cloud registration. To the best of our knowledge, this is the first work to show how explainability can advance point cloud registration for real-world applications\n\n2. We extend our approach with active learning to adapt and integrate new concepts.\n\n3. We validate the GP-CA design through an extensive ablation study\n\n4. We demonstrate clearly the runtime-efficiency, the high accuracy, and the sample-efficiency of our method, with SOTA baseline comparison across four datasets.\n\n5. We propose how GP-CA can be employed for recovery actions in a real-world robot setting.",
            "llm_summary": "【关注的是什么问题】\n1. 点云配准中的不确定性量化与原因解释（≤40词）\n2. 如何将不确定性解释转化为人类可理解的概念级因果归因（≤40词）\n3. 在实际机器人场景中实现快速、鲁棒的失败恢复策略（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出高斯过程概念归因GP-CA，结合点云嵌入、GPC分类器实现对不确定性的概念级归因，并通过主动学习发现新不确定源；可用于在ICP失败时给出针对性的恢复操作，提升运行时鲁棒性与效率。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models",
            "authors": "Shijia Ge,Yinxin Zhang,Shuzhao Xie,Weixiang Zhang,Mingcai Zhou,Zhi Wang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "submitted to AAAI 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.18778",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18778",
            "arxiv_html_link": "https://arxiv.org/html/2509.18778v1",
            "abstract": "Visual imitation learning frameworks allow robots to learn manipulation skills from expert demonstrations. While existing approaches mainly focus on policy design, they often neglect the structure and capacity of visual encoders—limiting spatial understanding and generalization. Inspired by biological vision systems, which rely on both visual and proprioceptive cues for robust control, we propose VGGT-DP, a visuomotor policy framework that integrates geometric priors from a pretrained 3D perception model with proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer (VGGT) as the visual encoder and introduce a proprioception-guided visual learning strategy to align perception with internal robot states, improving spatial grounding and closed-loop control. To reduce inference latency, we design a frame-wise token reuse mechanism that compacts multi-view tokens into an efficient spatial representation. We further apply random token pruning to enhance policy robustness and reduce overfitting. Experiments on challenging MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines such as DP and DP3, particularly in precision-critical and long-horizon scenarios.",
            "introduction": "未获取到引言",
            "llm_summary": "【关注的是什么问题】\n1. 如何将视觉编码器的结构与能力用于提升机器人视觉-动作嵌入的泛化和精确控制\n2. 如何融合预训练3D感知模型的几何先验与机器人本体的 proprioceptive 信息以改善空间定位与闭环控制\n3. 如何降低多视角视觉输入的推理时延并提升鲁棒性\n4. 如何通过 token 重用和随机剪枝提升效率与抗过拟合能力\n【用了什么创新的方案】\n- 以 Visual Geometry Grounded Transformer (VGGT) 作为视觉编码器，将几何先验整合到 visuomotor 策略中\n- 引入 proprioception-guided visual learning，使感知对齐内部状态并增强空间着陆与闭环控制\n- 设计 frame-wise token reuse 机制，将多视图 token 压缩为高效的空间表示以降低推理延迟\n- 采用随机 token pruning 增强策略鲁棒性并抑制过拟合\n- 在 MetaWorld 任务上对比 DP、DP3，显示显著提升，尤其在精度关键与长时程场景\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning",
            "authors": "Omar Rayyan,John Abanes,Mahmoud Hafez,Anthony Tzes,Fares Abu-Dakka",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "For project website and videos, see httpsthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18757",
            "code": "https://mv-umi.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18757",
            "arxiv_html_link": "https://arxiv.org/html/2509.18757v1",
            "abstract": "Recent advances in imitation learning have shown great promise for developing robust robot manipulation policies from demonstrations. However, this promise is contingent on the availability of diverse, high-quality datasets, which are not only challenging and costly to collect but are often constrained to a specific robot embodiment. Portable handheld grippers have recently emerged as intuitive and scalable alternatives to traditional robotic teleoperation methods for data collection. However, their reliance solely on first-person view wrist-mounted cameras often creates limitations in capturing sufficient scene contexts. In this paper, we present MV-UMI (Multi-View Universal Manipulation Interface), a framework that integrates a third-person perspective with the egocentric camera to overcome this limitation. This integration mitigates domain shifts between human demonstration and robot deployment, preserving the cross-embodiment advantages of handheld data-collection devices. Our experimental results, including an ablation study, demonstrate that our MV-UMI framework improves performance in sub-tasks requiring broad scene understanding by approximately 47% across 3 tasks, confirming the effectiveness of our approach in expanding the range of feasible manipulation tasks that can be learned using handheld gripper systems, without compromising the cross-embodiment advantages inherent to such systems.\nVideos can be found here: https://mv-umi.github.io",
            "introduction": "Imitation Learning (IL) provides a compelling pathway toward acquiring general robot policies capable of performing long-horizon tasks across diverse environments. This approach, particularly through supervised methods like Behavioral Cloning (BC), enables robots to acquire complex behaviors by learning to imitate human-directed actions in response to observations. Recent advances in architectures that better model this mapping [1, 2, 3, 4], coupled with enhancements in embodiments and hardware integrations [5, 6], have made this route increasingly convincing.\n\nRecent studies on data scaling laws in imitation learning [7] show that robot policy performance follows training scenario diversity, emphasizing the need for extensive and diverse data for robust policies. Data collection typically lies between two extremes. On one end, robot teleoperation enables the acquisition of high-quality, precise data with minimal embodiment discrepancies. However, this method is time-consuming and costly, as it requires an actively operated robot. On the other end, the internet is replete with videos of humans performing various tasks. However, substantial effort is required to establish structured explicit mappings between observed states and actions from these videos.\n\nAs a middle-ground, portable handheld grippers [8, 9, 10, 11, 12] have emerged as inexpensive and intuitive to use data collection devices. By relying exclusively on a wrist-mounted camera, they enable non-experts to record demonstrations without the need for a robotic manipulator. While this egocentric viewpoint minimizes visual discrepancies between training and deployment, resulting in cross-embodiment policies, it demands that the robot maintain a longer memory context to recall scene elements that move out of the constraining wrist view.\n\nIn this work, we propose a novel framework that augments the conventional wrist-mounted camera in handheld gripper systems with a third-person camera viewpoint, without incurring distributional shifts. We achieve this by performing real-time masking of the human demonstrator in the third-person video stream, effectively removing the operator’s presence from the training data. As a result, the model benefits from a broader view of the environment, while relying less on memory for scene remembrance. A side benefit we find of this masking is its removal of correlations between the demonstrator’s motions and the gripper’s actions, encouraging the policy to focus on task-relevant cues such as the manipulated objects rather than overfitting to human-specific signals. We also utilize a custom-made three-jaw gripper for some of the tasks that require greater dexterity in this work. This design allows for greater payload weight in comparison to other hand-held devices, at the cost of its volume. Schematics and instructions to reproduce the hardware are open-sourced separately. The hardware aspect is not a key focus in this paper.\n\nSummary of Contributions:\n\nMulti-View Cross-Embodiment Framework: MV-UMI fuses wrist-mounted and third-person views using SAM-2 segmentation and inpainting to eliminate domain shift, boosting performance in context-dependent tasks by 47%.\n\nEnd-to-End Open-Source System: Complete pipeline, including hardware design, data collection, training code, and deployment tools, is publicly released to advance cross-embodiment manipulation research, https://mv-umi.github.io.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在跨 embodiment 的示范学习中利用多视角来缓解域间差异（跨人体/设备）的影响\n2. 如何在手持抓取设备的数据集上融合第一视角和第三视角以提升上下文理解能力\n3. 如何通过去除示范者在第三视角中的影像来减少示范者动作与抓手动作的相关性\n4. 如何在不增加分布漂移的前提下扩展数据视角以提升对场景理解的鲁棒性\n5. 如何在跨视角学习中保持对任务相关 cues 的聚焦，避免过拟合人类信号\n\n【用了什么创新的方案】\n核心解决方案：在手持夹具的第一视角与第三视角之间进行实时多视图融合，利用 SAM-2 对人类示范者进行分割并在第三视角视频中进行去人化处理，从而获得更广的环境观察并降低域偏移；结合自定义三撬式抓手以提高耐受载荷，输出端到端的开放源码数据收集、训练与部署管线；通过掩码化第三视角实现对示范者的去域化，减少示范者动作与抓取动作的相关性，提升跨嵌入学习的任务自适应性。实验结果显示在需要广域场景理解的子任务上约提升47%。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        }
    ]
}