{
    "2025-09-24": [
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies",
            "authors": "Lars Ankile,Zhenyu Jiang,Rocky Duan,Guanya Shi,Pieter Abbeel,Anusha Nagabandi",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19301",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19301",
            "arxiv_html_link": "https://arxiv.org/html/2509.19301v1",
            "abstract": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor control policies. However, these approaches are limited by the quality of human demonstrations, the manual effort required for data collection, and the diminishing returns from increasing offline data.\nIn comparison, reinforcement learning (RL) trains an agent through autonomous interaction with the environment and has shown remarkable success in various domains. Still, training RL policies directly on real-world robots remains challenging due to sample inefficiency, safety concerns, and the difficulty of learning from sparse rewards for long-horizon tasks, especially for high-degree-of-freedom (DoF) systems.\nWe present a recipe that combines the benefits of BC and RL through a residual learning framework. Our approach leverages BC policies as black-box bases and learns lightweight per-step residual corrections via sample-efficient off-policy RL.\nWe demonstrate that our method requires only sparse binary reward signals and can effectively improve manipulation policies on high-degree-of-freedom (DoF) systems in both simulation and the real world. In particular, we demonstrate, to the best of our knowledge, the first successful real-world RL training on a humanoid robot with dexterous hands.\nOur results demonstrate state-of-the-art performance in various vision-based tasks, pointing towards a practical pathway for deploying RL in the real world.",
            "introduction": "Enabling robots to learn and improve directly in their deployment environments remains a fundamental challenge in robotics. Recently, significant progress has been made in training visuomotor control policies in the real world with behavior cloning (BC) from human demonstrations [1, 2, 3, 4, 5, 6, 7, 8, 9]. However, this success requires significant infrastructure, as well as numerous hours of manual and cumbersome data collection.\nEven if unlimited data could be collected for every task, not only is human teleoperator performance generally suboptimal, but there is also emerging evidence that policy performance saturates with increasing demonstrations [10, 6, 11, 12, 13].\n\nReinforcement learning (RL) offers a complementary paradigm: agents learn autonomously through trial and error in the environment. Deep RL has shown great success in various domains [14, 15, 16, 17, 18, 19, 20, 21], including in-hand manipulation [22, 23] and locomotion [24, 25, 26, 27]. However, strong RL performance generally requires large amounts of data from online interactions, so its application has been mainly in simulation [28, 29] since real-world data are expensive and potentially unsafe to gather in large amounts.\n\nA natural direction to improve BC policies is to leverage online RL [12, 30, 31, 32], combining the strengths of each: BC policies provide a strong prior that can regularize exploration in the RL process, while online RL enhances policy performance by learning from interactions with the environment. However, modern BC architectures are typically deep models with tens of millions to billions of parameters that utilize action chunking or diffusion-based approaches, which can make it challenging to apply RL methods directly to optimize the policy. A simple yet powerful recipe that avoids several of the above issues is residual RL [33, 34, 35, 36, 12, 32, 31], where RL is applied not to learn a full policy, but only to learn corrective terms on top of a fixed base controller. Previous work has demonstrated that residual RL can indeed enhance the reliability of a pre-trained policy. Still, it has so far been limited to learning in simulation [12, 32, 31] or demonstrating results in simple or constrained settings [33, 34, 35, 36]; Applications to high-DoF systems learning directly in the real world are still lacking.\n\nIn this work, we present an off-policy residual fine-tuning (ResFiT) approach that utilizes online RL to enhance BC policies. By treating the base policy as a black box and learning a per-step residual correction that is independent of chunk size and policy parameterization, we sidestep the challenges of directly optimizing huge base policies. By carefully designing our off-policy recipe, we make the RL process sample efficient enough to scale to high-DoF bimanual systems, require only sparse binary reward signals, and be safe enough to deploy in the real world. We demonstrate robust performance on sparse-reward, long-horizon, vision-based tasks, showing that our approach achieves state-of-the-art performance for a range of tasks in simulation. We also investigate each design decision in our recipe. To the best of our knowledge, we provide the first demonstration of RL on a humanoid robot with five-fingered hands, trained entirely in the real world.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在保留行为克隆（BC）基线的同时通过离线/在线强化学习实现对复杂高自由度系统的高效微调与改进\n2. 如何实现对高DoF系统的安全、样本高效的Residual RL，以最少数据和稀疏奖励实现现实世界鲁棒性\n3. 如何在不直接优化大型基准策略的前提下，对其进行逐步残差修正以提升 visuomotor 控制性能\n4. 如何将离线BC与在线RL结合，解决数据质量、数据收集成本、以及现实世界训练的安全性挑战\n5. 如何在真实 humanoid/五指手等高复杂度机械臂上实现端到端的实证RL训练与评估\n\n【用了什么创新的方案】\n核心解决方案：提出离线行为克隆基线上的残差强化学习（ResFiT），将基线策略视为黑盒，通过学习逐步的每步残差信号来实现修正；使用面向离线/在线混合的高效RL配方，确保在高DoF、稀疏二值奖励下也能实现样本友好、真实世界的训练，并在仿真与现实中获得前所未有的鲁棒性与性能提升；通过残差学习避免直接优化庞大基线策略，降低参数化、数据需求和安全风险；首次在真实 humanoid/五指手上实现全现实世界的RL微调。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration",
            "authors": "Yang Jin,Jun Lv,Han Xue,Wendi Chen,Chuan Wen,Cewu Lu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19292",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19292",
            "arxiv_html_link": "https://arxiv.org/html/2509.19292v1",
            "abstract": "Intelligent agents progress by continually refining their capabilities through actively exploring environments. Yet robot policies often lack sufficient exploration capability due to action mode collapse. Existing methods that encourage exploration typically rely on random perturbations, which are unsafe and induce unstable, erratic behaviors, thereby limiting their effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a framework that enhances policy exploration and improvement in robotic manipulation. SOE learns a compact latent representation of task-relevant factors and constrains exploration to the manifold of valid actions, ensuring safety, diversity, and effectiveness. It can be seamlessly integrated with arbitrary policy models as a plug-in module, augmenting exploration without degrading the base policy performance. Moreover, the structured latent space enables human-guided exploration, further improving efficiency and controllability. Extensive experiments in both simulation and real-world tasks demonstrate that SOE consistently outperforms prior methods, achieving higher task success rates, smoother and safer exploration, and superior sample efficiency. These results establish on-manifold exploration as a principled approach to sample-efficient policy self-improvement.",
            "introduction": "“We want AI agents that can discover like we can, not which contain what we have discovered.”\n            — Richard Sutton, The Bitter Lesson\n\nIn recent years, data-driven robot learning [10, 53, 5, 7, 25] has attracted considerable attention, particularly for its potential to enhance robotic manipulation capabilities through large-scale data collection and training. By modeling visuomotor behaviors with neural networks, these approaches allow robot policies to learn from expert demonstrations and achieve near-human performance across a variety of tasks.\n\nDespite these advances, most existing methods still rely heavily on human teleoperation for data acquisition [53, 13] and policy refinement [30, 31], which presents several challenges. A primary concern is the high cost of teleoperation, as it typically requires skilled operators and specialized equipment, thereby limiting the scalability of data collection. More critically, teleoperated demonstrations often fail to cover the diverse scenarios a robot could encounter in the real world, resulting in distributional bias [52] and compounding error [39]. The problem is further exacerbated by the fact that human operators may act based on contextual cues inaccessible to robot sensors. Robots, on the other hand, may internalize human habits rather than task-relevant behaviors. As a result, simply scaling up teleoperated data is not the optimal path toward improving policy performance.\n\nInstead of passively imitating human-provided behavior, a line of research addresses this challenge by enabling robot policy self-improvement [6, 23, 35, 32]—actively exploring the environment to collect diverse experience and leveraging that experience to refine policies. Under this paradigm, robots can autonomously discover novel behaviors that go beyond the coverage of human demonstrations. By iteratively practicing the learned behaviors, they also develop a deeper understanding of the natural variability in their actions, ultimately leading to a more robust and resilient policy.\n\nThe key to sample-efficient robot policy self-improvement lies in effective exploration. Prior work [3, 23] has shown that imitation-learned policies often overfit demonstrations, collapse into single-modal motions, and fail to produce diverse behaviors. Without proper exploration, these policies tend to repeat failed behaviors, limiting their ability to discover improved solutions. While random exploration strategies can occasionally yield novel behaviors [29], they are generally ineffective in high-dimensional action spaces [28] and can pose safety risks in real-world deployment [16], causing potential hardware damage. This necessitates a more structured approach to exploration—one that ensures safety and effectiveness without sacrificing the diversity of experiences.\n\nTo this end, we propose SOE, a novel framework for Sample-Efficient Robot Policy Self-improvement via On-Manifold Exploration. The core idea of our method is to ensure that exploration remains constrained to the manifold of valid actions—critical for both safety and effectiveness. Prior works often perturb the action space directly [29] or inject random noise [23], leading to temporally inconsistent and unsafe behaviors, particularly under “action chunking” representations [53]. In contrast, we perform exploration in a compact latent space learned through a variational information bottleneck (VIB). The latent representation in this space preserves only task-essential information in observation while discarding irrelevant details, ensuring exploration remains structured and efficient. As illustrated in Fig. 1, by operating on this latent representation, our framework enables effective on-manifold exploration and more robust policy improvement. Furthermore, we demonstrate that in the latent space, action chunks are naturally disentangled into distinct modes. Leveraging this property, we achieve controllable exploration, which allows users to guide exploration toward preferred directions, thereby enhancing interpretability and further boosting sample efficiency. Implemented as a plug-in module, our approach can be seamlessly integrated with existing imitation learning algorithms and jointly optimized, without any degradation in their performance.\n\nTo evaluate the effectiveness of our method, we conduct extensive experiments across a variety of robot manipulation tasks in both simulation and real world. The results show that SOE consistently outperforms prior exploration methods in effectiveness, motion smoothness, and sample efficiency. With just one round of policy self-improvement, our method achieves substantial gains over the base policy, including an average relative improvement of 50.8% on real-world tasks. Additional experiments in simulation and ablation studies further confirm multi-round performance improvements and the contribution of each component in our framework. Collectively, these findings demonstrate that on-manifold exploration provides a structured, safe, and effective approach to sample-efficient robot policy self-improvement.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在高维机器人动作空间中实现安全、有效且样本高效的策略自我改进中的探索？\n2. 如何通过潜在表征约束探索到行动流形以避免无效或危险的随机扰动？\n3. 如何实现对探索的可控性与可解释性，同时保持对既有模仿学习的无干扰性？\n4. 如何在仿真与真实世界任务中提升任务成功率、平滑性和样本效率？\n\n【用了什么创新的方案】\nSOE在任务相关因素的紧凑潜在表示上进行探索，利用变分信息瓶颈学习一个仅保留任务本质信息的潜在空间，并在该潜在流形上进行探索以保持动作的有效性和安全性。它将探索与现有策略模型无缝对接，可作为插件模块嵌入到任意模仿学习框架中；潜在空间还使动作块在不同模式上解耦，从而实现可控探索，并支持人工引导以提高效率。实验结果表明在仿真和真实任务中，SOE实现更高的成功率、更平滑且更安全的探索以及更强的样本效率。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces",
            "authors": "Kuanqi Cai,Chunfeng Wang,Zeqi Li,Haowen Yao,Weinan Chen,Luis Figueredo,Aude Billard,Arash Ajoudani",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19261",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19261",
            "arxiv_html_link": "https://arxiv.org/html/2509.19261v1",
            "abstract": "Robotic manipulation in dynamic environments often requires seamless transitions between different grasp types to maintain stability and efficiency. However, achieving smooth and adaptive grasp transitions remains a challenge, particularly when dealing with external forces and complex motion constraints. Existing grasp transition strategies often fail to account for varying external forces and do not optimize motion performance effectively. In this work, we propose an Imitation-Guided Bimanual Planning Framework that integrates efficient grasp transition strategies and motion performance optimization to enhance stability and dexterity in robotic manipulation. Our approach introduces Strategies for Sampling Stable Intersections in Grasp Manifolds for seamless transitions between uni-manual and bi-manual grasps, reducing computational costs and regrasping inefficiencies. Additionally, a Hierarchical Dual-Stage Motion Architecture combines an Imitation Learning-based Global Path Generator with a Quadratic Programming-driven Local Planner to ensure real-time motion feasibility, obstacle avoidance, and superior manipulability. The proposed method is evaluated through a series of force-intensive tasks, demonstrating significant improvements in grasp transition efficiency and motion performance.\nA video demonstrating our simulation results can be viewed at https://youtu.be/3DhbUsv4eDo.",
            "introduction": "Robotic manipulation in dynamic forceful operations—such as collaborative cutting or drilling—demands real-time adaptation to varying external forces that critically affect grasp stability. Consider a human-robot woodworking scenario (Fig. 1) where the robot must continuously adjust between uni-manual and bi-manual grasps to counteract changing cutting and drilling forces. This fundamental requirement exposes two unresolved challenges in existing methods: efficient grasp transitions by minimizing execution time and arm movement and motion performance awareness, as crucial metrics like manipulability and joint limits essential for control safety are often overlooked. To bridge this gap, we propose an imitation-guided planning framework that integrates efficient grasp transitions with motion performance constraints, ensuring both stability and dexterity in forceful tasks.\n\nMulti-step manipulation planners have long tackled regrasping and grasping transitions [1]. Traditional grasping involves transporting an object by repeatedly releasing and regrasping it as needed [2]. Conventional regrasp planners rely on a supporting surface for single-arm manipulation [3, 4], while recent research extends these strategies to dual-arm scenarios [5, 6, 7].\nHowever, existing methods do not explicitly account for dynamic external forces, which vary over time, nor do they optimize regrasp transitions during forceful interactions. Studies in forceful human-robot collaboration [8, 9, 10] focus on regulating contact forces but assume a fixed or pre-determined grasp. The key challenge remains: determining where and how a robot should grasp for stability and when to transition seamlessly under complex external forces. Recent works [11, 12, 13] have made progress, but achieving stable grasps that withstand varying forces while ensuring efficient planning, manipulability, and dexterity remains difficult.\nThis paper addresses two key challenges in forceful robotic manipulation: efficient grasp transitions and motion performance optimization, proposing a novel framework to overcome them.\n\nEfficient Grasp Transitions. Previous methods [11, 13] mostly rely on random sampling-based planners for grasp transitions, often resulting in high computational costs and unstable changes. To reduce task execution time and minimize the movement distance of the robot arm, we introduce Strategies for Sampling Stable Intersections in Grasp Manifolds for seamless uni-manual and bi-manual transitions. Our Directional Gradient-Based Resampling locally adjusts the unimanual manipulator along the negative gradient, ensuring stability while maintaining a secure unimanual grasp and minimizing movement. For tasks with multiple grasp changes, Multi-Grasp Transition Check (MTC) identifies a shared intermediate configuration, reducing redundant regrasping. To further boost efficiency, we propose a Hierarchical Dual-Stage Motion Architecture, combining an Imitation Learning-based Global Path Generator with a QP-driven local planner for real-time motion optimization and obstacle avoidance, enabling faster collision-free path generation than sampling-based methods.\n\nMotion Performance Optimization.\nIn forceful operations, robotic stability is determined by three motion performance factors: manipulability, dexterity, and joint limits, which ensure kinematic feasibility under varying forces. However, many recent methods [14, 15] tend to overlook these critical aspects.\nOur framework optimizes motion performance at both the grasp configuration level and during execution. To enhance manipulability and avoid kinematic limitations, we introduce a Motion Performance Map that encodes feasibility, manipulability, and joint limit proximity. This map guides grasp sampling toward optimal workspace regions, improving selection efficiency. During execution, we enforce manipulability constraints within the QP framework, ensuring control authority over the end-effector while avoiding singularities and joint limits. This real-time optimization enables stable, collision-free motion.\n\nBy integrating these advancements, our framework enhances grasp transition efficiency while ensuring superior motion performance in both grasp selection and execution, effectively addressing key limitations in existing forceful robotic planning approaches.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在动态外部力下实现稳定而高效的双手抓持与无缝转换（uni-manual到bi-manual）？\n2. 如何在抓取转移中兼顾运动性能（可控性、可控度、关节极限）并降低计算成本？\n3. 如何在强力操作场景中实时规划可行路径并避免碰撞，同时优化抓取点选择？\n\n【用了什么创新的方案】\n策略性在抓取流形中采样稳定交点以实现单臂到双臂的平滑转变，并用方向梯度重采样局部调整单臂抓持以保持稳定性。提出多抓取转变检查以寻找共用中间配置，降低冗余重新抓取。建立分层双阶段运动架构：基于模仿学习的全局路径生成器+QP驱动的局部规划器实现实时、避障且高可 manipulability 的路径规划。引入运动性能地图（可行性、可操作性、关节极限接近度）以引导抓取选择并在执行中通过QP约束实现对端执行器的控制权和避免奇异点。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Proactive-reactive detection and mitigation of intermittent faults in robot swarms",
            "authors": "Sinan Oğuz,Emanuele Garone,Marco Dorigo,Mary Katherine Heinrich",
            "subjects": "Robotics (cs.RO); Multiagent Systems (cs.MA); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19246",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19246",
            "arxiv_html_link": "https://arxiv.org/html/2509.19246v1",
            "abstract": "Intermittent faults are transient errors that sporadically appear and disappear. Although intermittent faults pose substantial challenges to reliability and coordination, existing studies of fault tolerance in robot swarms focus instead on permanent faults. One reason for this is that intermittent faults are prohibitively difficult to detect in the fully self-organized ad-hoc networks typical of robot swarms, as their network topologies are transient and often unpredictable. However, in the recently introduced self-organizing nervous systems (SoNS) approach, robot swarms are able to self-organize persistent network structures for the first time, easing the problem of detecting intermittent faults. To address intermittent faults in robot swarms that have persistent networks, we propose a novel proactive–reactive strategy to detection and mitigation, based on self-organized backup layers and distributed consensus in a multiplex network. Proactively, the robots self-organize dynamic backup paths before faults occur, adapting to changes in the primary network topology and the robots’ relative positions. Reactively, robots use one-shot likelihood ratio tests to compare information received along different paths in the multiplex network, enabling early fault detection. Upon detection, communication is temporarily rerouted in a self-organized way, until the detected fault resolves.\nWe validate the approach in representative scenarios of faulty positional data occurring during formation control, demonstrating that intermittent faults are prevented from disrupting convergence to desired formations, with high fault detection accuracy and low rates of false positives.",
            "introduction": "Reliability in networked systems requires consistently accurate information exchange among components, often under dynamic and uncertain conditions [1, 2]. If communication links fail or become unreliable during multi-hop communication, system convergence and performance guarantees can be compromised [3, 4, 5, 6]. In self-organized robot swarms, this challenge is exacerbated by asynchronous ad-hoc communication and decentralized coordination of actuation and decision making. Robots in a self-organized swarm rely solely on local information and communication with nearby robots, without any estimation of the global state of the swarm or its environment, often leading to prolonged convergence times and vulnerability to the spread of incorrect information [7]. Frequent communication between robots can cause faulty information to spread quickly and potentially degrade overall swarm performance or lead to permanent failures.\n\nSelf-organized robot swarms exhibit some inherent fault tolerance, through redundancy and a lack of single points of failure [8, 9].\nHowever, many fault types are not mitigated by this passive tolerance and instead require dedicated mechanisms for detection and mitigation [10, 11, 12, 13]. Somewhat counter-intuitively, self-organized robot swarms are inherently much more tolerant to complete robot failures than to partial ones [14]. For example, a single robot producing faulty or malicious information has been shown to be capable of severe disruption to overall swarm behavior [14, 15]. Faulty robots can also physically obstruct the rest of the swarm, and this interference can paradoxically be worsened by the redundancy that provides swarms with some types of inherent fault tolerance [16].\n\nOther faults to which self-organized robot swarms are vulnerable and which require dedicated mechanisms for detection and mitigation are intermittent faults (IFs). IFs are temporary faults that can appear, disappear, and reappear [17], potentially caused by communication interference, sensor malfunctions, or software bugs [18]. IFs are difficult to detect and diagnose due to their transience [19] and can cause significant disruptions without leaving an easily detectable trace [18].\nA representative example involves intermittent GPS signal degradation in cluttered environments, which can induce sporadic localization errors. These errors propagate through decentralized state estimation protocols, gradually undermining coordination mechanisms without generating explicit failure indicators.\nIn real applications, e.g., in robot swarms deployed in inaccessible or dangerous environments [20, 12], the consequences of IFs to mission performance and to safety can be severe and in some cases could be irreversible. Detecting and resolving IFs before they escalate is key to minimizing disruption: early detection can prevent cascading failures leading to erroneous execution of tasks and can prevent culmination in permanent failures, either of individual robots or the swarm as a whole [21].\n\nIFs are difficult to detect in robot swarms with fully self-organized ad-hoc networks, because the network topology is transient and often unpredictable. IFs are much more straightforward to detect in fully centralized systems and in networks with static structures, for example in sensor networks [22, 23, 24]. However, for multi-robot systems, full centralization and fully static networks also present downsides, such as single points of failure and limited scalability.\n\nOur recently introduced self-organizing nervous systems (SoNS) [25] approach combines aspects of centralization and decentralization through self-organized hierarchy. Using the SoNS approach, robot swarms are coordinated via temporary logical networks that are hierarchical and culminate in a dynamic “brain” robot (i.e., leader), but which are not imposed from the outside, being instead established and maintained in a self-organized manner. This provides robot swarms with persistent and predictable network structures that are more amenable to detecting IFs, without introducing any single points of failure.\nIn short, the SoNS approach allows, for the first time, to apply centralized fault detection and mitigation strategies to robot swarms without sacrificing their oft-cited benefits of scalability, flexibility, and a lack of single points of failure.\n\nSwarm robotics usually studies passive tolerance to permanent faults [26]—that is, faults such as electromechanical failures that will remain unless they are actively repaired.\nWhen relying on passive fault tolerance, studies have usually demonstrated that a swarm continues its mission after some or many robots have failed, either by continuing with fewer robots  [10, 27, 28] or by replacing/repairing the failed robots without pausing the mission [29, 30, 31, 25].\n\nSwarm robotics studies that focus specifically on fault tolerance do not typically rely on passive tolerance, instead developing dedicated mechanisms to handle permanent faults.\nThe majority of these methods detect and react to permanent electromechanical failures after they have occurred [21, 32], often relying on time-out mechanisms in which a robot is considered non-operational if it does not respond to a message within a certain time. Existing methods for detecting permanent faults include LED synchronization [29], simulation comparison [33], shared sensor data analysis [28], and behavioral feature vectors (BFVs) [11].\nThese methods often focus on detection, assuming that once a fault is detected, a repair or other intervention is possible during normal operation (e.g., [34, 29, 21]). Although such repairs might be unrealistic in inaccessible, hazardous, or congested environments [35, 21, 12], future methods for autonomous repair could be developed to complement detection. In short, the existing reactive methods can be considered effective for many types of permanent faults [10].\nHowever, the above-mentioned detection approaches are unlikely to be applicable to the transience of IFs and their long response times [36] would likely be too slow for the early detection and recovery that IFs require. Methods to detect and repair IFs in robot swarms still need to be developed.\n\nTo the best of our knowledge, there are no existing swarm robotics methods focused on IF detection and recovery. Strategies developed for IFs in other types of systems, such as model-based analysis (e.g., discrete-event-system models [37], causal models [38]) and quantitative analysis (e.g., parameter estimation [39], geometric approaches [40], Kalman-like filtering [41]), provide valuable insights but primarily target single-unit systems with static and known system models [24, 42], which is incompatible with self-organized systems such as robot swarms. Likewise, IF strategies developed for sensor networks [22, 23] typically use fully centralized architectures to correct information transmission and reception [24], and are therefore incompatible with self-organized systems.\n\nFurthermore, although fully centralized monitoring is highly effective for detecting and correcting IFs, it can present problems of inflexibility, limited scalability, and single points of failure (e.g., at the point where monitoring is centralized). Fully self-organized approaches, by contrast, would be highly flexible and offer greater scalability and a lack of single points of failure, but would present problems of limited accuracy and potentially slow reaction times.\nIn this paper, we aim to combine elements of each system type to get the benefits of both. Using our proposed proactive–reactive approach, robots can monitor each other using self-organizing hierarchy, detecting IFs accurately and remedying them proactively.\n\nTo demonstrate our proposed proactive–reactive approach, we use the SoNS concept of self-organizing hierarchy in a robot swarm, which has been shown to incorporate temporarily centralized structures into an otherwise self-organized robot swarm without introducing single points of failure or inherently limiting scalability [25, 30, 43, 44, 45, 46]. We build on our recent theoretical foundations for self-organizing hierarchical frameworks: hierarchical Henneberg construction (HHC) [47]. In our previous work [47], we demonstrated HHC for key self-reconfiguration problems (framework merging, robot departure, and framework splitting), derived the mathematical conditions of these problems, and developed algorithms that preserve rigidity and hierarchy using only local information.\n\nIn the remainder of this paper, we assume all graphs are constructed using these already demonstrated HHC algorithms, and refer to such graphs as HHC-constructed graphs. See Appendix A for details on how HHC and SoNS are related.\n\nIn fault tolerance for multi-robot systems, both proactive and reactive mechanisms are important [48].\nIn this paper, we propose a novel proactive–reactive method to detect and mitigate IFs in robot swarms.\nIn the proposed proactive–reactive method, the robots first use distributed consensus to preemptively self-organize dynamic backup communication paths before IFs are detected. Then, the robots compare information received via primary and backup paths to detect IFs, using a one-shot likelihood ratio test. When IFs are detected, the robots react by rerouting communication through the dynamic backup paths. In this paper, we apply the proposed proactive–reactive method to a scenario of intermittently faulty relative positional information within multi-robot formations that have a hierarchical structure towards a fault-free leader, and demonstrate that the method mitigates IFs and robots are able to continue with the desired formations.\n\nThe main technical contributions of this paper can be summarized as follows:\n\nWe address a current gap in robot swarm networking, specifically how to establish back-up communication paths for leader–follower formation control in a self-organized robot swarm. We address this gap by extending the biased minimum consensus (BMC) [49] protocol for shortest path planning in static graphs. We introduce the adaptive biased minimum consensus (ABMC) protocol for dynamic graphs—addressing time-varying topologies, node neighborhoods, and costs. We demonstrate that our ABMC protocol addresses the minimum-cost path problem, with two objectives integrated into a single cost function: to minimize the number of hops to the destination (the leader robot) and to minimize the degree of network congestion (by minimizing the occurrence of parallel edges).We provide the mathematical properties and stability analysis of the ABMC protocol as a distributed consensus mechanism in dynamic graphs with piecewise constancy, including providing the necessary and sufficient conditions to uniquely determine an equilibrium point representing a minimum-cost backup path.\n\nWe address a current gap in robot swarm fault tolerance, specifically tolerance against intermittent faults (IFs). We address this gap by proposing a novel proactive–reactive fault-tolerance strategy for detection and mitigation of IFs in robot swarms. Our proposed strategy uses the ABMC protocol to construct backup network layers and combines it with a distributed likelihood ratio (LR) protocol to dynamically reroute traffic in the constructed multiplex network. We propose the mathematical conditions and design the distributed algorithms for backup layer construction and for execution of the proactive–reactive strategy for IF detection and mitigation. We also provide the time and space complexity and efficiency properties of both distributed algorithms. Finally, we demonstrate the proactive–reactive fault-tolerance strategy in formations of 20 robots with moving leaders.\n\nThe rest of the paper is organized as follows. In Sec. II, the foundational concepts regarding hierarchical frameworks are presented, along with the existing BMC protocol. In Sec. III, we formulate three key problems addressed in this paper: construction of dynamic minimum-cost backup paths, detection of IFs using the constructed backup paths, and mitigation of the detected IFs using the constructed backup paths. The first problem is addressed in Secs. IV and V, and the second and third problem are addressed in Sec. VI. Finally, in Sec. VII we validate our contributions in experiments of representative scenarios. The conclusions are summarized in Sec. VIII.",
            "llm_summary": "【关注的是什么问题】\n1. Intermittent faults (IFs) in self-organized robot swarms and their impact on formation convergence and reliability\n2. 如何在自组织机器人群体中检测与缓解IFs，兼顾可扩展性与无单点故障\n3. 在动态拓扑中实现备份路径的自组织构建与快速故障检测\n4. 将集中式检测策略与去中心化群体控制结合的主动-被动（proactive–reactive）两层防护\n\n【用了什么创新的方案】\n核心解决方案：提出主动-被动（proactive–reactive）IF检测与缓解框架，利用自组织层次化的备份通信路径实现多路径冗余；通过自组织的ABMC协议在动态图构建最小代价备份路径，并使用分布式一次性似然比检验比较主路径与备份路径的信息以实现早期IF检测；IF检测后，故事通路通过自组织方式重新路由通信直至故障消失；在20机器人队形实验中验证方法提升鲁棒性与收敛性。并将ABMC与LR检验结合，给出时空复杂度分析与稳定性条件。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap",
            "authors": "Tianyu Wu,Xudong Han,Haoran Sun,Zishang Zhang,Bangchao Huang,Chaoyang Song,Fang Wan",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 4 figures, accepted to Data@CoRL2025 Workshop",
            "pdf_link": "https://arxiv.org/pdf/2509.19169",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19169",
            "arxiv_html_link": "https://arxiv.org/html/2509.19169v1",
            "abstract": "The transfer of manipulation skills from human demonstration to robotic execution is often hindered by a “domain gap” in sensing and morphology. This paper introduces MagiClaw, a versatile two-finger end-effector designed to bridge this gap. MagiClaw functions interchangeably as both a handheld tool for intuitive data collection and a robotic end-effector for policy deployment, ensuring hardware consistency and reliability. Each finger incorporates a Soft Polyhedral Network (SPN) with an embedded camera, enabling vision-based estimation of 6-DoF forces and contact deformation. This proprioceptive data is fused with exteroceptive environmental sensing from an integrated iPhone, which provides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS application, MagiClaw streams synchronized, multi-modal data for real-time teleoperation, offline policy learning, and immersive control via mixed-reality interfaces. We demonstrate how this unified system architecture lowers the barrier to collecting high-fidelity, contact-rich datasets and accelerates the development of generalizable manipulation policies. Please refer to the iOS app at https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.",
            "introduction": "The success of modern robot learning paradigms, from Learning from Demonstration (LfD) [1, 2] to offline reinforcement learning, is fundamentally dependent on the quality and richness of the underlying data [3]. For contact-rich manipulation tasks, robust policies require more than just kinematic trajectories; they demand a holistic understanding of interaction forces, tactile feedback, and environmental context [4, 5]. Consider a human deftly handling a delicate object: the action is a symphony of precise motion, modulated forces, and continuous tactile adjustments [6]. Replicating such skills requires capturing this multi-modal information stream in its entirety.\n\nHowever, existing data collection methodologies present significant challenges. First, they often rely on a patchwork of disparate, expensive sensors—such as external motion capture systems, wrist-mounted force/torque sensors, and complex tactile skins [7, 8]—resulting in cumbersome and costly setups. This high barrier to entry limits the scale and diversity of data collection efforts [9]. Second, and more critically, a persistent domain gap exists between the human demonstrator and the robotic learner [10]. Data is often collected using one set of hardware (e.g., an instrumented glove) and deployed on a robot with entirely different sensor suites and end-effector morphology. This mismatch necessitates complex domain adaptation techniques and is a primary reason why policies trained on demonstration data often fail to generalize to physical hardware [11].\n\nTo address these challenges, we present MagiClaw, a unified hardware platform designed to seamlessly bridge the gap from human demonstration to robotic deployment. MagiClaw is a dual-purpose, two-fingered gripper that merges three key innovations:\n\nUnified Hardware Form Factor: The exact same MagiClaw device can be used as a hand-held tool for human demonstration or mounted on a robot arm for autonomous execution. This hardware consistency minimizes the sensor and morphological domain gap, facilitating direct policy transfer.\n\nVision-Based Proprioceptive Fingertips: Each finger integrates a Soft Polyhedral Network (SPN) [12] with an embedded miniature camera. This novel design enables visuotactile perception, inferring 6-DoF forces, torque, and high-resolution contact deformation from the distortion of the internal lattice structure, thereby obviating the need for costly external force sensors.\n\nIntegrated Multi-Modal Exteroception: An attached iPhone leverages its powerful sensor suite (LiDAR, RGB cameras, IMU) and ARKit framework [13] to provide synchronized, rich environmental context, including gripper pose, depth maps, and high-resolution video.\n\nOur primary contribution is an integrated system that fundamentally streamlines the collection of holistic, contact-centric data for robot learning. By fusing proprioceptive force/tactile data from the fingertips with exteroceptive visual and spatial data from a commodity smartphone, MagiClaw offers a low-cost, powerful, and user-friendly solution for both teleoperation and autonomous policy development. We posit that by democratizing access to such high-fidelity, multi-modal data, MagiClaw can serve as a catalyst for developing more robust and generalizable manipulation skills, advancing the pursuit of universal action embodiment in robotics.",
            "llm_summary": "【关注的是什么问题】\n1. 领域差距导致的人类示范到机器人执行的迁移困难（≤40词）\n2. 需要高质量多模态数据但现有传感器系统昂贵、零散、耦合度高的问题（≤40词）\n3. 如何在同一硬件上实现手持示范与机器人执行之间的无缝切换（≤40词）\n4. 如何通过 visuotactile 与环境感知实现对力、接触变形的高保真推断（≤40词）\n5. 如何降低数据采集成本并提高数据多样性以提升策略泛化能力（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：MagiClaw 将两指软聚合网格内嵌相机实现视觉-本体感知的六自由度力与接触变形估计；每指作為 SPN 传感器；通过 iPhone 的 LiDAR、RGB、深度数据进行同步的外感知；手持与机器人端可互换的统一硬件形式；自带 iOS 应用实现实时远控、离线学习数据流；降低数据采集成本、提升多模态数据质量与一致性。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination",
            "authors": "Mark Gonzales,Ethan Oh,Joseph Moore",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 Pages, 7 Figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19168",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19168",
            "arxiv_html_link": "https://arxiv.org/html/2509.19168v1",
            "abstract": "In this paper, we present a receding-horizon, sampling-based planner capable of reasoning over multimodal policy distributions. By using the cross-entropy method to optimize a multimodal policy under a common cost function, our approach increases robustness against local minima and promotes effective exploration of the solution space. We show that our approach naturally extends to multi-robot collision-free planning, enables agents to share diverse candidate policies to avoid deadlocks, and allows teams to minimize a global objective without incurring the computational complexity of centralized optimization. Numerical simulations demonstrate that employing multiple modes significantly improves success rates in trap environments and in multi-robot collision avoidance. Hardware experiments further validate the approach’s real-time feasibility and practical performance.",
            "introduction": "Local minima pose a fundamental challenge for finite-horizon, gradient-based planning approaches. In multi-robot scenarios, local minima can arise not only from the environment but also from dynamic factors, such as the changing trajectories of teammates, which may inadvertently block or cut off routes that would otherwise be viable. These pitfalls often cause robots to become stuck, find suboptimal solutions, or fail to coordinate effectively in complex environments.\n\nSampling-based planners, such as Model Predictive Path Integral (MPPI) [1] and Cross-Entropy Method (CEM) [2, 3], attempt to improve the trajectory cost by stochastically sampling and evaluating trajectories in the cost landscape. In practice, these methods utilize hyperparameters, such as sampling variance, number of samples, and the horizon length, to adapt the exploration to the environment. However, both MPPI and CEM typically sample trajectories around the prior best policy, leading to a concentration of samples in a narrow region of the solution space. This localized search impedes the planner’s ability to effectively navigate around traps or escape from local minima once they occur, especially in environments with challenging topology. As a result, the planner can become stuck in suboptimal regions, regardless of the variance or adaptation strategy.\n\nIn multi-robot systems, the difficulty is exacerbated by the need for robots to coordinate planned trajectories. Centralized control approaches [4, 5, 6, 7] can, in principle, achieve globally optimal coordination; however, they suffer from scalability issues and high computational costs as the team size increases. Distributed methods, while scalable, often require robots to individually select their optimal trajectory, subsequently negotiating with teammates to reach a feasible consensus. When each robot contributes only a single candidate trajectory, the team risks deadlock or persistent local minima, as a lack of trajectory diversity reduces the likelihood of discovering collision-free, cooperative maneuvers, especially when teammates dynamically update their plans or block each other’s routes in real-time.\n\nTo overcome these limitations, we introduce a multimodal sampling and clustering framework that maintains multiple policy candidates for each robot, thereby increasing diversity and robustness against local minima in both environmental and collaborative planning contexts.\n\nOur contributions are:\n\nA cross-entropy planning approach capable of preserving multiple policy modes for increased planning robustness.\n\nA multi-robot coordination framework that enables reasoning about sets of candidate policies to avoid local minima and deadlocks more reliably.\n\n1. A cross-entropy planning approach capable of preserving multiple policy modes for increased planning robustness.\n\n2. A multi-robot coordination framework that enables reasoning about sets of candidate policies to avoid local minima and deadlocks more reliably.",
            "llm_summary": "【关注的是什么问题】\n1. 本地最优陷阱与多模态政策在单机器人导航中的鲁棒性（≤40词）\n2. 多机器人协同中的死锁、协调与全局目标优化的可扩展性（≤40词）\n3. 受限搜索空间中多模态策略对避障与探索的提升（≤40词）\n\n【用了什么创新的方案】\n- 引入跨熵法的多模态策略规划，保留多条候选策略以提升对局部极小值的鲁棒性。\n- 基于采样与聚类的框架，在多机器人场景中共享多策略集合，避免死锁并实现分布式协同。\n- 通过 receding-horizon 与多模态分布采样实现对全局成本的高效优化，降低集中化计算开销。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer",
            "authors": "Kangmin Kim,Seunghyeok Back,Geonhyup Lee,Sangbeom Lee,Sangjun Noh,Kyoobin Lee",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 5 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19142",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19142",
            "arxiv_html_link": "https://arxiv.org/html/2509.19142v1",
            "abstract": "Bimanual grasping is essential for robots to handle large and complex objects. However, existing methods either focus solely on single-arm grasping or employ separate grasp generation and bimanual evaluation stages, leading to coordination problems including collision risks and unbalanced force distribution. To address these limitations, we propose BiGraspFormer, a unified end-to-end transformer framework that directly generates coordinated bimanual grasps from object point clouds. Our key idea is the Single-Guided Bimanual (SGB) strategy, which first generates diverse single grasp candidates using a transformer decoder, then leverages their learned features through specialized attention mechanisms to jointly predict bimanual poses and quality scores. This conditioning strategy reduces the complexity of the 12-DoF search space while ensuring coordinated bimanual manipulation. Comprehensive simulation experiments and real-world validation demonstrate that BiGraspFormer consistently outperforms existing methods while maintaining efficient inference speed (<<0.05s), confirming the effectiveness of our framework. Code and supplementary materials are available at https://sites.google.com/bigraspformer",
            "introduction": "Bimanual grasping enables robots to manipulate large, heavy, or unwieldy objects beyond single-arm capabilities, making it essential for tasks such as lifting furniture, carrying long boards, or moving large boxes [1, 2]. However, most robotic grasping research has focused on single-arm systems, primarily on learning to detect 6-DoF grasp poses from point clouds [3, 4, 5, 6, 7, 8]. While effective for single-arm tasks, these approaches cannot be directly extended to bimanual scenarios. First, bimanual grasping expands the action space to 12-DoF, doubling the computational complexity. Second, it introduces new challenges, including collision avoidance, balanced force/torque distribution, and dual-arm coordination for post-grasp manipulation.\n\nFor bimanual grasping, only a few methods have been proposed so far. The DA2 dataset [9] introduced the first benchmark by extending single-arm datasets [10, 11, 3] with dual-arm-specific metrics such as force closure, dexterity, and torque balance [9, 12]. However, most existing approaches adopt modular architectures that separate grasp generation and evaluation. For example, Dual-PointNetGPD [9] evaluates the quality of grasp pairs from given candidates, requiring external single-arm grasp generators. Similarly, CGDF [13] directly generates bimanual grasps but lacks integrated quality prediction, instead relying on additional scoring modules or heuristic pairing strategies [14, 15]. As a result, current methods yield limited diversity, poor coordination, and high computation due to modular pipelines.\n\nIn this paper, we propose BiGraspFormer, the first unified end-to-end framework that directly generates coordinated bimanual grasps from object point clouds (Fig. 1). The key insight is that single-grasp features can effectively guide bimanual grasp generation, rather than treating dual-arm coordination as two independent problems. BiGraspFormer introduces a novel Single-Guided Bimanual (SGB) strategy: it first generates diverse single-arm grasp candidates, then leverages their learned features through specialized attention mechanisms to jointly predict bimanual poses and quality scores. This unified approach eliminates separate modules and explicitly models coordination between grasps, enabling stable and efficient dual-arm manipulation. Comprehensive experiments in both simulation and real-world environments demonstrate that BiGraspFormer achieves superior success, diversity, and speed compared to existing methods.\n\nOur contributions are summarized as follows:\n\nWe propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\nWe propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\nWe introduce the Single-Guided Bimanual (SGB) strategy, which leverages single-arm grasp features to guide bimanual generation, reducing computational complexity and enhancing dual-arm coordination.\n\nWe achieve state-of-the-art bimanual grasping performance while maintaining fast inference suitable for real-world deployment.\n\n1. We propose BiGraspFormer, the first unified end-to-end transformer for diverse, stable bimanual grasp generation.\n\n2. We introduce the Single-Guided Bimanual (SGB) strategy, which leverages single-arm grasp features to guide bimanual generation, reducing computational complexity and enhancing dual-arm coordination.\n\n3. We achieve state-of-the-art bimanual grasping performance while maintaining fast inference suitable for real-world deployment.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在点云上直接生成协调的双臂抓取姿态（12-DoF）以实现稳定的双臂物体操作\n2. 现有方法的模块化导致双臂协作差、碰撞风险与力分配不均等问题\n3. 如何在端到端框架中有效建模双臂协调并提高推理速度\n\n【用了什么创新的方案】\nSingle-Guided Bimanual (SGB) 策略：先通过变换器解码器生成多样的单臂抓取候选，再利用这些单臂抓取的特征，通过专门的注意力机制共同预测双臂抓取的姿态与质量分数，从而端到端地产生协调的双臂抓取。该统一框架 eliminates 分离的抓取生成与评估模块，直接从对象点云进行协调抓取的生成与评分。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation",
            "authors": "Sarvesh Prajapati,Ananya Trivedi,Nathaniel Hanson,Bruce Maxwell,Taskin Padir",
            "subjects": "Robotics (cs.RO)",
            "comment": "8 pages, 10 figures, submitted to Robotic Computing & Communication",
            "pdf_link": "https://arxiv.org/pdf/2509.19105",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19105",
            "arxiv_html_link": "https://arxiv.org/html/2509.19105v1",
            "abstract": "Successful navigation in outdoor environments requires accurate prediction of the physical interactions between the robot and the terrain. To this end, several methods rely on geometric or semantic labels to classify traversable surfaces. However, such labels cannot distinguish visually similar surfaces that differ in material properties. Spectral sensors enable inference of material composition from surface reflectance measured across multiple wavelength bands. Although spectral sensing is gaining traction in robotics, widespread deployment remains constrained by the need for custom hardware integration, high sensor costs, and compute-intensive processing pipelines. In this paper, we present RGB Image to Spectral Signature Neural Network (RS-Net), a deep neural network designed to bridge the gap between the accessibility of RGB sensing and the rich material information provided by spectral data. RS-Net predicts spectral signatures from RGB patches, which we map to terrain labels and friction coefficients. The resulting terrain classifications are integrated into a sampling-based motion planner for a wheeled robot operating in outdoor environments. Likewise, the friction estimates are incorporated into a contact-force–based MPC for a quadruped robot navigating slippery surfaces. Thus, we introduce a framework that learns the task-relevant physical property once during training and thereafter relies solely on RGB sensing at test time. The code is available at https://github.com/prajapatisarvesh/RS-Net.",
            "introduction": "Autonomous robots are increasingly deployed in everyday settings, ranging from self-driving taxis [1] and search-and-rescue missions [2, 3] to wildfire prevention [4, 5]. In such unstructured environments, reliable autonomy demands more than obstacle avoidance. It requires precise reasoning about how terrain properties influence motion. For example, vehicles must modulate braking on icy roads, and off-road platforms should bypass dense swamps to avoid entrapment. These scenarios show that perception must move beyond geometry and semantics toward reliable estimates of robot–terrain interactions.\n\nSeveral off-road motion planning pipelines use RGB cameras to identify terrain from images [6, 7, 8]. In some cases, visually similar surfaces with different physical properties, such as ice on asphalt, may be mislabeled, leading to invalid traversability cost maps. Depth cameras and LiDAR are often used to estimate the ease of motion over a surface [9, 10]. However, the robot must first drive the terrain to create a dataset, which risks hardware damage and necessitates tuning specific to the operating site.\n\nIn contrast, spectral sensors offer a non-invasive way to estimate material properties. This is accomplished by leveraging distinct patterns of light absorption and reflection, known as spectral signatures, to characterize underlying material composition. These capabilities are finding use in robotics applications such as wildfire risk monitoring [3], manipulation [11, 12], and exploration [13]. By mapping spectral signatures to physical quantities such as moisture content, rigidity, or surface type, the same sensing stack can be repurposed across robots and environments with minimal changes to the processing pipeline. However, challenges such as custom mounts, calibration requirements, large datasets, and high sensor costs currently limit deployment at scale.\n\nRGB cameras are inexpensive, widely available, and already standard in robotic perception pipelines such as object detection [14] and tracking [15]. Compared to hyperspectral systems, they are cheaper, lighter, and more power efficient, which simplifies integration on mobile platforms. Advances in deep learning [16] now allow RGB imagery to approximate measurements traditionally obtained from more information-dense sensors.\n\nMotivated by this, we seek to retain the deployment advantages of RGB cameras while recovering spectral sensor features. We introduce RGB Image to Spectral Signature Neural Network or RS-Net, a deep neural network architecture trained on spectral data collected from diverse materials. It maps RGB image patches to their corresponding spectral signatures. These estimates are passed to a lightweight feedforward neural network whose weights are fine-tuned for the target physical property. We retrain this network once per task, enabling the same neural network architecture to perform terrain classification and friction estimation. Our entire inference pipeline runs at approximately 5 Hz, making it suitable for real-time robotic applications. Fig. LABEL:fig:paper_intro outlines the proposed architecture.\n\nWe validate our method in both simulation and hardware experiments. The terrain classification is used in a sampling-based motion planner for outdoor navigation of a skid-steer robot. Similarly, the friction estimates are integrated into a model predictive control (MPC) scheme for a quadrupedal robot operating on slippery surfaces. Finally, we also discuss how the proposed approach generalizes to other robots and additional physical properties relevant to off-road planning.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在不增加硬件成本的情况下，通过RGB图像推断材料光谱特征以更准确地评估地形与摩擦\n2. how to bridge RGB sensing with spectral properties to improve terrain traversability prediction and motion planning\n3. 将光谱特征转化为可用于采样式路径规划和MPC的物理属性\n4. 实时性与通用性：在多种环境与机器人上实现低成本、快速推断的框架\n\n【用了什么创新的方案】\nRS-Net 将 RGB patch 映射到光谱签名，在此基础上训练一个轻量前馈网络输出地形标签和摩擦系数；整个管线在测试时仅需 RGB，推理约 5 Hz；通过在训练阶段明确学习任务相关的物理属性，使同一架构可一次性对不同任务进行微调并发布到实际机器人上。代码开放，支持仿真与硬件实验中的地形导航和滑移表面的 MPC/规划集成。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation",
            "authors": "Hongli Xu,Lei Zhang,Xiaoyue Hu,Boyang Zhong,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "project website:this https URL, 11 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.19102",
            "code": "https://sites.google.com/view/funcanon",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19102",
            "arxiv_html_link": "https://arxiv.org/html/2509.19102v1",
            "abstract": "General-purpose robotic skills from end-to-end demonstrations often leads to task-specific policies that fail to generalize beyond the training distribution.\nTherefore, we introduce FunCanon, a framework that converts long-horizon manipulation tasks into sequences of action chunks, each defined by an actor, verb, and object.\nThese chunks focus policy learning on the actions themselves, rather than isolated tasks, enabling compositionality and reuse. To make policies pose-aware and category-general, we perform functional object canonicalization for functional alignment and automatic manipulation trajectory transfer, mapping objects into shared functional frames using affordance cues from large vision–language models.\nAn object-centric and action-centric diffusion policy FuncDiffuser trained on this aligned data naturally respects object affordances and poses, simplifying learning and improving generalization ability.\nExperiments on simulated and real-world benchmarks demonstrate category-level generalization, cross-task behavior reuse, and robust sim-to-real deployment, showing that functional canonicalization provides a strong inductive bias for scalable imitation learning in complex manipulation domains.\nDetails of the demo and supplemental material are available on our project website\n https://sites.google.com/view/funcanon.",
            "introduction": "As robots transition from controlled laboratory settings to unstructured real-world environments, developing robust and generalizable manipulation policies becomes increasingly critical. A fundamental challenge is enabling agents to generalize across unseen objects, diverse poses, and varying tasks — a capability that remains elusive for current imitation learning approaches.\n\nImitation learning methods based on RGB images [1] or point clouds [2] often suffer from limited precision and generalization due to viewpoint sensitivity, noisy observations, and redundant scene encodings. In contrast, 3D scene representations have shown promise in improving generalization [3]. To further address these challenges, object-centric representations, which focus on structured, object-level information such as 6D poses [4, 5] and scene flow [6], have gained significant attention. SPOT [4] demonstrates that SE(3) pose diffusion policy can improve cross-embodiment generalization, even when trained solely on passive human videos.\nExisting object-centric approaches [4] often depend on instance-specific, goal-conditioned trajectories, limiting generalization across categories and tasks. We attribute this to viewing manipulation as monolithic programs rather than modular, reusable behaviors.\nIn the field of computer vision, prior work, such as UAD [7] and Object Canonicalization [8], mainly targets improving visual representations and semantic understanding. Related efforts have also investigated category-level affordance pose estimation [9]. However, these methods have not explored how such representations can be leveraged for improving category-level alignment of robotic manipulation data, or augmenting manipulation trajectories.\n\nKey open questions remain: how can generalized representations be leveraged to synthesize diverse manipulation data, model long-horizon tasks, and train robust, generalizable manipulation policies?\n\nTo address these challenges, we propose FunCanon, a framework that models manipulation as compositions of reusable action primitives—such as pouring, grasping, or inserting—defined over functionally aligned bi-object interactions. By leveraging affordance cues from large vision-language models (VLMs), FunCanon canonicalizes semantically related objects (e.g., kettles and pitchers) into shared functional frames. This functional alignment enables the automatic manipulation trajectory transfer and the training of pose-aware, object-centric diffusion policies that focus solely on interaction dynamics, decoupled from specific object identities, camera viewpoints, or task semantics. To achieve robust and generalizable manipulation, we first decompose long-horizon tasks into meaningful action chunks, each specifying an actor, an action, and an object. This segmentation is performed by a large multimodal language model (MLLM), such as GPT-4o, in combination with large vision models (LVM) that extract affordance cues to guide chunking based on functional relevance. Next, we integrate these affordance cues with precise object pose estimates to perform functional alignment, canonicalizing objects into shared functional frames. This process identifies action-related affordance regions and aligns bi-object poses, producing a semantically grounded representation of manipulation interactions. Leveraging this functional alignment, automatic trajectory transfer method is proposed to augment training data on RLBench base tasks to increase data diversity and functional coverage. During policy training and inference, our object-centric diffusion policy receives inputs encoding both affordances, poses of bi-object pairs, point clouds and action verb and estimate actions.\nOur main contributions are:\n\nIntroducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\nIntroducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\nLeveraging large vision-language models for affordance-driven functional canonicalization for functional alignment and automatic trajectory transfer, enabling pose-aware and category-generalizable policy learning.\n\nDeveloping an object-centric diffusion policy trained on functionally aligned data, achieving both instance-level and category-level generalization and robust sim-to-real transfer.\n\n1. Introducing FunCanon, which decomposes complex manipulation tasks into reusable action primitives grounded in functionally aligned object pairs. We explicitly incorporate the grasping phase within action primitives, addressing a key gap in prior approaches.\n\n2. Leveraging large vision-language models for affordance-driven functional canonicalization for functional alignment and automatic trajectory transfer, enabling pose-aware and category-generalizable policy learning.\n\n3. Developing an object-centric diffusion policy trained on functionally aligned data, achieving both instance-level and category-level generalization and robust sim-to-real transfer.",
            "llm_summary": "【关注的是什么问题】\n1. 如何将长时程操纵任务分解为可重复使用的动作原语以提高泛化性\n2. 如何利用功能对齐和对象-功能框架实现从示例到通用策略的迁移\n3. 如何在无关对象身份、视角和任务语义的前提下实现姿态感知的策略学习\n4. 如何通过对齐数据训练对象中心的扩散策略以增强实例级和类别级泛化能力\n\n【用了什么创新的方案】\n- 将长时程操作分解为由行为者-动作-对象组成的可重用动作原语，并在功能对齐的双对象框架内进行分段\n- 通过大视觉语言模型的 affordance 提供和功能 canonicalization 将相关对象映射到共享功能框架，实现自动轨迹迁移\n- 基于对齐数据训练对象中心的扩散策略 FuncDiffuser，使策略在对象姿态、可供性和交互动力学上具备姿态感知能力并实现良好跨域泛化\n- 结合多模态分段（MLLM/ LVM）与功能对齐，进行自动化的轨迹迁移和增强数据多样性\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation",
            "authors": "Zhennan Jiang,Kai Liu,Yuxin Qin,Shuai Tian,Yupeng Zheng,Mingcai Zhou,Chao Yu,Haoran Li,Dongbin Zhao",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19080",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19080",
            "arxiv_html_link": "https://arxiv.org/html/2509.19080v1",
            "abstract": "Robotic manipulation policies are commonly initialized through imitation learning, but their performance is limited by the scarcity and narrow coverage of expert data. Reinforcement learning can refine polices to alleviate this limitation, yet real-robot training is costly and unsafe, while training in simulators suffers from the sim-to-real gap. Recent advances in generative models have demonstrated remarkable capabilities in real-world simulation, with diffusion models in particular excelling at generation. This raises the question of how diffusion model-based world models can be combined to enhance pre-trained policies in robotic manipulation. In this work, we propose World4RL, a framework that employs diffusion-based world models as high-fidelity simulators to refine pre-trained policies entirely in imagined environments for robotic manipulation. Unlike prior works that primarily employ world models for planning, our framework enables direct end-to-end policy optimization. World4RL is designed around two principles: pre-training a diffusion world model that captures diverse dynamics on multi-task datasets and refining policies entirely within a frozen world model to avoid online real-world interactions. We further design a two-hot action encoding scheme tailored for robotic manipulation and adopt diffusion backbones to improve modeling fidelity. Extensive simulation and real-world experiments demonstrate that World4RL provides high-fidelity environment modeling and enables consistent policy refinement, yielding significantly higher success rates compared to imitation learning and other baselines. More visualization results are available at https://world4rl.github.io/.",
            "introduction": "Despite recent progress in robotic manipulation, the field still faces critical challenges for practical deployment. Imitation learning is widely used to bootstrap policies from demonstrations, but its effectiveness is constrained by the inconsistency[1] and limited diversity[2, 3, 4] of available datasets. Although offline reinforcement learning (RL) can extract better policies from imperfect data, its susceptibility to overestimation[5] still makes it difficult to work effectively with limited datasets. Online RL offers a natural way to refine such pre-trained policies through interaction. However, real-robot RL, while capable of overcoming dataset limitations, suffers from high interaction costs and significant safety risks that hinder large-scale training. Training in simulation avoids these risks but inevitably introduces discrepancies from real-world physics, leading to a persistent sim-to-real gap[6].\n\nIn recent years, generative models have achieved remarkable progress in the visual domain[7], with diffusion models[8] demonstrating particularly strong performance in image[9] and video generation[10, 11]. Such generative capacity opens new opportunities for modeling complex and dynamic environments, offering a promising path toward learnable world simulators that provide realistic yet flexible environments for RL training in robotic manipulation.\n\nBuilding on this idea, we introduce World4RL, a framework that systematically integrates diffusion world models into RL for robotic manipulation. World4RL follows a two-stage paradigm: we first pre-train a diffusion world model on multi-task datasets to capture diverse dynamics, and then refine policies entirely within the frozen model to avoid costly and unsafe online interactions. Serving as a high-fidelity simulator, the world model is composed of a diffusion transition model that predicts future observations conditioned on current observations and actions, and a reward classifier that provides sparse success signals, enabling policy optimization without real-world rollouts.\n\nThis design of framework contrasts with prior approaches such as IRASim[12] and NWM[13], which primarily use generative video models for planning at test time rather than for direct policy training. A closer line of work, DiWA[14], also employs world models for policy learning. However, it relies on recurrent state-space models (RSSM[15]), which lead to blurry generations and compounding rollout errors. In contrast, World4RL leverages diffusion backbones that generate sharper and temporally coherent rollouts, thereby supporting effective end-to-end reinforcement learning.\n\nTo further adapt world models to robotic manipulation, which involves high-dimensional action spaces and complex environment interactions compared to navigation[13] and games[16], we investigate two critical design choices: a two-hot action encoding[17] scheme that provides an efficient representation of continuous actions while enabling lossless reconstruction, thereby serving as a robust bridge between the RL agent and the world model, and diffusion backbone architectures that determine the fidelity and consistency of predictions. These considerations are essential for enabling diffusion world models to serve not only as visual predictors but also as reliable simulators for policy training. To this end, our work makes the following key contributions.\n\nWe propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\nWe propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\nTo improve modeling fidelity and enable more effective policy refinement, we design a two-hot action encoding tailored for robotic manipulation and adopt a diffusion backbone as the world model.\n\nWe validate the effectiveness of World4RL through extensive experiments, showing that it consistently outperforms competitive baselines and significantly enhances policy refinement, improving success rates by 16% and 25% in simulation and real-robot experiments, respectively.\n\n1. We propose World4RL, a systematic framework that integrates diffusion world model into RL training for robotic manipulation.\n\n2. To improve modeling fidelity and enable more effective policy refinement, we design a two-hot action encoding tailored for robotic manipulation and adopt a diffusion backbone as the world model.\n\n3. We validate the effectiveness of World4RL through extensive experiments, showing that it consistently outperforms competitive baselines and significantly enhances policy refinement, improving success rates by 16% and 25% in simulation and real-robot experiments, respectively.",
            "llm_summary": "【关注的是什么问题】\n1. 如何利用扩散模型构建高保真世界模型以提升机器人操控策略的学习效率与安全性（≤40词）\n2. 如何在不在线真实交互的前提下，通过在冻结的扩散世界模型中对策略进行端到端优化来实现改进（≤40词）\n3. 如何设计适合高维持续动作空间的两热编码以及扩散骨干网络以提升生成的时序一致性与预测保真度（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出 World4RL，将扩散式世界模型作为高保真仿真器用于策略的离线强化学习训练；先在多任务数据集上预训练扩散世界模型，再在冻结的模型中进行端到端策略优化；使用两热动作编码和扩散骨干来提升建模保真度与鲁棒性；通过扩散转移模型预测未来观测并结合奖励分类器提供稀疏成功信号，实现无真实滚动的策略 refinement。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions",
            "authors": "Laura Connolly,Aravind S. Kumar,Kapi Ketan Mehta,Lidia Al-Zogbi,Peter Kazanzides,Parvin Mousavi,Gabor Fichtinger,Axel Krieger,Junichi Tokuda,Russell H. Taylor,Simon Leonard,Anton Deguet",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19076",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19076",
            "arxiv_html_link": "https://arxiv.org/html/2509.19076v1",
            "abstract": "Image-guided robotic interventions involve the use of medical imaging in tandem with robotics. SlicerROS2 is a software module that combines 3D Slicer and robot operating system (ROS) in pursuit of a standard integration approach for medical robotics research. The first release of SlicerROS2 demonstrated the feasibility of using the C++ API from 3D Slicer and ROS to load and visualize robots in real time. Since this initial release, we’ve rewritten and redesigned the module to offer greater modularity, access to low-level features, access to 3D Slicer’s Python API, and better data transfer protocols. In this paper, we introduce this new design as well as four applications that leverage the core functionalities of SlicerROS2 in realistic image-guided robotics scenarios.",
            "introduction": "Medical robotics is an evolving and rapidly growing research field with the potential to transform standard clinical practice. It is possible that robots will one day transcend human capabilities while offering higher efficiency, lower costs, improved training outcomes and better safety [1]. The advancement of image-guided robotics in particular, which are systems that rely on both medical imaging and robotics, is critical for achieving this potential. This is because image-guided robots can be used to fuse preoperative and intraoperative realities [2].\n\nThere are several combinations of imaging modalities and robotic systems have been explored in this capacity. For example, the SpineBot uses computed tomography (CT) imaging to help define the trajectory of pedicle screws, and robotics to guide the surgeon through those trajectories [3], [4]. Another example is the MrBot, which was designed to help perform percutaneous needle interventions within the confines of a magnetic resonance imaging (MRI) scanner [5]. Similarly, the Artemis robot was designed to facilitate transrectal prostate biopsy under ultrasound guidance with MRI fusion [6]. These are just a few examples of the numerous procedures and therapies where the use of image-guidance in tandem with robotics has been investigated. More recently, advancements in image-guided robotics have enabled: navigation of catheters into blood vessels with magnetic continuum devices [7], autonomous needle steering for lung biopsy [8] and teleoperated neurovascular interventions [9].\n\nDespite this extensive investigation, there are only a few areas of intervention where image-guided robotic systems that have achieved widespread adoption and financial commercial success such as robotic bronchoscopy, radiation oncology and neurosurgery [2] [10]. One contributor to this slow growth and adoption is the lack of a common integration approach. For any image-guided robotic system, integration of the imaging modality and the robot is the most important factor for usability. However, several companies and research systems take their own unique approach to integration. This results in device-specific software, expensive research licenses, incompatible communication protocols, and overall, a high barrier to entry to develop such systems. Considering these challenges and their potential threat to continued development, it is imperative to provide a common integration scheme for image-guided robotics. We hypothesize that this will prevent re-engineering and promote reproducibility across different clinical applications.\n\nFrom a development perspective, image-guided therapy (IGT) platforms and medical robotics platforms are often separated. In the realm of IGT, the open-source medical imaging platform, 3D Slicer is the most commonly used research platform [11] [12]. With over one million downloads and an active research and support community, 3D Slicer is used for segmentation, virtual reality, image analysis, artificial intelligence, and several other applications [13]. Several research platforms have been enabled by or added to 3D Slicer such as SlicerIGT [12], SlicerVR [14], MONAI label [15], and Total Segmentator [16]. There are also open-source platforms like OpenIGTLink and the PLUS toolkit that allow users to interface commercial hardware with 3D Slicer to build complete IGT systems [17] [18]. As a result of these efforts, 3D Slicer is considered the de-facto open source software for developing navigated, image-guided interventions.\n\nIn the field of medical robotics, robot operating system (ROS), an open-source middleware designed to support robotics development, is the predominant framework. ROS is a modular development framework that provides tools for autonomous navigation, simulation, visualization, and control [19]. Like 3D Slicer, ROS has a very active research community that is constantly contributing to the platform. For medical robotics specifically, several research tools like the Computer Integrated Surgical Systems Surgical Assistant Workstation (CISST-SAW) libraries [20] and the Asynchronous Multi-Body Framework (AMBF) [21] support ROS. The da Vinci Research Kit (dVRK), a popular open-source medical robot [22], also supports ROS for software development. Furthermore, many commercially available robots provide a ROS interface off the shelf.\n\nIn an effort to pursue a common integration scheme for image-guided robotics research, we decided to bridge these two ecosystems. As evidenced by the numerous published papers that employ both 3D Slicer and ROS (71 papers available on Google Scholar using keywords “3D Slicer” AND “robot operating system”), there is also demand from the community for this integration. Previous attempts to bridge 3D Slicer and ROS such as the ROS-IGTL bridge [23], custom applications for specific robots [24] [25], and our initial offering of SlicerROS2 [26] have fallen short of meeting all of the needs of a common integration tool. These needs include greater usability by providing access to low-level features, robust data transfer protocols that support commonly used message types, thorough documentation, and maintainability [27]. We have since redesigned SlicerROS2 to further support image-guided robotics research considering these requirements. The details of this new design are described in the following sections. The contributions of this paper are: 1) A newly designed research module for efficient data transfer between 3D Slicer and ROS 2 and 2) four relevant applications that demonstrate how it can be used for rapid research prototyping.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在图像引导的机器人干预中实现 3D Slicer 与 ROS2 的高效、通用集成\n2. 提高跨平台数据传输的鲁棒性与对低级功能的可访问性\n3. 快速原型化研究所需的模块化、可维护的工具体系\n4. 在医学机器人研究中促进可重复性与协同开发的标准化方案\n【用了什么创新的方案】\n核心解决方案：重新设计的 SlicerROS2 模块实现更高模块化和低级功能访问，整合 3D Slicer 的 Python 与 C++ API，改进数据传输协议，支撑多种消息类型，并提供四个实际应用场景以展示核心功能在图像引导机器人中的应用与原型化能力\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation",
            "authors": "Geonhyup Lee,Yeongjin Lee,Kangmin Kim,Seongju Lee,Sangjun Noh,Seunghyeok Back,Kyoobin Lee",
            "subjects": "Robotics (cs.RO)",
            "comment": "9 pages, 9 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.19047",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19047",
            "arxiv_html_link": "https://arxiv.org/html/2509.19047v1",
            "abstract": "Contact-rich manipulation tasks such as precision assembly require precise control of interaction forces, yet existing imitation learning methods rely mainly on vision-only demonstrations. We propose ManipForce, a handheld system designed to capture high-frequency force–torque (F/T) and RGB data during natural human demonstrations for contact-rich manipulation. Building on these demonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT). FMT encodes asynchronous RGB and F/T signals using frequency- and modality-aware embeddings and fuses them via bi-directional cross-attention within a transformer diffusion policy. Through extensive experiments on six real-world contact-rich manipulation tasks—such as gear assembly, box flipping, and battery insertion—FMT trained on ManipForce demonstrations achieves robust performance with an average success rate of 83% across all tasks, substantially outperforming RGB-only baselines. Ablation and sampling-frequency analyses further confirm that incorporating high-frequency F/T data and cross-modal integration improves policy performance, especially in tasks demanding high precision and stable contact.\nHardware, software, and video demos are available at: https://sites.google.com/view/manipforce/홈.",
            "introduction": "Contact-rich manipulation tasks such as precise assembly [1, 2, 3, 4], battery disassembly [5], and non-prehensile handling [6] require high precision and force-aware manipulation. Humans naturally perceive contact forces and their subtle changes when assembling parts, adjusting their strategies accordingly. Yet most robotic approaches rely solely on visual demonstrations, missing the rich F/T information humans provide.\n\nRecent advances in imitation learning [7, 8, 9] have demonstrated strong potential for dexterous and contact-rich manipulation by learning directly from human demonstrations.\nHowever, these methods still rely on high-quality demonstration data, which is costly and difficult to collect for fine-grained interactions.\nHand-held data collection systems such as UMI [10] have been proposed to address this challenge by enabling natural human demonstrations without the expertise requirements and remote-control limitations of teleoperation.\nWhile effective for simplifying demonstration collection, UMI does not capture force–torque (F/T) information, which is essential for accurately modeling contact behaviors.\nMore recent work [11] combines visual and F/T data but relies on point clouds to represent the scene, which introduces complex setup requirements and fundamentally limits the ability to perceive small objects and fine clearances essential for contact-rich manipulation.\nFurthermore, from a learning perspective, this approach down-samples high-frequency F/T signals to match the image frame rate, losing rich temporal information necessary for modeling contact dynamics.\n\nTo address these limitations, we introduce ManipForce a handheld system for simultaneous RGB–F/T data collection during natural human demonstrations, and the Frequency-Aware Multimodal Transformer (FMT), which learns robust policies from the collected data for diverse, precise, and contact-rich manipulation tasks.\n\nManipForce consists of a dual handheld camera setup with a wrist-mounted F/T sensor to capture both visual and high-frequency force signals during human-guided demonstrations.\nThis configuration enables robust perception of small objects, tight clearances, and fine-grained contacts, allowing collected demonstrations to transfer directly to robotic execution.\nWe replace SLAM-based wrist tracking with 3D ArUco marker pose estimation to maintain accuracy during close-contact interactions without environmental dependencies, and apply tool gravity compensation to ensure precise and interaction-focused F/T measurements.\nWe propose the FMT, which learns from asynchronous RGB (30 Hz) and F/T (>200 Hz) signals using a Transformer-based Diffusion Policy [7] architecture.\nTo exploit the higher-frequency force signals relative to images, the model tokenizes both RGB and F/T inputs using learnable frequency and modality embeddings.\nThis design enables the model to effectively handle heterogeneous modalities with asynchronous sampling rates.\nIn addition, bi-directional cross-attention modules fuse complementary information across modalities.\nWe evaluate our approach on six contact-rich manipulation tasks spanning precision assembly, non-prehensile manipulation, and complex disassembly, and observe significant performance gains over RGB-only baselines.\nAblation studies further confirm that high-frequency F/T sensing, unified positional embeddings, and bi-directional cross-attention each make complementary contributions to robust multimodal policy learning.\n\nOur main contributions are:\n\nWe introduce ManipForce, a handheld RGB–F/T data collection system enabling diverse and fine-grained contact-rich manipulation demonstrations.\n\nWe propose FMT, which handles inputs with asynchronous sampling rates through frequency-aware multimodal representation learning and cross-attention within a Transformer architecture, enabling robust policy learning for contact-rich manipulation.\n\nWe demonstrate robust performance on diverse contact-rich manipulation tasks—including gear assembly, plug insertion, battery disassembly, and lid operations—consistently outperforming RGB-only baselines.\n\n1. We introduce ManipForce, a handheld RGB–F/T data collection system enabling diverse and fine-grained contact-rich manipulation demonstrations.\n\n2. We propose FMT, which handles inputs with asynchronous sampling rates through frequency-aware multimodal representation learning and cross-attention within a Transformer architecture, enabling robust policy learning for contact-rich manipulation.\n\n3. We demonstrate robust performance on diverse contact-rich manipulation tasks—including gear assembly, plug insertion, battery disassembly, and lid operations—consistently outperforming RGB-only baselines.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在接触丰富的操纵任务中同时利用高频力矩信息与RGB视觉实现鲁棒仿生策略\n2. 如何在异步采样率下融合多模态数据以提升策略学习的准确性与稳定性\n3. 如何减少对复杂场景感知的依赖并实现自然人类演示的高效数据收集\n\n【用了什么创新的方案】\nManipForce 提供可手持的 RGB–F/T 数据采集系统，结合高频力矩与 RGB 数据；FMT 使用频率感知的多模态表示对异步 RGB(30 Hz) 与 F/T(>200 Hz) 信号进行编码，采用双向交叉注意力在 Transformer diffusion 策略中进行跨模态融合，显著提升对接触力与微小几何的建模能力；并通过 3D ArUco 标定与工具重力补偿实现近距离接触的高精度演示对齐与采样。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors",
            "authors": "Qingzheng Cong,Steven Oh,Wen Fan,Shan Luo,Kaspar Althoefer,Dandan Zhang",
            "subjects": "Robotics (cs.RO)",
            "comment": "14 pages, 8 figures. Equal contribution: Qingzheng Cong, Steven Oh, Wen Fan. Corresponding author: Dandan Zhang (d.zhang17@imperial.this http URL). Additional resources atthis http URL",
            "pdf_link": "https://arxiv.org/pdf/2509.19037",
            "code": "http://stevenoh2003.github.io/TacEva/, http://ac.uk",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19037",
            "arxiv_html_link": "https://arxiv.org/html/2509.19037v1",
            "abstract": "Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because of the high spatial resolution they offer and their relatively low manufacturing costs. However, variations in their sensing mechanisms, structural dimension, and other parameters lead to significant performance disparities between existing VBTSs. This makes it challenging to optimize them for specific tasks, as both the initial choice and subsequent fine-tuning are hindered by the lack of standardized metrics. To address this issue, TacEva is introduced as a comprehensive evaluation framework for the quantitative analysis of VBTS performance. The framework defines a set of performance metrics that capture key characteristics in typical application scenarios. For each metric, a structured experimental pipeline is designed to ensure consistent and repeatable quantification. The framework is applied to multiple VBTSs with distinct sensing mechanisms, and the results demonstrate its ability to provide a thorough evaluation of each design and quantitative indicators for each performance dimension. This enables researchers to pre-select the most appropriate VBTS on a task by task basis, while also offering performance-guided insights into the optimization of VBTS design. A list of existing VBTS evaluation methods and additional evaluations can be found on our website:\nhttps://stevenoh2003.github.io/TacEva/.",
            "introduction": "Robots have yet to attain the level of manipulative dexterity exhibited by humans, a challenge rooted in the difficulty of accurately acquiring detailed contact information in physical environments [1]. Tactile sensing has therefore become indispensable for delicate and precise robotic manipulation in embodied intelligence systems [2]. A notable development in this domain has been the rise of vision-based tactile sensors (VBTSs) [3]. These sensors employ high-resolution cameras to capture detailed contact surface information, thereby integrating seamlessly with computer vision and image-based deep learning methods. Nevertheless, among VBTSs, we see a wide variety of architectures, structural dimensions, and fabrication techniques, depending on the exact nature of the application requirements [4].\n\nThe rapid development of VBTSs has created an urgent need for standardized performance evaluation. Selecting an appropriate VBTS for a specific task scenario remains challenging, as distinct sensor designs offer substantial variation in performance. A universal evaluation protocol would therefore facilitate fair comparison across sensor designs, support informed selection, and guide design optimization. However, a significant gap persists in the field: no standardized framework currently exists for VBTS evaluation, and the inconsistency of current metrics limits objective, reproducible, and comprehensive cross-sensor assessment. This challenge is further compounded by the inherently multi-modal nature of tactile sensing [5], which necessitates coordinated evaluation across multiple performance dimensions.\n\nThe application scenarios for VBTSs are inherently diverse. This makes it difficult to establish universal, broadly applicable evaluation metrics that remain meaningful across the field. Further complexity arises from the fact that VBTSs are commonly fabricated using silicone elastomers, introducing additional considerations related to soft-material mechanics, optics, and imaging. Owing to differences in their underlying sensing mechanisms, each VBTS design inevitably exhibits its own profile of strengths and weaknesses. As a result, certain positive performance characteristics will typically tend to be emphasized, while weaker ones may be under-reported. This is likely to complicate informed selection, making it more challenging for prospective users.\n\nTo address these challenges, we propose TacEva, a systematic evaluation framework that integrates performance quantification with a structured and reproducible assessment pipeline. TacEva is designed to provide consistency and comparability across VBTS designs, enabling practitioners to make evidence-based decisions while obtaining a holistic understanding of sensor performance (see Fig. 1). It also offers sensor developers clear guidance for targeted optimization during the design process. By defining and standardizing a comprehensive set of performance metrics and evaluation protocols, TacEva aims to provide a unified and transparent characterization of VBTS designs, thereby facilitating objective comparison, reliable selection, and more informed innovation for future sensor development.",
            "llm_summary": "【关注的是什么问题】\n1. 缺乏标准化评估框架导致VBTSs在不同设计间难以公平比较与任务级优化（≤40词）\n2. 多模态触觉传感的评估指标缺乏统一、可重复的量化流程（≤40词）\n3. 不同VBTS sensing机制、结构与材料差异造成性能特征分布不一致，难以选型与设计优化（≤40词）\n4. 缺少可跨传感器的全面综合性能指标与任务驱动的性能指示（≤40词）\n\n【用了什么创新的方案】\nTacEva 提供一个综合的性能评估框架，定义一套标准化、可重复的评测指标和实验流水线，覆盖典型应用场景的关键性能维度，并可对多种VBTS 进行横向比较与任务驱动的选型与优化指导。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion",
            "authors": "Shuai Liu,Meng Cheng Lau",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "11 pages, 5 figures, 1 table, Computational Science Graduate Project",
            "pdf_link": "https://arxiv.org/pdf/2509.19023",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19023",
            "arxiv_html_link": "https://arxiv.org/html/2509.19023v1",
            "abstract": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a two-stage reinforcement learning framework for humanoid walking that requires no motion capture data or elaborate reward shaping. In the first stage, a compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via Proximal Policy Optimization. This generates energy-efficient gait templates. In the second stage, those dynamically consistent trajectories guide a full-body policy trained with Soft Actor–Critic augmented by an adversarial discriminator, ensuring the student’s five-dimensional gait feature distribution matches the ROM’s demonstrations. Experiments at 1 m/s and 4 m/s show that ROM-GRL produces stable, symmetric gaits with substantially lower tracking error than a pure-reward baseline. By distilling lightweight ROM guidance into high-dimensional policies, ROM-GRL bridges the gap between reward-only and imitation-based locomotion methods, enabling versatile, naturalistic humanoid behaviors without any human demonstrations.",
            "introduction": "Achieving natural humanoid locomotion is a longstanding goal in both robotics and computer animation. From bipedal robots that can walk and run with human-like grace, to virtual characters that move realistically in games, the ability to synthesize lifelike walking gaits remains a persistent challenge. Traditional model-based control has produced impressive feats, but often requires painstaking design and does not always capture the natural motion nuances of human walking Kuindersma et al. (2016). In recent years, reinforcement learning (RL) has emerged as a promising data-driven paradigm for developing locomotion controllers Radosavovic et al. (2024). RL allows simulated humanoids to learn complex gait behaviors through trial-and-error, offering the potential to discover agile and robust walking strategies that would be difficult to manually design.\n\nReinforcement learning (RL) methods for natural locomotion generally fall into two main paradigms aimed at producing lifelike gait behaviors.\n\nA purely objective‑driven RL policy can discover stable walking gaits by optimizing energy and stability rewards. These reference-free methods train locomotion policies from scratch by optimizing carefully crafted reward functions, without any motion capture examples. The reward terms are designed to encourage physically plausible and human-like traits, such as forward speed with energy efficiency, maintaining center-of-mass stability and upright posture, periodic foot contact patterns, and symmetric gait cycles between left and right legs. By rewarding such objectives, controllers can spontaneously develop stable walking gaits that emerge naturally from learning. Notably, researchers have demonstrated that symmetric bipedal walking can arise solely from reward design and curriculum training, without any reference motions Yu et al. (2018). Indeed, recent work showed that an RL policy trained with biomechanically inspired rewards could produce natural walking behaviors purely through self-exploration Peng et al. (2025). The appeal of this approach is that it does not require any pre-recorded motions – the agent invents its own walking cycle. However, designing the reward function is notoriously difficult: the policy’s behavior is highly sensitive to the choice and weighting of reward terms, often requiring elaborate hand-tuning and expertise. Even with careful tuning, purely objective-driven policies may develop subtle artifacts or unnatural quirks since there is no direct template of “human” motion to imitate. In summary, while motion-free RL can yield impressively natural gaits under the right conditions, it faces challenges in reward engineering and consistency of motion style.\n\nAlternatively, imitation‑driven RL uses human mocap examples to ensure motion realism but at the cost of dataset dependency. To directly ensure natural movement quality, a dominant approach is to imitate example locomotion trajectories from motion capture data. In this paradigm, the RL agent is guided by reference motions of humans (or animals) and receives rewards for matching the reference pose and velocity at each time step, or uses adversarial critics to judge realism against a motion dataset Peng et al. (2018, 2021). DeepMimic pioneered this line of work by showing that standard RL algorithms can learn robust control policies capable of imitating a broad range of example motion clips from a motion capture library Peng et al. (2018). By combining motion imitation objectives with task goals, DeepMimic enabled physically simulated characters to reproduce dynamic skills (flips, spins, walks) with high fidelity to the mocap examples. Following this, numerous studies have pushed the state of the art in imitation-based locomotion. For instance, ASE (Adversarial Skill Embeddings) uses large unstructured motion datasets to train latent skill models via adversarial imitation, yielding a repertoire of reusable behaviors that look remarkably life-like Peng et al. (2022). Another example is CALM (Conditional Adversarial Latent Models), which learns a rich latent representation of human movement through imitation learning, capturing the complexity and diversity of human motion while allowing direct user control over the character’s style and direction Tessler et al. (2023). Most recently, the concept of Behavioral Foundation Models have emerged – here a policy is pre-trained on massive collections of motion data (in an unsupervised manner) to serve as a generalist locomotion model. These foundation models, once trained on unlabeled motion trajectories, can be prompted to perform new tasks in a zero-shot fashion, while retaining human-like gait qualities Tirinzoni et al. (2025a). Motion imitation approaches thus achieve state-of-the-art realism in simulated walking; policies closely mimic human kinematics and can produce motions nearly indistinguishable from motion capture. The downside, however, is their heavy reliance on curated motion data – one must have access to large datasets of reference gaits, and the learned skills are inevitably tied to the distribution of motions in those datasets.\n\nDespite these advances, achieving natural, human‑like locomotion without any motion capture data remains an open challenge. Purely objective‑driven methods often fail to reproduce the fluidity and subtle timing of human gait, while imitation‑based approaches are fundamentally constrained by the availability and diversity of mocap archives.\n\nTo bridge this gap, we introduce a reduced‑order model–guided reinforcement learning(ROM-GRL) framework. It leverages a simplified locomotion model as a stand‑in for motion capture, providing high‑level gait guidance to the RL agent without any human demonstrations. Our framework unfolds in two stages. In the first stage, we train a lightweight teacher model to generate efficient, dynamically consistent gait templates that capture the essence of natural walking. In the second stage, we distill these templates into a full-body controller by rewarding adherence to the teacher’s motion distribution, ensuring smooth, human-like locomotion. By separating high-level gait planning from detailed control, we achieve a single walking policy that is both robust and naturally fluid, all without relying on motion capture data or intricate reward design.\n\nIn summary, our approach marries the insights from model-based gait generation with the flexibility of reinforcement learning. By using a ROM to guide RL instead of direct motion capture, we maintain a purely physics-driven training regime while still inducing realistic movement patterns. The proposed framework demonstrates that natural humanoid locomotion can emerge without demonstrations, closing the gap between reward engineering and motion imitation. We validate that our ROM-guided RL method produces walking controllers that are stable across different speeds and exhibit natural gait symmetry and fluidity comparable to reward-based policies, suggesting a novel solution to produce life-like humanoid locomotion in the absence of motion data.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在没有运动捕捉数据的情况下实现自然、稳定的人形机器人行走\n2. 如何在强化学习中兼顾高层步态规划与全身控制的协同学习\n3. 如何避免繁琐的奖励设计而仍获得自然的步态与对称性\n4. 如何将简化的ROM释放为对高维策略的有效引导\n\n【用了什么创新的方案】\n两阶段ROM-GRL：先用4-DOF简化模型通过PPO生成高效、动态一致的步态模板；再用这些模板对全身策略进行蒸馏，使策略通过对ROM分布的遵循实现自然、对称的行走，并结合带对抗判别的Soft Actor–Critic以匹配ROM示范分布，且无任何人类示范数据。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
            "authors": "Dapeng Zhang,Jin Sun,Chenghui Hu,Xiaoyan Wu,Zhenlong Yuan,Rui Zhou,Fei Shen,Qingguo Zhou",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19012",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19012",
            "arxiv_html_link": "https://arxiv.org/html/2509.19012v1",
            "abstract": "The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.",
            "introduction": "Robotics has long been a prominent area of scientific research. Historically, robots primarily relied on pre-programmed instructions and engineered control policies to perform task decomposition and execution. These methods were commonly applied to simple, repetitive tasks, such as factory assembly lines and logistics sorting. In recent years, the rapid advancement of artificial intelligence has enabled researchers to exploit the feature extraction and trajectory prediction capabilities of deep learning across diverse modalities, including images, text, and point clouds. By integrating techniques such as perception, detection, tracking, and localization, researchers have decomposed robotic tasks into multiple stages to meet execution requirements, thereby advancing the development of embodied intelligence and autonomous driving. However, most of these robots still operate as isolated agents, designed for specific tasks and lacking effective interaction with humans and external environment.\n\nTo address these limitations, researchers have begun exploring the incorporation of large language models (LLMs) and vision language models (VLMs) to enable more accurate and flexible robotic manipulation. Modern robotic manipulation methods [1, 2] typically leverage vision language generative paradigms (e.g., autoregressive models [3, 4, 5, 6] or diffusion models [7]), combined with large-scale datasets [8] and advanced fine-tuning strategies. We refer to these as VLA foundation models, which have substantially improved the quality of robotic manipulations. Fine-grained action control over generated content provides users with greater flexibility, unlocking the practical potential of VLA for task execution.\n\nDespite their promise, reviews of pure VLA methods remain scarce. Existing surveys either focus on taxonomy over VLM foundational models or provide broad overviews of robotic manipulation as a whole. Firstly, VLA methods represent a nascent field in robotics, with no established methodological landscape or consensus taxonomy, making it challenging to systematically summarize these approaches. Secondly, current reviews either classify VLA approaches based on differences in foundational models or present a comprehensive analysis of robotic applications across the entire history of the field, often emphasizing traditional methods at the expense of emerging techniques. While these reviews offer valuable insights, they provide only cursory examinations of robotic models or concentrate primarily on foundational models, leaving a significant gap in the literature regarding pure VLA methods.\n\nIn this paper, we investigate VLA methods and associated resources, providing a focused and comprehensive review of existing approaches. Our goal is to present a clear taxonomy, systematically summarize VLA research, and elucidate the development trajectory of this rapidly evolving field. After a brief overview of LLMs and VLMs, we focus on the policy strategies of VLA models, highlighting the unique contributions and distinctive features of previous studies. We classify VLA approaches into 4 categories: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods, and provide a detailed analysis of their motivations, core strategies, and mechanisms. As shown in Fig. 2, we present a VLA skeleton of these methods.\nWe examine application domains, including robotic arms, quadruped robots, humanoids, and wheeled robots (autonomous vehicles), offering a comprehensive assessment of VLA deployment across diverse scenarios. Given the strong dependence of VLA models on datasets and simulation platforms, we provide a concise overview of these resources. Finally, based on the current VLA landscape, we identify key challenges and outline future research directions—including data limitations, inference speed, and safety—to accelerate the advancement of VLA models and generalizable robotics.\n\nThe overall structure of this survey is illustrated in Fig. 1. First, Section 2 provides an overview of the background for VLA research. Section 3 presents the existing VLA approaches in the robotics field. Section 4 introduces the datasets and benchmarks employed by VLA approaches. Sections 5 and 6 discuss simulation platforms and robotic hardware. Section 7 further discusses the challenges and future directions for VLA-based robotic methods. Finally, we summarize the paper and provide our perspective on future developments.\n\nIn summary, our contributions are as follows:\n\nWe present the well-structured taxonomy of pure VLA methods, classifying approaches based on their action-generation strategies. This facilitates understanding of existing methods and highlights core challenges in the field.\n\nThe survey emphasizes the defining characteristics and methodological innovations of each category and technique, providing a clear perspective on current approaches.\n\nWe provide a comprehensive overview of associated resources (datasets, benchmarks and simulation platforms) for training and evaluating VLA models.\n\nWe investigate the practical impact of VLA in robotics, identify key limitations of existing techniques, and propose potential avenues for further exploration.\n\n1. We present the well-structured taxonomy of pure VLA methods, classifying approaches based on their action-generation strategies. This facilitates understanding of existing methods and highlights core challenges in the field.\n\n2. The survey emphasizes the defining characteristics and methodological innovations of each category and technique, providing a clear perspective on current approaches.\n\n3. We provide a comprehensive overview of associated resources (datasets, benchmarks and simulation platforms) for training and evaluating VLA models.\n\n4. We investigate the practical impact of VLA in robotics, identify key limitations of existing techniques, and propose potential avenues for further exploration.",
            "llm_summary": "【关注的是什么问题】\n1. 如何对纯VLA方法进行清晰分类与系统综述，以揭示其核心动机与实现机制？（≤40词）\n2. VLA模型在机器人执行中的生成策略、数据/基准资源、仿真平台与应用域的整合与挑战是什么？（≤40词）\n3. 现有VLA方法的局限性、安全性与可扩展性如何影响通用机器人能力的提升？（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出基于动作生成策略的纯VLA方法分型（自回归、扩散、强化学习、混合、专用），并对各类方法的动机、核心策略与实现机制进行系统比较；梳理数据集、基准与仿真平台；并给出未来方向与挑战。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Category-Level Object Shape and Pose Estimation in Less Than a Millisecond",
            "authors": "Lorenzo Shaikewitz,Tim Nguyen,Luca Carlone",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18979",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18979",
            "arxiv_html_link": "https://arxiv.org/html/2509.18979v1",
            "abstract": "Object shape and pose estimation is a foundational robotics problem, supporting tasks from manipulation to scene understanding and navigation. We present a fast local solver for shape and pose estimation which requires only category-level object priors and admits an efficient certificate of global optimality. Given an RGB-D image of an object, we use a learned front-end to detect sparse, category-level semantic keypoints on the target object. We represent the target object’s unknown shape using a linear active shape model and pose a maximum a posteriori optimization problem to solve for position, orientation, and shape simultaneously. Expressed in unit quaternions, this problem admits first-order optimality conditions in the form of an eigenvalue problem with eigenvector nonlinearities. Our primary contribution is to solve this problem efficiently with self-consistent field iteration, which only requires computing a 4×44\\times 4 matrix and finding its minimum eigenvalue-vector pair at each iterate. Solving a linear system for the corresponding Lagrange multipliers gives a simple global optimality certificate. One iteration of our solver runs in about 100 microseconds, enabling fast outlier rejection. We test our method on synthetic data and a variety of real-world settings, including two public datasets and a drone tracking scenario. Code is released at https://github.com/MIT-SPARK/Fast-ShapeAndPose.",
            "introduction": "A diverse set of robotics applications benefits from object shape and pose estimation. Autonomous cars, for example, need to locate obstacles and other cars [1], while household manipulators need to locate objects to interact with [2].\nIn many of these applications the object shape is not known exactly but its category is available (e.g., from a semantic segmentation method). We consider this setting and derive a shape and pose estimator using category-level priors.\n\nThe work of Shi et al. [3] established a certifiably optimal approach for category-level shape and pose estimation using a semidefinite relaxation. We consider a similar problem setup but emphasize both speed and certifiability. A fast estimator allows fast reaction to new inputs, performance with limited compute, and comprehensive outlier rejection [4, 5]. Certifiability provides an a posteriori guarantee that the estimate returned is statistically optimal. When the certificate fails, the user can decide to trust the output, try a different initialization, or acquire a new batch of measurements.\n\nOur algorithm relies on the eigenvalue structure of the first-order optimality conditions written in the quaternion representation of rotations. It returns local solutions which are often globally optimal. To verify this, we introduce a fast global optimality certifier based on Lagrangian duality. Specifically, our contributions are:\n\nA fast local solver for category-level shape and pose estimation\nusing self-consistent field iteration [6].\n\nA fast a posteriori certificate of global optimality for our local solutions.\n\nExperimental evaluation of runtime and accuracy on synthetic data, a drone tracking scenario, and two large-scale datasets.\n\nThe remainder of the paper is organized as follows. We begin with a literature review (Section II) and quaternion preliminaries (Section III). Then, we give the problem formulation in Section IV and reformulate it with quaternions in Section V.\nTo solve the nonlinear eigenproblem, we use self-consistent field iteration for local solutions and SDP optimality conditions to certify global optimality in Section VI. In Section VII, we show our method is significantly faster than other local solvers and learned baselines.\n\n1. A fast local solver for category-level shape and pose estimation\nusing self-consistent field iteration [6].\n\n2. A fast a posteriori certificate of global optimality for our local solutions.\n\n3. Experimental evaluation of runtime and accuracy on synthetic data, a drone tracking scenario, and two large-scale datasets.",
            "llm_summary": "【关注的是什么问题】\n1. 在低计算资源下，基于类别级先验进行对象形状与姿态的快速估计\n2. 给定RGB-D，如何通过稀疏语义关键点实现高效、可证伪的全局最优性口径\n3. 如何在四元数表示下将非线性优化转化为可迭代求解并给出全局最优性证据\n\n【用了什么创新的方案】\n核心解决方案：提出基于自洽场迭代的快速局部求解器，用线性主动形状模型表示未知形状，构造最大后验优化以同时估计位置、姿态和形状；在四元数表示下，将一阶最优性条件转化为特征值问题，通过迭代求解一个4×44×4的矩阵并获取最小特征值-向量对；通过拉格朗日乘子求解线性系统，提供快速的全局最优性证书（可后验判断是否可信）。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation",
            "authors": "Minoo Dolatabadi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18954",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18954",
            "arxiv_html_link": "https://arxiv.org/html/2509.18954v1",
            "abstract": "LiDAR-based localization and SLAM often rely on iterative matching algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align sensor data with pre-existing maps or previous scans. However, ICP is prone to errors in featureless environments and dynamic scenes, leading to inaccurate pose estimation. Accurately predicting the uncertainty associated with ICP is crucial for robust state estimation but remains challenging, as existing approaches often rely on handcrafted models or simplified assumptions. Moreover, a few deep learning-based methods for localizability estimation either depend on a pre-built map—which may not always be available—or provide a binary classification of localizable versus non-localizable, which fails to properly model uncertainty.\nIn this work, we propose a data-driven framework that leverages deep learning to estimate the registration error covariance of ICP before matching, even in the absence of a reference map. By associating each LiDAR scan with a reliable 6-DoF error covariance estimate, our method enables seamless integration of ICP within Kalman filtering, enhancing localization accuracy and robustness. Extensive experiments on the KITTI dataset demonstrate the effectiveness of our approach, showing that it accurately predicts covariance and, when applied to localization using a pre-built map or SLAM, reduces localization errors and improves robustness.",
            "introduction": "In recent years, autonomous vehicles have become an integral component of intelligent transportation systems, driving continuous research to push the boundaries of their capabilities. A key requirement for autonomous driving is achieving precise self-localization at the centimeter level [1]. The Global Navigation Satellite System (GNSS) is a cost-effective and widely used method for vehicle localization. Although GNSS delivers reliable positioning in open-sky environments, its accuracy is significantly compromised in urban settings due to factors such as signal blockage, non-line-of-sight (NLOS) conditions, and multipath effects [2]. To address these challenges, vision-based approaches—particularly those leveraging LiDAR—have been proposed as alternative or complementary solutions [3]. In these systems, map matching techniques like the Iterative Closest Point (ICP) algorithm are frequently employed to align sensor data with pre-existing maps, thereby enhancing localization precision [4].\n\nHowever, vision-based localization methods—like other relative localization techniques—are susceptible to error accumulation, where minor errors can progressively lead to significant drift over time. This phenomenon is observed in both Simultaneous Localization and Mapping (SLAM) and pre-built map-based localization approaches, although it tends to be more pronounced in SLAM [5]. In the context of ICP, previous work has demonstrated that factors such as featureless environments (e.g., tunnels) and the presence of dynamic objects can adversely affect the matching accuracy [6].\n\nState estimation methods, such as Kalman filtering, are commonly employed to mitigate these problems. Yet, these techniques depend on an accurate error model (often represented in the simplest form by an error covariance matrix), which is challenging to determine for matching algorithms\n[7].\n\nTo address this limitation, several studies have proposed data-driven approaches to predict ICP error, either using classification-based methods [8] or by directly estimating the covariance [7].\n\nSpecifically, we introduce a data-driven framework that predicts the full six-degree-of-freedom (6-DoF) ICP registration covariance from a single LiDAR scan prior to correspondence search and ICP refinement. The output is a symmetric positive-definite (SPD) 6×66\\times 6 covariance on S​E​(3)SE(3) suitable for probabilistic fusion. An overview of the pipeline is shown in Fig. 1.\n\nContributions.\n\nPre-ICP, per-scan S​E​(3)SE(3) covariance. We predict the full 6×66\\times 6 registration covariance directly from a single LiDAR scan, capturing translation–rotation coupling before ICP is run. Per-scan training targets are obtained by estimating empirical covariances from Monte Carlo ICP under randomized initializations.\n\nKalman fusion and evaluation. Each scan is paired with a reliable 6-DoF covariance, used as the measurement noise in a standard Kalman filter. This improves pose accuracy over fixed or heuristic covariances on the evaluated sequences.\n\nPracticality. The model does not require a pre-built map at inference, supporting both SLAM and map-based operation.\n\n1. Pre-ICP, per-scan S​E​(3)SE(3) covariance. We predict the full 6×66\\times 6 registration covariance directly from a single LiDAR scan, capturing translation–rotation coupling before ICP is run. Per-scan training targets are obtained by estimating empirical covariances from Monte Carlo ICP under randomized initializations.\n\n2. Kalman fusion and evaluation. Each scan is paired with a reliable 6-DoF covariance, used as the measurement noise in a standard Kalman filter. This improves pose accuracy over fixed or heuristic covariances on the evaluated sequences.\n\n3. Practicality. The model does not require a pre-built map at inference, supporting both SLAM and map-based operation.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在 ICP 匹配前直接估计六自由度（6-DoF）注册协方差以提升 LiDAR 基于定位的鲁棒性\n2. 缺少地图时如何进行无地图依赖的协方差预测，并实现与卡尔曼滤波的无缝融合\n3. 面对 featureless 或动态环境，如何更准确地量化 ICP 的不确定性并减少定位误差\n4. 将深度学习预测的协方差用于先验状态估计，从而提升 SLAM/基于地图的定位的鲁棒性\n\n【用了什么创新的方案】\nPre-ICP 直接从单帧 LiDAR 预测对称正定的 6×6 协方差矩阵，捕捉平移-旋转耦合；以蒙特卡洛 ICP 在随机初始值下得到的经验协方差作为训练目标。将该协方差作为卡尔曼滤波的测量噪声，实现 ICP 前后的一体化鲁棒定位；方法在推断阶段不依赖预构建地图，支持 SLAM 与地图匹配场景。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations",
            "authors": "Hanqing Liu,Jiahuan Long,Junqi Wu,Jiacheng Hou,Huili Tang,Tingsong Jiang,Weien Zhou,Wen Yao",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18953",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18953",
            "arxiv_html_link": "https://arxiv.org/html/2509.18953v1",
            "abstract": "Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60%, with object transformations causing up to 97.8% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.",
            "introduction": "Vision-Language-Action (VLA) models represent a paradigm shift in robotic manipulation, integrating visual perception, language understanding, and action generation into unified end-to-end systems [1]. Recent deployments across manufacturing [2], healthcare [3], and service robotics [4, 5] demonstrate their transformative potential. However, in real-world deployments, VLA models inevitably face challenging physical variations, such as spatial transformations, illumination variations, and visual disruptions, which can dramatically alter robot behavior without being immediately detectable, posing significant safety risks. Therefore, it is crucial to investigate VLA robustness across various physical conditions systematically.\n\nExisting research has explored the robustness of VLA-based robotic systems through approaches like adversarial patches [6], which generate localized perturbations via gradient-based white-box attacks to achieve visual interference. However, these methods suffer from critical limitations: they violate physical plausibility constraints and fail to capture the rich spectrum of real-world physical variations. Moreover, their reliance on gradient access restricts applicability to black-box deployment scenarios. Based on these limitations, we aim to generate more diverse and realistic physical variations for comprehensively evaluating VLA robustness, while two key challenges must be addressed: (1) How to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility? (2) How to discover worst-case scenarios without prohibitive real-world data collection costs efficiently?\n\nTo address these challenges, we propose Eva-VLA, a unified framework for evaluating vision-language-action models’ robustness. Our key innovation lies in transforming discrete physical variations into continuous optimization problems. First, as shown in Fig. 1, we decompose real-world variations into three distinct domains: object 3D transformations parameterized with rotation angles(α\\alpha, β\\beta, γ\\gamma), illumination variations defined by point light parameters including position(xx, yy), radius(σ\\sigma), intensity(II), and adversarial patch placement specified by (Δ​x\\Delta x, Δ​y\\Delta y). This parameterization enables systematic exploration of the variation space while maintaining physical plausibility through explicit constraints. Second, to overcome the black-box nature of VLA models and non-differentiable simulation environments, we employ Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [7], a gradient-free optimization algorithm, to efficiently discover worst-case scenarios by iteratively optimizing physical variations parameters. This approach enables comprehensive vulnerability assessment without requiring model gradients or expensive real-world data collection.\n\nOur main contributions are as follows: ❶ To the best of our knowledge, we are the first to decompose real-world physical variations into three key domains—object 3D transformation, illumination variations, and adversarial patches—enabling a comprehensive evaluation of VLA robustness under these physical variations. ❷ We propose Eva-VLA, a novel framework that transforms discrete physical variations into continuous parameter optimization. By leveraging a simulator environment that allows us to reset to the same conditions, we ensure the repeatability and reliability of the evaluation process, which enables efficient exploration of worst-case scenarios without the need for expensive real-world data collection. ❸ Through extensive experiments on state-of-the-art model OpenVLA across multiple benchmarks, we expose significant fragility in current VLA systems, with failure rates exceeding 60% across all variation categories, with object transformations causing up to 97.8% failure in long-horizon tasks. These findings provide crucial insights for developing more robust VLA architectures and underscore the urgent need for improved robustness training methodologies.",
            "llm_summary": "【关注的是什么问题】\n1. 如何系统评估 Vision-Language-Action (VLA) 模型在真实世界物理变异下的鲁棒性（robustness）  \n2. 如何在不依赖大量真实数据的情况下发现最坏场景（worst-case scenarios）并保证评估可重复性  \n3. 如何将离散物理变异转化为连续优化问题以便可控探索  \n4. 如何覆盖三大变异域（object 3D transformation、illumination、adversarial patches）的全面评估  \n【用了什么创新的方案】\n核心方案：提出 Eva-VLA 框架，将离散物理变异转化为连续参数优化问题；将变异分解为 object 3D transformations、illumination variations、adversarial patches 三大域，采用 CMA-ES（协方差矩阵适应进化策略）进行黑盒优化以高效发现 worst-case 场景，并在可复现的仿真环境中重置到相同条件进行评估，从而在无需梯度信息或大量真实数据的情况下系统评估 VLA 的鲁棒性。通过对 OpenVLA 在多基准上的大规模实验，揭示了超过 60% 的失败率，且对象变换在长时任务中可达 97.8% 的失败率，强调了现实部署中的鲁棒性缺口与改进必要性。  \n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands",
            "authors": "Yanyuan Qiao,Kieran Gilday,Yutong Xie,Josie Hughes",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18937",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18937",
            "arxiv_html_link": "https://arxiv.org/html/2509.18937v1",
            "abstract": "Designing robotic hand morphologies for diverse manipulation tasks requires balancing dexterity, manufacturability, and task-specific functionality. While open-source frameworks and parametric tools support reproducible design, they still rely on expert heuristics and manual tuning. Automated methods using optimization are often compute-intensive, simulation-dependent, and rarely target dexterous hands. Large language models (LLMs), with their broad knowledge of human-object interactions and strong generative capabilities, offer a promising alternative for zero-shot design reasoning. In this paper, we present Lang2Morph, a language-driven pipeline for robotic hand design. It uses LLMs to translate natural-language task descriptions into symbolic structures and OPH-compatible parameters, enabling 3D-printable task-specific morphologies. The pipeline consists of: (i) Morphology Design, which maps tasks into semantic tags, structural grammars, and OPH-compatible parameters; and (ii) Selection and Refinement, which evaluates design candidates based on semantic alignment and size compatibility, and optionally applies LLM-guided refinement when needed. We evaluate Lang2Morph across varied tasks, and results show that our approach can generate diverse, task-relevant morphologies. To our knowledge, this is the first attempt to develop an LLM-based framework for task-conditioned robotic hand design.",
            "introduction": "Designing the morphology and structure of a robotic hand for a specific application is a fundamental yet long-standing challenge in robotics.\nUnlike the design of general-purpose manipulators, for example anthropomorphic robotic hands, task-specific robotic hands must balance dexterity and generality with optimization for a specific task, whilst remaining manufacturable [1, 2].\nThe design of dexterous, yet tasks-specific hands requires a full understanding of the task, and also fabrication constraints. For example, whilst increasing finger count or joint complexity improves capability but complicates fabrication and control.\nAs a result, tasks specific hand design has typically relied on expert-driven heuristics and iterative prototyping, which are time-consuming and hard to scale.\n\nThe development of parametric hand designs can provide a design space for task-specific hand morphology design.\nOpen-source hardware projects such as the Yale OpenHand Project [3, 4] have released reproducible underactuated hand designs, lowering the barrier for building functional prototypes. Parametric frameworks like the Open Parametric Hand (OPH) [5] further expose a structured design space with tunable parameters, enabling systematic exploration of hand morphologies.\nHowever, both approaches still depend on human designers to interpret tasks, identify likely grasp types, and manually adjust parameters. Without automated mapping from task requirements to design instantiations, scaling to diverse, task-specific morphologies remains difficult.\n\nIn parallel, the robotics community has explored automated robot morphology optimization and generation through evolutionary optimization, grammar-based synthesis, and differentiable pipelines [6, 7, 8, 9].\nThese works demonstrate that morphology can indeed be generated algorithmically, but they are often computationally expensive, require carefully crafted objective functions, and are typically coupled to physics-based simulators and rely on the curation of algorithmic representation of a fitness function or objective.\nMoreover, they have rarely targeted dexterous hand design, where functional requirements such as fingertip precision, lateral pinching, or stabilizing support are used to determine the resulting morphology.\nThus, these approaches lacks generality and relies on expert identification of key functional metrics or objective functions.\n\nRecently, large language models (LLMs) have been explored in robotics, with applications in navigation and manipulation where language is grounded into action policies [10, 11].\nWhilst these efforts focus mainly on control and planning,\nLLMs are not only capable of semantic reasoning but also encode broad background knowledge across biology, engineering, and everyday practice, which traditional optimization-based methods typically lack.\nThis combination makes them promising candidates for reasoning about form and functionality in design.\nSome recent works have adapted LLMs for computer-aided design (CAD) [12, 13], translating text into parametric part models.\nYet these methods remain restricted to geometry generation and do not address morphology design, where task semantics such as lateral pinching or stabilizing support directly dictate structural choices.\nTo the best of our knowledge, the use of LLMs for robotic hand morphology design remains unexplored, motivating our study.\n\nIn this paper, we propose Lang2Morph, a language-driven pipeline for robotic hand morphology generation. Our key insight is that LLMs are well suited to reasoning about task semantics and mapping them into symbolic and geometric design representations. Unlike prior methods that rely on expert-driven heuristics or costly physics-based simulation, Lang2Morph leverages LLM reasoning and semantic feedback to achieve scalable, task-conditioned morphology generation, as illustrated in Fig. 1, where a user-provided instruction is mapped into a design rationale and a CAD-ready morphology.\n\nLang2Morph builds upon the Open Parametric Hand (OPH) framework [5], which defines a structured and fabricable design space for robotic hands. OPH supports single-piece 3D printing, allowing generated designs to be directly manufactured without additional assembly or simulation. This enables our method to output physically realizable morphologies from natural language instructions.\n\nSpecifically, the pipeline comprises two major modules: (i) LLM-powered Morphology Design, which performs dual-level task analysis (semantic and structural), followed by geometry parameterization and constraint-aware validation to produce OPH-compatible parameters; and (ii) LLM-Guided Selection and Refinement, which ranks rendered variants based on semantic alignment and size compatibility, and optionally provides design refinements through feedback.\nTogether, these components form an end-to-end pipeline that generates fabricable, task-adaptive morphologies directly from natural language task description.\n\nOur main contributions are as follows:\n\nWe present Lang2Morph, a novel framework that generates robotic hand morphologies from natural-language instructions using large language models.\n\nWe design a two-stage pipeline that combines symbolic grammar generation, geometric parameterization, and semantic feedback for task-adaptive hand design.\n\nWe explore the use of LLMs for early-stage robot morphology generation, offering a flexible alternative to expert tuning.\n\nWe evaluate our method on a range of manipulation tasks, demonstrating improved design validity, diversity, and semantic alignment.\n\n1. We present Lang2Morph, a novel framework that generates robotic hand morphologies from natural-language instructions using large language models.\n\n2. We design a two-stage pipeline that combines symbolic grammar generation, geometric parameterization, and semantic feedback for task-adaptive hand design.\n\n3. We explore the use of LLMs for early-stage robot morphology generation, offering a flexible alternative to expert tuning.\n\n4. We evaluate our method on a range of manipulation tasks, demonstrating improved design validity, diversity, and semantic alignment.",
            "llm_summary": "【关注的是什么问题】\n1. 如何从自然语言任务描述自动生成可直接制造的任务适应性机器人手形态\n2. 在确保可制造性与任务相关性的同时，减少对人工经验启发式设计的依赖\n3. 面向 dexterous 手掌的任务特定形态设计的高效自动化路径\n4. 将语言模型用于高层语义分析到几何参数化、结构约束的端到端设计\n\n【用了什么创新的方案】\nLang2Morph 使用大语言模型进行双层任务分析（语义与结构），将自然语言指令映射到 OPH 兼容的参数化几何；结合符号语法生成与几何参数化，通过语义对齐和尺寸兼容性评估进行候选选择与 refinement；实现从文本到可直接 3D 打印的任务特定形态的端到端流程，且无需仿真驱动。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation",
            "authors": "Masato Kobayashi,Thanpimon Buamanee",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18865",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18865",
            "arxiv_html_link": "https://arxiv.org/html/2509.18865v1",
            "abstract": "We propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral control-based imitation learning to handle more than one task within a single model. Conventional bilateral control methods exploit joint angle, velocity, torque, and vision for precise manipulation but require task-specific models, limiting their generality.\nBi-VLA overcomes this limitation by utilizing robot joint angle, velocity, and torque data from leader-follower bilateral control with visual features and natural language instructions through SigLIP and FiLM-based fusion.\nWe validated Bi-VLA on two task types: one requiring supplementary language cues and another distinguishable solely by vision. Real-robot experiments showed that Bi-VLA successfully interprets vision-language combinations and improves task success rates compared to conventional bilateral control-based imitation learning.\nOur Bi-VLA addresses the single-task limitation of prior bilateral approaches and provides empirical evidence that combining vision and language significantly enhances versatility.\nExperimental results validate the effectiveness of Bi-VLA in real-world tasks.\nFor additional material, please visit the website: https://mertcookimg.github.io/bi-vla/",
            "introduction": "Robotic manipulation is increasingly important in human-centered applications such as cooking, eldercare, and interactive service robots [1, 2, 3, 4].\nUnlike traditional industrial robots that excel at repetitive and pre-programmed routines, service and collaborative robots must adapt to dynamic environments and interact with objects of diverse shapes, sizes, and material properties [5, 6]. Achieving such adaptability requires learning frameworks capable of acquiring human-like manipulation strategies [7].\n\nImitation learning (IL) has emerged as a promising approach for transferring human manipulation skills directly to robots [8].\nLeader-follower teleoperation has become a common pipeline for collecting demonstrations. For example, ALOHA and Mobile ALOHA use position-based unilateral control to gather diverse datasets that enable a wide range of manipulation [9, 10].\nAlthough effective for kinematics-driven tasks, such unilateral control omits force feedback, which limits robustness in contact-rich interactions.\n\nBilateral control-based imitation learning addresses these limitations by exchanging both position and force information between the demonstrator and robot [11, 12].\nBilateral control allows demonstrators to feel contact forces directly, yielding richer demonstrations and improving generalization across objects with different hardness and weights.\n\nBuilding on this foundation, recent methods have explored combining bilateral control with modern architectures.\nBilateral Control-Based Imitation Learning via Action Chunking with Transformers (Bi-ACT) [13] integrates bilateral control with visual observations via Transformers, yielding improved manipulation accuracy.\nHowever, it remains restricted to single-task settings, limiting its practicality in dynamic environments where multiple tasks must be handled seamlessly.\nIn parallel, Bilateral Control-Based Imitation Learning via Natural Language and Action Chunking with Transformers (Bi-LAT) [14] introduced natural language instructions into bilateral control-based imitation learning, demonstrating effective force modulation in manipulation.\nWhile this work highlights the promise of language integration, it focuses on regulating applied force and does not address the challenge of enabling a single model to adapt to multiple task contexts.\n\nIn this paper, we propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that unifies robot joint angle, velocity, and torque data from bilateral control with visual and language features, as shown in Fig. 1.\nBy leveraging SigLIP-based text embeddings [15] and FiLM-based EfficientNet feature fusion [16], Bi-VLA learns a shared representation of vision and language aligned with robot state information. Unlike prior bilateral frameworks, Bi-VLA is designed to handle multiple tasks within a single model, enabling flexible task switching without retraining or manual model selection.\n\nThe main contributions of this paper are summarized as follows:\n\nWe propose Bi-VLA, the bilateral control-based imitation learning framework that fuses vision and language features into a unified representation.\n\nWe demonstrate that Bi-VLA enables a single model to perform multiple tasks, overcoming the single-task limitation inherent in prior bilateral control-based imitation learning approaches.\n\nWe validate Bi-VLA through real-robot experiments on two distinct tasks, showing improved performance and adaptability compared to conventional methods.\n\n1. We propose Bi-VLA, the bilateral control-based imitation learning framework that fuses vision and language features into a unified representation.\n\n2. We demonstrate that Bi-VLA enables a single model to perform multiple tasks, overcoming the single-task limitation inherent in prior bilateral control-based imitation learning approaches.\n\n3. We validate Bi-VLA through real-robot experiments on two distinct tasks, showing improved performance and adaptability compared to conventional methods.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在单一模型中实现多任务的双边控制模仿学习与任务切换（≤40词）\n2. 如何将视觉与语言信息与机器人关节角度、速度、力矩数据融合以提升操控鲁棒性（≤40词）\n3. 如何在现实机器人环境中验证视觉-语言融合对多任务泛化的提升（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：在双边控制基础上，结合 SigLIP 文本嵌入和 FiLM 机制对 EfficientNet 提取的视觉特征进行融合，形成一个统一的视觉-语言-状态表示；通过该表示实现单模型处理多任务、并通过任务切换无需重新训练；在两类任务与真实机器人上验证性能提升。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation",
            "authors": "Suzannah Wistreich,Baiyu Shi,Stephen Tian,Samuel Clarke,Michael Nath,Chengyi Xu,Zhenan Bao,Jiajun Wu",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
            "comment": "Accepted to CoRL 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18830",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18830",
            "arxiv_html_link": "https://arxiv.org/html/2509.18830v1",
            "abstract": "Human skin provides a rich tactile sensing stream, localizing intentional and unintentional contact events over a large and contoured region.\nReplicating these tactile sensing capabilities for dexterous robotic manipulation systems remains a longstanding challenge.\nIn this work, we take a step towards this goal by introducing DexSkin. DexSkin is a soft, conformable capacitive electronic skin that enables sensitive, localized, and calibratable tactile sensing, and can be tailored to varying geometries.\nWe demonstrate its efficacy for learning downstream robotic manipulation by sensorizing a pair of parallel jaw gripper fingers, providing tactile coverage across almost the entire finger surfaces.\nWe empirically evaluate DexSkin’s capabilities in learning challenging manipulation tasks that require sensing coverage across the entire surface of the fingers, such as reorienting objects in hand and wrapping elastic bands around boxes, in a learning-from-demonstration framework. We then show that, critically for data-driven approaches, DexSkin can be calibrated to enable model transfer across sensor instances, and demonstrate its applicability to online reinforcement learning on real robots.\nOur results highlight DexSkin’s suitability and practicality for learning real-world, contact-rich manipulation.\nPlease see our project webpage for videos and visualizations: https://dex-skin.github.io/.",
            "introduction": "Tactile feedback is essential for robust and dexterous manipulation in natural and artificial systems. In humans, mechanoreceptors within the skin provide a rich sensory stream that guides tasks ranging from handling delicate objects to using tools with force [1]. This tactile feedback enables more precise reactive control than can be achieved with human and proprioceptive feedback alone [2].\n\nEmulating this tactile sensitivity in robotic systems has long been a challenge, starting from sensing hardware. Research efforts typically focus on rigid or partially flexible sensors that offer low spatial coverage, limited adaptability, and poor conformability to complex surfaces. In contrast, everyday tasks such as rotating a key in-hand or picking up delicate berries require sensing coverage in multiple contact regions.\nSimilarly, wrapping an elastic band around an object requires a sensor to detect, distinguish, and localize dynamic contact events on all surfaces of a human hand or end effector accurately, applying the appropriate force without allowing the band to slip.\n\nLearning-based approaches offer a general way to harness tactile sensing information. However, data-driven systems pose requirements beyond sensing coverage. Seemingly small distribution shifts in sensor readings from wear or replacement can render previously trained models unusable, thus, output signals must be replicable across sensor hardware instances. Additionally, for real-world online learning, sensors must remain consistent, durable, and precise under repeated stresses during trial-and-error interaction, and output interpretable signals amenable to reward or cost specification.\n\nTo address these challenges, we introduce a novel soft tactile skin named DexSkin that is particularly suitable for robotic learning applications. DexSkin can be conformably integrated onto robotic end-effectors with unparalleled spatial coverage.\nIt is based on a capacitive mechanism, and features high sensitivity and robustness under repeated interactions.\nBecause each of the dozens of taxels on the skin are individually addressable, DexSkin can localize and characterize simultaneous contacts from distinct regions.\nIt can also be calibrated to provide consistent readings across distinct sensor units, enabling re-use of learned networks. At the same time, it can withstand deformations encountered during typical dexterous tasks such as pinching, twisting, and bending.\n\nIn this work, we introduce the DexSkin framework and its fabrication, as well as a representative integration with a soft cylindrical robotic fingertip that sensorizes the distal dome of the finger and 294∘ of the circumference. Then, we evaluate DexSkin’s applicability to robot learning. First, we test whether DexSkin’s coverage and tailorability expands the range of learnable manipulation tasks. Then, we evaluate its calibration performance, which are critical for working with and transferring learned tactile models. Finally, we demonstrate its suitability for online robotic learning settings by performing real-world reinforcement learning for a delicate object picking task. The results highlight DexSkin’s robust applicability to a wide range of robotic tasks and morphologies, and its particular practicality for robot learning researchers and practitioners.",
            "llm_summary": "【关注的是什么问题】\n1. 需要大面积、可 conform 的触觉传感覆盖以实现高鲁棒的接触丰富操控\n2. 如何在多传感单元上获得高灵敏、可标定、跨传感器的一致性读取\n3. 在 learns-from-demonstration 与 在线强化学习 场景中实现对不同几何和材料的可转移性\n4. 传感硬件对姿态、变形的耐久性及对数据分布偏移的鲁棒性\n\n【用了什么创新的方案】\nDexSkin 是一种软性、可 conform 的电容式电子皮肤，具有大量独立可寻址的触觉单元（taxels），可覆盖手指表面且对变形鲁棒；通过逐个触觉单元标定实现跨传感器的一致性，使已学习的模型可在不同传感器实例间转移；支持对多点同时接触的定位与表面广覆盖的感知，适用于从演示学习到在线强化学习的任务。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations",
            "authors": "Lukas Zanger,Bastian Lampe,Lennart Reiher,Lutz Eckstein",
            "subjects": "Robotics (cs.RO); Multiagent Systems (cs.MA); Software Engineering (cs.SE)",
            "comment": "7 pages, 2 figures, 2 tables; Accepted to be published as part of the 2025 IEEE International Conference on Intelligent Transportation Systems (ITSC 2025), Gold Coast, Australia, November 18-21, 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18793",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18793",
            "arxiv_html_link": "https://arxiv.org/html/2509.18793v1",
            "abstract": "Vehicles are becoming increasingly automated and interconnected, enabling the formation of cooperative intelligent transport systems (C-ITS) and the use of offboard services. As a result, cloud-native techniques, such as microservices and container orchestration, play an increasingly important role in their operation.\nHowever, orchestrating applications in a large-scale C-ITS poses unique challenges due to the dynamic nature of the environment and the need for efficient resource utilization.\nIn this paper, we present a demand-driven application management approach that leverages cloud-native techniques – specifically Kubernetes – to address these challenges. Taking into account the demands originating from different entities within the C-ITS, the approach enables the automation of processes, such as deployment, reconfiguration, update, upgrade, and scaling of microservices.\nExecuting these processes on demand can, for example, reduce computing resource consumption and network traffic.\nA demand may include a request for provisioning an external supporting service, such as a collective environment model.\nThe approach handles changing and new demands by dynamically reconciling them through our proposed application management framework built on Kubernetes and the Robot Operating System (ROS 2).\nWe demonstrate the operation of our framework in the C-ITS use case of collective environment perception and make the source code of the prototypical framework publicly available at https://github.com/ika-rwth-aachen/application_manager.",
            "introduction": "In future cooperative intelligent transport systems (C-ITS), various entities, such as vehicles equipped with driving automation systems, sensor-equipped roadside infrastructure units, edge/cloud servers, and control centers, will be connected, exchange data, and may offer computational resources.\nThese advancements enable new applications – such as collective environment perception, cooperative decision-making, computation offloading, and intelligent traffic management – that can contribute to improved comfort and safety for road users [1], [2], [3].\nNot only cloud and edge servers but also vehicles and roadside units can be part of a distributed computing system.\nHowever, these applications may also introduce complexity that is difficult to manage. The dynamic nature of C-ITS, the presence of resource-constrained entities, and the strict requirements for safety and security pose unique challenges.\n\nCloud-native techniques provide a promising foundation for the development and operation of scalable applications in dynamic environments. Such techniques involve paradigms like containerization, microservice architectures, and container orchestration. They enable loosely coupled systems which are manageable and resilient [4]. Said techniques and paradigms have the potential to contribute to the advancement of C-ITS.\n\nKubernetes has evolved as the de facto standard for orchestrating containerized applications in distributed systems. It is open-source and widely adopted by software companies worldwide [5].\nNevertheless, Kubernetes lacks methods that are domain-specific, e.g., to C-ITS, considering that specific tasks like the deployment of required applications are only needed at certain times or may depend on the specific content of data exchanged in the C-ITS.\nWe have developed the approach RobotKube [6] to extend the regular capabilities of Kubernetes. RobotKube comprises software components designed to automate the identification of requirements and the formulation of specific Kubernetes workloads.\nThese components include the event detector and the application manager.\n\nIn this paper, we propose a demand-driven application management approach and present the methodology behind the application manager as part of an application management framework.\nThis methodology integrates seamlessly into the RobotKube architecture and complements parts of RobotKube which were not detailed yet.\nThe application management framework – comprising the application manager and a set of custom operators – addresses the orchestration challenges in C-ITS through a demand-driven approach.\nIn this context, applications are deployed, reconfigured, scaled, and updated based on the current demands of C-ITS entities.\n\nWith our work, we make the following main contributions:\n\nPresentation of the methodology for demand-driven application management allowing to deploy, reconfigure, update, upgrade, and scale applications based on demands of entities in a C-ITS.\n\nPrototypical implementation of the application manager and the custom operators in an application management framework based on Kubernetes and ROS 2.\n\nDemonstration and evaluation of the capabilities of the application management framework in the complex C-ITS use case of collective environment perception involving various C-ITS entities.\n\nOpen source code publication of the application management framework and the demonstration use case allowing for reproducibility and extensibility.\n\nComplementation of RobotKube [6] by providing the concrete methodology of the application manager.\n\n1. Presentation of the methodology for demand-driven application management allowing to deploy, reconfigure, update, upgrade, and scale applications based on demands of entities in a C-ITS.\n\n2. Prototypical implementation of the application manager and the custom operators in an application management framework based on Kubernetes and ROS 2.\n\n3. Demonstration and evaluation of the capabilities of the application management framework in the complex C-ITS use case of collective environment perception involving various C-ITS entities.\n\n4. Open source code publication of the application management framework and the demonstration use case allowing for reproducibility and extensibility.\n\n5. Complementation of RobotKube [6] by providing the concrete methodology of the application manager.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在动态且资源受限的C-ITS环境中对应用进行按需驱动的管理与部署（≤40词）\n2. 如何将云原生技术（Kubernetes、ROS 2、微服务）整合用于C-ITS中的应用编排与再配置（≤40词）\n3. 如何通过需求感知实现应用的部署、重配置、更新、升级与扩缩以优化资源与网络开销（≤40词）\n【用了什么创新的方案】\n核心解决方案：提出基于需求驱动的应用管理框架，结合 Kubernetes 与 ROS 2，开发应用管理器与自定义 Operator，用事件检测器识别需求并动态编排、部署、重配置、升级和扩缩，支持对外部服务等需求的 provisioning；框架作为 RobotKube 的扩展，公开源码以便复现与扩展。\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "Human-Interpretable Uncertainty Explanations for Point Cloud Registration",
            "authors": "Johannes A. Gaus,Loris Schneider,Yitian Shi,Jongseok Lee,Rania Rayyes,Rudolph Triebel",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18786",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18786",
            "arxiv_html_link": "https://arxiv.org/html/2509.18786v1",
            "abstract": "In this paper, we address the point cloud registration problem, where well-known methods like ICP fail under uncertainty arising from sensor noise, pose‐estimation errors, and partial overlap due to occlusion. We develop a novel approach, Gaussian Process Concept Attribution (GP-CA), which not only quantifies registration uncertainty but also explains it by attributing uncertainty to well-known sources of errors in registration problems. Our approach leverages active learning to discover new uncertainty sources in the wild by querying informative instances. We validate GP-CA on three publicly available datasets and in our real-world robot experiment. Extensive ablations substantiate our design choices. Our approach outperforms other state-of-the-art methods in terms of runtime, high sample-efficiency with active learning, and high accuracy. Our real-world experiment clearly demonstrates its applicability. Our video also demonstrates that GP-CA enables effective failure-recovery behaviors, yielding more robust robotic perception.",
            "introduction": "Point cloud registration refers to the problem of estimating a relative transformation between two sets of 3D points [1]. This problem is essential in many robotic perception tasks, such as simultaneous localization and mapping (SLAM) [2], 3D reconstruction [3], and 6-DoF object pose estimation [4], to name a few. A widely used approach for point cloud registration is Iterative Closest Point (ICP) [5].\nHowever, in ICP, well-known causes\nof uncertainty are\nsensor noise, poor initialization of the optimization process of the relative transformation between the two point clouds, and insufficient overlap between the two point clouds (e.g., due to occlusions), which can make ICP inaccurate or unreliable in practice  [6]. Hence, several researchers attempt to improve the robustness of the registration process [7].\n\nAmong others, several probabilistic approaches were proposed to quantify uncertainty in point cloud registration [6, 8, 9, 10, 11]. The underlying idea is that uncertainty estimates provide insight into the reliability of the obtained results, enabling the identification and rejection of unreliable registrations. Moreover, information about uncertainty is often used in many downstream tasks, including sensor fusion, state estimation, and 3D reconstruction [6]. To this end, a variety of tools have been developed to quantify uncertainty in point cloud registration, ranging from closed-form Gaussian solvers [11] to particle-based methods [6].\n\nWhile quantifying uncertainty helps to improve the registration, in practice, the magnitude of uncertainty is rarely sufficient in practical robotic perception tasks, as it does not reveal why registration failed or how to recover from it. Distinct failure causes — such as sensor noise, poor initialization, or occlusion [6] — require different actions to recover from failure.\nIn this work, we propose a novel approach, Gaussian Process Concept Attribution (GP-CA). A major difference to existing work is that, our approach not only quantifies uncertainty, but also explains uncertainty in a human-interpretable manner. Our GP-CA integrates active learning, learned point-cloud representations, and a Gaussian Process classifier to provide concept-level uncertainty attribution. The active learning allows fast adaptation to new uncertainty sources.\nIn preliminary work [12], the explainability method SHAP was used to analyze the influence of different uncertainty sources on ICP-based registration. However, it cannot identify which uncertainty source is present in a registration and depends on manual control over uncertainty sources and heavy computation, limiting applicability and hindering its feasibility for robotic decision-making.\n\nGP-CA attributes uncertainty to semantically meaningful concepts that can be used later to enable robots to select targeted recovery actions. An example is depicted in Fig. 1, where the robot is tasked to perform a 6D pose estimation of objects using the ICP algorithm. Initially, an object of interest (the bottle) is occluded by another object, which causes the ICP algorithm to fail. Using our method, the occlusion is identified as the cause of failure, thereby enabling targeted recovery actions (e.g., changing the viewpoint).\nHence, our main idea of explaining uncertainty in point cloud registration can improve the robustness of the perception tasks.\n\nWhen registration fails or reports high uncertainty, GP-CA embeds the ICP-aligned source point cloud using a learned representation. Then, a multi-class Gaussian Process classifier (GPC) maps these representations to confidence scores over different concepts. In this way, we attribute the uncertainty to specific concepts, thereby identifying its source. The concepts are predefined by sets of examples. Still, a robot may encounter unknown sources of uncertainty during its operations. Therefore, GP-CA is equipped with an active learning mechanism [13, 14], which can learn a new concept by querying the user for the label and choosing the most informative data to learn from. Across experiments on the LINEMOD [15], the YCB [16], and the Coffee Cup [17] dataset and a custom real-world RGB-D dataset from our laboratory, we validate GP-CA and provide extensive ablation studies. We further propose how GP-CA can enable a more robust robotic perception by executing a set of failure recovery actions associated with uncertainty explanations.\n\nIn summary, our main contributions are as follows.\n\nWe propose a novel approach, GP-CA, which enables human-interpretable uncertainty explanations in point cloud registration. To the best of our knowledge, this is the first work to show how explainability can advance point cloud registration for real-world applications\n\nWe extend our approach with active learning to adapt and integrate new concepts.\n\nWe validate the GP-CA design through an extensive ablation study\n\nWe demonstrate clearly the runtime-efficiency, the high accuracy, and the sample-efficiency of our method, with SOTA baseline comparison across four datasets.\n\nWe propose how GP-CA can be employed for recovery actions in a real-world robot setting.\n\n1. We propose a novel approach, GP-CA, which enables human-interpretable uncertainty explanations in point cloud registration. To the best of our knowledge, this is the first work to show how explainability can advance point cloud registration for real-world applications\n\n2. We extend our approach with active learning to adapt and integrate new concepts.\n\n3. We validate the GP-CA design through an extensive ablation study\n\n4. We demonstrate clearly the runtime-efficiency, the high accuracy, and the sample-efficiency of our method, with SOTA baseline comparison across four datasets.\n\n5. We propose how GP-CA can be employed for recovery actions in a real-world robot setting.",
            "llm_summary": "【关注的是什么问题】\n1. 点云配准中的不确定性量化与原因解释（≤40词）\n2. 如何将不确定性解释转化为人类可理解的概念级因果归因（≤40词）\n3. 在实际机器人场景中实现快速、鲁棒的失败恢复策略（≤40词）\n\n【用了什么创新的方案】\n核心解决方案：提出高斯过程概念归因GP-CA，结合点云嵌入、GPC分类器实现对不确定性的概念级归因，并通过主动学习发现新不确定源；可用于在ICP失败时给出针对性的恢复操作，提升运行时鲁棒性与效率。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models",
            "authors": "Shijia Ge,Yinxin Zhang,Shuzhao Xie,Weixiang Zhang,Mingcai Zhou,Zhi Wang",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "submitted to AAAI 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.18778",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18778",
            "arxiv_html_link": "https://arxiv.org/html/2509.18778v1",
            "abstract": "Visual imitation learning frameworks allow robots to learn manipulation skills from expert demonstrations. While existing approaches mainly focus on policy design, they often neglect the structure and capacity of visual encoders—limiting spatial understanding and generalization. Inspired by biological vision systems, which rely on both visual and proprioceptive cues for robust control, we propose VGGT-DP, a visuomotor policy framework that integrates geometric priors from a pretrained 3D perception model with proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer (VGGT) as the visual encoder and introduce a proprioception-guided visual learning strategy to align perception with internal robot states, improving spatial grounding and closed-loop control. To reduce inference latency, we design a frame-wise token reuse mechanism that compacts multi-view tokens into an efficient spatial representation. We further apply random token pruning to enhance policy robustness and reduce overfitting. Experiments on challenging MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines such as DP and DP3, particularly in precision-critical and long-horizon scenarios.",
            "introduction": "未获取到引言",
            "llm_summary": "【关注的是什么问题】\n1. 如何将视觉编码器的结构与能力用于提升机器人视觉-动作嵌入的泛化和精确控制\n2. 如何融合预训练3D感知模型的几何先验与机器人本体的 proprioceptive 信息以改善空间定位与闭环控制\n3. 如何降低多视角视觉输入的推理时延并提升鲁棒性\n4. 如何通过 token 重用和随机剪枝提升效率与抗过拟合能力\n【用了什么创新的方案】\n- 以 Visual Geometry Grounded Transformer (VGGT) 作为视觉编码器，将几何先验整合到 visuomotor 策略中\n- 引入 proprioception-guided visual learning，使感知对齐内部状态并增强空间着陆与闭环控制\n- 设计 frame-wise token reuse 机制，将多视图 token 压缩为高效的空间表示以降低推理延迟\n- 采用随机 token pruning 增强策略鲁棒性并抑制过拟合\n- 在 MetaWorld 任务上对比 DP、DP3，显示显著提升，尤其在精度关键与长时程场景\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 05:04:47",
            "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning",
            "authors": "Omar Rayyan,John Abanes,Mahmoud Hafez,Anthony Tzes,Fares Abu-Dakka",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "For project website and videos, see httpsthis https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18757",
            "code": "https://mv-umi.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18757",
            "arxiv_html_link": "https://arxiv.org/html/2509.18757v1",
            "abstract": "Recent advances in imitation learning have shown great promise for developing robust robot manipulation policies from demonstrations. However, this promise is contingent on the availability of diverse, high-quality datasets, which are not only challenging and costly to collect but are often constrained to a specific robot embodiment. Portable handheld grippers have recently emerged as intuitive and scalable alternatives to traditional robotic teleoperation methods for data collection. However, their reliance solely on first-person view wrist-mounted cameras often creates limitations in capturing sufficient scene contexts. In this paper, we present MV-UMI (Multi-View Universal Manipulation Interface), a framework that integrates a third-person perspective with the egocentric camera to overcome this limitation. This integration mitigates domain shifts between human demonstration and robot deployment, preserving the cross-embodiment advantages of handheld data-collection devices. Our experimental results, including an ablation study, demonstrate that our MV-UMI framework improves performance in sub-tasks requiring broad scene understanding by approximately 47% across 3 tasks, confirming the effectiveness of our approach in expanding the range of feasible manipulation tasks that can be learned using handheld gripper systems, without compromising the cross-embodiment advantages inherent to such systems.\nVideos can be found here: https://mv-umi.github.io",
            "introduction": "Imitation Learning (IL) provides a compelling pathway toward acquiring general robot policies capable of performing long-horizon tasks across diverse environments. This approach, particularly through supervised methods like Behavioral Cloning (BC), enables robots to acquire complex behaviors by learning to imitate human-directed actions in response to observations. Recent advances in architectures that better model this mapping [1, 2, 3, 4], coupled with enhancements in embodiments and hardware integrations [5, 6], have made this route increasingly convincing.\n\nRecent studies on data scaling laws in imitation learning [7] show that robot policy performance follows training scenario diversity, emphasizing the need for extensive and diverse data for robust policies. Data collection typically lies between two extremes. On one end, robot teleoperation enables the acquisition of high-quality, precise data with minimal embodiment discrepancies. However, this method is time-consuming and costly, as it requires an actively operated robot. On the other end, the internet is replete with videos of humans performing various tasks. However, substantial effort is required to establish structured explicit mappings between observed states and actions from these videos.\n\nAs a middle-ground, portable handheld grippers [8, 9, 10, 11, 12] have emerged as inexpensive and intuitive to use data collection devices. By relying exclusively on a wrist-mounted camera, they enable non-experts to record demonstrations without the need for a robotic manipulator. While this egocentric viewpoint minimizes visual discrepancies between training and deployment, resulting in cross-embodiment policies, it demands that the robot maintain a longer memory context to recall scene elements that move out of the constraining wrist view.\n\nIn this work, we propose a novel framework that augments the conventional wrist-mounted camera in handheld gripper systems with a third-person camera viewpoint, without incurring distributional shifts. We achieve this by performing real-time masking of the human demonstrator in the third-person video stream, effectively removing the operator’s presence from the training data. As a result, the model benefits from a broader view of the environment, while relying less on memory for scene remembrance. A side benefit we find of this masking is its removal of correlations between the demonstrator’s motions and the gripper’s actions, encouraging the policy to focus on task-relevant cues such as the manipulated objects rather than overfitting to human-specific signals. We also utilize a custom-made three-jaw gripper for some of the tasks that require greater dexterity in this work. This design allows for greater payload weight in comparison to other hand-held devices, at the cost of its volume. Schematics and instructions to reproduce the hardware are open-sourced separately. The hardware aspect is not a key focus in this paper.\n\nSummary of Contributions:\n\nMulti-View Cross-Embodiment Framework: MV-UMI fuses wrist-mounted and third-person views using SAM-2 segmentation and inpainting to eliminate domain shift, boosting performance in context-dependent tasks by 47%.\n\nEnd-to-End Open-Source System: Complete pipeline, including hardware design, data collection, training code, and deployment tools, is publicly released to advance cross-embodiment manipulation research, https://mv-umi.github.io.",
            "llm_summary": "【关注的是什么问题】\n1. 如何在跨 embodiment 的示范学习中利用多视角来缓解域间差异（跨人体/设备）的影响\n2. 如何在手持抓取设备的数据集上融合第一视角和第三视角以提升上下文理解能力\n3. 如何通过去除示范者在第三视角中的影像来减少示范者动作与抓手动作的相关性\n4. 如何在不增加分布漂移的前提下扩展数据视角以提升对场景理解的鲁棒性\n5. 如何在跨视角学习中保持对任务相关 cues 的聚焦，避免过拟合人类信号\n\n【用了什么创新的方案】\n核心解决方案：在手持夹具的第一视角与第三视角之间进行实时多视图融合，利用 SAM-2 对人类示范者进行分割并在第三视角视频中进行去人化处理，从而获得更广的环境观察并降低域偏移；结合自定义三撬式抓手以提高耐受载荷，输出端到端的开放源码数据收集、训练与部署管线；通过掩码化第三视角实现对示范者的去域化，减少示范者动作与抓取动作的相关性，提升跨嵌入学习的任务自适应性。实验结果显示在需要广域场景理解的子任务上约提升47%。\n\n【相关性评分】\n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation",
            "authors": "Nishant Doshi,Amey Sutvani,Sanket Gujar",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18734",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18734",
            "arxiv_html_link": "https://arxiv.org/html/2509.18734v1",
            "abstract": "One of the challenges faced by Autonomous Aerial\nVehicles is reliable navigation through urban environments.\nFactors like reduction in precision of Global Positioning System\n(GPS), narrow spaces and dynamically moving obstacles make the\npath planning of an aerial robot a complicated task. One of the\nskills required for the agent to effectively navigate through such an\nenvironment is to develop an ability to avoid collisions using\ninformation from onboard depth sensors. In this paper, we propose\nReinforcement Learning of a virtual quadcopter robot agent\nequipped with a Depth Camera to navigate through a simulated\nurban environment.",
            "introduction": "In recent years, Quadcopters have been extensively used\nfor civilian task like object tracking, disaster rescue, wildlife\nprotection and asset localization. It presents interesting\napplication avenues especially in tasks such as automated mail\ndelivery system, fire protection and disaster management.\nHowever, quadcopter navigation through urban environments\nis a complex task because of frequent dynamic obstacles\n(Humans, Posters, etc.). Also, the GPS navigation system can\nperform poorly when surrounded by tall buildings in urban\nenvironment, dilating the precision of the 3D position fix. It\nbecomes more dangerous when the quadcopter is flying\nthrough tight spaces and is uncertain of its position, increasing\nthe chances of collision. The quadcopter also needs to take\nsmart action after detecting dynamic obstacles (Humans,\nVehicles, animals, traffic signals etc.) during navigation in\nruntime in urban environment. Traditionally, obstacle\navoidance techniques have been designed as end point solution\nin an aerial robot navigation. One of the promising approach\nfor this problem is deep reinforcement learning. In this paper a\nsimple model is developed for the task of detecting and\navoiding common civilian obstacles encountered by a\nquadcopter while navigating a path in an urban environment.\n\nFrom the reinforcement learning view, the main challenge\nhere is that, the policy should update itself during runtime for\nstochastic obstacles detected in the environment and take the\noptimal action accordingly. Also, the navigation problem has\nsparse distributed reward in state space which is a challenge for\nlearning the shortest distance.\n\nThe objective of this project is to train a quadcopter to\nnavigate without hitting obstacles and taking a shortest path\naround through a high-rise urban environment where stochastic\nand dynamic obstacles are frequent.\n\nThe organization of the paper is as follows: Section I\nprovides a general introduction to the challenges for\nquadcopter urban navigation. Section II provides a\nprerequisites required to understand the experiments. Section\nIII defines the problem outlining the agent used and the\nenvironment. Section IV gives a brief description about the\nAirSim simulator, while section V defines the solution\napproaches for the problem defined. Section VI describes the\nexperiments and training and testing arena used. Section VII\ndiscusses the results for the experiments, while section VIII\ndescribes the future attempt that can be made and section IX\ndescribes the challenges faced during the experiments.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在城市环境中实现四旋翼无人机的可靠导航。  \n2. 如何处理动态障碍物对无人机路径规划的影响。  \n3. 如何在稀疏奖励环境中优化深度强化学习策略。  \n\n【用了什么创新的方案】  \n本研究提出了一种基于深度强化学习的虚拟四旋翼机器人代理，利用深度摄像头在模拟城市环境中进行导航。该方法通过实时更新策略来应对环境中的随机障碍物，并优化路径选择以避免碰撞，同时解决了稀疏奖励的问题。  \n\n【相关性评分】  \n分数：1分",
            "llm_score": 1,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Query-Centric Diffusion Policy for Generalizable Robotic Assembly",
            "authors": "Ziyi Xu,Haohong Lin,Shiqi Liu,Ding Zhao",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "8 pages, 7 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.18686",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18686",
            "arxiv_html_link": "https://arxiv.org/html/2509.18686v1",
            "abstract": "The robotic assembly task poses a key challenge in building generalist robots due to the intrinsic complexity of part interactions and the sensitivity to noise perturbations in contact-rich settings. The assembly agent is typically designed in a hierarchical manner: high-level multi-part reasoning and low-level precise control. However, implementing such a hierarchical policy is challenging in practice due to the mismatch between high-level skill queries and low-level execution. To address this, we propose the Query-centric Diffusion Policy (QDP), a hierarchical framework that bridges high-level planning and low-level control by utilizing queries comprising objects, contact points, and skill information. QDP introduces a query-centric mechanism that identifies task-relevant components and uses them to guide low-level policies, leveraging point cloud observations to improve the policy’s robustness. We conduct comprehensive experiments on the FurnitureBench in both simulation and real-world settings, demonstrating improved performance in skill precision and long-horizon success rate. In the challenging insertion and screwing tasks, QDP improves the skill-wise success rate by over 50% compared to baselines without structured queries.",
            "introduction": "Contact-rich manipulation has been widely recognized as a critical task when building generalist intelligent robots [1, 2, 3]. In this field, robot assembly [4, 5] stands out because it requires policies that are both precise and versatile to control the robot arm and interact with multiple objects.\nDespite the access to some offline demonstration data from human priors, robotic assembly poses two key challenges: inter-part relational reasoning and intra-part precise control in the online deployment. As is visualized in Figure 1, the first challenge arises from the long-horizon, multi-part nature of the task, which demands accurate prediction of the next skill based on current observations, as well as identifying the correct objects for interaction. The second challenge becomes especially difficult in contact-rich scenarios where successful assembly hinges on precise object alignment. Even minor noise or occlusions in raw sensory observations can completely fail the sim-to-real transfer of low-level non-prehensile control policies.\n\nVarious methods have been explored to address these challenges. One line of work focuses on high-level reasoning by leveraging cross-modality affordance-based approaches [6], graph-based key point reasoning [7], or skill-based retrieval [8, 9]. However, these methods rely heavily on heuristic-based low-level controllers and predefined skill libraries, thus limiting their adaptability and precision.\nMeanwhile, recent advances in robot learning have significantly improved the precision and adaptability of low-level policies, thanks to (i) powerful imitation learning backbones such as diffusion models [10, 11, 12] and transformer-based architectures [13, 14], (ii) enhanced learning regimes like residual policies [15] and safe failure prevention methods [16], and (iii) the integration of sensor modalities beyond vision and proprioception, such as tactile sensing [17, 9] or point clouds [18] for improved contact modeling.\nStill, transferring these policies from simulation to the real world remains challenging due to the inherent difficulty of accurately simulating contact dynamics.\n\nIn addition to the individual challenges of high-level and low-level policy design, integrating these two levels in a hierarchical framework introduces further complications. For instance, high-level policies may mis-specify objects or skills, causing the low-level policy to rely on the false queries that include irrelevant factors such as background pixels or non-impactful objects, ultimately leading to failure in real-world environments. Hence, it is crucial to establish a parsimonious proxy between high-level and low-level modules.\n\nTo address these challenges and establish a robust interface between high-level and low-level modules, we propose Query-centric Diffusion Policy (QDP). Our framework leverages powerful pre-trained foundation models to extract high-level information by specifying both the requisite skills and target objects as a query. This query then serves as a powerful precondition to guide point cloud-based low-level control, forcing the robot agent to focus on the current task-relevant components when generating the action chunks under different contexts.\nOur contributions are threefold:\n\nWe introduce QDP, a query-centric diffusion policy framework that selects task-relevant queries for guiding low-level policies, enabling accurate skill selection and precise object interaction.\n\nWe introduce QDP, a query-centric diffusion policy framework that selects task-relevant queries for guiding low-level policies, enabling accurate skill selection and precise object interaction.\n\nWe propose a query-conditioned policy learning scheme to model the complex geometry captured by point cloud observations, improving precision and facilitating sim-to-real transfer.\n\nWe demonstrate the effectiveness of our approach on FurnitureBench in both simulation and real-world settings, showcasing robust performance under object misalignment and human intervention.\n\n1. We introduce QDP, a query-centric diffusion policy framework that selects task-relevant queries for guiding low-level policies, enabling accurate skill selection and precise object interaction.\n\n2. We propose a query-conditioned policy learning scheme to model the complex geometry captured by point cloud observations, improving precision and facilitating sim-to-real transfer.\n\n3. We demonstrate the effectiveness of our approach on FurnitureBench in both simulation and real-world settings, showcasing robust performance under object misalignment and human intervention.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂的机器人组装任务中实现高层次的多部件推理与低层次的精确控制。  \n2. 如何解决在接触丰富的环境中，低层次控制策略的噪声敏感性和仿真到现实的转移问题。  \n3. 如何有效整合高层和低层策略，以避免错误的对象或技能指定导致的失败。  \n\n【用了什么创新的方案】  \n提出了Query-centric Diffusion Policy (QDP)框架，通过查询机制选择任务相关的组件，指导低层策略的执行。QDP利用点云观察来增强政策的鲁棒性，改善技能选择的准确性和物体交互的精确性。该框架在FurnitureBench上进行了全面实验，展示了在技能精度和长时间成功率上的显著提升。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space",
            "authors": "Sangjun Noh,Dongwoo Nam,Kangmin Kim,Geonhyup Lee,Yeonguk Yu,Raeyoung Kang,Kyoobin Lee",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "7 main scripts + 2 reference pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18676",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18676",
            "arxiv_html_link": "https://arxiv.org/html/2509.18676v1",
            "abstract": "Learning robust visuomotor policies that generalize across diverse objects and interaction dynamics remains a central challenge in robotic manipulation. Most existing approaches rely on direct observation-to-action mappings or compress perceptual inputs into global or object-centric features, which often overlook localized motion cues critical for precise and contact-rich manipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework that leverages scene-level 3D flow as a structured intermediate representation to capture fine-grained local motion cues. Our approach predicts the temporal trajectories of sampled query points and conditions action generation on these interaction-aware flows, implemented jointly within a unified diffusion architecture. This design grounds manipulation in localized dynamics while enabling the policy to reason about broader scene-level consequences of actions. Extensive experiments on the MetaWorld benchmark show that 3D FDP achieves state-of-the-art performance across 50 tasks, particularly excelling on medium and hard settings. Beyond simulation, we validate our method on eight real-robot tasks, where it consistently outperforms prior baselines in contact-rich and non-prehensile scenarios. These results highlight 3D flow as a powerful structural prior for learning generalizable visuomotor policies, supporting the development of more robust and versatile robotic manipulation. Robot demonstrations, additional results, and code can be found at https://sites.google.com/view/3d-fdp.",
            "introduction": "Learning robust manipulation skills in unstructured environments is a fundamental challenge in robotics. Policies deployed in such settings must generalize across diverse object geometries, appearances, dynamics, and contextual variations. Visual imitation learning offers a scalable alternative by enabling robots to acquire visuomotor skills from expert demonstrations without requiring task-specific reward functions [1, 2]. Recent approaches [3, 4, 5, 6, 7] often employ end-to-end architectures that map perception directly to control, leveraging transformers whose attention mechanisms excel at capturing global context and integrating multiple modalities.\n\nMore recently, diffusion models have shown significant promise in this domain [8, 4]. By formulating action generation as an iterative denoising process, they provide stable training and support complex, multimodal action distributions. When combined with structured inputs such as point clouds, these models achieve strong performance across diverse manipulation tasks [9, 10, 11]. Despite these advances, most existing methods still follow a direct observation-to-action mapping, often compressing perceptual inputs into global or object-centric features. Such representations can overlook localized motion cues that are essential for precise and contact-rich manipulation. While some works introduce intermediate representations, approaches based on object poses [12, 13] can struggle to capture scene-level dynamics, and those predicting future frames [14, 15, 16, 17, 18, 19] are often computationally intensive.\n\nTo address these limitations, we introduce the 3D Flow Diffusion Policy (3D FDP), an architecture that explicitly models local interaction dynamics through an intermediate representation of 3D scene flow. Our method predicts the temporal trajectories of sampled 3D query points and uses this predicted flow to infer actions. Unlike prior approaches [12, 20] that rely on object-level representations, our model learns both flow prediction and action generation jointly within a unified diffusion framework (Fig. 1). This integrated design offers two key advantages. First, 3D flow captures fine-grained correlations between the robot gripper and manipulated objects, providing spatial grounding that supports contact-aware behavior. Second, by modeling how local motion propagates across the scene, the policy can reason about the downstream effects of manipulation, such as how inserting a book might disturb surrounding objects on a cluttered shelf. These capabilities improve generalization across both geometric and dynamic variations.\n\nWe evaluate 3D FDP on the MetaWorld benchmark [21] and a set of real-world manipulation tasks. On MetaWorld, our approach achieves state-of-the-art performance across all difficulty levels, with particularly strong improvements on medium and hard tasks. In real-robot experiments involving diverse physical interactions, 3D FDP demonstrates capabilities beyond baseline policies, successfully handling contact-rich and non-prehensile tasks where previous methods fail. These results demonstrate that 3D scene flow provides an effective inductive bias for learning generalizable visuomotor policies. Our main contributions are as follows:\n\nWe introduce a scene-level 3D flow representation that captures interaction-aware motion throughout the scene.\n\nWe develop a unified diffusion-based architecture that jointly learns 3D flow prediction and action generation in an end-to-end manner.\n\nWe demonstrate that 3D FDP achieves state-of-the-art results on 50 MetaWorld tasks and outperforms prior baselines in real-world manipulation scenarios.\n\nWe provide a comprehensive analysis that demonstrates the effectiveness of our 3D flow representation, investigating key aspects such as conditioning strategies and query point density.\n\n1. We introduce a scene-level 3D flow representation that captures interaction-aware motion throughout the scene.\n\n2. We develop a unified diffusion-based architecture that jointly learns 3D flow prediction and action generation in an end-to-end manner.\n\n3. We demonstrate that 3D FDP achieves state-of-the-art results on 50 MetaWorld tasks and outperforms prior baselines in real-world manipulation scenarios.\n\n4. We provide a comprehensive analysis that demonstrates the effectiveness of our 3D flow representation, investigating key aspects such as conditioning strategies and query point density.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何学习能够在多样化对象和交互动态中泛化的稳健视觉运动策略。  \n2. 现有方法如何忽视局部运动线索，影响接触丰富的操控精度。  \n3. 如何有效地结合3D场景流与动作生成以改善操控性能。  \n\n【用了什么创新的方案】  \n我们提出了3D Flow Diffusion Policy (3D FDP)，一种新颖的框架，通过场景级3D流作为结构化中间表示，捕捉细粒度的局部运动线索。该方法预测采样查询点的时间轨迹，并基于这些交互感知流条件生成动作，所有这些都在统一的扩散架构中共同实现。此设计使操控 grounded 在局部动态上，同时使策略能够推理动作的更广泛场景级后果。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout",
            "authors": "Kaixin Chai,Hyunjun Lee,Joseph J. Lim",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18671",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18671",
            "arxiv_html_link": "https://arxiv.org/html/2509.18671v1",
            "abstract": "In mobile manipulation, the manipulation policy has strong preferences for initial poses where it is executed. However, the navigation module focuses solely on reaching the task area, without considering which initial pose is preferable for downstream manipulation.\nTo address this misalignment, we introduce N2M, a transition module that guides the robot to a preferable initial pose after reaching the task area, thereby substantially improving task success rates. N2M features five key advantages: (1) reliance solely on ego-centric observation without requiring global or historical information; (2) real-time adaptation to environmental changes; (3) reliable prediction with high viewpoint robustness; (4) broad applicability across diverse tasks, manipulation policies, and robot hardware; and (5) remarkable data efficiency and generalizability.\nWe demonstrate the effectiveness of N2M through extensive simulation and real-world experiments.\nIn the PnPCounterToCab task, N2M improves the averaged success rate from 3% with the reachability-based baseline to 54%.\nFurthermore, in the Toybox Handover task, N2M provides reliable predictions even in unseen environments with only 15 data samples, showing remarkable data efficiency and generalizability.\nProject website: https://clvrai.github.io/N2M/",
            "introduction": "Mobile manipulators, which integrate mobility and environmental interaction capabilities, hold significant promise for a wide range of real-world applications.\nBy leveraging scene understanding Rana et al. (2023); Hughes et al. (2022); Rosinol et al. (2020) and navigation modules Zheng et al. (2025); Chai et al. (2024); Chang et al. (2023), these robots can reach the task area based on the task descriptions, and subsequently accomplish the task by executing pre-trained manipulation policies Fu et al. (2024); Chi et al. (2024); Black et al. (2024).\n\nHowever, existing works mainly focus on enhancing navigation and manipulation independently, while not giving sufficient attention to the interplay between them.\nIn this paper, we identify an inherent misalignment between navigation and manipulation, which significantly reduces the task success rate.\nSpecifically, due to factors such as joint limitation and training data distribution, the performance of the manipulation policy is sensitive to the initial pose from which execution begins.\nMeanwhile, navigation merely focuses on guiding the robot to task areas without considering which initial pose is preferable for executing the manipulation policy.\n\nThe most direct solution would be to develop an end-to-end model handling both navigation and manipulation Yang et al. (2024), thereby avoiding challenges in inter-module coordination. However, due to the inherent complexity of both navigation and manipulation, the design, training, and data collection for such end-to-end models remain an open problem.\nAn alternative approach within modular frameworks is to enhance the robustness of the manipulation policy. However, visuomotor policies are sensitive to viewpoint changes Heo et al. (2023), necessitating data collection from various initial poses throughout the task area Gu et al. (2022), which is costly.\n\nIn this paper, we propose a simple but effective transition module, named N2M (Navigation-to-Manipulation), serving as a bridge between navigation and manipulation. As depicted in Fig. 1, after reaching the task area, the robot is transferred from the end pose of navigation to an initial pose that is preferable for executing the manipulation policy, thereby improving the task success rate.\n\nWe identify five fundamental challenges for bridging navigation and manipulation, and propose our corresponding solutions.\n\nAdaptability to non-static environments.\nThe environments are typically non-static, requiring predictions to adapt to environmental changes. To support this, N2M predicts the preferable initial pose from the ego-centric RGB point cloud with a single forward pass. This efficient design enables N2M to generate real-time predictions in dynamic environments, as demonstrated in Section 5.2.\n\nMulti-modality of preferable initial poses.\nMultiple preferable initial poses may exist within the task area. Consequently, predicting a single pose is insufficient, as it can cause the model to learn an interpolation between viable poses Bishop (1994), which may not be preferable to execute manipulation policies. To address this multi-modality, N2M predicts the distribution of preferable initial poses, which is represented with a Gaussian Mixture Model (GMM) Bahl et al. (2023).\n\nCriterion of preferable initial poses.\nManipulation performance depends on multiple factors: policy architecture, training data distribution, robot configuration, task, and environment. Rather than attempting to model these complex relationships, we directly evaluate the pose through policy rollouts.\nDuring data collection, we position the robot at various poses and execute the manipulation policy, and successful execution indicates a preferable initial pose.\nLearning initial pose preferences directly from policy rollouts ensures that N2M’s predictions align with the policy’s actual performance while simultaneously enabling broad applicability across diverse policies, tasks, and robot hardware, as shown in Sections 4.2, 5.1, and 5.2.\n\nViewpoint Robustness.\nSince the robot navigation end poses can be anywhere within the task area, N2M needs to provide reliable predictions at various viewpoints. To achieve this, we augment N2M’s training data from multiple viewpoints. Experiments in Sections 4 and 5 demonstrate that N2M reliably predicts preferable initial poses across the whole task area. Interestingly, we note that our proposed data augmentation approach also significantly improves data efficiency and generalizability. We will further analyze the reason behind these benefits in Section 6.\n\nData Efficiency.\nCollecting rollouts requires substantial time and human effort, as each rollout must be monitored and manually labeled with success or failure.\nWe incorporate two main strategies to make N2M data-efficient:\nFirst, we design the module to directly predict the initial pose distribution, rather than low-level action Lee et al. (2019);\nSecond, we augment the dataset through viewpoint rendering to increase its coverage and diversity. In Sections 4.3, 4.4, and 5, we demonstrate that N2M has remarkable data efficiency and generalizability.\n\nOur contributions are as follows:\n\nFirst, we identify a critical misalignment between navigation and manipulation modules and introduce N2M, which predicts preferable initial poses on the fly from ego-centric observations.\n\nSecond, we propose learning policy initial pose preferences from rollouts, without making additional assumptions about tasks, policies, and hardware, thereby endowing N2M with broad applicability.\n\nThird, we achieve remarkable data efficiency and generalizability through a novel data augmentation approach combined with our carefully designed input-output architecture.\n\nFinally, we conduct extensive experiments validating the effectiveness of our proposed N2M module across various settings and release our code to facilitate community exploration.",
            "llm_summary": "【关注的是什么问题】  \n1. 现有导航和操作模块之间的协调不足，导致任务成功率低。  \n2. 操作策略对初始姿态的敏感性未被导航模块考虑。  \n3. 数据收集和训练过程中的高成本和低效率问题。  \n\n【用了什么创新的方案】  \nN2M（Navigation-to-Manipulation）模块作为导航与操作之间的桥梁，实时预测可执行操作的初始姿态。它依赖于自我中心的RGB点云进行高效预测，采用高斯混合模型表示多种可行姿态，直接从策略回放中学习初始姿态偏好，显著提高数据效率和泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Distributionally Robust Safe Motion Planning with Contextual Information",
            "authors": "Kaizer Rahaman,Simran Kumari,Ashish R. Hota",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18666",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18666",
            "arxiv_html_link": "https://arxiv.org/html/2509.18666v1",
            "abstract": "We present a distributionally robust approach for collision avoidance by incorporating contextual information. Specifically, we embed the conditional distribution of future trajectory of the obstacle conditioned on the motion of the ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional kernel mean embedding operator. Then, we define an ambiguity set containing all distributions whose embedding in the RKHS is within a certain distance from the empirical estimate of conditional mean embedding learnt from past data. Consequently, a distributionally robust collision avoidance constraint is formulated, and included in the receding horizon based motion planning formulation of the ego agent. Simulation results show that the proposed approach is more successful in avoiding collision compared to approaches that do not include contextual information and/or distributional robustness in their formulation in several challenging scenarios.",
            "introduction": "Safe motion planning in uncertain and dynamic environments is a fundamental challenge in the field of robotics and autonomous systems [1, 2]. Past works have proposed a plethora of approaches, including sampling-based methods [3], collision avoidance using velocity obstacles [4], and optimization-based techniques [5, 6]. In recent years, optimal control with a receding horizon implementation, popularly known as model predictive control (MPC), have gained popularity as it enables the designer to systematically include the effects of robot dynamics, constraints on robot states and inputs as well as collision avoidance constraints while planning the motion of the robot.\n\nWhile early works on MPC based motion planning focused on avoiding static obstacles [7, 8, 6], more recent works have considered dynamic obstacles in the robust and stochastic MPC frameworks. While robust MPC tends to result in conservative solutions, stochastic MPC techniques formulate collision avoidance conditions in terms of probabilistic constraints or via suitable risk measures [9]. However, the safety guarantees provided by the above techniques may not hold when the probability distribution of the future obstacle position is unknown and/or time-varying.\n\nConsequently, several recent works have proposed distributionally robust motion planning techniques where the stochastic collision avoidance constraints hold for an entire family of distributions (or ambiguity set) that are close (in terms of the Wasserstein distance) to the empirical distribution constructed from the observed data [10, 11, 12, 13, 14]. However, not all available past data are equally relevant at a given context or scenario. In particular, when we consider multiple agents that share the same environment, and are in the vicinity of each other, the future position of the obstacle is a function of the future position of the ego agent. Therefore, in order to obtain more accurate solutions, it is essential to consider ambiguity sets that depend on the actions of the ego agent and other available contextual information. Despite its significance, there is no prior work which considers decision-dependent ambiguity sets or any other contextual information while formulating the distributionally robust safe motion planning problem.\n\nA few recent works have explicitly included the behavior of the ego vehicle for trajectory prediction. Specifically, [15] investigated the impact of ego vehicle planning on nearby vehicles’ trajectories using an LSTM-based encoder combined with a convolutional social pooling module. The authors in [16] modeled each vehicle as a wave characterized by amplitude and phase, proposing that wave-pooling better captures dynamic states and high-order interactions through wave superposition. More recently, [17] provided a detailed comparison of several contextual trajectory prediction techniques. However, the robustness of neural network based trajectory prediction to distribution shifts is not adequately explored in the literature. Neural network based techniques also do not provide any rigorous (e.g., finite-sample) guarantees on the probability of collision among vehicles.\n\nIn this paper, we aim to fill this research gap. First, we compute the empirical estimate of the conditional kernel mean embedding (CKME) [18] of the (conditional) distribution of the future obstacle position as a function of the (i.e., conditioned on the) current states and the predicted trajectory of the ego agent. Then, we formulate an optimal control problem that embeds the ego-conditioned predicted trajectory in the constraints. In particular, the CKME operator provides a closed-form expression of the future trajectory of other vehicles as a function of the states and inputs of the ego vehicle, i.e., the decision variables of the optimal control problem. In order to robustify against distribution shifts, the conditional value at risk of the collision avoidance constraint is required to hold for all distributions whose mean embeddings are within a specified maximum-mean discrepancy (MMD) distance from the empirical estimate of the CKME; a similar approach was recently examined in [19] in the context of optimal control. Another recent paper [20] explored RKHS to select probable trajectories and adopted a sampling-based optimization approach, which is distinct from our approach. We present a tractable approximation of the above constraints following the reformulations developed in [21, 22] such that off-the-shelf nonlinear optimization solvers, such as IPOPT, can be deployed to solve the optimal control problem.111Neural network based predictors do not provide a simple closed form expression of the predicted trajectories as a function of the input applied to the ego vehicle. Therefore, computing optimal control inputs with predicted trajectories used in the constraints is not straightforward.  We provide detailed simulation results involving an autonomous ground vehicle moving on a road with obstacle agent(s), and show that the proposed approach successfully avoids collision in several challenging scenarios.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在不确定和动态环境中实现安全的运动规划。  \n2. 如何有效利用上下文信息来增强碰撞避免的鲁棒性。  \n3. 如何处理未来障碍物位置的概率分布未知或时变的情况。  \n\n【用了什么创新的方案】  \n本文提出了一种基于条件核均值嵌入（CKME）的分布鲁棒运动规划方法，通过将障碍物未来轨迹的条件分布嵌入到再生核希尔伯特空间（RKHS）中，构建了一个依赖于自我代理动作的模糊集合。通过优化控制问题，确保在所有与CKME的经验估计保持在特定距离内的分布下，碰撞避免约束的条件值风险得以满足。该方法在多个挑战场景中展现出优越的碰撞避免能力。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer",
            "authors": "Yarden As,Chengrui Qu,Benjamin Unger,Dongho Kang,Max van der Hart,Laixi Shi,Stelian Coros,Adam Wierman,Andreas Krause",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18648",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18648",
            "arxiv_html_link": "https://arxiv.org/html/2509.18648v1",
            "abstract": "Safety remains a major concern for deploying reinforcement learning (RL) in real-world applications. Simulators provide safe, scalable training environments, but the inevitable sim-to-real gap introduces additional safety concerns, as policies must satisfy constraints in real-world conditions that differ from simulation. To address this challenge, robust safe RL techniques offer principled methods, but are often incompatible with standard scalable training pipelines. In contrast, domain randomization, a simple and popular sim-to-real technique, stands out as a promising alternative, although it often results in unsafe behaviors in practice. We present SPiDR, short for Sim-to-real via Pessimistic Domain Randomization—a scalable algorithm with provable guarantees for safe sim-to-real transfer. SPiDR uses domain randomization to incorporate the uncertainty about the sim-to-real gap into the safety constraints, making it versatile and highly compatible with existing training pipelines. Through extensive experiments on sim-to-sim benchmarks and two distinct real-world robotic platforms, we demonstrate that SPiDR effectively ensures safety despite the sim-to-real gap while maintaining strong performance.",
            "introduction": "Reinforcement learning (RL) has made significant strides in recent years, demonstrating remarkable progress across a range of domains. These include achieving superhuman capabilities in games (Mnih et al., 2015; Silver et al., 2016), fine-tuning large language models (Ouyang et al., 2022), advancing applications in healthcare (Fox et al., 2020; Zhu et al., 2020), robotics (Lee et al., 2020; Degrave et al., 2022; Lin et al., 2025) and autonomous driving (Cusumano-Towner et al., 2025; Cornelisse et al., 2025).\nYet despite these achievements, ensuring safety and preventing harmful behaviors remains a critical challenge and a prerequisite for unlocking the full potential of RL as a ubiquitous element in everyday life (Amodei et al., 2016; Gu et al., 2022).\n\nThe use of simulators has been a key component behind the success of many of the mentioned applications  (Visentin et al., 2014; Makoviychuk et al., 2021; Degrave et al., 2022; Kazemkhani et al., 2024). Training in simulation allows agents to learn from unsafe interactions, which in reality would lead to catastrophic outcomes. In addition, learning complex behaviors fully online can be prohibitively time-consuming. Modern simulators accelerate training, reducing hours of real-world experience to minutes on consumer-grade GPUs (Rudin et al., 2022). However, while being a major driver in the development of the above examples, even state-of-the-art simulators often fall short in precisely mirroring the real-world. Indeed, “all models are wrong” (Box, 1976)—the so-called sim-to-real gap can make simulation-trained policies violate real-world constraints, which can be particularly dangerous in high-stakes settings where safety must be guaranteed on first contact.\n\nExisting literature to address this challenge often relies on tools from robust optimization (Queeney and Benosman, 2024; Kitamura et al., 2024; Zhang et al., 2024). While being theoretically grounded, such methods typically require practitioners to significantly alter their existing training pipelines, rendering them less prevalent in practice. In contrast, due to its simplicity, domain randomization has become the de facto tool for sim-to-real transfer (Tobin et al., 2017; Peng et al., 2018; Lee et al., 2020; Degrave et al., 2022). Despite its success, in problems that require adherence to safety constraints, domain randomization lacks safety guarantees and often fails to satisfy the constraints in practice (cf. Queeney and Benosman, 2024, and Figure˜3). Therefore, a method that provably guarantees safe sim-to-real transfer, while being highly compatible with standard training practices, is still missing.\n\nIn this work, we address this gap by presenting a simple method that builds on domain randomization while ensuring safety under sim-to-real transfer. We theoretically show that unsafe transfer can be associated with large uncertainty about the sim-to-real gap, quantified as the disagreement among next-state predictions from domain-randomized dynamics models. This key idea is illustrated in Figure˜1, where spikes in uncertainty (e.g. at t=4.6t=4.6 and t=5.3t=5.3) coincide with unstable or unsafe behaviors, such as stumbling or flipping. Motivated by this insight, we propose to penalize the cost with the uncertainty to achieve safe sim-to-real transfer, leading to the design of SPiDR. Notably, SPiDR seamlessly integrates with state-of-the-art RL algorithms (Schulman et al., 2017; Haarnoja et al., 2019), delivering strong empirical performance on both in simulated and real-world safe RL tasks while ensuring constraint satisfaction, even under severe model mismatch.\n\nWe address an important challenge to real-world adoption of RL: zero-shot safe sim-to-real transfer, where an agent must learn a safe and effective policy using only simulated interactions. We propose SPiDR, a practical algorithm with formal safety guarantees that integrates easily into popular sim-to-real pipelines.\n\nWe address an important challenge to real-world adoption of RL: zero-shot safe sim-to-real transfer, where an agent must learn a safe and effective policy using only simulated interactions. We propose SPiDR, a practical algorithm with formal safety guarantees that integrates easily into popular sim-to-real pipelines.\n\nWe validate SPiDR on two real-world robotic platforms, where it achieves zero-shot constraint satisfaction, substantially outperforming other baselines in terms of safety and performance. These results provide empirical evidence that our theoretical guarantees translate to the real-world, suggesting that SPiDR can be safely used in real-world deployment.\n\nFinally, we extensively evaluate SPiDR on well established simulated continuous control benchmarks, including the RWRL benchmark (Dulac-Arnold et al., 2020), Safety Gym (Ray et al., 2019) and RaceCar environments (Kabzan et al., 2020), where SPiDR consistently satisfies safety constraints while achieving strong task performance.\n\n1. We address an important challenge to real-world adoption of RL: zero-shot safe sim-to-real transfer, where an agent must learn a safe and effective policy using only simulated interactions. We propose SPiDR, a practical algorithm with formal safety guarantees that integrates easily into popular sim-to-real pipelines.\n\n2. We validate SPiDR on two real-world robotic platforms, where it achieves zero-shot constraint satisfaction, substantially outperforming other baselines in terms of safety and performance. These results provide empirical evidence that our theoretical guarantees translate to the real-world, suggesting that SPiDR can be safely used in real-world deployment.\n\n3. Finally, we extensively evaluate SPiDR on well established simulated continuous control benchmarks, including the RWRL benchmark (Dulac-Arnold et al., 2020), Safety Gym (Ray et al., 2019) and RaceCar environments (Kabzan et al., 2020), where SPiDR consistently satisfies safety constraints while achieving strong task performance.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在sim-to-real转移中确保安全性，尤其是零-shot情况下。  \n2. 现有的安全强化学习方法在实际应用中往往不兼容标准训练流程。  \n3. 域随机化在满足安全约束方面的不足。  \n\n【用了什么创新的方案】  \nSPiDR（Sim-to-real via Pessimistic Domain Randomization）是一种新颖的算法，通过将对sim-to-real差距的不确定性纳入安全约束，确保在模拟到现实的转移中实现安全性。该方法与现有的强化学习算法无缝集成，能够在模拟和现实世界中有效地满足安全约束，同时保持强大的任务性能。SPiDR的设计使其在保持高兼容性的同时，提供了形式化的安全保证。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Do You Need Proprioceptive States in Visuomotor Policies?",
            "authors": "Juntu Zhao,Wenbo Lu,Di Zhang,Yufeng Liu,Yushen Liang,Tianluo Zhang,Yifeng Cao,Junyuan Xie,Yingdong Hu,Shengjie Wang,Junliang Guo,Dequan Wang,Yang Gao",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Project page:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18644",
            "code": "https://statefreepolicy.github.io",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18644",
            "arxiv_html_link": "https://arxiv.org/html/2509.18644v1",
            "abstract": "Imitation-learning–based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0% to 85% in height generalization and from 6% to 64% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment.",
            "introduction": "Imitation-learning-based visuomotor policies [1, 2, 3, 4, 5] have been widely used in robotic manipulation.\nLeveraging large-scale demonstration datasets [6, 7, 8, 9] and fine-tuning powerful pre-trained policies have enabled robots to achieve remarkable performance across diverse real-world tasks.\n\nFor precise and reliable control, these visuomotor policies typically incorporate not only visual observations of the task environment but also proprioceptive state (hereafter referred to as state) inputs [10, 3, 11], such as end-effector poses and joint angles. The state inputs provide compact and accurate information about the robot’s configuration, but they also make the policy prone to overfitting by simply memorizing the training trajectories.\nTherefore it severely limits spatial generalization [12, 13, 14] if the training data lacks diversity [15].\nIn today’s context, where collecting demonstration data with wide state coverage (i.e., diverse spatial locations of task-relevant objects) is prohibitively expensive, this has become a critical bottleneck for the development of visuomotor policies.\n\nIn this study, we propose to completely remove the state input in visuomotor policies to enhance their spatial generalization ability, hereafter referred to as “State-free Policies.”\nThis design is built upon two conditions:\n\nRelative end-effector (EEF) action space [16]: The visuomotor policies predict relative displacements of the end-effector based on the current observation. Among different action spaces, the relative EEF action space most naturally supports the generalization of policies.\n\nFull task observation: Another key condition for effective State-free Policies is to ensure sufficient task-relevant visual information, which we term “full task observation”. This enables visuomotor policies to fully “see” the task-relevant objects in the task.\n\nThis mechanism of State-free Policies forces the policy to develop a deeper understanding of the task environment rather than simply memorizing the trajectories, thereby enabling State-free Policies to achieve advantages that state-based policies cannot provide:\n\nSpatial Generalization: Since State-free Policies do not rely on state inputs, they avoid overfitting to the training trajectories.\nTherefore, they exhibit strong height and horizontal generalization abilities, where height refers to variations of the task-relevant object’s location in the vertical direction, and horizontal refers to variations of the object’s location in the 2D plane.\n\nData efficiency: Even in in-domain settings, state-based policies require diverse demonstrations to avoid overfitting to specific trajectories. In contrast, removing the state input eliminates this dependence on trajectory diversity, allowing State-free Policies to be fine-tuned with less demonstration data. This reduces the cost of data collection, which is often a major bottleneck in deploying real-world robots.\n\nCross-embodiment adaptation: Since State-free Policies rely only on visual inputs and predict actions in the relative EEF space, they exhibit stronger cross-embodiment adaptation ability than state-based policies. They do not require additional adaptation to different state spaces, so the same task can be easily adapted to new embodiments with fewer fine-tuning steps.\n\nCross-embodiment adaptation: Since State-free Policies rely only on visual inputs and predict actions in the relative EEF space, they exhibit stronger cross-embodiment adaptation ability than state-based policies. They do not require additional adaptation to different state spaces, so the same task can be easily adapted to new embodiments with fewer fine-tuning steps.\n\nWe have conducted extensive experiments across a diverse range of tasks, robot embodiments, and policy architectures.\nIn both real-world and simulation environments, State-free Policies achieve comparably great in-domain performance to state-based policies.\nMost importantly, when trained on strictly collected real-world demonstration data\n(i.e., the task-relevant object location has a constrained initial distribution range),\nState-free Policies exhibit significantly stronger spatial generalization ability than state-based policies.\nFor further benefits, e.g., data efficiency and cross-embodiment adaptation ability, they also demonstrate obvious advantages over state-based policies, highlighting their potential for scalable and practical deployment in real-world robotic systems.\n\n1. Relative end-effector (EEF) action space [16]: The visuomotor policies predict relative displacements of the end-effector based on the current observation. Among different action spaces, the relative EEF action space most naturally supports the generalization of policies.\n\n2. Full task observation: Another key condition for effective State-free Policies is to ensure sufficient task-relevant visual information, which we term “full task observation”. This enables visuomotor policies to fully “see” the task-relevant objects in the task.\n\n1. Spatial Generalization: Since State-free Policies do not rely on state inputs, they avoid overfitting to the training trajectories.\nTherefore, they exhibit strong height and horizontal generalization abilities, where height refers to variations of the task-relevant object’s location in the vertical direction, and horizontal refers to variations of the object’s location in the 2D plane.\n\n2. Data efficiency: Even in in-domain settings, state-based policies require diverse demonstrations to avoid overfitting to specific trajectories. In contrast, removing the state input eliminates this dependence on trajectory diversity, allowing State-free Policies to be fine-tuned with less demonstration data. This reduces the cost of data collection, which is often a major bottleneck in deploying real-world robots.\n\n1. Cross-embodiment adaptation: Since State-free Policies rely only on visual inputs and predict actions in the relative EEF space, they exhibit stronger cross-embodiment adaptation ability than state-based policies. They do not require additional adaptation to different state spaces, so the same task can be easily adapted to new embodiments with fewer fine-tuning steps.",
            "llm_summary": "【关注的是什么问题】  \n1. 过度依赖proprioceptive状态输入导致策略对训练轨迹的过拟合。  \n2. 现有的visuomotor策略在空间泛化能力上存在严重限制。  \n3. 数据收集的多样性不足成为开发visuomotor策略的瓶颈。  \n\n【用了什么创新的方案】  \n提出了一种“无状态策略”（State-free Policy），完全去除proprioceptive状态输入，仅基于视觉观察预测动作。这种策略在相对末端执行器动作空间中构建，确保充分的任务相关视觉信息，从而增强空间泛化能力。实验结果表明，该策略在多种真实世界任务中表现出显著的空间泛化能力、数据效率和跨躯体适应性，提升了实际部署的可行性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments",
            "authors": "Yuan Zhou,Jialiang Hou,Guangtong Xu,Fei Gao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18636",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18636",
            "arxiv_html_link": "https://arxiv.org/html/2509.18636v1",
            "abstract": "Formation maintenance with varying number of drones in narrow environments hinders the convergence of planning to the desired configurations. To address this challenge, this paper proposes a formation planning method guided by Deformable Virtual Structures (DVS) with continuous spatiotemporal transformation. Firstly, to satisfy swarm safety distance and preserve formation shape filling integrity for irregular formation geometries, we employ Lloyd algorithm for uniform P​A¯\\underline{PA}rtitioning and Hungarian algorithm for A​S¯\\underline{AS}signment (PAAS) in DVS. Subsequently, a spatiotemporal trajectory involving DVS is planned using primitive-based path search and nonlinear trajectory optimization. The DVS trajectory achieves adaptive transitions with respect to a varying number of drones while ensuring adaptability to narrow environments through affine transformation. Finally, each agent conducts distributed trajectory planning guided by desired spatiotemporal positions within the DVS, while incorporating collision avoidance and dynamic feasibility requirements. Our method enables up to 15% of swarm numbers to join or leave in cluttered environments while rapidly restoring the desired formation shape in simulation. Compared to cutting-edge formation planning method, we demonstrate rapid formation recovery capacity and environmental adaptability. Real-world experiments validate the effectiveness and resilience of our formation planning method.",
            "introduction": "In recent years, formation flight becomes the foundation requirement for aerial swarms in practical applications, such as collaborative exploration [1], light show [2], search and rescue [3]. For large-scale swarms [4], [5], formation inevitably encounters agent loss in narrow environments [6, 7, 8]. Naturally, the integration of new members can restore the impaired formation and enhance the efficiency of mission execution [9], [10]. Therefore, dynamic adaptation of formation reconfiguration for changes in the number of drones is essential to ensure operational resilience under unforeseen disturbance. Besides, formation needs to experience adaptively overall shape transformations to navigate through narrow environments and maintain its configuration as much as possible to keep resilience [11, 12, 13, 14].\n\nWhen the number of drones in formations varies, the abrupt variation of formation cooperative constraints tends to degrade the feasibility of trajectory optimization. In narrow environments, the inherent conflict between formation maintenance and collision avoidance also deteriorates the feasibility, resulting in no solution for the classical planning method without global formation adjustment mechanism.\n\nNumerous works have demonstrated formation systems with swarm number variations.\nArtificial Potential Fields (APF) [4], [8], [15] are commonly used to guide the formation generation with variable swarm numbers. This approach eliminates the need for explicit assignment of desired positions, enabling adaptive number variation of formation. However, the slow convergence rate and inadequate swarm collision avoidance hinder its applicability in real-time formation planning. Although explicit position and assignment during number variations enable rapid formation recovery [16], [17], [18], they often fail to guarantee robust performance in narrow environments, where collision avoidance remains challenging during formation recovery.\nAdaptive deformation of the formation shape is widely adopted for safe and rapid navigation through obstacle environments [11], [13]. Nevertheless, distributed cooperative planning is insufficient without effective guidance for formation in narrow environments. Consequently, adjusting the overall deformation of the swarm provides an effective resolution [11]. However, this category of approaches is limited by either locally reactive mechanisms [13] and simply adjusts the local target that hinders formation guidance and environmental adaptability [11], or an excessive pursuit of collision-free passage that severely compromises formation maintenance integrity while neglecting post-maneuver recovery capability. These methods often struggle to rapidly restore desired formations in cluttered environments. Furthermore, most studies of formation planning do not address adaptive adjustment with variations in swarm number [11], [19], [20].\n\nTo achieve real-time formation flight with a variable number of drones, we propose a number adaptive formation planning system. According to the desired formation shape, we employ explicit partitioning and assignment to designate formation positions for the swarm. We focus on designing a Deformable Virtual Structures (DVS) trajectory with spatiotemporal motion and deformation that can effectively guide formation recovery and maintenance. Specifically, we introduce virtual rigid structures with two additional degrees of freedom, including scaling and affine transformation in DVS. The concept of virtual structures has been applied in multiple fields, but predominantly uses rigid virtual structures which are unable to adaptively deform [21, 22, 23]. In this work, we design a spatiotemporal trajectory optimization method for DVS, which utilizes compact piecewise polynomial representations in Cartesian and deformable spaces to formulate nonlinear optimization problems. This enables adaptive adjustment of the scale of formations according to swarm number variations, while permitting appropriate affine deformation of the virtual structures to facilitate the formations to navigate in narrow environments. Integrating PAAS and DVS with spatiotemporal transformation, we designate this framework as Deformation Guidance (DG), and agents perform distributed trajectory planning under the guidance of DG. Extensive simulation and real-world experiments demonstrate that our method supports variable number formation planning while achieving a high success rate in navigating through narrow environments.\n\nThe contributions of this paper are as follows:\n\nTo satisfy swarm safety distance and preserve shape filling integrity for irregular formation geometries, we implement the Lloyd algorithm for uniform partitioning in formation planning with variable number, with assignment via the Hungarian algorithm. We experimentally verified that the computational efficiency of PAAS is sufficient to achieve real-time planning.\n\nTo satisfy swarm safety distance and preserve shape filling integrity for irregular formation geometries, we implement the Lloyd algorithm for uniform partitioning in formation planning with variable number, with assignment via the Hungarian algorithm. We experimentally verified that the computational efficiency of PAAS is sufficient to achieve real-time planning.\n\nWe propose a spatiotemporal trajectory optimization method for DVS with scaling and affine transformation to guide formation planning in narrow environments. We employ a primitive path search to efficiently obtain high-dimensional DVS paths as initial values for DVS trajectory optimization, which can be transformed into unconstrained optimization problems for real-time solution.\n\nWe incorporate DVS-based spatiotemporal formation constraint in distributed trajectory optimization to achieve rapid formation recovery and enhanced environmental adaptability. Extensive experiments and benchmarks validate the efficiency and advantages of our system.",
            "llm_summary": "【关注的是什么问题】  \n1. 变动数量的无人机在狭窄环境中的编队维护问题。  \n2. 传统规划方法在动态调整编队形状和避免碰撞方面的局限性。  \n3. 如何实现实时的编队飞行以适应无人机数量的变化。  \n\n【用了什么创新的方案】  \n本文提出了一种基于可变形虚拟结构（DVS）的编队规划方法，通过连续的时空变换来指导编队恢复和维护。采用Lloyd算法进行均匀划分，并结合匈牙利算法进行分配，以满足安全距离和形状完整性要求。设计了一种具有缩放和仿射变换能力的DVS轨迹优化方法，能够在狭窄环境中有效导航。通过将DVS与时空变换相结合，支持动态的编队调整，确保在复杂环境中快速恢复所需的编队形状。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training",
            "authors": "Shuo Cheng,Liqian Ma,Zhenyang Chen,Ajay Mandlekar,Caelan Garrett,Danfei Xu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18631",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18631",
            "arxiv_html_link": "https://arxiv.org/html/2509.18631v1",
            "abstract": "Behavior cloning has shown promise for robot manipulation, but real-world demonstrations are costly to acquire at scale. While simulated data offers a scalable alternative, particularly with advances in automated demonstration generation, transferring policies to the real world is hampered by various simulation and real domain gaps. In this work, we propose a unified sim-and-real co-training framework for learning generalizable manipulation policies that primarily leverages simulation and only requires a few real-world demonstrations. Central to our approach is learning a domain-invariant, task-relevant feature space. Our key insight is that aligning the joint distributions of observations and their corresponding actions across domains provides a richer signal than aligning observations (marginals) alone. We achieve this by embedding an Optimal Transport (OT)-inspired loss within the co-training framework, and extend this to an Unbalanced OT framework to handle the imbalance between abundant simulation data and limited real-world examples. We validate our method on challenging manipulation tasks, showing it can leverage abundant simulation data to\nachieve up to a 30% improvement in the real-world success rate\nand even generalize to scenarios seen only in simulation.",
            "introduction": "Behavior cloning pomerleau1988alvinn  is a promising approach for acquiring robot manipulation skills directly in the real world, due to its simplicity and effectiveness in mimicking expert demonstrations robomimic2021 ; florence2022implicit . However, achieving robust and generalizable performance requires collecting large-scale datasets khazatsky2024droid ; open_x_embodiment_rt_x_2023  across diverse environments, object configurations, and tasks. This data collection process is labor-intensive, time-consuming, and costly, posing significant challenges to scalability in real-world applications.\n\nRecently, with rapid advancements in physics simulators Genesis ; Xiang_2020_SAPIEN , procedural scene generation raistrick2024infinigen ; deitke2022️ , and motion synthesis techniques mandlekar2023mimicgen ; cheng2023nod , there has been growing interest in leveraging simulation as an alternative source of training data. These simulation-based approaches enable scalable and controllable data generation, allowing for diverse and abundant supervision at a fraction of the real-world cost. However, transferring policies trained in simulation to the physical world remains a non-trivial challenge due to sim-to-real gap—the discrepancies between the simulated and real-world environments that a policy encounters during execution. These differences can manifest in various forms, such as variations in visual appearance, sensor noise, and action dynamics andrychowicz2020learning ; tobin2017domain . In particular, learning visuomotor control policies that remain robust under changing perceptual conditions during real-world deployment continues to be an open area of research.\n\nCommon strategies to bridge this domain gap include domain randomization andrychowicz2020learning ; tobin2017domain  and data augmentation hansen2020self ; yarats2021mastering , though these often require careful tuning. Domain adaptation (DA) techniques aim to explicitly align distributions, either at pixel bousmalis2017unsupervised ; james2019sim  or feature levels tzeng2014deep ; long2015learning ; zhao2019learning . However, many feature-level methods align only marginal observation distributions (e.g., MMD tzeng2014deep ; long2015learning ), which can be insufficient for fine-grained manipulation alignment as it may not preserve action-relevant relationships across domains. More recently, sim-and-real co-training—simply training a single policy on mixed data from both domains wei2025empirical ; maddukuri2025sim —has shown surprising effectiveness. We argue that while beneficial for data diversity, such co-training approaches typically lack explicit constraints for feature space alignment across domains, potentially hindering optimal transfer and generalization because they don’t enforce a consistent mapping of task-relevant structures.\n\nWe present a unified sim-and-real co-training framework that explicitly learns a shared latent space where observations from simulation and the real world are aligned and preserve action-relevant information. Our key insight is that aligning the joint distributions of observations and their corresponding actions or task-relevant states across domains provides a direct signal for learning transferable features.\nConcretely, we leverage Optimal Transport (OT) courty2016optimal  as an alignment objective to learn representations where the geometric relationships crucial for action prediction are consistent, irrespective of whether the input comes from simulation or the real world. Further more, to robustly handle the data imbalance in co-training with abundant simulation data and limited real-world data, we further extend to an Unbalanced OT (UOT) formulation fatras2021unbalanced ; chizat2018scaling  and develop a temporally-aware sampling strategy to improve domain alignment learning in a mini-batch OT setting.\n\nOur contributions are: (1) a sim-and-real co-training framework that learns a domain-invariant yet task-salient latent space to improve real-world performance with abundant simulation data, (2) an Unbalanced Optimal Transport framework and temporally-aware sampling strategy to mitigate data imbalance and improve alignment quality in mini-batch OT training, (3) comprehensive experiments using both image and point-cloud modalities, evaluating sim-to-sim and sim-to-real transfer across diverse manipulation tasks, demonstrating up to a 30% average success rate improvement and achieving generalization to real-world scenarios for which the training data only appears in simulation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效地将模拟数据中的策略转移到真实世界中。  \n2. 如何处理模拟与真实世界之间的域间差距。  \n3. 如何在有限的真实世界示例下利用丰富的模拟数据进行训练。  \n\n【用了什么创新的方案】  \n提出了一种统一的sim-and-real共训练框架，通过学习一个域不变且任务相关的特征空间，来提高机器人操作策略的可迁移性。核心思想是对观察和对应动作的联合分布进行对齐，利用Optimal Transport (OT)作为对齐目标，确保无论输入来自模拟还是现实，几何关系在动作预测中保持一致。此外，扩展到Unbalanced OT框架以处理数据不平衡，并开发了时间感知采样策略，以改善小批量OT训练中的域对齐学习。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving",
            "authors": "Jay Patrikar,Apoorva Sharma,Sushant Veer,Boyi Li,Sebastian Scherer,Marco Pavone",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "8 pages, 5 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.18626",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18626",
            "arxiv_html_link": "https://arxiv.org/html/2509.18626v1",
            "abstract": "Learning-based autonomous driving systems are trained mostly on incident-free data, offering little guidance near safety–performance boundaries. Real crash reports contain precisely the contrastive evidence needed, but they are hard to use: narratives are unstructured, third-person, and poorly grounded to sensor views. We address these challenges by normalizing crash narratives to ego-centric language and converting both logs and crashes into a unified scene–action representation suitable for retrieval. At decision time, our system adjudicates proposed actions by retrieving relevant precedents from this unified index; an agentic counterfactual extension proposes plausible alternatives, retrieves for each, and reasons across outcomes before deciding. On a nuScenes benchmark, precedent retrieval substantially improves calibration, with recall on contextually preferred actions rising from 24% to 53%. The counterfactual variant preserves these gains while sharpening decisions near risk.",
            "introduction": "End-to-end learning-based autonomous vehicle (AV) systems are trained primarily through imitation learning on positive, incident-free driving data [1, 2].\nThis data is typically collected by expert human drivers driving sensor-instrumented vehicles in a variety of driving scenarios, resulting in a dataset pairing the sensor observations that the AV will encounter with the action that the human driver chose in that moment.\nWhile this data helps define “good” driving that an AV should imitate, it does not provide direct supervision of what behaviors are to be avoided.\nSome have aimed to address this gap through auxiliary reward functions defining a rules-based definition of risky driving [3], but such rules can be challenging to specify: Risk is difficult to quantify due to uncertainty over other road user’s behaviors. Moreover, competent driving requires appropriately managing the risk that is taken on to make progress; remaining stopped is the safest policy, but not competent driving behavior.\n\nInstead, in this work, we consider an alternative data-driven approach to provide negative supervision for AV decision making. Specifically, we explore the use of crash reports as a complementary source of driving knowledge.\nAgencies such as the National Highway Traffic Safety Administration (NHTSA) collect structured narrative accounts of real-world accidents, including the actions taken and the conditions under which failures occurred. While these reports lack the rich multimodal data of first-person human-driven AV logs, they contain valuable causal and contextual information that can support counterfactual reasoning. While these reports can’t directly be used in policy training, recent advances in vision-language models (VLMs) capable of reasoning across sensor and text domains offer a compelling avenue for bringing such valuable sources of negative data into AV decision making.\n\nIn this paper, we study how negative data influences VLM reasoning capabilities in AV decision making tasks by developing a retrieval-augmented-generation (RAG) pipeline for AV safety adjudication. Specifically our contributions are as follows:\n(i) a GraphRAG [4] style retrieval pipeline for both positive and negative driving precedent, using a unified structured language representation for both sensor-domain positive data and language-domain negative data; (ii) an agentic extension which uses additional test-time compute to reason about counterfactuals prior to making a safety judgment; (iii) evaluation of both approaches in terms of alignment with human judgement on the safety of possible actions in driving scenarios, showing the impact of negative crash report data on VLM decision making capability.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用负数据（如事故报告）来改善自动驾驶系统的决策能力。  \n2. 如何将非结构化的事故叙述转换为可用于自动驾驶的结构化信息。  \n3. 如何在决策时使用检索机制来提高安全性判断的准确性。  \n\n【用了什么创新的方案】  \n本研究提出了一种基于检索增强生成（RAG）管道的方法，利用事故报告作为负监督数据来提升自动驾驶决策的安全性。通过将正负驾驶先例统一为结构化语言表示，构建了一个GraphRAG风格的检索管道。同时，增加了一个代理扩展，在决策前进行反事实推理，以提高与人类判断的一致性，并评估了负数据对视觉语言模型（VLM）决策能力的影响。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones",
            "authors": "Maximilian Adang,JunEn Low,Ola Shorinwa,Mac Schwager",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18610",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18610",
            "arxiv_html_link": "https://arxiv.org/html/2509.18610v1",
            "abstract": "Large vision-language models have driven remarkable progress in open-vocabulary robot policies, e.g., generalist robot manipulation policies, that enable robots to complete complex tasks specified in natural language. Despite these successes, open-vocabulary autonomous drone navigation remains an unsolved challenge due to the scarcity of large-scale demonstrations, real-time control demands of drones for stabilization, and lack of reliable external pose estimation modules.\nIn this work,\nwe present SINGER for language-guided autonomous drone navigation in the open world using only onboard sensing and compute. To train robust, open-vocabulary navigation policies, SINGER leverages three central components: (i) a photorealistic language-embedded flight simulator with minimal sim-to-real gap using Gaussian Splatting for efficient data generation, (ii) an RRT-inspired multi-trajectory generation expert for collision-free navigation demonstrations, and these are used to train (iii) a lightweight end-to-end visuomotor policy for real-time closed-loop control.\nThrough extensive hardware flight experiments, we demonstrate superior zero-shot sim-to-real transfer of our policy to unseen environments and unseen language-conditioned goal objects. When trained on ∼\\sim700k-1M observation action pairs of language conditioned visuomotor data and deployed on hardware, SINGER outperforms a velocity-controlled semantic guidance baseline by reaching the query 23.33%\\mathbf{23.33\\%} more on average, and maintains the query in the field of view 16.67%\\mathbf{16.67\\%} more on average, with 𝟏𝟎%\\mathbf{10\\%} fewer collisions.",
            "introduction": "Everyday, humans demonstrate notable semantic and physical understanding of their environments. For example, given a task to go to a specified location, a person relatively easily transforms the language instruction into a physical goal location using semantic cues and navigates to the desired location, safely avoiding collisions.\nAlthough autonomous drones excel at agile flight, they are often limited to controlled environments with pre-specified goal locations. In this work, we ask the question: “Can we train a vision-language drone navigation policy to reach previously unseen goal objects in a previously unseen environment using only on board sensing and compute?”\n\nAdvances in diffusion policies [1] and vision-language-action (VLA) models [2, 3] have led to significant research breakthroughs in robot policy learning from expert demonstration via imitation, particularly in robot manipulation.\nSpecifically, leveraging imitation learning on large-scale robot manipulation datasets [4, 5], state-of-the-art policies endow robots with the requisite task understanding and planning capabilities necessary to perform complex tasks entirely from task descriptions provided in natural language, e.g., to “pick up the apple and place it on a plate.”\nHowever, this paradigm has been largely unsuccessful in drone navigation, due to scarcity of large-scale drone navigation datasets, and effective semantic distillation methods for open-world drone navigation. This is exacerbated by inherent challenges in collecting large quantities of high quality visuomotor data on highly dynamic and naturally unstable drones.\n\nTo address the data scarcity challenge, prior work [6, 7] trains visuomotor policies for drone navigation in simulation, but the effectiveness of the resulting policies are often limited by the non-negligible sim-to-real gap. SOUS-VIDE [8] introduces FiGS, a high-fidelity Gaussian-Splatting-based drone simulator to narrow the sim-to-real gap for stronger real-world transfer; however, FiGS lacks the semantic knowledge required for open-world drone navigation, limiting its deployment to only environments and trajectories seen during training.\n\nIn this paper, we introduce SINGER (Semantic In-situ Navigation and Guidance for Embodied Robots), a pipeline for training language-conditioned drone navigation policies addressing the aforementioned limitations. SINGER consists of three central components: (i) a semantics-rich photorealistic flight simulator based on 3D Gaussian Splatting for efficient data generation with expert demonstrations,\n(ii) a high-level rapidly exploring random trees (RRT*) based planner that efficiently computes spatially spanning collision-free paths to a language-specified goal by time-inverting an expanded tree, and\n(iii) a robust low-level visuomotor policy that tracks the resulting high-level plans with real-time feedback.\nWith these components, SINGER trains a lightweight viusal policy that runs onboard a drone in real-time for online navigation given a natural-language goal object.\n\nTo build an effective flight simulator, we blend the high-fidelity scene-reconstruction capabilities of Gaussian Splatting [9] with the generalizable open-world vision-language semantic features computed by CLIP [10], achieving minimal sim-to-real gap.\nThis core design choice underpins SINGER’s strong zero-shot generalization capabilities to unseen tasks and environments at inference time. In particular, by abstracting goal specification to a semantic (vision-language) space, SINGER effectively aligns a small dataset of synthetic expert trajectories with a broad set of tasks, yielding a data-efficient training scheme for robust visuomotor policies. We augment this training approach with domain randomization for added robustness.\n\nAt deployment, we inference CLIPSeg [11] to produce open-vocabulary semantic images of the environment as conditioning inputs, processed by an end-to-end visuomotor drone policy for low-level drone commands.\n\nThrough our experiments, we show that SINGER outperforms baseline methods in achieving sub-meter proximity to goal by 23.33%23.33\\% with 10%10\\% less collisions and keeping the query in the field of view 16.67%16.67\\% more often without relying on external pose estimation or map-based navigation methods.\n\nWe summarize our contributions as follows:\n\nWe introduce a high-fidelity drone simulator for efficient imitation learning in language-specified drone navigation problems built on language embedded Gaussian Splatting.\n\nWe design a RRT* trajectory planner that efficiently finds thousands of collision-free feasible trajectories across multiple Gaussian Splatting scenes and multiple semantic classes, used to produce large quantities of data for training a generalist policy.\n\nWe present a real-time, lightweight, low-level visual policy architecture for language guided drone navigation using onboard sensing and compute.\n\nUsing these components, we train robust visuomotor policies for drone guidance given a natural language goal specification that generalizes to never before seen environments and semantic queries.\n\n1. We introduce a high-fidelity drone simulator for efficient imitation learning in language-specified drone navigation problems built on language embedded Gaussian Splatting.\n\n2. We design a RRT* trajectory planner that efficiently finds thousands of collision-free feasible trajectories across multiple Gaussian Splatting scenes and multiple semantic classes, used to produce large quantities of data for training a generalist policy.\n\n3. We present a real-time, lightweight, low-level visual policy architecture for language guided drone navigation using onboard sensing and compute.\n\n4. Using these components, we train robust visuomotor policies for drone guidance given a natural language goal specification that generalizes to never before seen environments and semantic queries.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何实现无人机在开放环境中的语言引导导航？  \n2. 如何解决无人机导航中的数据稀缺和sim-to-real差距问题？  \n3. 如何在无人机上实现实时的低级视觉控制策略？  \n\n【用了什么创新的方案】  \nSINGER提出了一种新的无人机导航策略，通过三个核心组件实现语言引导的自主导航：首先，利用基于3D Gaussian Splatting的高保真飞行模拟器生成高效的专家演示数据；其次，设计了一种基于RRT*的多轨迹生成规划器，以确保碰撞自由的导航；最后，开发了一种轻量级的端到端视觉运动策略，实现实时闭环控制。该方法通过在未见环境和目标对象上的实验，展示了优越的零-shot sim-to-real迁移能力。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving",
            "authors": "Chengran Yuan,Zijian Lu,Zhanqi Zhang,Yimin Zhao,Zefan Huang,Shuo Sun,Jiawei Sun,Jiahui Li,Christina Dao Wen Lee,Dongen Li,Marcelo H. Ang Jr",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18609",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18609",
            "arxiv_html_link": "https://arxiv.org/html/2509.18609v1",
            "abstract": "End-to-end motion planning is promising for simplifying complex autonomous driving pipelines. However, challenges such as scene understanding and effective prediction for decision-making continue to present substantial obstacles to its large-scale deployment. In this paper, we present PIE, a pioneering framework that integrates advanced perception, reasoning, and intention modeling to dynamically capture interactions between the ego vehicle and surrounding agents. It incorporates a bidirectional Mamba fusion that addresses data compression losses in multimodal fusion of camera and LiDAR inputs, alongside a novel reasoning-enhanced decoder integrating Mamba and Mixture-of-Experts to facilitate scene-compliant anchor selection and optimize adaptive trajectory inference. PIE adopts an action-motion interaction module to effectively utilize state predictions of surrounding agents to refine ego planning. The proposed framework is thoroughly validated on the NAVSIM benchmark. PIE, without using any ensemble and data augmentation techniques, achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of prior state-of-the-art methods. Comprehensive quantitative and qualitative analyses demonstrate that PIE is capable of reliably generating feasible and high-quality ego trajectories.",
            "introduction": "End-to-end motion planning has emerged as a promising paradigm for general robotic systems, including autonomous vehicles (AVs). This data-driven approach has the potential to enable AVs to handle complex and previously unseen scenarios, a capability that becomes increasingly critical as urban environments grow denser and more intricate. By leveraging sensor data directly, end-to-end methods [1, 2, 3] aim to consolidate the traditionally segmented autonomy pipeline—encompassing perception, prediction, and planning—into a single, cohesive framework.\n\nDespite the promising performance of end-to-end methods, several key challenges remain. First, fusing multimodal data (e.g., image and LiDAR inputs) often leads to compression-induced losses when reducing historical information or fusing features from different sources. Second, while data-driven approaches offer the potential for enhancing environmental understanding, the complexity of real-world driving requires more sophisticated models that are capable of both reasoning and dynamically adjusting their strategies. Third, incorporating the predictions of other traffic participants into the end-to-end planning pipeline often introduces substantial computational overhead. Developing efficient methods to seamlessly integrate these predictions into the planning process remains an open challenge, presenting considerable opportunities for further advancements.\n\nTo address these issues, we present PIE, an encoder-decoder framework designed to model the interaction between the action of ego vehicle and the motion of nearby agents and to enable more nuanced reasoning about the driving environment. Our approach mitigates data loss and integrates prediction and planning effectively. The contributions of this work are threefold:\n\nBidirectional Mamba Fusion We introduce a bidirectional Mamba fusion that effectively improves the multimodal data fusion between camera and LiDAR. A notable improvement of 1.9 PDM score can be achieved by merely employing this fusion approach based on the Transfuser backbone.\n\nBidirectional Mamba Fusion We introduce a bidirectional Mamba fusion that effectively improves the multimodal data fusion between camera and LiDAR. A notable improvement of 1.9 PDM score can be achieved by merely employing this fusion approach based on the Transfuser backbone.\n\nReasoning-Enhanced Decoder To improve scene reasoning in complex driving scenarios, we design an efficient decoder integrating the MoE, harnessing Mamba to enhance trajectory generation.\n\nAction-Motion Interaction We propose an action-motion interaction module via a shared cross-attention that directly integrates the velocity predictions of surrounding agents into ego action to model the dynamic interactions between traffic users.\n\nOur approach surpasses the previous state-of-the-art DiffusionDrive [4] by achieving an 88.9 PDM score and 85.6 EPDM score on the NAVSIM navtest split, demonstrating the superiority and effectiveness of the proposed modules.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效融合多模态数据（如相机和LiDAR）以减少压缩损失？  \n2. 如何在复杂的驾驶场景中实现更高效的推理和动态策略调整？  \n3. 如何将其他交通参与者的预测无缝整合进端到端规划流程？  \n\n【用了什么创新的方案】  \n核心解决方案：本文提出了PIE框架，通过引入双向Mamba融合技术来改善相机和LiDAR数据的多模态融合，减少数据压缩损失。设计了一个集成Mixture-of-Experts的推理增强解码器，以提高复杂驾驶场景的推理能力。同时，提出了一个动作-运动交互模块，通过共享的交叉注意力机制，将周围代理的速度预测直接整合到自我动作中，从而更好地建模交通用户之间的动态交互。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning",
            "authors": "Ana Luiza Mineiro,Francisco Affonso,Marcelo Becker",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Accepted to the 22nd International Conference on Advanced Robotics (ICAR 2025). 7 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18608",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18608",
            "arxiv_html_link": "https://arxiv.org/html/2509.18608v1",
            "abstract": "Reliable navigation in under-canopy agricultural environments remains a challenge due to GNSS unreliability, cluttered rows, and variable lighting. To address these limitations, we present an end-to-end learning-based navigation system that maps raw 3D LiDAR data directly to control commands using a deep reinforcement learning policy trained entirely in simulation. Our method includes a voxel-based downsampling strategy that reduces LiDAR input size by 95.83%, enabling efficient policy learning without relying on labeled datasets or manually designed control interfaces. The policy was validated in simulation, achieving a 100% success rate in straight-row plantations and showing a gradual decline in performance as row curvature increased, tested across varying sinusoidal frequencies and amplitudes.",
            "introduction": "Autonomous robots have seen significant growth in modern agriculture, particularly for under-canopy tasks such as plant phenotyping, crop row harvesting, and disease scouting. These applications require platforms that are not only compact and agile but also capable of accurately navigating between dense crop rows (Fig. 1) [1]. However, reliable navigation in such environments remains an active area of research due to several challenges, including clutter and occlusions caused by narrow row spacing and the high visual variability introduced by different plant growth stages [2].\n\nTo enable decision-making systems capable of navigating through plantations in under-canopy environments, robots typically rely on exteroceptive sensors [3]. This choice is driven by the unreliability of GNSS-based localization in such environments, where signal degradation is common due to foliage occlusion [4]. As a result, there is a need for local perception strategies that allow the robot to infer navigation information from its immediate surroundings.\n\nRecent methods have adopted learning-based approaches that aim to extract row-following features directly from curated and labeled datasets, using cameras or LiDAR sensors as the primary source of perception data. On one hand, cameras provide rich semantic and textural information; however, their performance often degrades in low-light conditions, which are common in dense canopy environments [5, 6]. On the other hand, LiDAR offers accurate depth measurements and is unaffected by lighting variations, making it more robust in shaded or poorly lit areas [7, 8]. Additionally, while a sim-to-real gap exists for LiDAR data, the structural consistency of point clouds tends to generalize well across training scenarios. This characteristic allows learning approaches based on point cloud data to effectively leverage large-scale simulated datasets [9].\n\nIn addition, most of these methods typically divide the navigation task into separate modules (e.g., perception, locomotion controller), which introduces challenges in designing effective interfaces between them [10]. As a result, important environmental features may be lost or overly simplified when translated into reference values for the controller. Combined with the high cost and impracticality of collecting labeled data across diverse agricultural conditions, these limitations hinder generalization and reduce overall system robustness.\n\nFor instance, to address the limitations of curated datasets and the challenge of designing interfaces between navigation modules, deep reinforcement learning (RL) can be explored. This approach enables agents to learn end-to-end policies by mapping perceptual observations directly to navigation actions through interaction with the environment. The control policy is optimized via trial-and-error, guided by reward functions that encode the desired task [11, 12].\n\nIn this paper, we present an end-to-end row-following navigation system for under-canopy agricultural environments that maps raw 3D LiDAR data directly to control commands using a RL policy trained in simulation. Our approach avoids the need for manually designed interfaces between navigation modules and does not rely on expert-labeled datasets. Additionally, we propose a LiDAR downsampling technique that transforms raw point clouds into compact row maps, significantly reducing the observation space while preserving essential structural features for learning.\n\nThe key contributions of this work are:\n\nAn end-to-end learning system for crop row-following using only raw 3D LiDAR data.\n\nA voxel-based downsampling technique to convert LiDAR point clouds into compact 2D row maps.\n\n1. An end-to-end learning system for crop row-following using only raw 3D LiDAR data.\n\n2. A voxel-based downsampling technique to convert LiDAR point clouds into compact 2D row maps.",
            "llm_summary": "【关注的是什么问题】  \n1. 在农业环境中，如何实现可靠的导航以应对GNSS不可靠性和环境复杂性？  \n2. 如何有效利用3D LiDAR数据进行自主导航而不依赖于标注数据或手动设计的控制接口？  \n3. 如何通过深度强化学习优化导航策略以提高系统的鲁棒性和通用性？  \n\n【用了什么创新的方案】  \n本研究提出了一种端到端的学习系统，利用原始3D LiDAR数据直接映射到控制命令，采用深度强化学习策略在模拟环境中训练。通过引入体素下采样技术，将LiDAR点云转换为紧凑的2D行地图，显著减少了观察空间，同时保留了学习所需的结构特征，从而避免了手动设计模块接口的需求。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
            "authors": "Neel P. Bhatt,Yunhao Yang,Rohan Siva,Pranay Samineni,Daniel Milan,Zhangyang Wang,Ufuk Topcu",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Systems and Control (eess.SY)",
            "comment": "Codebase, datasets, and videos for VLN-Zero are available at:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18592",
            "code": "https://vln-zero.github.io/",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18592",
            "arxiv_html_link": "https://arxiv.org/html/2509.18592v1",
            "abstract": "Rapid adaptation in unseen environments is essential for scalable real-world autonomy, yet existing approaches rely on exhaustive exploration or rigid navigation policies that fail to generalize. We present VLN-Zero, a two-phase vision-language navigation framework that leverages vision-language models to efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic navigation. In the exploration phase, structured prompts guide VLM-based search toward informative and diverse trajectories, yielding compact scene graph representations. In the deployment phase, a neurosymbolic planner reasons over the scene graph and environmental observations to generate executable plans, while a cache-enabled execution module accelerates adaptation by reusing previously computed task–location trajectories. By combining rapid exploration, symbolic reasoning, and cache-enabled execution, the proposed framework overcomes the computational inefficiency and poor generalization of prior vision-language navigation methods, enabling robust and scalable decision-making in unseen environments. VLN-Zero achieves 2x higher success rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned baselines, and reaches goal locations in half the time with 55% fewer VLM calls on average compared to state-of-the-art models across diverse environments.††The full codebase, datasets, and videos for VLN-Zero are available at https://vln-zero.github.io/.",
            "introduction": "Deploying autonomous agents in new environments remains a fundamental challenge: policies trained in one setting often fail in another due to novel layouts, obstacles, or constraints, and consequently require fine-tuning or multi-shot inference.\nFor example, a robot trained to navigate one office building may struggle in a different building unless retrained, a process that is both slow and impractical for rapid deployment.\n\nThe central problem is twofold: (i) How can an agent efficiently construct a representation of an unseen environment, such as a scene graph, without exhaustive search?\n(ii) Given this representation, how can the agent efficiently generate constraint-satisfying plans in real time without fine-tuning or multi-shot inference?\n\nExisting approaches, ranging from frontier-based exploration to reinforcement learning with fixed policies, struggle with either computational inefficiency, lack of generalization, or both. Vision-language navigation models (VLNs) offer promise; however, current approaches suffer from slow, exhaustive exploration, weak task decomposition, and high training and query cost.\n\nWe argue that addressing these limitations requires rethinking the interaction between perception, symbolic reasoning, and policy adaptation. Specifically, agents must be able to (i) rapidly acquire symbolic representations of their environment to minimize exploration cost, and (ii) leverage these representations for efficient navigation in new environments without retraining or extensive fine-tuning.\n\nTo address these challenges, we introduce VLN-Zero, a two-phase zero-shot framework that combines vision-language model (VLM) guided exploration with neurosymbolic navigation. In the exploration phase, the agent interacts with the environment using structured and compositional task prompts, guiding exploration toward informative and diverse trajectories to construct a compact scene graph with semantic area labels. In the deployment phase, a neurosymbolic planner reasons over this scene graph, environmental observations to generate executable plans, eliminating reliance on fixed policies. To further improve scalability, we propose a cache-enabled execution procedure that stores previously computed task–location trajectories for reuse, accelerating both exploration and deployment.\n\nIn summary, VLN-Zero offers three key contributions:\n\nVLM-guided rapid exploration: We design structured, compositional prompts that steer a VLN agent to propose exploration actions while incrementally constructing compact symbolic scene graphs. This enables coverage of novel environments within a time- and compute-constrained exploration budget while avoiding unsafe behaviors.\n\nZero-shot neurosymbolic navigation: We introduce a planner that reasons jointly over\nscene graphs, task prompts, and real-time observations, transforming free-form natural language\ninstructions into constraint-satisfying action sequences without fine-tuning or multi-shot inference.\n\nCache-enabled execution for fast adaptation: We develop a trajectory-level caching mechanism\nthat stores validated task–location pairs, allowing the system to reuse previously computed plans.\nThis reduces redundant VLM queries to minimize execution time, cost, and compute demands which accelerates real-world deployment.\n\n1. VLM-guided rapid exploration: We design structured, compositional prompts that steer a VLN agent to propose exploration actions while incrementally constructing compact symbolic scene graphs. This enables coverage of novel environments within a time- and compute-constrained exploration budget while avoiding unsafe behaviors.\n\n2. Zero-shot neurosymbolic navigation: We introduce a planner that reasons jointly over\nscene graphs, task prompts, and real-time observations, transforming free-form natural language\ninstructions into constraint-satisfying action sequences without fine-tuning or multi-shot inference.\n\n3. Cache-enabled execution for fast adaptation: We develop a trajectory-level caching mechanism\nthat stores validated task–location pairs, allowing the system to reuse previously computed plans.\nThis reduces redundant VLM queries to minimize execution time, cost, and compute demands which accelerates real-world deployment.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在未见环境中高效构建场景图表示，而不依赖于耗时的探索？  \n2. 如何在实时生成约束满足的导航计划，而无需进行微调或多次推理？  \n\n【用了什么创新的方案】  \n核心解决方案：VLN-Zero是一个两阶段的零-shot框架，结合了基于视觉语言模型（VLM）的快速探索和神经符号导航。在探索阶段，使用结构化提示引导VLM进行有效的探索，构建紧凑的场景图。在部署阶段，神经符号规划器基于场景图和环境观察生成可执行计划，同时通过缓存机制加速适应，重用先前计算的任务-位置轨迹，从而提高决策效率和可扩展性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA",
            "authors": "Zeyi Kang(1),Liang He(2),Yanxin Zhang(3),Zuheng Ming(4),Kaixing Zhao(5) ((1) Northwestern Polytechnical University, (2) University Sorbonne Paris Nord)",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18576",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18576",
            "arxiv_html_link": "https://arxiv.org/html/2509.18576v1",
            "abstract": "Multimodal semantic learning plays a critical role in embodied intelligence, especially when robots perceive their surroundings, understand human instructions, and make intelligent decisions. However, the field faces technical challenges such as effective fusion of heterogeneous data and computational efficiency in resource-constrained environments. To address these challenges, this study proposes the lightweight LCMF cascaded attention framework, introducing a multi-level cross-modal parameter sharing mechanism into the Mamba module. By integrating the advantages of Cross-Attention and Selective parameter-sharing State Space Models (SSMs), the framework achieves efficient fusion of heterogeneous modalities and semantic complementary alignment. Experimental results show that LCMF surpasses existing multimodal baselines with an accuracy of 74.29% in VQA tasks and achieves competitive mid-tier performance within the distribution cluster of Large Language Model Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a 4.35-fold reduction in FLOPs relative to the average of comparable baselines while using only 166.51M parameters (image-text) and 219M parameters (video-text), providing an efficient solution for Human-Robot Interaction (HRI) applications in resource-constrained scenarios with strong multimodal decision generalization capabilities.",
            "introduction": "In contemporary research, powerful multimodal understanding capabilities have emerged as the foundational element for enabling robotic perception, cognition, and interaction within complex dynamic environments[1]. In the domain of embodied intelligence, Vision-Language Pre-training (VLP) [2] has advanced into a critical technological paradigm for the development of sophisticated intelligent robotic systems, offering substantial support for the realization of more intelligent HRI [3]. Concurrently, to address the challenges inherent in multimodal learning, such as the scarcity of labeled data and the prohibitive costs of annotation, self-supervised learning [4] has garnered considerable attention and research focus. More precisely, by setting multi-task optimization objectives (multimodal masked modeling, contrastive learning, etc.) [5], these methods provide possibilities for common robotic tasks, such as environmental understanding [6], decision-making (Visual Question Answering (VQA) [7], Embodied Question Answering (EQA) [8]) or even more advanced cross-modal general understanding [9].\n\nHowever, embodied intelligence [10] still faces numerous challenges that limit learning capabilities in visual-language decision tasks. At the semantic understanding level, current models [11, 12, 13] struggle to reconstruct fine-grained mask labels, resulting in an information gap between local features, mask features, and global scene understanding. In addition, the efficiency problem of long sequence modeling cannot be ignored, as the Transformer architecture’s computational complexity grows quadratically when processing large-scale sequence data [14, 15], making it difficult to achieve optimal trade-offs between cross-modal understanding performance and hardware efficiency.\n\nIn response to the above challenges, this paper proposes the lightweight LCMF architecture, which achieves high-quality multimodal understanding and inference acceleration on low-computation robotic platforms. LCMF uses a semantic diffusion mechanism [11] to address the information gap in multi-scale visual semantics and enhance the ability to model fine-grained masked information. For cross-modal interaction, Cross-Modality Mamba (CMM) extends the Mamba state-space model to the multimodal domain, achieving comprehensive optimization in hardware awareness, time efficiency, and lightweight design. At the level of multimodal semantic fusion, Enhanced Mamba Fusion (EMF) introduces efficient semantic bridging mechanisms and fine-grained feature modulation techniques, enabling the effective integration of heterogeneous modality semantics.\n\nIn summary, our contributions include:\n\n1) CMM implements multi-level sharing of state space parameters and parallel modeling of multimodal long sequence semantics, achieving linear computational complexity and inference acceleration.\n\n2) LCMF has implemented a lightweight Mamba-Transformer (Selective SSMs-Attention) architecture in the fields of unimodal feature extraction, multimodal (image, text, video) interaction, and multimodal fusion.\n\n3) Under a significantly reduced parameter scale compared to existing multimodal baselines and LLM Agents, LCMF achieves improved computational efficiency while maintaining strong performance on downstream tasks such as VQA and EQA, demonstrating its effectiveness in efficient multimodal modeling.\n\nThe rest of this article is organized as follows.\n\nSection II reviews research on Mamba variant architecture, VQA, EQA.\nSection III details the LCMF model architecture, pretraining, fine-tuning, and evaluation methods.\nSection IV describes the experimental setup, performance evaluation, and ablation experiments on specific downstream tasks.\nFinally, Section V concludes this article.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效融合异构数据以提升机器人在复杂环境中的理解能力？  \n2. 如何在资源受限的环境中实现高效的多模态学习和决策？  \n3. 当前模型在视觉语言决策任务中面临的学习能力限制是什么？  \n\n【用了什么创新的方案】  \n本研究提出了轻量级的LCMF框架，通过引入多级跨模态参数共享机制，结合Cross-Attention和选择性状态空间模型（SSMs），实现了异构模态的高效融合和语义互补对齐。该框架在保持较低计算复杂度的同时，显著提升了多模态理解能力，并在视觉问答（VQA）和视频问答（EQA）任务中表现出色，展示了其在资源受限场景下的有效性和强大的多模态决策泛化能力。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Spatial Envelope MPC: High Performance Driving without a Reference",
            "authors": "Siyuan Yu,Congkai Shen,Yufei Xi,James Dallas,Michael Thompson,John Subosits,Hiroshi Yasuda,Tulga Ersal",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18506",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18506",
            "arxiv_html_link": "https://arxiv.org/html/2509.18506v1",
            "abstract": "This paper presents a novel envelope-based model predictive control (MPC) framework designed to enable autonomous vehicles to handle high-performance driving across a wide range of scenarios without a predefined reference.\nIn high-performance autonomous driving, safe operation at the vehicle’s dynamic limits requires a real-time planning and control framework capable of accounting for key vehicle dynamics and environmental constraints when following a predefined reference trajectory is suboptimal or even infeasible. State-of-the-art planning and control frameworks, however, are predominantly reference-based, which limits their performance in such situations.\nTo address this gap, this work first introduces a computationally efficient vehicle dynamics model tailored for optimization-based control and a continuously differentiable mathematical formulation that accurately captures the entire drivable envelope.\nThis novel model and formulation allow for the direct integration of dynamic feasibility and safety constraints into a unified planning and control framework, thereby removing the necessity for pre-defined references.\nThe challenge of envelope planning, which refers to maximally approximating the safe drivable area, is tackled by combining reinforcement learning with optimization techniques.\nThe framework is validated through both simulations and real-world experiments, demonstrating its high performance across a variety of tasks, including racing, emergency collision avoidance and off-road navigation.\nThese results highlight the framework’s scalability and broad applicability across a diverse set of scenarios.",
            "introduction": "High-performance autonomous driving technology has advanced rapidly and significantly in the past decade [1].\nThe grand opening of the first autonomous racing competition has thrust autonomous driving technology into the spotlight, showcasing performance that rivals expert human drivers in tasks demanding high levels of precision and speed [2]. Recent developments in autonomous driving have enabled reliable performance in advanced collision avoidance capabilities [3, 4, 5], complex drifting maneuvers [6, 7, 8], and challenging off-road navigation [9, 10, 11].\nThese advancements have greatly expanded the potential applications of autonomous vehicles across various fields. To execute these extreme maneuvers safely, the vehicle must operate at the limits of its capabilities.\nHowever, this is a challenge, because even a small deviation from the desired trajectory can result in catastrophic outcomes.\n\nIn this regard, the autonomous system should be designed to effectively utilize all feasible operational regions including the limits, ensuring performance is not unnecessarily sacrificed for safety via overly conservative constraints.\nFor instance, every inch of the race track is critical for drivers to adjust their strategies and enhance their performance. In addition, the algorithm’s design should ensure scalability when applied to a diverse set of scenarios with varying levels of complexity.\nIn this context, the term ‘scalability’ refers to the algorithm’s ability to handle an increasing amount of tasks or scenarios without a significant compromise in performance or without significant redesign.\nIt is also desirable that the system be capable of generating optimal trajectories online in real-time without having to depend on a predefined reference, because deviations from the original plan may render the predefined reference suboptimal or even infeasible.\n\nHowever, as the literature review in Sec. II reveals, existing methods typically rely on predefined references and therefore either limit vehicle performance to the quality of that reference or do not provide optimal and scalable solutions in performance-demanding scenarios.\n\nTo address this gap, this paper presents a novel, spatial envelope model predictive control (MPC) framework for reference-free high performance driving.\nThe proposed framework builds upon a new, computationally efficient vehicle dynamics model tailored for closed-loop optimization based planning and control, capturing the essential dynamics required for aggressive maneuvers.\nA twice continuously differentiable mathematical formulation of the entire driving envelope is introduced to conservatively estimate the drivable region to be used in MPC.\nThis enables the MPC to break from the restrictive constant-speed assumption previously used in spatial envelope MPC [12, 13, 14, 5], and instead optimize speed, as well, to maximize performance while maintaining safety.\n\nTo the authors’ knowledge, this is the first published MPC algorithm that is experimentally validated for safe and effective high-performance driving at the handling limits in a fully reference-free setting.\nThe algorithm is validated across a wide range of scenarios, including racing, off-road navigation and emergency collision avoidance, demonstrating both generality and real-world applicability.\n\nFinally, a new spatial envelope planning technique is introduced to further enhance applicability. A hybrid approach that combines optimization-based formulation with reinforcement learning is developed to segment the drivable area into blocks, enabling scalable planning in complex environments.\n\nThe original contributions are summarized as follows:\n\nA validated 3-DoF single-track dynamic model that accounts for longitudinal load transfer and the friction circle limit, while remaining computationally efficient in a fully reference-free setting.\n\nA validated 3-DoF single-track dynamic model that accounts for longitudinal load transfer and the friction circle limit, while remaining computationally efficient in a fully reference-free setting.\n\nA hard constraint formulation to mathematically express the spatial envelope with guaranteed conservativeness.\n\nA real-time Model Predictive Control (MPC) formulation that leverages the first two contributions to optimize vehicle trajectories online without any predefined path.\n\nA reinforcement learning approach to design a set of blocks to approximate arbitrary shapes of spatial envelopes in real time.\n\nValidation of the proposed MPC formulation in racing, emergency collision avoidance and off-road environments.\n\nThe rest of the paper is organized as follows. Sec. II reviews the relevant literature. Sec. III-A describes the 3 DoF single-track vehicle dynamics. Sec. III-B describes the MPC formulation including the conservative spatial envelope constraints. Sec. III-C describes the real-time spatial envelope planner. Sec. IV describes the model fidelity test. The results and discussion of the proposed spatial envelope MPC are presented in Sec. IV.\nFrom Sec. V-A to E, the simulation and experimental results of spatial envelope MPC are conducted and analyzed in multiple scenarios.\nIn Sec. III-F, the proposed spatial envelope planning technique is presented and discussed.\nFinally, Sec. VI concludes the study.",
            "llm_summary": "【关注的是什么问题】  \n1. 高性能自动驾驶中缺乏有效的参考轨迹规划与控制方法。  \n2. 现有方法过于依赖预定义参考，限制了车辆性能。  \n3. 如何在复杂环境中实现实时、无参考的轨迹优化。  \n\n【用了什么创新的方案】  \n本研究提出了一种新颖的空间包络模型预测控制（MPC）框架，旨在实现高性能的无参考自动驾驶。该框架结合了高效的车辆动力学模型和连续可微的数学公式，能够准确捕捉可行驶区域。通过将动态可行性和安全约束直接整合到规划和控制框架中，消除了对预定义参考的依赖。此外，采用强化学习与优化技术相结合的方法，能够在复杂环境中进行可扩展的规划。该框架在多种场景下进行了验证，包括赛车、紧急避障和越野导航，展示了其广泛的适用性和高性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain",
            "authors": "Junnosuke Kamohara,Feiyang Wu,Chinmayee Wamorkar,Seth Hutchinson,Ye Zhao",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18466",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18466",
            "arxiv_html_link": "https://arxiv.org/html/2509.18466v1",
            "abstract": "Model predictive control (MPC) has demonstrated effectiveness for humanoid bipedal locomotion; however, its applicability in challenging environments, such as rough and slippery terrain, is limited by the difficulty of modeling terrain interactions.\nIn contrast, reinforcement learning (RL) has achieved notable success in training robust locomotion policies over diverse terrain, yet it lacks guarantees of constraint satisfaction and often requires substantial reward shaping.\nRecent efforts in combining MPC and RL have shown promise of taking the best of both worlds, but they are primarily restricted to flat terrain or quadrupedal robots.",
            "introduction": "Legged locomotion conventionally employs model-based controllers (MBCs), particularly Model Predictive Control (MPC), due to their optimization-based constraint satisfaction [1, 2].\nWhile whole-body dynamics models [3] are more accurate,\nresearchers use simplified models [4, 5, 6, 7] for computational efficiency and consequently suffer from model mismatch due to simplification of the dynamics.\nAs a result, simplified models exhibit poorer tracking accuracy and instability, particularly during contact [8].\nAdditionally, MPC with simplified dynamics usually requires predefined contact sequence and swing leg trajectory, which limits its adaptivity to diverse terrains.\nOverall, the deterministic but inaccurate dynamic model and manual constraint design of MPC restrict its robustness and versatility, limiting its applicability to diverse terrains in the real world.\n\nIn contrast, learning-based controls (LBC), exemplified by Reinforcement Learning (RL) methods, have gained wide attention for their robustness and agility [9, 10, 11, 12, 13].\nBy training policies parameterized by neural networks, RL policies can achieve zero-shot transfer from simulation to reality.\nHowever, training robust policies requires substantial environmental interactions and extensive reward shaping.\nFurthermore, RL policies lack explicit constraint satisfaction because of the absence of explicit constraints.\n\nMotivated by the unique advantages of both sides, recent years have witnessed a surge of methods combining model-based and learning-based approaches, leveraging the safety offered by MPC’s explicit constraints as well as powerful reactive behaviors offered by RL [14, 15].\nIn legged robotics, there are two main threads of combination.\nThe first thread uses MPC within a policy.\nRecent works either adopt a hierarchical architecture, where RL parametrizes MPC’s components, including system dynamics, center of mass reference trajectory, and gait frequency [16, 8, 17, 18]; or follows a parallel architecture, where RL policies refine MPC outputs by adding corrective actions such as footholds and joint commands [19, 20, 21].\nAnother thread uses MPC as an expert policy, training the policy through behavior cloning or RL with imitation loss to increase sample efficiency and motion accuracy [22, 13, 23, 24].\nEach of these designs carries trade-offs:\nMPC as an expert improves training efficiency by imitating MPC motions, yet it incurs significant computational overhead during training due to repeated optimization solves, making training in parallelized RL environments particularly challenging [24].\nWhile parallel architectures offer flexibility by directly augmenting MPC outputs, they raise safety concerns since the RL policy bypasses feasibility constraints from optimization.\nHierarchical architectures, in contrast, preserve the optimization structure and computational complexity, as the policy is evaluated before solving the optimization problem.\nThis ensures the feasibility and constraint satisfaction within the optimization framework.\n\nDespite these advances, most combined approaches for bipedal locomotion remain limited to flat terrain, as prior works primarily emphasize improving tracking accuracy rather than adaptability [19, 21], leaving integration of MPC and RL for rough-terrain-adaptive locomotion unexplored.\nIn this work, we aim to enhance the adaptability of humanoid locomotion, enabling responsive and robust behaviors in the face of terrain disturbances.\nWe leverage a hierarchical method that augments MPC via RL by incorporating rich whole-body information into the simplified system model, adjusting the gait frequency to modulate step length, and modifying the swing foot trajectory to improve robustness against challenging terrain.\nWe focus on addressing the limitations of MPC with simplified dynamics: model mismatch, predefined swing leg curve, and static gait frequency.\nThe RL policy learns residual dynamics through whole-body dynamics simulation, as well as swing leg curve parameters, including apex height and control points, and dynamic gait frequency within one locomotion cycle.\n\nThese learned adaptations enable reactive behaviors, including recovery from foot entrapment and severe slippage.\nWe implement our method on bipedal locomotion tasks with the HECTOR robot [25] in NVIDIA IsaacLab, a state-of-the-art GPU-accelerated simulator [26].\nOur framework significantly improves robustness against disturbances on diverse terrains, including slippery surfaces, stairs, and stepping stones.\nAdditionally, we conduct ablation studies on the three residual modules to analyze the contribution of each component.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高双足机器人在复杂地形上的适应性和鲁棒性。  \n2. 现有的模型预测控制（MPC）在动态建模和适应性方面的局限性。  \n3. 强化学习（RL）在约束满足和训练效率方面的不足。  \n\n【用了什么创新的方案】  \n本研究提出了一种层次化方法，通过将强化学习（RL）与模型预测控制（MPC）相结合，增强了双足机器人在复杂地形上的适应性。该方法利用丰富的全身信息来调整简化系统模型，学习残余动态和摆腿曲线参数，并动态调整步频，从而提高在滑面、楼梯和踏石等多样地形上的鲁棒性。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task",
            "authors": "Jannick van Buuren,Roberto Giglio,Loris Roveda,Luka Peternel",
            "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18463",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18463",
            "arxiv_html_link": "https://arxiv.org/html/2509.18463v1",
            "abstract": "This paper explores how deliberate mutations of reward function in reinforcement learning can produce diversified skill variations in robotic manipulation tasks, examined with a liquid pouring use case. To this end, we developed a new reward function mutation framework that is based on applying Gaussian noise to the weights of the different terms in the reward function. Inspired by the cost-benefit tradeoff model from human motor control, we designed the reward function with the following key terms: accuracy, time, and effort. The study was performed in a simulation environment created in NVIDIA Isaac Sim, and the setup included Franka Emika Panda robotic arm holding a glass with a liquid that needed to be poured into a container. The reinforcement learning algorithm was based on Proximal Policy Optimization. We systematically explored how different configurations of mutated weights in the rewards function would affect the learned policy. The resulting policies exhibit a wide range of behaviours: from variations in execution of the originally intended pouring task to novel skills useful for unexpected tasks, such as container rim cleaning, liquid mixing, and watering. This approach offers promising directions for robotic systems to perform diversified learning of specific tasks, while also potentially deriving meaningful skills for future tasks.",
            "introduction": "For robots to successfully operate in unstructured and unpredictable real-world environments, they need the ability to constantly adapt and learn many tasks. One way to do this is to learn from human demonstration [1]. However, human involvement can be costly, and humans are not always available to correct or teach robots new skills. Indeed, an alternative is reinforcement learning (RL) that allows the robots to autonomously acquire new skills through trial‑and‑error interaction with their environment [2]. The robot is given an objective function (typically from a human), which then guides its autonomous exploration to obtain a policy of how to perform a given task. At each timestep, the robot observes the current state, executes an action according to its policy, and receives a scalar reward. Over many episodes, it refines its policy to maximise the expected sum of discounted rewards, thereby acquiring skills optimised for long‑term success.\n\nRL has been successfully applied to robots to solve a diverse range of tasks, ranging from pick-and-place actions [3, 4], object lifting [5], to assembly [6], as well as play ball-in-the-cup game [7], table tennis [8], and air hockey [9]. Within this context, the liquid pouring task [10, 11, 12, 13] stands out as a particularly compelling benchmark for investigating the role of reward function design in shaping learned behaviours. Unlike binary success criteria seen in stacking or placement tasks, pouring involves balancing multiple continuous objectives, such as avoiding spillage, reducing effort, and maximising efficiency, making it highly sensitive to how learning is incentivised.\n\nRL has achieved impressive results in specific robotic tasks with well-crafted and tailored reward functions. This typically results in good skills specialised for the given task, but lacks generalisation capabilities when new tasks arise. Learning new tasks is typically relatively long and sample-inefficient, especially in complex tasks without prior knowledge and where rewards are sparse or delayed. Rather than relying solely on environmental feedback, an agent can benefit from understanding the reward logic itself, such as temporal dependencies, conditional sequences, or subgoals—thereby improving learning efficiency and policy quality [14, 15, 16]. By leveraging structured reward representations, agents can more effectively sequence and reuse behaviours, enabling them to adjust previously learned skills to new or modified tasks. This structured approach facilitates faster adaptation, as agents can generalise from prior experience rather than starting from scratch each time [17].\n\nAnother approach to reduce learning time and improve generalisability is to utilise direct prior task knowledge from models or human demonstrations [18, 2]. In that way, an agent already has a rough policy, which then only needs to be refined and optimised for the given specifics of the robot and the environment. However, resulting policies that are not considered optimal for the given specific task are often discarded. We argue that such “failed” or “suboptimal” policies should not be discarded, since such skill variations resulting from various mutations might be useful starting points for learning new tasks.\n\nMutation of policy in robot learning when subject to physical interaction with humans and unpredictable environments has been observed in [19]. A follow-up study [20] investigated how the policy mutations occur during the learning and what kind of variations of skill emerge in a sawing task. The study concluded that certain policy variations may not be optimal or suitable for the original task, but can be useful for optimising some other parameters/tasks. This highlighted the potential for diversification of skills and the importance of not discarding the policies that appear to be suboptimal for the current task, as they might be a good starting point for unforeseen new tasks. However, the work so far relied on mutations from a passive environment, while intentional mutations with a systematic mechanism are still missing.\n\nTo address this challenge, we introduce a reward mutation framework that treats the reward function as a tunable mechanism for active skill diversification (Fig. 1). While online reward-shaping has been investigated for improvement of sample efficiency and optimisation of a specific task in [21], differently, the proposed approach mutates the reward function to discover new skills. As a starting point, we study a liquid pouring task performed by a Franka Emika Panda robotic manipulator in the NVIDIA Isaac Sim simulation environment, where the reward is composed of three weighted terms: pouring accuracy, time spent, and effort spent. The goal of the study is to systematically explore how the mutation of the weights of these reward terms induces the emergence of diverse policies. We perform training of agents with Proximal Policy Optimization (PPO) [22] under 25 distinct reward configurations. Some of the emerging policies are useful for executing the original task in different ways (fast and slow), some are identified to be useful for unforeseen other tasks (container rim cleaning, liquid mixing, watering), and some are not useful for any identifiable tasks.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何通过变异奖励函数来实现机器人技能的多样化。  \n2. 现有的强化学习方法在新任务学习中的效率和泛化能力不足。  \n3. 如何有效利用“次优”策略作为新任务学习的起点。  \n\n【用了什么创新的方案】  \n本研究提出了一种奖励变异框架，通过对奖励函数中不同权重施加高斯噪声，系统性地探索了不同奖励配置对学习策略的影响。该方法允许机器人在液体倒入任务中产生多样化的技能，从而不仅优化原有任务的执行方式，还为意外任务（如容器边缘清洁、液体混合和浇水）提供了有用的技能。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems",
            "authors": "Haeyoon Han,Mahdi Taheri,Soon-Jo Chung,Fred Y. Hadaegh",
            "subjects": "Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18460",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18460",
            "arxiv_html_link": "https://arxiv.org/html/2509.18460v1",
            "abstract": "Perception systems provide a rich understanding of the environment for autonomous systems, shaping decisions in all downstream modules. Hence, accurate detection and isolation of faults in perception systems is important. Faults in perception systems pose particular challenges: faults are often tied to the perceptual context of the environment, and errors in their multi-stage pipelines can propagate across modules. To address this, we adopt a counterfactual reasoning approach to propose a framework for fault detection and isolation (FDI) in perception systems. As opposed to relying on physical redundancy (i.e., having extra sensors), our approach utilizes analytical redundancy with counterfactual reasoning to construct perception reliability tests as causal outcomes influenced by system states and fault scenarios. Counterfactual reasoning generates reliability test results under hypothesized faults to update the belief over fault hypotheses. We derive both passive and active FDI methods. While the passive FDI can be achieved by belief updates, the active FDI approach is defined as a causal bandit problem, where we utilize Monte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find control inputs that maximize a detection and isolation metric, designated as Effective Information (EI). The mentioned metric quantifies the informativeness of control inputs for FDI. We demonstrate the approach in a robot exploration scenario, where a space robot performing vision-based navigation actively adjusts its attitude to increase EI and correctly isolate faults caused by sensor damage, dynamic scenes, and perceptual degradation.",
            "introduction": "Autonomous systems such as self-driving cars, unmanned aerial vehicles (UAV), and autonomous robots rely on perception systems to convert heterogeneous sensor measurements into a coherent representation of their surrounding environment [1]. The role of the perception system is to provide accurate and timely information on objects, terrain, and the surrounding environment so that higher-level modules in an autonomous system (e.g., localization, motion planning, and control) can guarantee safety and achieve mission objectives [2]. The combination of utilizing heterogeneous sensors (e.g., LiDAR, radar, cameras) and deep learning-based algorithms has led to recent advances in perception-based control. However, this has also resulted in an increased level of complexity in perception systems, which makes detecting their faults and algorithmic errors challenging [3, 4]. Considering the importance of a perception system in the guidance and control of an autonomous system, perception faults can result in the complete loss of a mission. For instance, on 6 June 2025, the Japanese lunar lander Resilience (Hakuto-R Mission 2) had a hard landing during its final descent on the Moon when its laser range finder began outputting erroneous altitude values in the last few kilometers before touchdown [5]. This highlights the need for accurate monitoring systems that can address the problem of fault detection and isolation (FDI) in perception systems.\n\nThe method presented in this paper can handle a broad range of fault and failure types, including both physical malfunctions and algorithmic errors in perception systems that cause deviations from their intended functionality. On the physical side, sensors can suffer calibration shifts, temporary occlusions, and environmental interference [7]. At the algorithmic level, deep neural networks (DNN) can misclassify objects due to distribution shifts (i.e., out-of-distribution inputs), and multi-sensor fusion can become erroneous due to calibration issues [8, 9, 10]. Moreover, faults that occur at an early stage of a perception system’s pipeline propagate through it and do not remain isolated [11]. Hence, FDI methodologies that rely on physical redundancy may not be sufficient [12]. Thus, one needs to study and investigate FDI methodologies based on the available analytical redundancy in perception systems. Once a certain fault is detected and isolated, a fault recovery control can be implemented.\n\nThe faults that occur in Simultaneous Localization and Mapping (SLAM) and Visual Inertial Odometry (VIO) systems are sensor faults [3, 7], tracking failures [15, 16], data association failures [17, 18], and filtering inconsistency problems [19]. Sensor faults are caused by hardware damage or software malfunction. Faults in front-end modules, such as tracking and data association failures, are often caused by visually deprived conditions (i.e., textureless surfaces and repetitive patterns), dynamic scenes (i.e., aggressive camera motion), and undesirable lighting conditions (i.e., high-contrast images). Lastly, the filtering inconsistency problems, a type of fault in back-end modules, result from large inter-frame transformations that trigger the accumulation of linearization errors.\n\nThe work in [8] compares perception outputs with a predefined fault threshold for runtime monitoring. Additionally, [3] developed fault diagnostic graphs to associate errors with individual perception module outputs, as evaluated by diagnostic tests. Although these works enable FDI, they rely on having redundant sensors, which can be costly. To enhance the robustness of SLAM [20] developed image quality metrics to select confident features or scenes. Similarly, feature quality metrics that assess keypoint co-visibility between frames [15, 21, 3] and the dynamic scene metrics that leverage vehicle velocity [15], optical flow [22], and image sharpness [23, 24] have been proposed.\n\nWe define perception reliability tests for various fault modes to capture differences between fault-free and fault-induced behaviors. We utilize the structural causal model (SCM) formalism of Pearl [25] and its operational rules for interventions and counterfactual queries, where we treat each hypothesized fault mode as an intervention on the perception pipeline. We then introduce and define an information-theoretic metric based on the Kullback–Leibler (KL) divergence between the reliability test results and those from a baseline fault-free case to measure the detectability and isolability of the hypothesized faults. This metric, designated as Effective Information (EI), captures how control inputs influence the reliability test results by affecting the autonomous system’s state. To the best of our knowledge, this is the first work that studies the FDI as a counterfactual reasoning problem for a closed-loop autonomous system and also connects the informativeness of control inputs to the detection and isolation of the hypothesized faults. Finally, we show that finding the control input that helps maximizing the EI leads to having a causal bandit problem [26], where each action arm corresponds to an intervention on the control input that improves our FDI accuracy. A Monte-Carlo Tree Search (MCTS) approach with Upper Confidence Bound (UCB) [27, 28] that penalizes large deviations from primary mission objectives (e.g., tracking a trajectory) is employed to solve the mentioned causal bandit problem.\n\nThe main contributions of this paper are as follows.\n\nWe exploit analytical redundancy of the perception system and actively use control inputs for FDI by applying the do-operator from causal inference. This is achieved via a counterfactual reasoning approach, where it is analyzed how control inputs affect reliability test outcomes under various fault hypotheses. A quantitative detection and isolation metric measuring the informativeness of each control input for FDI is introduced.\n\nWe formulate the problem of selecting control inputs for FDI as a causal bandit problem. Using a MCTS strategy with UCB, we maximize a weighted reward function that prioritizes inputs informative about the most likely fault modes. In addition, our reward function penalizes large deviations from the desired trajectory of the system.\n\nOur FDI method uses the distribution of reliability test results under various fault modes and accounts for the uncertainty inherent in the perception system’s outputs. Thus, our method encodes more information than mean value and threshold-based FDI methods, which only reflect the central tendency of a distribution.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在感知系统中准确检测和隔离故障。  \n2. 传统的物理冗余方法在故障检测中的局限性。  \n3. 如何利用反事实推理提高故障检测和隔离的有效性。  \n\n【用了什么创新的方案】  \n本研究提出了一种基于反事实推理的故障检测和隔离（FDI）框架，利用分析冗余而非物理冗余来构建感知可靠性测试。通过生成假设故障下的可靠性测试结果，更新对故障假设的信念。我们将FDI问题建模为因果赌博问题，使用蒙特卡洛树搜索（MCTS）和上置信界（UCB）策略来最大化有效信息（EI），从而优化控制输入以提高故障检测和隔离的准确性。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands",
            "authors": "Yunshuang Li,Yiyang Ling,Gaurav S. Sukhatme,Daniel Seita",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18455",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18455",
            "arxiv_html_link": "https://arxiv.org/html/2509.18455v1",
            "abstract": "Nonprehensile manipulation, such as pushing and pulling, enables robots to move, align, or reposition objects that may be difficult to grasp due to their geometry, size, or relationship to the robot or the environment. Much of the existing work in nonprehensile manipulation relies on parallel-jaw grippers or tools such as rods and spatulas. In contrast, multi-fingered dexterous hands offer richer contact modes and versatility for handling diverse objects to provide stable support over the objects, which compensates for the difficulty of modeling the dynamics of nonprehensile manipulation.\nTherefore, we propose Geometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile manipulation with dexterous robotic hands. We study pushing and pulling by framing the problem as synthesizing and learning pre-contact dexterous hand poses that lead to effective manipulation. We generate diverse hand poses via contact-guided sampling, filter them using physics simulation, and train a diffusion model conditioned on object geometry to predict viable poses.\nAt test time, we sample hand poses and use standard motion planners to select and execute pushing and pulling actions.\nWe perform 840 real-world experiments with an Allegro Hand, comparing our method to baselines. The results indicate that GD2P offers a scalable route for training dexterous nonprehensile manipulation policies. We further demonstrate GD2P on a LEAP Hand, highlighting its applicability to different hand morphologies. Our pre-trained models and dataset, including 1.3 million hand poses across 2.3k objects, will be open-source to facilitate further research.\nOur project website is available at: geodex2p.github.io.",
            "introduction": "Nonprehensile actions are fundamental to how humans and robots interact with the physical world [4, 5, 6, 7].\nThese actions permit the manipulation of objects that may be too large, heavy, or geometrically complex to grasp directly.\nWhile there has been tremendous progress in nonprehensile robot manipulation [8, 9, 10, 11, 12], most work uses simple end-effectors such as parallel-jaw grippers, rods [13, 14], or spatulas [15]. In contrast, multi-fingered hands with high degrees-of-freedom (DOF) such as the Allegro Hand or LEAP Hand [16] enable contact patterns that can be especially useful for stabilizing complex, awkward, or top-heavy objects, or for coordinating contact across multiple objects, compensating for the challenges of modeling nonprehensile manipulation dynamics.\nHowever, despite their promise and recent progress [17] [18], leveraging high-DOF hands for nonprehensile manipulation remains relatively underexplored due to the challenges of modeling hand-object relationships and planning feasible contact-rich motions.\n\nIn this paper, we study pushing and pulling objects using the 4-finger, 16-DOF Allegro and LEAP Hands. We select pushing and pulling as representative tasks of nonprehensile manipulation because they are more commonly used for manipulating general daily objects and are\nmore amenable to scaling.\nOur insight is to recast this problem into one of synthesizing effective pre-contact hand poses, an approach inspired by recent success in generating large-scale datasets for dexterous manipulation [19, 20, 21, 22, 23, 24].\nWe propose a scalable pipeline for generating hand poses for pushing and pulling objects. This involves contact-guided optimization and validation via GPU-accelerated physics simulation with IsaacGym [25].\nThese filtered hand poses are then used to train a generative diffusion policy conditioned on object geometry, represented using basis point sets [1].\n\nAt test time, we use visual data to reconstruct an object mesh in physics simulation. The trained diffusion policy uses this mesh to generate diverse hand poses for pushing or pulling. We then validate the resulting hand poses in simulation, and execute the best-performing action in the real world.\nWe call this pipeline Geometry-aware Dexterous Pushing and Pulling (GD2P) with multi-fingered hands.\nFigure LABEL:fig:pull shows several real-world examples where the hand pose differs depending on object geometry. Overall, our experimental results across diverse daily objects demonstrate that GD2P is a promising approach for generalizable object pushing and pulling. It outperforms alternative methods such as querying the nearest hand pose in our data or using a fixed spatula-like hand pose, highlighting the need for a diffusion model to generate diverse hand poses.\n\nTo summarize, the contributions of this paper include:\n\nA scalable pipeline for generating and filtering dexterous hand poses for nonprehensile pushing and pulling.\n\nA diffusion model for geometry-conditioned hand pose prediction for nonprehensile pushing and pulling.\n\nA motion planning framework to execute these poses in the real world, with results across 840 trials showing that GD2P outperforms alternative methods.\n\nA dataset of 1.3 million hand poses for nonprehensile pushing and pulling across 2.3k objects with corresponding canonical point cloud observations.\n\n1. A scalable pipeline for generating and filtering dexterous hand poses for nonprehensile pushing and pulling.\n\n2. A diffusion model for geometry-conditioned hand pose prediction for nonprehensile pushing and pulling.\n\n3. A motion planning framework to execute these poses in the real world, with results across 840 trials showing that GD2P outperforms alternative methods.\n\n4. A dataset of 1.3 million hand poses for nonprehensile pushing and pulling across 2.3k objects with corresponding canonical point cloud observations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何利用多指灵巧手进行非抓取操作（如推拉）以处理复杂物体。  \n2. 如何生成有效的手部预接触姿势以提高非抓取操作的成功率。  \n3. 如何在真实环境中执行基于物体几何的手部姿势以实现有效的推拉操作。  \n\n【用了什么创新的方案】  \n提出了一种几何感知的灵巧推拉（GD2P）方法，通过接触引导采样生成多样的手部姿势，并利用物理仿真进行过滤。训练一个条件于物体几何的扩散模型来预测可行的手部姿势。测试时，使用标准运动规划器选择并执行推拉动作。该方法在840次真实实验中表现优于基线，展示了其在不同手形态上的适用性，并提供了130万个手部姿势的数据集以促进后续研究。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction",
            "authors": "Rishabh Madan,Jiawei Lin,Mahika Goel,Angchen Xie,Xiaoyu Liang,Marcus Lee,Justin Guo,Pranav N. Thakkar,Rohan Banerjee,Jose Barreiros,Kate Tsui,Tom Silver,Tapomayukh Bhattacharjee",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
            "comment": "Conference on Robot Learning (CoRL)",
            "pdf_link": "https://arxiv.org/pdf/2509.18447",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18447",
            "arxiv_html_link": "https://arxiv.org/html/2509.18447v1",
            "abstract": "Physical human–robot interaction (pHRI) requires robots to adapt to individual contact preferences, such as where and how much force is applied. Identifying preferences is difficult for a single contact; with whole-arm interaction involving multiple simultaneous contacts between the robot and human, the challenge is greater because different body parts can impose incompatible force requirements. In caregiving tasks, where contact is frequent and varied, such conflicts are unavoidable. With multiple preferences across multiple contacts, no single solution can satisfy all objectives–trade-offs are inherent, making prioritization essential. We present PrioriTouch, a framework for ranking and executing control objectives across multiple contacts. PrioriTouch can prioritize from a general collection of controllers, making it applicable not only to caregiving scenarios such as bed bathing and dressing but also to broader multi-contact settings. Our method combines a novel learning-to-rank approach with hierarchical operational space control, leveraging simulation-in-the-loop rollouts for data-efficient and safe exploration. We conduct a user study on physical assistance preferences, derive personalized comfort thresholds, and incorporate them into PrioriTouch. We evaluate PrioriTouch through extensive simulation and real-world experiments, demonstrating its ability to adapt to user contact preferences, maintain task performance, and enhance safety and comfort. Website: https://emprise.cs.cornell.edu/prioritouch.",
            "introduction": "Physical human–robot interaction (pHRI) requires physical contact. Contact is not uniform: individuals have distinct preferences for acceptable forces and contact locations [1, 2, 3, 4, 5]. For pHRI to be safe and effective, robots must personalize their behavior, and a critical aspect of personalization is contact preferences. Even for a single contact, identifying and respecting these preferences while ensuring task success is challenging. Many physical robot caregiving tasks, such as bathing [6, 7], dressing [8, 9], and transferring [10], require whole‑arm pHRI [11], where multiple segments of the robot arm simultaneously touch the human body. For example, during bed bathing (Fig. 1), the robot may need to reach over a user to wipe the upper arm while maintaining comfortable forces on the torso and shoulder. Although whole‑arm manipulation expands workspace and improves maneuverability, it also exacerbates conflicts: different body parts can impose incompatible force requirements, and no single policy can satisfy all objectives.\n\nTo bootstrap personalization, we elicit population‑level contact preferences offline and use them to seed a conservative base policy. However, a one‑size‑fits‑all policy is insufficient: (i) stated preferences can diverge from realized comfort under true contact (pressure/shear, approach, speed, duration); and (ii) preferences are context‑dependent and time‑varying (posture, clothing, fatigue). Therefore, online interaction is necessary to accommodate individual preferences. Experimenting directly with the user is risky and inefficient because each update can involve repeated physical contact and multiple feedback exchanges. This increases the user’s cognitive workload, prolongs the interaction, and may cause discomfort when forces are suboptimal or excessive.\n\nWe introduce PrioriTouch, a framework that casts contact preference learning as a learning‑to‑rank problem over control objectives. Given a reference trajectory produced by a high‑level policy (e.g., a contact‑aware planner generating end‑effector or joint‑space paths), PrioriTouch instantiates pose‑tracking and force‑regulation objectives from the current contact state. We develop LinUCB‑Rank, a contextual bandit that learns a priority policy; H‑OSC [12] then executes this ordering as a null space hierarchy, translating high‑level preferences into low‑level control. We initialize the policy with conservative priors derived from population-level user-study statistics. During interaction, LinUCB-Rank adapts the ordering online using sparse user feedback while safely refining the policy via simulation-in-the-loop learning before deploying it in real-world interactions. The framework is controller-agnostic: it can rank heterogeneous objectives, enabling principled trade-offs across simultaneous objectives.\n\nWe evaluate PrioriTouch across simulated and real-world environments, progressively increasing in complexity and realism. First, we design a simplified simulation scenario with predefined contacts and a static end-effector pose to isolate and specifically assess LinUCB-Rank’s ability to learn user contact preferences. Second, we demonstrate PrioriTouch in a simulated caregiving scenario involving robot-assisted bed bathing, requiring whole-arm contact to safely wipe a user’s limbs. Third, we showcase our approach’s capability in intricate multi-contact scenarios through a real-world 3D goal-reaching maze with multiple vertical cylinders representing distinct body-part contact preferences. Finally, we validate PrioriTouch’s practical feasibility by performing a realistic caregiving task in a user study with human subjects.\n\nOur contributions are summarized as follows:\n\nWe propose PrioriTouch, a framework that formulates contact preference learning as a ranking problem over control objectives and executes the learned priority ordering as a null space hierarchy via H-OSC for whole-arm pHRI.\n\nWe introduce LinUCB‑Rank, a contextual bandit that learns priority orderings from sparse user feedback while accounting for inter‑objective coupling in hierarchical control.\n\nWe enable safe and data-efficient learning through simulation-in-the-loop validation, where candidate priority updates are tested in a digital twin before real-world deployment.\n\nWe conduct a user study to inform realistic models of contact preferences for robot-initiated touch, which we leverage to simulate authentic user feedback in our evaluation.\n\nWe evaluate PrioriTouch through extensive simulation, real-world experiments, and a realistic caregiving user study, demonstrating effective adaptation to individual contact preferences without compromising task performance or comfort.\n\nOur framework integrates user contact preference learning with low-level control by parameterizing operational space control using the outputs of a learned ranking policy. This structured integration ensures that high-level feedback is directly translated into low-level force regulation and pose tracking, effectively bridging the gap between user preferences and robot control.\n\n1. We propose PrioriTouch, a framework that formulates contact preference learning as a ranking problem over control objectives and executes the learned priority ordering as a null space hierarchy via H-OSC for whole-arm pHRI.\n\n2. We introduce LinUCB‑Rank, a contextual bandit that learns priority orderings from sparse user feedback while accounting for inter‑objective coupling in hierarchical control.\n\n3. We enable safe and data-efficient learning through simulation-in-the-loop validation, where candidate priority updates are tested in a digital twin before real-world deployment.\n\n4. We conduct a user study to inform realistic models of contact preferences for robot-initiated touch, which we leverage to simulate authentic user feedback in our evaluation.\n\n5. We evaluate PrioriTouch through extensive simulation, real-world experiments, and a realistic caregiving user study, demonstrating effective adaptation to individual contact preferences without compromising task performance or comfort.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在物理人机交互中适应个体的接触偏好。  \n2. 在多接触场景中，如何处理不同身体部位的力要求冲突。  \n3. 如何高效地学习和调整接触偏好以确保安全和舒适。  \n\n【用了什么创新的方案】  \nPrioriTouch框架将接触偏好学习视为控制目标的排名问题，通过LinUCB-Rank上下文赌博机从稀疏用户反馈中学习优先级排序，并利用H-OSC以空心空间层次结构执行该排序。该方法结合了模拟环中的验证，确保在真实世界中的安全部署，同时通过用户研究获得个性化的接触阈值，以增强适应性和任务性能。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Latent Action Pretraining Through World Modeling",
            "authors": "Bahey Tharwat,Yara Nasser,Ali Abouzeid,Ian Reid",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18428",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18428",
            "arxiv_html_link": "https://arxiv.org/html/2509.18428v1",
            "abstract": "Vision-Language-Action (VLA) models have gained popularity for learning robotic manipulation tasks that follow language instructions. State-of-the-art VLAs, such as OpenVLA and π0\\pi_{0}, were trained on large-scale, manually labeled action datasets collected through teleoperation. More recent approaches, including LAPA and villa-X, introduce latent action representations that enable unsupervised pretraining on unlabeled datasets by modeling abstract visual changes between frames. Although these methods have shown strong results, their large model sizes make deployment in real-world settings challenging.\nIn this work, we propose LAWM, a model-agnostic framework to pretrain imitation learning models in a self-supervised way, by learning latent action representations from unlabeled video data through world modeling. These videos can be sourced from robot recordings or videos of humans performing actions with everyday objects. Our framework is designed to be effective for transferring across tasks, environments, and embodiments. It outperforms models trained with ground-truth robotics actions and similar pretraining methods on the LIBERO benchmark and real-world setup, while being significantly more efficient and practical for real-world settings.",
            "introduction": "Self-supervised learning has been a key enabler of recent breakthroughs in Large Language Models (LLMs) such as ChatGPT [1] and Gemini [2], where models learn from large amounts of text on the Internet. Inspired by this success, the robotics community is now ready for its own transformative moment, where we can build systems that learn action representations directly from raw, unstructured video data, rather than relying on curated action labels.\n\nMost current approaches to robot learning are heavily based on supervised learning frameworks. Methods like imitation learning and VLA models, including OpenVLA [3] and π0\\pi_{0} [4], require paired image action datasets often obtained through teleoperation. These action annotations are expensive to collect, difficult to scale, and prone to bias, limiting the generalizability of these systems across tasks, environments, and embodiments.\n\nIn this work, we introduce LAWM, a Latent Action pretraining framework through World Modeling that aims to overcome these limitations by combining an Imitation Learning Model with a World Model. Our objective, as shown in Fig. LABEL:fig:first-page, is to learn action representations from both robot-collected and human demonstration videos in a fully self-supervised way. These learned representations serve as action priors that can be effectively leveraged during finetuning on downstream tasks. The proposed framework, illustrated in Figure 2, is designed to be model-agnostic, meaning that it does not depend on any specific architecture for the imitation learning model or the world model. This flexibility allows for the integration of a variety of different models.\nOur pipeline follows a two-stage steps. The first stage is an end-to-end pretraining in a self-supervised way, with the learning signal derived from predicting the next image in a video sequence. The inputs to the system consist of: (i) an image frame from a human or robot performing a manipulation task, and (ii) a natural language instruction describing the goal of the task. The imitation learning model takes these inputs to produce action chunks representations. These representations of nn actions are paired with the current image frame and the next n−1n-1 frames, then fed into the world model to generate the next image frames that would result from executing the action in the current environment. The second stage is a finetuning stage, where labeled data are used to finetune only the imitation learning model to downstream tasks. During this phase, the world model is no longer used. The learned imitation learning model is now equipped with a robust prior from large-scale unlabeled videos and can be finetuned efficiently.\n\nWe summarize our main contributions and findings below:\n\nWe propose LAWM, a model-agnostic framework, to learn action chunk representations for imitation learning models from both robot and human videos without action labels.\n\nOur experiments show that our framework can learn superior action priors from human demonstrations and robotic manipulation videos without using ground-truth action labels, compared to supervised pretraining.\n\nWe demonstrate that our framework with small models such as BAKU [5] and Dreamerv3 [6] outperforms similar methods with large models such as villa-X [7] on the LIBERO benchmark  [8].\n\n1. We propose LAWM, a model-agnostic framework, to learn action chunk representations for imitation learning models from both robot and human videos without action labels.\n\n2. Our experiments show that our framework can learn superior action priors from human demonstrations and robotic manipulation videos without using ground-truth action labels, compared to supervised pretraining.\n\n3. We demonstrate that our framework with small models such as BAKU [5] and Dreamerv3 [6] outperforms similar methods with large models such as villa-X [7] on the LIBERO benchmark  [8].",
            "llm_summary": "【关注的是什么问题】  \n1. 当前机器人学习方法依赖于昂贵的监督学习框架，限制了系统的可扩展性。  \n2. 大规模标注数据集的收集困难且容易产生偏差，影响模型的泛化能力。  \n3. 现有方法在真实世界应用中面临模型规模过大和部署效率低的问题。  \n\n【用了什么创新的方案】  \n提出LAWM框架，通过世界建模在自监督方式下学习潜在动作表示，利用机器人和人类视频数据，无需动作标签。该框架采用两阶段流程：第一阶段进行自监督预训练，第二阶段在有标签数据上微调模仿学习模型。LAWM在LIBERO基准测试中表现优于依赖真实动作标签的模型，且在真实环境中更高效、实用。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections",
            "authors": "Navya Tiwari,Joseph Vazhaeparampil,Victoria Preston",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
            "comment": "6 pages, 5 figures. Accepted as a poster at Northeast Robotics Colloquium (NERC 2025). Extended abstract",
            "pdf_link": "https://arxiv.org/pdf/2509.18407",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18407",
            "arxiv_html_link": "https://arxiv.org/html/2509.18407v1",
            "abstract": "未获取到摘要",
            "introduction": "Intersections account for nearly 40% of U.S. crashes [1], with many occurring at uncontrolled or partially controlled locations [2]. Ambiguous right of way, compounded by occlusions, non-compliant drivers, and limited sensing leave drivers uncertain how to act [3]. Addressing these challenges requires assistive technology that reduces driver uncertainty and improves awareness. We propose an Advanced Driver Assistance System (ADAS) that fuses sensor data, interprets intersection context, and applies uncertainty-aware frameworks to recommend safe actions at uncontrolled intersections. We pose three research questions (RQs):\n\nHow can the accuracy of ego- and external-vehicle state estimation be improved while constraining uncertainty through frustum-based fusion of camera and lidar data, given real-time, computationally limited resources?\n\nHow can the accuracy of ego- and external-vehicle state estimation be improved while constraining uncertainty through frustum-based fusion of camera and lidar data, given real-time, computationally limited resources?\n\nTo what extent can a driver-assist system enable safe navigation of uncontrolled intersections by efficiently interpreting intersection context (lane markings, stop signs, traffic flow patterns, and pedestrian presence) under partial observability?\n\nHow effectively can different decision-making frameworks handle uncertainty at uncontrolled intersections, and what trade-offs emerge between safety, efficiency, and computational feasibility in real-time deployment?\n\nHere, we present an initial analysis of RQ3 in synthetic uncontrolled intersections. We show that probabilistic planners, particularly POMCP (Partially Observable Monte Carlo Planning) [7] and DESPOT (Determinized Sparse Partially Observable Tree) [15], outperform deterministic approaches in predicting the intent of other drivers and selecting collision-free actions, while maintaining safety under complex, partially observable scenarios. Continued work will integrate Sensor Fusion (RQ1) and Environment Perception (RQ2) modules for end-to-end, real-time navigation under realistic traffic and environmental conditions.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高自我和外部车辆状态估计的准确性，同时在实时计算资源有限的情况下约束不确定性？  \n2. 如何在部分可观测的情况下高效解读交叉口环境，以实现安全导航？  \n3. 不同决策框架在处理不确定性时的有效性如何，以及在实时部署中安全性、效率和计算可行性之间的权衡是什么？  \n\n【用了什么创新的方案】  \n提出了一种先进的驾驶辅助系统（ADAS），通过融合传感器数据、解释交叉口上下文，并应用不确定性感知框架，推荐在无人控制交叉口的安全行动。初步分析表明，概率规划方法（如POMCP和DESPOT）在预测其他驾驶员意图和选择无碰撞行动方面优于确定性方法，同时在复杂的部分可观测场景中保持安全。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation",
            "authors": "Rajitha de Silva,Jonathan Cox,James R. Heselden,Marija Popovic,Cesar Cadena,Riccardo Polvara",
            "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
            "comment": "Sumbitted to ICRA 2026",
            "pdf_link": "https://arxiv.org/pdf/2509.18342",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18342",
            "arxiv_html_link": "https://arxiv.org/html/2509.18342v1",
            "abstract": "Accurate localisation is critical for mobile robots in structured outdoor environments, yet LiDAR-based methods often fail in vineyards due to repetitive row geometry and perceptual aliasing. We propose a semantic particle filter that incorporates stable object-level detections, specifically vine trunks and support poles into the likelihood estimation process. Detected landmarks are projected into a bird’s eye view and fused with LiDAR scans to generate semantic observations. A key innovation is the use of semantic walls, which connect adjacent landmarks into pseudo-structural constraints that mitigate row aliasing. To maintain global consistency in headland regions where semantics are sparse, we introduce a noisy GPS prior that adaptively supports the filter. Experiments in a real vineyard demonstrate that our approach maintains localisation within the correct row, recovers from deviations where AMCL fails, and outperforms vision-based SLAM methods such as RTAB-Map.",
            "introduction": "Accurate localisation is a critical component of mobile robot navigation in outdoor environments [1]. Among the various approaches, LiDAR-based localisation remains widely adopted due to its reliable and precise perception of geometric structure. However, these methods rely solely on scene geometry, which can be problematic in outdoor agricultural settings like vineyards, where repetitive and ambiguous structures are common [2]. In such environments, incorporating semantic information complements the geometric structure offering a promising alternative to enhance localisation performance [3].\n\nIn this paper, we tackle the challenge of semantic ambiguity in geometry-based localisation within vineyard environments. The repetitive structure of vineyard rows often induces perceptual aliasing in LiDAR range data, resulting in localisation drift and errors. To overcome this limitation, we exploit semantically meaningful landmarks, specifically vine trunks and support poles whose distinctive spatial distributions provide stronger discriminative cues. Our approach detects these semantic objects and estimates their relative positions from RGB-D imagery, which are then projected onto the LiDAR frame. This enables a semantic-LiDAR particle filter that offers a robust alternative to conventional localisation methods.\n\nTraditional particle filters, such as Adaptive Monte Carlo Localisation (AMCL) [4], estimate a robot’s pose by evaluating the geometric consistency between sensor observations and a known map, an approach that has proven highly effective in structured indoor and urban settings where distinctive geometric features are abundant. Vineyards, however, present a markedly different challenge: their long, repetitive rows induce strong perceptual aliasing, while unstable elements such as foliage and grape clusters provide little reliability for long-term localisation [5]. We contend that robust localisation in such environments requires moving beyond raw geometry and explicitly exploiting semantics. Our key insight is that vine trunks and support poles serve as stable, distinctive landmarks whose consistent spatial distribution across rows can disambiguate pose estimates. Moreover, we introduce the concept of semantic walls, where the space between consecutive landmarks is modelled as a pseudo-structural boundary. This transforms sparse semantic detections into continuous row-level constraints, creating a representation that is far more robust to vineyard aliasing and seasonal variation. Together, these ideas lay the foundation for a semantic-LiDAR particle filter that redefines localisation in repetitive agricultural environments, as illustrated in Fig. 1.\n\nThe main contributions of this paper are threefold:\n(i) the design of a semantic particle filter that integrates object-level detections of stable vineyard landmarks (trunks and support poles) with a 2D semantic map, enabling robust localisation in highly repetitive environments;\n(ii) the introduction of the semantic walls concept, which augments sparse landmark detections by modelling pseudo-structural boundaries between adjacent landmarks, thereby strengthening row-level constraints and mitigating perceptual aliasing; and\n(iii) a systematic evaluation against established baselines, demonstrating the trade-offs in accuracy, robustness, and sensor requirements.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在重复结构的环境中实现准确的移动机器人定位。  \n2. 如何克服LiDAR方法在葡萄园中因感知别名而导致的定位漂移和错误。  \n3. 如何利用语义信息增强几何结构以提高定位性能。  \n\n【用了什么创新的方案】  \n提出了一种语义粒子滤波器，将稳定的对象级检测（如葡萄藤干和支撑杆）融入到定位过程中，通过将检测到的地标投影到鸟瞰图并与LiDAR扫描融合生成语义观测。此外，引入了语义墙的概念，将相邻地标连接成伪结构约束，以减轻行别名问题，并在语义稀疏的区域引入噪声GPS先验以维持全局一致性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020",
            "authors": "Marsette Vona",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18330",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18330",
            "arxiv_html_link": "https://arxiv.org/html/2509.18330v1",
            "abstract": "The Landform contextual mesh fuses 2D and 3D data from up to thousands of Mars 2020 rover images, along with orbital elevation and color maps from Mars Reconnaissance Orbiter, into an interactive 3D terrain visualization. Contextual meshes are built automatically for each rover location during mission ground data system processing, and are made available to mission scientists for tactical and strategic planning in the Advanced Science Targeting Tool for Robotic Operations (ASTTRO). A subset of them are also deployed to the ”Explore with Perseverance” public access website.",
            "introduction": "Like its predecessors including Spirit, Opportunity, and Curiosity, the Mars 2020 Perseverance rover carries a suite of stereo cameras to image the surrounding terrain [1, 2, 3]. Data from those instruments is used for multiple purposes including both on-board and ground-based navigation, engineering operations, and science analysis. An important use case is tactical and strategic science planning, where teams of mission scientists use the imagery to select areas of interest and plan subsequent observations.\n\nStereo vision produces a “tactical wedge” 3D terrain mesh for each stereo image pair, so-called because often a radial panorama of such wedges is acquired from a single rover location using the pan/tilt mast. Of course, only a fraction of nearby terrain is included since the cameras have limited fields of view, effective resolution decreases with distance, the rover occludes areas underneath itself, nearby rocks and hills create self-occlusions in the terrain, and stereo reconstruction fails in areas with insufficient texture. Nevertheless, viewing such panoramas of tactical wedges in 2D and 3D (latter also called the tactical mesh) has been a standard approach for science planning on Mars 2020 and its predecessors. Figure 1(a) shows such a view for site 40, drive 132 of the Perseverance rover, acquired on sols 821–832111One sol is the equivalent of a day on Mars..\n\nAn alternative to the tactical mesh is to use a portion of a digital elevation map (DEM) derived from orbital observations, as shown in Figure 1(b). The Mars 2020 mission typically uses colored DEM data from the HiRISE instrument on the Mars Reconnaissance Orbiter [4]. This orbital mesh can cover a much larger extent—up to 10s of km—and typically has no gaps in its coverage. However, at a typical resolution of 1 elevation sample (and 16 color samples) per square meter, it’s much coarser than the tactical mesh, which can have sub-millimeter resolution near the rover.\n\nIn this paper we introduce the contextual mesh, which we have developed to fuse up to thousands of images from in-situ stereo cameras together with orbital DEM data into a single 3D scene, shown in Figure 1(c) and Figure 2. The contextual mesh is produced by Landform, a subsystem within the Mars 2020 ground data system (M20 GDS), and typically viewed in the Advanced Science Targeting Tool for Robotic Operations (ASTTRO) collaborative web application [5, 6], also part of the M20 GDS [7].\n\nWhereas the tactical mesh offers the highest fidelity local terrain reconstruction, and the orbital mesh the longest range reconstruction, the intention of the contextual mesh is to provide spatial awareness. It is typically visualized from a first-person navigable 3D point of view in ASTTRO, showing not only local terrain features such as sand, pebbles, rocks, ridges, and hills, but also distant landmarks on the horizon. ASTTRO also displays a 3D model of the Perseverance rover on the terrain as it was posed at the corresponding time in the mission.\n\nEach contextual mesh is comprised of two tilesets in the open-standard 3DTiles format [8]. One tileset contains the terrain itself, typically extending to a 1km square with a 100m square central detail area. The other tileset is a hemispherical representation of the surrounding horizon and sky, enabling visualization of distant features potentially many kilometers away. ASTTRO displays both of these simultaneously so that users can see the context of local terrain features relative to both nearby hills and to the horizon.\n\nThe 3DTiles format enables data to be progressively streamed to distributed users in the web-based ASTTRO client. Only the subset of tile data required depending on the user’s current viewpoint is transferred and rendered, enabling dynamic level-of-detail, fast load times, and deployment to resource limited clients.\n\nLike the products of many data fusion and reconstruction algorithms, e.g. computed tomography, and considering that the input data contains noise and outliers, the contextual mesh may contain some artifacts. For example\n\nboundaries between areas reconstructed primarily from surface vs orbital data may have some discontinuities\n\noutlier images with extreme brightness variations may not be completely blended\n\nreconstructed geometry may have “island” topological artifacts due to noise and residual misalignment in the input data.\n\nThe Landform contextual mesh differs from many other photogrammetry and terrain fusion systems not only because it combines both surface and orbital data and has a sky sphere, but also in that it is entirely automated, whereas most other systems require some human intervention. It heavily leverages properties of Mars mission datasets, including pose priors from rover navigation, calibrated stereo camera data, and co-registered orbital data. These enable automated processing while maintaining reasonable quality relative to manual and semi-manual approaches.\n\nThe Landform codebase will soon be released as open source. And, throughout the mission, a selection of contextual meshes have been made publicly available for interactive viewing at the “Explore with Perseverance” website [9].\n\nIn this paper we summarize the research context of related terrain fusion approaches, describe the novel algorithms we developed to implement the contextual mesh system in Landform, and present examples of contextual mesh data products.\n\n1. boundaries between areas reconstructed primarily from surface vs orbital data may have some discontinuities\n\n2. outlier images with extreme brightness variations may not be completely blended\n\n3. reconstructed geometry may have “island” topological artifacts due to noise and residual misalignment in the input data.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何有效融合火星探测器的2D和3D数据以生成高质量的地形可视化。  \n2. 在自动化处理过程中如何保持数据融合的质量，减少人为干预。  \n3. 如何在动态环境中实现3D场景的实时可视化和交互。  \n\n【用了什么创新的方案】  \n本文提出了一种名为“Landform contextual mesh”的新方法，通过自动融合来自火星2020探测器的数千张立体图像与轨道数字高程图（DEM）数据，生成一个交互式的3D地形可视化。该方法利用了火星任务数据集的特性，自动处理并生成高保真度的地形重建，提供了局部和远程地形特征的空间意识。此外，使用3DTiles格式实现数据的渐进式流式传输，支持动态细节级别和快速加载时间。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Haptic Communication in Human-Human and Human-Robot Co-Manipulation",
            "authors": "Katherine H. Allen,Chris Rogers,Elaine S. Short",
            "subjects": "Robotics (cs.RO)",
            "comment": "9 pages, 18 figures, ROMAN 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18327",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18327",
            "arxiv_html_link": "https://arxiv.org/html/2509.18327v1",
            "abstract": "When a human dyad jointly manipulates an object, they must communicate about their intended motion plans. Some of that collaboration is achieved through the motion of the manipulated object itself, which we call “haptic communication.” In this work, we captured the motion of human-human dyads moving an object together with one participant leading a motion plan about which the follower is uninformed. We then captured the same human participants manipulating the same object with a robot collaborator. By tracking the motion of the shared object using a low-cost IMU, we can directly compare human-human shared manipulation to the motion of those same participants interacting with the robot. Intra-study and post-study questionnaires provided participant feedback on the collaborations, indicating that the human-human collaborations are significantly more fluent, and analysis of the IMU data indicates that it captures objective differences in the motion profiles of the conditions. The differences in objective and subjective measures of accuracy and fluency between the human-human and human-robot trials motivate future research into improving robot assistants for physical tasks by enabling them to send and receive anthropomorphic haptic signals.",
            "introduction": "In physical collaboration tasks like carrying a couch or moving a table, haptic signals are an important channel of communication between participants to coordinate the group action. In human-human interactions, the communication and interpretation of these signals is primarily subconscious, but prior research suggests that they may enable more efficient human-robot collaboration [1]. In order for robots to participate in this haptic conversation, we need to develop a more robust understanding of how haptic communication occurs in both human-human and human-robot interaction. This knowledge can then be used to develop models for interpreting haptic intent, provide robots with comprehensible and predictable behavior, and avoid unwanted oscillations in collaborative manipulation.\n\nIn this paper, we present a study of haptic interaction, without use of visual or auditory signaling, during the collaborative manipulation of a shared object. We compare human-human and human-robot dyads to test whether there are observable differences in the subjective fluency of human-human and human-robot dyads, and whether these correlate with changes in the character of acceleration profiles of the co-manipulated objects. We additionally collect data on human perceptions of robot collaborators to identify potential co-variables in subjective fluency.\n\nWe conducted a user study with 34 participants. In the study, two agents collaborated to move a shared object, with one participant designated as the motion leader and one as a follower. Each participant acted in one these roles, and interacted with both another human participant and a mobile manipulator robot. We collected measures of subjective and objective measures of task fluency, as well as video and IMU recordings. We found that the acceleration data from an IMU mounted on the shared object changes more smoothly in human-human dyads than in human-robot dyads, with more fluent collaborations having smaller accelerations overall and smaller changes in acceleration during the task. We further find that common objective measures of collaboration fluency (e.g. task duration) do not correlate linearly with subjective fluency measures, and propose alternate measures based on our data. This work contributes to our understanding of the differences and similarities in current human-human and human-robot haptic communication during collaborative manipulation, and provides insights that can inform future methods for autonomous haptic signaling by robots.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何理解人类之间的触觉通信在共同操控中的作用？  \n2. 人类与机器人之间的触觉通信是否存在显著差异？  \n3. 如何提高机器人在物理任务中的协作能力？  \n\n【用了什么创新的方案】  \n本研究通过用户研究比较了人类与人类和人类与机器人在共同操控共享物体时的触觉交互。使用低成本IMU捕捉物体运动，分析了不同条件下的加速度数据，发现人类之间的协作流畅性显著高于人机协作。研究还探讨了主观流畅性与客观加速度特征之间的关系，为未来机器人自主触觉信号的研究提供了见解。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Fine-Tuning Robot Policies While Maintaining User Privacy",
            "authors": "Benjamin A. Christie,Sagar Parekh,Dylan P. Losey",
            "subjects": "Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18311",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18311",
            "arxiv_html_link": "https://arxiv.org/html/2509.18311v1",
            "abstract": "Recent works introduce general-purpose robot policies.\nThese policies provide a strong prior over how robots should behave — e.g., how a robot arm should manipulate food items.\nBut in order for robots to match an individual person’s needs, users typically fine-tune these generalized policies — e.g., showing the robot arm how to make their own preferred dinners.\nImportantly, during the process of personalizing robots, end-users leak data about their preferences, habits, and styles (e.g., the foods they prefer to eat).\nOther agents can simply roll-out the fine-tuned policy and see these personally-trained behaviors.\nThis leads to a fundamental challenge: how can we develop robots that personalize actions while keeping learning private from external agents?\nWe here explore this emerging topic in human-robot interaction and develop PRoP, a model-agnostic framework for personalized and private robot policies.\nOur core idea is to equip each user with a unique key; this key is then used to mathematically transform the weights of the robot’s network.\nWith the correct key, the robot’s policy switches to match that user’s preferences — but with incorrect keys, the robot reverts to its baseline behaviors.\nWe show the general applicability of our method across multiple model types in imitation learning, reinforcement learning, and classification tasks.\nPRoP is practically advantageous because it retains the architecture and behaviors of the original policy, and experimentally outperforms existing encoder-based approaches.\nSee videos and code here: https://prop-icra26.github.io",
            "introduction": "Generalist policies enable robots to learn multiple tasks [1, 2].\nSo far these methods have traditionally been used in research labs and factories.\nBut we envision a future where robots enter domestic settings for assisting humans [3].\nFor example, consider a robot that is developed to help in a kitchen.\nThis robot will have some initial policy π0\\pi_{0} that users may want to finetune to match their own preferences and requirements.\nFor instance, perhaps the robot knows how to make a hamburger, but individual users prefer different ingredients, condiments, or even specific ways of stacking the burger.\nThis finetuning raises privacy concerns: the manufacturers can share the users’ data collected during finetuning with third-parties.\nConsequently, there is increasing demand for exploring new avenues to maintain the privacy and transparency of robotic agents [4].\nFollowing this, we come to a fundamental scientific question: how do we make systems that can learn and adapt to individual end-users, while still maintaining those user’s privacy?\n\nPrivacy in machine learning has traditionally been examined from two perspectives.\nFirst is data privacy, which concerns safeguarding the sensitive information of individuals represented in the dataset [5, 6, 7, 8].\nSecond is model privacy, which focuses on protecting the learned parameters of a neural network through techniques such as encryption or differentially-private learning [9, 10, 11, 12].\nIn this work, we adopt a third perspective with respect to robot learning: ensuring that a trained, personalized robot does not leak user preference information to other users.\nReturning to our example, privacy in this context means that the robot can be finetuned to learn your preferred way of making a burger while preventing unauthorized users from accessing those preferences even if they have access to the trained model.\nIn practice, this can be difficult to achieve because — if someone has access to the finetuned model — they can roll-out this model and infer the previous user’s preference by watching the robot actions.\nSo how do we safeguard privacy of user preferences?\nOur insight is that:\n\nConcretely, we leverage keys (Figure 1).\nA key is any feature that is unique to the user such as facial structures, vocal patterns, or a textual password.\nWhen finetuning the robot under out approach, a user combines their unique key with the intermediate features of the network and trains it to output their personalized actions.\nThis unique mechanism for personalizing robots safeguards user privacy since preference information remains inaccessible to anyone who does not have the user’s key.\nWithout careful design, keys may unintentionally cause the robot to forget its general-purpose policy.\nBut our technical approach avoids this pitfall — and preserves the initial model architecture — by leveraging the key to perform mathematical operations on the intermediate weights.\nOur proposed mechanism is not tied to a specific network architecture or application as we later demonstrate in our experiments with visual data, imitation learning, MLP classifiers, and reinforcement learning.\nIndeed, as shown in our experiments on robot arms, users can finetune the robot to make their desired hamburger without losing the robot’s previously learned behaviors, and without exposing their preference to other agents.\nWe see this work as a step towards safe and personalized human-robot interaction.\n\nOverall, we make the following contributions:\n\nKey-based Personalization of Robot Policies.\nWe present a formulation for key-based personalization of robot control policies.\nUnder this formalism, the robot learns to personalize to new users’ specifications while retaining its original, general behavior.\nThis formalism is nontrivial to implement in a learning algorithm, since the original and conditional policies operate in different domains, i.e., adding a key as the input requires changing the size of the pre-trained architecture.\nInstead, we use keys to transform the intermediate features of the pre-trained policy, circumventing the need for changing the architecture size.\n\nPersonalized and Private Robot Policies.\nWe present our implementation of the aforementioned key-based personalization with privacy guarantees.\nOur method, PRoP (Personalized and Private Robot Policies) retains the original network architecture, exhibits behavior of the original robot policy for unprivileged users, and personalizes to specific users through a privacy-oriented mechanism.\nImportantly, PRoP extends to arbitrary learning rules and architectures that enables simple, end-to-end training of the model.\n\nReal-world Validation and Empirically Verified Robustness.\nWe empirically test the performance of PRoP in a collection of controlled simulations and real-world studies, including Imitation Learning, Reinforcement Learning, Image Classification, and Task Allocation.\nWe further extend PRoP to more complex settings, such as language prose personalization and key-based obfuscation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在个性化机器人行为的同时保护用户隐私？  \n2. 现有的机器人政策如何防止用户偏好信息泄露？  \n3. 如何在保持原有政策架构的情况下实现个性化？  \n\n【用了什么创新的方案】  \n本研究提出了一种名为PRoP的模型无关框架，通过为每个用户分配一个唯一的密钥来实现个性化和隐私保护。该密钥用于对机器人的网络权重进行数学变换，使得机器人可以根据用户的偏好进行调整，而未授权用户则无法访问这些偏好信息。此外，该方法保留了原始政策的架构和行为，并在多种学习任务中表现优于现有方法。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies",
            "authors": "Jesse Zhang,Marius Memmel,Kevin Kim,Dieter Fox,Jesse Thomason,Fabio Ramos,Erdem Bıyık,Abhishek Gupta,Anqi Li",
            "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
            "comment": "11 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18282",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18282",
            "arxiv_html_link": "https://arxiv.org/html/2509.18282v1",
            "abstract": "Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: (1) end-effector paths specifying what actions to take, and (2) task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4×\\times real-world improvement for a 3D policy trained only in simulation, and 2–3.5×\\times gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need—where, what, and how.\nWebsite at https://peek-robot.github.io.",
            "introduction": "Imagine walking through a crowded store when your child suddenly cries out, “I want the Labubu!”\nThough you’ve never heard the word before, context clues guide your eyes to the fuzzy toy on the shelf, and you effortlessly weave through the crowd to grab it.\nWhat makes this possible is not raw perception ability, but the ability to interpret ambiguous instructions and distill them into just the right cues—where to focus, what actions to take, and how to perform these actions at the low level. Similarly, if given where to focus and what motions to take, a robot manipulation policy should be able to achieve the visual robustness and semantic generalization necessary for open-world deployment by focusing only on how to perform actions.\n\nA common tactic for training manipulation policies is through imitation learning of human-collected robotics data [1, 2, 3, 4], which attempts to learn the where, what, and how all at the same time.\nYet their performance degrades on novel objects, clutter, or semantic variations [5, 6], since the policy alone bears the burden of handling task, semantic, and visual complexity.\nSuch failures often entangle the axes of where, what, and how—for example, grasping a distractor simultaneously reflects misplaced attention, an incorrect object choice, and a wrong motion.\n\nOur key idea is to offload high-level reasoning to vision-language models (VLMs), which can excel at semantic and visual generalization [7, 8], leaving the policy to determine how low-level behavior should be executed. Instead of forcing the policy to directly parse raw images and instructions, a high-level VLM modulates the input representation to the low-level policy by providing: (1) a path that encodes what the policy should do, and (2) masks showing where to attend. By “absorbing” semantic and visual variation, the VLM provides the policy a simplified, annotated “peek” of the scene that gives the what and the where, while the policy only needs to learn how to perform the low-level actions. This intermediate representation helps policy execution inherit many of the VLM’s semantic and visual generalization capabilities. Our VLM-modulated representation is naturally policy-agnostic, allowing it to be applied to arbitrary image-input robot manipulation policies, including state-of-the-art RGB and 3D manipulation policies [9, 1, 3].\n\nTo concretely instantiate this insight into a practical algorithm, we introduce PEEK (Policy-agnostic Extraction of Essential Keypoints), which proposes a unified, point-based intermediate representation that trains VLMs to predict what policies should do and where to focus on. Specifically, we propose to finetune pretrained VLMs [10] to predict a sequence of points corresponding to (1) a path that guides the robot end-effector in what actions to take and (2) a set of task-relevant masking points that show the policy where to focus on (see Figure 1). During low-level visuomotor policy training and inference, we modulate the policy’s image observations by directly drawing these VLM-predicted paths and masks onto the image, allowing the policy to simply focus on how to act, rather than learning all three simultaneously. Doing so significantly bolsters policy generalization, combining the generality of high-level VLM predictions with the precision of low-level policy learning. In this paper, we instantiate a full-stack implementation of PEEK, from devising a scalable data annotation scheme that enables large-scale VLM finetuning on robotic datasets to representation-modulated training of low-level robot policies from simulation and real world data.\n\nIn 535 real-world evaluations across 17 task variations, we demonstrate that PEEK consistently boosts zero-shot policy generalization: a 3D policy (3DDA [9]) trained only in simulation achieves 41.4×\\times higher success in the real world when guided by PEEK, and both large-scale vision-language-action models (π0\\pi_{0} [3]) and small transformer-based policies [1] see 2–3.5×\\times success rate improvements. These results demonstrate the power of using high-level VLMs to absorb task complexity, providing low-level policies with exactly the minimal cues they need for generalizable manipulation.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何提高机器人操作策略的零-shot泛化能力。  \n2. 如何有效地将高层次推理任务分配给视觉-语言模型（VLMs）。  \n3. 如何简化机器人策略的输入表示以增强其性能。  \n\n【用了什么创新的方案】  \nPEEK（Policy-agnostic Extraction of Essential Keypoints）提出了一种统一的基于点的中间表示，旨在通过微调视觉-语言模型（VLMs）来预测机器人操作所需的路径和关注区域。这种表示直接叠加在机器人观察上，使得策略可以专注于如何执行操作，而不是同时学习“在哪里”、“做什么”和“如何做”。通过引入自动标注管道，PEEK能够在多个机器人数据集上生成标注数据，从而提升零-shot泛化能力，显著提高了在真实世界中的成功率。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "A Fast Initialization Method for Neural Network Controllers: A Case Study of Image-based Visual Servoing Control for the multicopter Interception",
            "authors": "Chenxu Ke,Congling Tian,Kaichen Xu,Ye Li,Lingcong Bao",
            "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.19110",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.19110",
            "arxiv_html_link": "https://arxiv.org/html/2509.19110v1",
            "abstract": "Reinforcement learning-based controller design methods often require substantial data in the initial training phase. Moreover, the training process tends to exhibit strong randomness and slow convergence. It often requires considerable time or high computational resources.\nAnother class of learning-based method incorporates Lyapunov stability theory to obtain a control policy with stability guarantees. However, these methods generally require an initially stable neural network control policy at the beginning of training.\nEvidently, a stable neural network controller can not only serve as an initial policy for reinforcement learning, allowing the training to focus on improving controller performance, but also act as an initial state for learning-based Lyapunov control methods.\nAlthough stable controllers can be designed using traditional control theory, designers still need to have a great deal of control design knowledge to address increasingly complicated control problems.\nThe proposed neural network rapid initialization method in this paper achieves the initial training of the neural network control policy by constructing datasets that conform to the stability conditions based on the system model.\nFurthermore, using the image-based visual servoing control for multicopter interception as a case study, simulations and experiments were conducted to validate the effectiveness and practical performance of the proposed method.\nIn the experiment, the trained control policy attains a final interception velocity of 15 m/s.",
            "introduction": "In recent years, an increasing number of studies have utilized learning-based methods to address control issues[1]. There are two types of learning-based methods for control problem.\nOne is the reinforcement learning (RL) method, and the other is the learning-based Lyapunov control (LLC) method.\nIn contrast to conventional control methods, RL techniques engage with the environment via trial and error to identify optimal strategies and may complete intricate tasks without dependence on exact models of the controlled entities.\nRL, akin to control systems, functions through feedback mechanisms.\nWhile RL largely uses input to refine its decision-making processes, control systems focus on achieving predetermined targets mainly by using static controller techniques during operation.\nThe training process of RL can be unstable and unsafety[2], especially in safety-critical situations like the unmanned aerial vehicle (UAV) visual servoing control.\nIn order to ensure that the trained policy can be applied in practice, the datasets used in the training process should encompass the Region of Interest (RoI)[3; 4], which is exceedingly challenging before obtaining a available control policy.\n\nThe Lyapunov stability method provides a definitive analytical and design framework in control theory, especially for nonlinear systems [5].\nNumerous research studies have recently integrated Lyapunov stability approaches into learning-based control, referred to as Lyapunov function learning, thereby providing formal stability guarantees for deep neural network policies.\nIn the studies [6; 7], the Lyapunov function is utilized as a critic function to assess policies performance.\nIn [8] and [9], Lyapunov functions are integrated into optimization frameworks to guarantee system stability.\nThe Lyapunov stability condition is incorporated into the reward design in [10] and [11].\nReferences [10] and [11] develop the target control policy by incorporating a Lyapunov function into the reward design.\nThe research in [12] proposes learning the Lyapunov function and its derivative (referred to as the D-function) from expert demonstration data while adhering to stability constraints, thus facilitating the development of a control policy that inherently ensures Lyapunov stability.\nNote that uniformly sampled data is necessary for this approach.\nOtherwise, the D-function employed may not accurately represent the actual system model.\nWhile these methods offer formal stability guarantees for the target policy and yield favorable outcomes, they depend on the posteriori expert controllers or trajectories and are not suitable for the original design of the control policy.\n\nThe posteriori expert controllers are also used as the initial policy of RL to circumvent the drawback of slow convergence at the beginning of the training.\nAlthough the conventional control theory can be applied for the controller designing, the rich experience of that is also important to solve a complex control problem.\nTherefore, this paper proposed an initial policy training method that involves constructing datasets that meet stability requirements and then training a neural network control policy based on the datasets.\nMoreover, acquiring a group of datasets without a stable control policy is exceedingly challenging.\nConversely, without a controller, acquiring the model of controlled objects may be more attainable than gathering data.\nUtilizing the datasets produced by mathematical models that adhere to Lyapunov stability, an untrained neural network may be directly developed into a control policy, circumventing the conventional control design and debugging procedures.\nThe trained neural network control policy can be enhanced further by RL methods or the LLC method that needs an initial stable control policy [2].\nIn this paper, the case of image-based visual servoing control for the multicopter interception is adopted to demonstrate the effectiveness of the proposed method.\nIn the experiment, a final flight speed of up to 15 m/s was achieved.\nNoted that the purpose of this method is to obtain a usable control policy with mediocre performance at least, and the optimization of the control policy still needs to be accomplished through RL and LLC methods.\n\nThe paper is organized as follows: Section II outlines the coordinate systems and mathematical models employed. Section III presents the rapid methodology for training neural network policies, applies it to the design of multicopter interception control, and validates the stability of the trained policies by the almost Lyapunov condition[13]. Section IV presents the experimental result of the trained policies implemented on quadrotor platforms.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何快速初始化神经网络控制器以提高训练效率？  \n2. 如何确保在强化学习和Lyapunov控制中使用的控制策略具备稳定性？  \n\n【用了什么创新的方案】  \n提出了一种快速初始化方法，通过构建符合稳定性条件的数据集来实现神经网络控制策略的初始训练。该方法避免了传统控制设计的复杂性，使得未训练的神经网络可以直接发展为控制策略，从而提高训练效率并确保稳定性。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Guaranteed Robust Nonlinear MPC via Disturbance Feedback",
            "authors": "Antoine P. Leeman,Johannes Köhler,Melanie N. Zeilinger",
            "subjects": "Optimization and Control (math.OC); Robotics (cs.RO); Systems and Control (eess.SY)",
            "comment": "Code:this https URL",
            "pdf_link": "https://arxiv.org/pdf/2509.18760",
            "code": "https://github.com/antoineleeman/robust-nonlinear-mpc",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18760",
            "arxiv_html_link": "https://arxiv.org/html/2509.18760v1",
            "abstract": "Robots must satisfy safety-critical state and input constraints despite disturbances and model mismatch.\nWe introduce a robust model predictive control formulation that is fast, scalable, and compatible with real-time implementation.\nOur formulation guarantees robust constraint satisfaction, input-to-state stability and recursive feasibility.\nThe key idea is to decompose the uncertain nonlinear system into (i) a nominal nonlinear dynamic model, (ii) disturbance-feedback controllers, and (iii) bounds on the model error. These components are optimized jointly using sequential convex programming.\nThe resulting convex subproblems are solved efficiently using a recent disturbance-feedback MPC solver.\nThe approach is validated across multiple dynamics, including a rocket-landing problem with steerable thrust. An open-source implementation is available at https://github.com/antoineleeman/robust-nonlinear-mpc.",
            "introduction": "Autonomous robots, whether agile drones, wheeled machines, or (autonomous) spacecrafts, must operate in dynamic and uncertain environments while satisfying strict safety and performance requirements [1].\nIn addition, model mismatch arises naturally due to many factors, such as wind gusts, actuators misalignments, or unmodelled frictions.\nIn robotics applications, disturbances such as wind gusts, actuator misalignments, or unmodeled friction are typically handled by introducing ad hoc safety margins in the control design, resulting in slower motions, reduced maneuverability, and under-utilization of the system’s capabilities.\n\nReinforcement learning, often with domain randomization, has recently shown success in achieving robust sim2real performance[2], particularly in contact-rich tasks. While learned policies can be executed in real time, training requires extensive offline computation, careful reward design, and heuristics to ensure convergence.\n\nIn contrast, trajectory-optimization methods enforce constraint satisfaction for nonlinear dynamics and are widely used in practice as a model-based control technique based on sequential convex programming [3].\nHowever, these methods typically do not ensure safety or stability in the presence of disturbances, which is critical for real-world deployment.\n\nIn this paper, we introduce a scalable robust model predictive control formulation for nonlinear systems that is safe-by-design.\nRobust model predictive control commonly accounts for disturbances by predicting a set containing all possible future states [4].\nTo reduce conservatism, these robust predictions are based on closed-loop predictions and a corresponding feedback law is typically optimized offline, e.g., using contraction metrics [5, 6].\nHowever, fixing the feedback a priori can limit closed-loop performance and the offline computations also limit scalability.\nRobust model predictive control approaches that optimize feedback laws to reduce conservatism have been proposed in [7, 8], which rely on (conservative) sequential over-approximations of the robust predictions.\nIn contrast, the disturbance feedback MPC [9] framework framework (also known as system level synthesis [10]) provides an exact characterization of the robust prediction for linear time-varying systems, thereby avoiding this compounding effect. Recent extensions [11, 12] further enable its application to nonlinear systems.\nWhile these formulations improve performance compared to fixed policy approaches, they generally do not provide guarantees of recursive feasibility or stability. However, such guarantees are crucial, since loss of feasibility at any step can cause safety constraint violations.\nContribution: Building on the nonlinear SLS formulation in [11],\nwe propose a fast and scalable robust model predictive control formulation for nonlinear systems with robust closed-loop guarantees.\nOur approach jointly optimizes the nominal nonlinear trajectory, a disturbance-feedback controller, and an upper bound on the prediction error.\n\nFormal guarantees are provided, i.e., robust constraint satisfaction, recursive feasibility (Thm. 1), and input-to-state stability (Thm. 2). Recursive feasibility is ensured by a novel treatment of the mismatch with respect to the nonlinear nominal prediction.\n\nFormal guarantees are provided, i.e., robust constraint satisfaction, recursive feasibility (Thm. 1), and input-to-state stability (Thm. 2). Recursive feasibility is ensured by a novel treatment of the mismatch with respect to the nonlinear nominal prediction.\n\nAn efficient sequential convex programming algorithm tailored to the robust MPC formulation is provided to enable real-time deployment. Each iteration consists of solving a nominal trajectory optimization with a quadratic program, updating a disturbance-feedback controller via Riccati recursions, and evaluating Jacobians of the nonlinear dynamics. The design is general and the provided code can be directly applied to systems with large state and input dimensions and long prediction horizons.\n\nReal-time feasibility (computation times) is demonstrated across different dynamics, including a quadcopter and a rocket landing. Robust performance is validated on the rocket-landing problem with steerable thrust including actuator dynamics, illustrated in Fig. 1, demonstrating robust constraint satisfaction with an average total latency of 19.7 [ms] per iteration.\nA comparison to a soft-constrained MPC baseline highlights increased safety and stability of the proposed approach.\n\nNotation:\nFor vectors or matrices aa and bb with the same number of rows, we denote their horizontal concatenation by [a,b][a,~b].\nWe denote stacked vectors or matrices by (a,b)=[a⊤,b⊤]⊤\\left(a,b\\right)=[a^{\\top},~b^{\\top}]^{\\top}. For a vector r∈ℝnr\\in\\mathbb{R}^{n}, we denote its ithi^{\\text{th}} component by rir_{i}.\nFor a sequence of matrices Mk,j∈ℝp×qM_{k,j}\\in\\mathbb{R}^{p\\times q}, indexed by k>j≥0k>j\\geq 0,\nwe define the shorthand horizontal concatenation M(k):=[Mk,k−1,Mk,k−2,…,Mk,0]∈ℝp×k​q.M_{(k)}\\vcentcolon=[M_{k,k-1},~M_{k,k-2},~\\dots,~M_{k,0}]\\in\\mathbb{R}^{p\\times kq}.\nFor a vector v∈ℝnv\\in\\mathbb{R}^{n}, we write its 1-norm as ‖v‖1=|v1|+…+|vn|\\|v\\|_{1}=|v_{1}|+\\ldots+|v_{n}| and its infinity norm as ‖v‖∞=maxi=1,…,n⁡|vi|\\|v\\|_{\\infty}=\\max_{i=1,\\ldots,n}|v_{i}|.\nFor a matrix M∈ℝm×nM\\in\\mathbb{R}^{m\\times n}, the matrix infinity norm is ‖M‖∞=maxi​∑j|Mi​j|\\|M\\|_{\\infty}=\\max_{i}\\sum_{j}|M_{ij}|.\nWe denote sets with calligraphic letters, e.g., 𝒲⊆ℝn\\mathcal{W}\\subseteq\\mathbb{R}^{n}.\nLet ℬm\\mathcal{B}^{m} be the unit ball defined by ℬm:={d∈ℝm|‖d‖∞≤1}\\mathcal{B}^{m}\\vcentcolon=\\{d\\in\\mathbb{R}^{m}|~\\|d\\|_{\\infty}\\leq 1\\}. The Minkowski sum of two sets 𝒜,𝒟⊆ℝn\\mathcal{A},\\mathcal{D}\\subseteq\\mathbb{R}^{n} is defined as 𝒜⊕𝒟:={a+d|a∈𝒜,d∈𝒟}\\mathcal{A}\\oplus\\mathcal{D}\\vcentcolon=\\{a+d|a\\in\\mathcal{A},d\\in\\mathcal{D}\\}.\nFor a vector-valued function f:ℝn→ℝqf:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{q}, we denote the Jacobian by ∂ϕ∂x|x∈ℝq×n\\frac{\\partial\\phi}{\\partial x}|_{x}\\in\\mathbb{R}^{q\\times n}. For the ithi^{\\text{th}} component (f)i(f)_{i}, we denote its Hessian by\n∂2(f)i∂x2|x∈ℝn×n\\frac{\\partial^{2}(f)_{i}}{\\partial x^{2}}|_{x}\\;\\in\\;\\mathbb{R}^{n\\times n}.\n\n1. Formal guarantees are provided, i.e., robust constraint satisfaction, recursive feasibility (Thm. 1), and input-to-state stability (Thm. 2). Recursive feasibility is ensured by a novel treatment of the mismatch with respect to the nonlinear nominal prediction.\n\n2. An efficient sequential convex programming algorithm tailored to the robust MPC formulation is provided to enable real-time deployment. Each iteration consists of solving a nominal trajectory optimization with a quadratic program, updating a disturbance-feedback controller via Riccati recursions, and evaluating Jacobians of the nonlinear dynamics. The design is general and the provided code can be directly applied to systems with large state and input dimensions and long prediction horizons.\n\n3. Real-time feasibility (computation times) is demonstrated across different dynamics, including a quadcopter and a rocket landing. Robust performance is validated on the rocket-landing problem with steerable thrust including actuator dynamics, illustrated in Fig. 1, demonstrating robust constraint satisfaction with an average total latency of 19.7 [ms] per iteration.\nA comparison to a soft-constrained MPC baseline highlights increased safety and stability of the proposed approach.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在存在扰动和模型不匹配的情况下确保机器人控制的安全性和稳定性。  \n2. 如何实现快速、可扩展的非线性模型预测控制，以满足实时实施的需求。  \n\n【用了什么创新的方案】  \n提出了一种鲁棒模型预测控制（MPC）方法，通过将不确定的非线性系统分解为名义非线性动态模型、扰动反馈控制器和模型误差界限，联合优化这些组件以确保鲁棒约束满足、输入到状态的稳定性和递归可行性。采用高效的顺序凸编程算法实现实时部署，验证了在不同动态下的实时可行性，特别是在火箭着陆问题中展示了鲁棒性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "An Extended Kalman Filter for Systems with Infinite-Dimensional Measurements",
            "authors": "Maxwell M. Varley,Timothy L. Molloy,Girish N. Nair",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO)",
            "comment": "8 pages",
            "pdf_link": "https://arxiv.org/pdf/2509.18749",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18749",
            "arxiv_html_link": "https://arxiv.org/html/2509.18749v1",
            "abstract": "This article examines state estimation in discrete-time nonlinear stochastic systems with finite-dimensional states and infinite-dimensional measurements, motivated by real-world applications such as vision-based localization and tracking.\nWe develop an extended Kalman filter (EKF) for real-time state estimation, with the measurement noise\nmodeled as an infinite-dimensional random field.\nWhen applied to vision-based state estimation, the measurement Jacobians required to implement the EKF are shown to correspond to image gradients.\nThis result provides a novel system-theoretic justification for the use of image gradients as features for vision-based state estimation, contrasting with their (often heuristic) introduction in many computer-vision pipelines.\nWe demonstrate the practical utility of the EKF on a public real-world dataset involving the localization of an aerial drone using video from a downward-facing monocular camera.\nThe EKF is shown to outperform VINS-MONO, an established visual-inertial odometry algorithm, in some cases achieving mean squared error reductions of up to an order of magnitude.",
            "introduction": "In this paper we focus on state estimation for systems with finite-dimensional states and infinite-dimensional measurements.\nThis focus is motivated by vision-based state estimation, control, and localization problems that arise across robotics [1, 2, 3] and control [4, 5, 6].\nIn such problems, the measurements take the form of images with dimensions determined by the camera’s resolution (i.e., number of pixels), while the underlying state of interest (e.g., position and orientation) is typically relatively low-dimensional.\nWith modern cameras offering increasingly high-resolution images, the emergent challenge in many of these problems is how best to estimate a low-dimensional state with arbitrarily high-dimensional measurements.\nTraditional approaches from computer vision and robotics for processing high-dimensional measurements rely on (spatial) feature extraction [3].\nHowever, such approaches may fail to exploit the dynamics and uncertainty of the state estimates in determining which features to extract or how to weight them in computing a state estimate.\nIn this paper, we therefore take a different approach by formulating an extended Kalman filter (EKF) capable of processing an entire (infinite-dimensional) image domain, and assigning dynamic weights (via gains) to every pixel based on its contribution to state estimates.\n\nAlthough Kalman filters and their nonlinear variants, such as the EKF, have long been used for estimation in robotics and control, the vast majority of vision-based filters are feature-based [7, 8, 6, 1, 3].\nThe use of these filters thus typically involves first reducing images to a sparse set of extracted keypoints before applying standard Kalman filter techniques.\nIn contrast, our novel EKF is capable of operating directly on dense image data (i.e., pixel intensities) directly in real-time, avoiding the need to extract features, and preserving the image structure.\nA key insight that allows the formulation of our filter, is the modeling of the measurement noise as an infinite-dimensional random field. This allows us to construct a continuous image-domain measurement model that naturally integrates with the structure of an infinite-dimensional EKF. In doing so, we are able to derive a system-theoretic justification for using image gradients in the filter update step. These image gradients are usually introduced heuristically in computer-vision pipelines (cf. [3]), but here emerge from the principles of the filter design itself.\n\nA number of different approaches have been employed in early works to derive the Kalman filter for distributed parameter systems [9, 10, 11, 12], although none of these works examined systems with finite-dimensional states and infinite-dimensional measurements as presented here. A survey contextualizing the methods and results of these early derivations is given in [13] and a modern, comprehensive examination of control and estimation of distributed parameter systems is given in [14]. In the case of nonlinear distributed parameter systems, the EKF is generally utilized either by reducing the dimensionality of the underlying system before designing the estimator (the early lumping approach) [15], or designing an infinite-dimensional distributed parameter EKF and using some discretization scheme for real-world implementation (the late lumping approach) [16].\n\nThis article makes the following key contributions, extending the optimal linear filter work in [17, 18].\nFirstly, the optimal linear filter originally derived in that prior work is generalized to construct an EKF for systems with finite-dimensional states and infinite-dimensional measurements, and with both nonlinear state dynamics and nonlinear measurement equations.\nWe provide a derivation of this EKF and establish and interpret the measurement Jacobians that arise within it, with the latter relating to image gradients in the case of image measurements.\nWe verify the efficacy of the EKF for vision-based state estimation on a real-world dataset.\nSpecifically, the filter estimates the position, velocity, acceleration, and yaw of an aerial drone equipped with an Inertial Measurement Unit (IMU) as well as optical cameras providing measurements in the form of grayscale downward-facing images. The estimates are evaluated against the ground truth included in the dataset, and the results are compared with the performance of the well-established monocular visual-inertial odometry algorithm VINS-MONO [2], showing that our filter generally achieves superior or comparable state estimation performance.\n\nThis article is structured as follows. Section II presents the notational definitions used throughout this work. Section III will define the system model that we will analyze, as well as the assumptions used throughout. Section IV will describe a linearization of the previously described system model, in preparation for an application of the optimal linear filter derived in [18] to this linearized system. Section V will present the filter procedure and two results, Proposition V.1 and Proposition V.2, which help to simplify the implementation and reduce the computational complexity of our algorithm. Section VI will give details pertaining to the dataset, as well as how the filter is implemented on the data within, and our chosen measures of filter performance. Using these performance metrics, Section VII demonstrates an empirical verification of the proposed filter and compares the results with those of VINS-MONO.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在有限维状态和无限维测量的系统中进行状态估计。  \n2. 传统特征提取方法在处理高维测量时的局限性。  \n3. 如何实时处理整个图像域以提高状态估计的准确性。  \n\n【用了什么创新的方案】  \n本文提出了一种扩展卡尔曼滤波器（EKF），能够直接在密集图像数据上进行实时状态估计，而无需提取特征。通过将测量噪声建模为无限维随机场，EKF能够动态地为每个像素分配权重，从而更好地利用图像梯度进行状态更新。这种方法提供了对图像梯度作为特征使用的系统理论依据，并在实际应用中表现出优于现有视觉惯性里程计算法的性能。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Dual Iterative Learning Control for Multiple-Input Multiple-Output Dynamics with Validation in Robotic Systems",
            "authors": "Jan-Hendrik Ewering,Alessandro Papa,Simon F.G. Ehlers,Thomas Seel,Michael Meindl",
            "subjects": "Systems and Control (eess.SY); Robotics (cs.RO)",
            "comment": "11 pages, 4 figures",
            "pdf_link": "https://arxiv.org/pdf/2509.18723",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18723",
            "arxiv_html_link": "https://arxiv.org/html/2509.18723v1",
            "abstract": "Solving motion tasks autonomously and accurately is a core ability for intelligent real-world systems.\nTo achieve genuine autonomy across multiple systems and tasks, key challenges include coping with unknown dynamics and overcoming the need for manual parameter tuning, which is especially crucial in complex  Multiple-Input Multiple-Output (MIMO) systems.",
            "introduction": "Accurate reference tracking is a critical control capability for a wide range of real-world applications, from industrial manufacturing to service robotics and biomedical systems [1, 2, 3], which often involve complex MIMO system dynamics.\nFor these systems to be effective and user-friendly, they must be capable of self-reliantly adapting to new tasks and environments. In other words, it is required to learn to perform reference tracking autonomously.\nThis capability is crucial to eliminate the need for expert-provided model information or time-consuming manual tuning.\n\nIn repetitive settings,  Iterative Learning Control (ILC) is an established method that enables highly accurate reference tracking, given a reference trajectory [4, 5, 6].\nHowever, satisfactory learning performance is almost always dependent on human expert knowledge, such as access to model information or the manual tuning of algorithmic (hyper) parameters.\nThe necessary manual tuning effort is typically even more aggravated in MIMO systems due to cross-coupling effects and scale variations between different inputs and outputs [7].\nMoreover, learning performance is often system- or reference-specific, which hinders genuine autonomous deployment across numerous systems and tasks.\n\nHence, an ILC method must possess the following three characteristics in order to enable real-world systems to autonomously learn to solve reference tracking tasks.\nFirst, the ILC method must neither require prior model information nor the manual tuning of parameters to enable autonomous application.\nSecond, the ILC method has to be applicable to MIMO dynamics, as these are often present in realistic settings.\nAnd third, the ILC method should be validated – ideally on multiple – real-world systems.\nBased on these criteria, we continue to review the state of research in ILC.\n\nFirst, there exists an extensive class of so-called  Model-Based Iterative Learning Control (MB-ILC) methods that can yield remarkable reference tracking performance in various real-world applications.\nFor example,  Norm-Optimal Iterative Learning Control (NO-ILC) schemes have been applied to gantry robots [8] and stroke rehabilitation [9], and NO-ILC can readily be applied to MIMO dynamics [10, 11].\nSimilarly,  Frequency-Domain Iterative Learning Control (FD-ILC) has been successfully applied to real-world systems with MIMO dynamics such as a marine vibrator [12] or nano-positioning systems [13].\nDespite these achievements, the aforementioned and other MB-ILC methods are limited because they require prior model information and typically involve manual tuning of learning parameters, which hinders their autonomous application.\n\nTo overcome the need for prior model information, so-called  Data-Driven Iterative Learning Control (DD-ILC) methods have been developed.\nA common approach is to use the input/output trajectory pairs from previous trials to estimate the gradient of the tracking errors with respect to the input trajectory to update the latter [14, 15, 16].\nOn the other hand, several DD-ILC schemes use experimental data to estimate a model of the plant dynamics and combine this plant approximation with well-known MB-ILC methods [11, 17].\nFor instance, some approaches use  Recursive Least Squares (RLS) to estimate a model of the system dynamics and combine it with NO-ILC or adaptive ILC [18, 19, 20].\nOther approaches combine  Iterative Learning Identification (ILI) and ILC [21, 22] to iteratively learn a model and input trajectory without prior model information.\nFD-ILC has been combined with an iterative learning approach for MIMO dynamics using a pseudo-inversion approach [23].\nNotably, there are approaches that iteratively learn a dynamic linearization of the plant dynamics, which can be utilized in a NO-ILC update law [24, 25, 26, 27, 28].\nWhat is common among all of these methods is that they overcome the need for prior model information, and many of them are applicable to MIMO dynamics.\nHowever, most of these methods have not been validated in real-world experiments, and all of these methods require the manual tuning of learning parameters, which precludes the autonomous application of the learning methods.\n\nTo overcome the need for prior model information and manual parameter tuning, MB-ILC has been combined with repeated model learning using Gaussian processes and self-parametrization schemes [29, 30, 31], and some of the methods have been validated on different real-world systems [29].\nHowever, these approaches are limited in terms of their applicability and validation in MIMO dynamics.\nWe, hence, conclude that there is no DD-ILC method that is autonomous in the sense that it neither requires prior model information nor manual parameter tuning, is applicable to MIMO dynamics, and has been validated on multiple—possibly real-world—systems.\n\nTo address these three issues, we propose a novel MIMO DILC framework that builds on previous results [32] and enables autonomous learning of reference tracking tasks in real-world systems with MIMO dynamics.\nSpecifically, the contributions of this paper are threefold:\n\nFirst, a novel DILC scheme for simultaneous model and control learning in MIMO systems, while requiring neither prior model information nor manual parameter tuning. It exploits a novel iterative learning paradigm that generalizes ILC approaches for iterative model learning, thus enabling the learning of system models using established ILC methods. The algorithmic architecture is illustrated in Figure 1.\n\nSecond, a theoretical analysis providing convergence conditions of the proposed algorithm under mild assumptions. We emphasize that iterative model learning in complex MIMO systems poses significant challenges, such as an overparametrized model, for which we present novel analysis to prove convergence.\n\nThird, an extensive empirical validation with two real-world MIMO systems and a six-degree-of-freedom industrial robot simulation. We demonstrate, in contrast to the vast majority of existing works, the truly autonomous learning capabilities of DILC without any model information or human tuning effort. To the best of our knowledge, this is the first time that a DD-ILC method has solved different reference tracking tasks in multiple real-world systems with MIMO dynamics, without requiring prior model information or manual parameter tuning. We highlight that DILC solves many reference tracking tasks within 1010-2020 trials and learns even complex motions in less than 100100 iterations.\n\nThis paper is structured as follows.\nWe formally define the considered problem in Section II and introduce preliminaries on ILC in Section III.\nThe proposed method and its theoretical properties are detailed in Section IV.\nThe simulative and experimental results are presented in Section V.\nFinally, we conclude the paper in Section VI.\n\nNotation: We denote the set of real numbers by ℝ\\mathbb{R}, the set of natural numbers by ℕ\\mathbb{N}, the set of all natural numbers greater than or equal to a∈ℕa\\in\\mathbb{N} by ℕ≥a\\mathbb{N}_{\\geq a}, and the set of natural numbers in the interval [a,b]⊂ℕ[a,b]\\subset\\mathbb{N} by ℕ[a,b]\\mathbb{N}_{[a,b]}.\nWe denote vectors (matrices) by lower-case (upper-case) letters in bold, e. g., 𝐯∈ℝN\\mathbf{v}\\in\\mathbb{R}^{N} (𝐀∈ℝN×N\\mathbf{A}\\in\\mathbb{R}^{N\\times N}).\nIf not explicitly stated, all vectors are column vectors, and by writing [𝐯]i[\\mathbf{v}]_{i}, we refer to the ii-th entry of 𝐯\\mathbf{v}.\nBy writing [𝐀]i,j[\\mathbf{A}]_{i,j}, we refer to the ii-th entry of the jj-th column of 𝐀\\mathbf{A}.\nTo vectorize 𝐀\\mathbf{A}, we write vec​(𝐀)\\mathrm{vec}(\\mathbf{A}).\nThe Euclidean norm of a vector 𝐯\\mathbf{v} is denoted by ‖𝐯‖\\left\\lVert\\mathbf{v}\\right\\rVert, and the induced Euclidean norm of a matrix 𝐀\\mathbf{A} is denoted by ‖𝐀‖\\left\\lVert\\mathbf{A}\\right\\rVert.\nThe weighted norm with respect to a positive definite matrix 𝐖≻0\\mathbf{W}\\succ 0 with 𝐖=𝐖⊤\\mathbf{W}=\\mathbf{W}^{\\top} is denoted by ‖𝐯‖𝐖=𝐯⊤​𝐖𝐯\\left\\lVert\\mathbf{v}\\right\\rVert_{\\mathbf{W}}=\\sqrt{\\mathbf{v}^{\\top}\\mathbf{W}\\mathbf{v}}.\nWe denote the identity matrix of size N×NN\\times N by 𝐈N\\mathbf{I}_{N}, and the zero matrix of suitable dimension by 𝟎\\mathbf{0}.\nThe Kronecker product of two matrices 𝐀\\mathbf{A} and 𝐁\\mathbf{B} is 𝐀⊗𝐁\\mathbf{A}\\otimes\\mathbf{B}.\nWe recall that a function α:ℝ≥0→ℝ≥0\\alpha:\\mathbb{R}_{\\geq 0}\\rightarrow\\mathbb{R}_{\\geq 0} is of class 𝒦\\mathscr{K} if it is continuous, strictly increasing, and satisfies α​(0)=0\\alpha(0)=0.\nBy ℒ\\mathscr{L}, we refer to the class of functions θ:ℝ≥0→ℝ≥0\\theta:\\mathbb{R}_{\\geq 0}\\rightarrow\\mathbb{R}_{\\geq 0} that are continuous, non-increasing, and satisfy lims→∞θ​(s)=0\\lim_{s\\rightarrow\\infty}\\theta(s)=0, and by 𝒦​ℒ\\mathscr{K}\\negthinspace\\negthinspace\\mathscr{L} to the class of functions β:ℝ≥0×ℝ≥0→ℝ≥0\\beta:\\mathbb{R}_{\\geq 0}\\times\\mathbb{R}_{\\geq 0}\\rightarrow\\mathbb{R}_{\\geq 0} with β​(⋅,s)∈𝒦\\beta(\\cdot,s)\\in\\mathscr{K} and β​(r,⋅)∈ℒ\\beta(r,\\cdot)\\in\\mathscr{L} for any fixed s∈ℝ≥0s\\in\\mathbb{R}_{\\geq 0} and r∈ℝ≥0r\\in\\mathbb{R}_{\\geq 0}, respectively.\nLast, we denote the space of all block-lower-triangular Toeplitz matrices of dimension NN with sub-matrices, ∀n∈ℕ[1,N]\\forall n\\in\\mathbb{N}_{[1,N]}, 𝐓¯n∈ℝL×M\\bar{\\mathbf{T}}_{n}\\in\\mathbb{R}^{L\\times M}, by 𝒯NL,M\\mathcal{T}^{L,M}_{N}, that is,\n\n1. First, a novel DILC scheme for simultaneous model and control learning in MIMO systems, while requiring neither prior model information nor manual parameter tuning. It exploits a novel iterative learning paradigm that generalizes ILC approaches for iterative model learning, thus enabling the learning of system models using established ILC methods. The algorithmic architecture is illustrated in Figure 1.\n\n2. Second, a theoretical analysis providing convergence conditions of the proposed algorithm under mild assumptions. We emphasize that iterative model learning in complex MIMO systems poses significant challenges, such as an overparametrized model, for which we present novel analysis to prove convergence.\n\n3. Third, an extensive empirical validation with two real-world MIMO systems and a six-degree-of-freedom industrial robot simulation. We demonstrate, in contrast to the vast majority of existing works, the truly autonomous learning capabilities of DILC without any model information or human tuning effort. To the best of our knowledge, this is the first time that a DD-ILC method has solved different reference tracking tasks in multiple real-world systems with MIMO dynamics, without requiring prior model information or manual parameter tuning. We highlight that DILC solves many reference tracking tasks within 1010-2020 trials and learns even complex motions in less than 100100 iterations.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在复杂的MIMO系统中实现自主的参考跟踪任务？  \n2. 如何消除对先前模型信息和手动参数调整的需求？  \n3. 如何在多个真实世界系统中验证学习方法的有效性？  \n\n【用了什么创新的方案】  \n提出了一种新的MIMO DILC框架，能够在不需要先前模型信息或手动参数调整的情况下，实现对参考跟踪任务的自主学习。该框架结合了迭代学习控制（ILC）方法，提供了理论分析以证明算法的收敛性，并通过在两个真实世界的MIMO系统和一个六自由度工业机器人仿真中的广泛验证，展示了其真正的自主学习能力。  \n\n【相关性评分】  \n分数：4分",
            "llm_score": 4,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction",
            "authors": "Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO); Image and Video Processing (eess.IV)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18566",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18566",
            "arxiv_html_link": "https://arxiv.org/html/2509.18566v1",
            "abstract": "Reconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond temporal resolution, making them a superior sensing choice for dynamic human reconstruction. Accordingly, we present a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. Specifically, a unified set of 3D Gaussians carries a learnable semantic attribute; only Gaussians classified as human undergo deformation for animation, while scene Gaussians stay static. To combat blur, we propose an event-guided loss that matches simulated brightness changes between consecutive renderings with the event stream, improving local fidelity in fast-moving regions. Our approach removes the need for external human masks and simplifies managing separate Gaussian sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.",
            "introduction": "Human reconstruction from monocular videos is a critical task in computer vision and graphics, with applications spanning virtual reality [1], augmented reality [2], and film production [3]. Recent neural rendering advancements, including Neural Radiance Fields (NeRFs)[4] and 3D Gaussian Splatting (3DGS)[5], enable highly-fidelity, photorealistic 3D reconstruction. Building on this, various 3D human reconstruction methods have emerged. Examples include 3DGS-Avatar [6] and ASH [7], which focus on animatable avatars, and HUGS [8], which reconstructs human and scene simultaneously using separate Gaussian sets.\n\nDespite these promising results, existing methods still face significant challenges.\nFirst, most approaches require an external human mask, necessitating a prior segmentation step that can introduce artifacts.\nSecond, rapid human motion in frame-based camera captures often leads to motion blur, deteriorating image quality.\nWhile some methods attempt to deblur RGB images or integrate event data for reconstruction, their generalizability is limited.\nExFMan [9] is a notable exception that leverages event data for dynamic human reconstruction but lacks static scene modeling.\n\nTo address these challenges, we introduce a unified framework for reconstructing animatable humans and static scenes from a monocular event camera (Fig. 1).\nUnlike HUGS [8], which uses separate Gaussian sets, our method encodes both human and scene in a single set of 3D Gaussians with semantic attributes, refined during training via rendering feedback.\nFurthermore, synthetic events generated from rendered images are aligned with real event streams, providing supervision that alleviates motion blur.\n\nWe evaluate our method on two newly created datasets, ZJU-MoCap-Blur and MMHPSD-Blur, generated by simulating motion blur to test performance under challenging conditions.\nExperiments show that our unified human-scene reconstruction framework surpasses the state-of-the-art HUGS [8], with notable gains on ZJU-MoCap-Blur: +19.5%19.5\\% PSNR, +3.95%3.95\\% SSIM, and –32.5%32.5\\% LPIPS.\nIn summary, our main contributions are:\n\nA novel framework for unified human and scene reconstruction using a single semantically attributed set of 3D Gaussians.\n\nThe integration of event data to mitigate motion blur and enhance the reconstruction quality of fast-moving subjects.\n\nAn extensive evaluation on self-generated motion-blurred datasets that demonstrates state-of-the-art performance in challenging high-speed scenarios.\n\n1. A novel framework for unified human and scene reconstruction using a single semantically attributed set of 3D Gaussians.\n\n2. The integration of event data to mitigate motion blur and enhance the reconstruction quality of fast-moving subjects.\n\n3. An extensive evaluation on self-generated motion-blurred datasets that demonstrates state-of-the-art performance in challenging high-speed scenarios.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何从单目视频中重建动态人类与静态场景，尤其是在快速运动下。  \n2. 现有方法依赖外部人类掩码，增加了处理复杂性与潜在伪影。  \n3. RGB帧在快速运动中容易产生运动模糊，影响重建质量。  \n\n【用了什么创新的方案】  \n提出了一种新颖的事件引导的人类-场景重建框架，通过单一的3D高斯点集联合建模动态人类和静态场景。该方法利用事件数据来减轻运动模糊，并通过事件引导损失提高快速运动区域的重建质量。此外，框架简化了高斯点集的管理，消除了对外部人类掩码的需求。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Policy Gradient with Self-Attention for Model-Free Distributed Nonlinear Multi-Agent Games",
            "authors": "Eduardo Sebastián,Maitrayee Keskar,Eeman Iqbal,Eduardo Montijano,Carlos Sagüés,Nikolay Atanasov",
            "subjects": "Systems and Control (eess.SY); Multiagent Systems (cs.MA); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18371",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18371",
            "arxiv_html_link": "https://arxiv.org/html/2509.18371v1",
            "abstract": "Multi-agent games in dynamic nonlinear settings are challenging due to the time-varying interactions among the agents and the non-stationarity of the (potential) Nash equilibria. In this paper we consider model-free games, where agent transitions and costs are observed without knowledge of the transition and cost functions that generate them. We propose a policy gradient approach to learn distributed policies that follow the communication structure in multi-team games, with multiple agents per team. Our formulation is inspired by the structure of distributed policies in linear quadratic games, which take the form of time-varying linear feedback gains. In the nonlinear case, we model the policies as nonlinear feedback gains, parameterized by self-attention layers to account for the time-varying multi-agent communication topology. We demonstrate that our distributed policy gradient approach achieves strong performance in several settings, including distributed linear and nonlinear regulation, and simulated and real multi-robot pursuit-and-evasion games.",
            "introduction": "Multi-robot problems encompass a variety of expected behaviors [1, 2, 3, 4], including cooperative, conflicting or competitive actions. For instance, in a perimeter-defense setting [5, 6], multiple teams must coordinate to effectively defend a region from potential attackers (Fig. 1). These kinds of problems can be formulated as multi-team dynamic games [7, 8, 9, 10, 11], where each multi-agent team is viewed as a player with specific goals and constraints, and where agents interact with teammates (intra-team interactions) and agents on other teams (inter-team interactions). These settings are typically nonlinear and dynamic, requiring complex interactions that evolve with time as a function of how the agents play the game. These challenging features are specially relevant when we seek distributed policies subject to the communication constraints imposed by the topology of the teams; and in the absence of a mathematical description of the game dynamics and costs, demanding model-free approaches that only rely on transition and cost samples to assess the performance of the teams. Inspired by distributed policies in linear quadratic games, we present a novel policy gradient approach to learn distributed policies for nonlinear dynamic games that are both effective and scalable.\n\nModel-based methods for nonlinear multi-agent dynamic games rely on iterative linearization of the system dynamics and quadratic approximation of the game cost [12, 13, 14]. This allows for fast computation with guarantees of convergence to a saddle configuration [15] but imposes a centralized calculation that limits the applicability in distributed settings, where the agent communication is restricted according to a graph topology. To overcome such limitations, it is possible to restrict the class of nonlinear multi-agent dynamic games to potential games [16, 17, 18], where it is assumed that a potential function exists such that the relative incentives in modifying one agent’s policy is equal to the difference in value of the potential function. Under this constraint, it is possible to derive algorithms that compute open-loop optimal trajectories for the agents under centralized [19] or distributed topological constraints [20, 17]. However, open-loop policies lack robustness and require knowledge on how the multi-agent topology will evolve with time. In contrast, we propose a novel policy parameterization that is distributed by construction and does not require network topology prediction. In all previous cases, a model of the system dynamics and the structure of the cost function is needed to compute the actions. Instead, to address general nonlinear multi-agent dynamic games, we propose a model-free policy gradient approach that relies only on transition and cost samples.\n\nModel-free solutions for games are limited due to the non-stationarity of the Nash equilibria (if one exists) [21, 22]. Traditional approaches either focus on providing theoretical guarantees of convergence or addressing practical settings assuming the existence of such Nash equilibria. An instance of the former is [23], where distributed linear quadratic regulators are learned assuming that the sequence of graphs representing the communication structure of the game is known. From a different perspective, when the linear cost function is known and the strategies of all players are available, the problem can be posed as a multi-team distributed optimization program [24, 25]. In practical settings, existing solutions rely on multi-agent reinforcement learning algorithms [26] that consider independent heterogeneous agents to apply policy gradient methods [27, 28, 29, 30]. In this work, we bring together the benefits of both alternatives by proposing a self-attention-based policy parameterization built from first principles and which enforces distributed execution constraints. The distributed policy is trained using a policy gradient learning method that considers, simultaneously, the policies of all the agents from all different teams, addressing the non-stationarity of the game in practice.\n\nOur main contribution is a policy gradient method for learning distributed policies in model-free nonlinear multi-agent dynamic games (Sec. II). Our approach uses a nonlinear feedback gain formulation of the agent policies, parameterized using self-attention layers (Sec. III). The use of self-attention enables to enforce intra- and inter-team graph constraints, handling time-varying communication and achieving invariance with respect to the total number of agents. Furthermore, a neural network parameterization of the policies motivates the use of a policy gradient method to learn effective and scalable policies in the model-free setting.\nThe method also allows to learn heterogeneous policies per team, such that the teams adjust to specific goals. We demonstrate that our method applies broadly, from linear quadratic settings under topology constraints to multi-agent reinforcement learning in competitive games with simulated and real robots (Sec. IV).",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在动态非线性多智能体游戏中学习分布式策略。  \n2. 如何处理多智能体之间的时间变化通信拓扑。  \n3. 如何在缺乏系统动态和成本函数模型的情况下实现模型无关的学习。  \n\n【用了什么创新的方案】  \n提出了一种基于自注意力机制的策略梯度方法，用于学习动态非线性多智能体游戏中的分布式策略。该方法通过非线性反馈增益的参数化，能够处理时间变化的多智能体通信结构，并且不需要对网络拓扑进行预测。通过使用神经网络参数化，方法能够有效地学习异质策略，适应不同团队的具体目标，并在多种设置中展示了良好的性能。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata",
            "authors": "Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers",
            "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
            "comment": "Accepted at NeurIPS 2025",
            "pdf_link": "https://arxiv.org/pdf/2509.18350",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18350",
            "arxiv_html_link": "https://arxiv.org/html/2509.18350v1",
            "abstract": "Accurate visual localization from aerial views is a fundamental problem with applications in mapping, large-area inspection, and search-and-rescue operations. In many scenarios, these systems require high-precision localization while operating with limited resources (e.g., no internet connection or GNSS/GPS support), making large image databases or heavy 3D models impractical. Surprisingly, little attention has been given to leveraging orthographic geodata as an alternative paradigm, which is lightweight and increasingly available through free releases by governmental authorities (e.g., the European Union). To fill this gap, we propose OrthoLoC, the first large-scale dataset comprising 16,425 UAV images from Germany and the United States with multiple modalities. The dataset addresses domain shifts between UAV imagery and geospatial data. Its paired structure enables fair benchmarking of existing solutions by decoupling image retrieval from feature matching, allowing isolated evaluation of localization and calibration performance. Through comprehensive evaluation, we examine the impact of domain shifts, data resolutions, and covisibility on localization accuracy. Finally, we introduce a refinement technique called AdHoP, which can be integrated with any feature matcher, improving matching by up to 95% and reducing translation error by up to 63%. The dataset and code are available at: https://deepscenario.github.io/OrthoLoC.",
            "introduction": "Visual localization for Unmanned Aerial Vehicles is essential for digital-twin modeling [60, 74], surveillance [29], search-and-rescue [51], and infrastructure inspection [34], yet faces unique challenges not addressed by ground-level localization systems. While ground-level approaches [56, 71, 70] benefit from similar viewpoints between images [59, 49, 57], aerial applications encounter dramatic perspective differences and require scalability over large areas [69, 72].\n\nCurrent UAV localization algorithms rely on retrieving the closest match from a database of posed images [72, 77], which is inaccurate, or on 3D models of the scene [69, 66], which are memory and computationally expensive. In limited resources settings, as it is often the case for connectivity-limited environments, this can result in accuracy degradation. Recent approaches like LoDLoc [78] improve storage efficiency by using  Level-of-Detail (LoD) but still assume unchanged environments, perform poorly in building-sparse areas such as highways, and its initialization depends on positioning sensors.\n\nIn contrast, a compelling solution involves geodata, such as orthographic aerial views (Digital Orthophotos) and elevation maps (Digital Surface Models). These provide a reliable, lightweight source for localizing UAV images, as shown in Figure˜1. Such data is increasingly accessible through free releases from European government geoportals [46, 17], and where public access is limited, can be synthesized using photogrammetric tools [20]. Geodata are scalable and better suited for low-resource settings. For example, covering an area of approximately 0.265 km² would require a 3D model of around 8 GB [69], whereas geodata requires about 30 times less memory. Surprisingly, no existing UAV localization approach seems to fully leverage these data sources. We believe this is mainly due to the absence of aligned cross-domain datasets and the lack of full-pose paired large-scale benchmarks specifically designed for localization using these types of geodata.\n\nTo fill this gap, we capture and release the  Orthographic Aerial Localization and Calibration Dataset (OrthoLoC). It comprises 5 main modalities such as UAV imagery, DOPs, DSMs, 3D point maps, and 3D meshes with a total of 16.4K images captured in 47 regions in 19 cities across 2 countries. Our dataset is the first to offer three key advantages: (1) paired UAV-geodata structure that decouples pose estimation from image retrieval, eliminating confounding error sources in the evaluations; (2) precise 6-DoF poses obtained through multi-view georeferenced photogrammetric reconstruction; and (3) additional reference data sources to increase the domain gaps in the dataset.\n\nWe have evaluated state-of-the-art methods on this novel localization and calibration task in a comprehensive benchmark. Additionally, we introduce a method-agnostic refinement technique called  Adaptive Homography Preconditioning (AdHoP) that further improves localization and calibration accuracy. The technique exploits the uniform structure of DOPs to perform homography-based warping by assuming quasi-planar surfaces common in built environments.\n\nOur evaluation reveals several insights. First, state-of-the-art matching algorithms can generalize to aerial perspectives but struggle with the substantial domain gap between perspective UAV imagery and orthographic reference data. Second, our AdHoP technique significantly reduces the perspective disparity, improving all metrics across the tested methods, particularly achieving up to 95% and 63% enhancements in matching and translation accuracy, respectively. Third, camera calibration in aerial settings presents unique challenges due to fundamental geometric ambiguities that affect parameters estimation. Finally, reference data characteristics including domain shifts, data resolutions, and covisibility. significantly impact localization performance, with higher resolution geodata providing improvement in accuracy.\n\nThe main contributions of this paper are: (1) OrthoLoC, the first UAV dataset providing alignment with geodata across multiple modalities and locations; (2) a unified benchmarking framework for UAV localization and calibration that integrates with state-of-the-art matching algorithms and includes our AdHoP technique for addressing perspective disparities; and (3) benchmarking results for camera localization and calibration and an analysis of performance factors including cross-domain challenges, data resolution effects, and covisibility.",
            "llm_summary": "【关注的是什么问题】  \n1. UAV视觉定位的准确性和资源限制下的挑战  \n2. 现有UAV定位算法对地理数据的利用不足  \n3. 跨域数据集缺乏导致的评估困难  \n\n【用了什么创新的方案】  \n本研究提出了OrthoLoC数据集，包含16,425幅UAV图像及多种地理数据，旨在解决UAV图像与地理数据之间的领域转移问题。通过配对结构，该数据集能够独立评估定位和校准性能。此外，提出的AdHoP技术可与任何特征匹配器集成，显著提高匹配精度和降低平移误差。  \n\n【相关性评分】  \n分数：2分",
            "llm_score": 2,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought",
            "authors": "Yu Ti Huang",
            "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18200",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18200",
            "arxiv_html_link": "https://arxiv.org/html/2509.18200v1",
            "abstract": "Conversational agents must translate egocentric utterances (e.g., “on my right”) into allocentric orientations (N/E/S/W). This challenge is particularly critical in indoor or complex facilities where GPS signals are weak and detailed maps are unavailable. While chain-of-thought (CoT) prompting has advanced reasoning in language and vision tasks, its application to multimodal spatial orientation remains underexplored. We introduce Conversational Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese conversational navigation projected from real-world environments, addressing egocentric-to-allocentric reasoning in non-English and ASR-transcribed scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: (1) extracting spatial relations, (2) mapping coordinates to absolute directions, and (3) inferring user orientation. A curriculum learning strategy progressively builds these capabilities on Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of resource-constrained settings. Experiments show that MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. Moreover, MCoT demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching. The model also maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity. These findings highlight the potential of structured MCoT spatial reasoning as a path toward interpretable and resource-efficient embodied navigation. Code and data are available at https://github.com/yu-ti-huang/Conversational-Orientation-Reasoning.",
            "introduction": "Humans naturally describe spatial environments in egocentric (agent-centric) terms (e.g., “The exit is on my right”), whereas navigation systems typically operate on allocentric (world-centric) orientations such as north, south, east, and west. Conversational navigation has emerged as a promising paradigm that enables users to specify goals through dialogue, offering a natural and human-centered means of guidance in unfamiliar environments Sundar et al. (2024); Sheshadri & Hara (2024); Kaniwa et al. (2024); Liu et al. (2024); Levi & Kadar (2025). However, the crucial problem of grounding egocentric language into allocentric orientation remains underexplored. Current approaches typically assume access to GPS, detailed maps, or fixed global frames de Vries et al. (2018); Chen et al. (2020), and have concentrated primarily on English-based scenarios. Recent progress has also relied heavily on large-scale models Ghosh et al. (2024); Tang et al. (2023), which show strong reasoning abilities but demand substantial computational resources, hindering deployment in resource-constrained settings such as mobile navigation and edge devices.\n\nResearch in embodied AI and MCoT has advanced vision-language navigation and action planning Mu et al. (2023); Sun et al. (2024); Liu et al. (2025); Shen et al. (2025); Pareek et al. (2024), but orientation reasoning from natural language has been largely overlooked. These approaches typically assume that the agent’s orientation is already known or operate on high-level action spaces rather than inferring fundamental spatial relationships de Vries et al. (2018); Chen et al. (2020). Meanwhile, large audio-language models (LALMs) Zhang et al. (2023); Xie & Wu (2024); Fu et al. (2025); Défossez et al. (2024) have advanced speech understanding and dialogue Tang et al. (2024); Gong et al. (2023); Ghosh et al. (2024); Kong et al. (2024), yet their reasoning abilities remain limited to perception-level tasks such as transcription or summarization yu Huang et al. (2024); Yang et al. (2024); Wang et al. (2025); Shi et al. (2025). While recent efforts like Audio-CoT Ma et al. (2025) show promise for enhanced speech-based reasoning, the challenge of transforming egocentric spatial descriptions into allocentric orientation inference remains unaddressed.\n\nTo address this gap, we introduce Conversational Orientation Reasoning (COR), a new benchmark for egocentric-to-allocentric orientation reasoning in Traditional Chinese conversational navigation. COR is derived from real-world urban transportation environments in Taiwan, projected into structured grid representations. Unlike prior studies that rely on vision or raw audio, COR combines ASR-transcribed egocentric language with structured landmark coordinates, evaluating both clean text and ASR transcripts to simulate realistic recognition errors. COR addresses the lack of non-English benchmarks in multimodal spatial reasoning, particularly under noisy ASR conditions.\n\nOur study is guided by three research questions:\n\nRQ1 (Effectiveness): How effective is multimodal CoT prompting for orientation reasoning compared to unimodal and unstructured baselines?\n\nRQ2 (Component analysis): What are the contributions of ASR preprocessing, multimodal fusion, and structured CoT steps?\n\nRQ3 (Robustness and generalization): How robust is the approach to linguistic variation, and how well does it generalize across different spatial domains?\n\nOur contributions are as follows:\n\nTask and benchmark. We introduce the COR benchmark for egocentric-to-allocentric orientation reasoning, combining ASR-transcribed speech with landmark coordinates.\n\nFramework. We develop a multimodal CoT framework with a structured three-step reasoning process that integrates noisy transcripts with spatial signals for orientation inference.\n\nEvaluation. We provide extensive experiments in Traditional Chinese, demonstrating effectiveness, component contributions, and robustness validation across linguistic variation, cross-domain generalization, and referential ambiguity beyond English-centric research.\n\n1. RQ1 (Effectiveness): How effective is multimodal CoT prompting for orientation reasoning compared to unimodal and unstructured baselines?\n\n2. RQ2 (Component analysis): What are the contributions of ASR preprocessing, multimodal fusion, and structured CoT steps?\n\n3. RQ3 (Robustness and generalization): How robust is the approach to linguistic variation, and how well does it generalize across different spatial domains?",
            "llm_summary": "【关注的是什么问题】  \n1. 如何将自我中心的语言描述转换为世界中心的方向推理？  \n2. 在缺乏GPS和详细地图的情况下，如何实现室内导航的有效性？  \n3. 如何在多模态环境中处理ASR识别错误和语言变化？  \n\n【用了什么创新的方案】  \n我们提出了对话方向推理（COR），这是一个新的基准，旨在解决自我中心到世界中心的方向推理问题。我们开发了一个多模态链式思维（MCoT）框架，通过结构化的三步推理过程，将ASR转录的语音与地标坐标相结合，提取空间关系、映射坐标到绝对方向，并推断用户的方向。该框架在资源受限的环境中表现出色，能够有效应对噪声条件下的对话和跨领域评估。  \n\n【相关性评分】  \n分数：5分",
            "llm_score": 5,
            "llm_error": ""
        },
        {
            "crawl_datetime": "2025-09-24 07:58:27",
            "title": "MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation",
            "authors": "Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin",
            "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Robotics (cs.RO)",
            "comment": "",
            "pdf_link": "https://arxiv.org/pdf/2509.18198",
            "code": "",
            "arxiv_abs_link": "https://arxiv.org/abs/2509.18198",
            "arxiv_html_link": "https://arxiv.org/html/2509.18198v1",
            "abstract": "Autonomous systems have advanced significantly, but challenges persist in accident-prone environments where robust decision-making is crucial. A single vehicle’s limited sensor range and obstructed views increase the likelihood of accidents. Multi-vehicle connected systems and multi-modal approaches, leveraging RGB images and LiDAR point clouds, have emerged as promising solutions. However, existing methods often assume the availability of all data modalities and connected vehicles during both training and testing, which is impractical due to potential sensor failures or missing connected vehicles. To address these challenges, we introduce a novel framework MMCD (Multi-Modal Collaborative Decision-making) for connected autonomy. Our framework fuses multi-modal observations from ego and collaborative vehicles to enhance decision-making under challenging conditions. To ensure robust performance when certain data modalities are unavailable during testing, we propose an approach based on cross-modal knowledge distillation with a teacher-student model structure. The teacher model is trained with multiple data modalities, while the student model is designed to operate effectively with reduced modalities. In experiments on connected autonomous driving with ground vehicles and aerial-ground vehicles collaboration, our method improves driving safety by up to 20.7%{\\it 20.7}\\%, surpassing the best-existing baseline in detecting potential accidents and making safe driving decisions. More information can be found on our website https://ruiiu.github.io/mmcd.",
            "introduction": "Autonomous technology has rapidly evolved over the past few decades, with advancements in perception [1, 2, 3, 4, 5], decision-making [6, 7, 8], and control systems [9, 10]. However, the deployment of autonomous vehicles still face challenges, particularly in accident-prone scenarios. These scenarios demand high robustness and reliability, as any failure in decision-making could have severe consequences. A single vehicle navigating these scenarios is prone to have accidents due to occlusions and limited sensor range. One promising solution to mitigate these risks is to have multi-vehicle connected systems [11, 12, 13]. By sharing information, vehicles can expand their field of view and reduce the chances of accidents. Another promising direction is the use of multi-modal data [14, 15, 16, 17], such as RGB images and LiDAR point clouds, to enhance the perception and decision-making capabilities of autonomous systems. Recent works have combined these two paradigms to develop multi-vehicle, multi-modal systems [18, 19, 20, 21], leveraging both connectivity and diverse sensor data to further improve autonomous driving performance.\n\nHowever, existing works often assume that the ego vehicle has consistent access to all sensors and connected vehicles during both training and testing. For example, methods utilizing both RGB and LiDAR data for training [14, 15, 22] assume the availability of both modalities during testing. This is not always realistic; for instance, LiDAR sensors may malfunction or become unavailable during testing, leaving only RGB data accessible, or some connected vehicles may not be able to share data, as shown in Fig. 1. Additionally, cost efficiency is a crucial consideration, as LiDAR sensors are more expensive than RGB cameras. Reducing the reliance on LiDAR sensors while still achieving high performance with RGB-only models during testing presents a more cost-effective solution.\n\nTo address these challenges, we introduce a novel multi-modal collaborative decision-making framework for connected autonomy, enabling the ego vehicle to make informed decisions by leveraging shared multi-modal data from collaborative vehicles. To handle scenarios where certain data modalities are missing during testing, we propose an approach based on knowledge distillation (KD) with a teacher-student model structure. Our multi-modal framework serves as the teacher model, trained with multiple data modalities (e.g., RGB and LiDAR), while the student model operates with reduced modalities (e.g., RGB). The knowledge distillation process ensures the student model maintains robust performance even with missing modalities during test time.\n\nIn summary, the main contributions of this paper are:\n\nWe introduce MMCD, a novel multi-modal collaborative decision-making framework for connected autonomy. Our approach fuses single or multi-modal observations provided by ego or connected vehicles in a principled way to make decisions for the ego vehicle in accident-prone scenarios. Our method improves the driving safety by up to 20.7%\\bf 20.7\\% in experiments on connected autonomous driving with ground vehicles and aerial-ground vehicles collaboration, outperforming the best-existing baseline.\n\nWe propose a cross-modal knowledge distillation-based approach for MMCD. Our model is trained with multi-modal cues (e.g., LiDAR and RGB) from connected vehicles but executes using single-modality observations (e.g., RGB). This design ensures robust performance in the presence of missing modalities during testing.\n\n1. We introduce MMCD, a novel multi-modal collaborative decision-making framework for connected autonomy. Our approach fuses single or multi-modal observations provided by ego or connected vehicles in a principled way to make decisions for the ego vehicle in accident-prone scenarios. Our method improves the driving safety by up to 20.7%\\bf 20.7\\% in experiments on connected autonomous driving with ground vehicles and aerial-ground vehicles collaboration, outperforming the best-existing baseline.\n\n2. We propose a cross-modal knowledge distillation-based approach for MMCD. Our model is trained with multi-modal cues (e.g., LiDAR and RGB) from connected vehicles but executes using single-modality observations (e.g., RGB). This design ensures robust performance in the presence of missing modalities during testing.",
            "llm_summary": "【关注的是什么问题】  \n1. 如何在事故多发环境中实现鲁棒的决策制定？  \n2. 如何处理测试时缺失数据模态的情况？  \n\n【用了什么创新的方案】  \n提出了MMCD（Multi-Modal Collaborative Decision-making）框架，通过融合来自自我和协作车辆的多模态观察，增强决策能力。采用基于知识蒸馏的教师-学生模型结构，教师模型使用多模态数据进行训练，而学生模型则在测试时能够有效地使用减少的模态，从而确保在某些数据模态缺失时仍能保持鲁棒性能。  \n\n【相关性评分】  \n分数：3分",
            "llm_score": 3,
            "llm_error": ""
        }
    ]
}